Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
Here comes a year full of knowledge & learning!,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Text Mining / Text Analytics / NLP  Sr Resource  Gurgaon (5+ years of experience)|New Year Resolutions for a Data Scientist|
Kunal Jain
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dear AVian,2015 was year of growth for us  we transformed from a blog to a community of data scientists. We launched our discussion portals, hackathon platform and job portal  and each of this initiative is shaping up well.Our mission of buildinga nation of data scientists is coming live every day. And all of this happened only because of your enthusiasm and support. We couldnt have thought of this journey without direction from you. Your comments, emails, letters and queries helps us decide our next steps.So, on this day of new year, I want to hear more from you. I want to hear from youwhat bothers you the most about data science? What aspects of data science you find hard to learn? What is blocking your successful journey? What is one thing you would want us to do right in 2016?Simply reply to this email on write in comments below.I dont know how things will turn out. But, I promise, Ill try my best to give you manyreasons of happiness & joy in 2016.Wishing you all a year full of joy, health, curiosity, knowledge and perseverance.Im waiting for your reply.Regards
Kunal Jain
Founder & CEO
Analytics Vidhya",https://www.analyticsvidhya.com/blog/2015/12/wishes-year-2016-analytics-vidhya/
New Year Resolutions for a Data Scientist,Learn everything about Analytics|Introduction|New Year Resolutions for a Data Scientist|Beginner Level|Intermediate Level|Advanced Level|End Notes,"1. Start with a programming language. Either R or Python|2. Learn Statistics & Mathematics|3. Enroll in one MOOC at a time (Most Difficult)|4. Engage, Discover and Socialize in Industry|1.Understand and Build your Machine Learning Skills|2. Focus on Ensemble and Boosting Algorithms|3. Explore Spark, NoSQL and other Big Data Tools|4. Educate Community Members|5. Participate in Data Science Competitions||1.Build a Deep Learning Model|2. Give Back to Community|3. Explore Reinforcement Learning|4. Rank in Top 50 on Kaggle|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Here comes a year full of knowledge & learning!|8 Proven Ways for improving the Accuracy of a Machine Learning Model|
Analytics Vidhya Content Team
|39 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"New Year is not just replacing your table calendar with a new one or waking up next morning rubbing your eyes.Its celebrating the joy of a new beginning. It gives aperfect reasontoinculcatenew habits. It is the arrival of new Hope.If you are reading this, Im sure data science excites you! You want 2016 to be a game changing year for you. Dont you?You can make it possible, if you commit to these resolutions today. You must understand that becoming a data scientist is a process, not an event. Its not an overnight success. Hence, you must patiently work towards your goal.Ive shared a list of resolutions every data scientist should make depending on where he / she is in their journey. This is of course a generic list, you should adopt it for your needs. I am also provided a checklist below which can be downloaded to track these goals.Note: These are generic resolutions meant for an aspiring / experienced data scientist. This article might not be useful for people from domain other than analytics.Ive categorized these resolutions according to three levels in the life of a data scientist. You decide which suits you the best and work accordingly. You can move to the next level, once you have satisfactorily completed your level. Ive also listed the best course available on the topic. For optimum benefit, Id suggest you to take these courses one by one. If you still find them hard, discuss with me, I may have an alternative for you. For your convenience, Ive also shared a checklist below which can be downloaded.Whos a Beginner?  If you are completely new to analytics and data science. You have no idea how this industry operates. And yet, curious to pursue your career in this field, you are a beginner.These should be your resolutions:Ive seen students trying hands on both R and Python. Eventually, they end up with nothing. This is a deadly approach. You must promise yourself to learn R or Python in depth. Both are open source tools hence widely used in companies too. Python is widely recognized as the easiest programming language. R still remains the favorite statistical tool. Choice is upto yours. Both are equally good.Coursesto Do: Complete Python from Codecademy. Complete R from DataCamp.Statistics is all about assumptions and progressions. But, you cant progress in this industry without statistics and mathematics. It lies in the heart of a data scientist. If you are weak at mathematics, its time change that equation. Get comfortable with powerful statistical techniques, algebra and probability, The are many awesome courses available on statistics by Khan Academy, Udacity etc. You can get started right nowif you installthese apps.Courses to Do: Complete Inferential and Descriptive Statistics by Udacity. Complete Algebra by Khan Academy.Massive Online Open Courses a.k.a MOOCs are free to access and study. But, this one is the most difficult promise you can make to yourself. Student often tend to enroll and study multiple courses at a time and complete none. Hence, you must focus on one course and finish it before proceeding to the next. You can check coursera, edX, Udacity to undertake any course.Courses to Do: Complete Data Science Specialization (for R) from Coursera. Complete Python for Data Science from Dataquest.You need to know whats happening in the industry. We live in a dynamic world. Things change overnight. May be a technology prevalent today might become obsolete tomorrow. You must talk with experienced professionals, industry experts and meet your future self. So start participating in discussions, meet-ups, follow blogs, join groups and read books. To check all these, you can followus onFacebook for latest updates on this part.Whos an intermediate level of data scientist ?  If you have finished the previous level, and youve experimented with basics of machine learning, you have gained knowledge to build predictive models, then you possess an intermediate level. Completing this level need huge determination and hours of practice. Are you ready for this challenge ?Machine Learning is the future of data science and technology. All the major companies have heavily invested in hiring candidates with this skill. No doubt, its in huge demand these days. And, this is a chance for you to get the best out of this situation. This year, you should dig deeper in machine learning. Master Regression, Clustering, CART in depth. Here youll find free resources on machine learning.Courses to Do: Complete Machine Learning by Andrew Ng.Once you feel confident about machine learning, get to the next models. Using boosting and ensemble, you can could achieve model accuracy much higher than other algorithms. This topic would be covered in the free resources shared above. But, promise yourself to conquer this topic with great understanding.Courses to Do:Read Kaggle Ensembling Guide. Complete Boosting with MIT Lecture.This year, you can start your journey in big data. Considering the fact that demand of big data professionals is surging, you must learn Spark. It has recently gained popularity. The future of big data lies in Spark. It is widely used tool to handle and manipulate big data. Along with spark, you can extend your expertise to NoSQL , Hadoop as well.Courses to Do: Take your first step withSpark.What could be better than sharing knowledge! This year, you should start sharing your knowledge with people who are struggling to learn data science. You can join active data science forums, answer their doubts and educate them with useful tips and hacks. You could also lead meet-ups happening in nearest circles.Things to Do: Follow us on Facebook.Time to test your knowledge. This year, you must participate in competitions. It would introduce you to your weak and strong areas. Moreover, youll become confident of the knowledge youve acquired. Id want you to rank in Top 500 data scientist on Kaggle. For now, you should aim to become the Last Man Standing.Things to Do: Participate on Kaggle. Participate on Data Hack.Addition: Competitions can be bit difficult at times. You can also check out these practice problems to check your skills and knowledge. They arent difficult but surely FUN!I dont need to define the people falling in this category. These people know of data science what most people are afraid to even try! Theyve reached a level where life is cozy and easy going. But still, they love challenges. They are experienced professionals. Here are some resolutions:This year, you have to set an example for the people aspiring to become a data scientist. You must promise yourself to try build model on deep learning this year. People around the world and already using it for making predictions. Its an advanced level of machine learning. The accuracy is obviously better than normal machine learning models.Courses to Do: Complete this Deep Learning Tutorial.I believe knowledge is meant to be shared not stored. The more you share, the more youll learn. Its being said, if you learn a new concept, explain it to 2 friends of yours. You are more likely to remember that concept for long.This year you must take a resolution of helping people in analytics community with your knowledge and experience. This will allow many struggling people to find a shore in this domain.Things to Do:Share your knowledge on Discuss.Reinforcement Learning is the most powerful yet less-discovered aspect of machine learning. This year, promise yourself to research in this field. It will surely be challenging but worth trying. Self driving cars, spy dronesare results of reinforcement learning. Once you start with this, youll automatically get into artificial intelligence.Coursesto Do: Complete tutorial by Andrew Moore.This year, you must promise yourself to uphold master status on Kaggle. Precisely, secure a rank in top 50 data scientist on Kaggle. Participate in competitions which suits best to your knowledge. Team up with other kagglers. At this level of competition, youll end up learning concepts which you wouldnt have learnt otherwise.To Do:Participate on Kaggle.Track your Progress. New Year Resolutions 2016 Checklist: DownloadI understand, these resolutions can be challenging for you, but still worth trying. You are free to take up a resolution according to your current situation. Ive simply enlisted the most important ones which an aspiring data scientist must take up.Last week, I realized that people arent confident enough in deciding a new year resolution. This was a concern for me. Hence, this lead me write this article. I hope, before 2016 ends, you would finish beginner level (assuming you are a fresher).This article would have cleared your confusions on making new year resolutions. As an aspiring data scientist, Ive already put a lot of things on your plate to eat. Chew one by one and proceed. If you find difficulty in successful completion of your resolutions, feel free to share your thoughts with me in the comments section below.",https://www.analyticsvidhya.com/blog/2015/12/special-year-resolutions-data-scientist/
8 Proven Ways for improving the Accuracy of a Machine Learning Model,Learn everything about Analytics|Introduction|8 Methods to Boostthe Accuracy of a Model|Caution!|End Notes,"1. Add more data|2. Treat missing and Outlier values|3. Feature Engineering|4. Feature Selection|5. Multiple algorithms|6. Algorithm Tuning|7. Ensemble methods|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|New Year Resolutions for a Data Scientist|Year in Review: Best of Analytics Vidhyafrom 2015|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Enhancing amodel performance can be challenging at times. Im sure, a lot of you would agree with me if youve found yourself stuck in a similar situation. You try all the strategies and algorithms that youve learned. Yet, you fail at improving the accuracy of your model. You feel helpless and stuck. And, this is where 90% of the data scientists give up.But, this is where the real story begins! This is what differentiates an average data scientist from a master data scientist. Do you also dream of becoming a master data scientist ?If yes, you need these 8 proven ways to re-structure your model approach. A predictive model can be built in many ways. There is no must-follow rule. But, if you follow my ways (shared below), youd surely achievehigh accuracy in your models (given that the data provided is sufficient to make predictions).Ive learnt these methods with experience. Ive always preferred to learn practically than digging theories. And, my approach has always encouraged me. In this article, Ive shared the 8 proven ways using which you can create a robust machine learning model. I hope my knowledge can help people in achieving great heights in their careers.The model development cycle goes through various stages, starting from data collection to model building.But, before exploring the data to understand relationships (in variables), Its always recommended to perform hypothesis generation. (To know more about hypothesis generation, refer tothis link). I believe this is the most under  rated step of predictive modeling.It is important that you spend time thinking on the givenproblem and gaining the domain knowledge. So, how does it help?This practice usually helps inbuilding better features later on, which are not biased by the data available in the data-set. This is a crucial stepwhich usuallyimproves amodels accuracy.At this stage, you are expected to applystructured thinking to the problem i.e. a thinking process which takes into consideration all the possible aspects of a particular problem.Lets dig deeper now.Now wellcheck out the proven way to improve the accuracy of a model:Having more data is always a good idea. It allows the data to tell for itself, instead of relying on assumptions and weak correlations. Presence of more data results inbetter and accurate models.I understand, we dont get anoption to add more data. For example: we do not get a choiceto increase the size of training data in data science competitions. But while working on a company project, I suggest you to ask for more data, if possible. This will reduce your pain of working on limited data sets.The unwanted presence of missing and outlier valuesin the training data often reduces the accuracyof a model or leads toa biased model. It leads to inaccuratepredictions. This is becausewe dontanalyse the behavior and relationship with other variables correctly. So, it is important to treat missing and outlier values well.Look at the below snapshot carefully. It shows that, in presence ofmissing values, the chances of playing cricket by females is similar asmales. But, if you look at the second table (after treatment of missing values based on salutation of name, Miss ), we can see that females have higher chances of playing cricket compared to males.Above, we saw the adverse effect of missing values on theaccuracy of a model. Gladly, we have various methods to deal with missing and outlier values:This step helps to extract more information from existing data.New information is extracted in terms of new features.These featuresmay have a higher ability to explain the variance in the training data. Thus, giving improved model accuracy.Feature engineering is highly influenced by hypotheses generation. Goodhypothesisresult in good features.Thats why,I always suggest to invest quality time in hypothesis generation. Feature engineering process can be divided into two steps:Feature Selectionis a process offinding out the best subset of attributes which better explains the relationship of independent variables with target variable.You can select the useful featuresbased on various metrics like:Hitting atthe right machine learning algorithm is the ideal approach to achieve higher accuracy. But, it is easier said than done.This intuition comes with experience and incessant practice. Some algorithms are better suited to a particular type ofdata sets than others. Hence, we should apply all relevant models and check the performance.Source: Scikit-Learn cheat sheet Weknow that machine learning algorithms are driven by parameters.These parametersmajorly influence the outcome of learning process.The objective of parametertuning is to find the optimum value for each parameter to improve the accuracy of the model. To tune these parameters, you musthave a good understanding of these meaning and their individual impact on model.You can repeat this process with a number of well performing models.For example: In random forest, we have various parameters like max_features, number_trees, random_state, oob_score and others. Intuitive optimization of these parameter values will result in better and more accurate models.You can refer article Tuning the parameters of your Random Forest model to learn theimpact of parameter tuning in detail. Below is random forest scikit learn algorithm with list of all parameters:-

This is the most common approach found majorly inwinning solutions of Data science competitions. This techniquesimply combines the result of multiple weak models and producebetter results.This can be achieved through many ways:To know more about these methods, you can refer article Introduction to ensemble learning.It is always a better idea to applyensemble methods to improve the accuracy of your model. There are two good reasons for this: a ) They are generally more complex than traditional methods. b) The traditional methods give you a good base level from which you can improve and draw from to create your ensembles.Till here, we have seen methods which canimprove the accuracy of a model.But, it is not necessary that higher accuracy models always perform better(for unseen data points).Sometimes, the improvement in models accuracy can be due to over-fitting too.8. Cross Validation: To find the right answer of this question, we must use cross validation technique.Cross Validation is one of the most important concepts in data modeling. It says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.This method helps us to achievemore generalized relationships. To know more about this cross validation method, you should refer article Improve model performance using cross validation.The process of predictive modeling is tiresome. But, if you can think smart, you can outrun your fellow competition easily. Simply, think of these 8 steps. Once you get the data set, follow these proven ways and youll surely get a robust machine learning model.But, these 8 steps can only help you, after youve mastered these steps individually. For example, you must know of multiple machine learning algorithms such that you can build an ensemble.In this article, Ive shared 8 proven ways which can improve the accuracy of a predictive model. These methods are widely known but not used in sequence as defined above.Did you find this tutorial useful? If you need any more help with machine learning models, please feel free to ask your questions in the comments below.",https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/
Year in Review: Best of Analytics Vidhyafrom 2015,Learn everything about Analytics|Introduction|10BestArticles of 2015|5 Best Infographics of 2015|5 Best Discussions of 2015|5 Tips from Winning Data Scientists|End Notes,"1. Essentials of Machine Learning Algorithms|2. Top Data Scientists to Follow & Best Tutorials on Github|3. 7 Types of Regression Techniques you should know!|4. Ultimate Guide to Data Exploration in Python using NumPy, matplotlib, pandas|5. Difference between Machine Learning and Statistical Modeling|6.Top Youtube Videos on Machine Learning, Neural Network and Deep Learning|7.Powerful Guide to Learn Random Forest in R and Python|8.Comprehensive Guide to Data Visualization in R|9.Python Tutorials from PyCon Montreal 2015|10.9 Popular Ways to Perform Data Visualization in Python|1. Cheatsheet  Machine Learning Algorithms|2. Job Comparison  Data Scientist vs Data Engineer vs Statistician|3. 10 Must Watch Movies on Data Science and Machine Learning|4. List of useful packages for Data Analysis in R|5. Must Read Books in Analytics & Data Science|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|8 Proven Ways for improving the Accuracy of a Machine Learning Model|Kaggle Solution: Whats Cooking ? (Text Mining Competition)|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People say that 90% of startups fail by the time they reach their year 2! I would like to thank you all that we have not only made it to the remaining 10% of the startups, but have come out with flying colors!I still remember last day at my job and my friends at work were curious How big a market could data scientists and business analysts be? As a first time entrepreneur with a 6m old daughter, I felt scared that I did not know the answer. I was leaving my cushy job to try out something no one had tried before. All I knew was the glaring knowledge gap I wanted to address and how passionately I felt about it.Thankfully, it all worked out. Today, we are one of the the largest and fastest growing data science community in the world. Our traffic has become 5x of what is was when 2015 started and it is still growing at a healthy pace. We started the year with launch of our discussion portals, added different forms of content likeinfographics/ cheat sheets, salary testandresource finder. In second half of the year, we also launched our hackathon platform.Through 2015, our community gotbigger and bigger. We felt a huge shift on work load. But, this was pleasing. We wrote our heart out to provide the best possible knowledge in the subject matter. And we hope you enjoyed it. Here are some of the best snippets of content created by our communityin 2015. Read them, give the knowledge a test and stay warm as the year comes to an end.We promise to make 2016 even more exciting and knowledge rich for you.Note: If you have anything to share(suggestion, opinion, good moments, badmoments), oranything, you canwrite tous at [emailprotected]Many of us tend to get confused in choosing aright algorithm. Its quite common actually. People fail to decide if logistic regression or decision tree would give better result. In stuck insuchsituations, this article would come to rescue you. Here youll find the complete explanation of 10 Machine Learning algorithms in Python & R. If you are a complete beginner, this should help you to get started with machine learning today.Github is not just for web programmers, as perceived by most of us. It is a open source repository data science folks as well. While working on this article, I was astonished to find the depth of free resources available here. I ended up with creating a list of top data scientist in the world. Being available on github, you can check their code repository and the projects theyve worked on. This is both inspiring and exciting to connect with such people.Those who aint curious,end up learning multiple regression and logistic regression. Thats it. In total, there are 7 types of regression techniques which can be used in various situations. Do you know just 5 of them? Im certain, many of you wont. Dont panic now.Here is a complete guide on 7 types of regression techniques used in predictive modeling.Exploring data sets and developing deep understanding about the data is one of the most important skill every data scientist should possess. People estimate that time spent on these activities can go as high as 80% of the project time in some cases. If you use Python, herea a complete beginners guide for data exploration in Python with codes. It uses python libraries such as NumPy, Matplotlib, Seaborn, Pandas.Since both involve predictive modeling, people wanted to know the difference between Machine Learning and Predictive Modeling. A clear line of demarcation has been made in machine learning and statistical modeling with 7 points.The difference between these two have gone down significantly over past decade. Both the branches have learned from each other a lot and will further come closer in future.Once I was finding free tutorials to learn machine learning. And, every time I searched for my query, google suggested me to watch Youtube. I was oblivious to this side of Youtube. I explored it and found a huge reserve of tutorials on data science. I created a playlist and shared this on internet. Im glad that people found it helpful. Heres a complete list of must watch Youtube videos on machine learning, deep learning and neural networks.This is a complete tutorial on learning Random Forest algorithm. It is widely used in all situations. Though, the accuracy of results may vary. Its a must have algorithm in your machine learning armory. Random Forests are incredibly powerful and can be implemented quickly. These days, random forest has become a clich method of checking variable importance.Here is a complete guide to creating basic to advanced level visualization in R Programming. R Programming offers a satisfactory set of inbuilt function and libraries (such as ggplot2, leaflet, lattice) to build visualizations and present data. These are convenient and allows you to create visualizations in no time.PyCon conferences are held every year around the world. They have helped millions of beginners and newbies to embrace python and become expert in it. Their hour long workshops and tutorials are enriched with practical experience. Heres a collection of best tutorials which you must watch, if you love python.Data scientists are no less than artists. They make paintings in form of digital visualization (of data) with a motive of manifesting the hidden patterns / insights in it. Python uses 2 libraries i.e. Matplotlib and Seaborn. Here is a demonstration of various charts using these libraries in Python.Now learn, machine learning algorithms even faster. Heres a cheatsheet which manifests codes in Python and R for machine learning algorithms. Here you wouldnt find conceptual explanation of the algorithms, but their practical use and application.There remains a deep confusion in recently grown job profiles. With machine learning, the work of statistician or data analyst may become outdated. But, the truth is different. These profiles do have a difference in their nature of work and responsibilities held. Here is an infographic which explains the role, responsibilities of these top jobs in analytics industry.When you watch movies, you would have realized that you dont forget its story, case, music all of a sudden. It stays in your mind. How about learning some data science this way? I chose these movies based on their relevancy, ratings and audience love for them. My personal favorite is Her movie. The operating system is intelligent and adorable. Here are the 10 must watch movies.R has a repository of more than 5ooo packages. If anyone to were to use all, it would be hard to remember. Instead, the packages have been categorized into role and nature of work. For example: you cant user dplyr for visualizing data. Hence, it is important to learn which packages is best suited in which situation. Here is an infographic on types of useful packages available in RReading books is the best way to gain wisdom and knowledge. Books provide concrete and truthful aspect of a subject matter. If you are an avid reader, here is a list of must read books for people keen to start their career in analytics. I made this document, considering the relevancy, rating of the books. These books will broaden your outlook and improve your ability to learn and improve faster.1. Comparison of PGDBA from IIT vs IIMC vs ISI vs Praxis Business School2. Which is the best statistics and analytics book ?3. What is the difference between Tableau and Qlikview ?4. How to create word cloud in python?5. Which course is better : Machine Learning by Andrew Ng or Learning from Data (on edX)?These winners emerged from our hackathons. They are now mentors for many young data scientists in our community. Below are the tips shared by these data scientists:1. As a beginner, you must commit yourself to learn feature engineering.2. Think out of the box. Learn to use H2o and GraphLab libraries in R or Python.3.You must sharpen your boosting and ensemble skills.4. No one can teach you parameter tuning. Youll learn it best by yourself. It is no rocket science. You justTry, Fail,     Rebound and Succeed.5. Dont get hopeless when your model accuracy doesnt improve. You should feel fortunate that you arestuck.         Because, this is where your real learning will begin.Source: ArticleWith this, we come to the end of this article. Once again, we would like to thank all our readers, users. Without you, it wouldnt have been possible to build a community which is growing everyday. Faster than ever. You should check out these resources. These are the best ones from 2015. Not only people loved them, but shared them on social media to the depth.Did you find this article useful ? Share your views and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/12/year-review-analytics-vidhya-from-2015/
Kaggle Solution: Whats Cooking ? (Text Mining Competition),Learn everything about Analytics|Introduction|Before you start|Whats Cooking ?|Solution|End Notes,"Step 1. Hypothesis Generation|Step 2. Download and Understand the Data Set|Step 3. Basics of Text Mining|Step 4. Importing and Combining Data Set|Step 5. Pre-Processing using tm package ( Text Mining)|Step 6. Data Exploration|Step 7. Model Building|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Year in Review: Best of Analytics Vidhyafrom 2015|A Complete Tutorial on SAS Macros For Faster Data Manipulation|
Analytics Vidhya Content Team
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tutorial on Text Mining, XGBoost and Ensemble Modelingin RI came across Whats Cookingcompetition on Kaggle last week. At first, I was intrigued by its name. I checked it and realized thatthis competition is about to finish. My bad! It wasa text mining competition. This competition went live for 103 days and ended on 20th December 2015.Still, I decided to test my skills. I downloaded the data set, built a model and managed to get a score of 0.79817 in the end. Even though, my submission wasnt accepted after the competition got over,but I could check my score. This got me in top 20 percentile.I used Text Mining, XGBoost and Ensemble Modeling to get this score. And, I used R. It took me less than 6 hours to achieve this milestone.I teamed up withRohit Hinduja,who is currently interning at Analytics Vidhya.To help beginners in R, here ismy solution in a tutorial format. In the article below, Ive adapted a step by step methodology to explain the solution. This tutorial requires prior knowledge of R and Machine Learning.I am confident that this tutorial can improve your R coding skills and approaches.Lets get started.Heres a quick approach to (for beginners) give a tough fight in any kaggle competition:Yeah! I could smell, it was a text mining competition. The data set had a list of id, ingredients and cuisine. There were 20 types of cuisine in the data set. The participants were asked to predict a cuisine based on available ingredients.The ingredients were available in the form of a text list. Thats where text mining wasused. Before reaching to the modeling stage, I cleaned the text usingpre-processing methods. And, finally with available set of variables, I used an ensemble of XGBoost Models.Note: My system configuration is core i5 processor, 8GB RAM and 1TB Hard Disk.Below is the my solution of this competition:Though many people dont believe in this, but this step dowonders whendone intuitively. Hypothesis Generationcan help you to think out of data. It also helps you understand the data and relationship between the variables. It should ideally be done after youve looked at problem statement (but not the data).Before exploring data, you must think smartlyon the problem statement. What could be the features which can influence your outcome variable? Think on these terms and write down your findings. I did the same. Below is my list of findings which I thought could help me in determining a cuisine:The data set shows a list of id, cuisine and ingredients. The data set is available in json format.The dependent variable is cuisine. The independent variable is ingredients. Train data set is used for creating model. Test data is used to checking the accuracy of the model. If you are still confused between the two, remember, test data set do not have dependent variable.Since the data is available in text format, I was determined to quickly build a corpus of ingredients (next step).Here is a snapshot of data set for your perusal in json format:For this solution, Ive used R (precisely R Studio0.99.484) in Windows environment.Text Mining / Natural Language Processing helps computers to understand text and derive useful information from it. Several brands use this technique to analyse customer sentiments on social media. Itconsists of pre-defined set of commands used to clean the data. Since, text mining is mainly used to verify sentiments, the incoming data canbe loosely structured, multilingual, textual or might have poor spellings.Some of the commonly used techniques in text mining are:Ive used these techniques in my solution too.Since the data set is in json format, I require different set of libraries to perform this step. jsonlite offers an easy way to import data in R. This is how Ive done:1. Import Train and Test Data Set2. Combine both train and test data set. Thiswill make our text cleaning process less painful. If I do not combine, Ill have to clean train and test data set separately. And, this would take alot of time.ButI need to add the dependent variable in test data set. Data can be combine using rbind (row-bind) function.As explained above, here are the steps used to clean the list of ingredients. Ive used tm package for text mining.1. Create a Corpus of Ingredients (Text)2. Convert text to lowercase3. Remove Punctuation4. Remove Stopwords5. Remove Whitespaces6. Perform Stemming6. After we are done with pre-processing, it is necessary to convert the text into plain text document. This helps in pre-processing documents as text documents.corpus <- tm_map(corpus, PlainTextDocument)7. For further processing, well create a document matrix where the text will categorized in columns1. Computing frequency column wise to get the ingredient with highest frequencyWe see that, there are may terms (ingredients) which occurs once, twice or thrice. Such ingredients wont add any value to the model. However, we need to be sure about removing these ingredients as it might cause loss in data. Hence, Ill remove only theterms having frequency less than 32. Lets visualize the data now. But first,well create a data frame.Here we see that salt, oil, pepper are among the highest occurring ingredients.You can change the freq values (in graph above) to visualize the frequency of ingredients.3. We can also find the level of correlation between two ingredients. For example, if you have any ingredient in mind which can be highly correlated with others, we can find it. Here I am checking the correlation of salt and oil with other variables. Ive assigned the correlation limit as 0.30. It means, Ill only get the value which have correlation higher than 0.30.4. We can also create a word cloud to check the most frequent terms. It is easy to build and gives an enhanced understanding of ingredients in this data. For this, Ive used the package wordcloud.5. Now Ill make final structural changes in the data.Here I find that, italian is the most popular inall the cuisine available. Using this information,Ive added thedependent variable cuisine in the data frame newsparse as italian.As my first attempt, I couldnt think of any algorithm better than naive bayes. Since I have a multi class categorical variable, I expected naive bayes to do wonders. But, to my surprise, the naive bayes model went inperpetuity. Perhaps, my machine specifications arent powerful enough.Next, I tried Boosting. Thankfully, the model computedwithout any trouble.Boosting is a techniquewhich convert weak learners into strong learners. In simple terms, I built three XGBoost model. All there were weak, means their accuracy werent good. I combined (ensemble) the predictionsof three model to produce a strong model. To know more about boosting, you can refer to this introduction.The reason I used boosting is because, it works great on sparse matrices. Since, Ive a sparse matrix here, I expected it to give good results.Sparse Matrix is a matrix which has large number of zeroes in it. Its opposite is dense matrix. In a dense matrix, we have very few zeroes. XGBoost, precisely, deliver exceptional resultson sparse matrices.I did parameter tuning on XGBoost model to ensure that every model behaves in a different way. To read more on XGBoost, heres a comprehensive documentation: XGBoostBelow is my complete code. Ive used the packages xgboost and matrix. The package matrix is used to create sparse matrix quickly.Now, Ive created a sparse matrix using xgb.DMatrix of train data set. Ive kept the set of independent variables and removed the dependent variable.Ive created a sparse matrix for test data set too. This is done to create a watchlist. Watchlist is a list of sparse form of train and test data set. It is served as an parameter in xgboost model to provide train and test error as the model runs.To understand the modeling part, I suggest you to read this document. Ive built 3 just models with different parameters . You can even create 40  50 models for ensembling. In the code below, IveusedObjective = multi:softmax. Because, this is a case of multi classification.Among other parameters, eta, min_child_weight, max.depth and gamma directly controls the model complexity. These parameters prevents the model to overfit.The model will be more conservative, if these values are chosen larger.Now I have three weak learners. You can check their accuracy using:The simple key is ensemble. Now, I have three data frame for model predict, predict2 and predict 3. Ive now extracted the cuisine column from predict and predict 2 into predict 3. With this step, I get all values of cuisines in one data frame. Now I can easily ensemble their predictionsIve used theMODE function to extract the predicted value with highest frequency per id.After following the step mentioned above, you can easily get the same score as mine (0.798). You would have seen, I havent used any brainy method to improve this model. i just applied my basics. Since Ive just started, I would like to see if I can push this further the highest level now.With this, I finish this tutorial for now! There are many things in this data set which you can try at your end. Due to time constraints, I couldnt spent much time on it during the competition. But, its time you put on your thinking boots. I failed at Naive Bayes. So, why dont you create an ensemble of naive bayes models? or may be, create a cluster of ingredients and build a model over it ?Im sure this strategy might give you a better score. Perhaps, more knowledge. In this tutorial, Ive built a predictive model on Whats Cooking ? data set hosted by Kaggle. I took a step wise approach to cover various stages of model building. I used text mining and ensemble of 3 XGBoost models. XGBoost in itself is a deep topic. I plan to cover it deeply in my forthcoming articles. Id suggest you to practice and learn.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/12/kaggle-solution-cooking-text-mining-competition/
A Complete Tutorial on SAS Macros For Faster Data Manipulation,Learn everything about Analytics|Introduction|Table of Contents|1. Introduction to SAS Macros|2. SAS Macros  Conditional and Iterative statement|3. SAS Macros  Functions|End Notes,"What are the benefits of usingSAS Macros ?|How does Macro Processor Work?|What are the components of Macro code?|What are Macro Variables ?|How to referencea Macro variable with text ?|SAS Macros|Adding parameter to SAS Macros|What is Conditional Processing?|What are Conditional Macro Expression ?|Example|Loops|Example:|%UPCASE()|%SUBSTR()|%SCAN()|%EVAL()|Question?|%SYSFUNC()|%STR()|%NSTR ()|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Kaggle Solution: Whats Cooking ? (Text Mining Competition)|Risk Analyst  Delhi (5+ years of experience)|
Sunil Ray
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve been writing the same lines of code repeatedly in SAS, you can stop now. It shouldnt be as laborious as youve been. Availability of SAS Macros can make your work faster and save your day. Ive worked on SAS for 3 years. I remember writing same lines of code every morning, until I learnt Macros. Since then, Ive persisted to learn every bit of technique to make coding faster in SAS.Macros provide an incredible way to automate a process. Over the years, Ive learnt these ways which Ive shared in this article. Macros are simple to learn but can be confusing at times. Hence, you must pay attention to the command and its output. Lots of people end of mixing the two.Here is a complete tutorial on using SAS Macros. I hope this will help you to become better and faster in SAS.The topics covered in this guide are as shown below.Initiate your SAS software and lets get started. Performing these commands as you read through would help you memorize these commands better.Illtake a simple example so that you canunderstand this concept better:Example:Lets look at the following SAS program:Above SAS code is written to extract policy level details for 09-Sep-14 and let us say that the user needs to run this code on a daily basis after changing the date (at both the places) to current date.Thankfully, SAS  like any other programming language, provides the facility of automating codes with use of SAS macro programming. Now, let us look at the same code once automated by SAS macros.In this case, the user does not need to provide value for current date. The code will automatically pick current date from system.SAS macros enable us to:-Above all, it helps to reduce the efforts required to read and write SAS Code. Macro programming is typically covered as advanced topic in SAS, but the basic concepts of SAS macros are easy to understand.I will now introduce the concept of SAS programming and I assume that you are aware about basics of SAS programming. We will look at how macro processor works, how to use SAS macros & macro variables and How to create flexible and reusable code to save time and efforts?A SAS program is a combination of Data steps, global statements, SAS Component Language (SCL), SQL statements and SAS Macro statements. Whenever we submit a program, it gets copied in memory (called input stack) followed by word scanner and there after it goes to compiler and gets executed. When we use Macros in our SAS program, an additional step is added to resolve macros. This step is called as MACRO Processor.As we can see above, we have an additional step Macro Processor when we work with macro i.e. SAS code with macro does not compile or execute faster. It only helps to reuse the SAS code and variables values.Macro code consists of two basic building blocks: Macros and Macro variables. In a SAS program they are referred differently as:-A macro variable is just like a standard variable, except that its value is not a data set and has only a single character value. The value of a macro variable could be a variable name, a numeral, or any text you want substituted in your program.Scope of Macro variable can be local or global,Add Mediadepending on how we have defined it. If it is defined inside a macro program, then scope is local (only available for that macro code). However, if we have defined it outside (in the main body), then we can use it anywhere in SAS program.%LET statement is used to create and assign value to macro variables.% LET <Macro Variable Name>=Value;Macro variable name follows the SAS naming convention and if variable already exists then value is overwritten.Value of macro variable in %Let statement can be any string and it has following characteristics:-Macro variables are referenced by using ampersand (&) followed by macro variable name.&<Macro variable Name>Example:-We can also declare multiple macro variables and use them at different places in SAS Code. Macro variable are not resolved when they are accessed within single quotes. Hence, we should use double quotes to reference them.We can reference macro variable with text in multiple ways:-Lets look at these with a simple example:-A period (.) is used as delimiter that defines the end of a macro variable.Lets look at an example:-Here, you can see that the program did not execute because we required (ORION.COUNTRY) and not (ORIONCOUNTRY). This happened because here period (.) is used as separator between two macro variables. Now, to use period as a separator between library name and dataset, we need to provide period (.) twice.SAS Macros are useful when we want to execute same set of SAS statements again and again. This is an ideal case for use of macro rather than typing / copy pasting same statements.Macro is a group of SAS statements that is identified by a name and to use it in program anywhere, we can reference it with that name.Syntax:-%Macro <Macro Name>;Macro Statements;%MEND;Macro Statements can contain combination of the following:-After definition of macro, we need to invoke/ call it when want to use it.Syntax for calling a MACRO: %<Macro Name> [;]Here we do not need to end this statement with a semicolon but it is a good programming practice to have it.Example:-You can see that we have used same set of statements twice using macro.We can define a Macro with parameters, which can be referenced within the macro. We can pass the parameters in two ways:-Positional Parameters: In this method, we supply parameter name at time of defining the macro and values are passed at time of Macro call.Syntax:-DefinitionCallingAt calling stage, values to parameters are passed in similar order as they are defined in the macro definition.Example:-Keyword Parameters:In this method, we provide parameter name with equals to sign and also can assign default value to the parameters. While calling the macro, we have to mention the parameter names followed by equals sign and value.In this method, parameters with equals sign can beSyntax:-DefinitionCallingExample:-Lets discuss some examples of SAS Macro Variable and SAS Macros, when you want to use a particular value multiple times in a program and its value changes over time.For example:-SAS Macros are typically considered as part of advance SAS Programming and are used widely in reporting, data manipulation and automation of SAS programs. They do not help to reduce the time of execution, but instead, they reduce repetition of similar steps in your program and enhance the readability of Programs.Lets now proceed further can learn the process ofcreating conditional and repetitive steps using SAS Macro.SAS macros provide us with the flexibility to use a piece of code multiple times by simply calling the macro. This flexibility can be exploited to reach next level of sophistication with use of conditional statements and loops using macro statements such as %IF, %DO. These macro statements should always be called inside the macro.As the name implies, conditional processing is used when we want to execute a piece of code based on the output of single or multiple conditions.Syntax:-Here, ELSE part of the syntax is optional.Both syntax work in similar manner  the only difference is, first one executes only one statement after THEN or ELSE, whereas the second syntax can execute multiple statements.I am referring to the expression in the condition as Conditional macro expression. In some cases, it is similar to SAS expression:Dissimilarities compared to SAS Expression:Lets say, we have transactional data for a company which deals in food business. Here manager requires details of all the sales on daily basis except on MONDAY. On Monday, he want a additional summary report for item wise total sales (People call it as MONDAY Report!)Here we will first define a separate macro for Daily and Monday reports (you can combine these together also).Now on daily basis it will call Macro Daily and check if today is MONDAY then it will run MACRO Monday. Similarly if we have multiple statements to execute after %IF-%Then then we should go for %IF-%THEN-%DO-%END.Loops are used to create a dynamic program, which executes for a number of iterations, based on some conditions (called conditional iteration). These statements are also valid only inside a macro.Syntax:Key things about syntax:Lets say we have a series of SAS data sets YR1990  YR2013 that contain business detail and now we want to calculate the average sales for each of these years.If we will not use the SAS macro then we need to write PROC MEANS steps for each of the year, which is not a good idea.Now lets look at how we can use SAS Macros to do this job easily:We can achieve this with %UNTIL and %WHILE.Conditional statements and loops empower SAS Macros to perform conditional and iterative tasks. They appear similar to SAS conditional statements and loops and they work similar in some cases. The major difference between the two is %if-%then can ONLY be used inside the MACRO program (whether inside the DATA step, or outside) and if-then is ONLY used inside the DATA step (whether MACRO or non-MACRO).Now, lets understand the use of Macro functions to manipulate text, perform arithmetic operations and execute SAS functions.These Macro functions have similar syntax, compared to their counterpart functions in data steps and they also return results in similar manner. They can manipulate macro variables and expressions and these functions get evaluated only at stage of Macro processing. Most importantly, these do not require quotes around character constant arguments.Lets look at some of these one by one:This function is used to convert case of letters to uppercase:Syntax:  %UPCASE (Argument)Example:It will print title as THIS IS DETAIL OF COUNTRY.This function returns the given number of characters from specified position.Syntax:  %SUBSTR (argument, position [, number of characters])If number of character is not supplied, %SUBSTR function will return characters from given position till end of the string.Example:Here we have first extracted country from macro variable abc and store it into def and after that used it to show title in upper case.This function will return the nth word in a string (having n words separated by delimiters).Syntax:  %SCAN (argument, n [, delimiter])It will return NULL value, if string does not have n words separated by delimiter and if we have not given delimiter then it will use default value for it. Default delimiters are blank , . ( & ! $ * ) ;  / %.Example: ABC will store string Vidhya, here it automatically identify the second word based on default delimiter and BCD will have value Analyt because we have mentioned that i is a delimiter and we required the first word.This function is used to perform mathematical and logical operation with macro variables. Remember, macro variable contains only text (numerical values are also stored as text) and therefore we cant perform any arithmetical and logical operation, if we try to use them directly. EVAL() function also returns a text result.Syntax:  %EVAL (Expression)Example: As we know, macro variables store value as text, macro variable B will store 3+1 and with the use of %EVAL, C will store 4.If, we create a variable like:What would variable D store?If you answered 6.2, you are wrong! This statement will throw up error because SAS understands . as character. Now, to evaluate floating numbers, we will use %SYSEVALF () function and above statement should be written as:This function is used to execute SAS functions in macro environment. With the help of this function, we can increase the list of functions available to the macro language significantly. This function can allow us to make almost all DATA step and user-written functions usable in SAS Macros.Syntax: %SYSFUNC (Function(arguments..) [, Format])It has two arguments, first is for the function and second is optional to format the output of function.Example: This will display the current date in date9 format.This function removes/masks the normal meaning of following token +  * /, > < = ;  LT EQ GT LE GE LE NE AND OR NOT blank. It also preserves leading and trailing blanks of the string. Please note that this does not mask & and %.Syntax:  %STR (argument)This functionsworks exactly as %STR, but it also masks macro triggers % and &.Example: Here first %PUT statement will return S 5 Analytics whereas second one will return S &D Analytics.This brings me to the end of this tutorial. I hope you learnt some new ways of manipulating data in SAS. In this tutorial, we looked at the basic concepts of SAS and how they become useful to accomplish repetitive tasks easily.We also looked at how SAS Macros can be used in iterative and conditional circumstances, followed by several functions to perform text manipulations and to apply arithmetical and logical operations in SAS Macros. These, in combination with conditional statements can provide a very powerful mechanism to automate / modularize your SAS programmes.Did you find this tutorial useful? If you need any more help with SAS Macros, please feel free to ask your questions in the comments below.",https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-sas-macros-faster-data-manipulation/
"Risk Analyst  Delhi (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|A Complete Tutorial on SAS Macros For Faster Data Manipulation|SQL commands for Commonly Used Excel Operations|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Risk AnalystLocation  DelhiAbout employer  ConfidentialDescriptionThe ideal Candidate will be a Credit Risk Statistician / Modeler to support the analytical, reporting and data needs of a young, exciting and well-funded start up.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Risk Analyst  Delhi and the following details:",https://www.analyticsvidhya.com/blog/2015/12/risk-analyst-delhi-3-years-experience/
SQL commands for Commonly Used Excel Operations,Learn everything about Analytics|Introduction|Why cant you use Excel for serious data science work?|List ofCommonExcel Operations|1. View Data|2. Sort Data|3. Filter Data|4. Delete Records|5. Add records|6. UpdateData in Existing Observations|7. Show unique values|8.Write an expression to generate new column|9. LookUp data from another table|10. Pivot Table|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Risk Analyst  Delhi (5+ years of experience)|Jr. Data Scientist  Mumbai (0-2 years of experience)|
Sunil Ray
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Learning SQL after Excel couldnt be simpler!Ive spent more than a decade working on Excel. Yet, there is so much to learn. If you dislike coding, excel could be your rescue into data science world (to some extent). Once you understand Excel operations, learning SQL is very easy.Now at this stage, you might ask, why cant I use excel for all my work. There are several reasons for it:Moving to SQL would address point 1 and point 2 to some extent.Moreover, SQL is one of the most sought out skills in a data scientist.If you dont know SQL yet and have worked in Excel,you can get started right now. Ive designed this tutorial with keeping in mind, the most commonly used excel operations. Your previous experience blended with this tutorial can quickly make you a SQL expert. (Note: If you find any trouble, please write to me in comments section below.Related : Basics of SQL and RDBMS for BeginnersHere is the list of commonly usedexcel operation. In this tutorial, Ive performed all these operations in SQL:To perform operations listed above, Ill use the data listed below (Employee)\:
In excel, we can view all the records directly. But, SQLrequires acommand to process this request. This can be done by usingSELECTcommand.Syntax:SELECT column_1,column_2,column_n | * FROM table_name;Exercise:A. View all data of Employee tableB. View only ECODE and Gender data of Employee tableSelect ECODE, Gender from EmployeeOrganizing information becomes important when you have more data. It helps to generate quick inferences. You can quickly organize an excel worksheet by sorting your data in ascending or descending order.Syntax:SELECT column_1,column_2,column_n | * FROM table_name order by column_1 [desc], column_2 [desc];Exercise:
A.Arrange records of Employee table in Descending order of Total_Payout.B.Arrange records of Employee table by City (ascending) and Total_Payout(descending).In addition to sorting, we often apply filter toanalyze data in a better way. When data is filtered, only rows that meet the filter criteria is displayedwhileother rows get hidden. Also, we can apply multiple criterion to filter data.Syntax:SELECT column_1,column_2,column_n | * FROM table_name where column_1 operator value;Below are the common list of operators, we can use to form a condition.Exercise:
A.Filter observations associated with city DelhiB.Filter observations of Department Admin and Total_Payout >= 500Deleting records or columns is a commonly used operation in Excel. In excel, we simply press Delete key on keyboard to delete a record. Similarly, SQL has command DELETE to removerecords from a table.Syntax:DELETE FROM table_nameWHERE some_column=some_value;Exercise:
A. Deleteobservations which have Total_Payout >=600It removes two record only since these two observations satisfythe condition stated above. But be careful! if we do not provide any condition, it will remove all records from atable.B. Deleteobservations which have Total_Payout >=600 and Department =AdminAbove command will remove only one record which satisfies the condition.We have seen methods to remove records, we can also add records to SQL table as we do in excel. INSERTcommand helps to perform this operation.Syntax:INSERT INTO table_nameVALUES (value1,value2,value3,); -> Insert values to all columnsOR,
INSERT INTO table_name (column1,column2,column3,)VALUES (value1,value2,value3,); -> Insert values to selected columns
Exercise: A. Add below records to the table Employee
B.Insert values to ECODE (A016) and Department (HR) only.Suppose, we want to updatethe name of HR department to Manpower for all employees. For such cases,SQL has a commandUPDATE which performs this function.Syntax:UPDATE table_nameSET column1=value1,column2=value2,WHERE some_column=some_value;Exercise: Rename Department HR to ManpowerSelect * from Employee; 
We can show unique values of variable(s) by applying DISTINCTkeyword prior tovariable name.Syntax:SELECT DISTINCT column_name,column_nameFROM table_name;Exercise: Show unique values of CityIn excel, we can create a column, based on existing column using functions or operators. This can be done in SQL using the commands below.Exercise: A.Create a new column Incentive which is 10% of Total_PayoutSelect *, Total_Payout*01 as Incentive from Employee; 
B.Create a new column City_Code which has first three characters of City.For more details onSQL functions, I would recommend you to refer this link.The most used function of excel by any BI professional / data analyst is VLOOKUP(). It helps to map data from other table to parent table. In other words,we can say that it is the excel way of joining 2 data sets through a common key.In SQL, we have similar functionality known as JOIN.SQL JOIN is used to combine rows from two or more tables, based on a common field between them. It hasmultiple types:Syntax:Exercise:Below is city category table City_Cat, now I want to map city category to Employee table and show all records of Employee table.Here, Iwant to show all records of table Employee. So,we will useleft join.To know more about JOIN operations, I would recommend you to refer this link.Pivot Table is an advanced way of analyzing data in excel. Not only it is useful, but it allows you to extract the hidden insights from data.Moreover, ithelps us to generate inference by summarizing data and allow usto manipulate it in different ways. This operation can be done in SQL by using aggregate functions and GROUP BY command.Syntax:SELECT column, aggregate_function(column)FROM tableWHERE column operator valueGROUP BY column;Exercise:A. Show sum of Total_Payout by GenderB. Show sum of Total_Payout and Count of Records by Gender and CityDid you find the article useful? Do let us know your thoughts about this transition guide in the comments section below",https://www.analyticsvidhya.com/blog/2015/12/sql-commands-common-excel-operations/
"Jr. Data Scientist  Mumbai (0-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|SQL commands for Commonly Used Excel Operations|Top Business Analytics Programs in India (2015  16)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Junior Data Scientist Location  MumbaiAbout employer ConfidentialJob description: Responsible for the development of high performance, distributed computing tasks using Big Data technologies such as Hadoop, NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Designs and drives the creation of new standards and best practices in the use of statistical data modelling, big data and optimization tools.Qualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asJunior Data Scientist  Mumbai and the following details:",https://www.analyticsvidhya.com/blog/2015/12/jr-data-scientist-mumbai-0-2-years-experience/
Top Business Analytics Programs in India (2015  16),Learn everything about Analytics|Introduction|2015  The year of new programs|Which programs are covered in these rankings?|Rankings (Executives / Experienced Professionals)|Rankings (For Less Experienced)|What is the criteria used for ranking these programs?|End Notes,"1. Indian School of Business (ISB) , Hyderabad|2. Great Lakes Institute of Management, Gurgaon / Chennai / Pune / Bangalore|3. Indian Institute of Management (IIM), Bangalore|4. MISB Bocconi, Mumbai|5. SP Jain School of Global Management, Mumbai|1. Indian Institute of Technology (IIT), Kharagpur|2. Praxis Business School, Kolkata|3. SP Jain School of Global Management, Mumbai|4. Narsee Monjee School of Management (NMIMS), Bangalore|5. Aegis School of Data Science, Mumbai|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Jr. Data Scientist  Mumbai (0-2 years of experience)|A Complete Tutorial on Time Series Modeling in R|
Kunal Jain
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"December stands out for us for multiple reasons  we are planning for 2016 & reflecting back on the fabulous year 2015 has been. It is also the time of the year, when we release our rankings for analytics programs in India. We published our rankingson Top Analytics Programs in India in 2014-15. Its time to revise our ranking according to the progress & growth of analytics programs in India in 2015. But before that, let us look at the changes, which happened in the landscape of Analytics Programs in India.Since we did our last round of rankings, there are some notable changes in landscape of analytics programs.Some of the best institutes in the country have come up with their analytics programs  most notable being the nexus between IIT Kharagpur, ISI Kolkata and IIM Calcutta, S.P. Jain Institue of Management & MISB Bocconi.Institutes have tried creating different programs in different formats (ranging from 6 month full time course to 2 year full time courses) focusing on needs of different audience.On the other hand, we have seen industry become more open to candidates from these programs as well. We have seen transitions happening through these programs and the placements across the programs (either officially supported or not) have gone up.So, here are the rankings for long duration courses on the parameters we chose (explained below). We hope that these rankings will help our audience make better career decisions.In these rankings, we have covered business analytics programs withduration more than 1 year. We havent covered the short term courses & training certifications here. We shall release those rankings soon.Further, we have ranked these programs in two categories: Programs for Working Professional (with experience)& Program for Freshers (less experience).Please see that these rankings are based on our interactions with various stakeholders from these institutes. We have done a comprehensive study and included all the institutes in India offering long term programs as part of our study.Also, the purpose behind releasing these rankings is to help our audience with their queries. This is by no means a judgement on the courses run by these institutes. Each program has its own set of pros and cons and you should look at the fit before taking an important career decision.ISB retains the No. 1 Ranking this year as well (from the last 2 years).This program accepts candidates with 4-6 years of work experience. Interestingly, freshers are also welcome, if someone turns out to be extraordinary (analytical thinking, business knowledge).  This course comprises of alternate month classroom session. In other half, the classes are held online. There is ample industry interaction and a 3  6 month project at the end of the course.The only downside of this program is expensein a part time program through the year (especially so, if you are a Non-Hyderabad resident). For an additional payment, you can also get a SAS certification from this program.Fees: INR 600,000+ taxesOn the surface, it might look like Great Lakes retained its position at No. 2. But, the actual story is much more than that. Great Lakes is fast catching up on the gap between them and ISB. They havelaunched two new campus at Bangalore and Pune.With this expansion, they are now available in 4 cities in India including Delhi NCR (Gurgaon) and Chennai. This program is well supported by industry leaders and leading companies. The enrollments in this program happens through a written test followed by personal interview.It would be interesting to see how this program from Great Lakes will fair in 2016.Fees: INR 355,000 (Gurgaon, Pune, Bangalore) / INR 425,000 ( Chennai ) + taxesAs compared to last year, IIM Bangalore has secured its position in our list. With a renowned brand name and widecoverage of analytics tools & techniques, this is one of the favorite courses in business analytics in India. This is a 1 year program. It course curriculumcoversEMiner, SPSS, R, Qlikview, SAS etc. Needless to say, students passed out of this program have managed to do well.This program is yet to introduce Big Data related technologies in their curriculum.Fees: INR 425,000 + taxesThis program is jointly delivered by SDA Boconni and Jigsaw Academy. Their faculty team is supported by international faculties from Bocconi. This program requires minimum 2 years of work experience. Itis a blend of online and offline learning which help working professionals in routine adjustments.Their endless effortless on ensuring placements and industry exposure, have helped many students securing a successful career.We think that the program is supported by high quality faculty and it is only a matter of time before brand Bocconi becomes a more familiar name among Indian corporate.Fees: INR 390,000 + taxesThis is a 1 year executive program. Classes are held on weekends. The program is designed such that students receive in depth knowledge of data science & analytics related concepts. This institute has a well established international presence. Students in the program are being taught by international faculties. Their enrollment process includes a written test and personal interview.The first batch of this program will roll out this year. It would be interesting to see the actual placement statistics and success stories.Fees: INR 620,000 + taxesThese programs are best suited for people looking for full time program in business analytics. The programs ranked below provide comprehensive knowledge in analytics and data science, with an exceptional focus on practical learning.This program is being offered jointly by top 3 institutions of India namely IIT Kharapur, IIM Calcutta and ISI Kolkata. You can expect abundance of knowledge sharing in this 2 year course. This is a full time program. If you manage to get into this course, consider your future secured (assuming youd work hard).This course in being taught in all these three campuses. You get to learn management from IIM, technology from IIT and data science from ISI. What better could you ask ? To enroll in this program, you written test and personal interview needs to be cleared.Fees: INR 1,600,000This is a 1 year Full Time program. This institute has delivered exemplary results in terms of placements & quality of knowledge delivered. With over 500 hours of classes, this institute is known for depth of analytics topics (tools & techniques) covered in the program. It covers tools such as SAS, R, Python, Big Data, Data Visualization and intensive industry projects. For people new to analytics, this could be a great place to start your career.It is indeed a value for money program. With their brand growth over time, this program could become the top choice for people determinedto builda successful career in analytics & data science.Fees: INR 425,000This is 6 months Full Time Program. It is suited for freshers and people with less than 5 years of experience. Over 400 hours of study, this program covers a wide range of subjects such as big data, machine learning, Python, R, SQL, data visualization and many other related expertise.Its first batch started in October 2015 of 30 students.Fees: INR 620,000 + taxesWith their exceptional placement figures, this program has ensured that students learning is in sync with industry skills demand. This is a Full Time Course. This course has lowest fee structure. The duration of this course is 1 year. With 390+ hours of class lectures, the program is designed to provide analytics knowledge pertaining to various domains (Marketing, Finance, Supply Chain, HR).Thiscouldbecome one of the best program, ifthey emphasized equally on in detail machine learning algorithms and data visualization.Fees: INR 250,000 + taxesThis is a 1 year Full Time program in association with IBM. This programMTNLs world class campus with state of art IBM Business Analytics Lab and IBM Cloud Computing Lab at Powai in Mumbai. This course would introduce you with various tools such as SAS, R, Tableau, Spark, IBM Cognos, Hadoop etc. The course is designed to provide comprehensive knowledge on business analytics and big data. The pedagogy involves 5 days classroom session and weekend online sessions.It would be interesting to see the placement statistics of this course once the batch passes out. If they continue to gain momentum, this program could help many companies with well educated candidates.Fees: INR 350,000 + taxesWeve ranked these business analytics programsbased onthe course offerings, brand recognition, value for money, course curriculum. Precisely, weve considered the following (below) parameters to rank these programs. Then, weights were assigned to these parameters. Thus, ensuring an output withmost appropriate rankings:These rankings would help you in deciding the best college suited to your needs. As mentioned above, this list is genuine and unbiased. These instituteshave been ranked on the basis of their course offerings and keeping in mind the benefit a student would reap.Have you decided to join any college ? Are you an alma materof any of these college ? Do share you experience / suggestions in the comments section below.Disclaimer: Some of these institutes use Analytics Vidhya as their marketing channel. Also, I teach as a guest faculty at some of these colleges. These ranking are purely independent of AVs collaboration with these institutes.",https://www.analyticsvidhya.com/blog/2015/12/top-business-analytics-programs-india-2015-2016/
A Complete Tutorial on Time Series Modeling in R,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Basics  Time Series Modeling|2. Exploration of Time Series Data in R|3. Introduction to ARMA Time Series Modeling|4. Framework and Application of ARIMA Time Series Modeling|Projects|End Notes,"Stationary Series|Why do I care about stationarity of a time series?|Random Walk|Lets spice up things a bit,|Dickey Fuller Test of Stationarity|Loading the Data Set|Detailed Metrics|Important Inferences|Auto-Regressive Time Series Model|Moving Average Time Series Model|Difference between AR and MA models|Exploiting ACF and PACF plots|Overview of the Framework|Step 1: Visualize the Time Series|Step 2: Stationarize the Series|Step 3: Find Optimal Parameters|Step 4: Build ARIMA Model|Step 5: Make Predictions|Applications of Time Series Model|Where did we start ?|What do you see in the chart shown above?|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Business Analytics Programs in India (2015  16)|10 Machine Learning Algorithms Explained to an Army Soldier|
Tavish Srivastava
|50 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the mostimportant factor which ensures success in a business. Its difficult to keep up with the pace of time. But, technology has developed some powerful methods using which we cansee things ahead of time. Dont worry, I am not talking about Time Machine. Lets be realistic here!Im talking about the methods of prediction & forecasting. One such method, which deals with time based data is Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to makeinformed decision making.Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyzesales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.So, if you arent sure about complete process of time series modeling, this guide would introduce you to various levels of time series modelingand its related techniques.Time toget started!Lets begin from basics. This includes stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity. If these terms are already scaring you, dont worry  they will become clear in a bit and I bet you will start enjoying the subject as I explain it.There are three basic criterion for a series to be classified as stationary series :1. The mean of the series should not be a function of time rather should be a constant. The image below has the lefthand graph satisfying the condition whereas the graph in red has a time dependent mean.2. The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)3. The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the red series.The reason I took up this section first was that until unless your time series is stationary, you cannot build a time series model. In cases where the stationary criterion are violated, the first requisite becomesto stationarize the time series and then try stochastic models to predict this time series. There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.This is themost basic concept of the time series. You might know the concept well. But, I found many people in the industry who interprets random walk as a stationary process. In this section with the help of some mathematics, I will make this concept crystal clear for ever. Lets take an example.Example: Imagine a girl moving randomly on a giant chess board. In this case, next position of the girl is only dependent on the last position.(Source: http://scifun.chem.wisc.edu/WOP/RandomWalk.html )Now imagine, you are sitting in another room and are not able to see the girl. You want to predict the position of the girl with time. How accurate will you be? Of course you will become more and more inaccurate as the position of the girl changes. At t=0 you exactly know where the girl is. Next time, she can only move to 8 squares and hence your probability dips to 1/8 instead of 1 and it keeps on going down. Now lets try to formulate this series :where Er(t) is the error at time point t. This is the randomness the girl brings at every point in time.Now, if we recursively fit in all the Xs, we will finally end up to the following equation :Now, lets try validating our assumptions of stationary series on this random walk formulation:1. Is the Mean constant ?We know that Expectation of any Error will be zero as it is random.Hence we get E[X(t)] = E[X(0)] = Constant.2. Is the Variance constant?Hence, we infer that the random walk is not a stationary process as it has a time variant variance. Also, if we check the covariance, we see that too is dependent on time.We already know that a random walk is a non-stationary process. Let us introduce a new coefficient in the equation to see if we can make the formulation stationary.Introduced coefficient : RhoNow, we will vary the value of Rho to see if we can make the series stationary. Here we will interpret the scatter visually and not do any test to check stationarity.Lets start with a perfectly stationary series with Rho = 0 . Here is the plot for the time series :Increase the value of Rho to 0.5 gives us following graph :You might notice that our cycles have become broader but essentially there does not seem to be a serious violation of stationary assumptions. Lets now take a more extreme case of Rho = 0.9We still see that the X returns back from extreme values to zero after some intervals. This series also is not violating non-stationarity significantly. Now, lets take a look at the random walk with rho = 1.This obviously is an violation to stationary conditions. What makes rho = 1 a special case which comes out badly in stationary test? We will find the mathematical reason to this.Lets take expectation on each side of the equation X(t) = Rho * X(t-1) + Er(t)This equation is very insightful. The next X (or at time point t) is being pulled down to Rho * Last value of X.For instance, if X(t  1 ) = 1, E[X(t)] = 0.5 ( for Rho = 0.5) . Now, if X moves to any direction from zero, it is pulled back to zero in next step. The only component which can drive it even further is the error term. Error term is equally probable to go in either direction. What happens when the Rho becomes 1? No force can pull the X down in the next step.What you just learnt in the last section is formally known as Dickey Fuller test. Here is a small tweak which is made for our equation to convert it to a Dickey Fuller test:We have to test if Rho  1 is significantly different than zero or not. If the null hypothesis getsrejected, well get a stationary time series.Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.Lets nowconsider an example to show you what a time series looks like.Here well learn to handle time series data onR. Our scopewill be restricted to data exploring in a time series type of data set and not go to building time series models.I have used an inbuilt data set of R called AirPassengers. The dataset consists ofmonthly totals of international airline passengers, 1949 to 1960.Following is the code which will help you load the data set and spill out a few top level metrics.Here are a few more operations you can do:Exploring data becomes most important in a time series model  without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.Lets nowtake up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.ARMA models are commonly used in time series modeling.In ARMA model, AR stands forauto-regression and MA stands formoving average. If these words sound intimidating to you, worry not  Illsimplify these concepts in next few minutes for you!We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose fromthe available time series models.First, Ill explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.Lets understanding AR models using the casebelow:The current GDP of a country say x(t) is dependent on the last years GDP i.e. x(t  1). The hypothesis being thatthe total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.Hence, we can formally write the equation of GDP as:x(t) = alpha * x(t  1) + error (t)This equation is known as AR(1) formulation.The numeral one (1) denotes that the next instance is solely dependent on the previous instance. The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.For instance, lets say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day,the temperature roseand the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing thatthe people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during thecold days. In following days, theproportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explainsthe inertia property of AR series:Lets take another caseto understand Moving average time series model.A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood atzero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market.Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so highthat the bag ran out of stock. As a result, some 100 odd customers couldntpurchase this bag. Lets call this gap as the error at that time point. With time,the bag hadlost itswoo factor. But still few customers were leftwho wentempty handed the previous day. Following is a simple formulation to depict the scenario :x(t) = beta* error(t-1) + error (t)If we try plotting this graph, it will look something like this :Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference getsexploited irrespective ofhaving the AR model or MA model. The correlation plot can give us the order of MA model.Once we have got thestationary time series, we must answertwo primary questions:Q1. Is it an AR or MA process?Q2. What order of AR or MA process do we need to use?The trick to solve these questions is availablein the previous section. Didnt you notice?The first question can be answered usingTotal Correlation Chart (also known as Auto  correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now lets reflect on what we have learnt above.In a moving average series of lag n, we will not get any correlation between x(t) and x(t  n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series, if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag(x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :              ACF                                   PACFThe blue line aboveshows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.                   ACF                                PACFClearly, the graph abovehas a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots.Now, Ill introduce you toa comprehensive framework to build a time series model. In addition, well also discuss about the practicalapplications of time series modelling.A quick revision, Till here weve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.This framework(shown below)specifies the step by step approach onHow to do a Time Series Analysis:As you would be aware, the first three stepshave already been discussed above. Nevertheless, the same has been delineated briefly below:It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.Once we know the patterns, trends, cycles and seasonality , we cancheck if the series is stationary or not. Dickey  Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesnt ends here!What if the series is found to be non-stationary?There are three commonly used technique to make a time series stationary:1. Detrending : Here, we simply remove the trend component from the time series. For instance, the equation ofmy time series is:x(t) = (mean + trend * t) + errorWell simply remove the part in the parentheses and build model for the rest.2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,x(t)  x(t-1) = ARMA (p , q)This differencing is called as the Integration part in AR(I)MA. Now, we have three parametersp : ARd : Iq : MA3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly.More on this has been discussed in the applications part below.The parameters p,d,q can be found using ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to d.With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should beour choice. We can also try some models with a seasonal component. Just in case, we noticeany seasonality in ACF/PACF plots.Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.Now, well use the same example that we have used above. Then,using time series, well make future predictions. We recommend you to check out the example before proceeding further.Following is the plot of the number of passengers with years. Try and make observations on this plotbefore moving further in the article.Here are my observations :1. There is a trend component which grows the passenger year by year.2. There looks to be a seasonal component which has a cycle less than 12 months.3. The variance in the data keeps on increasing with time.We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, lets test the resultant series.Augmented Dickey-Fuller TestWe see that the series is stationary enough to do any kind of time series modelling.Next step is to find the right parameters to be used in theARIMA model. We already know that the d component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :#ACF PlotsClearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Lets see how ACF and PACF curve come out after regressing on the difference.Clearly, ACF plot cuts off after the first lag. Hence, we understoodthat value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.Lets fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Withthis, we come to this end oftutorial on Time Series Modelling. I hope this will help you to improve your knowledgeto work on time based data. To reap maximum benefits out of this tutorial, Id suggest you to practice these R codes side by side and check your progress.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/
10 Machine Learning Algorithms Explained to an Army Soldier,Learn everything about Analytics|Introduction|There are 3 Categories of Machine Learning Algorithms|These arecommonly used Machine Learning Algorithms|1. Linear Regression|2. Logistic Regression|3. Tree Based Modeling|4. Bayesian Modeling|5. Support Vector Machines|6. kNN (k Nearest Neighbour)|7. k-means|8. Neural Network and Perceptrons|9. Ensemble Modeling|10. Anomaly Detection|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|A Complete Tutorial on Time Series Modeling in R|7 Important Ways to Summarise Data in R|
Analytics Vidhya Content Team
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you think deep, youd realize the whole process of predictive modeling is a war. Ruthless war. Dont you believe me?Consider the data set as your opponent. Your knowledge of data mining & ml algorithms as your weapons. Your win will depend ontheintuitive usage of your knowledge & strategy to get highest accuracy. So, how many times have you won this battle?Heres my short story.I realized it last week. While travelling to south, I met a person whose daughter is a data scientist. Thats all he knew when I asked him about his kids profession. He didnt get much time to spent with his daughter. He was a soldier.He inquired about me too.His eyes glowed with ecstasy when he found out that Im also a data scientist. He inquired further. Precisely, he was curious to know about MLAlgorithms. Its a lot being used in defense these days, he said.This was a tough task for me. Ididnt know how to explain the technical stuff to a soldier. I found out a way. I knewhe would surely understandthings about battle, warfare, strategy. I explained him machine learning in these terms. In the end, he was convinced that his daughter has landed herselfinto a challenging job. He felt proud.In this article, Ive revealed that approach used to explain MLalgorithms to the soldier. Though, there would be less soldiers who would read this, yet this idea promotes an interesting way of learning. This would surely help beginners who are struggling to understand these algorithms. Ive also added images to help you visualize the situation and learn.Note: The objective of this article is to help people learn machine learning in a fun and interesting way. Youll also notice every algorithm has its own special situation where it is used.This ishow I started!Supervised Learning : You go on a war and dont stop until you killall your enemies whoever comes your way. It include algorithms such as Linear Regression, Logistic Regression, Decision Tree, Random Forest etc.Unsupervised Learning :Your rival has challenged you for a war. Now, you decide after assessing your strengths and weakness, whether to accept the challenge or surrender. It include algorithms such as k-means, apriori etcReinforcement Learning: Youve accepted the challenge. The war has begun. After every hour, you are accessing your position in the battle. Are you loosing more men? Is the opponent dominating? And, accordingly you decide where to continue or surrender. It includes algorithm such as markov decision process.These algorithms are nothing but the weapons used to fight data sets. Make sure you learn them well to build your armory. Look, Ive 10 weapons in my armory. How many have you got?Lets see my way of explaining these algorithms to the soldier. You can also view the below video that gives you a quick overview of these techniques!Linear Regression is completely taking over your opposition. Once youve entered the battle ground, dont look back. Only take rest, once youve satisfactorily killed all your enemies.Logistic Regression is taking simplistic assumptions and then deciding whether to go on a war or not.This includes Decision Trees, Random Forest. Tree Based Modeling is simply Divide and Rule. You divide your opponents with some smart strategy and kill them.Bayesian Modeling is considering yourprobability of winning in different battle base types such as air battle, land battle, water battle. And, accordingly take the overall decision of battle.SVM is drawing out territory and boundary where youve advantage on field base. For example, your soldiers might be adept to fighting in specific areas such as desert / mountain / fields and declare the war accordingly.kNN is checking past outcomes and mapping accordingly. Evaluate your performance in past battles, contemplate on your weak and strong areas and prepare for the next battle accordingly.k-means is building up alliances (groups) withprovinces which share the same philosophy, goals and motives. The idea is to become more powerful than ever before.Neural Network is every soldier in your army decides whom to fight. Imagine, the situation arising when soldiers run towards enemy soldiers and instantly decide which soldier to fight. Naturally, every soldier would prefer to fight a weaker enemy soldier (watch and analyze) so that he can kill asmany quickly. Overall, this strategy would impact the performance of whole army.Ensemble Modeling is when an army consists of men skilled in various combats such as archery, knife fighting, swordsmen, shooting etc which shares a common motive of victory over enemies. All these men together would result in a formidable army.Anomaly Detection is checking for unusual patterns in your own army. You may have a secret agent among your soldier brothers. Keep your periodic checks on.I hope now you understand these algorithms well. This is how I helped that soldier to understand these algorithms. I believe, the most difficult things in nature, do have a simple explanation to offer. We just need to start relating this around us. As I just did. People new to machine learning, would now find them easier to remember and apply these algorithms.In this article, Ive explained machine learning algorithms to a soldier in terms of war, battle, and strategy.Do you find watching battle, wars interesting? If yes, you would surely find an interest here.Did you find it useful ? Do share your opinions / suggestions in the comments section below. Id love to talk.",https://www.analyticsvidhya.com/blog/2015/12/10-machine-learning-algorithms-explained-army-soldier/
7 Important Ways to Summarise Data in R,Learn everything about Analytics|Introduction|Methods to Summarise Data in R|1. apply|2. lapply|3. sapply|4. tapply|5. by|6. sqldf|7. ddply||End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|10 Machine Learning Algorithms Explained to an Army Soldier|Interview with Dr. Bibek Banerjee  Dean, BRIDGE School of Management|
Tavish Srivastava
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People remain confused when it comes to summarizing data real quick in R. There are various options. But, which one is the best ? Ive answered this question below. You must choose one at first. And, become expert at it. Thats how you should move to the next.People who transition from SAS or SQL are used to writing simple queries on these languages to summarize data sets. For such audience, the biggest concern is to how do we do the same thing on R.In this article I will cover primary ways to summarize data sets. Hopefully this will make your journey much easier than it looks like.Generally, summarizing data means finding statistical figures such as mean, median, box plot etc. If understand well with scatter plots & histogram, you can refer to guide on data visualization in R.Apply functionreturns a vector or array or list of values obtained by applying a function to either rows or columns. This is the simplest of all the function which can do this job. However this function is very specific to collapsing either row or column.lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.sapply does the same thing asapply but returns a vector or matrix. Lets consider the last example again.Till now, all the function we discussed cannot do whatSql can achieve. Here is a function which completes thepalette for R.Usage is tapply(X, INDEX, FUN = NULL, , simplify = TRUE), where X is an atomic object, typically a vector and INDEX is a list of factors, each of same length as X. Here is an example which will make the usage clear.Now comes a slightly more complicated algorithm. Function by is an object-oriented wrapper for tapply applied to data frames. Hopefully the example will make it more clear.What did the function do? It simply splits the data by a class variable, which in this case is the specie. And then it creates a summary at this level. So it does apply function on split frames. The returned object is of class by.If you found any of the above statements difficult, dont panic. I bring you a life line which you can use anytime. Lets fit in the SQL queries in R. Here is a way you can do the same.ddply(iris,""Species"",summarise, Petal.Length_mean = mean (Petal.Length))Additional Notes: You can also use packages such as dplyr, data.table to summarize data. Heres a complete tutorial on useful packages for data manipulation in R  Faster Data Manipulation with these 7 R Packages.In general if you are trying to add this summarisation step in the middle of a process and need a table as output, you need to go for sqldf or ddply. ddply in these cases is faster but will not give you options beyond just grouping. sqldf has all features you need to summarize the data in SQL statements.In case you are interested in using function similar to pivot tables or transposing the tables, you can consider using reshape. We have covered a few examples of the same in our article  comprehensive guide for data exploration in R.Challenge : Here is a simple problem you can attempt to solve using all the methods we have discussed. You have a table for all school kids marks in a particular city.Write a code to find the mean marks of each school for both class 1 and 2, for students with roll no less than 6. And print only the class whose mean score comes out to be higher for the school. For instance, if school A has a mean score of 6 for class 1 and 4 for class 2, you will reject class 2 and only take class 1 mean score for the school. In cases of tie, you can make a random choice. Assume that the actual table is much bigger and keep the code as generalized as possible.Did you find the article useful? Have you used any other function to summarize data on R. If yes, tell us about the function and your views on other functions discussed in this blog.",https://www.analyticsvidhya.com/blog/2015/12/7-important-ways-summarise-data/
"Interview with Dr. Bibek Banerjee  Dean, BRIDGE School of Management",Learn everything about Analytics,"Introduction|Kunal: What was the idea behind launching Predictive Business Analytics Program?|Kunal: How is your course different from other business analytics programs in India?|Kunal: Who is ideally suited to enroll in this program?|Kunal: What is the structure of the program  total classroom time, expectation from the candidate and the areas covered as part of the course?|Kunal: What is the teaching methodology adopted for this Program  Online / Offline / Hybrid?|Kunal: How much of the program is devoted to Industry Interaction and Live Project?|Kunal: Is there a capstone project / industry project at the end of the course? If yes, how much time are students expected to devote on it?|Kunal: Tell us about the Faculties of this program. Can we expect faculties from NorthWestern University also?|Kunal: How about placements? Would placement support be available for this program? How has been the placements till now?|Kunal: Youve recently launched online courses in analytics. Tell us a bit more about it?|Kunal: Currently, you are offering the program only in Delhi / When can we expect programs in other cities?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|7 Important Ways to Summarise Data in R|Enterprise BI Architect  Bangalore / Gurgaon (15+ years of experience)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Industry in India is expandingfast.More and more companies are introducing analytics in their core processes. This has resulted in increaseddemand ofanalytics professionals. Demand is so high that companiesat times fail to hiredesired skilled professionals. Because, there arent many. This imbalance in supply and demand has caught the attention of various educational bodies worldwide.Many institutions (independently or collaboratively) have taken the initiative to nourish thisindustry withsmart & skilled professionals.These initiatives involve intensive business analytics program & courses. These courses are designed to equip candidates with latestanalytical tools & techniques used in industry. This has enabledvariousbusinesses to establish analytics / data science teams.
One such institution,BRIDGE School of Management runs an 11 month long Predictive Business Analytics Program. Recently, Icaught up with Dr. Bibek Banerjee, Dean and Head of Academics, BRIDGE School of Management, to get in depth insights from this course. As a reader, this will help you to evaluate this program as an option tostart your career in business analytics.Below is my exclusiveconversation with Dr. Bibek Banerjee. I would like to thank him to sparing time out of his busy schedule for this interview.Dr. Banerjee: Demand for professionals experienced in Analytics is on the rise across organizations as every organization is looking for ways to exploit the power of Big Data. BRIDGEs data analytics program offers industry ready curriculum to help learners apply latest concepts and analytical techniques to leverage data in creating tangible business value.Leveraging the best-in-class knowledge, experience, expertise and technology from North-Western University, BRIDGE through its industry-focused Analytics programs goes beyond the theoretical knowledge of concepts and provides an innovative learning environment while focusing on upgrading their managerial, functional and interpersonal skills.Each of our programs is led by a Council composed of a Program Chair, academicians with domain expertise, Functional and HR Heads from large Indian and international corporates. This team manages the program in terms of its design, project flow, case studies and lecture-demonstrations, soft skills workshops and delivery structure of specific elements of the various courses. This team is also engaged in periodically reviewing each program in terms of its quality  and initiate appropriate measures to continually improve the programs.Northwestern University is one of the worlds leading educational institutions and their post graduate analytics program is ranked among the top 5 (one of the top 2) in the world for their Analytics Program. The certificate program has been specially created for India by academicians from Northwestern University and top industry experts using real-world problems and situations. The Northwestern and Bridge School initiative combines online content developed and taught by Northwestern faculty with weekly in-person sessions led by local specialist faculty at the Bridge Schools learning centers.Dr. Banerjee: BRIDGE School provides tangible take-aways that are relevant, comprehensible and transparent to our students. We usually minimize the abstract and maximize the practical skills and competencies needed to successfully affect the next turning point in their career. Some key differentiators:First, we are strategically positioned close to the industry  both physically as well as intellectually. Our two pilot centers: Cyber City Gurgaon, and Noida; are embedded among the corporate offices of most large and medium organizations.Second, the BRIDGE School resource base comprises of a wide variety of professional courses, workshops, simulators, business games and interactive e-libraries is truly world class. We are able to build this as we can leverage our collaborators from the best across the world. Like Northwestern University, home to the famous Kellogg School of Business.Third, we have customized our Analytics programs, from Marketing to Risk, catering to industry needs in India based on inputs from leading organizations, subject matter experts and industry associations.Let me also point out that as our faculty comprises senior leaders from leading organizations, they help learners relate real corporate situations and case studies with theory as they are equipped to teach practical and relevant applicable skills in the weekend sessions.BRIDGE School has empanelled over 30 organizations based in Delhi NCR across consulting, e-commerce, telecom, IT, BFSI only for the analytics opportunities.Furthermore, we offer career services for our students who seek greener opportunities. Our consultants work individually with students to provide opportunities to them in successfully unlocking their potential. BRIDGE seeks to focus on developing relevant inter-personal skills of learners, making them Corporate-ready with a focus on:-Communication: Preparing reports and delivering presentations, e-mail etiquette, and participation in group discussions.Team work and Managerial Skills: Collaborative work with peers, cross-functional working and conflict management and resolution.Business Development Skills: Client management, portfolio management etc.Negotiation Skills: Strategies and techniques for effective negotiation with clients and suppliers.Analytical Skills: Problem solving, forecasting and innovation.Adaptability: Multi-tasking and time management.Dr. Banerjee: In todays day and age, organizations ability to make business sense out of Big Data will decide their success trajectory. Hence, organizations across sectors need trained resources who can work with Analytics, Big Data and data sciences all though the organization value chain levels to derive business benefit. The Industry need both existing working professionals and freshers who are keen in making a career in Analytics to fill in the demand. BRIDGE School has the following specialized offerings in association with Northwestern University for both categories:Dr. Banerjee: The Certificate in Predictive Business Analytics is a 40 weeks program. A successful completion of the program will earn aspirants the coveted certification in Predictive Business Analytics from Northwestern University (SPS) & BRIDGE School of Management. This program has been adapted from Northwesterns MSPA degree program.The Advanced Certificate in Analytics is an 8 week long program. After completing the Certificate in Predictive Business Analytics, students may continue their education and earn an advanced certificate in one or more of the following areas:For fresh graduates, (0- 11 months of work experience), the Certificate in Predictive Business Analytics includes specialization and internship in a 58 week long program.Dr. Banerjee: Our teaching pedagogy is blended in nature which combines the best of virtual and face-to-face learnings.Northwestern Universitys CANVAS online learning environment provides an innovative learning platform that encourages collaborative approach between the candidates. Weekend classes delivered by Senior Practitioner Faculty (CEOs/VPs from leading organizations) help employees relate real corporate situations and case studies with theory. Complementing them is a panel of the best-in-class Indian and foreign academicians.Dr. Banerjee: The program focus is on practical learning via live projects/case studies, simulations of real life corporate situations. Learners apply analytics tools to real-world business contexts for improved decision-making and acquire hands-on experience working with leading statistical tools and software packages (such as R) in predictive modelling and the visual analysis of results.Weekend Programs Delivered by Practitioner Faculty  CEOs/VPs from leading organizations help employees relate real corporate situations and case studies with theory as they are equipped to teach practical, relevant applicable skills by bringing back learning from their workplace. Complementing them will be a panel of the best-in-class Indian and foreign academicians.Advanced state-of-the-art Virtual Learning Environment enables learners to access pre-recorded lectures of expert global faculty and industry leaders, have discussions with faculty and carry out group projects with a network of students from Fortune 500 companies at their own convenience  whenever, wherever.Dr. Banerjee: Yes, there are industry projects from companies like Webitude from time to time which are made available for the students. For example this year we are working with TiE members to organize live analytics projects and Case Contests for BRIDGE students; as well as a few mentorship engagements  this way BRIDGE students get unparalleled exposure to subjects that are current and stay abreast of the dynamical changes in the discipline in practice.Dr. Banerjee: Our EDUCATORS are a mix of practicing professionals and hand-picked academicians. This ensures that theory is always actioned with practicalities of running businesses in every aspect of our program delivery. Our Program Chair for Analytics is Dr. Srabashi Basu at BRIDGE SCHOOL OF MANAGEMENT. She is Ph.D., MA (Statistics  The Pennsylvania State University). M. Sc. (Statistics  The University of Calcutta). In her previous assignments, she has worked with Indian Statistical Institute. Industry practitioners who take Face-to-face sessions, include the Director of Risk Management, American Express India, the Principal Consultant of Retail Strategy & Analytics, Trequista Consulting, etc.Additionally, live sessions from Northwestern University faculty is part of the Program. Apart from live sessions, Northwestern University faculty are involved online on an ongoing basis. They engage with students in monitoring and evaluating discussion posts as well as evaluating assignments. Their turnaround cycle on doubt-clearing is usually within 24 hours. Special synchronous sessions are also arranged with NW faculty at regular intervals.Dr. Banerjee: For those candidates looking to get a job as a business analyst and build their career in analytics, BRIDGE has an extensive industry network to facilitate placements for its students. It is BRIDGEs commitment to make sure their students get quality placement opportunities on successful completion of the program.We have empanelled over 25 organizations based in Delhi NCR across consulting, ecommerce, telecom, IT sectors, BFSI for analytics opportunities. We do not have a campus approach to placement, it is a consultative process for all interested learners who opt in. Also with the industry requirements, learners who have >5 years of work experience, are provided assistance to opportunities. The average salary hike of the first round of placement drive is 51%.Dr. Banerjee: BRIDGE Onlines Data Analytics program offers a foundation program in data sciences with courses in database fundamentals including Core Database Concepts, Data Storage, Relational databases, Manipulating Data and Administering Databases.The program will help the students to acquire basic understanding of Analytics Application in Database Marketing, evolve into a Business Data analytics expert, while building a strong foundation in the analytics using Excel and generate creative and intelligent actionable insights to facilitate decision making using statistical and analytical methodsWhile there is no formal qualification required to study this course, we recommended the Analytics courses to be studied by working professionals in related domain & fresh graduates hoping to get into analytics domain. The Program will be especially helpful for:Dr. Banerjee: Our vision for BRIDGE school is to build that missing-BRIDGE, in collaboration with industry partners and the academia. Our goal is to create a portfolio of innovative Analytics Programs providing world class knowledge, expertise and learning experience for our biggest stakeholder  the young and aspiring professionals across India. Yes, geographical expansions to other key Analytics centers like Bangalore and Mumbai are planned.Kunal:Thank you Dr. Bibek Banerjee.Youve aptly highlighted the important aspectsof this program. Im sure this program would help students to establish a successful career in analytics industry. I wish you all the best.If you have any questions / queryrelatedthis program, feel free to post them in the comments section below.",https://www.analyticsvidhya.com/blog/2015/12/news-bridge-school-management-launched-predictive-business-analytics-program/
"Enterprise BI Architect  Bangalore / Gurgaon (15+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Interview with Dr. Bibek Banerjee  Dean, BRIDGE School of Management|Do Faster Data Manipulation using These 7 R Packages|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Enterprise BI ArchitectLocation  Bangalore / GurgaonAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asEnterprise BI Architect  Bangalore / Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/12/enterprise-bi-architect-bangalore-gurgaon-15-years-experience/
Do Faster Data Manipulation using These 7 R Packages,Learn everything about Analytics|Introduction|What is Data Manipulation ?|Different Ways to Manipulate / Treat Data:|List of Packages|dplyr Package|data.table Package|ggplot2 Package|reshape2 Package|readr Package|tidyr Package|Lubridate Package|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Enterprise BI Architect  Bangalore / Gurgaon (15+ years of experience)|10 Ultimate Tips and Tricks on Data Visualization in QlikView|
Analytics Vidhya Content Team
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Manipulation is an inevitable phase of predictive modeling. A robust predictive model cant just bebuilt using machine learning algorithms. But, with an approach to understand the business problem, the underlying data, performing required data manipulations and then extracting business insights.Among these several phases of model building, most of the time is usually spent in understanding underlying data and performing required manipulations. This would also be the focus of this article  packages to perform faster data manipulation in R.If you are still confused with this term, let me explain it to you. Data Manipulation is a loosely used term with Data Exploration. It involves manipulating data using available set of variables. This is done to enhance accuracy and precision associated with data.Actually, the data collection process can havemanyloopholes. There are various uncontrollable factors which lead to inaccuracy in data such as mental situation of respondents, personal biases, difference / error in readings of machines etc. To mitigate these inaccuracies, data manipulation is done to increasethe possible (highest) accuracy in data.At times, this stage is also known as data wrangling or data cleaning.There is no right or wrong way in manipulating data, as long as you understand the data and have taken the necessary actions by the end of the exercise. However, here are a few broad ways in which people try and approach data manipulation. Here are they:Hence, more often than not, use of packages is the de-facto method to perform data manipulation. In this article, I have explained several packages which makeR life easier during the data manipulation stage.Note: This article is best suited for beginners in R Language. You can install a packages using:install.packages('package name')For better understanding, Ive also demonstrated their usage by undertaking commonly used operations. Below is the list of packages discussed in this article:Note: I understand ggplot2 is a graphical package. But, it generally helps in visualizing data ( distributions, correlations) and making manipulations accordingly. Hence, Ive added it in this list. In all packages, Ive covered only the most commonly used commands in data manipulation.This packages is created and maintained by Hadley Wickham. This package has everything (almost) to accelerate your data manipulation efforts. It is known best for data exploration and transformation. Its chaining syntax makes it highly adaptive to use.It includes 5 major data manipulation commands:Simple focus on these commands and do great in data exploration. Lets understand these commands one by one. I have used 2 pre-installed R data sets namely mtcars and iris.This package allows you to perform faster manipulation in a data set. Leave your traditional ways of sub setting rows and columns and use this package. With minimum coding, you can do much more. Using data.table helps in reducing computing time as compared to data.frame. Youll be astonished by the simplicity of this package.A data table has 3 parts namely DT[i,j,by]. You can understand this as, we can tell R to subset the rows using i, to calculate j which is grouped by by. Most of the times, by relates to categorical variable. In the code below, Ive used 2 data sets (airquality and iris).ggplot offers a whole new world of colors and patterns. If you are a creative soul, you would love this package till depth. But, if you wish learn what is necessary to get started, follow the codes below. You must learn the ways to at least plot these 3 graphs: Scatter Plot, Bar Plot, Histogram.These 3 chart patterns covers almost every type of data representation except maps. ggplot is enriched with customized features to make your visualization better and better. It becomes even more powerful when grouped with other packages like cowplot, gridExtra. In fact, there are a lot of features. Hence, you must focus on few commands and build your expertise on them. I have also shown the method to compare graphs in one window. It requires gridExtra package. Hence, you must install it. Ive use pre-installed R data sets.For more information on this package, you refer to cheatsheet here: ggplot2 cheatsheetAs the name suggests, this package is useful in reshaping data. We all know the data come in many forms. Hence, we are required to tame it according to our need. Usually, the process of reshaping data in R is tedious and worrisome. R base functions consist of Aggregation option using which data can be reduced and rearranged into smaller forms, but with reduction in amount of information. Aggregation includes tapply, by and aggregate base functions.The reshape package overcome these problems. Here we try to combine features which have unique values. It has 2 functions namely melt and cast.melt : This function converts data from wide format to long format. Its a form of restructuring where multiple categorical columns are melted into unique rows. Lets understand it using the code below.cast : This function converts data from long format to wide format. It starts with melted data and reshapes into long format. Its just the reverse of meltfunction. It has two functions namely, dcast and acast. dcast returns a data frame as output. acast returns a vector/matrix/array as the output. Lets understand it using the code below.Note: While doing research work, I found this image which aptly describes reshape package.                                        source: r-statisticsAs the name suggests, readr helps in reading various forms of data into R. With 10x faster speed. Here, characters are never converted to factors(so no more stringAsFactors = FALSE). This package can replace the traditional read.csv() and read.table() base R functions. It helps in reading the following data:If the data loading time is more than 5 seconds, this function will show you a progress bar too. You can suppress the progress bar by marking it as FALSE. Lets look at the code below:You can also specify the data type of every column loaded in data using the code below:However, if you choose to omit unimportant columns, it will take care of it automatically. So, the code above can also be re-written as:P.S  readr has many helper functions. So, next when you write a csv file, use write_csv instead. Its a lot faster than write.csv.This package can make your data look tidy. It has 4 major functions to accomplish this task. Needless to say, if you find yourself stuck in data exploration phase, you can use them anytime (along with dplyr). This duo makes a formidable team. They are easy to learn, code and implement. These 4 functions are:Lets understand it closely using the code below:Separate function comes best in use when we are provided a date time variable in the data set. Since, the column contains multiple information, hence it makes sense to split it and use those values individually. Using the code below, I have separated a column into date, month and year.Lubridate package reduces the pain of working of data time variable in R. The inbuilt function of this package offers a nice way to make easyparsing in dates and times. This packages is frequently used with data comprising of timely data. Here I have covered three basic tasks accomplished using Lubridate.This includes update function, duration function and date extraction. As a beginner, knowing these 3 functions would give you good enough expertise to deal with time variables. Though, R has inbuilt functions for handling dates, but this is much faster. Lets understand it using the code below:Note: The best use of these packages is not in isolation but in conjunction. You could easily use this package with dplyr where you can easily select a data variable and extract the useful data from it using the chain command.These packages would not only enhance your data manipulation experience, but also give you reasons to explore R in depth. Now we have seen, these packages make coding in R easier. You no longer need to write long codes. Instead write short codes and do more.Every package has multi tasking abilities. Hence, I would suggest you to get hold of important function which can be used frequently. And, once you get familiar with them, you can dig deeper. I did this mistake initially. I tried at exploring all the features in ggplot2 and ended up in a confusion. Id suggest you to practice these codes as you read. This would help you build confidence on using these packages.In this article, Ive explained the use of 7 R packages which can make data exploration easier and faster. R known for its awesome statistical functions, with newly updated packages makes a favorite tool of data scientists too.",https://www.analyticsvidhya.com/blog/2015/12/faster-data-manipulation-7-packages/
10 Ultimate Tips and Tricks on Data Visualization in QlikView,Learn everything about Analytics|Introduction|List of Tips and Tricks Covered|Copy Objects|Copy Expression|Move chart components|Select Multiple Objects|Adding Visual elements to Table|Limit Dimension of a Chart|Conditionally Show or Hide Objects|Use comments in Qlikview scripting:||Use script tabs in Qlikview scripting|Keyboard Shortcuts|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Do Faster Data Manipulation using These 7 R Packages|Data Visualizer  Delhi / NCR (2+ years of experience)|
Sunil Ray
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"QlikView is a popular and simple to learn tool for data visualization. Its simpleinterface makes it afavorite amongnewbies in analytics. I loved it too. In fact, my journey in business intelligence began with QlikView. As from a non-programming background, I couldnt have asked for a better start.Over the years, Ive discovered someways of becoming productive at QlikView. I found these ways after spending endless hours working on this tool. Finally, I ended up finding these 10 tricks which can help you work faster. Without these tricks, I was almost spending 15 minutes in a task which can now bedonein less than 3 mins.In this article, Ive shared these 10 ultimate tips & tricks which cansave your time and increase your productivity while working on this tool.We oftencreate objects which are similar to one of existing objects with minor changes in dimension, expression or any other parameter(s). Its a time taking process.But, you can do it much faster now. Simply,copy the existing object and make the necessary changes.You can copy sheet and chart object both. Method is as follows:Duringdashboard development, we write multiple expressions (simple and complex). There are situationswhen we end up writing similar expressions for different chart objects. To make it efficient, we can re-use our existing expressions. Lets look at the methods:By default, position of chart components are fixed at the right hand side. But, in some cases we are required to change theposition for better visualization. For which,there is no direct option available. I toodidnt have any solution of this issue initially. But, with time I found out a way. This can be done by using combination of keyboard shortcuts. Here is the trick:Sometimes we want to perform similar operation such aschanging font size, layout, deletion with multiple objects. In such cases, we select all objects andperform the required operation. For selection, initially, we select one object then pressSHIFT key. Then,click on the title of other objects. Now, you can perform group operation (applicable) on these objects like delete,using properties option (right click on any selected object) change the layout, font and caption.
Usually, we show only numbers intabular representation. However, we can make the table more informative by adding visualelements to table. In Qlikview, we can apply three visualization techniques to table:Look at the below chart, it has too many dimensions makingit difficult to make inferences about data.
We can solve this challenge by:We can show or hide Qlikview objects based on the requirement. It helps to optimize the area of sheet object effectively. Under layout tab of properties window, there is an option to show or hide the sheet object (always or conditionally). Here you can write expression or use variable to control it. Generally, developers use Buttons to change the value of variable such asone button assigns value 0 to variable and another assigns 1.We can also show or hide dimension and expression of chart object and best use of this feature is Adhoc reporting or Customized reporting. You can refer article Customized reporting in Qlikview.Great programmers always add comments withdifferent segment of code. It makes easier to read and debug the code. In Qlikview scripting also, this practice is highly recommended.There are several ways to create comments in your script.To keep scripts maintainable, we do follow modular structure in coding. In similar line, we should use scripts tab to standardize our scripts. It helps in faster scriptediting and debugging. Qlikview scripts are run top to bottom on each tab, from the left most tab to the right.Above, you can that different tabs for XY Example, FilmsDatabase, Rating and Dictionary.Before checking theseshortcuts,first turn onShow Shortcut keys in ScreenTipsoption. With this, it willshow shortcut keys whenever we hover the mouse pointer over menu icons. Just like Microsoft Excel does. This helps a lotinmemorizing keyboard shortcuts.To enable it, right click on toolbar area > selectcommand Customize > Go toOptions tab >Checkbox on forShow shortcut keys in ScreenTips. Below are some useful keyboard shortcuts:
Not only these tips will make you work faster. But, youll also get more time to explore new things at your end. Visualizing data is equally important as analyzing data. Hence, you should strive to become better day by day in data visualization.In this article, we looked at QlikView tips and trickswhich willenhance your dashboard development experience. Here, I have mostly covered tips and tricks related associated withfront end or designer point of view like copying objects & expression,work withchart components, selecting multiple objects, limit dimension, visualization with table, conditionally show or hide objects, scripting best practices and finally keyboard shortcuts.Did you findthis article useful ? Doyou want to add some more tips and tricks ? Feel free to share your experience with us, Id love to hear you.",https://www.analyticsvidhya.com/blog/2015/12/10-tips-tricks-data-visualization-qlikview/
"Data Visualizer  Delhi / NCR (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|10 Ultimate Tips and Tricks on Data Visualization in QlikView|Hilarious Jokes & Videos on Statistics and Data Science|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data VisualizerLocation  Delhi / NCRAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Visualizer  Delhi / NCR and the following details:,https://www.analyticsvidhya.com/blog/2015/12/data-visualizer-delhi-ncr-2-years-experience/
Hilarious Jokes & Videos on Statistics and Data Science,Learn everything about Analytics|Introduction|Just Videos|Are you a Statistician / Data Analyst? You are Funny!|Innocent Dilbert & XKCD Special|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Visualizer  Delhi / NCR (2+ years of experience)|Manager/ Sr. Manager  Gurgaon (5+ years of experience)|
Analytics Vidhya Content Team
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Take it Easy. Data Science is Fun-tastic!If you love data science, youd find many aspects to it. A month back, I found 10 Best Movies on Machine Learning. A week later, I found 7 Documentaries on Statistics. Its time to explore the funny side of analytics.Ive compiled a list of best hilarious jokes (including images, videos) based on numbers, statistics, big data, machine learning. Hope youd enjoy reading them. During this process, I found many jokes. In fact too many. Hence, Ive listed the best ones I liked.P.S. To unlock the funny aspect of these jokes, you should have basic knowledge of data science related topics. All The Best!Imagine a world without proper search!Duration  1:32 minsA Thrilling Style of Overfitting in Machine LearningDuration  5:10 minsGeoff Hinton knows how the brain worksDuration  2:15 minsA statisticians wife had twins. He was delighted. He rang the minister who was also delighted. Bring them to church on Sunday and well baptize them, said the minister. No, replied the statistician. Baptize one. Well keep the other as a control.-:D:D:D:DTwo statisticians were traveling in an airplane from LA to New York. About an hour into the flight, the pilot announced that they had lost an engine, but dont worry, there are three left. However, instead of 5 hours it would take 7 hours to get to New York.A little later, he announced that a second engine failed, and they still had two left, but it would take 10 hours to get to New York.Somewhat later, the pilot again came on the intercom and announced that a third engine had died. Never fear, he announced, because the plane could fly on a single engine. However, it would now take 18 hours to get to New York.At this point, one statistician turned to the other and said, Gee, I hope we dont lose that last engine, or well be up here forever!-:D:D:D:DPatient: Will I survive this risky operation?
Surgeon: Yes, Im absolutely sure that you will survive the operation.
Patient: How can you be so sure?
Surgeon: 9 out of 10 patients die in this operation, and yesterday died my ninth patient.-:D:D:D:DOne day there was a fire in a wastebasket in the office of the Dean of Sciences. In rushed a physicist, a chemist, and a statistician. The physicist immediately starts to work on how much energy would have to be removed from the fire to stop the combustion.The chemist works on which reagent would have to be added to the fire to prevent oxidation. While they are doing this, the statistician is setting fires to all the other wastebaskets in the office.What are you doing? the others demand. The statistician replies, Well, to solve the problem, you obviously need a larger sample size.-:D:D:D:DThree statisticians went out hunting, and came across a large deer. The first statistician fired, but missed, by a meter to the left. The second statistician fired, but also missed, by a meter to the right. The third statistician didnt fire, but shouted in triumph, On the average we got it!-:D:D:D:DA statistics major was completely hung over the day of his final exam. It was a true/false test, so he decided to flip a coin for the answers. The statistics professor watched the student the entire two hours as he was flipping the coin  writing the answer  flipping the coin  writing the answer.At the end of the two hours, everyone else had left the final except for the one student. The professor walks up to his desk and interrupts the student, saying, Listen, I have seen that you did not study for this statistics test, you didnt even open the exam. If you are just flipping a coin for your answer, what is taking you so long? The student replies bitterly (as he is still flipping the coin), Shhh! I am checking my answers!-:D:D:D:DA company manager is flying across the desert in a hot air balloon when he realizes he is lost. He calls down to a man riding a camel below him and asks where he is.The man replies Youre 42 degrees and 12 minutes, 21.2 seconds north, 122 degrees , 10 minutes west, 212 metres above sea level, heading due east by north east.Thanks, replies the balloonist. By the way, are you a data analyst?Yes, replies the man, how did you know?Everything you told me was totally accurate, you gave me way more information than I needed and I still have no idea what I need to do.Im sorry, replied the camel-riding analyst. By the way, are you a company manager?Yes, said the balloonist, how did you know?Well, replied the analyst, Youve got no idea where you are, no idea what direction youre heading in, you got yourself into this fix by blowing a load of hot air, and now you expect me to get you out of it.-:D:D:D:DQuestion: Whats the difference between an introverted data analyst & an extroverted one?Answer: the extrovert stares at YOUR shoes.I enjoyed compiling this funny data on data science & statistics. After all, Statistics may be dull, but it has its moments. My idea behind this article was to introduce you to the lighter side of data science.Just to prove that intensive topics can be funnier too. Hence, dont get too busy in your work and you fail to explore its entertaining side.In this article, Ive listed the best hilarious jokes, videos, memes on data science and statistics. I hope you liked this post.Did I miss anything? Do share your favorite funny jokes/images/videos in data science. Share you views/ suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/12/hilarious-jokes-videos-statistics-data-science/
"Manager/ Sr. Manager  Gurgaon (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hilarious Jokes & Videos on Statistics and Data Science|Learn to Build Powerful Machine Learning Models with Amazon Service|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Manager/ Sr. ManagerLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Manager/ Sr. Manager  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/12/manager-sr-manager-gurgaon-5-years-experience/
Learn to Build Powerful Machine Learning Models with Amazon Service,Learn everything about Analytics|Introduction|Whats New inAmazon Machine Learning ?|Price Breakdown|Machine Learning Model usingAmazonService|Checking Model Results|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Manager/ Sr. Manager  Gurgaon (5+ years of experience)|Software Engineer  Machine Learning  Bangalore (3+ years of experience)|
Tavish Srivastava
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"After using Azure ML last week, I received multiple emails to publish a tutorial on Amazons ML. Thankfully, some of my meetings got postponed and I got time to write this.Here is some more good news for you, I present you a tool which will make it even more simpler. It will just remove all the guess work you had to do with Azure ML in choosing model and splits. Obviously, I am talking about the Amazon ML tool. Unfortunately, this time you wont get a trial pack but have to create your account giving up your credit card information. However, the tool is free to use and your credit card information is used only in case you breach the free tier.In this article, Ive demonstrated a step by step tutorial to build machine learning model with Amazon. Ive also shared a video tutorial at the end of this article.Lets make our first machine learning model with Amazon ML tool.Amazon is known for enhanced user experience,timely innovation and developments.Just 4 days back, Amazon added afeature forRandom Data Splitting and Cross Validation. Now you cantrain and evaluate machine learning models based on random input data split. This will help you to avoid overfitting and produce more accurate evaluations.Last Month, Amazon enabled real time predictions feature which lets users to preview real time prediction before creating the application. This features requires no code. Its push a button to get started feature.Also Read : Amazon re:Invent 2015 ( Machine Learning Reinvented)Basically, Amazon charges you for 2 services:Data Analysis and Model Building Fees  It depends size of input data, number of variables, types of transformation and number of computation hours. For this, youll be charged $0.42 per hour.Prediction Fees  It can further be divided into Batch Predictions and Real Time Predictions. Batch Predictions are when your application obtain many predictions at once. In real time predictions, you can request predictions for immediate use viaweb, mobile or desktop applications. Batch Prediction costs $0.10 per 1000 predictions. Real Time Prediction costs $0.0001 per prediction.Lets get to work now!1. Once you sign in, Youll find this asthe main page (shown below). Now, select Machine Learning models to moveto the first page of ML tool.5. Now press continue and click Review. In the final tab, youll find a summary of all inputs. Below is a sample:6. Finally, you press Finish and you are done.To check the results, go to Dashboard.In the dashboard, you canfindall type of objects created. Here are some key checks you can do:1. Check the data type : On clicking the ID of Banking.csv, you will find a dashboard to browse through the data.2.Now, Click Target Visualization. Youll findthe distribution of each column. For instance, following is the distribution of the target variable (y):3. Check Performance Metrics : To check performance metrics,click ID of Evaluation type. Below is the dashboard you get:4. As you can see, our model has a AUC of 0.94 . Also, this toolgives you an option to adjust score threshold. This is a very interesting simulation to witness the trade off between false positive and true positive. Here is an instance :In this chart, you can move the threshold score which gives you % correct and % error. The grey line is for distribution of 0s and black line is the distribution of 1s. The shaded portions represent type 1 and type 2 errors depending on which side of the cut off line the area falls. You also have a tool kit which is called the advanced metrics. These are other levers which can beadjusted to simulate the same graph. Here is a snapshot of this tool kit :Additional Resource: You may also be interested in this 53mins tutorial delivered at AWS re:invent 2015:Amazon ML tool is a really good tool for visualisation of data and results. The time which the tool take is slightly on the higher side when I compare it with H2O or other similar tool kits. However, the entire process is exceptionally simple to execute.In this article, Ive demonstrated a step by step process to build a machine learning model using Amazon ML service. As you have seen, its quite simple and codeless process. So, people who find coding to be intimidating should use such services often.Did you find this article helpful ? Share with us your experience with Amazon Machine Learning tool.",https://www.analyticsvidhya.com/blog/2015/12/tutorial-build-powerful-machine-learning-model-amazon-ml-service/
"Software Engineer  Machine Learning  Bangalore (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Learn to Build Powerful Machine Learning Models with Amazon Service|Senior Java Developer  Bangalore (4+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Software Engineer  Machine LearningLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSoftware Engineer  Machine Learning  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/12/software-engineer-machine-learning-bangalore-3-years-experience/
"Senior Java Developer  Bangalore (4+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Software Engineer  Machine Learning  Bangalore (3+ years of experience)|Abinitio  Lead/Architect/Designer  Bangalore / Gurgaon (10+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Java DeveloperLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredDesired:Additional QualificationThe role is of an individual contributor. Youll be expected to code at least 80% of the time. Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Senior Java Developer  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/12/big-data-engineer-bangalore-3-years-experience/
"Abinitio  Lead/Architect/Designer  Bangalore / Gurgaon (10+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Java Developer  Bangalore (4+ years of experience)|Abinitio  Sr Architect  Bangalore / Gurgaon (12+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Abinitio Lead/Architect/DesignerLocation  Bangalore / GurgaonAbout employer  ConfidentialRoleResponsibilitiesQualification and Skills RequiredJob Requirements (Technical + Managerial skills required)Educational qualificationExperience LevelSkills Required Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asAbinitio  Lead/Architect/Designer  Bangalore / Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/12/abinitio-leadarchitectdesigner-bangalore-gurgaon-10-years-experience/
"Abinitio  Sr Architect  Bangalore / Gurgaon (12+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Abinitio  Lead/Architect/Designer  Bangalore / Gurgaon (10+ years of experience)|Important Job Roles in Data Science Industry Today  Who Does What ?|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Abinitio  Sr ArchitectLocation  Bangalore / GurgaonAbout employer  ConfidentialRoleResponsibilitiesQualification and Skills RequiredJob Requirements (Technical + Managerial skills required)Educational qualificationExperience LevelSkills Required Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asAbinitio  Sr Architect  Bangalore / Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/12/abinitio-sr-architect-bangalore-gurgaon-12-years-experience/
Important Job Roles in Data Science Industry Today  Who Does What ?,Learn everything about Analytics|Introduction|The problem|Description of various data science roles:,"The Data Scientist|The Data Analyst|The Data Architect|The Statistician|The Database Administrator|The Business Analyst|Data and Analytics Manager|Author:||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Abinitio  Sr Architect  Bangalore / Gurgaon (12+ years of experience)|Senior Business Analyst  Bangalore (2-3 years of experience)|
Guest Blog
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One evening, I was catching up with a friend over a few drinks  lets call him Jon (name changed). He seemed determined to become a data scientist and was charting out his career plan accordingly.I quizzed him around his awareness of what a data scientist does and sniffed that he wasnt sure. So I threw this puzzle to him:There are 4 people A, B, C and D, each with one of the these designations: A Data Scientist, A Data Engineer, A Data Analyst and a Data Architect. I also had job descriptions of their roles, which I passed on to Jon. He had to match the description to the right designation.To penalize him for his errors, he had to take an alcohol shot for every wrong match he did. Can you guess the number of shots he took that evening? How many would you end up taking, if you were in this situation?I would just say that he was lucky that I didnt throw this puzzle at him with all the 8 designations I had JDs of that evening.While the demand for various data science roles is increasing by the day, people in industry have used the designations and descriptions a bit loosely. Hence, there is a lot of confusion around who does what in the industry!We hope thisinfographic The Data Science Industry: Who Does What designed by DataCamp helpsyou to find out ideal job role. We have provided a brief description of these roles and an infographic for you to take away.If you have any questions related to designations and roles, feel free to ask them through comments below. I will be as helpful as my time can permit. Enjoy!Data Scientist is most likely one of the most sizzling job titles that you can have these days, and with a yearly average salary of $118,70, they are amongst the top earners in the data science industry. The data scientist has the ability to handle the crude data using the latest technologies and techniques, can perform the necessary analysis, and can present the acquired knowledge to his associates in an informative way. Languages like R, Python and SQL are part of the data analysts basic knowledge. Much like the data scientist role, a broad skillset is also required for the data analyst role, which combines technical and analytical knowledge with ingenuity. This profile is often looked for by companies such as HP and IBM.Data is (being collected) everywhere and as a consequence, more and more organizations are in need of a data architect. Industries like banking and FMCG use data architects to integrate, centralize, protect and maintain their data sources. These architects often work with the latest technologies such as Spark and always need to be on top of the game to stay relevant. The title of statistician is regularly overlooked by or replaced by fancier sounding job titles. This is a bit of a pity, given that the statistician, with his solid foundation in statistical theories and methodologies, can be seen as the pioneer of the data science field. It is often he who reaps the information from the data and transforms it into actionable insights. To end, although the title can sound a bit dull compared to the others in this list, modern statisticians are always ready to rapidly ace new advancements and utilize these to benefit their research.As a database administrator, you ensure that the database is accessible to every stakeholder in the organizations, is performing legitimately and that the necessary safety measures are in place to keep the stored data save. You need to master different technologies going from SQL and XML up to a more general programming language like Java. This is probably the least technical profile mentioned on the infographic. However, the business analyst compensates for this lack of technical knowhow with a profound understanding of the various business processes that are in place. A business analyst therefore often performs the role of the middle person between the business folks and the techies. Organizations searching for business analysts are companies like Uber, Dell and Oracle.The data and analytics manager steers the direction of the data science team. This individual consolidates strong and specialized skills in a various arrangement of advancements (SQL, R, SAS,  ) with the social aptitudes required to deal with a group. Its a hard employment but if you feel up for the challenge, make sure to have a look at offerings from companies such as Coursera, Slack. Luckily, with a yearly average salary of $116k, the financial compensation is in line with the high requirements.Martijn Theuwissen is co-founder of DataCamp, which helps people learn R and Data Science from theirbrowser and their comfort through fun videos and interactive coding challenges.",https://www.analyticsvidhya.com/blog/2015/12/job-roles-data-science-industry-who-what/
"Senior Business Analyst  Bangalore (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Important Job Roles in Data Science Industry Today  Who Does What ?|Manager  Bangalore (6+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Business AnalystLocation  BangaloreAbout employer  ConfidentialDescriptionStarting with understanding the business problem, the SBA works with the Consultants & Managers to create the solution framework, followed by the analysis framework. This is followed by the BA managing his part of the work.The SBA is deeply knowledgeable of different analytics techniques and can critique among different techniques to come up with the best fit one. This entails the SBA spending a good amount of time researching and learning about new tools and techniques not only from his seniors in the company but also through great deal of self-learning. The SBA also needs to understand the client business and his domain quite well to be able to design meaningful solutions.The last but very important aspect of an SBAs work is to be able to manage a part of the work in global delivery model, ensuring smooth communication across borders.ResponsibilitiesAdditional Responsibilities at client siteQualification and Skills RequiredTechnical CapabilitiesOthersInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSenior Business Analyst  Bangalore and the following details:",https://www.analyticsvidhya.com/blog/2015/12/senior-business-analyst-bangalore-2-3-years-experience/
"Manager  Bangalore (6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst  Bangalore (2-3 years of experience)|Consultant  Bangalore (4 to 6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ManagerLocation  BangaloreAbout employer  ConfidentialDescriptionTo coordinate and manage the data collection, implementation and deployment of quantitative analytical applications/frameworks to enhance the quality of clients management information for business decision  making. The Manager (Management Analytics) will use industry-standard project management techniques to meet deadlines, budget constraints and technical requirements. The Manager will also be required to work on growing the business Affine is getting from the Client assigned to this person.The Manager (Management Analytics) is required to have technical expertise in basic statistical (i.e., quantitative) analytical techniques and business knowledge in different industries. In addition, they must guide and mentor Consultants, Business Analysts and Analysts in different but engagement-related disciplines. This person will need to draw on complex quantitative analysis expertise based on education and business experience in the following areas: critical reasoning, team management, attention to detail, pre-submission quality assurance of project report, and organization of client meetings.ResponsibilitiesProject Definition and Management: Business Development: Qualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asManager  Bangalore and the following details:",https://www.analyticsvidhya.com/blog/2015/12/manager-bangalore-6-years-experience/
"Consultant  Bangalore (4 to 6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Bangalore (6+ years of experience)|Tutorial  Getting Started with GraphLab For Machine Learning in Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ConsultantLocation  BangaloreAbout employer  ConfidentialDescriptionTo coordinate and manage the implementation and deployment of analytical applications/frameworks, while using project management techniques in order to meet deadlines, budget constraints and technical requirements.Consultants are required to have technical expertise in basic statistical analytical techniques and business knowledge in different industries. Also, they should be able to guide and mentor the analysts. Consultants holding bachelors degree are more likely to bring the following skill sets of critical reasoning, team management, and attention to detail, quality assurance and organizing client meetings. They are also required to be inquisitive and persuasive for effective project delivery.ResponsibilitiesProject Definition and Management: Qualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asConsultant  Bangalore and the following details:",https://www.analyticsvidhya.com/blog/2015/12/consultant-bangalore-4-6-years-experience/
Tutorial  Getting Started with GraphLab For Machine Learning in Python,Learn everything about Analytics|Introduction|Topics Covered|How it all started ?|What is GraphLab?|What are the Benefits of using GraphLab ?|How to Install GraphLab?|Getting started with Graphlab|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant  Bangalore (4 to 6 years of experience)|18 Useful Mobile Apps for Data Scientist / Data Analysts|
Sunil Ray
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GraphLab came as an unexpected breakthrough on my learning plan. After all,  Good Things Happen When You Expect Them Least To Happen. It all started with the end of Black Friday Data Hack. Out of 1200 participants, we got our winners and their interesting solutions.I read and analyzed them. I realized that I had missedon an incredible machine learning tool. A quick exploration told me that this tool has immense potential to reduce our machine learning modeling pains. So, I decided to explore it further. I now have dedicated a few days to understand its science and logical methods of usage. To my surprise, it wasnt difficult to understand.Were you trying to improve your Machine Learning model ? But failed mostly? Try this advanced machine learning tool. A month trial is free and 1 year subscription is available for FREE for academic use. Then, you can purchase subscription for following years.To get you started quickly, here is a beginners guide on GraphLab in Python. For ease of understanding, Ive tried to explain these concepts in simplest possible manner.GraphLab has an interesting story of its inception. Let me tell you in brief.GraphLab, known as Dato is founded by Carlos Guestrin. Carlos holds Ph.D in Computer Science from Stanford University. It happened around 7 years back. Carlos was a professor at Carnegie Mellon University. Two of his students were working on large scale distributed machine learning algorithms. They ran their model on top of Hadoop and found it took quite long to compute. Situations didnt even improve after using MPI (high performance computing library).So, they decided to build a system to write more papers quickly. With this, GraphLab came into existence.P.S  GraphLab Create is a commercial software by GraphLab.GraphLab Create isaccessed in python usinggraphlab library. Hence, in this article, GraphLab connotes GraphLab Create. Dont get confused.GraphLab is a new parallel framework for machine learning written in C++. It is an open source projectandhas been designed considering the scale, variety and complexity of real world data. It incorporates various high level algorithms such as Stochastic Gradient Descent (SGD), Gradient Descent & Locking to deliver high performance experience. It helps data scientists and developers easily create and install applications at large scale.But, what makes it amazing? Its the presence of neat libraries for data transformation, manipulation and model visualization. In addition, it comprises of scalable machine learning toolkits which has everything (almost) required to improve machine learning models. The toolkit includes implementation for deep learning, factor machines, topic modeling, clustering, nearest neighbors and more.Here is the complete architecture of GraphLab Create.There are multiple benefits of using GraphLab as described below:You can also use GraphLab once you have availed its license. However,you can also get started with free trialor the academic edition with 1 year subscription. So, prior toinstallation, your machinemustfulfill the system requirement to run GraphLab.System Requirement for GraphLab:If your system fails tomeet above requirement, you can use GraphLab Create on the AWS Free Tieralso.Steps for Installation:After youve installedGraphLab successfully, you can access it using import <library_name>.Here, Ill demonstrate the use ofGraphLab by solvinga data science challenge. I have the taken data setfrom Black Friday Data Hack.Now, look at the pre and post visualization of variable Age.
For more details on Data Manipulation using GraphLab, please refer this link.Finally, you can take this input variable to original data set.Similarly, you can apply other feature engineering operations to the data set based on your requirement. You can refer this link for more details.In Black Friday challenge, we are requiredpredict the numeric quantities Purchase i.e. we need a regression model to predict the Purchase.In GraphLab, we have three type of regression models:
A) Linear Regression
B) Random Forest Regression
C) Gradient Boosted Regression
If you have any confusion in algorithm selection, GraphLab takes care of that. Dont worry. It selects the right regression model automatically.Output:
To know more about other modeling techniques like clustering, classification, recommendation system, Text analysis, Graph Analysis, Recommendation Systems you can refer thislink. Alternatively, here is the complete user guide by Dato.In this article, we learnt about GraphLab Create which helps to handle large data set while building machine learning models. We also looked at the data structure of Graphlabwhichenables it to handle large data set like SFrame and SGraph. Id recommend you to use GraphLab. Youd love itsautomated features likedata exploration (Canvas, interactive web data exploration tool), feature engineering, selecting the right models and deployment.For better understanding, Ive alsodemonstrateda modeling exercise using GraphLab. In my next article on GraphLab, I will focus on Graph Analysis and Recommendation System.
Did you find this article helpful ? Share with us your experience with GraphLab.",https://www.analyticsvidhya.com/blog/2015/12/started-graphlab-python/
18 Useful Mobile Apps for Data Scientist / Data Analysts,Learn everything about Analytics|Introduction|Mobile Applications|End Notes,"|Brain Training||Programming / Data Analysis Tools|Statistics & Mathematics|MOOCs|Bonus|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Tutorial  Getting Started with GraphLab For Machine Learning in Python|Tutorial  Build a simple Machine Learning Model using AzureML|
Analytics Vidhya Content Team
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Does your passion lie in Data Science /Analytics?Currently, data science and machine learning are changing the world. Heres your chance to live your passion. To become better at what you do, you no longer need to stick around your laptop for long hours. Take a break and switch to faster way of learning. Switch to Mobile Apps.Did you know you can run Python in your phone?You heard right. Mobile apps have added immense excitement to our ways of learning. The subjects which were considered difficult to understand, are now taught using pictures and stories in your mobile / tablets. You can access them anywhere, whether you are in a bus, car, train, restaurant or anywhere else. All you need is a earphone to plug in (if necessary).In this article, Ive shared some useful apps (I found)which can improve your necessarydata science / analytics skills. These apps can improve your listening skills, logical skills, decision making skills, mathematical skills, statistical skills and much more. They are much more powerful than one could imagine.Ive groupedthese mobile apps in various categories. I presume, youknow your weak areas, so this would help you to target those sweet spots. These are android apps available for FREE at google playstore.Note: We do not intend to promote any mobile app using this article. These apps have been selected on the basis of their ratings and performance.Elevate(Downloads  1 million, Rating  4.5, Size  38.80 MB)It is a personalized brain training program designed to improve your cognitive skills. Your brain training sessioninclude 3 exercises every day. The exercises are selected on your past performance. If you are found to be weak in certain aspect, you can expect those exercises to repeat frequently. This app has various training programs butare limited in FREE version. Still, it succeeds in providing an exhilarating experience.Lumosity(Downloads  10 million, Rating  4.1, Size  49.71 MB)Lumosity is a personalized brain training program and has over 40 fun games. These games are intuitive enough to challenge yourcorethinking. This app will help you to improve reading, writing, logical and mathematical skills. The personalized training programs are really interesting and addictive. You get 3 exercises for brain training everyday. Advanced exercises are unlocked, which can be availed with payment.Neuro Nation(Downloads  5million, Rating  4.5, Size  18.32 MB)This is a fitness app for your brain. This canhelp you with improvement in memory, concentration, logical thinking and intelligence. It comprises of various set of exercises, creatively designed more than 60 programs. It also allows you challenge your peers and measure your weekly performance. Youll get limited programs in unpaid version. Still, they are worth trying. If you sincerely use these apps, for few minutes everyday, it can bring a huge change in your life.Math Workout (Downloads  5 million , Rating  4.2, Size  2.71 MB)Do you want to become good with numbers? This app would help you with that. You should now feel comfortable doingnumerical calculations of any type. This skill helps a lot in all stages of lives. In short, you must develop your mental math skills. Train your mind such that it can do numerical computation on finger tips. This is a beginner level app. It has various interesting exercises to help you acquire mathematical intuition.Math Tricks(Downloads  5 million, Rating  4.4, Size  6.90 MB)This app is for advanced users. Id recommend this app for every person aspiring to become a data scientist. You need to be smart with maths. Here youll find useful tricks for speeding upmathematical calculations. It comprises of tricks for square number, percentage, division, addition, exponent and much more. For every trick, it has 15 levels. You can also compete with your friends in multiplayer mode. Your performance statistics are also tracked.QPython(Downloads  500k, Rating  4.4, Size  13.17 MB)This app allows you to run python on your mobile. It assists your android device to run python scripts and projects. Having been highly appreciated on play store, this app works best on python 2.7. It consists ofPython interpreter, console, editor, and the SL4A Library for Android. It also includes many useful python libraries. It can also execute python code & files from QR codes. Thus, you are no longer limited to run python by your machines.Learn Python(Downloads  10k, Rating  4.7, Size  4.77 MB)You are no longer requiredto stick to your machines to learn Python. Heres apython tutorial for your android device. This tutorial covers python basics, data types, control structures, function and modules, exceptions, working with files, functional programming, object oriented programming and much more. For an enhanced learning experience, this tutorial includes fill in blanks, true false, re-arrange and question-answers. Its a great resources for beginners interested in python.R Programming(Downloads  10k, Rating  3.8, Size  561 KB)Just like Python, you can learn R too on your android devices. This app introduces you to basics of R Programming. Consider it as a shorted version of swirl in R. Id recommend this app for complete beginners. It consists of Vectors, Functions, Matrices, Factors, Data Frames, Lists and much more. After completing this tutorial, youd become ready to undertake your first data analysis.Excel Tutorial (Downloads  100k, Rating  4.2, Size  9.05 MB)Not just R or Python, you can also learn excel on your android device. This is one of the most recommended excel tutorial on playstore. This app comprises of tutorials covering wide range of excel topics from basic to advanced levels. It cover topics such as sorting, filtering, pivot table, keyboard shortcuts, whatif analysis etc. You better start managing your time and choose to invest inthese apps. Surely, youll experience a significant improvement in your knowledge.Termux(Downloads  10k, Rating  4.8, Size  134KB )Termux offers a powerful interface with an extensive Linux package collection. This app allows you to run text based games with frotz, projects with git and subversion, use python console, access servers, edit files with nano and vim and much more on your android devices. You can also install desired packages using an in-built apt package manager known from the Debian and Ubuntu Linux distributions.Basic Statistics(Downloads  50k, Rating  4.1, Size  2.72 MB)This app is for beginners in data science / analytics. Consider this app as your refresher on various statistical measures. It comprises of topics such as frequency distribution and graphs, data description, probability, distributions, estimations, hypothesis testing and much more. If you are preparing to takeanexam, this can be an ideal tutorial for you.Probability Distributions(Downloads  50k, Rating  4.5, Size  3.21 MB)After you have acquired basic knowledge of statistics, this app would make more sense to you. This app allows you to undertake many probabilistic functions in your android devices. It requires prior knowledge of probability distributions such as binomial. It allows you to compute probability mass function for poisson, hypergeometric and plot functions for gaussian, t test, chi-square, log-normal distributions and much more.Statistics and Sample Size(Downloads  10k, Rating  4.4, Size  3.43 MB)It is thestatistical calculator for your android devices. This help can help you calculate various statistical metrics such as sample size, statistical distribution table, statistical analysis and much more. Using this app, you can save your time in doing calculations. It does in seconds. It is a handy tool for basic and well as scientific statistics. This app supports only .csv files. But, I think that should suffice with our data analysis requirements.Khan Academy(Downloads  100k, Rating  4.5, Size  21.48 MB)You can watch khan academy videos on your android devices as well. As an aspiring data scientist, you must look for linear algebra, probability and statistics course on khan academy. This should help you to get prepared for rigid mathematics calculations in machine learning algorithms. This has a redesigned interface which delivers an enhanced user experience. You can also sync your khanacademy.org progress which this app.You must have well browsed their (MOOCs) website to undertake some data science / analytics course. A lot of you wouldnt know that you can continue your learning on your android devices as well. Below are some useful apps of popular open courses:-Udacity(Downloads  1 million, Rating  4.2, Size  5.40 MB)Udacity offers a wide range of courses. Now, you no longer need to wait for laptop accessibility. You are great if you have an android device. You can simply undertake and complete courses in your mobile as well. There is no difference in learning on their website vs app. Their apps user interface is nice and easy to use.Coursera(Downloads  5 million, Rating  4.3, Size  16.80 MB)Millions of students areacquiringnew skills from Coursera. To improve their learning experience, coursera also hasensured their android presence. Using this app, you can get timely notifications directly in your android device. With this app, you can learn anytime, anywhere without waiting for the best time to study. You would find all sort of features which coursera has enabled on their website.edX(Downloads  500k, Rating  4.2, Size  6.07 MB)If you are edX subscriber, you would like to download this app and continue your learning experience on your android device as well.This app allows you to download course videos, receive instant notifications, stream class videos via Wifi or cellular connection and much more. Like others, edX has made learning much more accessible and enjoyable.Udemy (Download  5million, Rating  4.3, Size  27.47 MB)With 5 million downloads, Udemy is continuously innovating users learning experience. With over 32000 courses and tutorials, you can learn almost anything. But, only if you are curious. People have appreciated its content and experience on play store. This app allows you to download lectures and tutorials on your android device.Analytics Vidhya (Download  10k, Rating  4.4, Size  6.5 MB)This is Analytics Vidhyas own mobile application. We are including it in this list as we feel this will be super useful to any data scientist who uses it. It has all the blog posts, including AVBytes, that we publish on AV. You can also save articles to read later. Its an ideal companion for your data science journey.It might difficult to download all these 18 apps in your android devices. Device memory posses untimely challenges sometime. I understand. Hence, you must consider your areas of improvement and accordingly work towards it. To reap maximum benefits from these apps, you must follow a disciplined schedule. Dedicate few minutes of daily life in these training programs tolearn useful things everyday.Did you find this article useful? Is there any other android app useful for data scientists ? Share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/12/18-mobile-apps-data-scientist-data-analysts/
Tutorial  Build a simple Machine Learning Model using AzureML,Learn everything about Analytics|Introduction|What is AzureML?|What are the resources available on AzureML?|Building a Model|Visualizing the Data Set and Output|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|18 Useful Mobile Apps for Data Scientist / Data Analysts|8 Ways to deal with Continuous Variables in Predictive Modeling|
Tavish Srivastava
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How difficult is it to build a machine learning model on R or Python?For beginners, its a Herculean task. For intermediates and experts, its just a matter of system capacity, problem understanding and a littletime. Machine Learning models sometime face the issue of system incompatibility. Specially, when the data set is huge. In such cases, either themodeltakes longer to compute or thesystem crashes. Hence, for beginners and experts, the use of machine learning offers untimelychallenges as well.The good news is, machine learning has become a lot easier is last few years. As a beginner in machine learning,you cankick start your machine learning journey withMicrosoft AzureML.In this article, Ill impartthe necessary information to get you started with Machine Learning. Also, Ive demonstrated a step by step tutorial to create a machine learning model using this software.The speed of computation on Microsoft AzureML is comparable to R or Python. Hence, Id sayits worth trying for experts also.AzureMLis a GUI implementation of machine learning algorithm by Microsoft. Using this tool, implementation of such algorithm becomes exceptionally easy. If you are versed with E-Miner, than understanding this tool wouldnt be difficult. I found this tool more resourceful and more graphical than E-Miner.Lets talk about various resources available with this tool.Now, you know the potential of AzureML. Lets now focus on the ways of using them. I will take up an easy to understand exampleto demonstrate the same. Id suggest you to practice these steps with me to get a better understanding of this tutorial.This is where you start  (By clicking on create a new experiment)You get an empty experiment table :Now you can choose a pallete:Step 1: Choose the Data set This can be your sample data or you can upload also. In this tutorial,Ill useBreast Cancer data from the in-built data sets. Just drag and drop this data in the main window.Step 2: Choose a sampling tool: You can use search option from pallette to find the split data option. Place this below your data set and join.You now see two touch points at the split data node. This basically means that you have two data sets ready to be taken forward. Towards theright side, you have the freedom to choose the type of split.Step 3 : Train a machine learning model : You will need two nodes for this step. Firstly, it will be the type of model you want to build. Secondly, it will be the train model node. You can refer to the following figure :You can still noticeanexclamation mark in the train model node. It suggests thatyou need to specify the target variable. Lets choose the target variable by clicking the mark. Now, you would see a window on right side. Now choose Launch column slector.Here I have chosen Class as the target variable.Step 4: Now you score: Refer to the following figureStep 5: Finally evaluateAnd RUN the model!To visualize any node, you simply go to node, pressright click, then click visualize.Here is how visual data looks like in our case :As you can see, Class variable only has two values as expected. This tool neatly draws distribution for each variable and allows you tocheck normality as well.Here is how the scored model looks like :As clearly visible, the estimated probabilities are mostly near zero and one. The cumulative distribution stays almost flat in between. Hence, the model outputshighly segregated values.Finally, here is how the evaluation graphs look like :As you can see, the model was highly efficient and it took me less than a minute to build and execute. The evaluation matrices computed is quite exhaustive and probably has the number you were looking for. I loved the tool because of the time efficiency and the user ease it provides.Did you find this article helpful ? Share with us your experiencewith Azure ML. Id love to hear you.",https://www.analyticsvidhya.com/blog/2015/11/build-simple-machine-learning-model-azureml/
8 Ways to deal with Continuous Variables in Predictive Modeling,Learn everything about Analytics|Introduction|What are Continuous Variables?|How to handle Continuous Variables?|Methods to dealwith Continuous Variables|Methods to work withDate & Time Variable|End Notes,"Binning The Variable:|Normalization:|Transformations for Skewed Distribution:|Use of Business Logic:|New Features:|Treating Outliers:|Principal Component Analysis:|Factor Analysis:|Create New Variables:|Create Bins:|Convert Date to Numbers:|Basics of Date Time in R|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Tutorial  Build a simple Machine Learning Model using AzureML|Data Scientist (3  8 years experience)  Delhi/NCR|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lets come straight to the point on this one  there are only 2 types of variables you see  Continuous and Discrete. Further, discrete variables can divided into Nominal (categorical) and Ordinal. We did a post on how to handle categorical variableslast week, so you wouldexpect a similar post on continuous variable. Yes, you are right  In this article,we will explain all possible ways for a beginner to handle continuous variables while doing machine learning or statistical modeling.But, before we actually start, first things first.Simply put, if a variable can take any value between its minimum and maximum value, then it is called a continuous variable. By nature, a lot of things we deal with fall in this category: age, weight, height being some of them.Just to make sure the difference is clear, let me ask you to classify whether a variable is continuous or categorical:Please write your answers in comments below.While continuous variables are easy to relate to  that is how nature is in some ways. They are usually more difficult from predictive modeling point of view. Why do I say so? It is because the possible number of ways in which they can be handled.For example, if I ask you to analyze sports penetration by gender, it is an easy exercise. You can look at percentage of males and females playing sports and see if there is any difference. Now, what if I ask you to analyze sports penetration by age? How many possible ways can you think to analyze this  by creating bins / intervals, plotting, transforming and the list goes on!Hence, handling continuous variable in usually a more informed and difficult choice. Hence, this article should be extremely useful to beginners.Binning refers todividing a list of continuous variables into groups. It is done to discover set of patterns in continuous variables, which are difficult to analyze otherwise. Also, bins are easy to analyze and interpret.But, it alsoleads to loss of information and loss of power. Once the bins are created, the information gets compressed into groups which later affects the final model. Hence, it is advisable to create small bins initially.This wouldhelp in minimal loss of information and produces better results. However, Ive encountered cases where small bins doesnt prove to be helpful. In such cases, you must decide for bin size according to your hypothesis.We should consider distribution of data prior to deciding bin size.For example: Lets take up the inbuilt data set state.x77 in R to create bins:In simpler words, it is a process of comparing variables at a neutral or standard scale. It helpsto obtain same range of values. Normally distributed data is easy to read and interpret. As shown below, in a normally distributed data, 99.7% of the observations lie within 3 standard deviations from the mean.Also, the mean is zero and standard deviation is one. Normalization technique is commonly used in algorithms such as k-means, clustering etc.A commonly used normalization method is z-scores. Z score of an observation is the number of standard deviations it falls above or below the mean. Its formula is shown below.x = observation, = mean (population), = standard deviation (population)For example: Randy scored 76 in maths test. Katie score 86 in science test. Maths test has (mean = 70, sd = 2). Science test has (mean = 80, sd = 3). Who scored better? You cant say Katie is better since her score is much higher than mean. Since, both values are at different scales, well normalize these value at z scale and evaluate their performance.z(Randy) = (76  70)/2 = 3z(Katie) = (86  80)/3 = 2Interpretation:Hence, we infer than Randy scored better than Katie. Because, his score is 3 standard deviations away from the class mean whereas Katies score is just 2 standard deviations away from mean.Transformation is required when we encounter highly skewed data. It is suggested not to work on skewed data in its raw form. Because, it reduces the impact of low frequency values which could be equally significant.At times, skewness is influenced by presence of outliers. Hence, we need to be careful while usingthis approach. The technique to deal with outliers is explained innextsections.There are varioustypes of transformation methods. Some are Log, sqrt, exp, Box-cox, power etc. The commonly used method is Log Transformation. Lets understand this using an example.For example: Ivescore of 22 students. I plot their scores and find out that distribution is left skewed. To reduce skewness, I takelog transformation (shown below). As you can see after transformation, thedata is no longer skewed and isready for further treatment.Business Logic adds precision to output of amodel. Data alone cant suggest you patterns which understanding its business can. Hence, in companies, data scientists often preferto spend time with clients and understand their business and market. This not only helps them to make an informed decision. But, also enables them to think outside the data. Once you start thinking, you are no longer confined within data.For example: You work on a data set from Airlines Industry. You must find out the trends, behavior and other parameters prior to data modeling.Once you havegotthe business logic, you are ready to make smart moves. Many a times, data scientists confine themselves within the data provided. They fail to think differently. They fail to analyze the hidden patterns in data and create new variables. But, you mustpractice this move. You wouldnt be able to create new features, unless youve explored the data to depths.This method helps usto add more relevant information to our final model. Hence, we obtain increase in accuracy.For example:I have a data set where youve following variables: Age, Sex, Height, Weight, Area, Blood Group, Date of Birth. Here we can make use of our domain knowledge. We know that (Height*Weight) can give us BMI Index. Hence, well create HW = (Height*Weight) as a new variable. HW is nothing but BMI (Body Mass Index). Similarly, you can think of new variables in your data set.Data are prone to outliers. Outlier is an abnormal value which stands apart from rest of data points. It can happen due to various reasons. Most common reason includechallengesarising in data collection methods. Sometime the respondents deliberately provide incorrect answers; or the values are actuallyreal. Then, how do we decide? You can any of these methods:Treating outliers is a tricky situation  one where you need to combine business understanding and understanding of data. For example, if you are dealing with age of people and you see a value age = 200 (in years), the error is most likely happening because the data was collected incorrectly, or the person has entered age in months. Depending on what you think is likely, you would either remove (in case one) or replace by 200/12 years.Sometime data set hastoo many variables. May be, 100, 200 variables or even more. In such cases, you cant build a model on all variables. Reason being, 1) It would be time consuming. 2) It might have lots of noise 3) A lot of variables will tell similar informationHence, to avoid such situation we use PCA a.k.a Principal Component Analysis. It is nothing but, finding out few principal variables which explain significant amount of variation in dependent variable. Using this technique, a large number of variables are reduced to few significant variables. This technique helps to reduce noise, redundancy and enables quick computations.In PCA, components are represented by PC1 or Comp 1, PC2 or Comp 2.. and so on. Here, PC1 will have highest variance followed by PC2, PC3 and so on. Our motive should be to select components witheigen values greater than 1. Eigen values are represented by Standard Deviation. Let check this out in R below:Factor Analysis was invented by Charles Spearman (1904). This is a variablereduction technique. It is used to determine factor structure or model. It also explains the maximum amount of variance in the model.Lets say some variables are highly correlated. These variables can be grouped by their correlations i.e. all variables in a particular group can be highly correlated among themselves but have low correlation with variables of other group(s). Here each group represents a single underlying construct or factor. Factor analysis is of two types:Lets do exploratory analysis in R. As we run PCA previously, we inferred that Comp 1, Comp 2 and Comp 3. Weve now identified the components. Below is the code for EFA:Note: VARIMAX rotation involves shift in coordinateswhich maximizes the sum of the variances of the squared loadings. It rotates the alignment of coordinates orthogonally.Presence of Data Time variable in a data set usually give lots of confidence. Seriously! It does. Because, in data-time variable, you get lots of scope to practice the techniques learnt above. You can create bins, you can create new features, convert its type etc. Date & Time is commonlyfound in this format:DD-MM-YYY HH:SS or MM-DD-YYY HH:SSConsidering this format, lets quickly glance through the techniques you can undertake while dealing with data-time variables:Have a look at the date format above. Im sure you can easily figure out the possible new variables. If you have still not figure out, no problem. Let me tell you. We can easily break the format in different variables namely:Ive listed down the possibilities. You arent required to create all the listed variables in every situation. Create only those variables which only sync with your hypothesis. Every variable would have an impact( high / low) on dependent variable. You can check it using correlation matrix.Once you have extracted new variables, you can now create bins. For example: Youve Months variable. You can easily create bins to obtain quarter, half-yearly variables. In Days, you can create bins to obtain weekdays. Similarly, youll have to explore with these variables. Try and Repeat. Who knows, you might find a variable of highest importance.You can also convert date to numbers and use them as numerical variables. This will allow you to analyze dates using various statistical techniques such as correlation. This would be difficult to undertake otherwise. On the basis of their response to dependent variable, you can then create their bins and capture another important trend in data.There are three good options for date-time data types: built-in POSIXt, chron package, lubridate package. POSIXt has two types, namelyPOSIXct and POSIXlt. ct can stand for calendar time and lt is local time.You cant explore data unless you are curious and patience. Some people are born with them. Some acquire them with experience. In anyway, the techniques listed above would help you to explore continuous variables at any level. Ive tried to keep the explanation simple. Ive also shared R codes. However, I havent shared their output. You can run these codes. Try to infer the findings.In this article, Ive shared 8 methods to deal with continuous variables. These include binning, variable creation, normalization, transformation, principal component analysis, factor analysis etc. Additionally, Ive also shared the techniques to deal with date time variables.Did you find this article helpful ? Did I miss out on any technique?Which is the best technique of all?Share your comments / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/8-ways-deal-continuous-variables-predictive-modeling/
"Data Scientist (3  8 years experience)  Delhi/NCR|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|8 Ways to deal with Continuous Variables in Predictive Modeling|Data Scientist (Research)  Delhi/NCR|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  Delhi/NCRAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredTop Line requirements across these roles include:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Delhi/NCR and the following details:,https://www.analyticsvidhya.com/blog/2015/11/data-scientist-3-8-years-experience-delhincr/
"Data Scientist (Research)  Delhi/NCR|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (3  8 years experience)  Delhi/NCR|Simple Methods to deal with Categorical Variables in Predictive Modeling|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist (Research)Location  Delhi/NCRAbout employer  ConfidentialDescriptionThe primary work area will be in the realm of statistics, hypotheses testing, correlations & pattern recognition. The goal of this data scientist will be to look at various situations/scenarios/problems and apply different models to data to understand which model works with maximum efficiency & accuracy. Post testing, these algorithms will be deployed on the platform as one or more of the following: automated reports, automated analytics, custom reports etc.This role needs a high understanding of the data science realm: what models are out there currently? What models can be applied to a particular scenario? What direction should reporting & analytics take in the future? Are there more efficient ways of doing certain things we are already doing?ResponsibilitiesQualification and Skills RequiredRequirements:Top Line requirements across these roles include:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist (Research)  Delhi/NCR and the following details:",https://www.analyticsvidhya.com/blog/2015/11/data-scientist-research-delhincr/
Simple Methods to deal with Categorical Variables in Predictive Modeling,Learn everything about Analytics|Introduction|What are the key challenges with categorical variable?|Provenmethods to deal with Categorical Variables|End Notes,"Convert to Number|Combine Levels|Dummy Coding|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist (Research)  Delhi/NCR|Secrets from winners of our best ever Data Hackathon!|
Sunil Ray
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Categorical variables are known to hide and mask lots of interesting information in a data set. Its crucial to learn the methods of dealing with such variables. If you wont, many a times, youd miss out on finding the most important variables in a model. It has happened with me. Initially,I used to focusmore on numerical variables. Hence, never actually got an accurate model. But, later I discoveredmy flaws and learnt the art of dealing with such variables.If you are a smart data scientist, youd hunt down the categorical variables in the data set, and dig out as much information as you can. Right?But if you are a beginner, you might not know the smart ways to tackle such situations.Dont worry. I am here to help you out.After receiving a lot of requests on this topic, I decided to write down aclear approach to help you improveyour models using categorical variables.Note: This article is best written for beginners and newly turned predictive modelers. If you are an expert, you are welcome to share some useful tipsof dealing with categorical variablesin the comments section below.Ive had nasty experiencedealing with categorical variables. I remember working on a data set, where it took me more than 2 days just to understand the science of categorical variables. Ive facedmany such instances where error messages didnt let me move forward. Even, my proven methodsdidnt improve the situation.But during this process, I learnt how to solve thesechallenges. Id like to share all the challenges I faced while dealing with categorical variables. Youd find:Here are some methods I used to dealwith categorical variable(s). A trick to get goodresult from these methods is Iterations. You must know that all these methods may not improve resultsin allscenarios, but we should iterate our modeling process withdifferent techniques. Later, evaluate the model performance. Below are the methods:In this article, we discussed the challenges you might face while dealing with categorical variable in modelling. We also discussed various methods to overcome those challenge and improve model performance. Ive used Python for demonstrationpurpose and kept the focus of article for beginners.In order to keep article simple and focused towards beginners, I have not described advanced methods like feature hashing. I will take it up as a separate article in itself in future.You must understand that these methods are subject to the data sets in question. Ive seen even the most powerful methods failing to bring model improvement. Whereas, a basic approach can do wonders. Hence, you must understand the validity of these models in context to your data set. If you still face any trouble, I shall help you out in comments section below.Did you find this article helpful ? Do you know ofother methods whichwork well withcategorical variables? Please share your thoughts in the comments section below. Id love to hear you.",https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/
Secrets from winners of our best ever Data Hackathon!,Learn everything about Analytics|Introduction|The preparations:|The Competition:||Problem Statement|WinningStrategies of this Competition|My learning and takeaways:|End Notes,"Evaluation Metric|Download Data Set|Rank 3  Sudalai Rajkumar a.k.a SRK (Used XGBoost in Python with some cool features)|Rank 2  Nalin Pasricha (Used Matrix Factorization using Graphlab in Python)|Rank 1  Jeeban Swain (Used Deep Learning and GBM in R)|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Simple Methods to deal with Categorical Variables in Predictive Modeling|Fraud Analyst  Gurgaon (6-8 years of experience)|
Kunal Jain
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the books I read in initial days of my career was titled What got you Here, Wont Get You There. While the book has been written quite a few years back, the learnings from the book still hold! In his book, Marshall Goldsmith explains that at times the habits which got us at our current place would hold us back from taking ourselves to the next level.I saw this live in action (once again) in the hackathon we hosted last weekend! Close to 1200 data scientists across the globeparticipated in Black Friday Data Hack. It was a 72 hours data science challengeandI am sure every participant from the competition walked awayas a winner  taking awaya ton of learning or earning from it!Even in such a short duration contest, all the toppers challenged themselves and took a path less traveled. And guess what, it paid off! Not only did they break away from other data scientists, but they were also able to come out with solutions which stood out from other solutions. More on this later!Like always, we decided to experiment withground rules in our hackathon. This time, we simplified the design (no surprises during the hackathon) and made sure that we put in extra efforts on making sure the dataset is good.Over the coming days multiple datasets were rejected, various problems were declared as Not Good Enough. Finally, we were able to get a dataset, which would have something for everyone. The beginners can perform basic exploration, while there was enough room for out of box thinking for advanced users.With close to 1,050 registrations and 3 days to go, we launched the competition on midnight of 19th  20th November. There was a positive feedback on the dataset with in an hour and people started taking stab at various approaches overnight.During the 3 days, we saw close to 2,500 submissions and 162 people making attempts at the leaderboard. The race to the top and top 10 was tough! People were constantly flowing in and flowing out of the race.The best part  there was constant learning for everyone in the process. Leaders happily shared the benchmark scripts. Newbies followed the cues and learnt the tricks of the trade.At the end, the winners stood out for what they had done over three days (approaches in detail below). Here were the top 3 finishers:Before I reveal their approaches, Id want to thank these people for their immense co-operation and time. I know that they participate in these hackathons for the learning and the community and have always been helpful whenever I have asked them for any help.Its great to connect with these people.Also Read: Exclusive Interview with SRK, Kaggle Rank 25A retail company ABC Private Limited wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products.The winner was judged on the basis of root mean squared error (RMSE).RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors.For practice purpose, you can download the data set from here. This link will remain active till 29th November 2015.Winners used techniques like Deep Learning, Recommender, Boosting to deliver a winning model with least RMSE. Not many have used such approaches. Hence, it is interesting to acknowledge their implementation and performance. Ive also shared their codes on our github profile.Below are the approaches, these winners used to secure their respective ranks:SRK says:Feature Engineering played a crucialrole for me in this challenge. Since, all the given variables were categorical in nature, I decided to begin with label encoding. So, I label encoded all the given input variables. Then, built a XGBoost model using these features. I didnt want it to end here. Therefore, in pursuit of improvement, I tried few other models as well (RandomForest, ExtraTrees etc). But, they failed toproducebetter results than GBM.Then, I decided to doFeature Engineering.Icreated two types of encoding for all the variables and added them to the original data set:
1. Count of each category in the training set
2. Mean response per category for each of the variables in training set.I learnt the second trick from Owen Zhang. Precisely, Slide no. 25 and 26 of his presentation on Tips for data science competitions. This turned out to be favorable for me.I tried it out and improved my score.Finally, it was time for my finishing move, Ensemble. I built a model model which with weighted average of 4XGB modelswhich had different subset of the above mentioned features as inputs.Link to CodeNalin says:I started as a R users. But, I was determined to learn Python. This challenge was a perfect place to test myself.So, Itook this challenge with Python. I knew I had a whole new world to explore,still I went ahead.It seemed, Python didnt turn out to be co-operative.I couldntinstall xgboost inPython. Then, I decided to find new ways.As a result, I ended up trying several things which were new to me: Python, SFrames and Recommenders.SFrames is similar to Pandas but is said to have some advantages over Pandas for large data sets. The only disadvantage is that, it is not so popular and well known. So, if you are stumped  Google doesnt help.Finally, I resorted to Recommenders. To get a recommender style solution, I used matrix factorization. The library I used was Graphlab.I used matrix factorization because, this algorithm configured the data as a matrix containing the target variable, with user ids and product ids as rows and columns. It then filled the blanks in this matrix using factorization. Additionally, it also consideredfeatures of the users or products such as gender, product category etc. to improve the recommendation. These are called side features in recommender jargon.I found the recommender model to be extremelypowerful, to an extent that I gotsecond rank with this algorithm.Surprisingly, with NOfeature engineering, minimal parameter tuning and cross-validation. Im certainly hooked and plan to experiment more with other libraries of recommender algorithms.I made 39 submissions in the contest. But, most of them were my failed attempts to get xgboost style results with linear models, random forests etc. Once I started using recommender models, my rank shot up with only a few high impact submissions.My final submission was an ensemble of three matrix factorization models, each with slightly different hyper-parameters.For those new to recommenders, I suggest you to take thisCourseras MOOC.To learn more about graphlab, SFrames, recommenders and machine learning in general,I suggest you to take this course by University of Washington on Coursera.Link to CodeJeeban says:Here is theapproach I used in this competition:Step 1:I considered all variables in the model by converting all of them into categorical features:Step 2: Outlier and Missing Value treatmentStep 3:Here is a tableof all the algorithms which I tried, with their cross-validation scores:Note:Even though I have tried 5 different algorithms to solve this problem, I selected only two algorithms for my final submission (Deep Learning and GBM). My final model was ensemble of 3 DL and 2 GBM with Weighted Average.Step 4: Here is my ensemble of 5 models:GBM ModelNote:I replaced all ve predicted values from all the model as there were no negative purchase values in training set but my models had some negative values.Feature SelectionI used only 3 variables for GBM model as model.as Product_ID,User_ID and Age were significant (shown below).GBM Model validation score in H2OGBM Model ParametersMost Important parameter in GBM in this case was nbin_cats=6000. This was basically for maximum number of levels in categorical variable. HereI have selected 6000 as User_ID andgot 5891 unique values (you can use 5891 as well but less than that will not give you good result).Even though I tuned my model on 70% of training sample and 30 % for validation, but for final submission I had rebuild the model on entire training set using these parameters. This has helped me toimprove the score by 20 points.Deep Learning ModelAll the variables considered for all Deep learning Model.My best score was from Deep Learning model with RMSE ~2433(Epochs=60 and layers=6) where I have created two more similar deep Learning model with hidden layers 6 and Epochs of (50, 90).Note:Layers:6 and Epochs:60 has the lowest RMSE of 2433Best Model Tuning ParametersI really enjoyed participating in this competition. The adrenaline rush to improve my model in 72 hours kept me going. I learnt one thing. Till the timeonedoesnt facechallenges in your life, it becomesextremely difficult to reach the next level. It was indeed a challenging opportunity.Link to CodeLike I said at the start, each of these winners did something which made them stand apart. They thought out of the box rather than just tuning parameters for a set of models and that helped them gain the ground. Here are a few things I would highlight as takeaways for myself and the participants:More than anything, you might have got huge inspiration from these people, their commitment and their unwillingness to give up. Thisisa great example for beginners trying to become successful in data science. Not many of us know to use Deep Learning and Graphlab in our modelsmodeling. Hence, you must start practicing these techniques in order to develop your data science skills.In this article, the winners of Black Friday Data Hack revealed their approaches which helped them to succeed in this challenge. The next challenge will be coming soon. Stay Tuned!Did you like reading this article? Tell me one thing you are taking away from here? Share your opinions, suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/secrets-from-data-hackathon/
"Fraud Analyst  Gurgaon (6-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Secrets from winners of our best ever Data Hackathon!|Collection Analyst  Gurgaon (6-10 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Fraud AnalystLocation  GurgaonAbout employer  ConfidentialDescriptionA fraud analyst is someone who investigates forgery and theft within customers accounts and transactions on behalf of a bank or a financial institution. They track and monitor the banks transactions and activity that comes through the banks customers accounts. It is their job to identify and trace any suspicious or high-risk transactions, and determine if there is improper activity involved and if there is risk to the bank or its customers.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asFraud Analyst  Gurgaon and the following details:",https://www.analyticsvidhya.com/blog/2015/11/fraud-analyst-gurgaon-6-8-years-experience/
"Collection Analyst  Gurgaon (6-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Fraud Analyst  Gurgaon (6-8 years of experience)|The Machine Learning Times of Year 2015  A Powerful Growth Story|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Collection AnalystLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesManages:Primary objective:Qualification and Skills RequiredNeed to have areas:Nice to have areas:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asCollection Analyst  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/11/collection-analyst-gurgaon-6-10-years-experience/
The Machine Learning Times of Year 2015  A Powerful Growth Story,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Collection Analyst  Gurgaon (6-10 years of experience)|Getting started with Machine Learning in MS Excel using XLMiner|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning is a core, transformative way by which were rethinking everything were doing.Were thoughtfully applying it across all our products, be it search, ads, YouTube or Play                                                                                      Sundar Pichai, CEO, Google2015 has been the year of machine learning. The revolutionto let machines make sense from huge data is gaining momentum by the day (we just created a few data points in writing and reading this article!).Not just Google, companies like Amazon, Accenture, Toyota,Hitachi, Tesla, Johnson & Johnsonand many more embraced machine learning at massive scale and improved their products & services.Also, it is not just big companies, startups have been an equal part of this revolution. Start-ups have come out with innovative applications of machine learning and some of them got acquired before they could test the market!In order to bring out the developments, weve created an e-paperMachine Learning Times (till Nov 15). Well also update it at the end December. Itconveys significantdevelopment happened during the year with Machine Learning.Catch the complete story below.Read the complete news here: Full News on Machine Learning in 2015",https://www.analyticsvidhya.com/blog/2015/11/infographic-rise-machine-learning-year-2015/
Getting started with Machine Learning in MS Excel using XLMiner,Learn everything about Analytics|Introduction|Do you find coding hardto understand?|What are the tasksXLMiner can do ?|Tutorial: Multiple Linear Regression|Tutorial: Logistic Regression|Tutorial: k  Means Clustering|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|The Machine Learning Times of Year 2015  A Powerful Growth Story|Business Intelligence Developer  Bangalore (3-4 years of experience)|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine Learning is nothing but building a machine which learns from its experience. And, becomes better with experience just like humans. We also learn from our experiences. Right ? Companies like Google, Facebook, Microsoft are using machine learning techniques at a larger scale.However, one common mis-conception people have is that they need to learn coding to start machine learning. While coding becomes necessary for any one who is doing machine learning seriosuly, but not to start it. You can look at GUI driven tool like Weka or even Excel to start with Machine Learning.Here, Ill introduce you to a simpler way to get started with Machine Learning.Machine Learning requires powerful coding / algorithmic skills. And thats why, people with computer science degree find it relatively easier to succeed in machine learning domain.But, the scenario has changed. Though, you cant escape coding completely, you canstill get started with machine learning. Once you get started, you can later brush up your coding skills.The good news is, now you can startmachine learning using Microsoft Excel. Yes! you heard it right.Frontline Solvershas introduced XLMINER DATA MINING add-in for MS Excel. It is an easy to use, made for professionals tool for data visualization, forecasting and data mining. Youd find it easy to use if:Also Read: Simple Yet Powerful Tricks To Analyze Data in ExcelI knew this was coming. Well! XLMiner can do lot of things which you do in R, Python or Julia. That too, without writing a piece of code. Itoffers a great deal in machine learning and data mining tasks. XLMiner supportsExcel 2007, Excel 2010 and Excel 2013 (32-bit and 64-bit). Here is the list of tasks which can be done using XLMiner:Note: It is not available for free. You can download it on 15 days trial period and later purchase two year license for $2495.In this article, Ill demonstrate the steps to perform Regression, Classification and Clustering in Excel. Id recommend you to work on small data sets in excel as it might crash. It is good to use on data sets likeTitanic.To get the best from this article, you must have / gain basic knowledge of these algorithms.If you need a quick refresher on machine learning, I recommend you to check out this tutorials:Essentials of Machine Learning AlgorithmsIve installed XLMiner. After installation, you willnotice XLMINER appearing inmain tabs(image below). You can also watch this overview of XLMiner platform.Lets get started !Regression is not a big deal. You can also perform it using add-in data analysis tool pack available in excel. It is good forstatistical analysis. For machine learning, you would need XLMiner. HereIve demonstrated multiple regression using XLMiner. For linear regression, all the steps remains same except you select one independent variable for modeling. Following are the steps:1. Ive used Boston Housing Data Set. This data represents housing prices in Boston based on various influencing factors. You can load the data set using: Help -> Examples -> Boston Housing.2. Here is the data set.3. There are no missing values in this data set. However, this add-in provides a convenient option to deal with missing values. You can access this option from here.Simply, select the variables where you find missing values. If missing values are represented by null,N/A or in any other form, mention it. Finally, you can choose the treatment method and done.4. Now well do feature selection. MEDV is the response variable. MEDV represents the median value of owner occupied homes in $1000.5. Use Shift + Click to select all independent variables at once. Send MEDV to Output variable. Click Next.6. Select correlation filters. Ive selected all three. Click Next7. Nowselect features. Lets findouttop 5 important predictor variables. Click Finish.8. Here is the variable importance chart. We see, LSTAT is the most important variable, followed by RM, PTRATIO, INDUS and TAX.9. Close this chart. You will see Output Navigator. This helps you to navigate between various output sheets. Lets check out selected predictors.10. Here are the selected predictors. Let proceed to build a regression model using these variables.11. Prior to modeling, lets divide (partition) this data into train and validation.12. On the basis of feature selection, select the variables to be included in partition. Leave the rest as default values and click OK.13. And, here weve got the training data set ready for modeling.14. Click on any cell in Selected Variables and proceed to build multiple regression model. Click Multiple Linear Regression15. Select the set of predictor and response variables. Click Next16. Select your required metrics. Click Finish17. Your multiple linear regression model is ready. Use the output navigator to access different metrics and model accuracy.Logistic Regression is a classic example of classification algorithm. Similar to multiple linear regression, below are the steps to build a logistic regression model. If you wish to quickly refresh your logistic regression concepts, you can refer to this tutorial: Simple Guide to Logistic Regression1. Load the data set Charles_bookclub. On XLMiner Ribbon, click on Help -> Example. Select this data set.This data set representsinformation associated with individuals who are members of a book club. Well build a model forpredicting whether a person will purchase a book about the city of Florence based on past purchases.2. Now, well divide the data set into training (70%) and validation (30%). This time you need to specify percentages for partition. Click OK3. Youll see a data partition sheet. Click on any cell in selected variables table and Clickon logistic regression as shown.4. Here you select the input and output variables. Florence is the output variable where it gets1 when a customer purchased a book about the city of Florence and 0 otherwise. Here 1 is success. 0 is failure as denoted in the option below. Leave the rest as default values. Click Next5. Select the confidence interval as 95%. If you tick Force constant term to zero, youll omit the constant term in the regression. Hence, dont select it. Click on advanced, and tick perform collinearity diagnostics. It will display useful information in dealing with correlated variables having large standard errors. Click OK. Now, click Variable Selection.6. Variable Selection helps us to deal with large number of predictor variables and find the best among them. Maximum size of best subset takes value from 1 to N, where N is the number of input variables. Well not change this value. In selection procedure, you can choose any as per your preferences. Ive chosenBest Subsets as it searches for all combination of variables and select only the best fit ones. Click OK. Click Next.7. Now well select the required computation coefficients to evaluate the model. Select Covariance matrix of coefficients and Residuals. Residuals will produce a table of fitted values and their residuals in the output. Click Finish.8. Here is your logistic regression model. If you scroll down this sheet, youll find various metrics useful to evaluate this model performance. A commonly used metric to check models accuracy is confusion matrix. As you scroll down, youd find this.If you are new to clustering, here is your quick refresher toClustering Analysis. In simple words, clustering is a technique of grouping variables with similar attributes. This technique is generally used for customer profiling and creating products as per their need.Lets look at the steps for perform k-means clustering in XLMiner.1. Load the data set Wine. Go to XLMiner ribbon, click Help -> Examples. Select Wine. In this data set, each row represents sample of wine belonging to 3 classes (A, B and C). On the basis of this data, well build a clustering model to determine the class of wine. Here is the data set.2. Click on any cell in data set. Then, click on k-means clustering.3. Type is the output variable. Hence, well select all variables except Type to be used in clustering. Click Next.4. Lets take number of clusters as 8. Because, with large number of clusters, sum of squared error(SSE) remains small. SSE is defined as the sum of the squared distance between each member of the cluster and its centroid. You can set any value of k, and evaluate the output from each to check which one is best. Setting random value to say 5, will let this algorithm to build the model from any random point. With this, XLMiner will generate 5 cluster sets and generate the output from best cluster. Leave the default values for rest and click Next.5. Leave the values as default. Click Finish6. Here is your clustering model. Check our various evaluation metrics to determinethe accuracy of this model.Random Starts Summary: This table determines the best start with lowest sum of squares distance. In this case (#1) is the best start. Once the best start is determined, the remaining output of the model is generated using the best start as starting point.Cluster Centers: Here you will find two boxes. The lower box shows the distance between the centroid of clusters. Larger the distance, different will be nature of clusters. For example, the difference between cluster 4 and cluster 8 is 1176.59. This suggests these clusters are very different. The upper box shows the variable values at the cluster centers.Data Summary: It represents the average distance of observations from the center of a cluster. We can infer then cluster 2 has lowest average distance from its centroid and cluster 6 has highest.7. Click on sheet KMC_Clusters. Here youll find the predicted clusters. Check the Record ID 1. It has been classified to cluster 6. Because, the distance of this observation is minimum to cluster 6. Similarly, all other observations have been classified on the basis of their nearest cluster.I wrote this tutorial just to get your started with machine learning in excel. Once you understand these algorithms, you can easily use them in R, Python or any other programming language. Since, many of us have worked on excel at some point, it wouldnt be difficult to understand these concepts in excel. If you get stuck, you can refer to help option in XLMiner Ribbon. The documentation is helpful and easy to understand.Now you know the steps, Id suggest you spend time in interpreting the model and iterate it to get the best fit. Excel mightslow down with large data sets, hence you should work on small data sets as to save your time in learning.Did you find this article useful? Have you ever worked on XLMiner? Id love to hear your experiences and suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/started-machine-learning-ms-excel-xl-miner/
"Business Intelligence Developer  Bangalore (3-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Getting started with Machine Learning in MS Excel using XLMiner|Lifetime Lessons: 20 Things Every Data Scientist Must Know Today|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Intelligence DeveloperLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Intelligence Developer  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/11/business-intelligence-developer-bangalore-3-4-years-experience/
Lifetime Lessons: 20 Things Every Data Scientist Must Know Today,Learn everything about Analytics|Introduction,"Some Useful Resources|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Business Intelligence Developer  Bangalore (3-4 years of experience)|Domain Consultant (Product)  Analytics Quotient Bangalore (3+ years of experience)|
Kunal Jain
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ive spent close toa decade in data science & analytics now. Over this period, I have learnt new ways of working on data sets and creating interesting stories. However, before I could succeed, I failed numerous times. Success doesnt come easy!How did I succeed? The answer is simple. Every time I failed,I said to myself, Letstake one more step. And I managed to travela long distance. I learnt statistics, data mining, SAS, R, Python, Machine Learning on the way.I confess that, in last 10 years,the methods of predictive modeling have become faster. Data is becoming larger than ever. We faced constraints when faced with Big Data. But, people came out with several big data technologies.Its overwhelming to see how the things have changed. But, there would still be many who are lagging to catch up with success in data science industry.Hence, I decided to share 20 thingswhich experience has taught me in the last 10 years. Hope you find them useful. The idea is to help people, who dont have a mentor to provide them this advice all the time. So go ahead and read!Learn PythonLearn Ensemble ModelingLearn Boosting AlgorithmsLearn Machine Learning AlgorithmsLearn k- fold Cross ValidationLearn Feature EngineeringResources onNeural Networks and Deep LearningMaster Structured Thinking Skill",https://www.analyticsvidhya.com/blog/2015/11/lifetime-lessons-20-data-scientist-today/
"Domain Consultant (Product)  Analytics Quotient Bangalore (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lifetime Lessons: 20 Things Every Data Scientist Must Know Today|7 Must Watch Documentaries on Statistics and Machine Learning|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Domain Consultant (Product)Location  BangaloreAbout employer Analytics QuotientAnalytics Quotient (AQ)  www.aqinsights.com provides analytics-powered solutions to marketing and business problems for leading global marketers. Some of our current key clients include:We are a fast-growing, ambitious and successful start-up and are looking for smart, enthusiastic people to join us. Some key facts about AQ:Job description: The Products team creates industry focused solutions that address specific marketing needs. We have a unique blend of analytical expertise and technological expertise to address business challenges.ResponsibilitiesQualification and Skills RequiredSALARY & OTHER DETAILSInterested people can apply for this job can mail their CV to[emailprotected]with subject asDomain Consultant (Product)  Analytics Quotient Bangalore",https://www.analyticsvidhya.com/blog/2015/11/domain-consultant-product-analytics-quotient-bangalore-3-years-experience/
7 Must Watch Documentaries on Statistics and Machine Learning,Learn everything about Analytics|Introduction|Why should you watch these documentaries?|Must Watch Documentaries|End Notes,"The Joy of Statistics|Future Intelligence|The Smartest Machine on Earth|Humans Need not Apply|Let my Data set Change Your Mindset|Deluged by Data|The Age of Big Data|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Domain Consultant (Product)  Analytics Quotient Bangalore (3+ years of experience)|Nobody Tells You  5 things Big Data CAN and Cannot Do|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Soon, our habitat will be invaded by unreal humans. Not only theyll influence our way of living, but also intervene in our modus operandi.Im not the only one who thinks this way. Last week I released a list of must watch movieson Machine Learning and Data Science.Ive watched 8 of them till now.These movies reveal the smart use of data and machine learning to make our lives better.Today, I am listing down some of the best documentaries on Data Science, Statistics and machine learning that I have come across.A documentary is nothing but a nonfictional representation of an incident. It consist of factual arguments and real life events. Unlike a movie, documentaries are more realistic and connected toa real-life purpose.We live in asphere of technological advancements which is majorlydriven by data. A decade back, the use of data was limited for statistical measures and studies. But now, everything has changed. Data influences our lives at personal and professional level. The combination of data, programming and mathematics have completely transformed the ways of doing business. Even businesses likewall street trading, casino gambling, sports are largely driven by data mining methods.Thats interesting ! But how did it all start ?Documentaries below will try to answer this question. If you are also a crazy soul for data science and machine learning, youll find them worthy.These documentaries are based on Statistics, Big Data, Artificial Intelligence, Mathematics and related topics.Duration: ~60 minutesFor those of you who find statistics boring, here is a proof thatits not. Its a breath-taking documentary which emphasizes on the use of statistics in real life. It will change the way you look at data and statistics. We fail to see the large influence of statistics around us. As Hans Rosling says, We fail to reckon but statistics is giving us a great understanding of life and universe beyond. There discover a lot of joyin these 59 minutes.Duration: ~ 43 minutesHumans are a curious species of living beings. We find excitement in exploring thingswhich can improve our lives. We always strive to achieve better than yesterday. We are only limited by our imagination. If you are also one of the kind who relate to last few statements, this amazing documentary will surely stimulate your curiosity levels. Youd finddevices which wouldrevolutionize our lives in coming future. This is the power of technology and data.Duration: ~52 minutesIn early 2011, IBM created a super computernamed Watson to challenge the human contestants on the popular game show Jeopardy. Here is the complete story of watson. Youd be surprise to know that watson had a brain size of 2400 home computers. This computer used techniques such as text analytics, machine learningalgorithms to find the correct answers. Watson was then used for humans welfare and successfully benefited 34 million customers through WellPoint health plans.https://www.youtube.com/watch?v=tAzeGkuQmUUDuration: ~ 15 minutesWill a robot replace your job? In the last decade, several tools have been built which are ready to replace human labour. As advanced version of these tools are Robots. Companies have heavily invested in establishing artificial intelligence systems to reduce the chances of human error.In order words, robots have started replacing low skilled human jobs. In this short documentary, youll learn about the changes ready to come in near future and some amazing stuffs which robots can do effortlessly.Duration: ~20 minutesI did not include this TED talk in my list until I had watched it completely. Though, it doesnt belong to the family of documentaries, yet I would insist you to watch it. In these 20 minutes, youll learn the importance of impeccable story telling using data visualization. A data scientist must master this skill. Mr. Rosling, beautifully demonstrates the use of statistics by comparing the life expectancy of various countries. For more interesting videos by Hans Rosling on statistics, visit here.Duration: ~43 minutesI always wanted to tell people the importance of data in our lives. But, I always used to run out of words until I found this documentary. It made my job easier. In these 43 minutes, youll experience a huge influence of data in your daily life. The devices which you use, tracks enormous amount of data overtime and gives businesses to other companies. Should you worry about this ? You really to watch it to get the answer. Due to these innovative methods of data collection, we are experiencing a massive surge in demand of data scientist.Duration: ~ 60 minutesData revolution is set to become more personal. Data scientists look for patterns in data sets. These patterns are dynamic in nature. An insight derived today might become obsolete tomorrow. In these 60 minutes, youll learn about thecommon sources of data which adds upto 2.5 billion GB of data generated every day.Moreover, youll learn about the impact of data mining on advertising, finance, astronomy, mankind, controlling crimes and many other activities which largely influences our lives.Initially, I collected 12 documentaries in total. But, after watching them, I realized these are the best ones. Ive purposely left out some documentaries on artificial intelligence. I found them similar tothese movies I shared last week. After watching them, I realized thatwe are the lucky ones who areliving and experiencing this revolution of data. Im sure well have lot of stories to tell to our coming generations.I hope you found these documentaries useful. My motive behind this articlewas to spread knowledge and awareness on importance of machine learningand statistics in an interesting way. If you are trying hard to become a data scientist, dont lose hope. Youve a lot on your plate to digest now.",https://www.analyticsvidhya.com/blog/2015/11/7-watch-documentaries-statistics-machine-learning/
Nobody Tells You  5 things Big Data CAN and Cannot Do,Learn everything about Analytics|Introduction|TinyExercise on Big Data|The80:20 rule|5 things Big Data CAN do|5 things Big Data CANNOT do|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|7 Must Watch Documentaries on Statistics and Machine Learning|Exclusive Interview with SRK, Sr. Data Scientist, Kaggle Rank 31 (DataHack Summit  Workshop Speaker)|
Tavish Srivastava
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Big Data makes us smarter, not wiser.  Tim Leberecht.The term Big Data got introducedin 1940s. Companies around the world haveput in ceaseless efforts to explore its potential. The global tech giants have massively increased their spending on leveraging big data technologies. This trend got quickly replicated among major industry players.As a result, according to a forecast issues by research firm(IDC), big data technology and services will grow at a CAGR of 23 percent through 2019. The big data annual spending will reach $48.6 billion in 2019.Thats how big data services are being accepted worldwide!Big Data has given a ray of hope to companies and enabled them to make use of data of any size and volume. The bits of data collected via our mobile phones, GPS, sensors devices is no longer useless. Every bit of data collected gets collected and processed to derive useful insights about us (customers).Amidst, the increasing benefits of Big Data, people fail to see the things it cantdo. This was surprising for me too.But soon I realized,Big data always complements business intuition but cant ever replace it.In this article, I present you my research of last 7 days. My crazy curiosity led me here. I just couldnt digest the fact that big data has all it takes for a company to succeed. Big Data is capable of many things. But incapable too.Note: My thoughts are not exhaustive but an attempt to put a framework. Feel free to add your perspectives in the comments section below.Black Friday Data Hack  9 Days to Go  Win Amazing PrizesThis exercise willprepare us for the future. We must know about things which are yet to come. Hence, If you are reading this, I invite youto take a stab on this question. You just need towrite(Ive already shared the answer though): 5 things which Big Data can do & 5 things big data can never do.For instance, if I conclude using a logic that X is not possible using any technology platform with Big Data. I will simply eliminate all the business problems which are related to X. Getting it?Below is my list. If you disagree with any of the element in my list, please justify! I will love to modify this list with time. Lets start with a short note on my ideology on using business intuition and business analyticsThe rule says,80% of time is spent in creating stories from past data, and 20% of time is spent in connecting those stories with current businessExplanation:I believe no analytical insights are useful until they are in syncwithbusiness intuition. Agree ? Moreover, with time, thedata driven component has grownexponentially. Companies are now flooding with data. But wouldthat really make a difference? No! Companies must realize, a right combination of successful business analytics over required business intuition is in ratio of 80:20. If we can build a story using analytics which describes the pastto predictfuture expectations80% of time, we need to invest20% of time thinking, how this information is useful to business. We must think of ways which can change our future and meet our broader business objectives. This requiresa strong business understanding and sound knowledge of business rules.The 20% component in this rule is non-replaceable.Hence,humans intervention is required for solving this 20% and possibly no machine can make up for it. Not even artificial intelligence.Because, humans think in a non-defined fashion which leads to creativity. Creativity is what I believe no machine can bring to the table. My list is inspired though this rule.I believe thisarticle will reach its true potential if people start trying out the exercise in this article. Try thinking in a more holistic view where you can see what machine cannot do ever. For instance, my starting point was the 80:20 rule that machine cannot bring creativity. This starting point helped me think of what are the pieces which needs creativity in the process of analytics.What is your list of do/dont? Did you like this post? Write your comments in the box below.",https://www.analyticsvidhya.com/blog/2015/11/5-big-data-can-cannot/
"Exclusive Interview with SRK, Sr. Data Scientist, Kaggle Rank 31 (DataHack Summit  Workshop Speaker)",Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Nobody Tells You  5 things Big Data CAN and Cannot Do|10 Must Watch Movies on Data Science and Machine Learning|
Kunal Jain
|27 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It took him just 2 years to secure a rank in Kaggle Top 30 from scratchMr. Sudalai Rajkumar a.k.a SRK, Lead Data Scientist at Freshdesk and previously worked as Sr. Data Scientist, Tiger Analyticshas become ahuge inspiration for aspiring data scientists around the world. Ive seen lots of people, driven by the spark of becoming a data scientist, but tend to develop disinterest when faced with difficulties.Sudalai Rajkumar and Bishwarup Bhattacharjee are taking 8-hours intense workshop during DataHack Summit 2017 on The MasterClass: How To Win Data Science Challenges. Attend this workshop atDataHack Summit 2017, 9  11 November in Bengaluru.The path to become a data scientist isnt easy as it looks. Even though, you are fortunate enough to have access to free online courses, but it requires unshakable determination to succeed. And, when it comes to securing rank in Top 30 kagglers, you cant achieve it without hours of coding and developing logical understanding.We decided to catch up withSRKtoknow about his success recipes and source of motivation, which has kept him going all these years. As we know, a journey without struggles isnt worthy of success, we found out his ways of overcoming with struggles.Below is my complete conversation with SRK:KJ: First of all, I would like to thank you for devoting time to us. Im sure, this interview would act as a motivational booster for young aspiring data scientists around the world. Lets start.KJ: You are currently Ranked 25 on Kaggle. How did your journey begin?SRK: My kaggle journey started two years back when I started learningdata science / analytics through MOOC courses.I was already working in analytics domain then. But, didntget anyopportunity to usethe advanced analytical techniques (whichI learnt from thecourses) in my work. Hence, I started looking foropportunities (projects) where I could use those techniques and subsequently came to know about Kaggle through my friends.Like every other aspiring data scientist, I started withthe classic Titanic problem for first couple of weeks. Later,I realized that I was unable to give my bestefforts since there were no time constraints,being a knowledge competition. Perhaps, I work best with deadlines.Then, Istarted working on the StumbleUpon Evergreen Classification Challenge. This challenge required building a classifier to categorize webpages as evergreen or non-evergreen. It was a classic binary classification problem. It involved good amount of text mining as well, whichmade it lot more intriguingfor a newbie like me. Not to forget, those benchmark codes really helped me learn a lot in the competition and kicked my Kaggle journey.KJ: Its being said, Success never comes to you unless you learn to embrace failures. Did you have hard time in dealing with data science / analytics as a beginner?SRK: Yes. Initially, I found it really hard to securerespectful position in the competitions. It took me a year to get my first top 10% finish. Then,almost another year to get the Kaggle Master status.As a beginner, I triedmany approaches while building models but most of them failed to give good results. I felt helpless and lost. But, when I look back in time, I realize that they are not actually failures. But, much needed lessonswhich helped me to perform better in the futurecompetitions.KJ: What helped you to overcome these difficulties?SRK:Undoubtedly,Kaggle forumshave helped me a lot. Ive learnt fromother peoples views, codes, results and implemented them to improve my mistakes. Like Analytics Vidhya, it is a superb destinationto learn lots of new things about analytics & data science. Personally, Ive learnt techniques including XGBoost, Deep learning, Online learning, t-SNE etc., from Kaggle forums.Above all, one more difficulty hampered my way. The thing was, some of thecompetitions turn out to be highly frustrating at times, especially when I used to put in lot of efforts into certain approach yet didnt get any improvement in results. Amidst such difficulties, I didnt give up! Instead, discovered a way.It is essentialto stay afloat during those times. Id generally switch to work on a different competition for the next couple of days and then come back to the earlier one. This break helped me clear my mind and gavea fresh perspective on the problem.In addition, I used to set short termtargets such as improving the score in the next couple of days by a certain margin, improving my rank in the competition and so on and keep working towards them.KJ: How do you decide in which Kaggle competition you should participate?SRK:I knew this was coming!When I started, I participated in almost all the competitions that came up. Even now I think I am doing the same, with a slight difference.The difference is that, in early days, I used to focus equally on all the competitions. But now, I tryto select the competitions best suited to my caliberand focus more on them. I believe Im best suited for competitionwhich challenges my knowledgeand needs good amount of effort in feature engineering.I still need to learnmulti-level stacking. Hence,I generally refrain myself from taking part actively in such competitions.KJ: In recent past, youve quickly climbed up in rankings on Kaggle. Whats your next target?SRK: Of course getting into top 10 in Kaggle rankings. And, if possible, Id love to secure the No.1 spot  But I think it is still a long way to go !!KJ: Do you prefer to work in teams or as in individual ? Which one do you recommend ?SRK: I prefer working in a team given that one has got seriously smartpartner(s). Typically, in a team,each membertries to think of different ideas / solutionsandcombining all these ideas potentially provides a better solution. This is analogous to ensemble modeling, where a number of varied models combine together to produce a single strong model.It is generally advisableto form teamat alater stage of the competition so that individuals have their own ideas / models before merging.But it is also important to work as an individual in few competitions for self-assessment. It will help us to know our strong and weak areas so as to tailor our learning plan accordingly.KJ: Tell us 3 things which youve learnt in life working as a Data Scientist ?SRK:Here are my 3 cents:1. Understanding the problem  It is really important to have a thorough understanding of the problem that we are trying to solve. Only after weve understood the problem clearly, we can derivesuitable insights from data to tackle the problem andobtain good results. This applies to real life as well.2. Structured Thinking  Its a unique way of thinking through the problems. Being a data scientist, one needs to be more structured in his/her thinking in order to obtain good results. Else, we might end up shooting in the dark as the number of options are way too many in most of the cases.3.Effective communication of results  Effective communication of derived results is as important as performing thedataanalysis. At times, it becomesdifficult to communicate the nuances of final analysis in simplelanguage to business people. As a Data Scientist, one mustlearn the art of effective communication.KJ: Is there a one for all road map for predictive modeling? According to you, what is an ideal approach (to work on a data set) to derive the best result ?SRK:I myself, is eager to know that road map if one such exists for predictive modeling. It would be much easier that way. However, here is the approach that I follow (in competitions) is:Once youve executed these7 steps, a basic framework will be ready to do more experimentation. Further, you canconcentrate more on:Last but not the least,wemust perform a solid local validation. Else, we might end up over fitting on the public leader board.KJ: Which machine learning algorithms are the most important to learn and practice ? Why ?SRK: Every algorithm has its own advantages and so it is absolutely necessary to learn as many as we can.However, here are few algorithmswhich have proved to be extremely helpful to me:KJ: What is your suggestion / advice for people keen to become a data scientist?SRK: As a (wanna-be) data scientistKJ:Im sure your invaluable experience and suggested guidance would helpmany young data scientists to discover the complicated world of data science and machine learning. Thank you once again.",https://www.analyticsvidhya.com/blog/2015/11/exclusive-interview-srk-sr-data-scientist-kaggle-rank-25/
10 Must Watch Movies on Data Science and Machine Learning,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Exclusive Interview with SRK, Sr. Data Scientist, Kaggle Rank 31 (DataHack Summit  Workshop Speaker)|Quick Introduction to Boosting Algorithms in Machine Learning|
Analytics Vidhya Content Team
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Some members of our team (including me) live by just 2 passions in life  Data Science & Movies! For us, slicing and dicing movies over Monday morning coffee is part of warming up ritual.So, we decided to do a poll among ourselves on the best movies related to data science and machine learning. We also thought that we would release the outcome of the results in form of an infographic.Needless to say there were heated debates and a few disappointed faces in our office! But I think the list we got to is a fairly representative of the thoughts of the group.While we debated on movies, we deliberately kept a few movies out. These were movies which showed the use of Data Science, but did not have a good scientific backing to them. Now, I understand that this could be highly subjective and if you think we have missed out on any movie, which should be part of this list, feel free to suggest them through comments below.Here are our top 10 movies on data science / machine learning. Enjoy!",https://www.analyticsvidhya.com/blog/2015/11/10-watch-movies-data-science-machine-learning/
Quick Introduction to Boosting Algorithms in Machine Learning,Learn everything about Analytics|What is Boosting?|How Boosting Algorithms works?|Types of Boosting Algorithms|Boosting Algorithm: AdaBoost|Boosting Algorithm: Gradient Boosting|End Note,"Introduction|Python Code|Python Code|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|10 Must Watch Movies on Data Science and Machine Learning|Tips for freshers to crack campus interviews for analytics / data science companies|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lots of analyst misinterpret the term boosting used in data science. Let me provide an interesting explanation of this term.Boostinggrants power to machine learning models to improve their accuracy of prediction.Boosting algorithms are one of the most widely used algorithm in data science competitions. The winners of ourlast hackathonsagree that they try boosting algorithm to improve accuracy of their models.In this article, I will explain how boosting algorithm works in very simple manner. Ive also shared the Python codes below. Ive skipped the intimidating mathematical derivations used in Boosting. Because, that wouldnt have allowed me to explain this concept in simple terms.Lets get started.Definition: The term Boosting refers to a family of algorithms whichconverts weak learner to strong learners.Lets understand this definition in detail by solving a problem of spam email identification:How would you classifyan email as SPAM or not? Like everyone else, our initial approach would beto identify spam and not spam emails using following criteria. If:Above, weve defined multiple rules to classifyan email into spam or not spam.But, do you think these rules individually are strong enough to successfully classifyan email? No.Individually, these rules arenot powerful enough to classify an email into spam or not spam.Therefore, these rules are called as weak learner.To convert weak learner to strong learner, well combine the prediction of each weak learner using methods like:
  Using average/ weighted average
  Considering prediction has higher voteFor example: Above,we have defined 5 weak learners. Out of these 5, 3 arevoted asSPAM and 2 are voted as Not a SPAM. In this case, by default, well consider an email as SPAM because wehave higher(3) vote for SPAM.Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, How boosting identify weak rules?To find weak rule, we applybase learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.Heres another question which might haunt you, How do we choose different distribution for each round?For choosing the right distribution, here are the following steps:Step 1: Thebase learner takes all the distributions and assign equal weight or attention to each observation.Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, weapply the next base learning algorithm.Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs fromweak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting payshigherfocus on examples which are mis-classied or have higher errors by preceding weak rules.Underlying engine used forboosting algorithms can be anything. It can be decision stamp, margin-maximizing classification algorithm etc.There are many boosting algorithms which use other types of engine such as:In this article, we will focus on AdaBoost and Gradient Boosting followed by their respective python codesand will focus on XGboost in upcoming article.This diagram aptly explains Ada-boost. Lets understand it closely:Box 1: You can see that we have assignedequal weights to each data point and applied a decision stump to classify them as + (plus) or  (minus). The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectlypredicted three + (plus) as  (minus). In such case, wellassign higher weights to these three + (plus) and apply another decision stump.Box 2:Here, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points. In this case,the second decision stump (D2) will try to predict them correctly.Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has causedmis-classification errors. This time with three -(minus). Again, we will assignhigher weight to three  (minus) and apply another decision stump.Box 3: Here, three  (minus) are given higher weights. Adecision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and  (minus) based on higher weight of mis-classified observation.Box 4: Here, we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithmhas classified these observation quite well as compared to any of individual weak learner.AdaBoost (AdaptiveBoosting) : It works on similar method as discussed above. Itfits a sequence of weak learners on different weightedtrainingdata. Itstarts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation whichhave been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if itaccepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.You can refer article Getting smart with Machine Learning  AdaBoostto understandAdaBoost algorithms in more detail.Here is a live coding window to get you started. You can run the codes and get the output in this window itself:You can tune the parameters to optimize the performance of algorithms, Ive mentioned below the key parameters for tuning:You can also tune the parameters ofbase learners to optimize its performance.In gradient boosting, ittrains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is anerror term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable.The principle idea behind this algorithm is to construct new base learners which can bemaximally correlated with negative gradient of the loss function, associated with the whole ensemble. You can refer article Learn Gradient Boosting Algorithmto understand this concept using an example.In Python Sklearn library, we use Gradient Tree Boosting or GBRT. It is a generalization of boosting to arbitrary differentiable loss functions. Itcan be used for both regression and classification problems.You can tuneloss function for better performance.In this article, we looked at boosting, one of the method of ensemble modeling to enhance the prediction power. Here, we have discussedthe science behindboostingand its two types: AdaBoost and Gradient Boost. We also studiedtheir respective python codes.In my next article, I will discuss about another type of boosting algorithms which is now a days secret of wining data science competitions XGBoost.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/
Tips for freshers to crack campus interviews for analytics / data science companies,Learn everything about Analytics|Introduction|Lets start with breaking few Myths|Here are few good thingsto do thing before interviews|What are interviewers primarily looking for in candidates|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Quick Introduction to Boosting Algorithms in Machine Learning|Senior Data Scientist  Bangalore (4-5 years of experience)|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im blessed that my memories last for long.1st week of December 2011 was a criticalweek for me.This was the first time whenI had toappearforjobinterviews at IIT Madras. I trusted myself completely. Yet, I was unsure of my performance ininterviews with analytics companies.I had great difficulty convincing recruiters that Im anideal person for thejob. They considered me unfit for analytics jobs.Actually, the trouble with me was, I did Mechanical Engineering. And, later I discovered my love forbusiness analytics. Im sure there would be many other like me finding themselves stuck in similar situations.The wheel of time has brought me to the other side of the table today. I now have experience of both the souls, a recruiter and a candidate. Once again, December is approachingfast. I can sense the anxiety and trepidation of candidate who would be appearing for campus interviews this season.I will help you now!Having interviewed many candidates, Ive analyzed the essential patterns for cracking campusinterviews. In fact, these patterns can help you to clear any type of analytics interview.In this article, Ive shared these insights along with some useful interview tips. Id recommend you to follow these tips while preparing for job interviews. Ive seen lots of candidates taking these tips for granted, and ending up disappointed.You should also check out the Ace Data Science Interviews course. It is the ultimate course, with plenty of videos and resources, to help you land your first data science role. Give yourself the gift of cracking data science interviews!Last but not least, dress well for your interview. There is a lot you cannot control in this recruitment process, but there is a lot which you probably can do very well. The list is not exhaustive but some very basic pointers which can help you prepare well. I will urge other recruiters to add what they look for in a candidate in interviews. Here is another awesome resource to prepare yourself.Best of luck to all candidates!Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/tips-crack-campus-interviews-non-core-companies/
"Senior Data Scientist  Bangalore (4-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Tips for freshers to crack campus interviews for analytics / data science companies|Engagement Manager  Bangalore (6-7+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Data ScientistLocation  BangaloreAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSenior Data Scientist  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/11/senior-data-scientist-bangalore-4-5-years-experience/
"Engagement Manager  Bangalore (6-7+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Scientist  Bangalore (4-5 years of experience)|Free Resources for Beginners on Deep Learning and Neural Network|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Engagement ManagerLocation  BangaloreAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asEngagement Manager  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/11/engagement-manager-bangalore-6-7-years-experience/
Free Resources for Beginners on Deep Learning and Neural Network,Learn everything about Analytics|Introduction|What is Neural Network?|What is Deep Learning?|What do Experts have to say?|Courses|Books|Blogs|Videos|Research Papers|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Engagement Manager  Bangalore (6-7+ years of experience)|Risk Analyst/ Business Analyst  Delhi NCR (3-4 years of experience)|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machines have already started their march towards artificial intelligence. DeepLearning and Neural Networks are probably the hottest topics in machine learning research today. Companies like Google, Facebook and Baidu are heavily investing into this field of research.Researchers believethat machine learningwill highly influencehuman life in near future. Human tasks will be automatedusing robots with negligible margin of error. Im sure many of us would never have imagined such gigantic power of machine learning.To ignite your desire, Ive listed the best tutorials on Deep Learning and Neural Networks available on internet today. Im sure this would be of your help! Take your first step today.Time for some motivation here. You must watch this before scrolling further. This ~3min video was released yesterday by Google. Enjoy!Time for proceed further. Firstly, lets understandDeep Learning and Neural Networkin simple terms.The concept of Neural Network began way back in 1980s. But, has gained re-ignited interest in recent times. Neural network is originally a biological phenomenon. Neural network is a network of interconnected neurons which maintain a high level of coordination to receive and transmit messages to brain & spinal cord.In machine learning, we refer Neural Network as Artificial Neural Network.Artificial Neural Network, as the name suggests, is a network (layer) of artificially created neurons which are then taught to adapt cognitive skills to function like human brain. Image Recognition, Voice Recognition, Soft Sensors, Anomaly detection, Time Series Predictions etc are all applications of ANN.In simple words, Deep Learning can be understood as an algorithm which is composed of hidden layers of multiple neural networks. It works on unsupervised data and is known to provide accurate results than traditional ML algorithms.Input data is passed through this algorithm, which is then passed through several non-linearities before delivering output. This algorithm allows us to go deeper (higher level of abstraction) in the network without ending up writing lot of duplicated code, unlike shallow algorithms. As it goes deeper and deeper,it filter the complex features and combines with those of previous layer, thus better results.Algorithms like Decision Trees,SVM, Naive Bayes are shallow algorithm.These involves writing lot of duplicated code and cause trouble reusing previous computations.Deep Learning throughNeural Network and takes us a step closer to Artificial Intelligence.Early this years, AMAstook place on Reddit with the masters of Deep Learning and Neural Network. Considering my ever rising craze to dig latest information about this field, I got the chance to attend their AMA session. Lets see what they have to said about the existence and future of this field:Geoffrey Hinton said, The brain has about 1014 synapses and we only live for about 109 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 105 dimensions of constraint per second.                                                                                 Yann LeCunn, on emotions in robot, said, Emotions do not necessarily lead to irrational behavior. They sometimes do, but they also often save our lives.If emotions are anticipations of outcome (like fear is the anticipation of impending disasters or elation is the anticipation of pleasure), or if emotions are drives to satisfy basic ground rules for survival (like hunger, desire to reproduce), then intelligent agent will have to have emotions                                                                               Yoshua Bengio said, Recurrent or recursive nets are really useful tools for modelling all kinds of dependency structures on variable-sized objects. We have made progress on ways to train them and it is one of the important areas of current research in the deep learning community. Examples of applications: speech recognition (especially the language part), machine translation, sentiment analysis, speech synthesis, handwriting synthesis and recognition, etc.                                                                               Jurgen Schmidhuber says, 20 years from now well have 10,000 times faster computers for the same price, plus lots of additional medical data to train them. I assume that even the already existing neural network algorithms will greatly outperform human experts in most if not all domains of medical diagnosis, from melanoma detection to plaque detection in arteries, and innumerable other applications.P.S. I am by no means an expert on Neural Networks. In fact, I have just started my journey in this fascinating world. If you think, there are other free good resources which I have not shared below, please feel free to provide the suggestionsBelow is the list of free resources useful to master these useful concepts:Machine Learning by Andrew Ng: If you are a complete beginner to machine learning and neural networks, this course is the best place to start. Enrollments for the current batch ends on Nov 7, 2015. This course provides a broad introduction to machine learning, deep learning, data mining, neural networks using some useful case studies. Youll also learn about the best practices of these algorithms and where are we heading with themNeural Network Course on Coursera: Who could teach Neural Network better than Hinton himself? This is a highly recommended courseon Neural Network. Though, it is archived now, you can still access the course material. Its a 8 week long course and would require you to dedicate atleast 7-9 hours/week. This course expects prior knowledge of Python / Octave / Matlab and good hold on mathematical concepts (vector, calculus, algebra).In addition to the course above, I found useful slides and lecture notes of Deep Learning programs from a few top universities of the world:Carnegie Mellon University  Deep Learning: This course ended on 21st October 2015. It is archived now. But, you can still access the slides shared during this course. Learning from slide is an amazing way to understand concepts quickly. These slides cover all the aspects of deep learning to a certain point. I wouldnt recommend this study material for beginners but to intermediates and above in this domain.Deep Learning for NLP This conference happened in 2013 on Human Language Technologies. The best part was, the knowledge which got shared. The slides and videos and well accessible and comprises of simple explanation of complex concepts. Beginners will find it worth watching these videos as the instructor begins the session from Logistic Regression and dives deeper into the use of machine learning algorithms.Deep Learning for Computer Vision This course was commenced at the starting of year 2015 by Columbia University. It focuses on deep learning techniques for vision and natural language processing problems. Thiscourse embraces theano as the programming tool. This course requires prior knowledge in Python and NumPy programming, NLP and Machine Learning.Deep Learning: This is an archived course. It happened in Spring 2014. It was instructed by Yann LeCunn. This is a graduate course on Deep Learning. The precious slides and videos are accessible. Id highly recommend this course to beginners. Youd amazed by the way LeCunn explains. Very simple and apt. To get best out of this course, Id suggest you to work on assignments too, for your self evaluation.This book is written by Christopher M Bishop. This book serves as a excellent referencefor students keen to understand the use of statistical techniques in machine learning and pattern recognition. This books assumes the knowledge of linear algebra and multivariate calculus. It provides a comprehensiveintroduction to statistical pattern recognition techniques using practice exercises.Because of the rapid development and active research in the field, there arent many printed and accessible books available on Deep Learning. However, I found that Yoshua Bengio, along with Ian Goodfellow and Aaron Courville isworking on a book.You can check itsrecent developments here.Neural Networks and Deep Learning: This book is written by Michael Neilson. It is available FREE online. If you are good at learning things at your own pace, Id suggest you to read this book. There are just 6 Chapters. Every chapters goes in great detail of concepts related to deep learningusing really nice illustrations.Here are some of the best bet I have come across:BeginnersIntroduction to Neural Networks: This is Chapter 10 of Book, The Nature of Code. Youll find the reading style simple and easy to comprehend. The author has explained neural network from scratch. Along with theory, youll also find codes(in python) to practice and apply them at your them. This not only would give you confidence to learn these concept, but would also allow you to experience their impact.Hackers Guide to Neural Networks: Though, the codes in this blog are written in Javascript which you might not know. Id still suggest you to refer it for the simplicity of theoretical concepts. This tutorial has very little math, but youll need lots of logic to comprehend and understand the following parts.IntermediatesRecurrent Neural Network Part 1, Part 2, Part 3, Part 4: After you are comfortable with basics of neural nets, its time to moveto the next level. This is probably the best guide you would need to master RNN. RNNis a form of artificial neural network whose neurons send feedback signals to each other. Id suggest you to follow the 4 parts religiously. It begins RNN from basics, followed byback propagation and its implementation.Unreasonable Effectiveness of RNN: Consider this as an additional resource on RNN. If you are fond of seeking options, you might like to check this blog. It start with basic definition of RNN and goes all the way deep into building character models. This should help you give more hands on experience on implementing neural networks in various situations.Backward Propogation Neural Network: Here youll find a simple explanation on the method of implementing backward propagation neural network. Id suggest beginners to follow this blog and learn more about this concept. It will provide you a step by step approach for understanding neural networks deeply.Deep Learning Tutorial by Stanford: This is by far the best tutorial/blog available on deep learning on internet. Having been recommended by many, it explains the complete science and mathematics behind every algorithm using easy to understand illustrations. This tutorial assumes basic knowledge of machine learning. Therefore, Id suggest you to start with this tutorial after finishing Machine Learning course by Andrew Ng.Complete Tutorial on Neural Networks: This complete playlist of neural network tutorials should suffice your learning appetite. There were numerous videos I found, but offered a comprehensive learning like this one.Note: In order to quickly get started, Id recommend you to participate in Facial keypoint Detection Kaggle competition. Though, this competition ended long time back, you can still participate and practice. Moreover, youll also find benchmark solution for this competition. Here is the solution:Practice  Neural Nets. Get Going!Deep Learning Lectures: Here is a complete series of lectures on Deep Learning from University of Oxford 2015. The instructor is Nando de Freitas. This tutorials covers a wide range of topics from linear models, logistic regression, regularization to recurrent neural nets. Instead of rushing through these videos, Id suggest you to devote good amount of time and develop concrete understanding of these concepts. Start from Lecture 1.Introduction to Deep Learning with Python: After learning the theoretical aspects of these algorithm, its now time to practice them using Python. This ~1 hour video is highly recommended to practice deep learning in python using theano.Deep Learning Summer School, Montreal 2015: Here are the videos from Deep Learning Summer School, Montreal 2015. These videos covers advanced topics in Deep Learning. Hence, I wouldnt recommend them to beginners. However, people with knowledge of machine learning must watch them. These videos will take your deep learning intellect to a new level. Needless to say, they are FREE to access.Also See: Top Youtube Videos on Machine Learning, Deep Learning and Neural NetworksI could list here numerous paper published on Deep Learning, but that would have defeated the purpose.Hence, to highlight the best resources, Ive listed some of the seminal papers in this field:Deep Learning in Neural NetworksIntroduction to Deep LearningDeep Boltzmann MachinesLearning Deep Architectures for AIDeep Learning of Representations: Looking ForwardGradient based training for Deep ArchitechtureBy now, Im sure you have a lot of work carved out for yourself. I found them intimidating initially, but these videos and blogs totally helped me to regain my confidence. As said above, these are free resources and can be accessible from anywhere. If you are a beginner, Id recommend you to start with Machine Learning course by Andrew Ng and read throughblogs too.Ive tried to provide the best possible resources available on these topics at present. As mentioned before, I am not an expert on neural networks and machine learning (yet)! So it is quite possible that I missed out on some useful resource.Did I miss out any useful resource? May be! Please share your views / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/free-resources-beginners-deep-learning-neural-network/
"Risk Analyst/ Business Analyst  Delhi NCR (3-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Free Resources for Beginners on Deep Learning and Neural Network|Simple Yet Powerful Excel Tricks for Analyzing Data|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Risk Analyst/ Business AnalystLocation  Delhi NCRAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asRisk Analyst/ Business Analyst  Delhi NCR and the following details:,https://www.analyticsvidhya.com/blog/2015/11/risk-analyst-business-analyst-delhi-ncr-3-4-years-experience/
Simple Yet Powerful Excel Tricks for Analyzing Data,Learn everything about Analytics|Overview||Introduction|Commonly used functions|Generating inference fromData|Data Cleaning|Essential keyboard shortcuts|End Notes,"|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Risk Analyst/ Business Analyst  Delhi NCR (3-4 years of experience)|Simple Guide to Logistic Regression in R and Python|
Sunil Ray
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ive always admired the immense power of Excel. This software is not only capable of doing basic data computations, but you can also perform data analysis using it. It is widely used for many purposes including the likes of financial modeling and business planning. It can become a good stepping stone for people who are new to the world of data analysis.Even before learning R or Python, it is advisable to have knowledge of Excel. It does no harm to add Excel in your skill sets. Excel, with its wide range of functions, visualization, arrays empowers you to quickly generate insights from data which would be hard to see otherwise.In fact, we have designed an entire comprehensive course on Excel for you! Make sure you check it out and give yourself the gift of mastering Excel.It has a few drawbacks as well. It cant handle large data sets very efficiently. Ive personally faced this issue. Try doing computations of data ~ 200,000 entries and youll notice that excel starts struggling. There are ways to work around and handle this data to some extent, but Excel is not a big data tool.In such cases, R or Python are the best bets.I feel fortunate that my journey started with Excel. Over the years, Ive learnt many tricks to work to deal with data faster than ever. Excel has numerous functions. It becomes confusing at times to choose the best one.In this article, Ill provide you some tips and tricks to work on Excel and save you time. This article is best suited to people keen to upgrade their data analysis skills.Note: If you think you are a master coder in data science, you wont find this article useful. For others, Id recommend you to practice these tricks to develop a concrete understanding of them.1. Vlookup(): It helps to search a value in a table and returns a corresponding value. Lets look at the table below (Policy and Customer). In Policy table, we want to map city name from the customer tables based on common key Customer id. Here, function vlookup() would help to perform this task.
For above problem, we can write formula in cell F4 as =VLOOKUP(B4, $H$4:$L$15, 5, 0) and this will return the city name for all the Customer id 1 and post that copy this formula for all Customer ids.Tip: Do not forget to lock the range of the second table using $ sign  a common error when copying this formula down. This is known as relative referencing.2. CONCATINATE():It is very usefulto combine text from two or more cells into one cell. For example: we want to create aURL based on input of host name and request path.
Above problem can be solved using formula, =concatenate(B3,C3) and copy it.Tip: I prefer using & symbol, because it is shorter than typing a full concatenate formula, anddoes theexactly same thing. The formula can be written as = B3&C3.3. LEN()  This functiontells you about the length of a cell i.e. number of characters including spaces and special characters .Example: =Len(B3) = 234. LOWER(), UPPER() and PROPER() These three functions help to change the text to lower, upper and sentence case respectively (First letter of each word capital).In data analysis project, these are helpful in converting classes of different case to a single case else these are considered as different classes of the given feature. Look at the below snapshot, column A has five classes (labels) where as Column B has only two because we have converted the content to lower case.
5. TRIM():This is a handy function used to clean text that has leading and trailing white space. Often when you get a dump of data from a database the text youre dealing with is padded with blanks. And if you dont deal with them, they are also treated as unique entries in a list, which is certainly not helpful.
6. If(): Ifind it one of the most useful function in excel. Itlets you use conditional formulas whichcalculate one way when a certain thing is true, and another way when false. For example, you want to mark each sales as High and Low. If sales is greater than or equals to $5000thenHigh elseLow.1. Pivot Table:Whenever you are working with company data, you seek answers for questions likeHow much revenue is contributed by branches of North region? or What was the average number of customers for product A? and many others.Excels PivotTable helps you toanswer these questions effortlessly. Pivot tableis asummary table that lets you count, average, sum, and perform other calculations according to the reference feature you have selected i.e. It converts a data table to inference table whichhelps us to take decisions. Look at the below snapshot:
Above, you can see that table on the lefthas sales detail against each customer with region and product mapping. In table to the right,we have summarized the information at region level which nowhelps us to generate a inference that South region has highest sales.Methods to create Pivot table:
Step-1: Click somewhere in the list of data. Choose the Insert tab, and click PivotTable. Excel will automatically select the area containing data, including the headings. If it does not select the area correctly, drag over the area to select it manually. Placing the PivotTable on a new sheet is best, so click New Worksheet for the location and then click OKStep-2: Now, you can see the PivotTable Field List panel, whichcontains the fields from your list; all you need to do is to arrange them in the boxes at the foot of the panel. Once you have done that, the diagram on the left becomes your PivotTable.
Above, you can see that we have arranged Region in row, Product id in column and sum of Premium is taken as value. Now you are ready with pivot table which shows Region and Product wise sum of premium. You can alsouse count, average, min, max and other summary metric. For more detail on Pivot table, I would suggest you to refer this link.2. Creating Charts:Building a chart/ graph in excel requires nothing more than selecting the range of data you wish to chart and pressF11. This will create aexcel chart in default chart style but you can change it by selecting different chart style. If you prefer the chart to be on the same worksheet as the data, instead of pressing F11, press ALT + F1.Of course, in either case, once you have created the chart, you can customize to your particular needs to communicate your desired message.To know about different properties of charts, I would recommend to refer this link.1. Remove duplicate values: Excel has inbuilt feature to remove duplicate values from a table. It removes theduplicate values from given table based on selected columns i.e. if you have selected two columns then it searches for duplicate value having same combination of both columns data.

Above, you can see that A001 and A002 have duplicate value but if we select both columns ID and Name then we have only one duplicate value (A002, 2).
Follow the these steps to remove duplicate values:Select data > Go to Data ribbon > Remove Duplicates2. Text to Columns:Lets say you have data stored in column as shown in below snapshot.
Above, you can see that values are separated by semi colon ;. Now to split these values in different column, I will recommend to use Text to Columns feature in excel. Follow below steps to convert it to different columns:Keyboard shortcuts are the best way to navigate cells or enter formulas more quickly. Weve listed our favorites below.Note: This isnt an exhaustive list. Feel free to share your favorite keyboard shortcuts in Excel in the comments section below. Literally, I do 80% of excel tasks using shortcuts.Excel is arguably one of the best programs ever made, and it has remained the gold standard for nearly all businesses worldwide. But whether youre a newbie or a power user, theres always something left to learn. Or do you think youve seen it all and done it all? Let us know what weve missed in the comments.Make sure you check out ourcomprehensive course on Excelas well!",https://www.analyticsvidhya.com/blog/2015/11/excel-tips-tricks-data-analysis/
Simple Guide to Logistic Regression in R and Python,Learn everything about Analytics|Overview||Introduction|Project to apply Logistic Regression|Problem Statement|What is Logistic Regression?|Derivation of Logistic Regression Equation|Performance of Logistic RegressionModel|Logistic Regression Model in R and Python|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Simple Yet Powerful Excel Tricks for Analyzing Data|6 Practices to enhance the performance of a Text Classification Model|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every machine learning algorithm works best under a given set of conditions. Making sure your algorithm fits the assumptions/requirements ensures superior performance. You cant use any algorithm in any condition. For example: Have you ever tried using linear regression on a categorical dependent variable? Dont even try! Because you wont be appreciated for getting extremely low values of adjusted R and F statistic.Instead, in such situations, you should try using algorithms such as Logistic Regression, Decision Trees, SVM, Random Forest etc. To get a quick overview of these algorithms, Ill recommend reading Essentials of Machine Learning Algorithms.With this post, Igive you usefulknowledge on Logistic Regression in R. After youve mastered linear regression, this comes as the natural following step in your journey. Its also easy to learn and implement, but you must know the science behind this algorithm.Ive tried to explain these concepts in the simplest possible manner. Lets get started.HR analytics is revolutionizing the way human resources departments operate, leading to higher efficiency and better results overall. Human resources have been using analytics for years.However, the collection, processing, and analysis of data have been largely manual, and given the nature of human resources dynamics and HR KPIs, the approach has been constraining HR. Therefore, it is surprising that HR departments woke up to the utility of machine learning so late in the game. Here is an opportunity to try predictive analytics in identifying the employees most likely to get promoted.Practice NowLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary/categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm). In 1972, Nelder and Wedderburn proposed this model with an effort to provide a means of using linear regression to the problems which were not directly suited for application of linear regression. Infact, they proposed a class of different models (linear regression, ANOVA, Poisson Regression etc) which included logistic regression as a special case.The fundamental equation of generalized linear model is:Here, g() is the link function, E(y) is the expectation of target variable and +x1 +x2 is the linear predictor (,, to be predicted). The role of link function is to link the expectation of y to linear predictor.Important PointsLets understand it further using an example:We are provided a sample of 1000 customers. We need to predict the probability whether a customer will buy (y) a particular magazine or not. As you can see, weve a categorical outcome variable, well use logistic regression.To start with logistic regression, Ill first write the simple linear regression equation with dependent variable enclosed in a link function:Note: For ease of understanding, Ive considered Age as independent variable.In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established usingtwo things: Probability of Success(p) and Probability of Failure(1-p). p should meet following criteria:Now, well simply satisfy these 2 conditions and get to the core of logistic regression. To establish link function, well denote g() with p initially and eventually end up deriving this function.Since probability must always be positive, well put the linear equation in exponential form. For any value of slope and dependent variable, exponent of this equation will never be negative.To make the probability less than 1, we must divide p by a number greater than p. This can simply be done by:Using (a), (b) and (c), we can redefine the probability as:where p is the probability of success. This (d) is the Logit FunctionIf p is the probability of success, 1-p will be the probability of failure which can be written as:where q is the probability of failureOn dividing, (d) / (e), we get,After taking log on both side, we get,
log(p/1-p) is the link function. Logarithmic transformation on the outcome variable allows us to model a non-linear association in a linear way.After substituting value of y, well get:This is the equation used in Logistic Regression. Here (p/1-p) is the odd ratio. Whenever the log of odd ratio is found to be positive, the probability of success is always more than 50%. A typical logistic model plot is shown below. You can see probability never goes below 0 and above 1.To evaluate the performance of a logistic regression model, we must consider few metrics. Irrespective of tool (SAS, R, Python) you would work on, always look for:1. AIC (Akaike Information Criteria)  The analogous metric of adjusted Rin logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.2. Null Deviance and Residual Deviance  Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.3. Confusion Matrix: It is nothing but a tabular representation of Actual vs Predicted values. This helps us to find the accuracy of the model and avoid overfitting. This is how it looks like:                                              Source: (plug  n  score)You can calculate the accuracy of your model with:From confusion matrix, Specificity and Sensitivitycan be derived as illustrated below:Specificity and Sensitivity plays a crucial role in deriving ROC curve.4. ROC Curve:Receiver Operating Characteristic(ROC) summarizes the models performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). For plotting ROC, it is advisable to assume p > 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p > 0.5. The area under curve (AUC), referred to as index of accuracy(A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.Note: For model performance, you can also consider likelihood function. It is called so, because it selects the coefficient values which maximizes the likelihood of explaining the observed data. It indicates goodness of fit as its value approaches one, and a poor fit of the data as its value approaches zero.The R code is provided below but if youre a Python user, heres an awesome code window to build your logistic regression model. No need to open Jupyter  you can do it all here:Considering the availability, Ive built this model on our practice problem  Dressify data set. You can download it here.Without going deep into feature engineering, heres the script of simple logistic regression model:This data require lots of cleaning and feature engineering. The scope of this article restricted me to keep the example focused on building logistic regression model. This data is available for practice. Id recommend you to work on this problem. Theres a lot to learn.By now, you would know the science behind logistic regression. Ive seen many times that people know the use of this algorithm without actually having knowledge about its core concepts. Ive tried my best to explain this part in simplest possible manner. The example above only shows the skeleton of using logistic regression in R. Before actually approaching to this stage, you must invest your crucial time in feature engineering.Furthermore, Id recommend you to work on this problem set. Youd explore things which you might havent faced before.Did I miss out on anything important ? Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/
6 Practices to enhance the performance of a Text Classification Model,Learn everything about Analytics|Introduction|End Notes,"1. Domain Specific Features in the Corpus|2. Use An Exhaustive Stopword List|3. Noise Free Corpus|
4. Eliminating features with extremely low frequency|
5. Normalized Corpus|6. Use Complex Features: n-grams and part of speech tags||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Simple Guide to Logistic Regression in R and Python|Business Analyst/Senior Business Analyst & Associates Manager  Gurgaon (2-7 years of experience)|
Shivam Bansal
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A few months back, I was working on creating a sentiment classifier for Twitter data. After trying the common approaches, I was still struggling to get good accuracy on the results.Text classification problems and algorithms have been around for a while now. They arewidely used for Email Spam Filtering by the likes of Google and Yahoo, for conducting sentiment analysis of twitter data and automatic news categorization in google alerts.However, while dealing with enormous amount of text data, models performance and accuracy becomes a challenge. The performance of a text classification model is heavily dependent upon the type of words used in the corpus and type of features created for classification. I used several practices to improve the results of my model.In this article, Ive illustrated thesix best practices to enhance the performance and accuracy of a text classification model which I had used:For a classification problem, it is important to choose the test and training corpus very carefully. For a variety of features to act in the classification algorithm, domain knowledge plays an integral part.For example, if the problem is Sentiment Classification on social media data, the training corpus should consist of the data from social sources like twitter and facebook.On the other hand if the problem is Sentiment Classification for news data, the corpus should consist of data from news sources. This is because the vocabulary of a corpus varies with domains. Social Media contains a lot of slangs and improper keywords like awsum, lol, gooood etc which are absent in any of the formal corpus such as news, blogs etc.Lets take an example of a naive bayes classification problem where the task is to classify the statements into two Classes: Class A and Class B. Wevetraining data corpus and a test data corpus. As mentioned here, the training corpus should contain the features from relevant corpus. Therefore,training corpus should consist of data points such as:Stopwords are defined as the most commonly used words ina corpus. Most commonly used stopwords are a, the, of, on,  etc. These words are usedto define the structure of a sentence. But, are of no use in defining the context. Treating these type of words as feature words would result in poor performance in text classification. These words can be directly ignored from the corpus in order to obtain a better performance. Apart from language stopwords, There are some other supporting words as well which are of lesser importance than any other terms. These includes:Language Stopwords  a, of, on, the  etcLocation Stopwords  Country names, Cities names etcTime Stopwords  Name of the months and days (january, february, monday, tuesday, today, tomorrow ) etcNumerals Stopwords  Words describing numerical terms ( hundred, thousand,  etc)After removal of these entities from the test data, test data would be reformed to following:In most of the data science problems, it is recommended to undertakea classification algorithm on a cleaned corpus rather than a noisy corpus. Noisy corpus refers tounimportant entities of the text such as punctuations marks, numerical values, links and urls etc. Removal of these entities from the text wouldincrease the accuracy, because size of sample space of possible features set decreases.Yet, it becomes essentialto note that, these entities should only be removed if the classification problem does not usethese entities. For example  emoticons such as :), :P,  are important for sentiment classification but may not be important for other text classifications.After eliminating the noisy entities like hashtags, url text, the training corpus would look like:Keywords which occur in lesser frequency in the corpus usually does not play a role in text classification. One can get rid of these low occurring features, resulting in better performance of the model. For example  if the frequency counts of words of a corpus looks like the following:It is clear that terms fig and dale have occurred in low frequency as compared to other terms. Hence, if we choose a threshold of 10, all keywords with less frequency can be ignored, resulting in good accuracy.Words are the integral part of any classification technique. However, these words are often used with different variations in the text depending on their grammar (verb, adjective, noun, etc.). It is always a good practice to normalize the terms to their root forms. This technique is known as Lemmatization. For example, the words:All can be normalized down to the word Play as far as the classifier is concerned. Performance will be good when there is single feature for ten variations rather than ten features for each variations.In some cases, features as the combination of words provides better significance rather than considering single words as features. Combination of N words together are called N-grams. It is known that Bigrams are the most informative N-Gram combinations. Adding bigrams to feature set will improve the accuracy of text classification model.Similarly considering Part of Speech tags combined with with words/n-grams will give an extra set of feature space. also increase the classifications. For example its better to train the model, such that word book when used as NOUN means book of pages, when used as VERB means to book a ticket or something else.In this article, we discussed few practices to improve the accuracy of a text classifier model. These gave me an improvement of ~10%  20% in accuracy depending on the use case.This is obviously not a complete list, but it provides a nice introduction for optimization of text classification algorithms. If you feel there are any other techniques which I havemissed, feel free to share in comments section below.",https://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/
"Business Analyst/Senior Business Analyst & Associates Manager  Gurgaon (2-7 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|6 Practices to enhance the performance of a Text Classification Model|Data Scientist  Mumbai (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Analyst/Senior Business Analyst & Associates ManagerLocation  GurgaonAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Analyst/Senior Business Analyst & AM  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/10/business-analystsenior-business-analyst-gurgaon-2-7-years-experience/
"Data Scientist  Mumbai (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst/Senior Business Analyst & Associates Manager  Gurgaon (2-7 years of experience)|Analyst  Ahmedabad (1-2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  MumbaiAbout employer  ConfidentialAbout Company- An exciting starts up to work with, who is expanding their wings and is growing exponentially. The leadership team has rich experience in Analytics & Data Science across verticals and they are looking forward to expand their team. The requirement can be found below:DescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Mumbai and the following details:",https://www.analyticsvidhya.com/blog/2015/10/data-scientist-mumbai-2-4-years-experience/
"Analyst  Ahmedabad (1-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Mumbai (2-4 years of experience)|26 Things I Learned in the Deep Learning Summer School|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  AnalystLocation  AhmedabadAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asAnalyst  Ahmedabad and the following details:,https://www.analyticsvidhya.com/blog/2015/10/analyst-ahmedabad-1-2-years-experience/
26 Things I Learned in the Deep Learning Summer School,Learn everything about Analytics|Introduction|26 Precious Things I learnt,"1. The need for distributed representations|2. Local minima are not a problem in high dimensions|3. Derivatives derivatives derivatives|4. Weight initialisation strategy|5. Neural net training tricks|6. Gradient checking|7. Motion tracking|8. Syntax or no syntax? (aka, is syntax a thing?)|9. Distributed vs Distributional|10. The state of dependency parsing|11. Theano|12. Nvidia Digits|13. Fuel|14. Multimodal linguistic regularities|15.Taylor series approximation|16. Computational intensity|17. Minibatches|18. Training on adversarial examples|19. Everything is language modelling|20. SMT had a rough start|21. The State of Neural Machine Translation|22. MetaMind classifier demo|23. Optimising gradient updates|24. Theano profiling|25.Adversarial nets framework|26. arXiv.org numbering|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Analyst  Ahmedabad (1-2 years of experience)|Data Scientist  Delhi/NCR (3+ years of experience)|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published at marekrei.comIn the beginning of August, I got the chance to attend the Deep Learning Summer School in Montreal. It consisted of 10 days of talks from some of the most well-knownneural network researchers. During this time I learned a lot, way morethan I could ever fit into a blog post. Instead of trying to pass on 60 hours worth of neural network knowledge, Ihave made a list of small interesting nuggets of information that I was able to summarise in a paragraph.At the moment of writing, the summer school websiteis stillonline, along with all the presentation slides. All of the information and most of the illustrations come from these slides and are the work of theiroriginal authors.The talks in the summer school were filmed as well, hopefully they will also find their way to the web.Update: The Deep Learning Summer School videos are now onlineAlright, lets get started.During his first talk, Yoshua Bengio said This is my most important slide. You can see that slide below:Lets say you have a classifier that needs to detect people that are male/female, have glasses ordont have glasses, and are tall/short. With non-distributed representations, you are dealing with 2*2*2=8 different classes of people. In order to train an accurate classifier, you need to have enough training data for each of these 8 classes. However, with distributed representations, each of these properties could be captured by a different dimension. This means that even if your classifier has never encountered tall men with glasses, it would be able to detect them, because it has learned to detect gender, glasses and height independently from all the other examples.The team of Yoshua Bengio have experimentally found that when optimising the parameters of high-dimensional neural nets, there effectively are no local minima. Instead, there are saddle points which are local minima in some dimensions but not all. This means that training can slow down quite a lot in these points, until the network figures out how to escape, but as long as were willing to wait long enough then it will find a way.Below is a graph demonstrating a network during training, oscillating between two states: approaching a saddle point and then escaping it.Given one specific dimension, there is some small probability p with which a point is a local minimum, but not a global minimum, in that dimension. Now, the probability of a point in a 1000-dimensional space being an incorrect local minimum in all of these would be p^1000, which is just astronomically small. However, the probability of it being a local minimum in some of these dimensions is actually quite high. And when we get these minima in many dimensions at once, then training can appear to be stuck until it finds the right direction.In addition, this probability p will increase as the loss functiongets closer to the global minimum. This means that if we do ever end up at a genuine local minimum, then for all intents and purposes it will be close enough to the global minimum that it will not matter.Leon Bottou had some useful tables with activation functions, loss functions, and their corresponding derivatives. Ill keep these here for later.Update: The min and max functions in the ramp formula should be switched.The current recommended strategy for initialising weights in a neural network is to sample values uniformly from [b,b], whereHk and Hk+1 are the sizes of hidden layers before and after the weight matrix. Recommended byHugo Larochelle, published byGlorot & Bengio(2010).A few practical suggestions fromHugo Larochelle:If you implemented your backprop by hand and its not working, then theres roughly 99% chancethat the gradientcalculation hasa bug. Use gradient checking to identify the issue. The idea is to use the definition of a gradient: how much will the model error change, if we increase a specific weight by a small amount.A more in-depth explanation is available here: Gradient checking and advanced optimizationHuman motion tracking can be done with impressive accuracy. Below are examples from the paper Dynamical Binary Latent Variable Models for 3D Human Pose Tracking by Graham Taylor et al. (2010). The method usesconditional restrictedBoltzmann machines.Chris Manning andRichard Socher have put a lot of effort into developing compositional models that combine neural embeddings with more traditional parsingapproaches. This culminated with aRecursive Neural Tensor Network(Socher et al., 2013), which uses both additive and multiplicative interactions to combine word meanings along a parse tree.And then, the model was beaten (by quite a margin) by theParagraph Vector (Le & Mikolov, 2014), which knows absolutely nothing about the sentence structure or syntax. Chris Manning referred to this result as a defeat for creating good compositional vectors.However, more recent work using parse trees has again surpassed this result.Irsoy & Cardie (NIPS, 2014) managed to beat paragraph vectors by going deep with their networks in multiple dimensions. Finally, Tai et al. (ACL, 2015)have improved the results again by combining LSTMs with parse trees.The accuraciesof these models on the Stanford 5-class sentiment dataset are as follows:So it seems that, at the moment, models using the parse tree are beating simpler approaches. Im curious to see if and when the next syntax-free approachwill emerge that will advance this race. After all, the goal of many neural models is not to discard the underlying grammar, but to implicitly capture it in the same network.Chris Manning himself cleared up the confusion between the two words.Distributed: A concept is represented as continuous activation levels in a number of elements. Like a dense word embedding, as opposed to 1-hot vectors.Distributional:Meaning is represented by contexts of use. Word2vec is distributional, but so are count-based word vectors, as we use the contexts of the word to model the meaning.Comparison of dependency parsers on the Penn Treebank:The last result is from Google pulling out all the stops, byputting massive amounts of resources into training the Stanford neural parser.Well, I knew a bit about Theano before, but I learned a whole lot more during the summer school. And it is pretty awesome.SinceTheanooriginates fromMontreal, it was especially helpful to be able to ask questions directly from the people who are developing it.Most of the information that was presented is available online, in the form ofinteractive python tutorials.Nvidia has a toolkit called Digits that trains and visualizes complex neural network models without needing to write any code.And theyre selling DevBox  a machine customized for running Digits and other deep learning software (Theano, Caffe, etc). It comes with 4 Titan X GPUs and currently costs $15,000.Fuel is a toolkit that manages iteration over your data sets  it can split them into mini batches, manage shuffling, apply various pre-processing steps, etc. There are prebuilt functions for some established data sets, such as MNIST, CIFAR-10, and Googles 1B Word corpus.It is mainly designed for use with Blocks, a toolkit that simplifies network construction with Theano.Remember king  man + woman = queen? Turns out that works with images as well (Kiros et al., 2015).When we are at point x0 and take a step to x, then we can estimate the function value in the new location by knowing the derivatives, using the Taylor series approximation.Similarly, we can estimatethe loss of a function, when we update parameters 0 to.where g contains the derivatives with respect to , and H is the Hessian with second order derivativeswith respect to .This is the second-order Taylor approximation, but we could increase the accuracy by adding even higher-order derivatives.Adam Coates presented a strategy for analysing the speed ofmatrix operations on a GPU. Its a simplified model that says your time is spent on either reading/writing to memory or doing calculations. It assumes you can do both in parallel sowe are interested in which one of them takes more time.Lets say we are multiplying a matrix with a vector:If M=1024 and N=512, then the number of bytes we need to read and store is:And the number of calculationswe need to do is:If we have a GPU that can do6 TFLOP/s and has memory bandwidth of 300GB/s, then thetotal running time will be:This means the process is bounded by the 7s spent on copying to/from thememory, andgetting a faster GPU would not make any difference. As you can probably guess, this situation gets better with bigger matrices/vectors, and when doing matrix-matrix operations.Adam also described the idea of calculating the intensity of an operation:In the previous scenario, this would beLow intensity means the system is bottle necked on memory, and high intensity means its bottlenecked by the GPU speed. This can be visualized, in order to find which of the two needs to improve in order to speed up the whole system, and where the sweet spot lies.Continuing from the intensity calculations, one way of increasing the intensity of your network(in order to be limited by computation instead of memory), is to process data in minibatches. This avoids some memory operations, and GPUs are great at processing large matrices in parallel.However, increasing the batch size too much will probably start to hurting the training algorithm and converging can take longer. Its important to find a good balance in order to get the best results in the least amount of time.It was recently revealed thatneural networks are easily tricked by adversarial examples. In the example below, the image on the left is correctly classified as a goldfish. However, if we apply the noise pattern shown in the middle, resulting in the image on the right, the classifier becomes convinced this is a picture of a daisy. The image is fromAndrej Karpathys blog postBreaking Linear Classifiers on ImageNet, and you can read more about it there.The noise pattern isnt random though  the noise is carefully calculated, in order to trick the network. But the point remains: the image on the right is clearly still a goldfish and not a daisy.Apparently strategies like ensemble models, voting after multiple saccades, and unsupervised pretraining have all failed against this vulnerability. Applying heavy regularisation helps, butnot before ruining the accuracy on the clean data.Ian Goodfellow presented the idea of training on these adversarial examples.They can be automatically generated and added to the training set. The results below show that in addition to helping with the adversarial cases, this also improves accuracy on the clean examples.Finally, we can improve this further by penalizing the KL-divergence between the original predicted distribution and the predicted distribution on the adversarial example. This optimizes the network to be more robust, and to predict similar class distributions for similar (adversarial) images.Phil Blunsom presented the idea that almost all NLP can be structured as a language model. We can do this by concatenating the output to the input and trying to predict the probability of the whole sequence.Translation:P(Les chiens aiment les os || Dogs love bones)Question answering:P(What do dogs love? || bones .)Dialogue:P(How are you? || Fine thanks. And you?)The latter two need to be additionally conditioned on some world knowledge. The second part doesnt even need to be words, but could be labels or some structured output like dependency relations.When Frederick Jelinek and his team at IBM submitted one of the first papers on statistical machine translation to COLING in 1988, they got the following anonymous review:The validity of a statistical (information theoretic) approach to MT has indeed beenrecognized, as the authors mention, by Weaver as early as 1949. And was universallyrecognized as mistaken by 1950 (cf. Hutchins, MT  Past, Present, Future, EllisHorwood, 1986, p. 30ff and references therein). The crude force of computers is notscience. The paper is simply beyond the scope of COLING.Apparently a very simple neural model can produce surprisingly good results. An example of translating from Chinese to English, from Phil Blunsoms slides:In this model, the vectors for the Chinese words are simply added together to form a sentence vector. The decoderconsists of a conditional language model which takes the sentence vector, together with vectors from the two recently generated English words, and generates the next word in the translation.However, neural models are still not outperforming the very best traditional MT systems. They do come very close though. Results from Sequence to Sequence Learningwith Neural Networksby Sutskever et al. (2014):Update: @stanfordnlp pointed out that there are some recentresults where the neural model does indeed outperform the state-of-the-art traditional MT system. Check out Effective Approaches to Attention-based Neural Machine Translation (Luong et. al., 2015).Richard Socher demonstrated the MetaMindimage classification demo, which you can train yourself by uploading images. I trained a classifier to detect Edison and Einstein (couldnt find enough unique images of Tesla). 5 example images for both classes, testing on one held out image each. Seemed to work pretty well.Mark Schmidt gave two presentations about numerical optimisation in different scenarios.In a deterministic gradient method we calculate the gradient over the whole data set and then apply the update. The iteration cost is linear with the data set size.Instochastic gradient methods we calculate the gradient on one data point and then apply the update. The iteration cost is independent of the data set size.Each iteration of the stochastic gradient descent is much faster, but it usually takes many more iterations to train the network, as this graph illustrates:In order to get the best of both worlds, we can use batching. More specifically, we could do one pass of the dataset with stochastic gradient descent, in order to quickly get on the right track, and then start increasing the batch size. The gradient error decreases as the batch size increases, although eventually the iteration cost will become dependent on the dataset size again.Stochastic Average Gradient (SAG) is a method that gets around this, providing a linear convergence rate with only 1 gradient per iteration. Unfortunately, it is not feasible for large neural networks, as it needs to remember the gradient updatesfor every datapoint, leading to large memory requirements.Stochastic Variance-Reduced Gradient (SVRG) is another method that reduces this memory cost, andonly needs 2 gradient calculations per iteration (plus occasional full passes).Mark said a student of his implemented a variety of optimisation methods (AdaGrad, momentum, SAG, etc). When asked, what he would use in a black box neural network system, the student said two methods: Streaming SVRG (Frostig et al., 2015), and a method they havent published yet.If you put profile=True into THEANO_FLAGS, it will analyse your program, showing a breakdown of how much is spent on each operation. Very handy for finding bottlenecks.Following on from Ian Goodfellows talk on adversarial examples, Yoshua Bengio talked about having two systems competingwith each other.System D is a discriminative system that aims to classify between real data and artificially generated data.System G is a generative system, that tries to generate artificial data, which D would incorrectly classify as real.As we train one, the other needs to get better as well. In practice this does work, althoughthe step needs to be quite small to make sure D can keep up with G. Below are some examples from Deep Generative Image Models using aLaplacian Pyramid of Adversarial Networks  a more advanced version of this model which tries to generate images of churches.The arXiv number contains the year and month of the submission, followed by the sequence number. So paper1508.03854 was number3854 in August 2015. Good to know.",https://www.analyticsvidhya.com/blog/2015/10/26-learned-deep-learning-summer-school/
"Data Scientist  Delhi/NCR (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|26 Things I Learned in the Deep Learning Summer School|Machine Learning Engineer  Delhi / NCR (3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  Delhi/NCRAbout employer  ConfidentialDescriptionWe are currently looking for a qualified candidate to join a Data Scientist. You will be responsible for developing core predictive and decision analytics. Focus will be on using the best of what lies at the forefront of technology and data science to address complex, real-world digital marketing challenges.ResponsibilitiesQualification and Skills RequiredRequirements:Other skills wed appreciate: Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Delhi/ NCR and the following details:",https://www.analyticsvidhya.com/blog/2015/10/data-scientist-delhincr-3-years-experience/
"Machine Learning Engineer  Delhi / NCR (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Delhi/NCR (3+ years of experience)|Must Read Books for Beginners on Big Data, Hadoop and Apache Spark|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Machine Learning EngineerLocation  Delhi / NCRAbout employer  ConfidentialDescriptionWe are currently looking for a qualified candidate to join a Machine Learning Engineer. You will be responsible for developing core predictive and sentiment analytics for brands in the Marketing Space. Focus will be on using the best of what lies at the forefront of technology and data science to address complex, real-world digital marketing challengesResponsibilitiesQualification and Skills RequiredRequirements:Other skills wed appreciate: Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asMachine Learning Engineer  Delhi /NCR and the following details:",https://www.analyticsvidhya.com/blog/2015/10/machine-learning-engineer-delhi-ncr-3-years-experience/
"Must Read Books for Beginners on Big Data, Hadoop and Apache Spark",Learn everything about Analytics|Introduction|Who is this article aimed to?|Big Data for Layman|Big Data for Techies Hadoop|Big Data for Techies  Apache Spark|End Notes,"The Human Face of Big Data|Big Data: A Revolution That Will Transform How We Live, Work, and Think|Datacylsm: Who We Are( When We Think No Ones Looking)|The Signal and the Noise: Why So Many Predictions Fail  But Some Dont|The Second Machine Age: Work, Progress and Prosperity in a Time of Brilliant Technologies|Hadoop For Dummies|Hadoop: The Definitive Guide|Hadoop Operations|Agile Data Science: Building Data Analytics Applications with Hadoop|Hadoop in Practice|Professional Hadoop Solutions|MapReduce Design Patterns: Building Effective Algorithms and Analytics for Hadoop|Learning Spark: Lightning -Fast Big Data Analysis|Spark: Learn Spark in a DAY!|Advanced Analytics with Spark: Patterns for Learning from Data at Scale|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Machine Learning Engineer  Delhi / NCR (3+ years of experience)|13 Tips to make you awesome in Data Science / Analytics Jobs|
Analytics Vidhya Content Team
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How many of you would agree/disagree with this statement:Google knows and understands you better than what you yourself do?Do let me know your views through comments below.I have been thinking about the statement above for some time and it might be difficult to take an absolute stance, but the very fact that you need to think about it signifies the importance of data. Think about it, our view about our own self is biased by who we want to be. Our view about ourselves is influenced by emotions, recency and limitations of human memory. But, Google doesnt have these limitations!Companies are now betteraware ofour lifestyle, choices and daily routine than we do. Thanks to our data stored by smartphones, wrist bands, fitness tracker, shopping bills etc.But, what good will my data do to these companies?I asked the same question to myself, until I read one of the books listed below.Technologies such as Hadoop, MapReduce, Apache Spark have brought a revolution in the ways ofanalyzing big data. Spark, being the latest, promises lightning fast cluster computing.This is probably the best time to make your career in Big Data. I believe, nothing beat books when it comes to learning a concept to its core.In this article, Ive listed the best books for beginners on Hadoop, Apache Spark and Big Data.This article is for complete beginners in Big Data. It assumes no prior knowledge of big data.In order to simplify the learning experience, Ive also divided the books in 2 clusters:As the name suggests, the first cluster introduces the enormousworld of Big Data to common people. These books wouldntteach you the techniques to develop Big Data capabilities, but enable you to understand the domain.The second cluster of books are meant for the techies  people looking to build a career in Big Data. These books are treasures of technical knowledge, which should enable you to a shining Sparking career ahead.This book is written by Rick Smolan and Jennifer Erwitt. In this book,youll learn about interesting ways using which big data is providing a healthier life to children and old age people. It features 10 essays and stunning infographics published by noticed writers of the industry. It connects big data with real stories of human life and its transformation. Im sure this book will definitely add to your existing big data perspective.This book is written by Kenneth Cukier and Viktor Mayer Schonberger. This book takes you on a world tour of values added by big data across all industries. This book will help you to stay ahead of the key trends defining businesses in coming years. Jeff Jonas, Chief Scientist, IBM Entity Analytics said, The book teems with great insights on the new ways of harnessing information, and offers a convincing vision of the future. It is essential reading for anyone who uses  or is affected by  big data.This book is written by Christian Rudder. Its a New York Times Bestseller. Do I need to say anything more? Well! heres a quick glimpse. This book covers some of the best cases of big data and its profound impact on our lives. It introduces to a world which is majorly driven by numbers and data than just humans. Definitely a must to keep bookin your book self.This book is written by Nate Silver. It comprises of interesting cases driven by statistics, economics, predictions. It also makes one aware of the common pitfalls to avoid while making predictions and offers a wealth of knowledge on prediction and forecasting. This is a must read book for data scientists, analysts, statisticians and anyone who admires the power of data.This book is written by Erik Brynjolfsson, Andrew McAfee and Jeff Cummings. Before you start reading it, you must know its an audio book. This book takes a big leap into the future and shows the indomitable reignof machine and computers on humans. It defines the era of industrial revolution and the next one too(maybe upcoming).Itpresents a realistic versionof digital advancements on various facets of human life.This book is written by Dirk Deroos. This bookeasy to read and understand, and meant for beginners (as name suggests). It makes areader understand the value of big data & hadoop. It explains the origin of hadoop, its benefits, functionality, practical applications and makesyou comfortable dealing with it. It also familiarizes you with hadoop ecosystem, cluster, mapreduce, design patterns and much more operations with Hadoop.This book iswritten by Tom White. Itdescribes useful methods to build,maintain reliable, scalable and distributed systems with Apache Hadoop. It explains the concept of HDFS and Mapreduce in great detail. This book deliversgreat results when read with discipline. Beginners will find it hard to understand at first. But,as you read through chapters, youll start loving them.This book is written by Eric Sammer. As the name suggests, this book will teach you the methods tomaintain large and complex hadoop clusters. Eric has not only covered the essentials of Hadoop, but also has provided some priceless approaches which can help a person to perform these tasks efficiently. Youll find dedicated chapters to maintenance, backups, monitoring, troubleshooting etc. It covers every possible component of Hadoop which should be known by a Big Data Engineer.This book is written by Russell Jurney. This book provides you necessary knowledge to build effective analytics applications using Hadoop in enterprise environment. It uses tools such as Python, Apache Pig, D3.js to create an agile environment for data exploration using examples. These example codes are available on github. This book is suited for intermediate users having good knowledge of data analytics.This book is written by Alex Holmes. This is probably the best practice book on Hadoop. It features 85 examples on Hadoop in Q & A format. Using these problems, youll explore the hidden aspects of hadoop and learn the ways of building and deploying specific solution as per catered needs.More than just examples, itll also introduce you the methods of integrating MapReduce and R. Author has effortlessly explained the complicated concepts in plain simple english. It is highly recommended for Beginners.This book is written by Boris Lublinsky, Kevin T Smith, Alexey Yakubovich. This book is a detailed guide which explains Hadoop framework and APIs integration to provide real world solutions. Moreover, it exposes the inner workings of APIs to allow architects and developers to better leverage and customize them. More than just implication, it teaches the best scenarioswhere these codes (Java and XML) should be used.This book is written by Donald Miner. This books assumes that reader has basic knowledge of hadoop. It is best suited for advanced beginners keen to master mapreduce algorithms. It describes various uses of MapReduce with Hadoop. It contains various methodologies helpful to solve manyhadoop problems quickly. It summarizes these concepts with interesting examples.This book is written by Holden Karau, Andy Konwinski, Patrick Wendell and Matei Zaharia. This is best suited for people new to Spark. It explains difficult concepts in simple and easy to understand english. I recommend this book for beginners. This book teaches you to leverage sparks powerful built-in libraries, including Spark SQL, Spark Streaming and Mlib. Above all, itll allow you tomaster topics like data partitioning and shared variables.This book is written by Acodemy. Another book for beginners. This book covers the basic of spark and its related component. It is good enough to get you started with Spark, but you cant expect more than that. It follows a step by step method of explaining abstruse concepts and theories. In the end, this book teaches you the methods to use for spark at its greatest capability.This book is written by Sandy Ryza, Uri Laserson, Sean Owen and Josh Wills. After youve read any of the above listed books, this comes as a natural next step. Time to raiseyour knowledge of spark. This book highlights the procedure to deal with large scale data analysis with spark.Along with spark, it covers statistical methods to teach the ideal analytics approach. This book commands a basic knowledge of machine learning, statistics, Java, Python or Scala.Disclosure:The amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission.In this article, Ive listed some of the best books (which I perceive) on Big Data, Hadoop and Apache Spark. These books are must for beginners keen to build a successful career in big data.Books demand discipline and persistence. I had neither. Until I picked a book and read it cover to cover.If you havent done it yet, its your turn now. The books listed above comprises of all the knowledge essential to take your first step in big data.Technologies like Hadoop, Apache Spark are in huge demand across the world. Companies have data, they even have technologies, but they dont have skilled manpower to work on them.Did I leave outauseful book on Big Data, Hadoop or Apache Spark? Share you views in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/books-big-data-hadoop-apache-spark/
13 Tips to make you awesome in Data Science / Analytics Jobs,Learn everything about Analytics|Introduction|Here is the list in priority order,"1.Understand the business before you start solving problems|2. Think hard of whether you are solving an underlying problem or just an outcome|3. Spend more time on finding out the right evaluation metric and how much is required for implementation|4.Follow the diverge-converge thinking process to avoid pre-mature convergence|5. Break industry silos to think of alternate solutions|6. Engage with business counterparts throughout the process|7. Think of simplest implementation levers to bring your idea to life|8. While making a business deck, make sure you lay it out in their language|9. Learn to speak business language while presenting to business leaders|10. Actively follow up on the implementation plan|11. Actively participate in Data Hackathons|12. Read blogs and books on upcoming tools and techniques on analytics|13. Learn upcoming tools to know what is possible and what is not|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Must Read Books for Beginners on Big Data, Hadoop and Apache Spark|Beginners guide to Web Scraping in Python using BeautifulSoup|
Tavish Srivastava
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I was fortunate to getearlyopportunities of workingon numerous data science projects. I enjoyedthis part the most. Even more, when I realized that my efforts will add value to some company.However, the miserable part was, less than 30% of the data science projects actually got implemented to their potential. I gotshattered realizingthat my efforts got wasted. But, I wasnt the only one. Almost, every other analyst had the same feeling of disappointment.Even today, the real challengewhich data scienceindustry faces is the lack of coordination between business folks and analytics folks. To my surprise, Ive even noticed that these people prefer to sit away from each other in same offices.Only if both these skills set were common in professionals, we would have seen a much higher project implementations.Over last four years, Ive spent lots of time thinking of bestpractices to make a project successful.I found that, If you have right people sitting in your office, haveclearly defined business problems and a culture whichmotivates out-of-box thinking, you have a break-through in the pipeline.Therefore, to become successful at your data science / analytics jobs, Id recommend you to follow the tips mentioned below. These are tried and tested tips. To reap the maximumbenefits, Id suggest you follow them with discipline. Ive benefited from them. Its your turn now!I know you are an analyst and all you care about is numbers. But, what differentiates an awesome business analyst from average data analyst? Its their potentialto understand business. You should try to understand business even before you take up your first project. Here are a few things you should definitely explore:a. Customer level information:Total number of active customers, month on month customer attrition, segments defined by business on portfolio.b. Business Strategies: How do we acquire new customers, what are the channels. How do we retain valuable customers.c. Product Information: How does your customer interact with your products? How do you earn money through your product? Is your product a direct revenue maker or is just an engagement tool?If you can answer all these questions, you are in a good shape to start your first project.                   I have observed that analysts aim forobjectives which arenot even the main concernof the problem. For instance, lets imagine we found that more acustomer calls at customer care, higherwill be his propensity to abandon the services.Now, if we begin solving for method tominimize the calls at customer care, we probably wont reduce the attrition rate. Rather, I already see higher dissatisfaction in your customers if you do not have a human justifying your faults. This is probably an easy kill, and you will refusetoget into such easy traps. But, real life problems are much difficult to find. I will say, it is much easier to solve a defined problem than finding what is the right problem to solve.This probably is the easiestpuzzle to solve for an analyst yeta simpletrap to fall in. Let me explain it usingfew examples.Suppose, you are trying to build a targeting model for a marketing campaign. Which of the metric will you choose to gauge your model:I will always choose KS in this case, given that Lift will only give you estimate on a particular decile. Hence, it probably wont help us to find the total target population and the break point. AUC-ROC will be an estimate for the overall population, which is not our intent in this case. Log-Likelihood is probably the biggest misfit in this case, as all matters to us is the rank order and not the actual probability.I have seen this as the biggest problem in many functions/industries. Today business leaders seekinnovation in every thing they do.To truly innovate, you need to follow a systematic way to diverge and converge. The extent to which you need to diverge will come along as you get more experienced in this approach. What we mean is to think of all possible ways to crack the problem being unbiased on feasibility, time for development and traditional approaches. Once, you are convinced that you have covered the universe, you now apply all your constraints to narrow down the approaches.Analytics is being used in every possible industry. But, why do we not go beyond our traditional approaches and look for solution in other industries?For instance, a recommended video solution implemented in E-Commerce industry can be very well used in a blogging portal like Analytics Vidhya. The only way to do this is to interact with people working in other industry and learn about their efforts using analytics.Right from the first day of your analysis, you should interact with business partners. One thing which I have seen going wrong in general is that analyst and business partner get in touch on the solution non-frequently. Business partners want to stay away from the technical details and so does the analyst from business. This does no good to the project. It is very essential to maintain constant interaction to understand implementation of the model parallel to building of the model.I know you are a statistician and love to confuse business people using complex formulations. Bringing such complexity in discussion with business people might help you get out of the immediate conversation but decreases the chances of successful implementation.Here is what you need to do: Once you have the output variables, try to find out a simple lever which can make it easier for the business to understand. Let me give you an example of this approach. We were trying to find out the right agents who will be topperformers once they onboard. We came up with the stratfied population and their expected performance. However, we had to choose a lever which can change the population mix. What we did was very simple: we implemented differential fee strategy to change the mix of applicant and hence the mix of our population.The target variable is never the end product of your analysis. It is always a business deck! So you need to put a lot of effort while bringing out your idea in a crisp and effective way. Try learning terminologies with which your audience can connect and think of what will business partner look out if you were in their shoes.I recently started learning chinese for one of my projects. The entire project was extremely easy but I found that even with a robustmodel, I was doing a bad job at selling it to business. The reason being my gap of understanding their internal discussions. It is very essential to speak a language of your audience. I have seen very simple models being appreciated and smartest models being rejected. The only difference being the ability of the analyst to speak business while presenting their models.Coming to the last but not the least, what happens once every one is convinced with the effectiveness of your model. Your job is still not done. Set up monthly follow ups with business to understand how project was implemented, is it being used in the right send.Apart from these 10 basic practices, here are a few on the technical front I would recommend:One thing which you will realize with time is that analytics industry is extremely dynamic. However, if you are a person who likes to be in their comfort zone, you will soon find your skill sets redundant. One thing which I found extremely useful is participating in data science competitions and competing with your peers/learn from your peers. Kaggle and Analytics Vidhya are good points to take up a few challenges.I believe this is another way to keep yourself up to date. I recommend reading these lists to start:Must Read Books for Beginners on Machine Learning and Artificial IntelligenceMust Read Books for Data ScientistsMust Read Books for Data ScientistsGet out of your comfort zone of programming on SAS, R or Python. Try learning upcoming technologies to handle big data. SPARK and JAVA will be my recommendation to start with.I have witnessed the difference of marrying business with analytics. The list is no way exhaustive but will give you a kickstart.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/tips-tricks-awesom-data-science-jobs/
Beginners guide to Web Scraping in Python using BeautifulSoup,"Learn everything about Analytics|Overview||Introduction|Ways to extract information from web|What is Web Scraping?|Libraries required for web scraping|Basics  Get familiar with HTML (Tags)|Scraping a web page using BeautifulSoup|But, why cant I just use Regular Expressions?|End Note|","If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|13 Tips to make you awesome in Data Science / Analytics Jobs|The D Hack, Ask Us Anything with past hackathon winners and practice problems|
Sunil Ray
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The need and importance of extracting data from the web is becoming increasingly loud and clear. Every few weeks, I find myself in a situation where we need to extract data from the web to build a machine learning model.For example, last week we were thinking of creating an index of hotness and sentiment about various data science courses available on the internet. This would not only require finding new courses, but also scraping the web for their reviews and then summarizing them in a few metrics!This is one of the problems / products whose efficacy depends more on web scraping and information extraction (data collection) than the techniques used to summarize the data.There are several ways to extract information from the web. Use ofAPIs being probably the best way to extract data from a website. Almost all large websites like Twitter, Facebook, Google, Twitter, StackOverflow provide APIs to access their data in a more structured manner.If you can get what you need through an API, it is almost always preferred approach over web scraping. This is because if you are getting access to structured data from the provider, why would you want to create an engine to extract the same information.Sadly, not all websites provide an API. Some do it because they do not want the readers to extract huge information in a structured way, while others dont provide APIs due to lack of technical knowledge. What do you do in these cases? Well, we need to scrape the website to fetch the information.There might be a few other ways like RSS feeds, but they are limited in their use and hence I am not including them in the discussion here.Web scraping is a computer software technique of extracting information from websites.This technique mostlyfocuses on the transformation of unstructured data (HTML format) on the web into structured data (database or spreadsheet).You can perform web scraping in various ways, including use of Google Docs to almost every programming language. I would resort to Python because of its ease and rich ecosystem. It has a library known as BeautifulSoup which assists this task. In this article, Ill show you the easiest way to learn web scraping using python programming.For those of you, who need a non-programming way to extract information out of web pages, you can also look at import.io. It provides a GUI driven interface to perform all basic web scraping operations. The hackers can continue to read this article!As we know,Python is an open source programming language. You may find many libraries to perform one function. Hence, it is necessary to find the best to use library. I prefer BeautifulSoup(Python library), sinceit is easy and intuitive to work on. Precisely, Illusetwo Python modules for scraping data:BeautifulSoup does not fetch the web page for us. Thats why, I use urllib2 in combination with the BeautifulSoup library.Python has several other options for HTML scraping in addition to BeatifulSoup. Here are some others:While performing web scarping, we deal with html tags. Thus,wemust have good understanding of them. If you already know basics of HTML, you can skip this section. Below is the basic syntax of HTML:This syntax has various tags as elaborated below:Other useful HTML tags are:If you are new to this HTML tags, I would also recommend you to referHTML tutorial from W3schools. This will give you a clear understanding about HTML tags.Here,I am scraping data from a Wikipedia page.Ourfinal goalis to extract list of state, union territory capitals in India. And some basic detail like establishment, former capital and others form this wikipedia page. Lets learn withdoing this project step wise step:Above, you can see that, we have only one output. Now to extract all the links within <a>, we will use find_all().
Above, it is showing all links including titles, links and other information. Now to show only links, we need to iterate over each a tag and then return the link using attribute href with get.Now to identify the right table, we will use attribute class of table and use it to filter the right table. In chrome, you can check the class name by right click on the required table of web page > Inspect element > Copy the class name OR go through the output of above command find the class name of right table.Finally, we have data in dataframe:

Similarly, you can perform various other types of web scraping using BeautifulSoup. This will reduce yourmanual efforts to collect data from web pages. You can also look at the other attributes like .parent, .contents, .descendants and .next_sibling, .prev_sibling and various attributes to navigate using tag name. These will help you to scrap the web pageseffectively.-Now, if you know regular expressions, you might be thinking that you can write code usingregular expression which can do the same thing for you. I definitely had this question. In my experience with BeautifulSoup and Regular expressions to do same thing I found out:So, it boils down to speed vs. robustness of the code and there is no universal winner here. If the information you are looking for can be extracted with simple regex statements, you should go ahead and use them. For almost any complex work, I usually recommend BeautifulSoup more than regex.In this article, we looked at web scraping methods using BeautifulSoup and urllib2 in Python. We also looked at the basics of HTML and perform the web scraping step by step while solving a challenge.Id recommend you topractice this and useit for collecting data from web pages.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/
"The D Hack, Ask Us Anything with past hackathon winners and practice problems",Learn everything about Analytics|The D Hack|Ask Us Anything (AUA) with all our past Hackathon winners|Practice problems for everyone|What else would you want?,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Beginners guide to Web Scraping in Python using BeautifulSoup|Data Scientist  Marketing  Bangalore (4-9+ years of experience)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Here is a famous quote from John Keats:Nothing ever becomes real till it is experienced-John KeatsWhile Keats might have said it in different context (and I dont know the context), I can surely vouch for it in Data Science. It is to enable this experiential learning that we started doing data hackathons.And I am happy to announce some awesome learning initiatives coming up in next few days (or nights!).We are ready to kick off the festive season in style  through the D Hack. This is going to be our largest data hackathon ever. D stands for a lot of things  Double the competition, Double the prize money and of course the festive season ahead (Diwali being the biggest Indian festival).The last hackathon we conducted saw more than 500 registrations. This time we aim to double this number. So, if you havent registered for the competition  do it now! A few things to keep in mind:What an experience it was. Amazing competition. Couldnt have got any better. I have seen people winning by fractions (3rd or 4th decimals) of seconds in MotoGp, FormulaOne and always wondered what it meant to the participants.But to win a competition at the 4th decimal was one heck of an experience.Analytics VidhyaThanks a lot for the competition.Really loved the way you organized it.A weekend well spent-Phani Srinath, Winner of Data Hackathon 3.XNow, if you have still not made up your mind, here is what will make the offering irresistable!As a way to impart knowledge, all our past winners of hackathons have kindly agreed to do an hour long Ask Us Anything (AUA). Yes, you heard it right  all five winners open to any questions just to help you become better at data science.What else can you ask for  exciting problem, seriously tough competition with tons of knowledge being imparted. If you let this go  you cant get it back. So one last chance to register for the D Hack its now or never. The AUA invites would be rolled out to people registered in D Hack first and later for people who have asked for itOne of the requests, we have got time and again is to have practice problems for people. We will use our hackathon platform to release various practice problems shortly. We will add a new problem every few weeks in coming months  so make sure you check out the platform regularly. Of course, you will get notifications, once you register for a hackathon.Honestly, I cant think of any place, which provides a better learning opportunity to a data science professional. But, that does not mean we can not improve it further!Let us know, what other things you would want to accelerate your data science learning and we might create it for you. Do let us know your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/10/hack-hackathon-winners-practice-problems/
"Data Scientist  Marketing  Bangalore (4-9+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|The D Hack, Ask Us Anything with past hackathon winners and practice problems|Powerful Trick to choose right models in Ensemble Learning|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist  MarketingLocation  BangaloreAbout employerConfidentialJob description: Data scientist will be part of Solutions Team based out of India and will work closely work with partners, principals, consultants and data engineers to build different scales of analytical solutions involving variety of data sources and technology for CPG and Retail clientsQualification and Skills RequiredMarketing:Desired skills and experienceWhat will make you successful?Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Marketing  Bangalore and the following details:",https://www.analyticsvidhya.com/blog/2015/10/data-scientist-marketing-bangalore-4-9-years-experience/
Powerful Trick to choose right models in Ensemble Learning,Learn everything about Analytics|Introduction|OurObjective|The Algorithm|End Notes,"|Step 1 : Find the KS of individual models|Step 2:Index all the models for easy access|Step 3: Choose the first two models as the initial selection and set a correlation limit|Step 4: Iteratively choose all the models which are not highly correlated with any of the any chosen model|Step 5: Time to check the performance of individual sequential combination|Step 6:Choose the combination where the performance peaks|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Marketing  Bangalore (4-9+ years of experience)|Job Comparison  Data Scientist vs Data Engineer vs Statistician|
Tavish Srivastava
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I hope youve followed my previous articles on ensemble modeling. In this article, Ill share a crucial trick helpful to build models usingensemble learning. This trick will teach you,How to choose the right models for your ensemble process?Are you ready? Lets Begin!Imagine the following scenario (great, if you can relate to it).You are working on a classification problem and have built 1000 machine learning models. Each of the model gives you an AUC in the range of 0.7 to 0.75 . Your task is to combine all the models together to build a more stable and more predictive model by taking up simple bag of these models.How would you answer it?To answer this question, lets try to dig into what are we really looking for in these models.We are trying to combine a set of high-performing diverse models to get a model that has higher stability and higher performance.In the sentence above, two things are essential to note:Now, that we know our model objective, lets try to quantify both these attributes.In this article Ill will choose KS-stat as the Performance Metric and Pearson Coefficient as the Diversity Metric.There is no perfect algorithm to select the right set of variables. However, in this article I have laid down a methodology which I have found extremely effective in finding the right model set. Following is the step by step methodology with the relevant codes :We start with finding the performance of individual models. You can use following code :Lets try to be effective while referencing our models. Here is a powerful way to do the same:You start with the most powerful models in your kitty. And then:Here is where you make the final comparison using the performance and the diversity factor (Pearson Coefficient)Now you have a list of models selected in the vector models_selected.Having chosen a sequence of models, now is the time to add each combination and check their performance.Here is the plot I get after executing the codeIt is quite clear from the above graph that we see significant benefits by combining around 12 diverse high-performing models. Our final KS goes up to 49 from 36 (individual model maximum).How about applying this trick to solve real life problem? Register here for our largest ever Hackathon  The D Hack and compete against more than 500 data scientists across the globe and get a chance to win $300.Ensemble model gives significant lift over individual models because it is able to combine the positive attributes of multiple models. There is no specific way to combine models together, however I have seen with experience best selections get more intuitive.Have you tried ensemble learning? If yes, did you see a significant benefit. What method did you use? Share with us your comments/queries/inputs on this article.",https://www.analyticsvidhya.com/blog/2015/10/trick-right-model-ensemble/
Job Comparison  Data Scientist vs Data Engineer vs Statistician,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Powerful Trick to choose right models in Ensemble Learning|How Amazon re-invented Data Science at Amazon AWS re:Invent 2015?|
Analytics Vidhya Content Team
|64 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Science is one of the flourishing industry. Countries and companies around the world are continuously experiencing a rush in the amount of data collected. They are determined to hire experts who can work on their data and improve their lives.Such experts are known by many names. Most popular being, Data Scientist. Among others include Data Engineers, Data Architects, Statisticians etc. But, how many of us are clear about the difference in these roles and designations?As Ive experienced, people are confused between a Data Scientist, Data Engineer and Statistics. Some end up concluding, all these people do the same job, its just their names are different.I gotastonished at hearing such answers.With these thoughts in mind, I decided to create a simple infographic to help you understandthe job roles of a Data Scientist vs Data Engineer vs Statistician. This will help you to decide the best job role for you in coming future.If you have more questions about various designations and their roles, you can always post them in our discussion portals.",https://www.analyticsvidhya.com/blog/2015/10/job-comparison-data-scientist-data-engineer-statistician/
How Amazon re-invented Data Science at Amazon AWS re:Invent 2015?,Learn everything about Analytics|Introduction|Amazon re:Invent 2015|Why am I telling you all this ?|Data Science Products|6 Must Watch Videos for a Big Data Scientist|End Notes,"AWS IoT|Amazon QuickSight|Amazon Kinesis Firehose|AWS Database Migration Service|AWS Lambda (Update)|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Job Comparison  Data Scientist vs Data Engineer vs Statistician|5 Questions which can teach you Multiple Regression (with R and Python)|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The ways of analyzing and visualizing data is changing, we must embrace this change.Let me begin with a8 line (quick) story.King Horik, hired a civil engineer to build a city center in his kingdom. He specifically instructed the engineerfor the urgency of this construction.After knowing all the details and specifications, the civil engineer promised to do so, but said, It will take a year where Ineed to bringall my men for this work. But,it would cost you more.King realized, with all engineersmen, the priceto build this city center would cost him a fortune. He had already spent enough on storing food for his citizens. He got worried. He called his son, the wisest of all men in his kingdom. He overtook the responsibility of building city center.He hired a consultant, and described their condition. The consultant offered him a price. He (consultant) said, I would build the city center at my cost. But youll have pay for every citizen entering my city center. And, the deal was made.Learningfromthe story: City Center is Data Science Applications. The consultant is Amazon. Rest is history.Amazon celebrated its 4th Annual re:Invent 2015 from 6th  9 Oct at Las Vegas, USA. Had I been around Vegas at this time of the year, I would have definitely grabbed aseat at this festival.Let me tell you what happened at this festival (in brief).At re:Invent 2015, Amazon AWS showcased its years of hard work and dedication spent in building goodincredible products. You name a technology /service, and amazon had a product to offer.Be it Internet of Things, Analytics, Databases, Storage and Content Delivery, Security and Identity, Compute, Mobile Services, Management tools, amazon totally captured worlds attentionby its unique offer pricing and services.Amazon AWS has becomethe fastest growing Multi-Billion Dollar Enterprise IT Company in this world.The guy presentingis Andy Jassy, Senior Vice President, AWS (Proof: Image below).Amazon has built great products and it has offered these products at incredible prices. But,why are you telling me all this? I bet you thought this for a moment! Well.Amazon has been the king of cloud computing and when they start rolling out products aimed for data scientists and business intelligence (BI) professionals, it is better that you watch rather than being caught off-guard!Like always, the products Amazon has come out with arent made for just organizations, but for personal use as well.And they aim to disrupt a multi-billion dollar market with deep customer dis-satisfaction. Let me tell you how!P.S  This article only highlights big data & analyticsrelated knowledge from re:Invent 2015. I do not intend to promote any product or service here.Not only the process of collecting, uploading, storing and processing data on AWS has become faster, butaddition of data-centric services has made thisa comprehensive platformfor a data scientist. Below are some products introduced at re:Invent 2015:IoT is no longer a dream. Its right here.AWS IoT enables a cloud service which allows an easy and secure connection with cars, factory floors, sensory grids, factory engines and almost anything which transmits data. Amazon has made sure that this serviceis great fit even for devices that have limited memory, processing power or battery life.AWS IoT is build up of components such as things shadows, rules engine, message broker, SDKs, things registry whose sole job is to ensure the devices stay connected irrespective of bad connectivity, lack of storage and other unfavorable conditions.The best part is, first 250,000 messages exchanged among these devices are free! ANd it is dirt cheap even after that.If this excites you, check out Dr. Werner Vogels, CTO, Amazon introducing AWS IoT.Next surprise, Amazon has madethe process of delivering Business Intelligence (BI) solutions faster (in seconds) with QuickSight.For those of you, who know the BI industry, you would know how broken it is! Before the QlikView / Tableau became popular, a BI project remained gargantuan task. It would cost you a lot, people will be working on it for years and by the time some thing is ready, the customer requirement would have changed! After spending ton of money, human resource and time, the Organization would move to next vendor with a hope that they will deliver and similar story continues for most of the projects.While solutions like QlikView and Tableau have already changed this to some extent, Amazon is setting things up for disruption.QuickSight, not only delivers faster BI solutions, but at 1/10th the cost of traditional available solution. Isnt that amazing ?This software is designed to work on many data types of collected using ad targeting, customer segmentation, forecasting & planning, marketing & sales analytics, inventory & shipment tracking and many more. In short, if you have data and you want its insights, switch to QuickSight. It will beavailable at $9 per user month for a year commitment and would be $18 for corporate users.Here arethe pricing plan:I am just waiting to give this a spin!You didnt like QuickSight? No problem!Amazons Firehose provides the facility of integrating your existing BI tools and dashboards, thereby enablingreal time analytics. It provides the easiest way to load streaming data into AWS.This software allows you to spend more time focusing on your application and less time on your infrastructure. Since, Firehose, automatically takes care of monitoring, scaling and data management for users. It can also be used with other amazon services such as Lambda, Redshift, EMR, aimed at making this process of data management reliable and faster. Check this 2 minute video.As the name suggests, this service allows you to shift data to AWS easily and securely. Infact, while migration happens, the source data remains fully operational.This service can be used with all forms of data from all types of widely used commercial and open databases. For example: You can migrate data from Oracle to Oracle, as well as Oracle Microsoft SQL Server. There is no limitation at data transferring even using heterogeneousdata sources.This includes facility such as Schema conversion tools and many other build in features which facilities successful transfer of data between databases.If you are still unaware of Lambda, let me introduce it to you!At times, you would have faced the difficulty of running code on server. Like yourserver would have failed to compute codesuccessfully, or worse ( it crashed). Atleast I have!Lambda provides you the opportunity to run code without thinking about servers. Incredible factis, you only pay for the compute time  there is not charge when your code is not running. You just need to upload your code and sit back. This software takes care ofeverything required to run and scale the code with high availability.This year, at re:Invent 2015 had amazon has announced Lambdas extended support for Python function, increased function duration, function versioning & aliasing and many more.Summing up, among all, I found these 5 products to be most usefulto a data science / big data professionals. Whether you analyze or you visualize data, these product surely will make your life simpler and happier. In case,you feel intrigued tofind about more products, you can always find them here.re:Invent wasnt just limited to product showcasing, but extended to delivering knowledge on some of the most talked about topics in data science / big data industry. Here are the top 6 videos, I found useful for a data science professional. Remember, the ways of analyzing and visualizing data is changing, we must embrace this change.1. Deep Learning  Going Beyond Machine Learning2. Data Science & Best Practices for Apache Spark3. Real-World Applications with Machine Learning4. Big Data Architectural Patterns and Best Practices5. Amazon Elastic Map Reduce6.Your First Big Data Application on AWSWith AWS re:Invent 2015, Amazon has not only launched / renewed its product line. It has challenged the traditional BI industry in the way it does best. It has also democratized IoT by launching its product.Whether these highly competitive products end up becoming the kings of their own markets?  only time will tell. But, Ill put my money on Amazon to do so.With this article, my aim was to provide you the reserve of data science knowledge from re:Invent 2015. Hopefully, Ive done justice to it. Finger crossed, though! While writing this last line, Im curious to know which product you would like the most? Do share it in the comments section below!",https://www.analyticsvidhya.com/blog/2015/10/amazon-aws-reinvent-2015-data-science/
5 Questions which can teach you Multiple Regression (with R and Python),Learn everything about Analytics|Introduction|What is Linear Regression?|How to find the best regression line?|What are the performance evaluation metrics in Regression?|What is Multi-Variate Regression?|How to implement regression in Python and R?|End Notes,"The D Hack  Double Prize Money Registrations Open Now|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How Amazon re-invented Data Science at Amazon AWS re:Invent 2015?|Data Scientist (Risk Analytics & Modeling)  Chennai (2+ years of experience)|
Sunil Ray
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A journey of thousand miles begin with a single step. In a similar way, the journey of mastering machine learning algorithms begins ideally with Regression.Itis simple to understand, and gets you started with predictive modeling quickly. While this ease is good for a beginner, I always advice them to also understand the working of regression before they start using it.Lately, I have seen a lot of beginners, who just focus on learning how to perform regression (in R or Python) butnot on the actual science behind it. I am not blaming the beginners alone. Here is a script from a 2 day course on machine learning:Running regression in Python and R doesnt take more than 3-4 lines of code. All you need to do is, pass the variables, run the script and get the predicted values. And congratulations! Youve run your first machine learning algorithm.The course literally spends no time even explaining this simple algorithm, but covers neural networks as part of the course. What a waste of resources!So, in this article, Ive explained regression in a very simple manner. I have covered the basics, so that you not only understand what is regression and how it works, but also how to compute the popular R and the science behind it.Just a word of caution, you cant use it all types of situation. Simple regression has some limitations, which can be overcome by using advanced regression techniques.Linear Regression isused forpredictive analysis. Itis a technique which explains the degree of relationship between two or more variables (multiple regression, in that case) using a best fitline / plane. Simple Linear Regression is used when we have, one independent variable and one dependent variable.Regression technique tries tofit a single line through a scatter plot (see below). The simplest form of regression with one dependent and one independent variable is defined by the formula:Y = aX + bLets understand this equationusing the scatter plot below:Above, you can see that a black line passes through the data points. Now, you carefully notice that this lineintersectsthe data points at coordinates(0,0), (4,8) and (30,60). Heres a question. Find the equation that describe this line? Your answer should be:Y= a * X +bNow, find the value of a and b?With out going in its working, the outcome after solving these equations is:a = 2, b = 0Hence, our regression equation becomes: Y= 2*X + 0 i.e. Y= 2*XHere, Slope = 8/4 =2 or 60/30 =2 and Intercept = 0 (as Y =0 when x is 0). So, equation would beY = 2*X + 0This equation is known as linear regression equation, where Y is target variable, X is input variable. a is known as slope and b as intercept.Itis used to estimatereal values (cost of houses, number of calls, total sales etc.) based on input variable(s). Here, we establish relationship between independent and dependent variables by fitting a bestline. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.Now, you might think that in above example, there can be multiple regression lines those can pass through the data points. So, how to choose the best fit line or value of co-efficients a and b.Lets look at the methods to find the best fit line.We discussed above that regression line establishes arelationship between independent and dependent variable(s). Aline which can explainthe relationship better is said to bebest fit line.In other words, the best fit line tends to return most accurate value of Y based on X i.e. causing aminimum difference between actual and predicted value of Y (lower prediction error). Make sure you understand the image below.Here are some methods which check for error:Lets evaluate performance of above discussed methods using anexample. Below I have plotted three lines(y=2.3x+4, y=1.8x+3.5 and y=2x+8) to find the relationship between y and x.Table shown belowcalculates theerror value of each data point and the total error value (E) usingthe three methods discussed above:Afterlooking at the table, the following inferences can be generated:Therefore, we can say that these coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.There are twocommon algorithms to find the right coefficients for minimum sum of squared errors, first one is Ordinary Least Sqaure (OLS, used in python library sklearn) and other one is gradient descent.As discussed above, to evaluate the performance of regression line, we should look at the minimum sum of squared errors (SSE). It works well but when it has one concern!Lets understand it using thetable shown below:Above you can see, weve removed 4 data points in right table and therefore the SSE has reduced (with same regression line). Further, if you look at the scatter plot, removed data points have almost similar relationship between x and y. It means that SSE is highly sensitive to number of data points.Other metric to evaluate the performance of linear regression is R-square and mostcommon metric to judge the performance of regression models. R measures,How much the change in output variable (y) is explained by the change in input variable(x).
R-squared is always between 0 and 1:In general, higher the R, more robust will be the model. However, there are important conditions for this guideline that Ill talk about in my future posts..Lets take the above example again and calculate the value of R-square.As you can see, R has less variation in score compare to SSE.One disadvantage of R-squared is that it can only increase as predictors are added to the regression model. This increase is artificial when predictors are not actually improving the models fit. To curethis, we use Adjusted R-squared.Adjusted R-squared is nothing but the change of R-square that adjusts the number of terms in a model. Adjusted R square calculates the proportion of the variation in the dependent variable accounted by the explanatory variables. It incorporates the models degrees of freedom. Adjusted R-squared will decrease as predictors are added if the increase in model fit does not make up for the loss of degrees of freedom. Likewise, it will increase as predictors are added if the increase in model fit is worthwhile. Adjusted R-squared should always be used with models with more than one predictor variable. It is interpreted as the proportion of total variance that is explained by the model.Lets now examine the process to deal withmultiple independent variables related to a dependent variable.Once you have identified the level of significance betweenindependent variables(IV) anddependent variables(DV), use these significant IVs to make more powerful and accurate predictions. This technique is known as Multi-variate Regression.Lets take an example here to understand this concept further.We know that, compensation of aperson depends on his age i.e. the older one gets, the higher he/she earns as compared to previous year. You build a simple regression model to explain this effect of age on a persons compensation . You obtain R2 of 27%. What does this mean?Lets try to think over it graphically.In this example, R as 27%, says, only 27% of variance in compensation is explained by Age.In other words, if you know a persons age, youll have 27% informationto make an accurate prediction about their compensation.Now, letstake an additional variable as time spent with the company to determine the current compensation. By this,R2 value increases to 37%. How do we interpret this value now?Lets understandthis graphically once again:Notice that a persons time with company holdsonly 10% responsible for his/herearning by profession. In other words, by adding this variable to our study, we improved our understanding of their compensationfrom 27% to 37%.Therefore, we learnt, by using two variables rather than one,improvedtheability to make accurate predictions about a persons salary.Things get much more complicated when your multiple independent variables are related to with each other. This phenomenon is known as Multicollinearity. This is undesirable. To avoid such situation, it is advisable to look for Variance Inflation Factor (VIF). For no multicollinearity, VIF should be ( VIF < 2).In case of high VIF, look for correlation table to find highly correlated variables and drop one of correlated ones.Along with multi-collinearity, regression suffers from Autocorrelation, Heteroskedasticity.In an multiple regression model, we try to predictHere, b1, b2, b3 bk are slopes for each independent variables X1, X2, X3.Xk and a is intercept.Example: Net worth = a+ b1 (Age) +b2 (Time with company)Linear regressionhas commonly known implementations in R packages and Python scikit-learn. Lets look at the codeof loading linear regressionmodel in R and Python below:Python CodeR CodeIn this article, we looked at linear regression from basicsfollowed by methods to find best fit line, evaluation metric, multi-variate regressionand methods toimplement in python and R. If you are new to data science, Id recommend you to master this algorithm, before proceeding to the higher ones.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/regression-python-beginners/
"Data Scientist (Risk Analytics & Modeling)  Chennai (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|5 Questions which can teach you Multiple Regression (with R and Python)|Manager  Analytics  Gurgaon & Chennai (5-8 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Scientist (Risk Analytics & Modeling)Location  ChennaiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSenior Analyst/Data Science/Risk Analytics & Modelling  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/10/senior-analyst-data-science-risk-analytics-modelling-bangalore-2-years-experience/
"Manager  Analytics  Gurgaon & Chennai (5-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Risk Analytics & Modeling)  Chennai (2+ years of experience)|Assistant Manager  Analytics  Gurgaon & Chennai (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation Manager  AnalyticsLocation  Gurgaon & ChennaiAbout employer  ConfidentialResponsibilitiesScope of WorkKey Results AreasKey relationshipsQualification and Skills RequiredSkills:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Manager  Analytics  Gurgaon & Chennai and the following details:,https://www.analyticsvidhya.com/blog/2015/10/manager-analytics-gurgaon-chennai-5-8-years-experience/
"Assistant Manager  Analytics  Gurgaon & Chennai (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Analytics  Gurgaon & Chennai (5-8 years of experience)|Interview with Data Scientist- Gregory Piatetsky Shapiro, Ph.D, President KDnuggets|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Assistant Manager  AnalyticsLocation  Gurgaon & ChennaiAbout employer  ConfidentialResponsibilitiesScope of WorkKey Results AreasKey relationshipsQualification and Skills RequiredSkills:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Assistant Manager  Analytics  Gurgaon & Chennai and the following details:,https://www.analyticsvidhya.com/blog/2015/10/assistant-manager-analytics-gurgaon-chennai-2-4-years-experience/
"Interview with Data Scientist- Gregory Piatetsky Shapiro, Ph.D, President KDnuggets",Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror join our Facebook Group|Share this:|Like this:|Related Articles|Assistant Manager  Analytics  Gurgaon & Chennai (2-4 years of experience)|Quick Guide to learn Statistics for R Users (with Titanic Data Set)|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",P.S. We just announced our upcoming competition  The D Hack. Register to fight it out for the top spots against some of the best data scientists across the globe!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How do you feel when you get a chance to interview your role model? Some one who has not only gone through the journey himself, but has also seen and helped thousands of people trying to undergo the same journey.I was having this similar feeling when I got a chance to interview someone I regard as my guru. He isthe first creator of a data science community on internet and an aspiration to the young folks in data science / analytics. Yes  you are absolutely right, I got a chance to interview Dr. Gregory Piatetsky Shapiro, co-founder of KDD conference and ACM SIGKDD association for Knowledge Discovery and Data Mining, and President of KDnuggets and yes! an awesome Data Scientist.Both of us are driven by the same passionto help people learn data science / analytics. It took me a few years, before I could reach out to Gregory for an interview personally. Not because he is difficult to reach out to, but because I wanted to make full use of this interaction with him.Do you know? :Gregory found KDnuggets even before Sergey and Larry createdGoogle.I asked lots of questions to Gregory, just like anybody at my place would have done, and honestly speaking, I relished (and benefited) from each word which came from Gregory.Here are the excerpts of my curious conversation with Gregory:KJ: Hi Gregory! It is a pleasure to interview you personally. You are one of my role models and I cant thank you enough for giving me this opportunity. Before I actually come down to the main questions, can you tell us about yourself as a person?Gregory:I consider myself a Data Scientist  someone who is interested in understanding how the world works. I have analytical & coding skills to try to understand it using data and various tools and algorithms. I was born in Moscow where I went to a top math school, but quickly realized that I dont have enough math talent and switched to computers. I came to US when I was 19 to study Computer Science at NYU and got a PhD in 1984, with a topic of applying Machine Learning to Databases. Since then I moved from being a software developer to a database / machine learning researcher to Chief Scientist at a couple of startups to a data mining consultant. Now, all of my work time is spent on running KDnuggets website and social networks. I have two sons  both finished college and are working. My wife and I love to travel to interesting places, especially with vineyards.KJ: You have been running KDnuggets for close to 20 years now. KDnuggets started even before Google came into existence. How did it all start?Gregory:I started KDnuggets (first called Knowledge Discovery Nuggets) as an email newsletter sent to about 50 researchers which attended KDD-93 workshop (which I organized), the second workshop on Knowledge Discovery in Data. In 1994, I also created a website Knowledge Discovery Mine (no longer active) to hold a directory of data mining software and other useful content, and in 1997, when I left GTE Labs and joined what would now be called a fintech startup, I moved the website to www.kdnuggets.com. Over the years, the focus changed from research-oriented directory, with a focus on data mining software, with emails sent every 2-3 weeks, to current focus on Industry, Big Data, and Data Science, with daily blog publication. KDnuggets recently exceeded 200,000 unique monthly visitors), and @kdnuggets was voted Best Twitter on Big Data. We are also active on KDnuggets LinkedIn Group and KDnuggets Facebook Page.I have great help from a strong team of editors and writers, based all over the world  including Singapore, Dublin, California, Chicago, and Toronto. I am based in Boston.I described my journey to data mining in more detail in Journeys to Data Mining: Experiences from 15 Renowned Researchers , Mohamed Medhat Gaber (Editor)Springer, 2012  here is an Excerpt from The Journey of Knowledge Discovery by Gregory Piatetsky-ShapiroKJ:You also co-founded ACM SIGKDD, the leading professional organization for Knowledge Discovery and Data Mining. How did that start?Gregory: After I organized the first 3 KDD workshops on Knowledge Discovery in Data (1989, 1991, and 1993, I was feeling tired from the activity. One of my best decisions was to recruit Usama Fayyad to organize 1994 workshop and then Usama, Ramasamy Uthurusamy, and I converted the workshop to a full conference in 1995. However, the workshops and conference were part of AAAI (American Assoc. on Artificial Intelligence) and we felt that the nascent field of KDD or data mining was very interdisciplinary and needed participation from database researchers, statisticians, and others , who would not come to an AI conference. In 1998, Won Kim who was previously a chair of ACM SIGMOD (database research group) approached Usama and me and proposed to form a new group under the umbrella of ACM  the largest group of computing professionals in the US. Thus SIGKDD was born and it has been running KDD conferences ever since. KDD Conferences remain the top research conference in the field.I have been involved in organizing KDD workshops and conferences for 20 years  from 1989 to 2009, when I served as KDD Chair. This has been entirely a volunteer work, frequently time consuming and occasionally frustrating (like the time I had to deal with VAT tax issues for KDD-09 conference in Paris), but my reward has been in feeling that I was doing a useful work for the entire Data Science community. Now feel like a proud parent whose baby has grown and is completely independent, and very much enjoy not doing any organizing.KJ:In Hindsight, what was one thing, which you would change in your journey?Gregory: Use a shorter and simpler name for a website that is easier to spell than KDnuggets!KJ:The analysts in industry cant stop debating about SAS vs. R vs. Python? What is your take on it? What is your advice to newbies in the field?Gregory: Python is certainly easier to learn and embed in other programs  you can see it was designed by a great programmer.R used to have a huge advantage in different statistical models and visualization , especially any packages made by Hadley Wickham, but Python is catching up, especially with scikit-learn and iPython.SAS however is not going away and remains important in the industry .I have done a recent poll on this.Compared to2013 poll results, the 2015 results show much higher stability, about 88% of R users in 2014 stayed with R and 91% stayed with Python. Percentage of primary R and primary Python users have grown, while percentage of users who chose Other or None have declined. You might find this helpful: Four main languages in analytics / data miningKJ: Experts in industry are divided whether AI will evolve as a threat. What is your view on this?Gregory:Just in the last few years we saw the amazing progress of AI and Machine Learning, especially Deep Learning. Google on smartphones now has better voice recognition than many humans and Deep Learning has exceeded human performance on many image recognition tasks. Some people claim that AI will never reach human level, but I remember that people were saying that AI will never beat a master in chess. Then it happened. Critics were saying AI will never beat a world champion in Chess, until IBM Deep Blue defeated Kasparov in 1997. So, predicting that AI will never be able to do X has consistently been a losing bet. I think it is inevitable that AI will reach or exceed human levels, but I hope not for the next 20-30 years.My main concern is not that AI will want to exterminate people like in Terminator movies but that AI will automate and remove many jobs, leading to a much higher level of unemployment. This will cause huge upheavals and require massive social redistribution.AI can also automate war  we already see armed drones.At the same time AI has a huge potential upside, for making life easier and better  just try to navigate a city without a GPS ! New personalized medicine will make lives better and longer.My generation has essentially failed to deal with climate change.Now it will be up to your generation to find the right balance of technology and social responsibility, and perhaps Data Science can help find better answers to worlds problems.KJ:How do you think Analytics and Data Science would evolve in India as an industry and as a career option for people in the coming years? What parallels can we draw from the U.S. and other evolved markets?Gregory:India has a huge number of talented people and I think Analytics and Data Science are an excellent career option for technically inclined young people.Indian is second only to US among KDnuggets readers.KJ: I will ask for a personal advice as well  What would be your advice to me and what pearls of wisdom can you share with me to evolve Analytics Vidhya community further?Gregory:I think you have done a great job already growing Analytics Vidhya, and I dont think you need any advice from me. (but you may consider a name that is easier to remember for non-Indian readers KJ: Thanks Gregory for taking time out of your busy schedule. I would like to thank you on behalf of our entire community and would hope that we would gain from your mentorship and guidance in future as well.This was like a dream come true for me. I was always curious about Gregorys journey, his perspective and what would be advice to some one like me. Not only this interview has helped me understand him as a person, but has motivated me to do a lot more than what I am doing.It leaves me smiling assome one who just got a chance to meet and talk to his role model.I cant thank Gregory enough for taking time out of his busy schedule.",https://www.analyticsvidhya.com/blog/2015/10/interview-data-scientist-gregory-piatetsky-shapiro-president-kdnuggets/
Quick Guide to learn Statistics for R Users (with Titanic Data Set),Learn everything about Analytics|Introduction|What will you learn from this article?|DescriptiveStatistics|Inferential Statistics|End Notes,"Measure of Central Tendency|Measure of Spread|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Interview with Data Scientist- Gregory Piatetsky Shapiro, Ph.D, President KDnuggets|Senior Manager (Analytics)  Bangalore (4-10 years of experience)|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People are keen to pursue their career as a data scientist. And why shouldnt they be? After all, this comes witha pride of holding the sexiest job of this century.But, in order to become one, you must master statistics in great depth.Statistics lies at the heart of data science. Think of statistics as the first brick laid to build a monument. You cant build great monumentsuntil you place a strong foundation. Experts say, If you struggle withdeciphering the statistical coefficients , youd have tough time ahead in data science.Dont worry! I shall help you decipher some of these statistical mysteries in this post. I will use R as my tool to perform the calculations, but you can choose any tool of your convenience.Statistics has two arms: Descriptive Statistics and Inferential Statistics. Both arms deal with analyzing data, but with different objectives. Descriptive statistics help us describe large data sets, while inferential statistics help us draw inference about population from a sample of this population.I havent explained these concepts in detail as they are already discussed in understanding statistics. Ive laid emphasis on the practical aspect of statistics rather than theoretical. Hence, I strongly recommend to refer to this article to maximum benefit from what is described here.Ive considered titanicdata set for practical purpose.Note: This article is best suited for beginners and intermediates in data science, who are facing difficulty in dealing with statistical concepts.Descriptive, the term originated from the word describe means explain. It is used to explainthe characteristics of a data set using measures such asmean, mode, median, variance etc. These measures summarize data and help you draw meaningful patterns.Descriptive Statistics can further be divided into two parts:Lets understand these measures one by one:Mean (Average):It is calculated as the sum of all values in a data set / number(count) of values. It can be used with both discrete and continuous variables, but, it is often used with continuous variables.A major disadvantage of using mean is, it is highly affected by outliers. Also, mean cannot be used while dealing with categorical (nominal) valuesandskewed distributions. In such cases, we make use of mode/ median.Lets find, the average value of Fare (variable in data set) charged to board Titanic Ship.Interpretation: On an average, passengers have paid $32 to board the titanic.Mode: Think of Mode as repeat. Mode is nothing but the value that has been repeated the maximum number of times in a data set. In other words, it is the value of a observation having highest frequency.R does not have any inbuilt function to find out Mode. However, it does have a mode() function which returns the type or storage mode of an object instead.Lets see how to find out the age withhighest frequency.Interpretation:Most common age among passengers on Titanic was 24 years. As you can see, there were30 passengers on board who are 24 years old (highest among all).Median:Think of Median as the mid point ofall values in data set. Median divides the data set into two equal halvessuch that one halfis less than median and the other greater.Median values are lessaffected by outliers. Also, it is based on all observations and easy to compute. Lets compute the median of Fare.Interpretation:The mid value of Fare variable is $14.45. This means $14.45 divides the data into two halves. If you compute its range, along with this statistic, youll find the this variable is highly skewed (dont worry, if you dont understand  we will discuss these terms shortly).Measure of spread represents the variability in a sample or population. Using this measure, you get to know the amount of variation among observations, which can later be treated using suitable measures. Often, people relate measure of spread with box plot summary. Ive discussed it as well.Range: It represents the complete extent of data. It shows the lowest value and the highest value in a set of observation. We can calculate range using this simple function:Quartile:It divides the data set into 4 equal parts using Q1 (First Quartile), Q2 (Second Quartile) and Q3 (Third Quartile). Lets understand it using an example, assume we have a numerical variable having values from 1 to 100. Here, value for Q1 would be 25 (value at 25th percentile), value for Q2 would be 50 (value at 50th percentile i.e. median) andvalue for Q3 would be 75 (value at 75th percentile).Quartiles are well understood when used with box plots. Box Plots is a five number summary which represents Minimum, Maximum, Q1, Q2 and Q3. Lets find out.Interpretation: After analyzing thisbox plot of Class and Age, we can infer that the median value of Class 1 passengers is higher than that of Class 2 and Class 3. It means Class 2 and Class 3 had younger passengers than Class 1. Infact, class 3 had the youngest age group of passengers. The tiny circles outside the range of Class 2 and Class 3 represents the outlier values.Variance and Standard Deviation: Variance and Standard Deviations are closely related. In simple words:SQRT(variance) = Standard DeviationVariance is the average of squared difference from mean. In R, you dont need to get into mathematics of these measures since we have inbuilt functions to calculate these values. Lets find out the variance & standard deviation for Fare variable.Distributions:Simply put, distribution refers to the way population is spread on a particular dimension. This article helps you understand various terms used to describe shapes of distribution.Lets look at the distribution of Fare using a bar chart:Question: How is this distribution skewed? Positive or Negative?Some modeling techniques (like regressions) make an assumption about the underlying distributions of the population. In case your population does not fulfill them, you may need to make some transformations to improve the results of your models.Live Update:Register Now in The D Hack and win Amazon Vouchers worth Rs. 20,000 (~$300)These statistical measures is used to make inferences about larger population from sample data. In other words, we can say that inferential statistical measures help us to make judgement for population on the basis of insights generated from sample. Lets find out the inference which we can draw from titanic data set:Hypothesis Testing: You can quickly revise your basics of hypothesis testing with this guide to master hypothesis in statistics.Lets say, we want to check the significance of variable Pclass for hypothesis testing . Lets assume that upper class( Class 1)has better chances of survival than its population.To verifyour assumption, letsuse z-test and see, if passengers of upper classhave bettersurvival rate than its population.H: There is no significant difference in the chances of survival of upper and lower classH1: There is a better chance of survival for upper class passengersFinally, our z value comes out to be 7.42 which affirms our suggestion that upper classhas better chances of survial than itspopulation. High z value denotes that p value will be very low.Similarly, you can check the significance of other variables.Chi-Square Test:This test is used to derive the statistical significance of relationship between the variables. Also, it tests whether the evidence in the sample is strong enough to generalize that the relationship for a larger population as well. Chi-square is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. It returns probability for the computed chi-square distribution with the degree of freedom.Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence.R has a inbuilt function for chi-square test. Lets now check this test in our data set :Interpretation:Since their p values < 0.05, we can infer, Sex and Pclass are highly significant variables and must be included in our final modeling stage. But, this is just the beginning. As you continue checking for other variables, youll find other significant variables as well. Dont forget feature engineering. It can totally change your game.Correlation: Correlation determines the level of association between two variables. You can quickly revise your basics from 7 most asked questions on correlation.Lets find out the level of association in the variables of this data set. It is considered to be a good practice to check out scatter plot among the variables to find access their correlation. For example: Lets plot Fare vs Age.Can you see any correlation in these variables? I certainly dont because there is no pattern (positive or negative) which can suggest us of their correlation. Lets confirm our result now:Interpretation: Just like we assumed, this statistics further corroborates our assumption that there exist very low association between fare and age. This test confirm the strength of correlation is meager 9.6%. Similarly, you can find association among other variables. If you find any highly correlated variable, you can also drop of these variables from your final model as it wont add lot of new information in the model.Regression: Regression is used to access the level of relationship between two or more variables. It is driven by predictor (independent variable) and response(dependent variable). For a quick refresher, I suggest you to read 7 types of regression techniques.Here are few things you must acknowledge before proceeding for regression:Dependent Variable(Y): SurvivalIndependent Variable(X): Pclass, Age, Sex, SibSp, Parch, Fare, EmbarkedInterpretation: As expected, the output is disappointing. AIC criterion, which measures the goodness of fit is extremelyhigh(727.39). Lower the AIC value, better will be the model. You can read more about AIC here.There are two things which we learn here:When I started with statistics, I knew the theory of these measures, but I was not clear about practical uses of these measures and how they help. In this article, Ive illustrated the used of statistical concepts using a practice problem (titanic).Some of the statistical measure arent so useful when used in isolation. For example, Mean.You know the mean of one variable. What do you do now? Nothing, until you compare it with other variables. Hence, you should be sure when to use which measure. I found a quick guide on, when to use which statistical measure?, published by California State University. This can surely be of your help.If there is any further knowledge which you think can be added to this article, please feel free to share it in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/inferential-descriptive-statistics-beginners-r/
"Senior Manager (Analytics)  Bangalore (4-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Quick Guide to learn Statistics for R Users (with Titanic Data Set)|AM- Business Analytics  Gurgaon (3-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Manager (Analytics)Location  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSenior Manager (Analytics) and the following details:,https://www.analyticsvidhya.com/blog/2015/10/senior-manager-analytics-bangalore-4-10-years-experience/
"AM- Business Analytics  Gurgaon (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Manager (Analytics)  Bangalore (4-10 years of experience)|Understanding basics of Recommendation Engines (with case study)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  AM- Business AnalyticsLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asAM- Business Analytics and the following details:,https://www.analyticsvidhya.com/blog/2015/10/am-business-analytics-gurgaon-3-6-years-experience/
Understanding basics of Recommendation Engines (with case study),Learn everything about Analytics|Introduction|What are Recommendation Engines ?|Types of Recommendation Engine:|Content Based Recommendations|Collaborative Filtering|End notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|AM- Business Analytics  Gurgaon (3-6 years of experience)|8 Productivity hacks for Data Scientists & Business Analysts|
Shivam Bansal
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ever wondered, what algorithmgoogleuses to maximize its target ads revenue?. What about the e-commerce websiteswhich advocates you through options such as people who bought this also bought this. OrHow does Facebook automatically suggest us to tag friends in pictures?The answer is Recommendation Engines. With the growing amount of information on world wide web and with significant rise number of users, it becomesincreasingly important for companiesto search, map and provide them with the relevant chunk of information according to their preferences and tastes.Companies nowadays are building smart and intelligent recommendation engines by studying the past behavior of their users. Hence providing them recommendations and choices of their interest in terms of Relevant Job postings, Movies of Interest, Suggested Videos, Facebook friends that you may know and People who bought this also bought this etc.Oftentermed as Recommender Systems, they are simple algorithms whichaim to provide the most relevant and accurate items to the user by filtering useful stuff from of a huge pool of information base. Recommendation engines discovers data patterns in the data set by learning consumers choices and produces the outcomes that co-relates to their needs and interests.In this article, we will explain two types of recommendation algorithms that are also used by most of the tech giants like Google and Facebook in their advanced recommender system modules.As a typical business problem,Consider a scenario of an e-commerce website which sells thousandsof smartphones. With growing number of customers every day, the task in hand is to showcase the best choices of smartphones to the users according to their tastes and preferences.To understand how recommendation engine works, lets slice the data into a sample set of five smartphones with two major features Battery and Display. The five smartphones have following properties:Using these characteristics, we can create an Item  Feature Matrix. Value in the cell represents the rating of the smartphone feature out of 1.Item  Feature Matrix
Our sample set also consist of four active users with their preferences.Using their interests, we can create a User  Feature Matrix as follows:We have two matrices: Item  Feature and User  Feature. We can create the recommendation of smartphones for ourusers using following algorithms:Content based systems, recommends item based on a similarity comparison between the content of the items and a users profile. The feature of items are mapped with feature of users in order to obtain user  item similarity. The top matched pairs are given as recommendations, as demonstrated below: Representing every user by a feature vector:Also, every item representation as a feature vector:and so onContent Based Item  User Mapping Recommendations are given by the equation:For User U1 (Aman), Smartphone recommendation is:Smartphones S2, S3 and S1 has the highest recommendation scores, Hence S2, S3 and S1 are recommended to Aman.Content-based recommendation lacks in detecting inter dependencies or complex behaviors. For example: People might like smartphones with Good Display, only if it has retina display and wouldntotherwise.Collaborative Filtering algorithm considers User Behaviour for recommending items. They exploit behaviour of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.We have a similar User  Feature Matrix as content based:User  Feature MatrixThis time we dont know features of the items but we have user behaviour. i.e. How the Users brought/rated the existing items.User- Behaviour Matrixwhere values of the behaviour matrix can be described as:This user behavior matrix can be used to derive unknown features of the most liked items. Lets try to derive features of S1 using this behavior matrix.S1 is rated 5 by U1S1 is rated 4.5 by U2S1 rating by U3 & U4 are not knownUsing this information Feature Vector of S1 can be assumed as:and the equations are:solving these equations, gives x1 = 5.5 and x2 = 0.5Similarly,Now all the feature vectors are known. Hence the recommendations will be mappings of User Feature Vectors and Item Feature Vectors. Thusfor Aman, based on his preferences and behaviours, recommendation will be:which comes out the be S1, S2 and S3 again. Since S1 and S2 are already rated by Aman, So we will recommend him a new smartphone S3.In the above example where we assumed that there are two primary features of S1 as governed by the users who rated it. In real case, we end up with more number of features than this. For example, if we had data for all the N number of users who rated S1, then feature vector look like:In this article, we learnt abouttwo types of Recommendation Engines: Content based recommendations and Collaborative Recommendations. There exists more advanced techniques like ALS : Alternating Least Square Recommendations and Hybrid Recommendation Engines. The Recommendation Engines have become an important need with the growing information space. Did you find this article useful ? Have you also worked on recommender systems? Share your opinions / views in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/
8 Productivity hacks for Data Scientists & Business Analysts,Learn everything about Analytics|Introduction|What causes these iterations in analysis?|Impact of these productivity killers|Tips to avoid unhealthy iterations and increasing productivity|End Notes:,"Tip 1: Focus on big problems (and big problems only):|Tip 2: Create a presentation of your analysis before you start (with possible layouts and branches)|Tip 3: Define data requirements upfront:|Tip 4: Make sure your analysis is reproducible:|Tip 5: Keep standard libraries of codes ready and accessible:|Tip 6: Similarly, keep alibrary of intermediate datamarts:|Tip 7: Always use an holdout sample / cross-validation to avoid over-fitting|Tip 8: Work in chunks and take breaks regularly:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Understanding basics of Recommendation Engines (with case study)|AM- Fraud Analytics  Gurgaon (3-6 years of experience)|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I was catching up with one of my friends from a past organization. She had always been interested in data science, but was only able to break into it about 10 months ago. She had joined an organization as a data scientist and was clearly learning a lot in her (relatively) new role. Over our conversation, she mentioned a fact / question, which has stuck with me since then. She said that irrespective of how well she performs, she ends up doing every project / analysis multiple times before it is satisfactory for her manager. She also mentioned that these iterations cause her work to take lot more time than it should actually require in hindsight!Does that sound familiar to you? Do you repeat your analysis multiple times before it becomes presentable and throws out answers to the required questions? Or you end up writing codes for similar activities again and again? If it does, you are at the right place. Ill share a few ways in which you can increase your productivity and kill these unwanted iterations.P.S. Dont get me wrong here. I am not saying that iterations are bad in entirety. In fact, data science as a subject requires you to do things in iterations at times. But not all iterations are healthy and it is those unhealthy iterations we need to avoid. They will be the focus of this article.I am defining an iteration healthy / unhealthy using the following definition. Any iteration in analysis, which ishappening due to any reason apart from flow of new information is an unhealthy iteration (there is one exception to this, which is mentioned below). Let me explain a few such scenarios:On the other hand, if your iteration is happening because you built a model 6 months back and you now have new information, itis a healthy iteration. Another scenario for healthy iteration is when you deliberately start with simple models to develop better understanding and then build complex models.Now, I am sure I have not covered all possible scenarios here, but I am sure these examples are good enough for you to judge whether an iteration in your analysis is healthy or unhealthy.Lets get this clear  no one wants unhealthy iterations and productivity killers in their analysis. Missing out on a few variables initially and then running the entire analysis again after collecting them would not interest any data scientist. Also, there is no fun in doing the same analysis again!Thisproductivity loss and iterations create frustation and dis-satisfaction among the analysts / data scientists and hence should be avoided at all costs.I am sure every organization has a lot of small problems, which can be solved using data. But, they are not the best use of the data scientists. Focus on just those 3-4 problems, which can have huge impact on the organization. These problems would be challenging and would give you the maximum leverage for your analysis as well. You should not try to solve a smaller problem, if the bigger problem is unsolved.This might sound trivial, but the number of organizations which make this mistake are non-trivial! I see banks working on marketing analytics when their risk scoring can be improved. Or insurance companies trying to build a reward program for agents, when their customer retention can be improved using analytics.I do this all the time and I cant tell how beneficial this is. The first thing you should do as soon as you start a project is to layout the presentation of your analysis. This might sound counter-intuitive to start with, but once you develop this habit, it can reduceyour project turn around time to a fraction of what it takes otherwise.So, what do you do?You layout the storyin form of a presentation / a word document or just a story on pen and paper. The actual form is immaterial. What is important is that you layout all possible outcomesat the start of the journey. For example, if you are looking to reduce the charge offs, a structure to lay out on your presentation would be something like this:Next, you can take up each factor and define what do you need to see to conclude whether it has driven the increase in charge-off and how will you go about doing this? For example, if the charge-offs for the bank have increased because of increase in credit limit of customers, you would:Once you have done this with every possible branch of your analysis, you have created a good starting point for yourself.This flows from the last step directly. If you have laid out the analysis comprehensively, you would know the data requirements by the end of it. Here are a few tips to help you out:Again, this might sound as a simple tip  but you see both the beginners as well as the advanced people falter on it. The beginners would perform steps in excel, which would include copy paste of data. For the advanced users, any work done through command line interface might not be reproducible.Similarly, you need to extra cautious while working with notebooks. You should control your urge to go back and change any previous step which uses the data set which has been computed later in the flow. Notebooks can be very powerful, if the flow is maintained. If the flow is not maintained, they can be very tardy as well.There is no point in re-writing codes for simple operations again and again. Not only it takes extra time, but it might lead to possible syntax errors. Another tip to make the most of this is to create a library of these common operations and share it across your entire team.This will not only make sure the entire team uses the same code, but also make them more efficient.A lot of times, you need same piece of information again and again. For example, you will need total customer spend on a credit card for several analysis and reporting. While you can calculate it every time you need from the transaction tables, it is much better to again create intermediate datamarts of these tables to save time and efforts spent in creating these tables. Similarly, think of summary tables for marketing campaigns. There is no point in re-inventing the wheels every time.A lot of beginners under-estimate the power of holdout or cross-validation. A lot of tend to believe that if train is sufficiently large, there are hardly / no chances of over-fitting and hence a cross-validation or holdout sample is not required.More often that not, this turns out to be blooper in the end. Dont believe me  check out Kaggle public and private leader boards for any competition. You will always find a few entries in top 10 who end up dropping their ranks as they ended up overfitting their solutions. And you would hope these to be more advanced data scientists.When do I work the best? Its when I provide myself a 2-3 hours window to work on a problem / project. You cant multi-task as a data scientist. You need to be focuses on a single problem at a time to make sure you get the best out of yourself. 2-3 hour chunks work best for me, but you can decide yours.So, those were some productivity hacks I use for increasing my productivity. I cant emphasize the importance of getting things right the first time enough. You have to get into a habit of getting it right every time  that is what will make you an awesome data scientist.Do you have any tips which makes you more productive? If yes, share it with us in comments below.",https://www.analyticsvidhya.com/blog/2015/10/productivity-hacks-data-scientists-business-analysts/
"AM- Fraud Analytics  Gurgaon (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|8 Productivity hacks for Data Scientists & Business Analysts|Senior Consultant  Ahmedabad (4-8 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  AM- Fraud AnalyticsLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asAM- Fraud Analytics  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/10/am-fraud-analytics-gurgaon-3-6-years-experience/
"Senior Consultant  Ahmedabad (4-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|AM- Fraud Analytics  Gurgaon (3-6 years of experience)|Enterprise & Data science  Analyst  Bangalore (2-3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior ConsultantLocation  AhmedabadAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSenior Consultant  Ahmedabad and the following details:,https://www.analyticsvidhya.com/blog/2015/10/senior-consultant-ahmedabad-4-8-years-experience/
"Enterprise & Data science  Analyst  Bangalore (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Consultant  Ahmedabad (4-8 years of experience)|News  Full Time / Part Time Big Data and Analytics Program at SP Jain School of Global Management|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Enterprise & Data science  AnalystLocation  BangaloreAbout employer  ConfidentialDescriptionQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asEnterprise & Data science  Analyst  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/10/enterprise-data-science-analyst-bangalore-2-3-years-experience/
News  Full Time / Part Time Big Data and Analytics Program at SP Jain School of Global Management,Learn everything about Analytics|Introduction,"|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Enterprise & Data science  Analyst  Bangalore (2-3 years of experience)|Cheatsheet  11 Steps for Data Exploration in R (with codes)|
Kunal Jain
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Note: The enrollments for current batch are closed. You can apply for the February batch 2016. Apply Here,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As you read this, freshly generated terabytes of would have been collected this second and stored in huge database management systems. With new devices & instruments of data collection, it has become increasingly essential for organizations to make use of data and bring innovation, productivity and better decision making. After all, data is useless until it is processed & explored.To deal with this deluge of structured & unstructured data, companies are constantly looking out for Big Data Experts to utilize this data. But, the shortage of these resources is daunting and frustating for these companies.In order to address this gap, SP Jain School of Global Management has proudly announced their Big Data and Analytics Program. Like always, we started getting queries about the program from our readers. Hence, we decided to meet their Director of the program  Dr. Mahendra Mehta. Here is a brief summary of our discussion with him:AV: How did the program come into being? Do you see a lot of demand from your employers in this area already?Dr. Mahendra MehtaIt hardly needs to be re-asserted that Big Data is poised to registerhigh growth numbers in terms of employment opportunities inthe next 10-15 years. We haveobserved that companies are struggling to fulfill the demand of data science professionals in their ranks. Almost all major universities in the US have initiated a masters level program in big data and analytics. But what about an Indian student or professional who wishes to acquire the pertinent skill set for a career in big data?A number of universities and institutes in India are offering a program in big data, however we realized that there is no program that is intensive and that focused on covering the fundamentals as well as the essential skill sets required to be employed as a data science professional.We conceived the Big Data and Analytics Program at SP Jain School of Global Management as an intensive program whichincorporates the essentialconcepts, knowledge and skill sets of Big Data which are highly sought by companies today.We do not want to create just tool experts who are adept at mechanical usage without knowing the fundamentals inside the box. Nor, do we want them to be theoretical masters with no practical skill sets. Therefore,we have laid equal emphasis on theoretical concepts, practical tools as well as hands-on case studies.AV: Who is the target audience of this program?I would say any and everybody who wants to make a career in the field of big data and analytics can participate. This field is expanding so rapidly that there will be openings across all levels of experienced candidates in a few years from now. This program is best suited for candidates having these three qualities:First: Background in a quantitative field such as mathematics, economics, statistics, engineering, physics or chemistry. While an undergraduate level is preferable, even students who have studied mathematics till 10+2 level and who still remember their concepts are suitable for the course.Second: Exposure to principles of programming. This need not be professional exposure. Even if the student has done some programming at school or college level, they are ready for the course. And even if they have not done any programming in school or college, a beginners course on programming in languages such as C++, Java or Python, offered on any of the online learning platforms such as coursera, edx or codecademy will lay the suitable programming background for them.Third: A serious desire to learn big data and analytics. Our course is aimed at making students readily employable in the field of big data and analytics. We intend to equipthem with a set of diverse skills. Learning them will require a serious commitment and investment of time and effort from the students. Our course is not for the casual learner. We do not provide an overview or just theoretical knowledge. We are offering an extremely intensive course with extensive coverage.AV: What is the structure of the program  total classroom time, expectation from the candidate and the areas covered as part of the course?The program is being offered in two formats  a full-time program, that runs for six months and a part-time program that runs for a year. The total number of contact hours for each format is the same  400 hours. We designed the full-time format for those who have not yet embarked on their professional life yet such as students who have completed or are in the final year of their bachelors , masters or doctoral degree. Working professionals who want to take a break from their career to re-equip themselves with new skills are also welcome to join the full-time program. The classes of the full-time program will be held on weekdays i.e. Monday to Friday. For industry professionals who cannot afford to take a break, we have designed the part-time format. The classes of the part-time program will be held on weekends. There is no distinction between the two formats except the number of contact hours a week and hence the time-span of the program. None of the programs is superior or inferior to the other, they are just variations tailored to different needs of the students. 90% of the modules will have the same instructors on both the part-time and the full-time mode.The program seeks to equip students with all the essential tools to turn them into data science professionals. Hence the syllabus is expansive and the coverage is deep. The modules can be broadly divided into 5 categories:Basics: Statistics, Python, R, Relational DB & SQL, Excel, Tableau, UbuntuTools & Technologies: Hadoop, Data warehousing, NoSQL, Cloud computingAnalytics: Machine Learning, Data Visualization,Specialization: Natural Language Processing, Recommender SystemGeneral topics: Business metrics, Data science in start-ups, Design patterns in Statistical Computing, Basics of Problem solving, Stream processing, Dashboard, Data Security, Business applications in Data science, Future of Data ScienceIn addition to these technical modules, students will also undertake the professional readiness program, which is also undertaken by our MBA students. The professional readiness program trains students on the necessary interviewing and personal skills and imparts leadership, decision-making and other real-world business skills.AV: What is the teaching methodology adopted for this Program  Online / Offline / Hybrid?We believe that the best learning takes place in classrooms. For this reason, all modules will be covered in their entirety in classrooms and labs with actual face-to-face contact with professors. The professors will also be readily available outside class-room hours for the students during the duration of the module, and can be connected viaemail and phone after the completion of the module. Each lecture hall is equipped with state of the art recording equipment. Each lecture will be recorded and uploaded online for students for later reference.AV: How much of the program is devoted to Industry Interaction and Live Project? Again are these professionals across the globe or just in India?Roughly 20% of the program is hands-on and projects. In these modules, professionals working in big data will guide students on working in real-world problems incorporating live data.AV: Is there a capstone project / industry project at the end of the course? If yes, how much time are people expected to spend on it?Yes, we do have a live-project component of about 10% at the end of the program. That means about 40 hours will be devoted to an industry project at the end of the curriculum.AV: Tell us about the Profile of the Faculty. Considering the fact that this program is supported by SP Jain Global, do we get to see faculties coming from abroad to commence classes?The faculty is a mix of analytics industry professionals and renowned academicians. We have also invited faculty members from overseas universities to teach some of the modules. Some of our key faculty members are:Dr Mahendra MehtaDr Mahendra Mehta is the director of the program and was formerly the Head of Analytics at Citibank, India. Dr Mehta has been a visiting faculty with S P Jain for over 10 years now. Dr Mehta has published several papers on machine learning, neural networks and was a radar scientist with HUL for 15 years. Dr Mehta has a PhD in electrical engineering from IIT Bombay.Prof Sunil LakdawalaDr. Sunil D. Lakdawala, a Ph.D. from Yale University-USA and post graduate from I.I.T.-Mumbai is a visiting faculty with S P Jain Global Institute of management. He has over 20 years of experience in IT with various IT consultancy companies including CMC Ltd. and Tech Span India Ltd. He has specialized in the area of Business Intelligence System (Data Warehousing and Data Analysis Techniques like OLAP, Data Mining) and has a rich experience in consultancy, project execution and training in that area.Mr. Tobias SetzMr. Setz has a Master and Bachelor of Science degree from ETH Zurich and is currently a Research Assistant at ETH Zurich, Institute for Theoretical Physics. He is one of the key members of the Rmetrics Association and maintainer of its broad open source software libraries. Mr Setz has conducted several R/Rmetrics Workshops in Europe which include Computational Finance and Financial Engineering, Basic R, Advanced R Programming and webinars on advanced statistics and visualization (Zurich & Mumbai)Dr Satish PatilDr Satish Patil holds a PhD from the University of Minnesota, USA, and has 8+ years of experience in the field of Pharma & Healthcare domain. He is an inventor of 4 US patents over the last 4 years, has helped several companies & startups with Big Data & Analytics solutions using Big Data technologies, advanced machine-learning algorithms and statistical models.His core competency lies in Big Data, applied math and statistical analysis, machine learning, artificial intelligence and data visualisation.AV: Is there a lab / virtual lab environment which S.P.Jain has created to support this course? If not, how is the student expected to learn in Big Data environment?Yes, the course will be conducted at the dedicated labs and simulation centers at the new Lower Parel campus of S P Jain School of Global Management. The labs will enable students to work in a windows as well as Linux based environment and are equipped with high-end systems with all essential data science tools and technologies like Python, SQL, and Hadoop etc.AV: How about placements? Would placement support be available for this program? Can we expect international placements as part of this program?S P Jain extends placement support to all candidates who enroll in the BDAP program. We also offer a professional readiness program that equips them with the right skills to negotiate their way through the selection process. Like I said, before the curriculum has received an enthusiastic response from several companies working on big data and analytics. These include start-ups, established companies planning to start their own Big data business as well as specialist analytics consulting and outsourcing firms.We would like to thank Dr. Mahendra Mehta and his team for sparing time for this interview. S.P.Jain School of Global Management has surely upped the ante by coming up with this unique offering. 6 month full time program is an interesting format and something being tried for the first time in industry. We thank S.P.Jain Institute of Global Management for joining the party and wish them all the best for running these courses.If you have any query, concern, feedback regarding this course, please feel free to leave your response in the comments section below.",https://www.analyticsvidhya.com/blog/2015/10/news-sp-jain-global-big-data-analytics-program/
Cheatsheet  11 Steps for Data Exploration in R (with codes),Learn everything about Analytics|Introduction|Download Here,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|News  Full Time / Part Time Big Data and Analytics Program at SP Jain School of Global Management|Notes, impressions, experience and excitement from PyCon India 2015, Bengaluru|
Analytics Vidhya Content Team
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you wish to build an impeccable predictive model, trust me, neither any programming language nor any machine learning algorithm can award it to you unless you perform data exploration.Just like a baby learns to walk before running, every data scientist should learn to explore data prior to getting accustomed to algorithms. Data Exploration has paramount importance in predictive modeling.Data Exploration not only uncoversthe hidden trends and insights, but also allows you to take the first steps towards building a highly accurate model.Considering the popularity of R Programming and its fervid use in data science, Ive created a cheat sheet of data exploration stages in R. This cheat sheet is highly recommended for beginners who can perform data exploration faster using these handy codes. All you need to do is, customize the codes according your need.Note: This Cheat Sheet is also available for Download in PDF version below.",https://www.analyticsvidhya.com/blog/2015/10/cheatsheet-11-steps-data-exploration-with-codes/
"Notes, impressions, experience and excitement from PyCon India 2015, Bengaluru",Learn everything about Analytics,"Awesome turnout at the event|Profile of the attendees|Keynotes & KG award|The nominated talks  Let Python-someness begin|The lightening talks and Shakthimaan|Open Space on Data Science:|My Notes and Takeaways:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Cheatsheet  11 Steps for Data Exploration in R (with codes)|Business Analyst / Sr. Business Analyst  Gurgaon (2+ years of experience)|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If there is one conference I love to follow, it is PyCon! We have been following the PyCon U.S. for last 2 years and I have learnt a lot by watching the videos of the talks and the workshops happening in PyCon. This time I decided to attend the India chapter of PyCon  PyCon India 2015, which happened in Bengaluru from 2nd  4th October 2015.Due to some earlier commitments, I could not attend the workshops on 2nd. But, I made sure to make the most of the event in remaining 2 days. Following were some notes and impressions I carried from PyCon India:It was heartening to see the turn out at the event. We got to know later that more than 1500 people registered for the event with ~20% females. This was probably the largest tech event I have personally attended in last 10 years.For the entire 2 days, the event received jam packed auditoriums with people full of enthusiasm and energy. Here is a snippet of 4th morning, I gathered from Twitter:My initial impression was that the conferencehad higher presence of students / developers in early stages of career than I had thought. However, I think this was more to do with the fact that it was difficult to reach to venue on the morning of event, if you are coming from outside Bengaluru (the airport was about 2 hours drive from the venue). As the conference proceeded, I could see more senior developers, data scientists and pythonsits making their way to the event.I loved the fact that key notes on both the days and KG award were given to people leading and actively involved in Python community. Here are the details about the speakers and their brief profiles:Coming to the main part of the event, the talks were nominated and voted by the community. You can see the complete list of talks here. Each of the talk had high quality content and participation from the audience. Here are some of the talks associated with Data Science, which you can watch whenever the videos are up:Solving Logical Puzzles with Natural Language ProcessingMachine learning techniques for building a large scale production ready prediction system using PythonAnalyzing arguments during a debate using Natural Language Processing in PythonExplore Big Data using Simple Python Code and Cloud EnvironmentOne of the thing which stood out during the event was that the entire event was run on time.Lightening talks are essentially small 3 minute talks by people to share their work, passion or any thing else (of course related to Python) with the audience. They were crisp talks, some focusing on fun, while other on their hacks and information.A special mention to the lightening talk from Shakthimaan  he delivered nursery rhymes & yet another lightening talkflawlessly. While I cant replicate the experience and his skill in delivering the talk, you can still go ahead and read the content.Since, there was a lot of enthusiasm in the room for data science, we also made a lightening talk about what we do at Analytics Vidhya.Open space was another awesome format, where people could gather outside the talks and discuss about any topic they want with others. These were floated on a board and people could join the open talks on their own. We found one of Data Science and met some of our followers and data hackers here. Here are some stills from the open space on data science:It was pleasure to meet Bargava& Dorai in person among other followers. Bargava also prepared a poster on Ensemble modeling, which was put on display.Like all events, PyCon India came to a close. I walked out refreshed from the event carrying a big smile and lot of warmth from our followers. The event left me inspired and motivated for putting in more efforts in time to come! See you every one again at PyCon India 2016!",https://www.analyticsvidhya.com/blog/2015/10/notes-impressions-experience-excitement-pycon-india-2015/
"Business Analyst / Sr. Business Analyst  Gurgaon (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Notes, impressions, experience and excitement from PyCon India 2015, Bengaluru|Data Scientist/Senior Data Scientist  Bangalore (2+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Analyst / Sr. Business AnalystLocation  GurgaonAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Analyst / Sr. Business Analyst  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/10/business-analyst-sr-business-analyst-gurgaon-2-years-experience/
"Data Scientist/Senior Data Scientist  Bangalore (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst / Sr. Business Analyst  Gurgaon (2+ years of experience)|Building a Logistic Regression model from scratch|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Scientist/Senior Data ScientistLocation  BangaloreAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist/Senior Data Scientist  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2015/10/data-scientistsenior-data-scientist-bangalore-2-years-experience/
Building a Logistic Regression model from scratch,Learn everything about Analytics|Why am I asking you to build a Logistic Regression from scratch?|Refreshers of mathematics terminology|The Code,"Logit Function:|Likelihood Function|Log Likelihood|Derivative of Likelihood Function|Hessian Matrix (second derivative)|Newton Raphson Method|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist/Senior Data Scientist  Bangalore (2+ years of experience)|Beginners guide to Design of Experiments (with case study on banner advertisement)|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you understand how does logistic regression work? If your answer is yes, I have a challenge for you to solve. Here is an extremely simple logistic problem.X = { 1,2,3,4,5,6,7,8,9,10}Y = {0,0,0,0,1,0,1,0,1,1}Here is the catch : YOU CANNOT USE ANY PREDEFINED LOGISTIC FUNCTION!Here is a small survey which I did with professionals with 1-3 years of experience in analytics industry (my sample size is ~200).I was amazed to see such low percent of analyst who actually knows what goes behind the scene. We have now moved towards a generation where we are comfortable to see logistic regression also as a black box. In this article, I aim to kill this problem for once and all. The objective of the article is to bring out how logistic regression can be made without using inbuilt functions and not to give an introduction on Logistic regression.Logistic regression is an estimation of Logit function. Logit function is simply a log of odds in favor of the event. This function creates a s-shaped curve withthe probability estimate, which is very similar to the required step wise function. Here goes the first definition :Logistic regression is an estimate of a logit function. Here is how the logit function looks like:Now that you know what we are trying to estimate, next is the definition of the function we are trying to optimize to get the estimates of coefficient. This function is analogous to the square of error in linear regression and is known as the likelihood function. Here goes our next definition.Given the complicated derivative of the likelihood function, we consider a monotonic function which can replicate the likelihood function and simplify derivative. This is the log of likelihood function. Here goes the next definition.Finally we have the derivatives of log likelihood function. Following are the first and second derivative of log likelihood function.Finally, we are looking to solve the following equation.As we now have all the derivative, we will finally apply the Newton Raphson method to converge to optimal solution. Here is a recap of Newton Raphson method.Here is a R code which can help you make your own logistic functionLets get our functions right.This might seem like a simple exercise, but I feel that this is extremely important before you start using Logistic as a black box. As an exercise you should try making these calculations using a gradient descent method. Also, for people conversant with Python, here is a small challenge to you  can you write a Python code for the larger community and share it in comments below?",https://www.analyticsvidhya.com/blog/2015/10/basics-logistic-regression/
Beginners guide to Design of Experiments (with case study on banner advertisement),"Learn everything about Analytics|Introduction|The Concept of Testing (A/B, Split-Run, Flip-Flop and Test vs. Control)|Problems with Traditional Testing|The Concept of Design of Experiments|Design of Experiments without Interaction Effects|Design of Experiments with Interaction Effects|End Notes","Split  Run Testing|Flip-Flop Testing|Test vs. Control|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror join our Facebook Group.|Share this:|Like this:|Related Articles|Building a Logistic Regression model from scratch|Analytics Manager  Predictive Modelling and Business Analysis  Gurgaon (3-5 years of experience)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",About the Authors,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you visit a supermarket, you might feeloverwhelmed with the discounts and free gifts that you get with your purchase.Have you ever imagined, what makes a company decide if you will be excited more by discounts or free gift? How could they even know about you so closely?As analytics capabilities continue toevolve across businesses and geographies, it has been observed that marketingmanagers expect analytics departments to provide insights into numerous questions such as Do our customers love a free gift more than a discount? Do our customers respond to advertising that contain the picture of a sports icon? so on and many moreIt requires an analyst to delve deep into the data to find these answers, using all the available tools and techniques. But, what if we do not have the data? If the company has never leveraged a popular personalityfor advertising or if it has never offered a free gift, then how will data help us answer the question?Asituation where relevant data remainsunavailable is quite common these days. When encountered with such a situation, we either take help of expert judgment, or try to identify suitable proxies or ask the customer. Once we execute the latter, we obtainthe relevant data required to answer the question of interest. The process of asking the customer entails performingexperiments or tests where one is able to read the result and obtain answersforthe questions of interest.A/B testing, split-run testing or tests vs. control comparisons are common methodologies that are adopted to understand the impact of single factor on customer behaviour.In order to test the effectiveness of a marketing communication (mostly print advertisement), one can either use a split-run testing or a ip-op testing. Split-run testing is by far the most effective way of testing a print advertisement. For running a split-run testing, two different versions of the same advertisement, each with a different identication number, are placed in the publication as a split insertion on the same date.This will ensure that exactly half of the publications will carry version one of the advertisement and the other half will carry the second version. Hence, the results of the split-run test can be thought of as two advertisements run on a random sample of the publication. The way the advertisements are inserted ensures that the samples are absolutely random in every respect. A very similar concept can be used for testing website banner advertisements as well.In a case, wherea magazine does not offer the exibility of running a split-run campaign but has a separate regional publication for various regions, then one can use the region-1 publication for one version of the advertisement and the region-2 publication for another. This form of testing is called ip-op testing. It is an approximation of a split-run testing. The biggest shortcoming of this testing is that the two samples are not random and hence, there can be an inherent regional bias in the test results.A control group is dened as a group of customers whichare identical to the customers andeligible for a campaign or any other targeted marketing action. However, theyarenot subjected to anyaction under consideration. The behaviour of customers in the control group is compared with the behaviour of customers who are subjected to the marketing action. This comparison provides a good understanding of the impact of the marketing action in question.The testing methodologies mentioned above provide robust answers for incremental impact of a single marketing intervention (or factor) one at a time. Then, what about the situation when the factors are too many in number?In such case, one needs to conduct a large number of tests to ascertain the impact of each intervention (or factor). As we know,it takes significant amount of time and money to read and infer the results of a test, thus it is advisable that one shouldtest the impact of multiple factors,do something different so as to ensure that one can generate all the required learnings within the limited budget that is available.What does one need to do differently? Lets find out using an example.Hence, in case one needs to test the impact of multiple factors, one needs to do something different so as to ensure that one can generate all the required learnings within the limited budget that is available. What does one need to do differently? Lets find out using an example discussed in following sections.Marketers often need to test the impact of a wide range of targeting, advertising, promotion, pricing and product options to find out optimal combination of factorsandobtain all the desired results at the minimum possible cost.As marketing budget is always limited, it becomes impossible to test all combinations of every marketing parameter. Therefore, marketers often build a testing framework which helps them in identifying the critical few learning that they would like to derive out of theavailabletest budget. In many cases, the concept of design of experiments is widely used in building the testing framework.Design of experiments or DoE is a common analytical technique implementedto design the right testing framework. To illustrate the use of design of experiments, letsbegin withweb banner advertising.There are multiple factors whichaffect the successes of a banner advertisement. It is important to quantify the success metric for a banner advertisement. The most common success metric that is used is called the Click Through Rate (CTR). Click through rate is a very simple metric which is calculated as: Number of visitors clicking the link in the advertisement divided by the number of visitors who are exposed to the advertisement.The success of a banner advertisement depends on numerous factorssuch as:website where the advertisement is displayed (possibly the most important), content of the advertisement, the placement of the advertisement etc. With available combination of advertising variables, the concepts of DoE can be very accurately applied and measured in this scenario.Enough of theory I guess, lets understand this concept practically now!For simplicity, Iveconsider an advertisement, which consists of the following features:This example involves the following parameters.The parameters (mentioned above) are also referred to as factors. The values that a parameter or factor takes is often referred to as levels or attributes. For example Position of the picture is a parameter or factor, and the values that it takes i.e. Left, Right and Middle are levels/attributes.Figure-1 illustrates the combinations (other than the presence or absence of animation).Figure-1: Depiction of the parameters of banner advertisementIn order to ascertain the effectiveness of all these components, it is critical to conduct experiments where visitors are exposed to all possible combinations shownabove and the effect of the same is measured on the click through rate.Table-1 depicts the total possible combinations. The cells marked in grey are the ones which take a value of zero for that particular combination. For example:Table-1: All possible combinations of the parametersIt can be observed that,there are 3 possible positions of the picture, 2 possible positions of the call to action link, 2 configurations with regards to animation (presence or absence) and 2 possible placements on the web site (left or right). Hence there will be 3*2*2*2 = 24 combinations that one could have; this is a large number of possible combinations to explore individually.Marketers have used the concept of design of experiments to limit the number of combinations (out of the set of all possible combinations) whichneeds to be tested to make meaningful inferences. Tounderstand, how design of experiments can help one in limiting the number of combinations that need to be tested, one needs to understand the effects of each attribute or level separately and the effect of these attributes acting in tandem.The levels of a particular parameter or factor are used as variables for constructing the response function for each combination listed in Table-1. For example the factor Position of picture comprises of 3 levels. Therefore, due to degree of freedom constraints, it would require two variables to construct the response equation; any two of the levels can be used as binary variables. In case of position, one can use Left and Right as two binary variables. If the picture position is on the left then the binary variable Left takes the value of 1 otherwise it takes the value of 0. If the picture position is on the right then the binary variable Right takes the value of 1, otherwise it takes the value of 0. If the picture position is in the middle, then both the variables Left and Right takes the value 0.Similarly, Icoulduse 1 variable each for the other parameters (as all the other parameters consists of two levels each). If one assumes no interaction effect between the factors, then the generic response function can be written as:In this expression CTR represents the probability of response or click through rate. s represent the effect of each attribute or level on probability of response.Based on past experience, it has been found that in most cases, responses can be predicted by using a logistic function. The generic response function needs to be applied to each design combination. The resulting function for each design combination is depicted in Table-2.Table-2: The Response Equation for all Possible Combinations of the ParametersFrom the table, it can be observed that if one tests combination C4 (ln(CTR4/(1-CTR4))=+1 +3 ) and C23 (ln(CTR23/(1-CTR23))= +5), then one could easily estimate the click through rate for combination C3 (ln(CTR3/(1-CTR3))=+1 +3 +5). It can be seen that:(ln(CTR4/(1-CTR4) ) + (ln(CTR23/(1-CTR23) ) = (ln(CTR3/(1-CTR3) )This feature is the key benefit of a properly designed experiment or test. By performing limited number of tests, it is possible to infer the results of some combinations, which have not been tested.A case, where one tests all the combinations involved is referred to as full factorial design. On the other hand, as mentioned above, if the marketer is able to eliminate certain combinations, and test a limited set of combinations, then the same is referred to as partial factorial designTable-3 illustrates how a limited set of experiments that can be used to compute all the required test results.Table-3: The Partial Factorial DesignThe analytical objective involves estimating the coefficients , 1, 2, 3, 4, 5. The following combinations can be used to estimate the coefficients:It can be observed that by conducting only 7 experiments (C4, C8, C12, C14, C6, C10 and C15), one can obtain all the information that can be obtained by conducting 24 experiments. Hence, the concept of design of experiments hasused to reduce the experiments from 24 to 7.The property mentioned above, is the major benefit of partial factorial designwhereone can obtain the required learning without conducting all the possible experiments. However, as mentioned earlier, this approach assumes that there exists no interaction between the factors. It will be a worthwhile exercise to find out the minimum number of experiments that one will have to perform if presence of interaction is considered.As a critic of the partial factorial approach, one could argue that the combination of an animation and placement of the advertisement to the right of the website would be more effective in conjunction, because most viewers tend to focus on the right side of the screen. This implies that the interaction between placement and animation needs to be taken into account. Hence the generic response function would take the following form:It would be worthwhile to find out the minimum number of experiments that one will have to conduct if one assumes the presence of interaction effects. It can be easily seen, that it is difficult to limit the number of experiments or tests that needs to be conducted if there are significant number of interactions.To generate the maximum learning from any test program, it is best to adopt a full factorial test design whereby all the possible combinations are tested. However, because of cost constraints a partial factorial design is often favoured. While adopting a partial factorial design, appropriate assumptions about interaction effects need to be put into place to limit the number of experiments that one needs to conduct.Based on prior business knowledge one can eliminate certain interactions, thereby reducing the number of tests that should be performed. In this case, if one assumes that the only interaction effect that exists is between the placement of the advertisement and animation, then it will be interesting to find out the number of tests that needs to be conducted to estimate all the coefficients involved.In this article, Ive elaborated the concept used behind Design of Experiments. By now, you would have got an intuition about the strategies that companies use to decide the best mode of advertisement for them. Earlier, companies use to face too much trouble in deriving positive returns on marketing budget, but this technique has not only saved million of hard cash, but has also provided a prudent method to reap benefits intelligently.Did you find this article useful? Have you ever made use of this concept at work? What was your experience?Ill be happy to hear from you in the comments section below.Sandhya Kuruganti and Hindol Basu are authors of a book on business analytics titled Business Analytics: Applications to Consumer Marketing, recently published by McGraw Hill. The book is available on Flipkart and Amazon India/UK/Canada. They are seasoned analytics professionals with a collective industry experience of more than 30 years.",https://www.analyticsvidhya.com/blog/2015/10/guide-design-of-experiments-case-study/
"Analytics Manager  Predictive Modelling and Business Analysis  Gurgaon (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Beginners guide to Design of Experiments (with case study on banner advertisement)|5 Easy questions on Ensemble Modeling everyone should know|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytics Manager  Predictive Modelling and Business AnalysisLocation  GurgaonAbout employer  ConfidentialResponsibilitiesScope of WorkKey Results AreasKey relationshipsQualification and Skills RequiredSkills:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asAnalytics Manager  Predictive Modelling and Business Analysis  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/10/analytics-manager-predictive-modelling-business-analysis-gurgaon-3-5-years-experience/
5 Easy questions on Ensemble Modeling everyone should know,"Learn everything about Analytics|Introduction|Which are the common questions (related to Ensemble Models)?|1. What is an Ensemble Model?|2. What are Bagging, Boosting and Stacking?|3. Can weensemble multiplemodels of same ML algorithm?|4. How can we identify the weights of different models for ensemble?|5. What are the benefits of ensemble model?|End Note","|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Analytics Manager  Predictive Modelling and Business Analysis  Gurgaon (3-5 years of experience)|Analytics Manager BI  Gurgaon (3-5 years of experience)|
Sunil Ray
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If youve ever participated in data science competitions, you must be awareof the pivotal role that ensemble modeling plays. In fact, it is being said that ensemble modeling offers one of the most convincing way to build highly accurate predictive models. Theavailability of bagging and boosting algorithms further embellishes this method to produce awesome accuracy level.So, next time when you build a predictive model, do consider using this algorithm. You would definitely pat my back for this suggestion. And, if youve already mastered this method, great. Id love to hear your experience about ensemble modeling in the comments section below.For the rest, I am sharing some of the most commonly asked questions on ensemble modeling. If you ever wish to evaluate any persons knowledge on ensemble, you can daringly ask these questions & check his / her knowledge.In addition, these are among the easiest questions, hence you cant dare to get them wrong!After analyzingvarious data science forums, I have identified the 5 most common questions related to ensemble modeling.These questions are highlyrelevant to data scientists new to ensemble modeling. Here are the questions:Lets discuss each question in detail.Lets try to understand it by solving a classification challenge.Problem: Set rules for classification of spam emailsSolution:We can generate various rules for classification of spam emails, lets look at the some of them:Above, Ive listed some common rules for filtering the SPAM e-mails. Doyou think that all these rules individually canpredict the correctclass?Most of us would say no  And thats true! Combining these rules will provide robust prediction as compared to prediction done by individual rules. This is the principle of Ensemble Modeling.Ensemble model combines multiple individual (diverse) models together and delivers superior prediction power.If you want to relate this to real life, a group of people are likely tomake better decisions compared toindividuals, especially when group members come from diverse background. The same is true with machine learning.Basically, an ensemble is a supervised learning technique for combining multipleweak learners/ modelsto produce a strong learner. Ensemble modelworks better, when we ensemble models with low correlation.A good example of how ensemble methods are commonly used to solve data science problems is the random forest algorithm (having multiple CART models). It performs better compared to individual CART model by classifying a new object where each tree gives votes for that class and the forest chooses the classification having the most votes (over all the trees in the forest). In case of regression, it takes the average of outputs of different trees.You can also follow this article Basics of Ensemble Learning Explained in Simple English for more knowledge on ensemble modeling.Lets look at each of these individually and try to understand the differences between these terms:Bagging(Bootstrap Aggregating) is an ensemble method. First, we create randomsamples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.Boosting provides sequential learning of the predictors. The first predictoris learned on the whole data set, while the following are learnt on the training set based on the performance of the previous one. Itstarts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy.Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well.Most common example of boosting is AdaBoost and Gradient Boosting. You can also look at these articles to know more about boosting algorithms.Getting smart with Machine Learning  AdaBoost and Gradient BoostLearn Gradient Boosting Algorithm for better predictions (with codes in R)Stackingworks in two phases. First, we use multiple base classifiersto predict the class. Second, anew learner is usedto combine their predictions with the aim of reducing the generalization error.Yes, we can combine multiple models of same ML algorithms, butcombining multiple predictions generated by different algorithms would normally give you better predictions. It is due to the diversification or independent nature as compared to each other. For example, the predictions of a random forest, a KNN, and a Naive Bayesmay be combined to create a stronger final prediction set as compared to combining three random forest model. The key to creating a powerful ensemble is model diversity. An ensemble with two techniques that are very similar in nature will perform poorly than a more diverse model set.Example:Lets say we have three models (A, B and C).A, B and C haveprediction accuracy of 85%, 80% and 55% respectively. But A and B are found to be highly correlated where as C is meagerly correlated with both A and B. Should we combine A and B? No, we shouldnt, because these models are highly correlated. Hence,we will not combine these two as this ensemble will not help to reduce any generalization error. I would prefer to combine A & C or B & C.One of the most common challenge with ensemble modeling is to find optimal weights to ensemble base models. In general, weassume equal weight for all models and takes the average of predictions. But, is this the best way to deal with this challenge?There are various methods to find the optimal weight for combining all base learners. These methods provide a fair understanding about finding the right weight. I am listing some of the methods below:You can also look atthe winning solution of Kaggle / data science competitions to understand other methods to deal with this challenge.There are two major benefits of Ensemble models:The aggregate opinion of a multiplemodels is less noisy than other models. In finance, we called it Diversification a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is also why your models will be better withensemble of models rather than individual. One of the caution with ensemble models are over fitting although bagging takes care of it largely.In this article, we have looked at the 5 frequently asked questions on Ensemble models.While answering these questions, we havediscussed about Ensemble Models, Methods of Ensemble, Why should we ensemble diverse models?, Methods to identify optimal weight for ensemble and finally Benefits. I would suggest you to look at the top 5 solutions of data science competitions and seetheir ensemble approaches to have better understanding and practice a lot. It will help you to understand what works or what doesnt.",https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/
"Analytics Manager BI  Gurgaon (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|5 Easy questions on Ensemble Modeling everyone should know|Damn Good Hiring Path to get yourself hired as a Data Scientist|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytics Manager  BILocation  GurgaonAbout employer  ConfidentialResponsibilitiesScope of WorkKey Results AreasQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asAnalytics Manager  BI  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2015/09/analytics-manager-gurgaon-delhi-ncr-3-5-years-experience/
Damn Good Hiring Path to get yourself hired as a Data Scientist,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Analytics Manager BI  Gurgaon (3-5 years of experience)|Hacks to perform faster Text Mining in R|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The race to become a data scientist doesnt end at just mastering R or Python. In fact, it starts from there. A Data Scientist is majorly characterized by three skills namely decision making, analytical thinking & structured thinking. It is being said, if a person possess these three skills sets, rest everything can be taught in no time.But what if, you dont possess any of these or you are not sure if you really have them? I believe that can be learnt too. But, needs a special training. Keep calm, this training is available below for Free!According to a recent trend, it has been observed that out of every 10 candidates appearing for the job of data scientist, only 1-2 candidates get the job. So, what about the rest? They get rejected! When asked recruiters about the reason of rejection, all of them stated one reason i.e. Lack of Structured Thinking.Im sure this would be depressing for candidates. Its like, you are an avid football fan and yet you dont know who Lionel Messi is. Ive met manystudents who claim to put in 100% efforts to prepare for job interviews, yet they fail to succeed.In fact, a similar incident happened with Sara, 24 years old aspiring data scientist. Failures couldnt suppress her ambitions, and she finally grabbed the job of data scientist in one of the largest firms in the world.Boost your chances to become a Data Scientist  Download Guide Here",https://www.analyticsvidhya.com/blog/2015/09/damn-good-hiring-path-hired-data-scientist/
Hacks to perform faster Text Mining in R,Learn everything about Analytics|Introduction|Top 4 Hacks in R|End Notes,"1. Keyword Match Algorithm|2. Word Match Algorithm|3. General Expressions|4. Word Association:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Damn Good Hiring Path to get yourself hired as a Data Scientist|Running scalable Data Science on Cloud with R & Python|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data science demands versatility. Move away from your regular methods, challenge your ways of working, explore new ways of doing things more efficiently. On reminiscing about my old days, my initial years in data science, I had also got trapped by this devil of complacency. At one point, I was not challenging myself enough. I wasnt experimenting with the ways of doing work. I accepted the things as they were, until I realized Complacency is a state of mind that exists only in retrospective: it has to be shattered before being ascertained. Now, whenever possible, I try to challenge my ways of working with a purpose of doing it faster and more efficient. It helps me to discover new ways of working in data science.Text Mining, is one of the most frequent yet challenging exercise faced by beginners in data science / analytics experts. The biggest challenge is one needs to thoroughly assess the underlying patterns in text, that too manually. For example: it is pretty common to delete numbers from the text before we do any kind of text mining. But what if we want to extract something like 24/7. Hence, the text cleansing exercise is highly personalized as per the objective of the exercise and the type of text patterns.Majorly, we work on two aspects of Text Mining:You may find numerous ways on internet to do sentiment analysis. However, subject extraction is very specific to the context. In this article, I have shared the top 4 hacks applied in the industry to do subject extraction in R. For ease, Ive also highlighted the strength and weakness associated with each trick.This is the most powerful tool to do text mining. Lets first look at the code in R to execute this stepNow lets try to see the strengths and weaknesses of this algorithm.Strengths
Weaknesses
This is the fix for the second weakness (mis-classified cases) in the previous algorithm. In this algorithm, we try to match words instead of keywords. Here is the R-code :Strengths
Weaknesses
This methods needs extensive research on the sentence structures. For ease of understanding, Ive taken an uncomplicated example of www.dummyvalue.com. Here is the code :Strengths
Weaknesses
I bet, this method is good enough to challenge you intellectually. So, that you could work on it, instead of giving away the entire code, Ive provided the step by step methods to do the same. If you still find it difficult, mention your request for code in the comment section below.Strengths
Weaknesses
Hope you find these 4 hacks useful enough to speed up your text mining process. Id encourage you to take a shot on the last algorithm code and share it in the comment box below. This list is no way exhaustive of what all can be done in subject extraction.All these algorithms can be used together on the same text to boost up the performance. However, in those cases you need to create decision points of when to use which algorithm.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/learn-top-4-hacks-perform-text-mining-faster/
Running scalable Data Science on Cloud with R & Python,Learn everything about Analytics|Introduction|Why run Data Science on Cloud?|Options to run Data Science in cloud:|Challenges in running data science on cloud:|End Notes,"Amazon Web Services (AWS)|Azure Machine Learning|IBM BlueMix:|Sense.io:|Domino DataLabs|DataJoy|PythonAnywhere|If you like what you just read & want to learn more on Big Data,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Hacks to perform faster Text Mining in R|Build a Predictive Model in 10 Minutes (using Python)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The complexity indata science is increasing by the day. This complexity is driven by three fundamental factors:So, in summary, we are generating far more data (and you are becoming a data point as you read this article!), we can store it at a low cost and can run computations and simulations on this data at a low cost!So, why do we even need to run data science on cloud? You might raise this question that if a laptop can pack 64 GB RAM, do we even need cloud for data science? And the answer is a big YESfor a variety of reasons. Here are a few of them:You can also read about components of cloud computing here.Now that you understand the need of cloud computing for data science. Let us look at various options to run R and Python on the cloud.Amazon is the king of cloud computing space. They have the largest market share, very good documentation, hassle free environment which can scale up quickly. You can launch a machine with R or RStudio as mentioned in this article. If you float a Linux server, it will come with Python pre-installed. You can install the additional libraries and modules you need.Because it is the most popular choice, it has a ecosystem and it is easier (than other alternatives) to find resources with right experience. On the flip side, Amazon is usually more expensive compared to some other options. It also does not provide. Also, the machine learning service is not available for Asia Pacific for some reason. So, if you are from these regions, you should select a server based in North America or launch and create your virtual machine on the cloud.If AWS is the champion, Azure is the challenger. Microsoft has definitely upped its efforts in providing an interface to execute end to end data science and machine learning workflows. You can set up machine learning workflow with their studio, float JuPyTer notebooks on the cloud or use their ML APIs directly.Microsoft has provided a free e-book and a MOOC on virtual academy to get you started.If Amazon and Microsoft have grown organically in their cloud presence, IBM has some different ideas.IBM acquired BlueMix and has started marketing its services aggressively lately. The offering is not as straight forward as AWS and Azure, but can still be used by setting up notebooks on the cloud.It will also be interesting how the data science community uses the APIs provided by Watson.If all what I have written above sounds too complicated, you should check out Sense. Sense projects can be deployed on click of a button. They offer services based on R, Python, Spark, Julia and Impala, flexibility to collaborate with teams and share analysis. Check out this video to have a first hand view of the offering:Domino isbased in San Francisco and provides a secure environment with support of languages like R, Python, Julia and Matlab. The platform provides version control and features to make collaboration and sharing across teams a seamless process.While cloud computing comes with its own benefits, there are a few challenges as well. I dont think these would stop increased usage of cloud in long term, but they can act as hurdles at times.Cloud computing is set togain more penetration for the benefits it offers and it is only a matter of time when some of these services become a norm (if they are not already). Hope you find these services useful and they come in handy, when you need them.",https://www.analyticsvidhya.com/blog/2015/09/scalable-data-science-cloud-python-r/
Build a Predictive Model in 10 Minutes (using Python),Learn everything about Analytics|Introduction|Breaking Down the process of Predictive Modeling|Lets start putting this intoaction|End Notes,"Stage 1: Descriptive Analysis / Data Exploration:|Stage 2:Data Treatment (Missing values treatment):|Stage 3.Data Modelling:|Stage 4. Estimation of Performance:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Running scalable Data Science on Cloud with R & Python|(Senior) Big Data Engineer  Bangalore  (4-8 years of Experience)|
Sunil Ray
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I came across this strategic virtue from Sun Tzu recently:What has this to do with a data science blog? This is the essence of how you win competitions and hackathons. You come in the competition better prepared than the competitors, you execute quickly, learn and iterate to bring out the best in you.Last week, we published Perfect way to build a Predictive Model in less than 10 minutes using R. Any one can guess a quick follow up to this article. Given the rise of Python in last few years and its simplicity, it makes sense to have this tool kit ready for the Pythonists in the data science world.I will follow similar structure as previous article with my additional inputs at different stages of model building. These two articles will help you to build your first predictive model faster with better power. Most of the top data scientists and Kagglers build their firsteffective model quickly and submit. This not only helps them get a head start on the leader board, but also provides a bench mark solution to beat.I always focus on investing qualitytime during initial phase of model building like hypothesis generation / brain storming session(s) / discussion(s) or understanding the domain. All these activities help me to relate to the problem, which eventually leads me to design more powerful business solutions. There are good reasons why you should spend this time up front:This stage will need a quality time so I am not mentioning the timeline here, I would recommend you to make this as a standard practice. It will help you to build a better predictive models and result in less iteration of work at later stages. Lets look at the remaining stages in first model build with timelines:P.S. This is the split of time spentonly for the first model buildLets go through the process step by step (with estimates of time spent in each step):In my initial days as data scientist, data exploration used to take a lot of time for me.With time, I have automated a lot of operations on the data. Given that data prep takes up 50% of the work in building a first model, the benefits of automation are obvious. You can look at 7 Steps of data exploration to look at the most common operations ofdata exploration.Tavish has already mentioned in his article that with advanced machine learning tools coming in race, time taken to perform this task has been significantly reduced. Since this is our first benchmark model, we do away with any kind of feature engineering. Hence, the time you might need to do descriptive analysis is restricted to know missing values and big features which are directly visible. In my methodology, you will need 2 minutes to complete this step (Assumption,100,000 observations in data set).Theoperations I perform for my first model include:There are various ways to deal with it. For our first model, we will focus on the smart and quick techniques to build your first effective model (These are already discussed byTavish in his article, I am adding a few methods)With such simple methods of data treatment, you can reduce the time to treat data to 3-4 minutes.I recommend to use any one ofGBM/Random Forest techniques, depending on the business problem. These two techniques are extremely effective to create a benchmark solution. I have seen data scientist are using these two methods often as their first model and in some cases it acts as a final model also. This will take maximum amount of time (~4-5 minutes).There are various methods to validate your model performance, I would suggest you to divide your train data set into Train and validate (ideally 70:30) and build model based on 70% of train data set. Now,cross-validate it using 30% of validate data set and evaluate the performance using evaluation metric.This finally takes 1-2 minutes to execute and document.Intent of this article is not towin the competition, but to establish a benchmark for our self. Lets look at the python codes to perform above steps and build your first model with higher impact.I have assumed you have done all the hypothesis generation first and you are good with basic data science usingpython. I am illustrating this with an example of data science challenge.Lets look at the structure:Step 1 : Import required libraries and read test and train data set. Append both.Step 2:Step 2 of the framework is not required in Python. On to the next step.Step 3: View the column names / summary of the datasetStep 4: Identify the a) ID variables b) Target variables c) Categorical Variables d) Numerical Variables e) Other VariablesStep 5 :Identify the variables with missing values and create a flag for thoseStep 6 : Impute Missing valuesStep7 :Create a label encoders for categorical variables and split the data set to train & test, further split the train data set to Train and ValidateStep 8: Pass the imputed and dummy (missing values flags) variables into the modelling process. I am using random forest to predict the classStep 9: Check performance and make predictionsAnd Submit!Hopefully, this article would give you a start to make your own 10-min scoring code. Most of the masters on Kaggle and the best scientists on our hackathons have these codes ready and fire their first submission before making a detailed analysis. Once they have some estimate of benchmark, they start improvising further. Share your complete codes in the comment box below.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.
",https://www.analyticsvidhya.com/blog/2015/09/build-predictive-model-10-minutes-python/
"(Senior) Big Data Engineer  Bangalore  (4-8 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Build a Predictive Model in 10 Minutes (using Python)|13 Amazing Applications / Uses of Data Science Today|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  (senior) Big Data EngineerLocation  BangaloreAbout employer ConfidentialThis company is a state-of-the-art big data and Advanced Analytics company, providing consulting and project delivery services & solutions, catering to enterprises of all sizes, across different industries such as Healthcare, Retail & Consumer Industries, Insurance & Manufacturing. Founded under the credo Big Decisions Assured, we offer advanced analytics platforms that help businesses make quick cogent data-driven decisions, essentially required to sustain and grow in tandem with evolving industry trends.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as(senior) Big Data Engineer  Bangalore and the following details:",https://www.analyticsvidhya.com/blog/2015/09/senior-big-data-engineer-aptus-data-labs-bangalore-4-8-years-experience/
13 Amazing Applications / Uses of Data Science Today,Learn everything about Analytics|Introduction|What to learn from this article?|Applications / Uses of Data Science|Coming Up In Future|End Notes,"Internet Search|DigitalAdvertisements (Targeted Advertising and re-targeting)|Recommender Systems|Image Recognition|Speech Recognition|Gaming|Price Comparison Websites|Airline Route Planning|Fraud and Risk Detection|Delivery logistics|Miscellaneous|Self Driving Cars|Robots|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|(Senior) Big Data Engineer  Bangalore  (4-8 years of Experience)|Your Guide to Master Hypothesis Testing in Statistics|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the questions people ask me commonly is:Is Big Data / Data Science really a buzz or a once in a life time opportunity?Different people have different answers and viewpoints to the question above. I dont want to get into this debate here. I am rather taking a safer approach here. I would tell you a few applications which are already impacting a lay mans life. You can read them for yourself and decide whether this is a buzz or an opportunity.In this article, Ive listed down some of the most common applications of data science that we use in our daily lives. Along side, youll also find some advanced applications which are yet to come in near future. The idea behind sharing them is not to tell you the tools and techniques used in these cases  it is even more fundamental in nature. I want to showcase the impact data science in making and excite you about what is in store for future.If you know of any other application, which I have missed, please feel free to add them through comments below.Using data science, companies have become intelligent enough to push & sell products as per customers purchasing power & interest. Heres how they are ruling our hearts and minds:When we speak of search, we think Google. Right? But there are many other search engines like Yahoo, Bing, Ask, AOL, Duckduckgo etc. All these search engines (including Google) make use of data science algorithms to deliver the best result for our searched query in fraction of seconds. Considering the fact that, Google processes more than 20 petabytes of data everyday. Had there been no data science, Google wouldnt have been the Google we know today.If you thought Search would have been the biggest application of data science and machine learning, here is a challenger  the entire digital marketing spectrum. Starting from the display banners on various websites to the digital bill boards at the airports  almost all of them are decided by using data science algorithms.This is the reason why digital ads have been able to get a lot higher CTR than traditional advertisements. They can be targeted based on users past behaviour. This is the reason why I see ads of analytics trainings while my friend sees ad of apparels in the same place at the same time.Who can forget the suggestions about similar products on Amazon? They not only help you find relevant products from billions of products available with them, but also adds a lot to the user experience.A lot of companies have fervidly used this engine / system to promote their products / suggestions in accordance with users interest and relevance of information. Internet giants like Amazon, Twitter, Google Play, Netflix, Linkedin, imdb and many more uses this system to improve user experience. The recommendations are made based on previous search results for a user.You upload your image with friends on Facebook and you start getting suggestions to tag your friends. This automatic tag suggestion feature uses face recognition algorithm. Similarly, while using whatsapp web, you scan a barcode in your web browser using your mobile phone. In addition, Google provides you the option to search for images by uploading them. It uses image recognition and provides related search results. To know more about image recognition, check out this amazing (1:31) mins video:Some of the best example of speech recognition products are Google Voice, Siri, Cortana etc. Using speech recognition feature, even if you arent in a position to type a message, your life wouldnt stop. Simply speak out the message and it will be converted to text. However, at times, you would realize, speech recognition doesnt perform accurately. Just for laugh, check out this hilarious video(1:30 mins) and the conversation between Cortana & Satya Nadela (CEO, Microsoft)EA Sports, Zynga, Sony, Nintendo, Activision-Blizzard have led gaming experience to the next level using data science. Games are now designed using machine learning algorithms which improve / upgrade themselves as the player moves up to a higher level. In motion gaming also, your opponent (computer) analyzes your previous moves and accordingly shapes up its game.At a basic level, these websites are being driven by lots and lots of data which is fetched using APIs and RSS Feeds. If you have ever used these websites, you would know, the convenience of comparing the price of a product from multiple vendors at one place. PriceGrabber, PriceRunner, Junglee, Shopzilla, DealTime are some examples of price comparison websites. Now a days, price comparison website can be found in almost every domain such as technology, hospitality, automobiles, durables, apparels etc.Airline Industry across the world is known to bear heavy losses. Except a few airline service providers, companies are struggling to maintain their occupancy ratio and operating profits. With high rise in air fuel prices and need to offer heavy discounts to customers has further made the situation worse. It wasnt for long when airlines companies started using data science to identify the strategic areas of improvements. Now using data science, the airline companies can:Southwest Airlines, Alaska Airlines are among the top companies whove embraced data science to bring changes in their way of working.One of the first applications of data science originated from Finance discipline. Companies were fed up of bad debts and losses every year. However, they had a lot of data which use to get collected during the initial paper work while sanctioning loans. They decided to bring in data science practices in order to rescue them out of losses. Over the years, banking companies learned to divide and conquer data via customer profiling, past expenditures and other essential variables to analyze the probabilities of risk and default. Moreover, it also helped them to push their banking products based on customers purchasing power.Who says data science has limited applications? Logistic companies like DHL, FedEx, UPS, Kuhne+Nagel have used data science to improve their operational efficiency. Using data science, these companies have discovered the best routes to ship, the best suited time to deliver, the best mode of transport to choose thus leading to cost efficiency, and many more to mention. Further more, the data that these companies generate using the GPS installed, provides them a lots of possibilities to explore using data science.Apart from the applications mentioned above, data science is also used in Marketing, Finance, Human Resources, Health Care, Government Policies and every possible industry where data gets generated. Using data science, the marketing departments of companies decide which products are best for Up selling and cross selling, based on the behavioral data from customers. In addition, predicting the wallet share of a customer, which customer is likely to churn, which customer should be pitched for high value product and many other questions can be easily answered by data science. Finance (Credit Risk, Fraud), Human Resources (which employees are most likely to leave, employees performance, decide employees bonus) and many other tasks are easily accomplished using data science in these disciplines.Though, not much has been reveled about them except the prototypes, and neither I know when they would be available for a common mans disposal. Hence, Ive kept these amazing application of data science in Coming Up section. We need to wait and watch how far Google can become successful in their self driving cars project. Robots, as we know, have lived for a while but arent being used as a commodity yet due to associated security issues. Lets see, what our future holds for us!Check out this ~3 min video to know more:Check out this ~3 min video of robots which resembles HumansImagine a world, where we are surrounded by robots like these. Will they do any good for us? Or these would lead to repercussions that mankind will have to endure. Lets have a discussion on this !I am sure, you had fun watching the videos. The intent wasnt just fun, but learning simultaneously. By now, you would developed an understanding of the boundless potential that data science has in this world. Almost everything on this planet, which generates data, falls under the radar of data science which can improve and optimize its existing process.Though, I wasnt sure where to add Coming up in future section, since much hasnt been revealed about their future advancement. Yet, I thought of leaving you with something controversial (robots) just to ignite a discussion on the future of robotics.If you liked this article, please share your comments / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/applications-data-science/
Your Guide to Master Hypothesis Testing in Statistics,Learn everything about Analytics|Introduction  the difference in mindset|A case study:|Basics of Statistics|What is Significance Level?|What are the steps to perform Hypothesis Testing?|Directional/ Non Directional Hypothesis Testing|End Notes,"Example|Example|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|13 Amazing Applications / Uses of Data Science Today|AM-Digital Analytics  EXL  Gurgaon (4-8 years of experience)|
Sunil Ray
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I started my career as a MIS professional and then made my way into Business Intelligence (BI) followed by Business Analytics, Statistical modeling and more recently machine learning. Each of these transition has required me to do a change in mind set on how to look at the data.But, one instance sticks out in all these transitions. This was when I was working as a BI professional creating management dashboards and reports. Due to some internal structural changes in the Organization I was working with, our team had to start reporting to a team of Business Analysts (BA). At that time, I had very little appreciation of what is Business analytics and how is it different from BI.So, as part of my daily responsibilities, I prepared my management dashboard in the morning and wrote a commentary on it. I compared the sales of first week of the current month to sales of previous month and same month last year to show an improvement in business. It looked something like this:In my commentary, I ended up writing that sales are better than last year and last month and applauded some of the new initiatives the Sales team had taken recently. I was thinking this was good work to show to my new manager. Little did I know, what was in store!When I showed the report to my new manager applauding the sales team, he asked why do I think this uplift is just not random variation in data?I had very little Statistics background at this time and I could not appreciatehis stand.I thought we were talking 2 different language. My previous manager would have jumped over this report and would have dropped a note to Senior Management himself! And here was my new manager asking me to hold my commentary.In todays article, I will explain hypothesis testing and reading statistical significance to differentiate signal from the noise in data  exactly what my new manager wanted me to do!P.S. This might seem like a lengthy article, but would be one of the most useful one, if you follow through.Let us say that average marks in mathematics of class 8th students of ABC School is 85. On the other hand,if we randomly select 30 students and calculate their average score, their average comes to be 95. What can be concluded from this experiment? Its simple. Here are the conclusions:How should we decide which explanation is correct? There are various methods to help you to decide this. Here are some options:The first two methods require more time & budget. Hence, arent desirable when time or budget are constraints.So, in such cases, a convenient method is to calculate the random chance probability for that sample i.e. what is the probability that sample would have average score of 95?. It will help you to draw a conclusion from the given two hypothesis given above.Now the question is, How should we calculate the random chance probability?.To answer it, we should first review the basic understanding of statistics.1. Z-Value/ Table/ p value: Z value is a measure of standard deviation i.e. how many standard deviation away from mean is the observed value. For example, the value of z value = +1.8 can be interpreted as the observed value is +1.8 standard deviations away from the mean. P-values are probabilities. Both these statistics terms are associated with the standard normal distribution. You can look at the p-values associated with each z-value in Z-table. Below is the formula to calculate z value:
Here X is the point on the curve,  is mean of the population and is standard deviation of population.
As I discussed, these methods always work with normal distribution (shown above) only, not with other distributions. In case, the population distribution is not normal, wed resort to Central Limit Theorem.2. Central Limit Theorem:This is an important theorem in statistics. Without going into definitions, Ill explain it using an example . Lets look at the case below. Here, we havea data of 1000 students of 10th standard with their total marks. Following are the derived key metrics of this population:And,frequency distribution of marks is:
Is this some kind of distribution you can recall? Probably not. These marks have been randomly distributed to all the students.Now, lets take a sample of 40 students from this population. So, how many samples can we take from this population? We can take 25 samples(1000/40 = 25). Can you say that every sample will have the same average marks as population has (48.4)? Ideally, it is desirable but practically every sample is unlikely to have the same average.Here we have taken 1000 samples of 40 students (randomly sample generated in excel). Lets look at the frequency distribution of these sample averages of thousands samples and other statistical metrics:

Does this distribution looks like the one we studied above? Yes, this table is also normally distributed. For better understanding, you can download this file from here and while doing this exercise youll come across the findings stated below:1. Mean of sample means (1000 sample means) is very close to population mean3. The distribution of sample means is normal regardless of the distribution of the actual population. This is known as Central Limit theorem. This can be very powerful. In our initial example of ABC School students, we compared the sample mean and population mean. Precisely, we looked at the distribution of sample mean and found out the distance between population mean and the sample mean. In such cases, you can always use a normal distribution without worrying about the population distribution.You can calculate the standard deviation and mean based on above findings and calculate z-score and p-value. Here random chance probability will help you to accept one of discussed conclusions from ABC Schools example (stated above). But, to satisfy the CLT theorem, sample size must be sufficient (>=30).Now, lets say we have calculated the random chance probability. It comes out to be 40%, then should I go with first conclusion or other one ? Here the Significance Levelwill help us to decide.We have taken an assumption that probability of sample mean 95 is 40%, which is high i.e. more likely that we can say that there is a greater chance that this has occurred due to randomness and not due to behavior difference.Had the probability been 7%, it would have been a no-brainer to infer that it is not due to randomness. There may be some behavior difference because probability is relatively low which means high probability leads to acceptance of randomness and low probability leads to behavior difference.Now, how do we decide what is high probability and what is low probability?To be honest, it is quite subjective in nature. There could be some business scenarios where 90% is considered to be high probability and in other scenarios could be 99%. In general, across all domains, cut off of 5% is accepted. This 5% is called Significance Level also known as alpha level (symbolized as ). It means that if random chance probability is less than 5% then we can conclude that there is difference in behavior of two different population. (1- Significance level) is also known as Confidence Level i.e. we can say that I am 95% confident that it is not driven by randomness.Till now,we looked at the tools to test a hypothesis, whether sample mean is different from population or it is due to random chance. Now, lets look at the steps to perform a hypothesis test and post that we will go through it using an example.Blood glucose levelsfor obese patients have a mean of 100 with a standard deviationof 15. A researcher thinks that a diet high in raw cornstarch will have a positive effect on blood glucose levels. A sample of 36 patients who have tried the raw cornstarch diet have a meanglucose level of 108. Test the hypothesis that the raw cornstarch had an effect or not.Solution:-Follow the above discussed steps to test this hypothesis:Step-1: State the hypotheses. The population mean is 100.H0: = 100
H1:  > 100Step-2: Set up the significance level. It is not given in the problem so lets assume it as5% (0.05).Step-3: Compute the random chance probability using z score and z-table.
For this set of data: z= (108-100) / (15/36)=3.20You can look at the probability by looking at z- table and p-value associated with 3.20 is 0.9993 i.e. probability of having value less than 108 is 0.9993 and more than or equals to 108 is (1-0.9993)=0.0007.Step-4: It is less than 0.05 so we will reject the Null hypothesis i.e. there is raw cornstarch effect.Note: Setting significance level can also be done using z-value known as critical value. Find out the z- value of 5% probability and it is 1.65 (positive or negative, in any direction). Now we can compare calculated z-value with critical value to make a decision.In previous example, our Null hypothesis was, there is no difference i.e. mean is 100 and alternate hypothesis was sample mean is greater than 100. But, we could also set an alternate hypothesis as sample mean is not equals to 100. This becomes important when we do reject the Null hypothesis, should we go with which alternate hypothesis:Here, the question is Which alternate hypothesis is more suitable?. There are certain points which will help you to decide which alternate hypothesis is suitable.In above two cases, we will go with One tail test. In one tail test, our alternate hypothesis is greater or less than the observed mean so it is also known as Directional Hypothesis test. On the other hand, if you dont know whether the impact of test is greater or lower then we go with Two tail test also known as Non Directional Hypothesis test.Lets say one of research organization is coming up with new method of teaching. They want to test the impact of this method. But, they are not aware that it has positive or negative impact. In such cases, we should go with two tailed test.In one tail test, we reject the Null hypothesis if the sample mean is either positive or negative extreme any one of them. But, in case of two tail test we can reject the Null hypothesis in any direction (positive or negative).Look at the image above. Two-tailed test allots half of your alpha to testing the statistical significance in one direction and half of your alpha in the other direction. This means that .025 is in each tail of the distribution of your test statistic. Why are we saying 0.025 on both side because normal distribution is symmetric. Now we come to a conclusion that Rejection criteria for Null hypothesis in two tailed test is 0.025 and it is lower than 0.05 i.e. two tail test has more strict criteria to reject the Null Hypothesis.Templer and Tomeo (2002) reported that the population mean score on the quantitative portion of the Graduate Record Examination (GRE) General Test for students taking the exam between 1994 and 1997 was 558  139 (  ). Suppose we select a sample of 100 participants (n = 100). We record a sample mean equal to 585 (M = 585). Compute the p-valuet0 check whether or not we will retain the null hypothesis ( = 558) at 0.05 level of significance ( = .05).Solution:Step-1: State the hypotheses. The population mean is 558.H0: = 558
H1:  558 (two tail test)Step-2: Set up the significance level. As stated in the question, it as5% (0.05).In a non-directional two-tailed test, we divide the alpha value in half so that an equal proportion of area is placed in the upper and lower tail. So, the significance level on either side is calculated as: /2 = 0.025. and z score associated with this (1-0.025=0.975) is 1.96. As this is a two-tailed test,z-score(observed) which is less than -1.96 or greater than 1.96 is a evidence to reject the Null hypothesis.Step-3: Compute the random chance probability orz score
For this set of data: z= (585-558) / (139/100)=1.94You can look at the probability by looking at z- table and p-value associated with 1.94 is 0.9738 i.e. probability of having value less than 585 is 0.9738 and more than or equals to 585 is (1-0.9738)=0.03Step-4: Here, to make a decision, we compare the obtained z value to the critical values (+/- 1.96). We reject the null hypothesis if the obtained value exceeds a critical values. Hereobtained value (Zobt= 1.94) is less than the critical value. It does not fall in the rejection region. The decision is to retain the null hypothesis.In this article, we have looked at the complete process of undertaking hypothesis testing during predictive modeling. Initially, we looked at the concept of hypothesis followed by the types of hypothesis and way to validate hypothesis to make an informed decision. We also have also looked at important concepts of hypothesis testing like Z-value, Z-table, P-value, Central Limit theorem.As mentioned in the introduction, this was one of the most difficult change in mindset for me when I read this first time. But it was also one of the most helpful and significant change. I can easily say that this change started me to think like a predictive modeler.In next article, we will look at the what-if scenarios with hypothesis testing like:Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/
"AM-Digital Analytics  EXL  Gurgaon (4-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Your Guide to Master Hypothesis Testing in Statistics|CRUNCH  Practical Big Data Conference, Budapest, October 28-30, 2015|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation AM  Digital AnalyticsLocation  GurgaonAbout employer EXLJob description: Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asDigital Analytics  EXL  Gurgaon,https://www.analyticsvidhya.com/blog/2015/09/digital-analytics-exl-gurgaon-4-8-years-experience/
"CRUNCH  Practical Big Data Conference, Budapest, October 28-30, 2015","Learn everything about Analytics|What is CRUNCH all about?|Who are the Key Speakers?|Why should you attend it?|How to Register ?
","Share this:|Like this:|Related Articles|AM-Digital Analytics  EXL  Gurgaon (4-8 years of experience)|Perfect way to build a Predictive Model in less than 10 minutes|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"CRUNCH is a Practical Big Data Conference in Budapest, between October 28-30, organised by two startups Prezi and Ustream (who joined forces to help to turn Budapest into a technology hub with non-profit conferences). From the 16 talks, and 4 workshops you can learn practical insights from the big players of data science, that you can apply from the next day in your company.Crunch will also feature 4 interesting workshops for participants. These look really useful, for example Evan Miller of How Not To Run An A/B Test fame will be running one! The other workshops are:Doug CuttingChief Architect at Cloudera, Founder of HadoopAlistair Croll
Entrepreneur, author of Lean Analytics bestsellerStephen Brobst
CTO, Teradata CorporationScott Gnau
CTO,HortonworksAndrea Burbank
Data Scientist, PinterestElena Verna
Director of Growth and Analytics, SurveyMonkeyEsh Kumar
Data Scientist, SpotifyCheck the full lineup at http://crunchconf.com/#speakersHere are the reasons to attend this event:",https://www.analyticsvidhya.com/blog/2015/09/crunch-practical-big-data-conference-budapest-october-28-30-2015/
Perfect way to build a Predictive Model in less than 10 minutes,Learn everything about Analytics|Overview|Introduction|Breaking Down the process of Predictive Modeling|End Notes,"Lets start putting this intoaction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|CRUNCH  Practical Big Data Conference, Budapest, October 28-30, 2015|24 Ultimate Data Scientists To Follow in the World Today|
Tavish Srivastava
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In the last few months, we have started conducting data science hackathons. These hackathons are contests with a well defined data problem, which has be be solved in short time frame. They typically last any where between 2  7 days.If month long competitions on Kaggle are like marathons, then these hackathons are shorter format of the game  100 mts Sprint. They are high energy events where data scientists bringin lot of energy, the leaderboard changes almost every hour and speed to solve data science problem matters lot more than Kaggle competitions.One of the best tip, I can provide to data scientists participating in these hackathons (or even in longer competitions) is to quickly build the first solution and submit. The first few submissions should be real quick.I have created modules on Python and R which can takes in tabular data and the name of target variable and BOOM! I have my first model in less than 10 minutes (Assuming your data has more than 100,000 observations). For smaller data sets, this can be even faster. The reason of submitting this super-fast solution is to create a benchmark for yourself on which you need to improve. I will talk about my methodology in this article.To understand the strategic areas, lets first break down the process of predictive analysis into its essential components. Broadly, it can be divided into 4 parts. Every component demands x amount of time to execute. Lets evaluate these aspects n(with time taken):Note: The percentages are based on a sample of 40 competition, I have participated in past (rounded off).Now we know where do we need to cut down time. Lets go step by step into the process (with time estimate):1.Descriptive Analysis : When I started my career into analytics, we used to primarily build models based on Logistic Regression and Decision Trees. Most of the algorithm we used involved greedy algorithms, which can subset the number of features I need to focus on.With advanced machine learning tools coming in race, time taken to perform this task can be significantly reduced. For your initial analysis, you probably need not do any kind of feature engineering. Hence, the time you might need to do descriptive analysis is restricted to know missing values and big features which are directly visible. In my methodology, you will need 2 minutes to complete this step (I assume a data with 100,000 observations).2.Data Treatment : Since, this is considered to be the most time consuming step, we need to find smart techniques to fill in this phase. Here are two simple tricks which you can implement :With such simple methods of data treatment, you can reduce the time to treat data to 3-4 minutes.3.Data Modelling : I have found GBM to be extremely effective for 100,000 observation cases. In case of bigger data, you can consider running a Random Forest. This will take maximum amount of time (~4-5 minutes)4. Estimation of Performance : I find k-fold with k=7 highly effective to take my initial bet. This finally takes 1-2 minutes to execute and document.The reason to build this model is not to win the competition, but to establish a benchmark for our self. Let me take a deeper dive into my algorithm. I have also included a few snippets of my code in this article.I will not include my entire function to give you space to innovate. Here is a skeleton of my algorithm(in R):Step 1 : Append both train and test data set togetherStep 2 : Read data-set to your memoryStep 3: View the column names/summary of the datasetStep 4: Identify the a) Numeric variable b) ID variables c) Factor Variables d) Target variablesStep 5 : Create flagsfor missing valuesStep 6 : Impute Numeric Missing valuesSimilarly impute categorical variable so that all missing value is coded as a single value say NullStep7 : Pass the imputed variable into the modelling process#Challenge: Try to Integrate a K-fold methodology in this stepStep 8 : Make predictionsStep 9 : Check performanceAnd Submit!Hopefully, this article would have given enough motivation to make your own 10-min scoring code. Most of the masters on Kaggle and the best scientists on our hackathons have these codes ready and fire their first submission before making a detailed analysis. Once they have some estimate of benchmark, they start improvising further. Share your complete codes in the comment box below.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/perfect-build-predictive-model-10-minutes/
24 Ultimate Data Scientists To Follow in the World Today,Learn everything about Analytics|Introduction|Research oriented Data Scientists|Data Scientists Turned Entrepreneur|Data Scientists in Action|End Notes,"Geoffrey Hinton|Yann Lecun|Yoshua Bengio|Jurgen Schmidhuber|Alex Sandy Pentland|Peter Norvig|Corinna Cortes|Micheal I Jordan|Andrew Ng|Daphne Koller|Hilary Mason|Sebastian Thrun|Jeff Hammerbacher|Jeremy Achin|Carla Gentry|DJ Patil|Adam Coates|Monica Rogati|Oliver Grisel|Owen Zhang|Sergey Yurgenson|Stanislav Semenov|Gilberto Titericz Jr.|Kirk Borne|Doug Cutting|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage|Share this:|Like this:|Related Articles|Perfect way to build a Predictive Model in less than 10 minutes|Cheatsheet  Python & R codes for common Machine Learning Algorithms|
Analytics Vidhya Content Team
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Having a hero / heroine helps you navigate through the difficulttimes. You look up to them and then think that the problems you thought were difficult are actually trivial in nature. If people can solve and deliver at a much larger scale, you can too!If you thought learning data science is difficult or deep neural nets is not your cup of tea  look up to the role modelswho created them. Following these role models provides you a daily inspiration, a motivation to find bigger purpose in life and to achieve it.Role models set goals for you and try to make you as good as they are. Role models are important. Kasey ZachariasIn this article, Ill introduce you to a league of ultimatedata scientists in the world. I think these data scientists have not only done awesome work, they have all left a legacy behind the work they have done. So, here is a small intro and tribute to these role models.These data science maestros have inspired and guided millions of candidates across the world through their freely accessible blogs, tutorials, videos etc. They are the ones who invented the term data scientist, dark knowledge and unlocked the mystery behind deep learning and neural nets.I have broadly classified these role models in three categories. I know that might sound silly  a follower classifying the role models, but I have done it in interest of providing some structure to the list. Here are the classes I have categorized people in:I know many of you would be keen to connect / follow with these data scientists. Hence, for your convenience, I have provided the links of their respective LinkedIn / Twitter profiles.You must have heard the term Back Propagation? He is the brain (co-inventor) behind this algorithm for training neural nets and deep learning simulations. Moreover, Geoff invented the term Dark Knowledge. Its inspired by the idea that most of the knowledge is in the ratios of tiny probabilities that have virtually no influence on the cost function used for training or on the test performance. He is widely known for his work on Artificial Neural Networks. In 2013, he joined Google and led its AI team. Geoff holds a PhD in Artificial Intelligence from Edinburgh. He and his research group have been the driving force behind the resurgence of neural networks and deep learning.Yann is currently working at Facebook as Director of AI Research wing. He is the founding director of NYU Center for Data Science. He has worked on several deep learning projects and has 14 US patents registered. Hes also associated with New York University as a Professor from the past 12 years. Yann holds a PhD in Computer Science from Universit Pierre et Marie Curie (Paris VI). His interest area lies in Machine Learning, Deep Learning, Computer Vision, Computational Neuroscience.Yoshua is the Founder and R&D Guru of ApSTAT Technologies. Hes also associated with Universit de Montral as a Professor from past 22 years. Previously, he worked with AT&T & MIT as a machine learning researcher. His contributions to Deep Learning and Artificial Intelligence have got him worlds attention. Amongst his numerous accomplishments, he holds Canada Research Chair in Statistical Learning Algorithms, NSERC Chair and many other. He holds PhD in Computer Science from McGill University.Triggered by an aim to build a self-improving Artificial Intelligence(AI) smarter than himself, Jurgen continual research works on RNN, Deep Learning & Computer Vision, Machine Learning is currently in use at Google, Microsoft, IBM, Baidu and many other companies. He has published 333 peer-reviewed papers, earned seven best paper/best video awards, the 2013 Helmholtz Award of the International Neural Networks Society, and the 2016 IEEE Neural Networks Pioneer Award.From the past 29 years, Alex has been associated with MIT as a professor. During this period, he co-founded several companies such as IDcubed.org, Sense Networks, Cogito Health, Ginger.io and many others. Among all his achievements, he was named as the Worlds Most Powerful Data Scientist by Forbes. He is also appointed as an advisor in various MNCs such as Nissan, Motorola, HBR, Telefonica etc. His interest area lies in machine learning, artificial intelligence, human computing etc.Started his career as a research scientist, Peter is currently appointed as the Research Director at Google. Prior to Google. he worked for 6 years with NASA as Head of Computational Science Division. Infact, hes been awarded with NASA Exceptional Achievement Award, ACM Fellow, AAAI Fellow and many others to mention. He holds a PhD degree in Computer Science from University of California, Berkeley. His interest area includes artificial intelligence, natural language processing, machine learning etc.Corinna Cortes is the Head of Google Research, NY. Corinna received her MS degree in Physics from University of Copenhagen and joined AT&T Bell Labs as a researcher. She has also worked at AT&T Labs, formerly AT&T Bell Labs for more than 10 years as a researcher. She received her PhD in Computer Science from the University of Rochester. Her research areas include Artificial Intelligence and Machine Learning, General Science and Algorithms & Theory. She is a competitive runner and mother of two.Michael is the Pehong Chen Distinguished Professor in the Department of Electrical Engineering and Computer Science and the Department of Statistics at UC Berkeley. His research in recent years has focused on Bayesian non parametric analysis, probabilistic graphical models, spectral methods, kernel machines and applications to problems in signal processing etc. He has earned several prestigious honors such as, he was named a Neyman Lecturer and a Medallion Lecturer by Institute of Mathematical Statistics (IMS). He holds a PhD in Cognitive Science from University of California. He was a professor at MIT from 1988 to 1998.Andrew Ng co-founded Coursera with Daphne Koller. He is currently working as a Chief Scientist at Baidu where hes involved in carrying out researches in Deep Learning and in scalable approaches to Big Data and AI. In addition, hes associated with Stanford University as an Associate Professor. Hes also the Founder and Lead of Googles Deep Learning Project. Through Coursera, along with other disciplines, he made the data science knowledge accessible to all for free! Above all, hes widely known for his Machine Learning Course.Daphne is the President and Co-Founder of Coursera. She hold a PhD degree in Computer Science from Stanford. Over the years, Daphne has been honored with various awards such as ONR Young Investigator Award, ACM Infosys Award, MacArthur Foundatin Fellowship etc. She served as a Professor at Stanford University for almost 18 years. Her interest areas lies in Machine Learning, Artificial Intelligence, Pattern Recognition etc.Hilary is the Founder of Fast Forward Labs. She also co-founded hackNY.org and DataGotham. Previously, she worked as a Chief Scientist at Bitly, and Assistant Professor at Johnson & Wales University. She got featured in the list of Fortune 40 under 40 ones to watch 2011, Craigs 40 under Forty 2012. Along with many other awards, she was the recipient of TechFellow Engineering Leadership Award 2012. Her interest areas lies in Machine Learning, Data Mining, Python.Sebastian is the Founder and CEO of Udacity. Previously, he founded Google[x] and worked as VP, Fellow at Google for 7 years. In addition, hes associated with Stanford University as a Research Professor. At Udacity, he aims to democratize education so that everyone in this world can have world class education. He dreams of making this world a better place by empowering all people in the world through better education. His key areas include Machine Learning and Artificial Intelligence.He along with DJ Patil coined the term Data Scientist. Jeff is the Founder and Chief Scientist of Cloudera. Previously, he led the Data Team at Facebook. The Data team was responsible for driving many of the applications of statistics and machine learning at Facebook. In addition to his present commitments, hes also an Assistant Professor at Mount Sinai School of Medicine. He holds a Bachelor Degree in Mathematics from Harvard University. His interest areas lies in Big Data, Machine Learning, Hadoop, Data Mining.Jeremy is the Co-founder of Data Robot. Prior to Data Robot, he worked for 5 years as Director of Research and Modeling at Travelers Insurance. Hes performed outstandingly in Kaggle Competitions. He managed to secure rank in top 10% participants in all the competitions he has participants. His interest areas lies in Predictive Modeling, Data Mining, Machine Learning etc. Data Robot is known for employing the best data scientist across the world. Since inception, Data Robot has emerged to become one of the fastest growing companies in US.Carla Gentryis a Data Scientist & Founder ofAnalytical Solution. She holds agraduate degree in Maths & Economics from University of Tennessee. She carries an invaluable experience of over 15 years which includes working forFortune 500 companies like Hershey, Kraft,Johnson & Johnson, Kelloggs and Firestone.She is one of the most popular personality in Big Data community to follow on Twitter.In 2013, the Information Week named her one of 10 IT Leaders to Follow on Twitter.DJ Patil along with Thomas.H.Davenport, authored the popular HBR article  Data Scientist: The Sexiest Job of 21st Century. Currently, hes employed as US Chief Data Scientist at White House Office of Science and Technology Policy. Previously, he worked as VP of RelateIQ and Head of Data Products and Chief Scientist at Linkedin. Many patents have been filed under his name. He was elected as 2014 Young Global Leader by the World Economic Forum. He hold a PhD in applied mathematics from University of Maryland.Adam holds PhD in Computer Science from Stanford University. Currently, hes appointed as the Senior Director at Baidu Silicon Valley AI Lab. His interest areas lies in Machine Learning, Deep Learning, Control & Robotics.Monica is currently working as Data Science Advisor with Insight Data Science. Previously, she worked with Linkedin as Senior Data Scientist, and was Vice President of Jaw Bone and held many more responsible positions. Her passion lies in turning data into products, actionable insights and meaningful stories. Her interest areas lies in Machine Learning, Statistical Text Mining, Recommender Systems etc. She holds a PhD in Computer Science from Carnegie Mellon University.Have you ever used Scikit-learn ? Hes one the brains behind contributing to Scikit-Learn. Oliver is currently working as a Software Engineer with Inria Parietal to work on improving scikit-learn and related tools. He holds a MSc in Advanced Computing from Imperial College of London. Hes specialized in machine learning with applications to natural language processing and knowledge extraction. Hes also known to be involved in delivering talks and tutorial sessions on scikit-learn and predictive modelling.Currently, working as Chief Product Office at Data Robot, Owen is the current World Rank 1 at Kaggle. He proved his mettle by winning and emerging as runner up in several kaggle competitions. Prior to Data Robot, he was serving AIG as the Vice President, Science and Sr. Director, Analytics and Research at Travelers Insurance. He hold a masters degree from University of Toronto. His interest area lies in Predictive Modeling, Data Mining.Sergey is currently working as a Data Scientist at Data Robot. Began as a physicist, Sergey realized his love for analytics later down the years. And then, there was no looking back. He emerged to be one of the Top 10 Data Scientist in 2012. Currently, ranked at 16th, Sergey has won several kaggle competitions. He loves to solve challenging problems that require innovation and unconventional solutions. Prior to Data Robot, he worked for 13 years as a Research Associate at Harvard Medical School. He holds a PhD in Physics from St. Petersburg State UniversityCurrently, the World Rank 3 in Kaggle, Stanislav has won various competitions including Otto Group Product Classification Challenge. Currently, he works as a Data Scientist Consultant. In addition, he is engaged as a Professor at Yandex School of Data Analysis. He holds a masters degree in applied mathematics and informatics from National Research University (Russia).This Electronics Engineer turned Data Scientist is currently positioned at World Rank 2 in Machine Learning and Data Mining Competitions hosted by Kaggle. Currently, he is employed as Automation Engineer at Petrobras in Brazil. Prior to this, he worked at big multinationals like Siemens and Nokia. After devoting 8 years in electronics, Gilberto discovered his love for data science in 2008 and since then, he has never looked back.Kirk is appointed as a Principal Data Scientist at Booz Allen Hamilton. Not only hes a data scientist, but also hes an Astrophysicist and Space Scientist. Hes named as 2014 IBM Big Data & Analytics Hero. Hes also got featured in a Ted Talk named Big Data, Small World. In addition to Booz Allen, hes associated with many companies as their advisory board member. Kirk holds a PhD degree in Astronomy from California Institute of TechnologyDoug is the daddy of Hadoop. Hes the reason behind Apache Lucene, Nutch, Hadoop and Avro open source projects into existence. Currently, hes working as Chief Architect with Cloudera. Prior to joining Cloudera, he worked with several MNCs like Apple, Yahoo etc. From the past 14 years, hes associated with Apache Software Foundation. He holds a bachelor degree from Stanford University.By now, I am sure you would also have noticed a common trait among all these data scientist. All these people are either driven by a vision or their entrenched love for data science.So if data science is your love, I would suggest to follow these role models closely and youll be on your path to greatness!In this article, Ive listed the best data scientist across the world. These people are masters in their own discipline and avidly seek opportunities to spread data science knowledge around the world.If you think, Ive missed out on any, please feel free to sharein the comments section below with your views.",https://www.analyticsvidhya.com/blog/2015/09/ultimate-data-scientists-world-today/
Cheatsheet  Python & R codes for common Machine Learning Algorithms,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror join our Facebook Group|Share this:|Like this:|Related Articles|24 Ultimate Data Scientists To Follow in the World Today|Learn Gradient Boosting Algorithm for better predictions (with codes in R)|
Analytics Vidhya Content Team
|34 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","For the super lazy Data Darbies, we will make your life even easier. You can download the PDF Versionof the cheat sheet here and copy paste the codes from it directly.|Keep this cheat sheet handy when you work on data sets.download the complete cheat sheet here: PDF Version",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In his famous book  Think and Grow Rich, Napolean Hill narrates story of Darby, who after digging for a gold vein for a few years walks away from it when he was three feet away from it.Now, I dont know whether the story is true or false. But, I surely know of a few Data Darby around me. These people understand the purpose of machine learning, its execution and use just a set 2  3 algorithms on whatever problem they are working on. They dont update themselves with better algorithms or techniques, because they are too tough or they are time consuming.Like Darby, they are surely missing from a lot of action after reaching this close! In the end, they give up on machine learning by saying it is very computation heavy or it is very difficult or I cant improve my models above a threshold  whats the point? Have you heard them?Todays cheat sheet aims to change a few Data Darbys to machine learning advocates.Heres a collection of 10 most commonly used machine learning algorithms with their codes in Python and R. Considering the rising usage ofmachine learning in building models, this cheat sheet is good to act as a code guide to help you bring these machine learning algorithms to use. Good Luck!",https://www.analyticsvidhya.com/blog/2015/09/full-cheatsheet-machine-learning-algorithms/
Learn Gradient Boosting Algorithm for better predictions (with codes in R),Learn everything about Analytics|Introduction|QuickExplanation|Lets beginwith aneasy example|Lets try to visualize one Classification Problem|Time to Practice  Example|End Notes,"Explainingunderlying mathematics|||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage|Share this:|Like this:|Related Articles|Cheatsheet  Python & R codes for common Machine Learning Algorithms|How Good is the Executive business analytics program by Jigsaw Academy and MISB Bocconi ?|
Tavish Srivastava
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The accuracy of a predictive model can be boosted in two ways: Either by embracing feature engineering or by applying boosting algorithms straight away. Having participated in lots of data science competition, Ive noticed that people prefer to work with boosting algorithms as it takes less time and produces similar results.There are multiple boosting algorithms like Gradient Boosting, XGBoost, AdaBoost, Gentle Boost etc. Every algorithm has its own underlying mathematics and a slight variation is observed while applying them. If you are new to this, Great! You shall be learning all these concepts in a weeks time from now.In this article, Ive explained the underlying concepts and complexities of Gradient Boosting Algorithm. In addition, Ive also sharedan example to learn its implementation in R.Note: This guide is meant for beginners. Hence, if youve already mastered this concept, you may skip this article here.While working withboosting algorithms, youll soon come across two frequently occurring buzzwords: Bagging and Boosting. So, how are they different? Heres a one line explanation:Bagging: Itis an approach where you take random samples of data, build learning algorithms and take simple means to find bagging probabilities.Boosting: Boosting is similar, however the selection of sample is made more intelligently.We subsequently give more and more weight to hard to classify observations.Okay! I understand youve questions sprouting uplike what do you mean by hard? How do I know how much additional weight am I supposed to give to a mis-classified observation. I shall answer all your questionsin subsequent sections.Keep Calm and proceed.Assume, you are given a previous model M to improve on. Currently you observe that the model has an accuracy of 80% (any metric). How do you go further about it?One simpleway is to build an entirely different model using new set of input variables and trying better ensemble learners. On the contrary, I have a much simpler way to suggest. It goes like this:What if I am able to see that error is not a white noise but have same correlation with outcome(Y) value. What if we can develop a model on this error term? Like,Probably, youll see error rate will improve to a higher number, say 84%. Lets take another step and regress against error2.Now we combine all these together :This probably will have a accuracy of even more than 84%. What if I can find an optimal weights for each of the three learners,If we found good weights, we probably have made even a better model. This is the underlying principle of a boosting learner. When I read the theory for the first time, I had two quick questions:Illanswer these questions in this article, however, in a crisp manner. Boosting is generally done on weak learners, which do not have a capacity to leave behind white noise. Secondly, boosting can lead to overfitting, so we need to stop at the right point.Look at the below diagram :We start with the first box. We see one vertical line which becomes our first week learner. Now in total we have 3/10 mis-classified observations. We now start giving higher weights to 3 plus mis-classified observations. Now, it becomes very important to classify them right. Hence, the vertical line towards right edge. We repeat this process and then combine each of the learner in appropriate weights.How do we assign weight to observations?We always start with a uniform distribution assumption. Lets call it as D1 which is 1/n for all n observations.Step 1 . We assume an alpha(t)Step 2:Get a weak classifier h(t)Step 3: Update the population distribution for the next stepwhere,Step 4 : Use the new population distribution to again find the next learnerScared of Step 3 mathematics? Let me break it down for you. Simply look at the argument in exponent. Alpha is kind of learning rate, y is the actual response ( + 1 or -1) and h(x) will be the class predicted by learner. Essentially, if learner is going wrong, the exponent becomes 1*alpha and else -1*alpha. Essentially, the weight will probably increase if the prediction went wrong the last time. So, whats next?Step 5 : Iterate step 1  step 4 until no hypothesis is found which can improve further.Step 6 : Take a weighted average of the frontier using all the learners used till now. But what are the weights? Weights are simply the alpha values. Alpha is calculated as follows:Irecently participatedin anonline hackathonorganized by Analytics Vidhya. For making the variable transformation easier, I combined both test and train data in the file complete_data. I started with basic import function and splitted the population in Devlopment, ITV and Scoring.Here is all you need to do, to build a GBM model.As you will see after running this code, all AUC will come extremely close to 0.84 . I will leave the feature engineering upto you, as the competition is still on. You are welcome to use this code to compete though. GBM is the most widely used algorithm. XGBoost is another faster version of boosting learner which I will cover in any future articles.I have seen boosting learners extremely quick and highly efficient. They have never disappointed me to get high initial scores on Kaggle and other platforms. However, it all boils down to how well can you do feature engineering.Have you used Gradient Boosting before? How did the model perform? Have you used boosting learners in any other capacity. If yes, I would love tohear your experiences in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/
How Good is the Executive business analytics program by Jigsaw Academy and MISB Bocconi ?,Learn everything about Analytics|Introduction|Who has launched the course?|Who is the Target Audience?|Structure of the Program|Teaching methodology and the approach to program|Industry Interaction and Projects|Profile of the Faculty|Plans for the Upcoming Batch|Placements of the Current Batch|Overall Verdict,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Learn Gradient Boosting Algorithm for better predictions (with codes in R)|Analyst  Ahmedabad  (1 + years of Experience)|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The demand of skilled data science / analytics professionals is surging with every bit of data being collected across the globe. Same is true for India as well. In order to address the demand, a lot of Indian universities have come up with Business Analytics programs. It was only a matter of time before which some of global universities start offering these programs as well.Last year, MISB Bocconi (Indian presence of SDA Bocconi, one of the top ranked European business schools) tied up with Jigsaw Academy to offer Executive Program in Business Analytics. Over this year, we have received several queries about this program from our audience. Like always, we made it a point to do a deep dive about the course, talk to several stakeholders and then publish our review about it.Lets look at the details of the program to start with.The program has been launched jointly by MISB Bocconi and Jigsaw Academy. The classes will be taught by both Bocconi faculty and Jigsaw faculty, depending on the topic. All the contact classes will be hosted in Bocconis Indian offshore campus in Powai, Mumbai, Maharashtra.Students will receive a final Certification by SDA Bocconi School of Management, which is a top ranked European B-School.My Take: I think the joint tie up of a Business school with international repute and domain excellence of Jigsaw makes a formidable combination to run the course.The ideal candidates for thisprogram are working professionals / executives keen to learn and switch to business analytics.Ideally, the participants have around 5 years of experience. The candidates need to have good quantitative skills, logical mind and a strong driveto learn analytics / data science.As per the designed curriculum, this course requiresparticipants to take 6 days of leave in a 10 month period. This program renders60 hours of live online classes held over 24 weekends (on Saturdays). They also have over 100 hours of pre-recorded videos.Given the mix of management and analytics topics, the course is expected tohelp participants make transition to managerial roles in business analytics. The case studies and topics are also geared at giving the participants managerial perspectives on how to solve business problems using analytics.My take: I think the target audience and the format makes sense. 120 hours of classroom interaction over 18 dayslook sufficient to nourish the learning appetite of candidates, which isalso complemented well by online classes and pre-recorded material.This Executive Program in Business Analytics is a mix of in-person and online classes.Itprovides120 hours of in person classes spread in6 contact sessions of 3 days each. These contact classes are held at MISB Bocconi campus in Powai, Mumbai.The aim of this course is to provide a deep understanding on various disciplines of business analytics, which includesstatistics, machine learning, time series, R, SAS, Big Data (Pig, Hive, Sqoop, Flume, HBASE, SPARK and Oozie), visualization, text mining, web analytics and digital marketing. In addition, this course also provides theopportunity to participate inBusiness Analytics sectoral applications in various sectors including Telecom, Banks, Retail, Healthcare and Insurance as well as areas such as Finance, Marketing, and Operations.My take: The course structure looks fairly comprehensive to me. They not only cover various disciplines of business analytics, but also big data, data science and machine learning. I think it will be a very intense program for a working professional, but who has benefited in this world without the hard work. The focus on Big Data is also unique to this course as most other courses do not delve in this topic in detail.One thing which came out pretty strongly on talking to the faculty was the focus on structured thinking. The program hasbeen designed to provide students a proportionate blend ofanalytics andmanagement principles to help the students becomesmart indealing with business challenges.The focus on problem solving with data using the right approach, framework, methodologies, and tools also stood out. The program has been studded with real life data sets with managerial perspectives. This allows candidates to get familiar with the complexities linked with real life datasets.My Take: This is usually a tricky situation for a lot of programs. Some choose to focus on hands on tools and techniques, while others focus more on structured thinking. MISB & Jigsaw plan to keep a fine balance between the two. I think this is where having partners with different strengths really helps. I looked at various other programs, including the one at IIMBangalore. I found the EPBA to be more relevant to the kind of skillsI was looking to develop. I wanted a course which focused less ontheoretical concepts and more on practical implementations, a coursewhich does not explain how a model has been derived but what are thepros and cons, how to use them? evaluate them? The EPBA seemed to fitall this criteria and promised to give me enough hands on knowledge,enabling me to judge a model and take decisions using data. Also thecertification from Bocconi would really add immense value to my careerprofile.          Sanjiv Jha, Director of Engineering Data and Analytics, Komli Media,Student at               Executive Program in Business Analytics (EPBA)I believe, industry interaction plays a crucial role in preparing candidates for upcoming business challenges. This program offers student interaction with atleast5-7 leaders in the analytics industry. Their current batch has already interacted with experts including the head of analytics in Tata Motors, a global auto company, lead consultant in Mobelium, a telecom analytics company, senior director at Axtria, a data analytics company, and head of sales in Tableau, a data visualization company.Students spend around 30-40% of their contact class hours with industry leaders including guest lectures and training by industry experts.Above all, students are also instructed to work onlive projects involving real data and business problems. This live project is expected to provide hands onindustry experience by working out data driven solution for a business problem.The project duration is for 8-10 weeks, which is from mid-September to November.I managed to get few examples of projects that other students have done in the past:My take: Initially 30-40% of contact classes with industry leaders seemed a bit lower thanother programs offered in India which provide up to 50% interaction with industry. However, an important point to understand here is, the quality of interaction matters most and the interaction here seemed top notch.This program has one of the finestpanelof faculties convenedfrom the top institutions of the country.On an average, their faculties have more than10 years of teaching experience in analytics or management.For the first batch, they havefaculty available from Bocconi, Italy, IIM Bangalore, IIM Lucknow, IIM Ranchi to undertakeclasses. In addition, their faculty panel also comprises ofJigsaw Academy faculty and industry experts.Here are the profiles of some of their teaching faculties:Veronica Vecchi, SDA Professor of Public Management and PolicyMaurizio Poli, SDA Professor of Quantitative MethodsOtherfaculty details from Bocconi and Jigsaw can be foundhere.I tried to drill down further to obtain some significant information. And fortunately, I managed to talk to Sarita Digumarti, co-founder and COO, Jigsaw Academy to vouch on this part. She added, The first batch of the program has got off to a great start, and we have started finalizing the curriculum for the second batch. We have received excellent feedback on both the content coverage as well as the industry interactions, so we are ensuring that we include additional topics that cover emerging tools and advanced analytics methods. We are also attempting to includewell known industry experts for the industry interactions. Well also striveto cover a broad spectrum of sectors there including E-commerce, FS, Telecom, Automotive etc. In addition, well beincreasing the total number of business case studies by 50%, so that students get an opportunity to get hands on with a larger variety of problems and solutions.The first batch will complete in December 2015. The team seems confident that 70%of the participants looking for jobs will be able to make a transition to analytics through this program.Current numbers: Out of a batch of 23, I was given to understand that 4 people are not looking for a job change and 2 people have already shifted jobs.Note:The first batch of this program will end in December 2015. So, it is still some time away.My take: If the team is able to deliver 70% placement in year 1, it would be quite commendable. As per the best of my knowledge, not many institutes have been able to achieve this rate in the first year of operation. If you compare the curriculum of the 3-4 main programmes, this isthe only one where one actually gets (a) the integration of big datawith analytics and (b) the actual hands-on approach to learning bigdata technologies like hadoop, pig, hive etc.               Prof. Maitrayee Mukherjee, Faculty at IIM Kashipur, Student atExecutive                      Program in Business Analytics (EPBA)I think that the combination of a top European Business school and a strong domain partner like Jigsaw make a formidable combination. It is already showing early signs of success. There are some finer tweaks the team needs to make in the upcoming batch  higher industry interaction being one of them. The team is well aware of this and has the capabilities to make these changes.At a price close to 4 Lakhs, I think this is a deal worth considering, if you are looking for an executive program in Business Analytics.For more details please find below the link to thecourse brochure.",https://www.analyticsvidhya.com/blog/2015/09/review-executive-program-business-analytics-misb-bocconi/
"Analyst  Ahmedabad  (1 + years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|How Good is the Executive business analytics program by Jigsaw Academy and MISB Bocconi ?|Startups bringing analytics and data science closer to you!|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  AnalystLocation  AhmedabadAbout employer  ConfidentialJob description: Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.IQR is a Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalyst  Ahmedabad",https://www.analyticsvidhya.com/blog/2015/09/analyst-iqr-consulting-ahmedabad-1-years-experience/
Startups bringing analytics and data science closer to you!,Learn everything about Analytics|Introduction:|Hotjar|PredictionIO:|Grow|Proximus|Smarking|Prayas Analytics|SumoMe|Insights Web Analytics|Neuron|Data Stories|End Notes:,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage|Share this:|Like this:|Related Articles|Analyst  Ahmedabad  (1 + years of Experience)|News: Spring intake for the Analytics program at Praxis Business School|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Science and analytics are changing every industry as you read this article! If you have been following Analytics Vidhya lately, I am sure you would know how strongly I believe in it. You can look at some exciting start-ups coming out, which are using analytics and data science to solve problems more efficiently than ever before.However, finding the right talent and tools is still difficult for a lot of companies wanting to use analytics. For example, how many SMEs can use dashboards to make real time decisions? or how many bloggers across the world know reading habits of their readers?This week I bring you a collection of products / companies, which are making anaytics more accessible to an end user. They are taking the data from customers, doing the heavy lifting and delivering results in simple and impactful manner  what else would a business owner ask for! Check them out below:Most organizationsrely on Google Analytics, Omniture or MixPanel to understand their customers, while they are on the companys website. But, all of them feel restrictive in some way or the other. Hotjar aims to solve this problem.Hotjar is a new and easy way to truly understand your web and mobile site visitors. Identify your hottest opportunities for improvement using Heatmaps, visitor recordings, funnels, forms, polls, surveys & user testers. If you just wat one take away from this article  check them out.PredictionIO bring predictive models for developers, ready to deploy out of box. It is an open-source machine learning server for developersand data scientists to create predictive engines for production environments, with zero downtime training and deployment. It is built on top of Apache Spark, HBase and Spray.Simple BI dashboards for small and medium businesses. The interfaces of the product looks really intuitive. I think Grow has actually understood their customer well and have designed the product to take care of a lot of inherent needs of the customer. I definitely feel compelled to recommend the product to a lot of SMEs who dont own a lot of data expertise in house.Analytics for the retailers and real world.Proximus allows retailers to accurately understand the behaviour of their shoppers in-store. Their technology is based on two things: aunique indoor location and navigation engine and a great BigData architecture that analyse the behaviour of thousands of shoppers every day. Check out their site for a nice visual story about how it is delivered.Big Data meets parking problem. Smarking allows parking managers to optimize pricing and staffing based on historical and predicted demand, improving revenue and reducing costs. They aim to combine parking data with traffic data, weather data and other data sources to create a prediction for parking managers. This should then help these managers price optimally for parking space.A Y combinator company, which is enabling A/B testing based on computer vision. The best part is that they use existing CCTV camera, so the store owners can start seeing the benefits before investing any thing in hardware. They are enabling brick and mortar business test in similar way as an e-Commerce company does.SumoMe provides content Analytics for blogs. With an integration in less than 37 seconds and reasonably priced plan, you get insights into reading behaviour and engagement of your customers. The primary application of SumoMe being Heatmaps of your content, which would help you decide your content strategy. They integrate with almost every blogging platform and are being used by more than 150,000 websites! How did I miss them before?Insights web analytics acts as a layer above Google Analytics and convert the data from Google Analytics to actionable insights. For example, they convert the effect of higher bounce rate on a page to the impact on traffic, customer and ultimately sales! Insights Web analytics changes the way people use Google Analytics, which happens to be the most commonly used product for web analytics.Neuron aims to apply machine learning to create segmentation of customers. they aim to drive personalization for customers to solve problems like customer attrition and life cycle management.Create data stories on click of the button. DataStories wants to crunch the time taken to convert an excel sheet with data to a story. The end user can select the kind of story he is looking from the data and get data stories on click of a button.Here was a collection of startups which are makingdata science and analytics more accessible to common people. Some of them are trying to change the data science industry itself  exciting times, as I said!Have you come across products / services which are makingData Science more accessible? If yes, I would love to hear more about them from you.",https://www.analyticsvidhya.com/blog/2015/09/startups-analytics-data-science-closer/
News: Spring intake for the Analytics program at Praxis Business School,Learn everything about Analytics|End Notes:,"Introduction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Startups bringing analytics and data science closer to you!|Statistical Analyst  Ahmedabad  (1-2 years of experience)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The academic scene in analytics continues to heat up. Freshers / Professionals continue to enter analytics industry with a determined objective of becoming successful in this discipline.Praxisreceived phenomenal response forJuly 2015 batch, and the steadyinflow of applications / queries over the following months, conveyed the keenness and passion among students to learn analytics. Thus, tapping this opportunity at hand,Praxis has decided to add one more batch to their existing program. Therefore, Praxis will now have two batches every year  a July intake and January intake respectively.If you are new to Praxis, Id suggest you to check out ourcomprehensive Review  Business Analytics Program, Praxis Business Schoolto gain knowledge on the crucial aspects of this program.We decided to catch up with dignitaries from PraxisBusiness School got a chance tochat with the Program Director  Dr. Prithwis Mukerjee about the January intake in specific and the program in general. Here are the excerpts from ourconversation with Dr. Mukerjee:                                           Dr. Prithwis MukherjeeAV(Analytics Vidhya): What prompted you to launch a Spring (January) batch? PM(Prithwis Mukerjee): Two things. One, we have been receiving an increasing number of applications for our July batch every year. Instead of making one batch very large (and hence a little difficult to manage given the number of lab hours we have in the program), we decided to opt for two batches in a year, just like in the US  a Fall batch and a Spring batch. Second, unlike the MBA program, Analytics does not conform to any well-established campus placement season  there is a growing need for well trained analytics professionals across the year and we get requests for people from firms all the time. Two batches a year may actually help in matching the demand for manpower better.AV: While most of the training programs in analytics in India is delivered either online or in hybrid mode, why does Praxis offer its entire program in instructor-led classroom training mode, that too full-time  almost forcing professionals to give up their jobs to join the program?PM: We believe that Data Science is a complex domain and requires immersive learning. It is neither a tool/ technique that one can pick up in a couple of months, nor a top-up for a business manager or consultant so that he can talk about analytics a little more intelligently than before. We are teaching a class that has chosen analytics as a career. We reckon the need to ensure that we cover the breadth and depth required to empower the student to step confidently into the world of analytics with the ability to contribute both at a conceptual and an operation level. This kind of training has to be face-to-face, full-time and rigorous, with lots of hands-on labs and projects. It also requires intensive interaction with a faculty team that blends theoretical soundness with cross functional expertise. In fact, a good number of our students come with the experience of having done one or two online courses  in that sense online and hybrid courses are helping generate interest among professionals.AV: Have there been any changes to the curriculum since we last (April 2015) met ?PM:Yes and No! The curriculum broadly addresses the same areas  statistics, machine learning, technology and business domain knowledge  the three pillars of data science. However, we change the curriculum by about 30% every time we teach  we have replaced a couple of courses and the content within each course evolves continuously. With access to more data and better connect with the industry; we are spending increasingly more time in learning by doing. In a nutshell, the program comprises 500+ hours of classroom training in the broad areas of statistics, data warehousing, data mining, data visualization, econometrics using tools like R, SAS, Excel, Tableau, Qlikview and exploring big data technologies like Hadoop, Pig, Hive, NoSQL (MongoDB). In addition, the program exposes students to business concepts and functional areas like finance, marketing, operations research and the application of analytics in domains like marketing, retail, web, finance, and telecom.AV: Who is this course for? What kind of students are you targeting?PM: We are looking for students who are serious about building a career in Analytics  people who would like to grow into the roles of data scientists and not just data analysts  and who are willing to commit one year of their lives to rigorous analytics learning. We look for people who are analytical, have a fondness for numbers and are not afraid of technology. They could be from any stream, as long as their mathematics foundation is strong and they can demonstrate problem-solving and storytelling skills. In terms of experience, we do not have any filters  people with experience in streams like IT or any business functions are welcome; so are fresh graduates/ post graduates as long as they are academically equipped to cope with the rigor of the program.AV: What is the typical batch profile like  in terms of background and experience?PM: 90% of our students are from Engineering, Economics, Mathematics, Statistics, Pure Science backgrounds. There are students from the commerce/ BBA stream as well  with strong math and analytical skills. Around 30% of these students are post graduates (including MBAs). The current batch has an average work experience of about 3 years  ranging from 0 to more than 5 years  even up to 10 years. There is substantial diversity in the type of work experience they come with  in firms like Accenture, Amazon, Axis Bank, CapGemini, Coal India, CTS, Deloitte, E & Y, HDFC Bank, Hewlett Packard, IBM, ICICI Bank, Infosys, Persistent Systems, Royal Bank of Scotland, TCS, TechMahindra, Thomson Reuters, Wipro etcAV: What about geography? Is Kolkata location a constraint?PM: Not at all. A great thing is that this program has made Kolkata a destination for students from across the country. Students come not only from Kolkata and the East  in fact the highest percentage of people is from the South (Bangalore, Hyderabad, Chennai and smaller cities) and North (Delhi NCR). This batch has seen an increasing traction from the Mumbai/ Pune belt. Kolkata is a great city to study in  plus I guess students will head to wherever they find value. Also, since placements happen across the country, the program becomes location-agnostic.AV: One of the USPs of your program is the placement support. Hows that doing?PM: Oh yesWe make them work so hard during the program, its only natural we support their transition into the analytics world! On a serious note, this is linked with the philosophy of the program  we are dedicated to help people serious about a career in analytics to take the first step. This involves creating a relevant program, involving the industry in designing and delivering courses and driving placements. Students become a part of the Praxis Placement Program which is designed to identify appropriate career opportunities and facilitate their transition into the exciting area of analytics. We now have a 4-batch strong analytics network that is doing exceptionally well. These students play a key role in helping their juniors in their career progression.AV: Interesting that you mention the alumni. That was my next question  how has been the industry acceptance of the program?PM: The Program was launched on the advice of some Analytics industry practitioners  and we have continued to keep a strong connect with the industry. This ensures that our curriculum is always relevant. I must make special mention of PwC and ICICI Bank  who provide knowledge support and participate in teaching as well, along with professionals from other companies.The program has been well received by the industry. Companies like Abzooba Inc, Axis Bank, BRIDGEi2i Analytics Solutions, Genpact, Happiest Minds, HDFC Bank, Hewlett Packard, HSBC Analytics, ICICI Bank, ICRA Techno Analytics, IDC, ITC Infotech, Karvy Analytics, Kie Square Consulting, Millward Brown, PwC, ValueLabs etc. have participated in the campus recruitment process. A good thing is that almost all these companies are repeat purchasers  which gives us a lot of joy.Praxis Analytics alumni have transitioned into companies like Cardekho.com, Dell Analytics, Fidelity, IBM, Lowes, Quikr.com, Target, TCS, Walmart Labs, WNS etc. and are doing every well. The Analytics job market is white hot, and the Praxis community is exploiting the opportunity to the hilt!AV:How should one apply? Is there a selection process?PM: Students need to complete the online application process on the website www.praxis.ac.in. Admissions are open right now. We do have a selection process  I will reiterate that this is a serious program for serious analytics aspirants. We cannot be unfair and admit candidates who we feel are not equipped to create a career for themselves in this field. We thus shortlist candidates on the basis of their applications and they need to appear for a written aptitude test and a personal interview (online or in person). Course commences from the second week of January 2016.Interested candidates can register here for more informationI think a Spring batch is an interesting move from Praxis. Praxis is clearly riding on the momentum gained from the hard work of team in last few years. I am personally excited about how analytics training arena is shaping up in India.If you have any questions about the program or the process for Spring batch, please feel free to post them here.",https://www.analyticsvidhya.com/blog/2015/09/news-praxis-business-school-spring-intake-analytics-program/
"Statistical Analyst  Ahmedabad  (1-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|News: Spring intake for the Analytics program at Praxis Business School|My recommendations  SlideShare Presentations on Data Science|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Statistical AnalystLocation  AhmedabadAbout employer ConfidentialJob description: Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.IQR is a Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.ResponsibilitiesQualification and Skills RequiredEducation & ExperienceInterested people can apply for this job can mail their CV to[emailprotected]with subject asStatistical Analyst Ahmedabad",https://www.analyticsvidhya.com/blog/2015/09/statistical-analyst-iqr-consulting-ahmedabad-1-2-years-experience/
My recommendations  SlideShare Presentations on Data Science,"Learn everything about Analytics|Introduction|Howcan you learn from these Presentations?|Data Science, in General|Machine Learning|Neural Networks & Deep Learning|Data Science Competitions||End Notes","How to Become a Data Scientist?|Data By The People, For The People|The Science of Managing Data Scientists|What does a Data Scientist do?|A Statistician View on Big Data and Data Science|Myths and Mathematical Superpowers of Data Scientists|How to Interview a Data Scientist?|Introduction to Machine Learning|Introduction to Supervised Machine Learning and Pattern Classification|Machine Learning and Hadoop|Machine Learning and Data Mining: 12 Classification Rules|Machine Learning and Mahout|10 Lessons learnt from Building Machine Learning Systems|Deep Neural Networks|Neural Network and its Applications|Introduction to Artificial Neural Networks|Artificial Intelligence|Tutorial on Deep Learning|Tutorials on Deep Learning and Applications|Deep Learning for Natural Language Processing|Hands on Deep Learning in Python|Tips For Winning Data Science Competitions|10 R Packages to Win Kaggle Competitions||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Statistical Analyst  Ahmedabad  (1-2 years of experience)|Powerful Guide to learn Random Forest (with codes in R & Python)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every one has their own learning sytle! If youneed close hand holding and guidance  an easy going MOOC is probably the best place to start. However, if you are a quick learner and dont need some one to explain a lot of context, some one who prefers to glance through concepts, apply them a bit and then again refer back to these concepts  presentations can be really handy!The beauty about learning from presentations is that you can quickly zoom in on the section you want to and come out at your own pace  no one decides that for you! If it fits your learning style, they can bethe quickestway of learning a new concept in data science, even faster than videos & blogs. These presentations allow you to achieve much more in less time. So, here is a list of some best data science presentations I have come across on SlideShare.Among all forms of media, presentations are probably the most crisp & to the point! You can use them in multiple ways:Just like our gigantic universe is composed of several heavenly bodies, data science is composed of several disciplineswhich completes its universe. Ive categorized the presentations in following segments:Note: These presentations have been shortlisted on the basis of their relevancy and views.You are free to access the list in any way sequence you wish. These presentations are in no order.Views  51,469
 Views  17,053
 Views  64,302
 Views  81,156
 Views  36,305
 View  42,095
 View  68,565

 Lets get a bit technical now!Views  102,230
 Views  14,677
 Views  17,822
 Views  19,503
 Views  40,394
 Views  30,750

 Views  24,610
 Views  59,550
 Views  43,554
 Views  40,180
 Views  14730
 Views  Uploaded Recently
 Views  Uploaded Recently
 Views  Uploaded Recently

 Views  9010
 Views  53, 828
 Not a presentation person? You can learn from YouTube videos, GitHub tutorials, data science books, reddit discussions, data science bootcamps and what not! Yet, heres another surprise for you.I enjoyed curating this list and wanted to add more presentations to this list, but I realized that the list isgetting too lengthy. Not to worry, Ill surely add it in my next article. In this article, I have enlisted useful resources (presentations) in Machine Learning, Deep Learning, Neural Networks and Deep Learning.This isnt an exhaustive list. This list comprises of my most liked presentations. If you wish to recommend me any useful presentation on slideshare which I might have missed, feel free to share it with me in the comments section.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/slideshare-presentations-data-science/
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. What is a Decision Tree ? How does it work ?|2. Regression Trees vs Classification Trees|3. How does a tree decide whereto split?|4. What are the key parameters of tree modelingand how can we avoid over-fitting in decision trees?|5. Are tree based models better than linear models?|6. Working with Decision Trees in R and Python|7. What are ensemble methods in tree based modeling ?|8. What is Bagging? How does it work?|9. What is Random Forest ? How does it work?|10. What is Boosting? How does it work?|11. Which is more powerful:GBM or Xgboost?|12. Working with GBM in R and Python|13. Working with XGBoost in R and Python|14. Where to practice ?||End Notes,"Types of Decision Trees|Important Terminology related toDecision Trees|Advantages|Disadvantages|Gini
|Chi-Square|Information Gain:|Reduction in Variance|Setting Constraints on Tree Size|Tree Pruning|How does it work?|Advantages of Random Forest|Disadvantages of Random Forest|Python & R implementation|How does it work?|GBM in R (with cross validation)|GBM in Python|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Case Study For Freshers (Level : Medium)  Call Center Optimization|Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|
Analytics Vidhya Content Team
|64 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), its important to learn these algorithms and use them for modeling.This tutorial is meant to help beginners learn tree based modeling from scratch. After the successful completion of this tutorial, one is expected to become proficient at using tree based algorithms andbuild predictive models.Note: This tutorial requires no prior knowledge of machine learning. However, elementary knowledge of R or Python will be helpful. To get started you can follow full tutorial in R and full tutorial in Python. You can also check out the Introduction to Data Science course covering Python, Statistics and Predictive Modeling.Decision treeis a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used inclassification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator ininput variables.Tree based modeling in R and PythonExample:-Lets say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket inleisure time. Now, I want to create a model topredict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.This is where decision tree helps, it will segregate the students based on all values of three variable andidentify the variable, which creates thebest homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To dothis, decision tree uses various algorithms, which we will discuss in the following section.Types of decision tree is based on the type of target variable we have. It can be of two types:Example:-Lets say we have a problem to predict whether a customer will pay his renewal premium with an insurance company(yes/ no). Here we know that income of customer is asignificant variable but insurance company does not have income details for all customers. Now, as we know thisis an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable.Lets look at the basic terminology used with Decision trees:These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leavesare the the bottom & roots are the tops (shown below).Both the trees work almost similar to each other, lets look at the primary differences & similaritybetween classification and regression trees:The decision of making strategic splits heavily affects a trees accuracy. The decision criteria is different for classification and regression trees.Decision trees use multiplealgorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.The algorithm selection is also based on type of target variables. Lets look at the four most commonlyused algorithms in decision tree:Gini says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.Steps toCalculate Gini for a splitExample:  Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In thesnapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini .Split on Gender:Similar for Split on Class:Above, you can see that Gini score for Split on Gender is higher than Split on Class,hence, the node split will take place on Gender.You might often come across the term Gini Impurity which is determined by subtracting the gini value from 1. So mathematically we can say,Gini Impurity = 1-GiniIt is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it bysum of squares of standardizeddifferences between observed and expected frequenciesof target variable.Steps toCalculate Chi-square for a split:Example: Lets work with above example that we have used to calculate Gini.Split on Gender:Split on Class:Perform similar steps of calculation for split on Class and you will come up with below table.Above, you can see that Chi-squarealso identify the Gender split is more significant compare to Class.Look at the image below and think which node can be described easily. I am sure, your answer isC because it requires lessinformation as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say thatC is a Pure node, B is less Impure and A is more impure.Now, we can build aconclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory isa measure to define this degree of disorganization in a systemknown as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50%  50%), it has entropy of one.Entropy can be calculated using formula:-Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.Steps to calculate entropy for a split:Example:Lets use this method to identify best split for student example.Above, you can see that entropy forSplit on Gender is the lowest among all,so the tree will split onGender. We can derive information gain from entropy as 1- Entropy.Till now, we have discussed the algorithms for categorical target variable. Reduction in variance is an algorithm used forcontinuoustarget variables (regression problems). This algorithm uses the standard formulaof variance to choose the bestsplit. The split with lower variance is selected as thecriteria to split the population:Above X-bar is mean of the values, X is actual and n is number of values.Steps to calculate Variance:Example:- Lets assign numerical value 1 for play cricket and 0 for not playing cricket. Now follow the steps to identify the right split:Above, you can see that Gender split has lower variance compare to parent node, so the split would take place on Gender variable.Until here, we learnt about the basics of decision trees and the decision making process involved to choose the best splits in building a tree model. As I said, decision tree can be applied both on regression and classification problems. Lets understand these aspects in detail.Overfitting is one of the key challenges faced while modeling decision trees. Ifthere is no limit set of a decision tree, it will give you 100% accuracy on training set becausein the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting ispivotal while modeling a decision tree and it can be done in 2 ways:Lets discuss both of these briefly.This can be done by using various parameters which are used to define a tree.First, lets look at the general structure of a decision tree:The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It isimportant to understandthe roleof parameters used in tree modeling. These parameters are available in R & Python.As discussed earlier, the technique of setting constraint is agreedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Lets consider the followingcase when youre driving:There are 2 lanes:At this instant, you are the yellow car and you have 2 choices:Lets analyze these choice. In the former choice, youll immediatelyovertake the car ahead and reachbehind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!This is exactly the difference between normal decision tree & pruning. A decision tree with constraints wont see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.So we know pruning is better. But howto implement it in decision tree? The idea is simple.Note that sklearns decision tree classifier does not currentlysupportpruning.Advanced packages like xgboost have adoptedtree pruning in their implementation.But the library rpart in R, provides a function to prune. Good for R users!If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees? Many of us have this question. And, this is a valid one too.Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Lets look at some key factors which will help you to decide which algorithm to use:For R users and Python users, decision tree is quite easy to implement. Lets quickly look at the set of codes that can get you started with this algorithm. For ease of use, Ive shared standard codes where youll need to replace your data set name and variables to get started.In fact, you can build the decision tree in Python right here! Heres a live coding window for you to play around the code and generate results:For R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc.In the code above:For Python users, below is the code:The literary meaning of word ensemble is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree basedmodels.Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, how much on an average are the predicted values different from the actual value. Variance means, how different will the predictions of the model be at the same point if different samples aretaken from the same population.You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors.Ensemble learning is one way to execute this trade off analysis.Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking. In this tutorial, well focus on Bagging and Boosting in detail.Baggingis a technique used to reduce the variance of our predictionsby combiningthe resultof multipleclassifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:
The steps followed in bagging are:Note that, herethe number of models built is not a hyper-parameters.Higher number of models are always better or may give similarperformance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.There are various implementations of bagging models. Random forest is one of them and well discuss it next.Random Forest is considered to be a panaceaof all data science problems. On a funny note, when you cant think of any algorithm (irrespective of situation), use random forest!Random Forestis a versatile machine learning method capable of performing both regression and classification tasks.It also undertakesdimensional reduction methods, treats missing values, outlier valuesand other essential steps of data exploration,and does a fairly good job.It isa type of ensemble learning method, where a group of weak models combineto form a powerful model.In Random Forest, we growmultipletrees as opposedto a single tree in CART model (see comparison between CART and Random Forest here, part1 and part2).To classify a new object based on attributes, each tree gives a classification and we say the tree votes for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.It works in the following manner.Each tree is planted & grown as follows:Tounderstand more in detail about this algorithm using a case study, please read thisarticle Introduction to Random forest  Simplified.Random forests have commonly known implementations in R packages and Python scikit-learn. Lets look at the codeof loading random forest model in R and Python below:RCodeDefinition: The term Boosting refers to a family of algorithms whichconverts weak learner to strong learners.Lets understand this definition in detail by solving a problem of spam email identification:How would you classifyan email as SPAM or not? Like everyone else, our initial approach would beto identify spam and not spam emails using following criteria. If:Above, weve defined multiple rules to classifyan email into spam or not spam.But, do you think these rules individually are strong enough to successfully classifyan email? No.Individually, these rules arenot powerful enough to classify an email into spam or not spam.Therefore, these rules are called as weak learner.To convert weak learner to strong learner, well combine the prediction of each weak learner using methods like:For example: Above,we have defined 5 weak learners. Out of these 5, 3 arevoted asSPAM and 2 are voted as Not a SPAM. In this case, by default, well consider an email as SPAM because wehave higher(3) vote for SPAM.Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, How boosting identify weak rules?To find weak rule, we applybase learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.Heres another question which might haunt you, How do we choose different distribution for each round?For choosing the right distribution, here are the following steps:Step 1: Thebase learner takes all the distributions and assign equal weight or attention to each observation.Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, weapply the next base learning algorithm.Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs fromweak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting payshigherfocus on examples which are mis-classied or have higher errors by preceding weak rules.
There are many boosting algorithms which impart additional boost to models accuracy. In this tutorial, well learn about the two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost.Ive always admired the boosting capabilities that xgboost algorithm. At times, Ive found that it providesbetter result compared to GBM implementation, but at times you might find that the gains are just marginal. When I explored more about its performance and science behind its high accuracy, I discovered many advantages of Xgboost over GBM:Before we start working, lets quickly understand the important parameters and the working of thisalgorithm. This will be helpful for both R and Python users. Below is the overall pseudo-code of GBM algorithm for 2 classes:This is an extremely simplified (probably naive) explanation of GBMs working. But, it will help every beginners to understand this algorithm.Lets considerthe important GBMparameters used to improve model performance in Python:Apart from these, there are certain miscellaneous parameters which affect overall functionality:I know its a long list of parameters but I have simplified it for you inan excel file which you can download from thisGitHub repository.For R users, using caret package, there are 3 main tuning parameters:Ive shared the standard codes in R and Python. At your end, youll be required to change the value of dependent variable and data set name used in the codes below. Considering the ease of implementing GBM in R, one can easily perform tasks like cross validation and grid search with this package.XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Its feature to implement parallel computing makes itat least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.R Tutorial: For R users, this is a complete tutorial on XGboost which explains the parameters along with codes in R. Check Tutorial.Python Tutorial: For Python users, this is a comprehensive tutorial on XGBoost, good to get you started. Check Tutorial.Practice is the one and true method of mastering any concept. Hence, you need to start practicingif you wish to master these algorithms.Till here, youve got gained significant knowledge on tree based models along with these practical implementation. Its time that you start working on them. Here are open practice problems where you can participate and check your live rankings on leaderboard:Tree based algorithm are important for every data scientist to learn. In fact, tree models are known to provide the best model performance in the family of whole machine learning algorithms. In this tutorial, we learnt until GBM and XGBoost. And with this, we come to the end of this tutorial.We discussed about tree based modeling from scratch. We learnt the important of decision tree and how that simplistic concept is being used in boosting algorithms. For better understanding, I would suggest you to continue practicing these algorithms practically. Also, do keep note of the parameters associated with boosting algorithms. Im hoping that this tutorial would enrich you with complete knowledge on tree based modeling.Did you find this tutorial useful ? If you have experienced, whats the best trick youve used while using tree based models ? Feel free to share your tricks, suggestions and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/
3 Compelling reasons why you must compete in Data Hackathon 3.x,Learn everything about Analytics|Introduction|Participate and compete against more than 500 data scientists across the globe|Benchmark yourself against the best brains in the industry (did I mentionon a live problem?)|Create a profile and portfolio employers can look at|Register Now,"The show is about to begin!|Share this:|Like this:|Related Articles|Powerful Guide to learn Random Forest (with codes in R & Python)|Business Analyst  Gurgaon / Pune (0-2 years of experience)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Give a man a fish  feed him for a dayTeach him how to fish  feed him for a life time!The person who said it has put in wisdom of a life time in this quote! Further, if there is one field where this fits aptly  it would be programming and data science. You cant teach coding or data science without asking your students to get their hands dirty!While we had successfully created a community of data science professionals over the last 2 years focused on learning analytics, we were still not satisfied. While people learnt through ourblog,discussion forumandtrainings comparison platform, we still had an itch to do lot more. We knew that while this is a good start, there is a lot more which can be done.I am excited, super excited, hyper excited today to launch our new hackathon platform  DataHack! We will open it up to a small group of people later tonight those who have registered for our Online Data Hackathon 3.x. There are several reasons why I think you should participate in Online Data Hackathon, if you havent already registered. Infact, I can speak about it for hours!But, Ill restrict it to a few key reasons in interest of time. Read on!This is the biggest hackathon we have ever conducted  the competition will be intense and the pressure will be immense. It is bound to have nail biting finish and only the people who give up during the contest dont win! Every one who finish fightingeither win a prize or walk away from the hackathon becoming a better data scientist with this experience.Where else can you find hundreds of people working on the same problem whilesharing their approach and insights?The experience is unparalleled.Our hackathons start from the place wheredata science capabilities of several organizations stop! This is not a homework in your Stats101 class  this is real stuff. These are problems faced by organizations  be prepared to deal with missing values, outliers, incoherent data  because that is how real life problems are.But that is not what I want to emphasize here. What gives us a bigger kick is that you get benchmarked against the best brains in the industry. We do all the hard work to bring the best data scientists on this platform, so that you compete against them. You get real time feedback on how your solution fares against that of your peers and these rockstars from across the globe.This is not it  you also get solutions from people who participated including the winner. What were various hypothesis people were working on? What gave them the lift which you did not get? How can you improve your model? What would they do, if they had some more time? All of this gets discussed on our discussion portal. Dont believe it  check it out for yourself hereWhile you are busy racing against the data scientists across the globe, webuild your profile and portfolios so that employers can find you when they want to hire the best resources from the industry. If you are thinking how would the employers know? Dont worry  we will do it for you. You focus on learning data science and we will sort out the remaining pieces of the puzzle. Here is a little preview into the action  weare already working with more than 50 employers in the industryand helping them hire the best talent.Does that sound like lot of action? I promise it is! I also promise that this is just the start of bigger things coming your way. To start with,this is just the start of an action packed weekend for more than 500 data scientists across the globe. And if you are sitting on the fence thinking whether to participate or not  jump in right now. There is no looking back from here!",https://www.analyticsvidhya.com/blog/2015/09/compelling-reasons-compete-data-hackathon/
"Business Analyst  Gurgaon / Pune (0-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|3 Compelling reasons why you must compete in Data Hackathon 3.x|Business Associate Consultant  Gurgaon / Pune (2-4 years of experience)|
Jobs Admin
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Business AnalystLocation  Gurgaon / PuneAbout employerConfidentialJob description: Business Analysts are active participants in creating and delivering solutions for our clients and project teams. BAs leverage their analytic skills to derive insights and solve problems. In particular, BAAs employ advanced analytic techniques in areas such as modeling, simulation and optimizationResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asBusiness Analyst  Gurgaon/Pune",https://www.analyticsvidhya.com/blog/2015/09/business-analyst-zs-associates-gurgaon-pune-years-experience/
"Business Associate Consultant  Gurgaon / Pune (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Gurgaon / Pune (0-2 years of experience)|Learn to use Forward Selection Techniques for Ensemble Modeling|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Business Associate ConsultantLocation  Gurgaon / PuneAbout employerConfidentialJob description:Business Associate Consultants coordinate the execution of project logistics for clients and project teams. BAACs leverage their analytic skills to derive insights and solve business problems across an array of sales and marketing engagements. In particular, BAACs apply problem-solving frameworks to client issues, support clients in change initiatives, and employ advanced analytic techniques to help guide client decisions.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asBusiness Associate Consultant  Gurgaon/Pune",https://www.analyticsvidhya.com/blog/2015/09/business-associate-consultant-gurgaon-pune-2-4-years-experience/
Learn to use Forward Selection Techniques for Ensemble Modeling,Learn everything about Analytics|Introduction|Principles involved in the Process|Understanding the R code|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Business Associate Consultant  Gurgaon / Pune (2-4 years of experience)|Infographic: Data Visualization Tools For Data scientists & analysts|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ensemble methods have theability to provide much needed robustness and accuracy to both supervised and unsupervised problems. Machine learning is going to evolve more and more and computations power becomes cheap and the volume of data continues to increase. In such a scenario, there is a limit to the improvement you can achieve by usinga single framework and attempting to improve its predictive power(using modification in variables).Ensemble Modeling follows the philosophy of Unity in Strength i.e. combination of diversified base models strengthens weak models.The success of ensemble techniques spreads across multiple disciplines likerecommendation systems, anomaly detection, stream mining, and web applications where theneed for combination of competing models is ubiquitous.If you wish to experience the powerful nature of ensemble, try using supervised and unsupervised model for a single task and merge their results. Youll notice thatthe merger delivered better performance.Last week,we talked about a simple method to ensemble multiple learners through neural networks. We created a black box, which tookin all learners and gave us a final ensemble predictions. In this article, I will take an alternate route(using R) to solve the same problem, with a higher control in the ensemble process. I have leveraged technique discussed in one of the Cornells paper Ensemble Selection from Libraries of Models.The underlying principle remains the same :Ensemble of diverse and high performance models are better than individual models.Forward Selection of learners : Imagine a scenario where we have 1000 learner outputs. We start with an empty bag and then in every iteration we add a new learner which benefits the bagon performance metric.Selection with Replacement : To select a new addition for the bag, we put our hand in the stack of 1000 leaner and pull out the best of the lot. Even if a learner is found to be a good fit, well still use this learner in the next iteration stack.Bagging of Ensemble Models : Ensemble learners are prone to over-fitting. To avoid this, we take a sample totry ensembling. Once we are done, we again use another sample. Finally, we bag all these models together using simple average of predictions or maximum votes.The R code to ensemble multiple learners is not very easy to follow. Hence,I have added steps(explanation) at every line of code for ease of understanding:Step 1 : Load the train and test filesStep 2 : Specify basic metrics like number of bags/iterations, number of learners/modelsStep 3 : Load the library needed for the performance metric (optional)Step 4 : Calculating individual performance of models for establishing benchmarksStep 5 : Using all the metrics specified apply forward selection with replacement in 1000 bagsEven though baggingtacklesmajority of over-fitting cases, still it is good to becautious aboutover-fitting in ensemble learners. A possible solution is set aside oneset ofpopulation untouched and try performance metrics using this untouched test population. The two methods mentioned are no way exhaustive list of possible ensemble techniques. Ensemble is more of an art than science. Most of the master kagglers are masters of this art.",https://www.analyticsvidhya.com/blog/2015/09/selection-techniques-ensemble-modelling/
Infographic: Data Visualization Tools For Data scientists & analysts,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Learn to use Forward Selection Techniques for Ensemble Modeling|Top Datapreneurs who made data science what it is today|
Kunal Jain
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Text < Table < Charts < Interactive Charts|P.S. Online Data Hackathon 3.x coming up on 5th & 6th September. Register now,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Here is a famous quote on learning:We Learn . . .
10% of what we read
20% of what we hear
30% of what we see
50% of what we see and hear
70% of what we discuss
80% of what we experience
95% of what we teach others.If we create a similar orderingon ability to interpret data in various forms  the order will surely look like this:On the other hand, the amount of data which needs processing and interpretation is increasing by the second. Combined, these two factors are making data visualizationan integral form of data science workflow  probably more important than ever before.In order to address this need for creating simple, yetpowerful visualization, there are multiple tools which can come in handy. However, a lot of analysts & data scientists are not aware of these tools. Hence, we have created an infographic, which provides high level overview of various tools people use for creating data visualization.What do you think about the infographic? Do you think, there are other tools which should be part of this infographic? If yes, let us know through comments below",https://www.analyticsvidhya.com/blog/2015/09/infographic-tools-data-visualization/
Top Datapreneurs who made data science what it is today,Learn everything about Analytics|Introduction|Who is a datapreneur?|Data Products|Data Science Services|Trainings|Data Science Communities|End Notes,"Jim Goodnight& John Sall (SAS)|Christian Chabot(Tableau)|Michael F Koehler (TeraData)|Arun C Murthy (Hortonworks)|Michael Saylor (MicroStrategy)|Roman Stanek (GoodData)|Lars Bjork( Qlik )|Alex Carp (Palantir)|Christophe Bisciglia (Cloudera, WibiData)|Josh James (Domo)|Dwight Merriman (MongoDB)|John Schroeder (MapR)|Jonathan Ellis (Datastax)|Gurjeet Singh (Ayasdi)|Carlos Guestrin (Dato)|Srikant Velamakani (Fractal)|Dhiraj C. Rajaram (Mu-Sigma)|Arnab Gupta (Opera Solutions)|Anil Kaul (AbsolutData)|Andrew Ng (Coursera)|Sebastian Thrun (Udacity)|Vik Paruchuri (Dataquest)||Gaurav VohraandSarita Digumarti (Jigsaw Academy)|Lovleen Bhatia (Edureka)|Anthony Goldbloom (Kaggle)|Gregory Piatetsky-Shapiro (Kdnuggets)|Rohit Sivaprasad (Datatau)|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Infographic: Data Visualization Tools For Data scientists & analysts|Capstone Projects  Great Lakes Business Analytics Program|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I am deeply passionate about 2 fields: Data Science and start-ups.I feel data science is the only way to enable logical decisions in this world and constantly improve yourself through self-exploration. On the other hand,I am also a start-up guy! Before on-boardingAnalytics Vidhya, I had done multiple internships across various start-ups.I have an inherent fascination and deep rooted respect for entrepreneurs. I consume any kind of content I get, which helps me understand them better. I hope one day, Ill be able to create significant value for this world we live in through my own pursuit!So, it was only a matter of time before I put out this list  A list of datapreneurs across the globe.A datapreneur is basically an entrepreneur focused on data science. In interest of avoiding the definition conflict, I am using data science in a broad sense  any effort made to extract information from data. So this would include Big Data, Business Intelligence, Business Analytics, Predictive Modeling, Machine learning etc.Please see that I am excluding entrepreneurs using data science to solve other problems. Hence, you will not see Larry Page & Sergey Brin in this list! Nor would you see Airbnb or Uber. Similarly, I have not included the likes of Doug Cutting (creator of Hadoop & Lucene). Hope this gives you a purpose of this list.Further, in order to represent this list in meaningful manner, I have divided the datapreneurs by their focus areas, namely:A few more things to note before we look at the list:The story of SAS began way back in 1976 when Jim and John decided to team up to build a product in business analytics known as SAS. SAS was originally developed to analyze agricultural data.The consistent growth registered in their revenue since 1976 has established their dominance over the years (SAS revenue in 2014 $3.09 Bn). SAS would probably be the worlds largest private software company.Jim Goodnight holds a doctorate in statistics from North Carolina State University, where he was a faculty member from 1972 to 1976. Harvard Business School named him a Great American Business Leader for his leadership of a business that has changed the way Americans have lived, worked and interacted. Jim is currently the CEO of SAS.John Sall is the Co-Founder and Executive Vice President at SAS. He earned a bachelors degree in history from Beloit College and a masters degree in economics from Northern Illinois University (NIU). Both NIU and NC State awarded him honorary doctorate. Hes the primary architectfor JMP Software. As Sall says,  the goal of JMPis to make statistical modeling as friendly, accessible and informative as possible, and to increase the efficiency of experimentation.Tableau is one of the most preferred product in the arena of data visualization. With in10 years of its existence, Tableau has positioned itself as a leader in Gartner report. It is one of the fastest growing technology companies in the world.Mr. Christian Chabot co-founded Tableau Software, Inc. and has been its Chairman and Chief Executive Officer since 2003. He co-founded Tableau with Chris Stolte and Pat Hanrahan.Christian has led the company to nine consecutive years of record sales and customer growth. He is the author of Understanding the Euro: The Clear and Concise Guide to the Trans-European Currency (McGraw-Hill, 1998). He holds an MBA from Stanford University, an M.Sc from the University of Sussex, and a BS from Stanfords School of Engineering.Teradata Corp. provides analytic data solutions, including integrated data warehousing, big data analytics and business applications. As of 2014, this company registered a market cap of $7.7 billion. Started in 1976, Teradata has evolved over the years with changing technologies and delivering the best solutions to their customers.Micheal is associated with Teradata for the last 37 years. He became CEO of Teradata in 2010, and the year 2011 became the best year in its company historywith more revenue growth and new customers added than any single year in its history.Teradata is one of the largest company in big data space today.Koehler holds a bachelors degree in business administration from the University of Delaware.Hortonworks was founded in 2011 by 24 engineers in the original yahoo Hadoop team, Hartonworks is known to have accumulated more Hadoop experience under one roof than any other organization in the world. The Hortonworks Data Platform combines the innovation of open source under the governance of the Apache Software Foundation with enterprise software rigor.The co-founding team of Hortonworks includesArun C Murthy, Alan Gates, Devaraj Das,Eric Baldeschwieler, Mahadev Konar, Owen OMalley, Sanjay Radia, Suresh Srivivas. The revenue made by hortonworks in 2014 was $100 million. Moreover, it registered an enormous y-o-y growth of 90%in the first quarter of 2015.Prior to co-founding Hortonworks, Arun was responsible for all MapReduce code and configuration deployed across the 42,000+ servers at Yahoo!.MicroStrategy was founded in 1989 in Wilmington, DE, by fellow MIT alumni Michael J. Saylor and Sanju Bansal. MicroStrategys early focus was on data mining software for businesses which later evolved into providingthe most flexible, powerful, scalable, and user-friendly analytics and identity management platforms, offered either on premises or in the cloud.MicroStrategy is positioned by Gartner, Inc. in the Leaders quadrant in Gartners 2013 Magic Quadrant for Business Intelligence and Analytics Platforms report, and in the Challengers quadrant in Gartners 2013 Magic Quadrant for Mobile Application Development Platforms report.Mr. Saylor has served as Chairman of the Board of Directors and Chief Executive Officer since founding MicroStrategy in November 1989. Mr. Saylor holds aB.S. in Aeronautics and Astronautics and B.S. in Science, Technology and Society from the Massachusetts Institute of Technology. Mr. Saylor is the author of bestselling book The Mobile Wave.GoodData provides an open analytics platformthat supports both ITs need for Data Governance, security and oversight and business users desires for self-service Data Discovery. GoodData carries a vision to change the way people experience business intelligence.Roman Stanek is a passionate entrepreneur and industry thought leader with over 20 years of high-tech experience. His latest venture, GoodData, was founded in 2007 with the mission to disrupt the business intelligence space and monetize big data. Prior to GoodData, Roman was Founder and CEO of NetBeans, the leading Java development environment (acquired by Sun Microsystems in 1999) and Systinet, a leading SOA governance platform (acquired by Mercury Interactive, later Hewlett Packard, in 2006).Qlik was originally founded by the late Bjrn Berg and Staffan Gestreliusin 1993.Their mission was to build an entirely new type of software; one able to reflect and embody the complex workings of the human mind, to create a product able to provide a truly intuitive user experience. QlikView is one of amazing software available for data visualization in the market today.Currently, Lars Bjork is the CEO of QlikTech.He holds an MBA from the University of Lund, Sweden and a Degree in Engineering from the Technical College in Helsingborg.Before Qlik, Mr. Bjrk held several positions as CFO at companies such as ScandStick and Resurs Finance. UnderMr. Bjrks leadership, Qlik has grown 3x in revenues.Started in 2004, Palantir helps companies to find answers to the most complex questions by making products for human driven analysis of real world data.Palantir hasreceived more than $215 million in U.S. government contract work since 2009, while FORBES estimated that the company took in about $450 million in revenue in 2013.Palantir was founded by Alex Carp, Peter Theil, Joe Lonsdale and Stephen Cohen. Alex is the current CEO of Palantir, a Palo Alto-based software firm worth an estimated $20 billion.Alex holdshas a bachelors degree from Haverford College, a Doctor of Jurisprudence degree from Stanford University,and a doctorate in neoclassical social theory from Frankfurt University.Christophe Bisciglia is the Founder of Cloudera, the leading developer and distributor of Hadoop, the open source software that powers the data processing engines of the worlds largest and most popular web sites and WibiData. At WibiData, he helpsto create exceptional customer experiences through intelligent application of data.Prior, Christophe worked as a senior engineer at Google where he founded and led Googles Academic Cloud Computing Initiative, which provides Google hosted computational resources to facilitate education and research to universities around the world. He completed his education from University of Washington.Josh founded Domo in 2010 to bring change in the process of managing business by top executives of the company by helping them to drivevalue from the tens of billions of dollars spent on traditional business intelligence systems.Prior to Domo, Josh served as CEO of Omniture, a SaaS-based web analytics company that he co-founded in 1996 and took public in 2006. Omniture was the number one returning venture investment out of 1,008 venture capital investments in 2004, as well as the number two performing technology IPO of 2006. He was named the 2006 Ernst & Young Entrepreneur of the Year and Brigham Young Universitys Technology Entrepreneur of the Decade. In 2009, he facilitated Omnitures sale to Adobe for $1.8 billion.Recently, MongoDB has been included in the list of 17 best startups to work in America. To help people learn MongoDB, they also own MongoDB University where youll find training courses for every set of audience.MongoDB was founded by Dwight and Eliot Horowitz in the year 2007. Dwight holds a computer science degree from Miami University.In 1995, he co-founded DoubleClick (acquired by Google for $3.1 billion) and served as Chief Technology Officer for 10 years. Earlier he was Co-Founder, Chairman, and the original architect of Panther Express (merged with CDNetworks), a content distribution network (CDN) technology. Dwight is also a Co-Founder of, and investor in, Business Insider and Gilt Groupe.This company relies on Apache Hadoop and claims to be the largest Hadoop distribution playerthat sells Hadoop projects and support services.Their core product is the MapR software that runs on clusters of commodity servers. The software is available in three editions.MapR was founded in 2009 bycurrent CEO, John Schroeder. John holds a bachelor degree in computer science from SIU. Prior to MapR, John held executive positions in number of software companies such asCalista Technologies, Rainfinity, Brio Technologies etc.Nearly 90% of MapRs revenues are derived from subscription to their software. The company is expected is close 2015 at a revenue figure of $200 million.Datastaxdevelops solution based on commercially supported, enterprise-ready Apache Cassandra, the open source NoSQL database technology widely-acknowledged as the best foundation for tackling the most challenging big data problems.Datastax was co-founded by Jonathan Ellis and Matt Pfeil. This company was founded in 2010 with a motive of transform business and enable life-changing customer experiences for some of the worlds most innovative companies.The valuation of the company is over $830 million, which is more than double the valuation of the company 12 months ago.Datastax has gotover 25 percent of the Fortune 100 as customers, and more than 400 customers around the world in 50 countries.Over the past 12 months, its been in hyper-growth mode.Ayasdiis an advanced analytics company that provides machine learning software to Fortune 500 companies to solve their complex data challenges. Ayasdi pioneered the use of Topological Data Analysis (TDA), to simplify and accelerate complex data analysis.Ayasdi was founded in 2008 at Stanford. Gurjeet Singh is Ayasdis CEO and Co-Founder. Gurjeet holds a B.Tech. from Delhi University, and a Ph.D. in Computational Mathematics from Stanford University.Before starting Ayasdi, he worked at Google and Texas Instruments. Gurjeet was named by Silicon Valley Business Journal as one of their 40 Under 40 in 2015.Ayasdi has shown promising growth in the past few years and raised $55 million in March this year.Dato is building the fastest machine learning analytics engine for graph and tabular data sets. Its key features include ultra fast data analytics, best-in-class predictive modelling, production-ready data science.Carlos is the CEO and co-founder of Dato (formerly GraphLab, Inc.) and the Amazon Professor of Machine Learning in Computer Science & Engineering at the University of Washington. He holds a PhD in Computer Science from Stanford University. He worked as associate professor at Carnegie Mellon University.Dato is the company behind the fastest and most complete platform for building predictive and intelligent applications.Srikanth Velamakanni, is the Co-founder and CEO of Fractal Analytics, one of the worlds most respected pure play Analytics companies. He co-founded this company with Pranay Agarwal.Srikanths passion for analytics makes him a thought leader in the space, an active public speaker and evangelist at large.Srikanth has a BS in Electrical Engineering from IIT-Delhi and MBA degree from IIM Ahmedabad. A former investment banker, he co-founded Fractal more than 14 years ago. Prior to Fractal, he worked on structured debt transactions and collateralized bond obligations at ANZ Investment Bank and ICICI.Dhiraj is the Founder and Chief Executive Officer of MuSigma. Dhiraj holds an MBA from the University of Chicago. He also received an M.S. in Computer Engineering from Wayne State University and a Bachelors degree in Electrical Engineering from College of Engineering Guindy, Anna University. He founded Musigma to build a man-machine ecosystem which helps companies institutionalize the decision supply chain.He is the mastermind behind streamlining companies process and workflows to deliver profitable growth. He is known to actively leadpeople towards achieving organizations vision and strategic direction, building teams, aligning organizational resources to a customer centric vision. Prior toMu Sigma, he lent his advisory services to Booz Allen Hamilton and PricewaterhouseCoopers.The beginning of Opera solution was marked by a loss in Netflix contest. After losing the netflix contest to build a recommender system, Arnab Gupta, Founder & CEO, hired data scientiststo work creating Netflix-like algorithms for things that other businesses need: fraud detection, marketing, stock picking, risk management, procurement, etc.
Arnab founded opera solutions in 2004 and has guided the company in becoming a premier center of Big Data science and practice. He holds a MBA degree from Harvard Business School. Prior to opera solutions, he also founded a business consulting firm Mitchell Madison Group and Zeborg, a business intelligence company. Opera is estimated to have annual revenue of $100 million.AbsolutData was founded by Anil Kaul withSudeshna Duttaand Suhale Kapoorin the year 2001with offices in US and India. This firm made $20 million in revenues in the year 2013.Absolutdata intends to empower companies to make better decisions through optimal use of data. In 2008, Absolutdata was rankedamong the fastest-growing companies in India and Asia by the Deloitte Technology Fast 50 India and the Deloitte Technology Fast 500 Asia Pacific programs.Anil has over twenty years of experience in marketing, strategic consulting and quantitative modeling. He has a PhD in quantitative marketing from Cornell University.He is a recognized thought leader in the industry, having published articles in leading management and academic journals such as the McKinsey Quarterly, Marketing Science, Journal of Marketing Research andInternational Journal of Research in Marketing.Coursera, one of the largest open source courses platform available on internet today, was founded by Andrew Ng withDaphne Koller.He also serves as Chief Scientist of Baidu, a Chinese language search engine.Previously, Andrew served as an Associate Professor of Computer Science at Stanford University and was the Director of the Stanford Artificial Intelligence Lab, the main AI research organization at Stanford, with 15 professors and about 150 students/post docs. In Fall 2011, he was the instructor of ml-class, a Machine Learning class that was one of Stanfords first massive online courses, and had an enrollment of over 100,000 students.He is a recipient of the Alfred P. Sloan Fellowship, and the 2009 IJCAI Computers and Thought award, one of the highest honors in AI.Sebastian founded Udacity with a motive of democratizing education. Sebastian dreams of providing world class education to everyone so that everyone has a fair chance to apply for any job and this world will be a better place to live. He wishes to empower all people in the world through quality education.In addtion, from the past 12 years, Sebastian is associated with Stanford University as a Research Professor. He was also fortunate to become a Google Fellow, as well as the inventor of the autonomous car and project lead on Google Glass. Sebastian has been named the 5th Most Creative Person in Business (Fast Company), among the 50 Smartest People in Tech (Fortune), and highlighted in 50 Best Inventions of 2010 (Time).Vik founded Dataquest in November 2014 with a motive of helping people learn real world data science skills. Dataquest allows you to gain hands on experience on R, Python, Linear Algebra and other essentials modules of data science interactively.Prior to Dataquest, Vik started multiple companies namely actigram labs, equirio.He has won various kaggle competitions in automated essay scoring and stock trading in the year 2012. His expertise lies in machine learning, web development and mobile development. He completed his graduation from University of Maryland. More than just knowledge, data questallows you to work on real datasets problems which adds to your learning confidence.Jigsaw aims to meet the growing demand for talent in the field of analytics by providing industry-relevant training and education to develop business-ready professionals.Gaurav is CEO of Jigsaw academy and has founded Jigsaw with Sarita Digumarti in the year 2010. Hehas over 10 years of experience in the field of analytics and has worked across multiple verticals including financial services, retail, FMCG, telecom, pharmaceuticals and leisure industries. Gaurav holds a MBA degree from IIM Bangalore. His expertise lies inProcess Migration and offshoring; Business Planning; Operations; Loyalty Management; CRM; Marketing Analytics; Risk Management.Edureka aims to make learning easy, interesting, affordable and accessible to millions of learners across the Globe.With the use of technology, excellent instruction and flexible schedule itaims to become the largest and most engaging learning platform on earth.Lovleen Bhatia, an IIT-BHUalumni co-foundedEdureka in mid-2011 with Kapil Tyagi, anIIT Bombayalumni.Lovleen Bhatia is instrumental in setting up the strategic direction of the company, defining companys priorities, driving the mission and companys work culture. He brings with him 13 years of expertise across domains like technology, digital marketing, growth hacking, customer acquisition and hiring. Prior to co-founding Edureka, he headed the R&D at DbyDX Labs.Anthony Goldbloom founded Kaggle withBen Hamner, a startup that helps companies outsource thorny problems to data crunchers like him.Back in 2008, He had a reporting internship at The Economist in Londona position he snagged by winning an essay contest. While working on a story about predictive modeling, he spoke to people at large companies who told him how hard it was for them to make sense of data they had collected. Many companies didnt even have anyone who could do it.That gave Goldbloom the idea: he would create a website where data scientists could compete to win cash in their spare time by solving such problems for companies.Gregory is a role model for us at Analytics Vidhya. He started Kdnuggets back in 1997, when people had no idea what data science is. Just imagine  this was before Google started! The best way to understand Kdnuggets is to think of them as Craigslist of analytics  If you need any thingin data science / analytics, Kdnuggets is probably your best bet.Gregory Piatetsky-Shapiro, Ph.D. is the Founder ofKDnuggets, which provides consulting in the areas of business analytics, data mining, data science, and knowledge discovery. He has extensive experience developing CRM, customer attrition, cross-sell, segmentation and other models for some of the leading banks, insurance companies, and telcos. He also worked on data analysis of clinical trial, microarray, and proteomic data for several leading biotech and pharmaceutical companies. He is also the co-founder of ACM SIGKDD, the leading professional organization for Knowledge Discovery and Data Mining.Rohit Sivaprasadstarted an online community similartohacker news for the subject hes interested in: data science. Hedoesnt want DataTau to turn into a virtual land of obscure technical questions and answers like Stack Overflow. Instead, hed like lots of people from all over the world to come to one site and talk with each other on a higher level.Sivaprasad himself is a data evangelist and has built up much of his data science knowledge on his own, through online courses. He has contributed to the scikit-learn machine learning toolkit for Python.P.S. I truly believe Kunal should be part of this list. He started Analytics Vidhya in April 2013 with a vision to remove the silos of data science knowledge across the world. Today,Analytics Vidhya is the worlds largest and fastest growing analytics community (as per Alexa ranking). However,our publishing guidelines prohibit me to put him in this list.I think we owe a lot to these datapreneurs for what they have done to create these products, services, trainings and communities. It is difficult to imagine the data science eco-system with out them. I also think that this is just the start of a revolution in making and we will see a lot of action in this area in coming years.What do you think about these contributions? Do you think, there are others who should be added to this list? Or some contribution I have missed? Do let me know your thoughts in comments below.",https://www.analyticsvidhya.com/blog/2015/08/top-entrepreneurs-big-data-analytics-data-science/
Capstone Projects  Great Lakes Business Analytics Program,Learn everything about Analytics|Introduction|Why share these projects?|Overview  How do these projects work?|Importance of Capstone Project for students|The Projects|End Notes,"Project 1: Credit Default Prediction|Project 2: Statistical Analysis of Consumer Durables Retail Sales|Project 3: Web & Text Mining  Sentiment Analysis|Project 4: Credit card  Risk Analytics|Project 5: Developing Least Cost Effective Intervention in Schools|Project 6: Lead Generation for Health Insurance Firms using Web and Social Media Data|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Top Datapreneurs who made data science what it is today|Interactive Data Visualization using Bokeh (in Python)|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have been associated with the Great Lakes Business Analytics Program as a visiting faculty for some time now. This is one of the ways, I interact with people wanting to make their career in analytics. The energy and the desire to make a career in business analytics in some of these students is just contagious!As part of the curriculum, people undergoing this course are expected to complete a Capstone Project. This Capstone Project is like a climax at the end of a yearlong movie  it looks difficult at the start, but everything you do in the entire movie comes together at the end.I had the privilege of sitting in the evaluation panel for a few projects. In todays article, I plan to give an overview of what happened in these projects.Well, there are a few reasons, why I wanted to share these projectsin form of an article with a larger audience:I have already informed Great Lakes about the article and Ive published this with their consent.As mentioned before, every person undergoing this course is expected to submit a Capstone Project. In order to do this project, a project group is formed with participants from diverse backgrounds. These groups, then work along with a mentor to complete these projects over a period of 4 5 months. About 80% of these projects happen with mentors from industry. Mentors from various companies like Value Labs, Whiskkers Marketing Pvt. Ltd., BRIDGE i2i, Analytics Vidhya participate in helping students with these projects.Analytics is a very hands on subject. I personally believe that until and unless you complete a few projects end to end (i.e. from data collection to building a model to implementing a model), your knowledge and approach stays theoretical in nature.The same view was reflected by students, when I talked to a few of them. Here is one of the student sharing his experienceundergoing the capstone project:During our course at Great Lakes, we learnt about various techniques like, ways of cleaning-up data, multiple algorithms for analysis and modeling, goodness of fit to check model fitness and had numerous sessions on business intelligence. But it is the Capstone Project that introduced us to the real world challenges of applying our learning, testing adequacy, usefulness and subjectivity of certain algorithms. We also got some real time insight into the processes and steps involved in a typical analytics project. Without Capstone Project our learning would have remained incomplete.  Sriram Alagarsamy, Great Lakes PGPBA AlumnusIn the rest of this article, Ill provide a few examples of the Capstone Projects, which happened in last batches of Great Lakes and the industry sponsors / mentors were happy to share the project details through this article.Students: Kanthimathi Gayatri Sukumar, S.R.Balaji, S.Ramnath, Rudragouda Patil, Mithur NiranjanIn this project, the students had built several models to predict credit default for 2 wheeler loans in villages in India. The variables available to the candidates included:The group had built several models including Logistic regression, Classification tree  CART, Classification tree (Ensemble)  Random forest, Neural networks and Discriminant analysis. Here is the comparison of various modeling techniques:The group finally used Logistic regression because of high accuracy and ease of insight generation (rather than a black box approach to modeling)Students: Soumya Tiwari, Remina Surendrababu, Arnab Majumdar, Vijayalekshmi L, Siju JosephThe project team analyzed about 0.5 million transactions for a retail chain having 20 branches across Chennai. The team collected the data for a period of three years to find a reasonable practical solution to the following business problems faced by a consumer durable retail chain:This group used R and built a recommender system using Apriori algorithm.Student: Anjana AgrawalAnjana has been doing freelance IT consulting to clients across the globe. Her Capstone Project required her to enable a political organization to understand peoples concerns, views and sentiments on topics in near real time. She also has a publication, which came out of her work (commendable! I must say).The project started by obtaining input text data from Twitter for one of the leading political organization and the tweets were scored for sentiment analysis. Output generated in terms of Common Key Words, Association between common key words, Sentiment Score was observed and analyzed for a period of approx. 3 weeks. This output was important for the organization considering the fact that this political organization was campaigning for the elections scheduled in near future and based on the Citizen sentiments, organization was able to refine their next course of action.The project ended up being used by the party in real time and helped them in addressing the concerns of its people quickly.Students: Amit Madan, Praveen PanwarThe objective of this project was to develop a model to calculate the probability of default (PD Model) for credit card holders of a bank. The model was supposed to be run after the customer has spent more than a year with the bank and predicts the probability of default over a period of next 3 months.Amit & Praveen had more than 2 million data points with 20 variables (18 numeric and 2 character) to start with. They created more than 50 additional variables based on their internal brainstorming. After removing variables with high correlation and the insignificant variables, they used Logistic Regression to build the model.The final model had a concordant ratio of 91.4% and a list of 230%. Here is the Lorenz curve for the same.It was amazing to see the amount or learning the students had undergone through this project. Even if you join a bank as an analyst, it would take you a few years before you can get your hands dirty on a problem like this!Amit reflected similar sentiment in my discussion with him later:There are three important steps in the computational modelling of any physical process (1) Problem definition (2) mathematical model and (3) computer simulation. CAPSTONE provided for an opportunity to synthesize domain expertise, mathematical representation and computational skills to re-produce a modelling framework so as to be able to cull out the insights present in a data. It was by virtue of CAPSTONE, we could apply all of what we had learned at Great Lakes and it helped generate an urge to keep undertaking similar assignments for sustained mental stimulation.   Amit Madan, Great Lakes PGPBA, AlumnusStudents: Balamuril S, RamKumar R, Senthil J, Sreeraman K, Sriram AThis group applied the power of analytics to impact government aided schools in 5 districts. They created clusters of schools, developed insights about their performance and translated them into quality improvement program for these schools.They further created a model to rank these schools, simulate the effect of government interventions and then measure them as well. The project should not only impact the 5 districts, but can have a far larger impact by implementing these insights for schools across the nation.Students: Harpreet Kaur, Aneet Sachdeva, Peeyush TiwariThis group first collected open data and social media data, applied text mining and natural language processing (NLP) techniques to extract features out of this data. They then applied various clustering and bag of words techniques to pull out insights into insurance purchase behavior. Some of these groups were identified as high potential leads for Health Insurance products.As you can see, the projects varied from predicting default in rural regions to helping parties contest elections. It was heartening to see the output from this group of people, most of whom had very little knowledge about analytics a year ago.I hope this article would provide learners in analytics with a few ideas to do their own Capstone Projects and would have provided a glimpse of the program to those interested. You can read more details about the program here.",https://www.analyticsvidhya.com/blog/2015/08/capstone-project-great-lakes-business-analytics-program/
Interactive Data Visualization using Bokeh (in Python),Learn everything about Analytics|Introduction|What is Bokeh?|What does Bokeh offer to a data scientist like me?|Visualization with Bokeh|End Notes,"Benefits of Bokeh:|Challenges with Bokeh:|Charts||||Plotting|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Capstone Projects  Great Lakes Business Analytics Program|Senior Executive  Policy Bazaar  Gurgaon  (1 + years of Experience)|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",|Charts Example-1: Create a bar chart and visualize it on web browser using Bokeh|Chart Example-2:Compare the distribution of sepal length and petal length of IRIS data set using Box plot on notebook|Chart Example-3:Create a lineplot to bokeh server|Plot Example-1: Create a scatter square mark on XY frame of notebook|Plot Example-2:Combine two visual elements in a plot|Plot Example-3: Add ahover tool and axis labels to above plot|Plot Example-4: Plot map of India using latitude and longitude data for boundaries,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently, I was going through avideo from SciPy 2015 conference, Building Python Data Apps with Blaze and Bokeh, recently held at Austin, Texas, USA. I couldnt stop thinking about the power these two libraries provide to data scientists using Python across the globe. In this article, I will introduce you to the world of possibilities in data visualization using Bokeh and why I think this is a must learn/use library for every data scientist out there. Source:bokeh.pydata.orgBokeh is a Python library for interactive visualization that targets web browsers for representation. This is the core difference between Bokeh and other visualization libraries. Look at the snapshot below, which explains the process flow of how Bokeh helps to present data to a web browser.Source:Continuum AnalyticsAs you can see, Bokeh has multiple language bindings (Python, R, lua and Julia). These bindings produce a JSON file, which works as an input for BokehJS(a Javascript library), which in turn presents data to the modern web browsers.Bokeh canproduce elegant and interactive visualization like D3.jswithhigh-performance interactivity over very large or streaming datasets. Bokeh can help anyone who would like to quickly and easily create interactive plots, dashboards, and data applications.I started my data science journey as a BI professional and then worked my way through predictive modeling, data science and machine learning. I have primarily relied on tools like QlikView & Tableau for data visualization and SAS & Python for predictive analytics & data science. I had near zero experience of using JavaScript.So, for all my data products or ideas, I had to either outsource the work or had to pitch my ideas through wire-frames, both of which are not ideal for building quick prototypes. Now, with Bokeh, I can continue to work in Python ecosystem, but still create these prototypes quickly.Given the benefits and the challenges, it is currently ideal to rapidly develop prototypes. However, if you want to create something for production environment, D3.js might still be your best bet.To install Bokeh, please follow the instruction given here.Bokeh offers both powerful and flexible features which impartssimplicity andhighlyadvanced customization. It providesmultiple visualization interfaces to the user as shown below:In this article, we will look at first two interfaces charts & plotting only. We will discuss models and other advance feature of this library in next post.As mentioned above, it is a high level interface used to present information in standard visualization form. These forms include box plot, bar chart, area plot, heat map, donut chart and many others. You can generate these plots just by passing data frames, numpy arrays and dictionaries.Lets look at thecommon methodology to create a chart:To understand these steps better, let me demonstrate these steps using examplebelow:We will follow above listed steps to create a chart:In the chart above, you can see the tools at the top (zoom, resize, reset, wheel zoom) and these tools allows youto interact with chart. You can also look at the multiple chart options (legend, xlabel, ylabel, xgrid, width, height and many other) and various example of charts here.To create this visualization, firstly, Ill import the iris data set using sklearn library. Then, follow the steps asdiscussed aboveto visualize chart in ipython notebook.Prior to plotting visualization to Bokeh server, you need to run it.If you areusing a conda package, you can use run command bokeh-serverfrom any directoryusing command. Else,python ./bokeh-server command should work in general. For more detail on this please refer this link Deploying Bokeh Server.There are multiple benefits of Plotting visualization on Bokeh server:To start plotting on Bokeh server,I have executed the command bokeh-serverto initialize it followed by the commands used for visualization.Plotting is an intermediate-level interface that is centered around composing visual glyphs. Here, you create a visualization by combining various visual elements(dot, circles, line, patch & many others)and tools(hover tool, zoom, Save, reset and others).Bokeh plots created using the bokeh.plotting interface comes with a default set of tools and visual styles. For plotting, follow the below steps:To understand these steps better, let me demonstrate these steps using examplesbelow:Similarly, you can create various other plots like line, wedges & arc, ovals, images, patches and many others, refer this link to see various example.For more details on visual attributes and tools refer these links:Note: I have data for polygon of latitude and longitude for boundaries of India in a csv format.I will use that for plotting.Here, we will go with patch plotting, lets look at the commands below:In this article, we looked at creating visualizations using Bokeh and methods to present themon notebooks, html and bokeh-server. We also looked at the methods to create customized visualization using plotting, here you can combine multiple visual elements to represent information.In my next post on Bokeh, I will discuss about models and more interactive features of visualization. For example, I have co-ordinates boundaries for each country across the world. Now,I want to create a visualization to represent each country map in a cycle (loop). It starts plotting with a country map and wait for a 3 sec and iterates for all countries. Sounds interesting! Isnt it?Did you find this article useful? Do let us know your thoughts about this article in the comments section below.",https://www.analyticsvidhya.com/blog/2015/08/interactive-data-visualization-library-python-bokeh/
"Senior Executive  Policy Bazaar  Gurgaon  (1 + years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Interactive Data Visualization using Bokeh (in Python)|R-analyst Cheat sheet: Data Visualization in R|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  SeniorExecutiveLocation  GurgaonAbout employer  Policy BazaarJob description: We have an urgent requirement for Executive / Sr. Executive  MIS meant for Policybazaar.com for Gurgaon location.Skills RequiredQualificationInterested people can apply for this job can mail their CV to[emailprotected]with subject as SeniorExecutive  Policy,https://www.analyticsvidhya.com/blog/2015/08/mis-executive-policy-bazaar-gurgaon-1-years-experience/
R-analyst Cheat sheet: Data Visualization in R,Learn everything about Analytics,"Introduction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Senior Executive  Policy Bazaar  Gurgaon  (1 + years of Experience)|Big Data / Analytics based startups at Y Combinator, Summer 2015 batch|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data visualization has become an integral part of data science work flow. Hence, your main tool needs to have strong capabilities on both the fronts  data analysis as well as data visualization. Gone are the days when you could live with a tool which was good only in one of these.With this change in landscape, R has gained immensepopularity because of its awesomedata visualization capabilities. With a few lines of code, you can create beautiful charts and data stories. R has awesome libraries to createbasic and more evolvedvisualizations like Bar Chart, Histogram, Scatter Plot, Map visualization, Mosaic Plot and various others.Here is the cheat sheet of popular visualization for representing data.You can keep this handy for your use.:For those who want to copy the relevant codes, you can download the PDF version of the sheet here.To view this complete article, visit Comprehensive Guide to Data Visualization in R.If you wish to gain a complete knowledge on data visualization, heres the ultimate guide on data visualization.",https://www.analyticsvidhya.com/blog/2015/08/cheat-sheet-data-visualization-r/
"Big Data / Analytics based startups at Y Combinator, Summer 2015 batch",Learn everything about Analytics|Analytics / Big Data startups in Y Combinator Summer 2015 batch:|End Note:,"SecondMeasure|Verge Genomics|Greenshoe|Ixchel Scientific|SourceDNA|GovPredict|Convox|Vernox|ShapeScale|Flirtey|Ross Intelligence|GrowSumo|Luna Sleep|TetraScience|Hickory|PlateJoy|PickTrace|Paribus|Leada|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|R-analyst Cheat sheet: Data Visualization in R|Finding Optimal Weights of Ensemble Learner using Neural Network|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If there is one startup accelerator, the tech world keeps a watch on  it is Y Combinator! The accelerator has produced the likes of Reddit, Dropbox & Airbnb in their 10 years of existence. Here is what some of the best brains in the industry say about it:Several of our best investments have come from Y Combinator. Y Combinator is the best program for creating top-end entrepreneurs that has ever existed. Marc Andreessen, General Partner, Andreessen HorowitzOn August 21, 2015 the twentiethbatch of Y Combinator went through their demo day! Here is a brief introduction to some of the best big data & analytics based startups, which are in store.Also read: Big data/ Analytics startups of 2015 winter batchCredit card companies can mineton of information from transactions of its customers. Imagine what you can do by inspecting billions of credit card transactions?Second Measuredelivers insightful and effective analysis byinspecting billions of credit card transactions and presenting detailed insights to investors and analysts through their products. Second Measure can delivervarious insights like:Verge Genomics uses genomic data analysis to find better drugs for brain diseases. Cause ofNeurological diseases are complex interactions between many genes. Most of the drugs fail because researchers target only one gene at a timebut Verge Genomicsdiscovered a way to map out the hundreds of genes that cause a disease, and then find drugs that target all the genes at once. Some of the benefits of this method are:How do you provide credit in a market where credit history is not available? Greenshoe is solving this problem in Africa by using smartphone data.It is a lending platform for emerging market (starting in Africa)that helps people get approved for short-term loans based on their activity on their phones. To apply for a loan, you have to follow the following process:Remember the joke You can never find an average man? Ixchel Scientific is applying this simple logic to make cancer treatment far more effective.It is a Biotech startup, which works on improving thesuccess rates for trialed cancer drugs using an environment that better represents the human body. Currently 95% of cancer drugs fail human testing after investing billions of dollars and decades inventing in development. Ixchel Scientific provides organ specific platform that predicts how drug will behave in the human body. They offers various services likecorrelation analysis with clinical responses, Drug candidate screening, fully customizable, Rescue failed drug candidates and others.SourceDNA has built a stockpile of the most detailed data on how developers use tools, SDKs and plugins to create the top-ranked iOS and Android apps, paid and free, in 20 countries. Their customers harness the intelligence to spot new competitors and identify new developer trends that surprise their product and sales teams. They continuously update the data and keep it astonishingly deep, keeping their customers uncannily ahead of the mobile market.GovPredict quantifies, tracks, and predicts legislative activity. With big data and powerful algorithms, they predict the likeliest cosponsors for legislation and convince them before competitors could reach out to them. Discover unlikely voting and cosponsorship alliances for specific kinds of bills, by keyword, originating committee, or sponsor party!Launch a Private Cloud in Minutes and it come up with the simplicity of Herokuand the power of AWS. It uses ECS, ELB, Kinesis, and many other great AWS services under the hood but automates it all away to give you a deployment experience even easier than Heroku. It has various features like Instant Deployment, Standardize Development and it runs in your AWS account.Urban construction anddevelopment is fraught with uncertainties and one of the far off places for analytics. However, Vernox is applying artificial intelligence techniques to the pile of e-mails, Word documents and Excel files that are exchanged between contractors and designers. From that data, Vernox labs produces automated design reviews that project managers can run through with a new proposal. They are also creating a Google like search engine which can be used to query for specific materials or components.So, you have been working out for 3 months now but you are not seeing the effect you would have liked to? Shape scale not only monitors basic things like changes in weight but also changes at multiple body parts over time. Shoulder growth of 10% over a month, fat gain of 5%, belly diameter growth of 1% are some of the results delivered by ShapeScale. It is your own body monitor. Using predictive algorithms on previous work out data, shape scale helps you get the body of your dreams.Unmanned aerial vehicles are a reality nowadays and here comes in Flirtey with its delivery by drones system. You place an order and a drone drops it at your home. Scary? Well, its convenient, no delays due to traffic or any other human intervention. Flirtey has partnered with the University of Nevada to deliver on its promise and is currently operating in New Zealand. With time it is going be an industry and not a thing of the future anymore.Imagine having the law at your fingertips. Imagine having a Google like service where you can query: When can a debtor reject a collective bargaining agreement? and pop comes some relevant links that give you the answers you were looking for.Built on top of IBMs Watson, Ross sifts through all the law literature and returns only relevant data.It also monitors the law to inform you about relevant changes to the law which might affect your case. This is the perfect synthesis of law and technology for the greater good.With GrowSumo, a company can manage all its partners from a single page and add new partners, train them, reward them and also track their progress. Re-sellers can find products that they know make their clients happy. You can also work with companies your client already loves and discover new services from that which you think your client might like. The more recommendations you make the more money you can make. For example an educational institute listed on GrowSumo can be an attractive options to an online education provider for any particular course. They can apply to the educational institution and get trained. If it works out well each can recommend the other to businesses they know would benefit. This is thus an online community of companies.A smart mattress cover that understands how you sleep and monitors your heart rate etc. while you are sleeping. Sleep is one of the most important functions that the body has to go through and we all know how groggy the next morning can be in absence of a good nights sleep. Just plug in Luna to a power source and let it handle this problem for you. It learns from your sleeping patterns and adjusts the bed temperature to your liking. It wakes you up in the moment of lightest sleep so that you feel refreshed when you wake up. Plug it in, connect to wi-fi and turn on the luna app. Sleep  Well.It is a IoT venture from the Hravard Innovation Lab that creates products (software and hardware) which help engineers and scientists to monitor their machines, experiments on the cloud. An engineer can fit sensors to pressure gauges and monitor them on the cloud. A scientist can monitor his lab or experiments without being present there through a camera built by Tetra. So,buy a tetra science product, fit it to your existing machines and log into your tetra science account to monitor, manage and analyze data using your computer or cell phone, IoT is here.Hickory provides continuous learning through personalized learning material to increase employee knowledge and application. You learn and the Hickory algorithm adjusts the material based on your mastery over the subject. So learn a material, get a review done by Hickory and the next material is based on how much you have learned. A personalized teacher at your service.It creates your personalized Digital Pantry to keep track of ingredients in your kitchen, so you always receive just what you need by asking innovative Personalized Quiz to understand your preferences, time constraints, and goals. It uses Waste Reduction Algorithm to utilize ingredients across recipes to minimize food waste while optimizing for ingredient freshness and variety.PickTrace is developed as a response to farms needs for a harvesting management. Today, it has grown into a suite of tools to perform entire operations like Real Time Analytics, Labor Management, Harvest Management and Legal Compilancce. It has a reliable, hassle-free solution, which helps farmers to improve productivity.It helps buyers to get money back when prices fall, missed better deal and missed a coupon. Paribus works quietly in the background to monitor for opportunities to get money back, you need to just connect your mailbox to let the Paribus Receipt Fetcher identify purchases. The Paribus team uses advance data structures, algorithms and data security to get your money back.
While Team Leada is not using machine learning or Big Data to solve a problem, they are teaching Big Data and Data Analysis in a unique blend of case studies, class room and support for coding. Professors can use them to supplement their lectures and Enterprises can use them to train the employees. While you visit their site, the team has also created four awesome books on data science after talking to thought leaders in the industry  you should check them out as well.So, there you see, Big Data and Data Science are touching every part of your life, be it learning, sleeping, harvesting to shopping! An exciting world and time to live in.All of these ideas and startups are exciting and on their track to change traditional industries fundamentally or create new ones and all of this is based on analytics and big data!What do you think about these startups? Do you feel as pumped up as I did when I read about them first? Do share your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/08/big-data-analytics-based-startups-combinator-summer-2015-batch/
Finding Optimal Weights of Ensemble Learner using Neural Network,Learn everything about Analytics|Introduction|Lets consider a simple problem|What could be a traditional approach to this problem?|Howtofind optimalweights using Neural Network?|End Notes,"|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Big Data / Analytics based startups at Y Combinator, Summer 2015 batch|List of Machine Learning Certifications and Best Data Science Bootcamps|
Tavish Srivastava
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Encountering ensemblelearning algorithm in winningsolutions of data science competitions has become a norm now. The ability to train multiple learners on a set of hypothesis, not only adds robustness to the model, but also enables it to deliver highly accurate predictions. In case you missed, I would recommend readingBasics of Ensemble Learning Explained in Simple Englishbefore you go forward.While building ensemble models,one of most common challenge that people face is to find optimal weights.Few of them fight hard to solve this challenge and the not sobrave ones, convince themselves to apply simple bagging, which assumes equal weight for all models and takes the average of all predicted values.It often works well because it removes variance error from individual models. However, as we know, assigningequal weights is not the best approach to obtain thebest combination of models. What could be an alternate way?In this article, Ive solved the problemof finding out the optimal weights in ensemble learners using neural network in R Programming.Imagine you have built 3 models on a given (hypothetical) data set. Each of these models predicts probability of output for an event.In the following figure : Model 1, Model 2 and Model 3 are the three predictive models. Each of them have their own uniqueness and can work as a team to perform even better than individually best model.We initially assume a 33.33% weight for each of the model and build an Ensemble model. Here, the challenge is to optimize these weights w1, w2 and w3 in such a fashion as to build a highly powerful ensemble model.Assume p1 , p2 and p3 are three outputs from the three models respectively. We need to optimize w1, w2 and w3 to optimize an objective function. Lets try to write down the constraints and objective function mathematically :Constraint :Objective function (Maximize Likelihood function) :This is a classic case of simplex optimization. However, with a huge number of models to bag, and diving into mathematical formulas every time can be stressful and time consuming at times. Hence,we need a smarter method here.Well now learn to findthese weights without getting into such mathematical formulation using neural network implementation.I understand,neural network implementation can be quite overwhelming at times. Hence, to solve thecurrent case in hand, wellnot deep dive into the complex concepts ofdeep neural network.Basically, neural network operates on finding weights for interlink between ( input variable tohidden node) and ( hidden node to output node). Our objective here is tofind the right combination of weights from input nodes directly to the output node.Here is an easy and diligentwayto accomplish this task. We restrict the number of hidden nodes to one. This will automatically adjust the weight from hidden node to output node as 1. This implies thatthe hidden node and the output node is no longer different.Here is a simplistic schema to represent what we just discussed :So we found a way! This will automatically accomplishwhat we were trying to do using simplex equations.Lets put this formulation down to a R codeI have implementedthis logic on multiple Kaggle problems and found that the results are quite encouraging. Here is one such example from a recent competition:In this article, Ive explained the method to find optimal weight in an ensemble model using a traditional approach and neural network implementation (recommended).",https://www.analyticsvidhya.com/blog/2015/08/optimal-weights-ensemble-learner-neural-network/
List of Machine Learning Certifications and Best Data Science Bootcamps,Learn everything about Analytics|Introduction|How can this article benefit you?|Global Machine Learning Certifications|11 Best Data Science Boot Camps & Fellowship|Free Resources for Machine Learning|End Notes,"Certificate in Machine Learning||Artificial Intelligence Graduate Certificate|Certification of Professional Achievement in Data Science|CSCI E-81 Machine Learning and Data Mining|Machine Learning at Udacity|Other Machine Learning Courses|BitBootcamp|Data Science Retreat|Data Science for Social Good|General Assembly|Insight Data Science|Insight Data Engineering|MetisData Science|NYC Data Science Bootcamp|Startup ML Fellowship|The Data Incubator|Zipfian Academy|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Finding Optimal Weights of Ensemble Learner using Neural Network|Data Analyst  Equifax  Bangalore (4-5+ years of experience)|
Analytics Vidhya Content Team
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every one has a different style of learning. Hence, there are multiple waysto become a data scientist. You can learn from tutorials, blogs, books, hackathons, videos and what not! I personally like self paced learning aided by help from a community  it works best for me. What works best for you?If the answer to above question was class room / instructor led certifications, you should check out machine learning certifications and data science bootcamps. They offer a great way to learn and prepare you for the role and expectations from a data scientist.More: 11 things you should know as a Data ScientistIn this article, Ive listed down the essentialresources to master the basic and advanced version of data science using:Global Machine Learning Certifications  This list highlights the widely recognized & renownedcertifications in machine learning which can add significant weight to your candidature, thereby increasing your chances to grab a data scientist job.Data Science Bootcamps  You can think of bootcamps as online / offline classroom trainingwhich are heldperiodically.The motive of these bootcamps is to empower aspiring data scientists with necessary skills & knowledge highly soughtby potential employers, in a short duration of time. These are like concentrated shots of learning consumed along with a bunch of fellow (aspiring) data scientists.Free Resources for Machine Learning This list highlights the free course material available on machine learning & related concepts. Interesting part is, I have included some resources from the top universities of the world which are not so commonly mentioned, but can turn out to be great if you follow them seriously.Please note that this is simply a list of best certifications / bootcamps / resources. You should look at them as the best options available and choose what fits you the best. They are not ranked.Lets get started!This course is provided by University of Washington. It is available in dual (online / offline) format. This course provides hands-on experience of machine learning using open source tools such as R-Studio, scikit-learn, Weka etc. By the end of this course, youre expected to gain the necessary knowledge required to fulfill business needs from a data scientist.This course is provided by Stanford Center for Professional Development. It is a graduation certification course which is to be completed in maximum of 3 years. This course is highly suitedfor candidates having a prior programming experience in C / C++. This course coversthe essential modules of AI including logic, knowledge representation, probabilistic models & machine learning.This certification course is provided by Data Science Institute (Columbia University).This certification offers multiple courses such as algorithms for data science, probability and statistics, machine learning for data science, exploratory data analysis. This course is best suited for candidates having prior knowledge in programming, statistics, linear algebra, probability & calculus.This certification course is provided by Harvard Extension School. The methodology used in this course is via live web conference using blackboard collaboration. Generally, these classes are arranged on Fridays. This course will begin from 4th September 2015. This is a 15 week long course which covers every essential aspect of machine learning algorithms and precisely explains the logic underlying them.Udacity offers a comprehensive certification course on machine learning wherein the concepts are aptly explained using interactive practice videos. They have a unique style of explaining things, which might just work for you.The course duration is 4 months. This course closely covers the aspect of supervised, unsupervised & reinforcement learning using real life examples and problems.You might also be interested to check out the best machine learning PhD, Graduation programs in the world (mostly in US) right now:The principal motive of these boot camps is ensuring the structured acquisition of data science concepts & knowledge, thereby empowering the participants with necessary skills requiredby the recruiters. This concept of teaching has rapidlyevolved inmany countries. The primary reason being, the inability of people to stay focused on self-paced courses and follow every stepas instructed. People now look for external support (teacher, mentor, instructor) to monitor their growth and development.Here I have highlighted the best of all boot camps being organized in the world.Ive chosen these bootcamps on the basis of enrollment status, placement support, mentors / instructors, curriculum.P.S. The list is in alphabetical orderThis program provides dual ways of enrolling participants i.e. Data Science Cohort & Big Data and Hadoop cohort. The program aims to address the shortage of big data & data science talent in the industry. It provides job placement assistance within a salary range of $75  $150k. The curriculum of both the courses is designed to focus on the essential aspects of data science & big data with a special focus on statistics and mathematics.Location: New YorkDuration: 4 weeks / 6 weeksPre-requisites: Background in SQL, Mathematics, Programming skillsThis program offers dual career track such that the candidates enrolling this program have the option of choosing to become a data scientist or a data engineer. This program relishes an amazing support of industry stalwarts. The class size happens to be relatively small which allows the instructor to pay attention to every candidate.Location: Berlin, GermanyDuration: 3 monthsPre-requisites: Experience in Programming, DatabasesThis program claims to train data scientists to tackle problems that really matter. This program is provided by University of Chicago. It teaches aspiring data science candidates to learndata mining, machine learning, big data and data science projectsand work with non-profits, federal agencies and local governments and make a social impact.Location: ChicagoDuration: 12 weeksPre-requisites: Graduates & Under GraduatesThis program teaches you core skills which includes using math & programming skills to make sense out of large data, analyzing and manipulating data using python, fundamental modeling techniques to mention a few. The ultimate aim of this course is to empower students with appropriate knowledge required to make informed decision making at the workplace.Location: San Francisco / New YorkDuration: 11 weeksPre-requisites: Good hold on Probability, Statistics, Python, RThis fellowship program intends to bridge the gap between academia and data science being practiced in the industry. This program receives the wide support of industry mentor and follow a pedagogy of project based learning. This course is FREE (you need to take placements through them  what else could you ask for!).Location:Silicon Valley/ New York, NYDuration: 7 weeksPre-requisites:PhD Degree / PostDocThe demand of data engineers has increased by 400% in the past 3 years. This fellowship program is designed to match the desired industry skills with skills acquired by candidates in academia. This course is FREE to enroll.Location:Silicon Valley (CA)Duration: 6 weeksPre-requisites: Knowledge in mathematics, science and software engineeringThe key features of this program includes in-person instructions from expert data scientists, career coaching & employment support. By the end of this project, candidates are expected to comfortably design, implement and communicate the results of data science projects creatively.Location: New York, NYDuration: 12 weeksPre-requisites: Prior knowledge of statistics and programmingThis bootcamp provides the much needed acceleration to reach the next level in your data science career path. It teaches real world, practical skills to become a data scientist / data engineer. In addition, the participants also get job search support. This program claims to have a 360 degree view of data science industry needs, and accordingly design the curriculum so that participants can be the best fit for industry needs.Location: Manhattan, NYDuration: 12 weeksPre-requisites:Experience in Programming, Quantitative disciplineThis fellowship is highly applicable for people keen to start their career with startups. This program presumes that data science is more of a skill than just acquiring knowledge which needs to honed by continuous practice. Hence, the candidates attending this program will learn to build real machine learning applications and established data science teams.Location: San Francisco, CADuration: 4 monthsPre-requisites: Software Engineering, Quantitative Analysis, Advanced quantitative degreesThe fellowship program enables you to jumpstart your career in data science. This program is widely supported byindustry leaderssuch as foursquare, the new york times, capital one, microsoft, ebay etc. This program is focused on providing training that links your analytical skills to job opportunities.Location: New York, NYDuration: 7 weeksPre-requisites:PhD / PostDocIn this bootcamp, youll undergo a structuredcurriculum which covers the essential aspects of data science. Participants are given real industryproblems for practicing data science techniques.The statistics at Zipfian website claims to have 93% placement, $115 average salary in less than 6 months. They also run a 6 week data fellowship.Location: San Franciso, CaliforniaDuration: 12 weeksPre-requisites: Quantitative background, familiarity with programming and statisticsHere youll also find resources from the top universities teaching machine learning including cornell, MIT, harvard, carnegie universities. These are self-paced tutorials which includes slides, videos, blogs and what not! These resources are in no order.1.Machine Learning course by Yaser Abu Mostafa This is one of the highly recommended course on Machine Learning. Usually, this course is provided on edX, but it has been closed now. It is expected to run again in 2017. You can still check out the course content and learn from them.2.Machine Learning (Andrew Ng) on CourseraThis course requires no further introduction. If you are in data science, chances are you already know of this course. One of the best course on machine learning for beginners by Andrew Ng. It starts by covering linear regression and progresses towards higher level algorithms.This course is available for FREE!3.Probabilistic Graphical Models This course is provided by Stanford University on Coursera. The course instructoris Daphne Koller (co-founder of Coursera). This course teaches you the basics of PGM representation, methods of construction using machine learning techniques.4.Neural Networks for Machine Learning This course is provided by University of Toronto on Coursera. The course instructor is Geoffrey Hinton. This course will make you familiar with the applications of machine learning such as artificial intelligence, image recognition, speech recognition, human motion and how they are being used. In this course, Geoff has beautifully explained the basic algorithms & practical tricks to get machine learning working.5. Scalable Machine Learning This course is provided by University of California on edX. This course allows you to learn underlying statistical and algorithmic principles required to develop machine learning pipelines, implementation of scalable algorithms for fundamental statistical models, hands-on experience on Apache Spark.6. Machine Learning Tutorials  Carnegie Mellon University CarnegieMellon University is widely known for its machine learning department. This resource provides tutorial videos & slides from the class of 2011. It consist of Andrew Moores tutorials as well. This tutorial focuses on explaining the concepts of supervised, unsupervised and reinforcement learning by building models.7. Machine Learning Quick Tutorials  Cornell University Heres the course material of Fall 2014 in Cornell University. This tutorial attempts to teach machine learning from the scratch using some interesting presentations. This course covers almost all the modules of machine learning. If you think you cant watch videos to learn these concepts, checking out these presentations should do good for you!8. MIT Open Course on Machine Learning This course is provided by Massachusetts Institute of Technology. If I am not wrong, this course has been archived but you can still access the course material. This tutorial aims to cover the underlying machine learning algorithms, starting from Regression, Classification till higher level concepts such as bayesian networks, collaborative filtering etc. It is available for download in PDF version.9.Machine Learning Algorithms Tutorial by Andrew Moore Andrew Moore is the Dean of the School of Computer Science at Carnegie Mellon University. Here are the set of tutorials which covers many aspects of statistical data mining, classical machine learning, foundation of probability to mention a few. These tutorials are available to download in PDF version. Id highly recommended beginners to follow this tutorial.
10.CSCI E-181 Machine Learning: This course is provided by Harvard Extension School. It consists of video lectures which are focused on machine learning algorithm. Since, not everyone is fortunate enough to get into Harvard, you surely shouldnt miss the erudite discussions and knowledge beingdisseminated by Harvard professors in these tutorials. I really admired the pedagogy used by professors in these tutorials.11.CSCI E-109 Data Science:This course is also provided by Harvard Extension School. I believe these are one of the best video tutorial available on learning data science in Python. The course instructor has beautifully explained suchstrenuous concepts using interesting examples and viewpoints. Id recommend beginners to take this course as it covers every underlying aspect of data science and machine learning.",https://www.analyticsvidhya.com/blog/2015/08/data-science-bootcamps-machine-learning-certifications/
"Data Analyst  Equifax  Bangalore (4-5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|List of Machine Learning Certifications and Best Data Science Bootcamps|Best way to learn kNN Algorithm using R Programming|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data AnalystLocation  BangaloreAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Equifax  Bangalore,https://www.analyticsvidhya.com/blog/2015/08/data-analyst-equifax-bangalore-4-5-years-experience/
Best way to learn kNN Algorithm using R Programming,Learn everything about Analytics|Introduction|What is kNN Algorithm?|How to select appropriate k value?|Example of kNN Algorithm|kNN Algorithm  Pros and Cons|Case Study: Detecting Prostate Cancer|End Notes,"Calculating Distance|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Analyst  Equifax  Bangalore (4-5+ years of experience)|Data scientist hack to find the right Meetup groups (using Python)|
Guest Blog
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Step 1- Data collection|Step 2- Preparing and exploring the data|Step 3  Training a model on data|Step 4  Evaluate the model performance|Step 5  Improve the performance of the model,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In this article, Ill showyou the application of kNN (k  nearest neighbor) algorithm using R Programming. But, before we go ahead on that journey, you should read the following articles:Well also discuss a case study which describes the step by step process of implementing kNNin building models.This algorithm is a supervised learning algorithm, where the destination is known, but the path to the destinationis not.Understanding nearest neighbors forms the quintessence of machine learning.Just like Regression, this algorithm is also easy to learn and apply.Lets assume we have several groups of labeled samples. The items present in the groups are homogeneous in nature. Now, suppose we have an unlabeled example which needs to be classifiedinto one of the several labeled groups. How do you do that? Unhesitatingly, using kNN Algorithm.k nearest neighbors is a simple algorithm that stores all available cases and classifies new casesby a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups.Choosing the number of nearest neighbors i.e. determining the value of k plays a significant role indetermining the efficacy of the model. Thus, selection of k will determine how well the data can be utilized to generalize the results of the kNN algorithm. A large k value has benefits which include reducing the variance due to the noisy data; the side effect being developing a bias due to which the learner tends to ignore the smaller patterns which may have useful insights.The following example will give you practical insighton selecting the appropriate k value.Lets consider 10 drinking items which are rated on two parameters on a scale of 1 to 10. The two parameters are sweetness and fizziness. This is more of a perception based rating and so may vary between individuals. I would be considering my ratings (which might differ) to take this illustration ahead. The ratings of few items look somewhat as:Sweetness determines the perception of the sugar content in the items. Fizziness ascertains the presence of bubbles in the drink due to the carbon dioxide content in the drink. Again, all these ratings used are based on personal perception and are strictly relative.From the above figure, it is clear we have bucketed the 10 items into 4 groups namely, COLD DRINKS, ENERGY DRINKS, HEALTH DRINKS and HARD DRINKS. The question here is, to which group would Maaza fall into? This will be determined by calculating distance.Now, calculating distance between Maaza and its nearest neighbors (ACTIV, Vodka, Pepsi and Monster) requires the usage of a distance formula, the most popular being Euclidean distance formula i.e. the shortest distance between the 2 points which may be obtained using a ruler.                                        Source: GamesetmapUsing the co-ordinates of Maaza (8,2) and Vodka (2,1), the distance between Maaza and Vodka canbe calculated as:dist(Maaza,Vodka) = 6.08Using Euclidean distance, we can calculate the distance of Maaza from each of its nearest neighbors. Distance between Maaza and ACTIV being the least, it may be inferred that Maaza is same as ACTIV in nature which in turn belongs to the group of drinks (Health Drinks).If k=1, the algorithm considers the nearest neighbor to Maaza i.e, ACTIV; if k=3, the algorithm considers 3 nearest neighbors to Maaza to compare the distances (ACTIV, Vodka, Monster)  ACTIV stands the nearest to Maaza.Pros: The algorithm is highly unbiased in nature and makes no prior assumption of the underlying data. Being simple and effective in nature, it is easy to implement and has gained good popularity.Cons: Indeed it is simple but kNN algorithm has drawn a lot of flake for being extremely simple! If we take a deeper look, this doesnt create a model since theres no abstraction process involved. Yes, the training process is really fast as the data is stored verbatim (hence lazy learner) but the prediction time is pretty high with useful insights missing at times. Therefore, building this algorithm requires time to be invested in data preparation (especially treating the missing data and categorical features) to obtain a robust model.Machine learning finds extensive usage in pharmaceutical industry especially in detection of oncogenic (cancer cells) growth. R finds application in machine learning to build models to predict the abnormal growth of cells thereby helping in detection of cancer and benefiting the health system.Lets see the process of building this model using kNN algorithm in R Programming. Below youll observeIve explained every line of code written to accomplish this task.We will use a data set of 100 patients (created solely for the purpose of practice) to implement the knn algorithm and thereby interpreting results .The data set has been prepared keeping in mind the results which are generally obtained from DRE (Digital Rectal Exam). You can download the data set and practice these steps as I explain.The data set consists of 100 observations and 10 variables (out of which 8 numeric variables and one categorical variableand is ID) which are as follows:In real life, there are dozens of important parameters needed to measure the probability of cancerous growth but for simplicity purposes lets deal with 8 of them!Heres how the data set looks like:Lets make sure that we understand every line of code before proceeding to the next stage:We find that the data is structured with 10 variables and 100 observations. If we observe the data set, the first variable id is unique in nature and can be removed as it does not provide useful information.The data set contains patients who have been diagnosed with either Malignant (M) or Benign (B) cancer(The variable diagnosis_result is our target variable i.e. this variable will determine the results of the diagnosis based on the 8 numeric variables)In case we wish to rename B asBenign and M as Malignant and see the results in the percentage form, we may write as:Normalizing numeric dataThis feature is of paramount importance since the scale used for the values for each variable might be different. The best practice is to normalize the data and transform all the values to a common scale.Once we run this code, we are required tonormalize the numeric features in the data set. Instead of normalizing each of the 8 individual variables we use:The first variable in our data set (after removal of id) is diagnosis_result which is not numeric in nature. So, we start from 2nd variable. The function lapply() applies normalize() to each feature in the data frame. The final result is stored to prc_n data frame using as.data.frame() functionLets check using the variable radius whether the data has been normalized.The results show that the data has been normalized. Do try with the other variables such as perimeter, area etc.Creating training and test data setThe kNN algorithm is applied to thetraining data set and the results areverified on the test data set.For this, we would divide the data set into 2 portions in the ratio of 65: 35 (assumed) for the training and test data set respectively. You may use a different ratio altogether depending on the business requirement!We shall divide the prc_n data frame into prc_train and prc_test data framesA blank value in each of the above statements indicate that all rows and columns should be included.Our target variable is diagnosis_result which we have not included in our training and test data sets.The knn () function needs to be used to train a model for which we need to install a package class. The knn() function identifies the k-nearest neighbors using Euclidean distance where k is a user-specified number.You need to type in the following commands to use knn()Now we are ready to use the knn() function to classify test dataThe value for k is generally chosen as the square root of the number of observations.knn() returns a factor value of predicted labels for each of the examples in the test data set which is then assigned to the data frame prc_test_predWe have built the model but we also need to check the accuracy of the predicted values in prc_test_pred as to whether they match up with the known values in prc_test_labels. To ensure this, we need to use the CrossTable() function available in the package gmodels.We caninstall it using:The test data consisted of 35 observations. Out of which 5 cases have been accurately predicted (TN->True Negatives) as Benign (B) in nature which constitutes 14.3%. Also, 16 out of 35 observations were accurately predicted (TP-> True Positives) as Malignant (M) in nature which constitutes 45.7%. Thus a total of 16 out of 35 predictions where TP i.e, True Positive in nature.There were no cases of False Negatives (FN) meaning no cases were recorded which actually are malignant in nature but got predicted as benign. The FNs if any poses a potential threat for the same reason and the main focus to increase the accuracy of the model is to reduce FNs.There were 14 cases of False Positives (FP) meaning 14 cases were actually benign in nature but got predicted as malignant.The total accuracy of the model is 60 %( (TN+TP)/35) which shows that there may be chances to improve the model performanceThis can be taken into account by repeating the steps 3 and 4 and by changing the k-value. Generally, it is the square root of the observations and in this case we took k=10 which is a perfect square root of 100.The k-value may be fluctuated in and around the value of 10 to check the increased accuracy of the model. Do try it out with values of your choice to increase the accuracy! Also remember, to keep the value of FNs as low as possible.For practice purpose, you can also solicit a dummy data setand execute the above mentioned codes to get a taste of the kNN algorithm. The results may not be that precisetaking into account the nature of data but one thing for sure: the ability to understand the CrossTable() function and to interpret the results is what we should achieve.In this article, Ive explained kNN algorithm using an interesting example and a case study where I have demonstrated the process to apply kNN algorithm in building models.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.This article was originally written by Payel Roy Choudhury, before Kunal did his experiment to set the tone. Payel has completed her MBA with specialization in Analytics from Narsee Monjee Institute of Management Studies (NMIMS) and is currently working with ZS Associates. She is looking forward to contribute regularly to Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2015/08/learning-concept-knn-algorithms-programming/
Data scientist hack to find the right Meetup groups (using Python),Learn everything about Analytics|Introduction|What are meetups?|The Challenge with manual approach|The Data Scientist Solution|Final Code|End Notes,"Step-0: Import Libraries|Step-1: Use API to readdata in JSON format|Step-2:Generate list of signed URLs for all given cities|Step-3: Read data from URL and access relevant features in a DataFrame|Step-4:Compare Meetup groups across various cities|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Best way to learn kNN Algorithm using R Programming|Ultimate app to find the best Data Science resources|
Sunil Ray
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","Register for Data Hackathon 3.X  Win Amazon Voucher worth Rs.10,000(~$200)|Number of Python groups across six countries|Average size of groups across countries
|Average ratingof groups across countries
|Top 2 groups for each country
",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Scientists are a breed of lazy animals! We detest the practice of doing any repeatable work manually. We cringe at mere thought of doing tedious manual tasks and when we come across one, we try and automate it so that the world becomes a better place!We have been running a few meetups across India for last few months and wanted to see what some of best meetups across the globe are doing. For a normal human, this would mean surfing through pages of meetups and finding out this information manually.Not for a data scientist!Meetup can be best understood as self organized gathering of people done to achieve a pre-defined objective. Meetup.comis the worlds largest network of local groups. Meetups mission is to revitalize local community and help people around the world to self organize.The process of searching meetups can be quite time consuming(Id rather say). There are multiple limitation attached to it (which Ive explained in the following section). But, how would a data scientist performthis task to save time? Of course, hed endeavor to automate this process!In this article, Ill introduce you to a data scientists approach of locating meetup groups using Python. Taking this as a reference, you can find groups located at any corner of the earth. You can also add your own layer of analysis to find out some interesting insights.Let us say, you want to find out and join some of the best meetups in your area. You can obviously do this task manually, but there are a few challenges you bump into:Assume that you are in a locality with more then 200 groups in area of your interest. How would you find the best ones?In this article, I have identified various Python meetup groups from cities of India, USA, UK, HK, TW and Australia. Following are the steps I will perform:These steps are quite easy to perform.Below, I am listing the steps to perform them. As mentioned before, this is just the start of possibilities which open up. You can use this information to pull out a lot of insights about various communities across the globe.Below are the list of libraries, I have used to code this project.Heres a quick overview of these libraries:You can fetch data from any website in various ways:For the websites which provide an API, it is usually the best way to fetch the information. The first method mentioned above is susceptible to layout changes on a page and can get very messy at times. Thankfully,Meetup.com provides various APIsto access required data. Using this API, we canaccess information about various groups.To access API based automated solution, we would require value for sig_id and sig (different for different users). Follow below steps to access these.Now, we should solicitsigned URLfor each search (in our case, city+topic) and output of these signed URLs will provide the detail information about matched groups:Now, we have list of URLs for all cities. Next, we will use urllib library to read data into JSON format. Then, well read the data to a list beforeconverting it toaDataFrame.Time toanalyse the data now and finding the appropriategroups based on various metrics like number of members, ratings,city and others. Below are the some basic findings, whichI have generated for python groups across different cities of India, USA, UK, HK, TW and Australia.To know more about these python codes, you can read articles on data exploration and visualization using python
Once again, US emerges as the leader in average number of members in each group whereas CN has lowest average.AU and US has similar average rating (~4) across groups.
Time to identify top two groups from each country based on number of members. You can also identify the groupsbased on rating.Here I have done basic analysis to illustrate this approach. You can access other APIs also to find information like upcoming events, number of events, duration of events and others and after that merge all the relevant information based on group_id (or key value).Below is final code of this exercise, you can play with it by putting your sig_id and sig key and search various results of different topic across different cities. I have also uploaded it on GitHub.In this article, we looked at the application of python to automate a manual process and the level of precisionto find the right Meetup groups. We used API to access information from web and transferred itto a DataFrame. Later, weanalysed this information to generate actionable insights.We can make this application more smart by adding additional information like upcoming events, number of events, RSVP and various other metrics. You can also use this data to carve out interesting insights about community and people. For example, does the RSVP to attendance rate to review rate funnel differ from country to country? Which countries plan their meetups the most in advance?Give it a try at your end and share your knowledge in the comment section below.",https://www.analyticsvidhya.com/blog/2015/08/data-scientist-meetup-hack/
Ultimate app to find the best Data Science resources,Learn everything about Analytics,"Find the best data science resources  Ultimate Resource Finder|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data scientist hack to find the right Meetup groups (using Python)|11 things you should know as a Data Scientist|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you felt at loss in the jungle of data science resources? Did you try finding a resource only to conclude there are too many of them? Or you couldnt find one which just enables your learning without confusing you further?The problem in learning data science today is not lack of resources, but the abundance of it!It has been our constant effort to provide you with the best of resources, in as simple manner as possible. To this effect, we launched our learning paths  which got a roaring response from all of you. Today, we are pleased to launch ultimate resource finder  another way to structure all the resources available out there in the wild.This resource finder aims to help you with all the resources you need in your journey to learn data science.Instead of offering you a third world of data science resources, weve enlisted only the best resources and have been highly recommended by experts & users across the world. The list of resources is dynamic and will be constantly under development.Please note, we are still adding resources to a few sections and will also continue to add more and more sections in this resource finder. If you have any suggestions / feedback on this tool, we will love to hear them. Please share them through the comments below.",https://www.analyticsvidhya.com/blog/2015/08/find-best-data-science-resources/
11 things you should know as a Data Scientist,Learn everything about Analytics|Background|Who is this guide meant for?|Lets start with setting up the machine|Other resources:|What Next?,"1. Hardware  choice of your machine|2. Operating System (OS)|3. Software  general|4. Software  Analytics / Data Science|5. Software  Data Visualization|6. Databases / File storage|6. Cloudservices|7. Industry blogs and newsletters|8. Mobile apps|9. Meetups|10. Datasets for practice|11. Communities and Social Media|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Ultimate app to find the best Data Science resources|7 Regression Techniques you should know!|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","Register for Data Hackathon 3.X  Win Amazon Voucher worth Rs.10,000(~$200)",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"During the meetups we conduct, we get a mix of audience. Fromcomplete starters in data science to experts in the field, every one attacks the problem under a single roof. However, one thing stands out when we interact with these people  a large proportion of these people (including some experts) didnt have their machines set up and tuned for data science. A lot of them never took time out to set themselves up for data science journey. As a result of which they came across some of the industry resources as a matter of chance.No one told them which blogs to follow, which newsletters to subscribe, where to read industry news. They also never tuned their machines or did not have the necessary hardware or software. This then leads to a lower productivity and even frustration in some cases, when they should be actually loving the experience.Still dont relate to it? Think of visiting a website, which take more than 10 seconds to load. You will likely get bored in this time, open up a new tab for another site or would just steer away from what was to be done. Same thing happens with data science. The longer your code runs, the chances of you steering away from work increases!This is how we came across this unsaid problem people face in industry and hence we thought to create a guide for people to get ready for data science.As mentioned above, this guide is meant for any one in data science industry, who has not tuned their machine to performance. I think it would be of more use to the beginners than to experts, but I have seen experts benefit from these tips equally well.The first thing to ensure is that you are on the right hardware for data science. There is not much any one can do, if your hardware does not have what you would need. Since laptops are the mainstream device for computing now a days, my recommendations below are for laptop. If you use a desktop / iMac, you can go with even better configuration.While this choice will ultimately boil down to how much you can shell out for a machine,I would recommend a machine with quad-core processor, preferably i7 (in case of Intel chips). Make sure you check that the processor you choose if quad core and not dual core. Lately, it has been really difficult to find good quad core chips. You can check benchmark performance of various chips in your budget against each other using sites like cpuboss.Next, it is always recommended to maximize your RAM to the extent possible. A lot of tools use RAM for computations and you dont want to run out of RAM while doing them (you eventually will in some cases!).If your budget allows, you should upgrade to SSD as your read / write operations with datasets will take a fraction of time compared to normal SATA hard disk.For those, who are really serious about learning machine learning and deep learning, it is recommended to have a NVIDIA GPU, so that you can run intense computations using CUDA.Here are a few good recommendations available currently:A few additional notes:People might argue that you dont need to invest in such an advanced machine. You might be better off working with a mediocre machine over the cloud. I personally like accessibility provided by a personal machine and the fact that I can start working at any place without hooking on to the internet.Once you have selected your machine, the next most important choice would be your OS.Once you have finalized the OS, make sure you tune your OS to high performance. For example, in Windows, you can disable the transition effects and animations in Windows (Run sysdm.cpl . Go to advanced Tab -> performance section -> Settings and then disable the visual effects), remove unnecessary startup programs and switch the power plan to Performance.Here is the list of a few softwares you will need apart from the analytics / data science tools (which are discussed in coming points).This section would vary depending on your choice of main tools you choose for data mining. If you are still to choose your main tool, check out this comparison  SAS vs. R vs. Python. If you already have a tool of choice, select the one which apply to you:Other options include MATLAB / Octave / RapidMiner.In addition to the softwares mentioned above, it makes sense to have a tool specifically for data visualization. They usually help a lot while data exploration and when you present the data story to your customers at the end of every project. Again there are a lot of options here. A comprehensive coverage of them would be an article in itself. If you just want one, I would recommend QlikView  it is easy to use, has a personal version which is free to download and can handle large data really well. Tableau is another popular choice, whichis very intuitive to use, but is not as effective for use on large datasets in my experience.If you know JavaScript, you can also use libraries like D3.jsAt times, when data set is huge or you are building an application for end users, you will need to use databases  SQL being the most common one. You can use MySQL or PostgreSQL. SQLite, which comes bundled in Python packages can be a effective option for small applications as well. If you work frequently on huge datasets, setting up a Hadoop cluster is inevitable. If you work on real time streams of data, you will need Spark as well.In addition to these databases, you should also keep a couple of NoSQL databases, in case you need them. I would recommend MongoDB and Neo4j for usage.By this time, your machine has almost all the resources you need for your data science journey. Now, let us look at a few other resources, you should use during your data science journey.What if you want to work on a dataset which is 400 GB in size? Even the machines I recommended above would fail to load this in their memory while usingR! It is scenarios like this, where a cloud account will come in handy. You can use either of the 2 services on cloud  Amazon Web Services (popularly known as AWS) or on Microsoft Azure. Both of them provide highly scalable solutions. The Azure platform in its new avatar is probably more user friendly, but Amazon is still the King of cloud services. You can sign up for accounts on both of them and give them a try.I am assuming you already have a subscription to Analytics Vidhya articles. If not, please go ahead and subscribe. Apart from Analytics Vidhya, you should follow KDNuggets and DataScienceCentral.On the newsletter front, I would recommendOReilly,DataScienceWeeklyand Data Elixir newsletters.I use my mobile to read a lot of content on the go. Whether I am travelling in metro or just have 5 minutes to sneak the latest publications, I rely a lot on my mobile for that. I use a combination of Prismatic and Flipboard to find new content. Combined, both of them provide me with all the latest gemstones published in the industry.In addition, I have Termux, a fully functional Linux terminal, just in case I need to ssh into a server while on the go. I use it occasionally to play around in a Python shell for quick prototyping as well.You can look out for meetups happening in your area. They provide opportunity to people to interact with like minded people. Analytics Vidhya conducts its hackathons in several cities in India. DataKind has several meetups as well.For a starter, you can look at this discussion on Analytics Vidhya. Apart from this,KDNuggets maintains a list of open datasets andUCI provides a lot of datasets for machine learning.You can also look at data.gov to find data from open sources.If you have not done so already, sign up for our discussion portals. You would not only interact with other data scientists from community, but can also participate in various hackathons we conduct. In addition to this, you should check out Kaggle competitions and DataTau for hacker news style industry news.In addition, you can also find data science community on Twitter, LinkedIn, GitHub, Facebook and Reddit. You can also subscribe to YouTube channelsI think you are all set now. You now have a machine with all the necessary software, tuned for performance. You would also be part of multiple communities and portalsto stay tuned with industry.If you have done all of this, you might be wondering what next? Stay tuned with us, we are coming up with a resource finder shortly, which will assume you have done all of this and will provide you necessary resourcesto master various concepts, tools and techniques in Data Science.In the meanwhile, if you think there should be some more steps or resources I have missed on, please feel free to add them here. I hope this article proves to be immensely helpful to all those people, who work with non-optimized machines and resources which leads to frustration and loss of productivity.",https://www.analyticsvidhya.com/blog/2015/08/ready-data-science-resources-common-questions-answered/
7 Regression Techniques you should know!,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Regression Analysis?|Why do we use Regression Analysis?|How many types of regression techniquesdo we have?|1. Linear Regression|2. Logistic Regression|3. PolynomialRegression|4. StepwiseRegression|5. RidgeRegression|6. LassoRegression|7.ElasticNet Regression|How to select the right regression model?|Projects|End Note,"Note  The discussions of this article are going on at AVs Discuss portal. Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|11 things you should know as a Data Scientist|News  Great Lakes launches Analytics Program in Bangalore, India|
Sunil Ray
|34 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",How toobtain best fit line (Value of a and b)?|Important Points:|Important Points:|Important Points:|Important Points:|Important Points:|Important Points:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Linear and Logistic regressions are usually the first algorithms people learn in data science. Due to their popularity, a lot of analysts even end up thinking that they are the only form of regressions. The ones who are slightly more involved think that they are the most important among all forms of regression analysis.The truth is that there are innumerable forms of regressions, which can be performed. Each form has its own importance and a specific condition where they are best suited to apply. In this article, I have explained the most commonly used 7 types of regression in data science in a simple manner.Through this article, I also hope that people develop an idea of the breadth of regressions, instead of just applying linear/logistic regression to every machine learning problem they come across and hoping that they would just fit!And if youre new to data science and looking for a place to start your journey, the data science course is as good a place as any to start! Covering the core topics of Python, Statistics and Predictive Modeling, it is the perfect way to take your first steps into data science.Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent(target)and independent variable (s) (predictor).This technique is used for forecasting, time series modelling and finding the causal effect relationship between the variables. For example, relationship between rash driving andnumber of road accidents by a driver is best studied through regression.Regression analysis is an important tool for modelling and analyzing data.Here, we fit a curve / line to the data points, in such a manner that the differences betweenthe distances of data points from the curve or line is minimized. Ill explain this in more details in coming sections.As mentioned above, regression analysis estimatesthe relationship between two or more variables. Lets understand this withan easy example:Lets say, you want to estimate growth in sales of a company based on current economic conditions. You have the recent company data whichindicates that the growth in sales is around two and a half times the growth in the economy. Using this insight, we can predict future sales of the company based on current & past information.There are multiple benefits of using regression analysis. They are as follows:Regression analysis also allows us to compare the effects of variables measured on different scales, such as the effect of price changes and the number of promotional activities.These benefits help market researchers / data analysts / data scientists to eliminate and evaluatethe best set of variables to be used forbuilding predictive models.There are various kinds of regression techniques available to make predictions. These techniques are mostly driven bythree metrics (number of independent variables, type of dependent variables and shape of regression line). Well discuss them indetail in the following sections.For the creative ones, you can even cook up new regressions, if you feel the need to use a combination of the parameters above, which people havent used before. But before you start that, let us understand the most commonly used regressions:It is one of the most widely known modeling technique. Linear regression is usually among the first few topics which people pick while learning predictive modeling.In this technique,the dependent variable is continuous, independent variable(s) can be continuous or discrete,and nature of regression line is linear.Linear Regressionestablishes arelationship between dependent variable (Y) and one or more independentvariables (X) using a best fit straight line (also known as regression line).It isrepresented by an equation Y=a+b*X + e, where a is intercept, b is slope of the lineande is error term. This equation can be used to predict the value of target variable based on given predictorvariable(s).The difference between simple linear regression and multiple linear regression is that, multiple linear regression has (>1) independent variables, whereas simple linear regression has only 1 independent variable. Now, the question is How do we obtainbest fit line?.This task can be easily accomplished by Least Square Method.It isthe most common method used for fitting a regression line. Itcalculates the best-fit line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line. Because the deviations are first squared, when added, thereisno cancelling out between positive and negative values.We can evaluatethe model performance using the metricR-square. To know more details about these metrics, you can read:Model Performance metrics Part 1, Part 2.Logistic regression is used to find the probability of event=Success and event=Failure. We shoulduse logistic regression when thedependent variable is binary (0/ 1, True/ False, Yes/ No) in nature. Here the value of Y ranges from 0 to 1 and it can represented by following equation.Above, p is the probability of presence of the characteristic of interest. A question that you should ask here iswhy have we used log in the equation?.Sincewe are working here with abinomial distribution (dependent variable), we need to choose a link function which is best suited for thisdistribution. And, it is logit function. In the equation above, the parameters are chosento maximize the likelihood of observing the sample values rather than minimizing the sum of squared errors (like in ordinary regression).A regression equation is a polynomial regression equation if the power of independent variable is more than 1. The equation below represents a polynomial equation:In this regression technique, the best fit line is not a straight line. Itis rather a curve that fits into the data points.This form of regression is usedwhen we deal with multiple independent variables. In this technique, the selection of independent variables isdone with the help of an automatic process, which involvesno human intervention.This featis achieved by observingstatisticalvalues like R-square, t-stats and AIC metric to discernsignificant variables.Stepwise regression basically fits the regression model by adding/dropping co-variates one at a time based on a specified criterion. Some of the most commonly used Stepwise regression methods are listed below:The aim of this modeling technique is to maximize the prediction power with minimum number of predictor variables.It is one of the method to handle higher dimensionality of data set.Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated). Inmulticollinearity, even though the least squares estimates (OLS) are unbiased, their variances are large which deviates the observed value farfrom the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.Above, we sawthe equation for linear regression.Remember? It can be represented as:y=a+ b*xThis equation also has an error term. The complete equation becomes:In a linear equation, prediction errors can be decomposed into two sub components. First isdue to the biased and second isdue to thevariance.Prediction error can occur due to any one of these two orboth components. Here, well discuss about the error caused due to variance.Ridge regression solves the multicollinearity problem throughshrinkage parameter  (lambda). Look at the equation below.In this equation, we have two components. First one is least square term and other one is lambda of the summation of2 (beta- square) where is the coefficient. This is added to least square term in order to shrink the parameter to have a very low variance.Similar to Ridge Regression, Lasso (Least Absolute Shrinkage and Selection Operator) also penalizes the absolute size of the regression coefficients. In addition, it is capable of reducing the variability and improving the accuracy of linear regression models. Look at the equation below:Lasso regression differs from ridge regression in a way thatit usesabsolute values in the penalty function, instead of squares. This leads topenalizing (or equivalently constraining the sum of the absolute values of the estimates) valueswhich causessome of the parameter estimates to turn out exactly zero. Larger the penalty applied, further the estimates getshrunk towards absolute zero. Thisresults to variable selection out of given n variables.ElasticNet is hybrid of Lasso and Ridge Regression techniques.Itis trained with L1 and L2 prior as regularizer.Elastic-net is useful when there are multiple features which are correlated. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridges stability under rotation.Beyond these 7most commonly used regression techniques, you can also look at other models like Bayesian, Ecological and Robust regression.Life is usually simple, when you know only one or two techniques. One of the training institutes I know of tells their students  if the outcome is continuous  apply linear regression. If it is binary  use logistic regression! However, higher the number of options available at our disposal, more difficult it becomes to choose the right one. A similar case happens with regression models.Withinmultiple typesof regression models, it is important to choose the best suited techniquebased on type of independent and dependent variables, dimensionality in the data and other essential characteristics of the data. Below are the key factors that you should practice to select therightregression model:Now, its time to take the plunge and actually play with some other real datasets. Try the techniques learnt in this post on the datasets provided in the following practice problems and let us know in the comment section how it worked out for you!By now, I hope you would have got an overview of regression. These regression techniques should be applied considering the conditions of data. One of the best trick to find out which technique to use, is by checking the family of variables i.e. discrete or continuous.In this article, I discussed about 7 types of regression and some key facts associated with each technique. As somebody whos new in this industry, Id advise you to learn these techniques and later implement them in your models.Did you find this article useful ? Share your opinions / views in the comments section below.",https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
"News  Great Lakes launches Analytics Program in Bangalore, India",Learn everything about Analytics,"KJ: What is the objective behind launching theanalytics program ?|KJ: After Chennai and Gurgaon, why are you launching this program in Bangalore?|KJ: What would be the structure of this PGPBA program?|KJ: Who is best suitedto join this program?|KJ: What are the tools covered in this program? How about the faculty?|KJ: Would youofferplacement services as part of this program?|KJ: Exciting! When does Bangalore batch begin?|KJ: Where are the classes going to be held and what will be the format?||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|7 Regression Techniques you should know!|Beginners Guide to learn about Content Based Recommender Engines|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics training landscape in India is evolving quickly and I have been lucky to be involved in this evolution. Outside of Analytics Vidhya, I spend some time teaching as a visiting faculty at Great Lakes Institute of Management. It gives me a lot of satisfaction and sense of fulfilment. While I was teaching at Great Lakes recently,I caught up withMr. Hari Krishnan Nair,Admissions Director Great Lakes PGPBA Program, who mentioned that Great Lakes is launching its Business Analytics program to Bangalore, India.As you might know, Great Lakes Institute of Management (PGPBA Program) already has a beautiful lush green campus in Chennai (Tamil Nadu) and a campus in Delhi-NCR. They are a premier management institute and have been running their analytics program for more than a couple of years now.Bangalore has a flourishing ecosystem of tech professionals (including data science and business analytics), so it makes complete sense to have a Bangalore presence for this program. But, I was still inquisitive to knowmore details about the program. Also, it would only a matter of time, when we start getting queries from our readers about this program. So, it is better to be pro-active and prepared.After the discussion, I am convinced that with their Bangalore campus, they have given aspiring data science professionalsaroundBangalore an exciting option tolearn analytics.Below are some excerpts of my conversation with Hari:Hari: The program is designed for working professionals who cannot quit their jobs but at the same time would like to build their competencies in business analytics. Our business analytics program has been designed to cater to these professionals and we bring immense experience of understanding the career needs of these professionals through our past experience. We have been running this program for ~2 years and it is one of the best programs offered in the country.Hari:Since the inception of Great Lakes business analytics program (Post Graduate Program in Business Analytics), we have seen a large number of professionals from Bangalore participating in the program.Currently around 34% of the professionals across our Chennai and Gurgaon programs are from Bangalore.Several companies with whom we interact have been suggesting us to take some initiative in delivering the PGPBA program in Bangalore to nurture the analytics talent in the IT hub of our country.Hari:Great Lakes PGPBA program is focused on business and application aspects of analytics thereby preparing candidates for managerial and techno-functional roles. The initial modules include dealing with subjects like Business Finance, Marketing & CRM, Supply chain management & operations, statistics, etc.Later in the program these subjects are linked with analytics with subjects like: Finance and risk analytics, marketing & retail analytics, web and social media analytics, supply chain and logistics analytics, etc. along with certain subjects related to domain application of analytics.This program alsofeatures a capstone project (live project) which candidates undergofor around 3 months.The program is co-created and co-delivered with industry professional from leading analytics organizations.Hari:The program has been endorsed by around 500+ professionals within a span of little over 2 years. Professionals joining the program range within work experience of 8 to 9 years with minimum work experience of 2 years and the highest which has gone till date is 28 years.The batch is a perfect picture of diversity across CXOs and industry leaders, mid-career executives and young professionalsThe class consists of candidates in leadership position such as President, Vice president, Business Unit heads, Associate directors etc. In addition to these leadership roles the class is composed of candidates across project management, quality assurance, team leads, module leads, engineering and business analyst rolesMajority of the candidates are from IT and Technology field followed by Analytics and consulting, BFSI, Healthcare, telecom and more.These professionalscomefrom organizationslikeCognizant, Accenture, IBM, Infosys, Deloitte, Amazon, Honeywell, Reliance, TATA AIA, Bank of America, Oracle, Cisco, IBM and more such reputed organizationsHari:The tools covered in this program are SAS, R and Tableau. The teaching faculties are from Great Lakes (Chennai and Gurgaon) and Industry faculty from leading analytics companies.We do not have formal placement process. However, to ensure uninterrupted flow of career opportunities for students, we undertake certain career support activities such as:Hari:We are slated to start the first batch in our Bangalore centre in the mid of September, 2015.Admission process for the same has already begun:Last Datefor submission of application for Bangalore centre is31stAugust, 2015Hari:Bangalore centre for PGPBA program will be in HSR layout.Classroom sessions will be held one weekend every month which will be supplemented by online webinars.KJ: How to apply / register for this program?Hari:Here is the link: Apply NowAs mentioned, this looks like a worthy offering from a premier institute in India. If you are some one based in Bangalore and are considering a move in analytics, this program is worth considering!",https://www.analyticsvidhya.com/blog/2015/08/great-lakes-analytics-program-bangalore/
Beginners Guide to learn about Content Based Recommender Engines,Learn everything about Analytics|Introduction|What are Content Recommender Systems?|How do Content Based Recommender Systems work?|What are the concepts used in Content Based Recommenders?|Howdoes Vector Space Model works?|Case study 1  How to calculate TF  IDF ?|Case study 2 Creating binary representation|Putting it all together  Building a Content based recommender for Analytics Vidhya (AV)|End Notes,"Term Frequency (TF) and Inverse Document Frequency (IDF)|Limitations of Content Based Recommender Systems|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|News  Great Lakes launches Analytics Program in Bangalore, India|Marketing Analytics: Essentials of Cross-Selling and Upselling (with a case study)|
Shuvayan Das
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most surprising part about Recommender Systems is, we summon to its suggestions / advice every other day, without even realizing that. Confused? Let me show you some examples.Facebook, YouTube, LinkedIn are among the most used websites on Internet today. Let us see how they use recommender systems. Youll be amazed!Facebook: Suggests us to make more friends using People You May Know sectionSimilarly LinkedIn suggests you to connect withpeople you may know andYouTube suggests you relevant videos based on your previous browsing history. All of these are recommender systems in action.While most of the people are aware of these features, only a few know that the algorithms used behind these features are known as Recommender Systems. They recommend personalized content on the basis of userspast / current preference to improve the user experience.Broadly, there are two types of recommendation systems  Content Based & Collaborative filtering based. In this article, welllearn aboutcontent based recommendation system.But before we proceed, let me define a couple of terms:According toWikipedia:Recommender systems or recommendation systems (sometimes replacing system with a synonym such as platform or engine) are a subclass of information filtering system that seek to predict the rating or preference that user would give to an item.Recommender systems are active information filtering systems which personalize the information coming to a user based on his interests, relevance of the information etc. Recommender systems are used widely for recommending movies, articles, restaurants, places to visit, items to buy etc.A content based recommender works with data that the user provides, either explicitly (rating) or implicitly (clicking on a link). Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more accurate.Image Source: easyrec.orgThe concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used in information retrieval systems and also content based filtering mechanisms (such as a content based recommender). They are used to determine the relative importance of a document / article / news item / movie etc.TF is simply the frequency of a word in a document. IDF is the inverse of the document frequency among the whole corpus of documents.TF-IDF is used mainly because of two reasons: Suppose we search for the rise of analytics on Google. It is certain thatthe will occur more frequently than analytics but the relative importance of analytics is higher thanthe search query point of view. In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).But while calculating TF-IDF, log is used to dampen the effect of high frequency words. For example:TF = 3 vs TF = 4 is vastly different from TF = 10 vs TF = 1000. In other words the relevance of a word in a document cannot be measured as a simple raw count and hence the equation below:Equation:Itcan be seen that the effect of high frequency words is dampened and these values are more comparable to each other as opposedto the original raw term frequency.After calculating TF-IDF scores, how do we determine which items are closer to each other, rather closer to the user profile? This is accomplishedusing the Vector Space Model which computes the proximity based on the angle between the vectors.In this model, each item is stored as a vector of its attributes (which are also vectors) in an n-dimensional space and the angles between the vectors are calculated to determine the similarity between the vectors. Next, the user profile vectors are also created based on his actions on previous attributes of items and the similarity between an item and a user is also determined in a similar way.Lets try to understand this with an example.Shown above is a 2-D representation of a two attributes, Cloud & Analytics. M1 & M2 are documents. U1 & U2 are users.The document M2 is more about Analytics than cloud whereas M1 is more about cloud than Analytics.I am sure you want to know how the relative importance of documents are measures. Hold on, we are coming to that.User U1, likes articles on the topic cloud more than the ones on analytics and vice-versa for user U2. The method of calculatingthe users likes / dislikes / measures iscalculated by taking the cosine of the angle between the user profile vector(Ui ) and the document vector.The ultimate reason behind using cosine is that the value of cosine will increase with decreasing value of the angle between which signifies more similarity. The vectors are length normalized after which theybecome vectors of length 1 and then the cosine calculation is simply the sum-product of vectors. We will be using the function sumproduct in excel which does the same to calculate similarities between vectors in the next section.Lets understand thiswith an example. Suppose,we search for IoT and analytics on Google and the top 5 links that appear have some frequency count of certain words as shown below:Among the corpus of documents / blogs which areused to search for the articles, 5000 contains the word analytics, 50,000 contain data and similar count goes for other words. Let us assume that the total corpus of docs is 1 million(10^6).Term Frequency (TF)As seen in the image above, for article 1, the term Analytics has a TF of 1+ log1021 = 2.322. In this way,TF is calculated for other attributes of each of the articles. These values make up the attribute vector for each of the articles.Inverse Document Frequency (IDF)IDF is calculated by taking the logarithmic inverse of the document frequency among the whole corpus of documents. So, if there are a total of 1 million documents returned by our search queryand amongst those documents, smart appears in 0.5 million documents. Thus, its IDF score will be:Log10 (10^6/500000) = 0.30.We can also see (image above), the most commonly occurring term smart has been assigned the lowest weight by IDF.The length of these vectors are calculated as the square root of sum of the squared values of each attribute in the vector:After we have found out the TF -IDF weights and also vector lengths, lets normalize the vectors.Each term vector is divided by the document vector length to get the normalized vector. So, for the term Analytics in Article 1, the normalized vector is: 2.322/3.800. Similarly all the terms in the document are normalized and as you can see (image above) each document vector now has a length of 1.Now, that we have obtained the normalized vectors, lets calculate the cosine values to find out the similarity between articles. For this purpose, we will take only three articles and three attributes.Cos(A1,A2) is simply the sum of dot product of normalized term vectors from both the articles. The calculation is as follows:As you can see the articles 1 and 2 are most similar and hence appear at the top two positions of search results. Also, if you remember we had said that Article 1 (M1) is more about analytics than cloud-Analytics has a weight of 0.611 whereas cloud has 0 after length normalization.This picture is anexample ofmovie recommender system called movielens. This is a movie recommendation site which recommends movies you should watch based on which movies you have rated and how much you have rated them.At a rudimentary level, my inputs are taken in by the system and a profile is created against the attributes which contains my likes and dislikes (may be based on some keywords / movie tags I have liked,or not done anything about).I have liked 52 movies acrossvarious genres. These items (movies) are attributessince it can help us to determine a users taste. Below is the list of top 6 movies recommended bymovie-lens. The user columns specify whether a user has liked / disliked a recommended movie. The 1/0 simply represent whether the movie has adventure/action etc. or not. This type of representation is called a binary representation of attributes.In the above example, user 1 has liked Star Wars whereas user 2 has not. Based on these types of inputs a user profile is generated.Othermetricslike count data, simple 1 / 0 representation are also used. The measure depends upon the context of use. For example: fornormal movie recommender system, a simple binary representation is more useful whereas for a search query in search engine like google, a count representation might be more appropriate.Now that we have learned all these concepts, lets actually try to build a simple content based recommender based on user taste for AV.Many articles have been published on AV.Every article published caters to a segment of audiencesuch thatsome people might like articles onmachine learning algorithms, while other might like R over machine learning. Hence, having a recommender system would help.To build the recommender system:First Step: In each week, for each user, we will track user engagement (like / share / comments)with various articles.Lets see how that can be done:We have used binary representation here. The image shown above represents, Article 1 is about big data, python and learning path. Similarly, article 2 is about R, python and machine learning. User 1 has liked article 1 (shared it on social media) and has not liked article 2. He/She has not engagedwith article 3,4,5 except reading them.Second Step: NormalizeFor binary representation, we can perform normalization by dividing the term occurrence(1/0) by the sqrt of number of attributes in the article. Hence,for Article 1: normalized attribute = 1/sqrt(3) = 0.577.A question that you must ask here is: what happened to TF? and why did we directly do normalization before calculating the TF scores? If you calculate TF without normalization,the TF scores will be 1+log 10 1 = 1 for attributes that occur and simply 0 for attributes that dont. And, thus the TF scores will also becomes 1/0.Lets see how the user profiles are generated by using the sumproduct function in excel.Each attribute column (see the highlighted learning paths columns above) is multiplied with each user attribute value (highlighted part of User 2). This is simply the dot product of vectors that we saw earlier. This gives a user profile (as shown in the User Profiles part above) an understanding of users taste.Thus, user 1 likes articles on big data most(highest score of 1.28) followed by learning paths and then machine learning.Similarly, user 2 like articles on machine learning the most.Now we have the user profile vectors and the article vectors, lets use these to predict which articles will be similar to the users taste.Note : The SUMPRODUCTfunction in the image above contains some vectors which are highlighted in the image. B36:F36 is the article 6 vector.The dot product of article vectors and IDF vectors gives us the weighted scores of each article.These weighted scores are again used fora dot product with the user profile vector (user 1 here). This gives a probability that the user will like a particular article. For article 1, the probability is 75%.This concept can be applied to n articles and we can find out which article a user will like the most. Therefore,along with new articles in a week, a separate recommendation can be made to a particular user based on the articles which he hasnt read already.Content based recommendershave their own limitations. They are not good at capturing inter-dependencies or complex behaviors. For example:I might like articles on Machine Learning, only when they includepractical application along with the theory, and not just theory. This type of information cannot be captured by these recommenders.In this article, we have seen two approaches to content based recommenders. They both use the TF-IDF weighting and vector space model implementations, albeit in different ways. So while the count data helped us understand the methodology of calculating the weighted scores of articles, the binary representation helped us understand how to calculate the scores for data represented as 1/0 and we also saw how user profiles are generated and predictions are made based on that.I have intentionally left the column PredUser2 empty in case you would like to go ahead and calculate it.Did you find this article useful ? Have you also worked on recommender systems? Share your opinions / views in the comments section below.",https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/
Marketing Analytics: Essentials of Cross-Selling and Upselling (with a case study),Learn everything about Analytics|Introduction|Definition  Cross-Sell and Up-Sell|Benefits of Cross Selling|Industry Exemplars|Cross-sell Offer Strategy||Cross-Sell Process|Cross-sell grid for identification of opportunities|Next Best Product to Recommend Model Framework|Bank Case Study: Next Best Product Recommendation Model||End Notes,"Share this:|Like this:|Related Articles|Beginners Guide to learn about Content Based Recommender Engines|What is the role of analytics in E-Commerce industry?|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Cross selling and Upselling is one of the most widelydiscussed concept in marketing analytics. Every other day when you visit a supermarket, restaurant to purchase something, this concept comes into live action. This concept is being taught in every marketing class across the world, thereby students are expected to know of it.You may be curiously wondering why we have selected this topic of Cross-Sell? Well, the answer to this is very obvious. In our experience (spawning several decades) of setting up and training teams, we have conducted hundreds of interviews. And guess what? For our standard interview question of Elaborate on a project you have worked on from start to end, the most frequent response has been a Cross-Sell Model! However, when a lot of people were quizzed around the basic concepts of cross-sell / up-sell, they were unable to provide satisfactory answers. They would know the code used, but were unable to explain this simple concept.So, here we are discussing our version of this popular and important Cross-Sell story! In order to explain the end to end process, we have also provided a case study in the second half of the article. We hope that this article becomes a ready reference for any one interested in implementing cross-sell / up-sell in their organization.Cross-sell involves the sale of multiple products offered by a single product/service provider to a new or existing customer. Up-sell is selling higher value products/services to an existing customer.For example:Cross-selling is a core component of a customer centric relationship strategy and requires an integrated view of the customer. The success of a cross-sell program depends on enablers such as organizational commitment; well-defined business strategy; effective execution; regular monitoring; and effective targeting strategy. Cross-selling has proved to be a defining strategy for profitable growth across multiple sectors.Cross Selling offers benefit to both the ends of marketing cycle i.e. customer and firm.For the FirmFor the CustomerBain, a top global consulting firm was given a project to increase the number of cross-selling relationships among existing customers and to increase spend of each customer targeted for a leading Bank. Bains analysis identified profitable customers for cross-selling and determined that, on average, cross-selling the new product would increase individual customer spend and profitability significantly.                                               Source: Bain  E-commerce: The long term strength of Amazon is its ability to recognize customers one-on-one and effectively cross-sell across categories based on targeted recommendations. The decision engine is collaborative filtering, which recommends items deemed to be similar to the items that the user or a similar like minded consumer liked in the past based on expanded view of customers purchase history and behaviours.Retail: Segmentation based cross-sell and product diversification is the key enabler for Tesco to reach top supermarket status in Britain. Targeting is based on differentiated profiles using segmentation and effective data usage of loyalty program, demographics and lifestyle attributes.Financial Services: Cross-sell is a core strategy for revenue growth for Wells Fargo, which has the highest cross-sell ratio in the industry at 6 products per household. The bank is an adopter of a successful bundling strategy backed by strong analytics support and effective tracking of cross-sell initiatives.Bundling the sale of products or services together as a combined offering using demand for the primary product to sell the secondary product. An example is a package / combo deal. This combined product or offer is at a discount so that it is more attractive to buy the bundle than products standalone. For example  Selling a combo meal for McDonalds is a good example of bundled sale.Sequential cross-sell involves selling different products or services at different points of the customers tenure with the firm. A good example of this is life-stage marketing, i.e. if a customer has bought a printer from you 6 months back, it might be time to check if he needs a cartridge.Varying strategies tailored to different segment level profiles constitute an integral component of a customer centric approach to cross-selling.The impact of bundling can be powerful. It normally leads to higher engagement and longer life cycle of the cusotmer. The following figure for a mobile service provider in Europe shows that subscribers opting for a 4-play bundle (4 services) have a quarter of the churn rate as compared to 1-play subscribers.The cross-sell process is outlined step by step in the following figure. Step 1, 4 and 5 will be elaborated upon subsequently.A cross-sell grid is a mapping of products held by customers and which is segmented by product group. The grid is created based on a unique customer identification key. The product on the vertical axis usually refers to the first relationship of the customer with the bank.Here is an example below. Inthis grid, a good percentage (43 % = 14,583/33,915)) of credit card customers hold current and savings deposit products. We can conclude that this specific cross-sell program is a successful one. However, a bigger push for unsecured and secured loans is required as the penetration of these loans among credit card customers is relatively low. Clearly this segment is an untapped opportunity.Next Best Product to Recommend Models are the foundation of cross-sell targeting analytics. These encompass triggers, segmentation, regression models and optimization.The models provide answers to the following questionsFurther, inter-purchase time (or time elapsed between purchases) is also modeled in the retail industry.Next, we will discuss a case study using segmentation and logistic regression model.1. Objective:The objective for the Bank is to improve the efficiency of Investment to Credit Card cross-sell program based on focused targeting. Investment is also called a liability product in the banking industry.2. Segmentation:For customized one- to- one marketing programs as each segment may have different needs and preferences. The Response Model will be developed for the Segment Affluent Income Low Risk.3. Data Preparation: Data Preparation for Response Model using Credit Card Customer Information and Historical Campaign Files4. Response Model: The following table shows the variables description and relationship to cross-sell response / propensity in descending order of importance using Logistic Regression5. Model Performance: The model rank orders based on a decile distribution split. It also demonstrates good performance throughout the score range based on cumulative lift ratio.Note: Rank Order implies that the response rate increases monotonically with increasing response score (for a model where higher response score indicates a higher expected response rate).Cumulative lift ratio is the cumulative response rate (by decile) divided by the sample response rate. Higher this lift ratio, better is the model discrimination power in distinguishing the responders from the non-responders.6. Implementation: Based on the response model, a cut-off of the score can be decided. Customers exceeding the cut-off should be considered for marketing. A suggested strategy could be:7. Product Prioritization for Multi-Product Cross-Sell Marketing: The decision logic to determine which product and product priority from a Banks basket of products is outlined.By now, we hope youve understood the usefulness of this concept in marketing analytics. The case study shared in this article is intended togive you a practical insight on the use of this concept in analytics. Cross Selling and Up-selling is one of the most prominent strategy used across marketing strategy of any company.Did you find this article useful ? How did your previous cross sell model perform? Share your views in the comments section below.About the AuthorsSandhya Kuruganti and Hindol Basu are authors of a book on business analytics titledBusiness Analytics: Applications to Consumer Marketing, recently published by McGraw Hill and is available on Flipkart and Amazon India. They are seasoned analytics professionals with a collective industry experience of more than 30 years.",https://www.analyticsvidhya.com/blog/2015/08/learn-cross-selling-upselling/
What is the role of analytics in E-Commerce industry?,Learn everything about Analytics,"Lets talk about the industry first!|Role of Analytics in E-Commerce|Functions supported by Analytics in E-Commerce Industry|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Marketing Analytics: Essentials of Cross-Selling and Upselling (with a case study)|Consultant  Product Development  Equifax  Bangalore (2  3 years of experience)|
Tavish Srivastava
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you are preparing for an interview into role of analytics, you need to do your ground work to get a basic understanding of domain. Also, you should know what is the role of analytics to do smarter business in this domain. But such information is not available in public neither is it available on Job Descriptions. Mostly one of your interview round will be to assess your capability to analyse a problem in their domain. If you know domain before hand, it will be a jackpot. In this article, I will introduce you to a few roles analytics plays in E-Commerce industry.E-Commerce is a very dynamically evolving industry and this is primarily because of its underlying ever-changing technology. Companies like Amazon, E-bay are capable of building predictive algorithms being executed in real time on big data environment.I just spoke 3 big words, which when combined deliverssomething unmatchable and uniquely executed by E-Commerce industry:If you have worked in financial industry, you will probably be aware of analytics playing a crucial role into risk and marketing strategy. However, E-Commerce industry goes beyond these two pillars. The primary job of E-Commerce industry is to make user experience on their website is delightful. Other than that they are simply a platform between sellers and buyers. With such focus on user experience, analytics itself becomes a product instead of just being business enabler. For instance, Recommender Engines you see on Amazon sidebar is a classic product. Now, you can appreciate the much broader role of analytics in E-Commerce industry. In the following section, we will talk more about broad functions where analytics is being actively used.This list is no way exhaustive but will cover broad roles in E-Commerce industry1. Supply Chain Management : This includes managing data for products right from warehouse to the customer.E-Commerce industries use analytics extensively to manage Inventory. Also a significant portion of work is into optimising transportation and pricing of delivery.2. Merchant/Customer Fraud Detection : I recently read a post on Facebook that someone found a showin the delivery box when he ordered a MAC for INR 90,000. This is what is known as Fraud. Even though the E-Commerce company might have nothing to do with this fraud, they are the one who pay for it. However, frauds are not always from the merchant side. Even though it is rare, customers also make false claims in frauds. Initially all these frauds were handled manually, but with time E-Commerce is moving towards developing predictive algorithm to detect frauds and avoid them if possible.3. Merchant Analytics : Merchants form the core of E-Commerce industry. If the merchant grows, E-commerce provider also grows. So E-Commerce players do extensive analysis for Merchants to get into new markets or set the right price for their goods. For instance, Amazon can recommend a Cricket Bat vendor to keep Hockey sticks because of a growing demand in his locality. Such decisions would have been much more expensive for the vendor, had they not partnered with E-Commerce players.4. Recommender Systems : As soon as I hear Recommender engines, I imagine YouTube. Recommender systems in E-Commerce industry is not very different from YouTube. These enginesserve as blueprint for customer to navigate through the store of this virtual environment. Recommender engines have been the strongest contribution of analytics to technology.Credit : TIME Magazine5. Product specific analytics : These teams generally work on product specific details for example  Satisfaction rate of customers for a product, forecast of sales for a product etc. Their work cut across verticals and are specific for a family of product or a single product.6. Online Marketing Analytics : As E-Commerce provides you a virtual environment to buy stuff, they have to market on the virtual environment extensively. The online marketing team generally works on bidding for ads on Google or other websites. They analyse the funnel of new prospect customers and maximize the likelihood of a customer clicking an ad .7. User Experience Analytics : This probably is the biggest task for analytics in E-Commerce industry. Its all about customer centricity because of the ease to shift from Amazon to Flipkart. This team primarily works on creating the right architecture of the website. This will include how is product searched across portfolio, what decides the rank ordering of products for a particular search, what is the best landing page of a customer coming from Facebook etc. They also test what type of layout is better for what type of customers.Image Source : EllianceThis list is not exhaustive as I can imagine a number of other areas where analytics can play a role. However, this list of 7 roles cover majority of analytics resources in E-Commerce industry.I hope this article gave you a sense on how E-Commerce industry leverages analytics to make customer experience delightful. I will encourage people working in this domain to add to this article and comment on the roles identified. Also in next few articles I will cover few more industries to provide a more holistic view of how analytics is shaping different industries.Did you enjoy reading this article? Do you think we missed calling out any role in E-Commerce industry? Are you inspired by any of the above mentioned roles?",https://www.analyticsvidhya.com/blog/2015/08/role-analytics-e-commerce-industry/
Consultant  Product Development  Equifax  Bangalore (2  3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|What is the role of analytics in E-Commerce industry?|Get Knowledge from Best Ever Data Science Discussions on Reddit|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Consultant  Product DevelopmentLocation  BangaloreAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant  Product Development  Equifax  BangaloreIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/08/consultant-product-development-equifax-bangalore-2-3-years-experience/
Get Knowledge from Best Ever Data Science Discussions on Reddit,"Learn everything about Analytics|Introduction||Table of Contents|AMA with Top Data Scientists|Top 9 Discussions on Tutorials / Books|Top 5 Discussions on Career / Jobs|Top 7 Discussions on Machine Learning / Deep Learning / Neural Networks|Top 6 Discussions on R Programming / Python|Top 6 Leisure Reads on ML, Python, Statistics|End Notes","1. AMA withGeoffrey Hinton|2. AMA with Yann LeCun|3. AMA with Andrew Ng and Adam Coates|4. AMA with Yoshua Bengio|5. AMA withJrgen Schmidhuber|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant  Product Development  Equifax  Bangalore (2  3 years of experience)|List of useful packages (libraries) for Data Analysis in R|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"While composing this enriching this list of data science discussions, I found thisawesomepoemdrafted statistically. Aint it pretty cool ? Dedicated to that every person who thinks this world is driven by data (including my team):A curvy young belle, Billie Jean,
Makes a measure of each man shes seen.
Yet the size of one sample
Was sufficiently ample
To skew median far from the mean.
The jealous lads started to hate him,
And the ladies all lined up to date him;
Not his charm, nor his suit
Sets the gals in pursuit,
But the infamous size of his datum.
That persistent young lass, Billie Jean,
Measured more, she was quite the machine!
Like a Bell, things now curved,
And all same, she observed,
Were the median, mode and the mean.What does this poem suggests? Of course, Ive shared it for a reason.And the subtle reason is, there are many more interesting ways to learn subjects which are difficult otherwise. Precisely, people learn data science and its related concepts using text books, guides, video-tutorials, interactive websites and every other way which comesat their disposal. Adding to this list, theres another interesting and inspiring way to learn. And that is using Discussions. Because of its need, we run a discussion portal on Analytics Vidhya as well. But todays article focuses on a much larger community  Reddit.Reddit, the Front Page of Internet, is a community known for its candid and outspoken discussions. A platform where spam-ism is despised and genuine contentis welcomed. It has content related to everything under the sun. Id be astonished, if not. However, for people who are new to Reddit, reading / participating on Reddit discussions can be very intimidating at the start. It can take some time before you understand how the forum works.But there is a ton of useful knowledge on Reddit, which you can learn. People who are active on Reddit would agree to it. Hence, in this article, we have summarized some of the best discussions related toMachine Learning, Deep Learning, Neural Networks, Artificial Intelligence, Python, R Programming, Big Data and Statistics.I hope that you benefit out of it, if you dont follow Reddit religiously or can not fit in the community.I would do injustice to this article, if I dont start it from AMAs. AMAs (or Ask Me Anything) are an important part of Reddit community and a setup where you can ask questions to experts in a particular field.I suppose these people require no formal introduction. They are considered to be the best of brains working towards the development of machine learning, deep learning, neural networks etc. If you ever seek inspiration, check out what are folks are upto. Beloware the AMAs with 3 key highlights in 4 lineseach:a) On human brain  The brain has about 1014 synapses and we only live for about 109 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 105 dimensions of constraint per second.b) On his career  My father was a Stalinist and sent me to a private Christian school where we had to pray every morning. From a very young age I was convinced that many of the things that the teachers and other kids believed were just obvious nonsense. Thats great training for a scientist and it transferred very well to artificial intelligence. But it was a nasty shock when I found out what Stalin actually did.c) On Dark Knowledge  Yes, I invented the term Dark Knowledge. Its inspired by the idea that most of the knowledge is in the ratios of tiny probabilities that have virtually no influence on the cost function used for training or on the test performance. So the normal things we look at miss out on most of the knowledge, just like physicists miss out on most of the matter and energy.a) On Career in Deep Learning Read, learn from online material, try things for yourself . Take as many math and physics course as you can, and learn to program. You have to figure out whats important, know what to ignore, and know how to approximate. These are skills you need to conceptualize, model, and analyze ML models.Another set of courses that are relevant is signal processing, optimization, and control/system theory.b) On most overlooked things in machine learning  Kernel methods are great for many purposes, but they are merely glorified template matching.There is nothing magical about margin maximization. Its just another way of saying L2 regularization (despite the cute math). There is no opposition between deep learning and graphical models. Many deep learning approaches can be seen as factor graphs.c) On emotions in Robots Emotions do not necessarily lead to irrational behavior. They sometimes do, but they also often save our lives.If emotions are anticipations of outcome (like fear is the anticipation of impending disasters or elation is the anticipation of pleasure), or if emotions are drives to satisfy basic ground rules for survival (like hunger, desire to reproduce), then intelligent agent will have to have emotions.a) Recommendation after CourseraMLcourse Herere a few common paths: 1. Many people are applying ML to projects by themselves at home, or in their companies. This helps both with your learning, as well as helps build up a portfolio of ML projects in your resume (if that is your goal). If youre not sure what projects to work on, Kaggle competitions can be a great way to start.b) Best place to work for ML & AI Engineers  I think Baidu, Google and Facebook are all great places to work!But Baidu Research is very much a startup environment. With ~40 people in our Silicon Valley team, we also invest a lot in employee development. I think these things make the best possible combination for driving machine learning research, which is why both of us (Adam & Andrew) had decided to join Baidu.c) On key drivers of Deep Learning  I think the two key drivers of deep learning are:  Rise of computation. Not just GPUs, but now the migration toward HPC (high performance computing, aka supercomputers).  Rise of availability of data, because of the digitization of our society, in which increasing amounts of activity on computers/cellphones/etc. creates data.a) On choice of academia  I like academia because I can choose what to work on, I can choose to work on long-term goals, I can work for the benefit of humanity rather than for a specific company, and I can talk about my work freely. Note that to different degrees, my esteemed colleagues in large industrial labs also enjoy some of that freedom.b) On RNN  Recurrent or recursive nets are really useful tools for modelling all kinds of dependency structures on variable-sized objects. We have made progress on ways to train them and it is one of the important areas of current research in the deep learning community. Examples of applications: speech recognition (especially the language part), machine translation, sentiment analysis, speech synthesis, handwriting synthesis and recognition, etc.c) On upcoming challenges in NLP  I believe that the really interesting challenge in NLP, which will be the key to actual natural language understanding, is the design of learning algorithms that will be able to learn to represent meaning.There are also more computational challenges: we need to be able to train much larger models (say 10000x bigger), and we cant afford to wait 10000x more time for training.a) On the future of RNN  The world of RNNs is such a big world because RNNs (the deepest of all NNs) are general computers, and because efficient computing hardware in general is becoming more and more RNN-like, as dictated by physics: lots of processors connected through many short and few long wires. Both supervised learning RNNs and reinforcement learning RNNs will be greatly scaled up.b) On his hobbies  In my spare time, I am trying to compose music, and create visual art.And while I am doing this, it seems obvious to me that art and science and music are driven by the same basic principle.I think the basic motivation (objective function) of artists and scientists and comedians is data compression progress, that is, the first derivative of data compression performance on the observed history.c)On future of AI  20 years from now well have 10,000 times faster computers for the same price, plus lots of additional medical data to train them. I assume that even the already existing neural network algorithms will greatly outperform human experts in most if not all domains of medical diagnosis, from melanoma detection to plaque detection in arteries, and innumerable other applications.1. Im struggling to learn Machine Learning on my own. How should I overcome this hurdle?Heres youll find a complete guide on overcoming periodical breakers while masteringmachine learning. More than learning, youll be inspired how other people have endured such situation.2. What are some good resource to learn Recurrent Neural Networks?Heres a list of all the resources people have found essential to learn this Recurrent Neural Networks. Youll also find the review / opinions of people with respective resources.3. Which is the best book for Machine Learning in Python ?Heres a list of all the books essential to master the concepts of machine learning. If you like to learn from books rather than blogs / videos, you just cant miss this.4. How should I start learning Natural Language Processing?People have learnt NLP using various resources available for free. They are listed in this discussions. Moreover, youll find other essential information on learning this concept.5. How to install Deep Dream on windows with vagrant dev environment?Deep Learning algorithms are trained by giving them a huge number of images, and telling them what object is in each image. Heres a step by step tutorial on setting up your machine for deep learning.6. What are the best resources for beginners to learn Big Data Analytics?Considering the seamless growth big data industry has, lot of people are deciding to enter this industry. Heres a compiled list of useful resources that has helped other people to make this move.7. What books or papers are must read for every professional statistician?If you are into statistical research and like to explore related concepts, you just cant miss this discussion. Here youll find a comprehensive list of best books / white papers on statistical learning.8. List of Blogs on Machine LearningAs the name suggests, if you want to learn machine learning, heres a list of top machine learning blogs which you can subscribe right away.9. Which are some of the best watch python videos?If learning from videos is what excites you, heres a compiled list of best ever python videos that people have shared with their reviews. Youll know what best amongst the best once you check this discussion.1. List of commonly asked interview questions on PythonAs the title suggest, heres a compiled of questions been asked to candidates in their python interviews. If you are lucky, you can also find their solution in the following threads.2. Advice on industry jobs for PhDs in Machine LearningMany people are opting to pursue PhD in Machine Learning, prominently in USA. Experienced ML professionals have shared their advice helpful to provide an overview of available / upcoming opportunities.3. How can I start my career in Artificial Intelligence?Thinking of working with artificial intelligence, is undoubtedly a brave and lucrative decision. Heres a learning path which enlightens the best ways to opt to start a career in AI.4. What could be the areas of interest to start a PhD in Machine Learning today?As mentioned above, with increased interest in PhD, people are searching for topics / areas of interest for the same. Here is a possible list of areas which are best for research.5.What are some of the best practices to learn and apply for an entry-level data analyst?In one line, this discussions can answer all your apprehensions related to career in data analytics. Experienced professionals from all over the world have generously contributed to provide best learning path.1. Why is Lua such a popular language for Machine Learning?Lua is being widely used by Facebook, Google, Twitter etc. Here are some surprising reasons why this language is being preferred over other top programming languages by the giants of internet social world.2. Whats so great about Extreme Learning Machines?ELM is basically a 2-layer neural net in which the first layer is fixed and random, and the second layer is trained. Heres a full description of this concept and its greatness.3. What are Deep Dream images ? How do I make my own?This is a complete installation guide to deep learning with essential instruction for dealing with images and related algorithms. Tutorials are also available.4.Is deep learning basically just neural networks with multiple hidden layers in it?The available answers are good enough to clarify all your confusions in deep learning, neural networks and the way they operates. People have explained in the best possible manner. Check out which suits best for you.5. Arrival of self driving cars by 2020 by using Deep LearningThis discussion is based on the future world of automation and programming. The arrival of self driving cars, use of multi-complex algorithms and replacement of humans with robots.6.New deep learning technique enables robot mastery of skills via trial and errorSpeaking of robots, heres another research done which introduces a new technique to make a robot better at performing tasks in day to day work using trial and error. The phase of robot evolution has started.7. Artificial Intelligence will take over the WorldThis is a popular discussion remained in news for a long time. Heres you find an enriching perspective to the growth and impact of AI in industries and its repercussions for human existence.1.R Users, what was something simple you learned late that you wish you learned early?People tend to discover the most interesting in a programming languages after they have spent sufficient time on them. But if you have just started with R, youre lucky! You might find this useful.2.Those of you who regularly use both R and Python for statistical analysis  when do you use each, and why?The ultimate winner among python and R can best be decided by their users, especially by those who use both. Heres a compiled list of opinions for the greatness of these languages.3.I know R. How easy would it be to pick up SQL?Even though you may not bring it for daily use, companies do require the skill of SQL in candidates. Heres are some useful career advice for you (If you already know R or Python).4.What are the top 10 built-in Python modules that a new Python programmer needs to know in detail?If you have recently learnt python, you must check out this discussion once. There are many python modules which are surely quite helpful but people dont know about them.5.Experienced Python Users: Whats the most recent new thing you learned about the language?Learning is a dynamic process. Heres compiled list of python codes, python tricks that people have discovered over the years and now have shared with their young generation.6. What are the most common misconceptions in Python?People working on python 2.x, python 3.x unknowingly tend to mess up with arguments, structures, loops etc. Heres a completelist of minor but significant misconceptions that people have come across and are crystal clear now.1. Explain me Bayesian Statistical Methods likeIm five year old?I found it quite interesting. The arduous concepts of bayesian have been explained in the most simplistic manner such that every other person can understand and never forget.2. Course Review:How is Andrew Ng Stanford Machine Learning course?Heres a genuine, unbiased course review of famous Andrew Ng ML course on courserafrom various people who have undertaken this course. This discussions should help you decide your next step.3. 11 facts about Data Science that you must knowOne of the fact is You should embrace Bayesian Approach. And the first discussions of this segments already explains it. Even more important for you learn that. Do check out the rest of 10 facts.4. List of Favourite Statistics JokesRhyme like a statistician, joke like a statistician. Youll find some of the hilarious yet intuitive jokes on statistics which you might not have heard of yet.5.How do you use python to automate tasks in life or at work?The role of python is not only limited to programming and data analysis, but goes much beyond that. Here people are sharing their logic of using python in their day to day tasks. Youll be amazed !6.What are some fun APIs and libraries to screw around with and learn from?There are many APIs which you might be unaware of, are quite useful and fun to play around with. You might like to bring any of them to use and do something crazy.If you are reading this part, Im sure you would have found some amazing stuff to add to your bookmark list.After going through all these discussions, I realized there were so many things which I was unaware of initially. Being said, there are many things which cant learn by books but by experience. And these discussions were filled with enriching experience and opinions.You might find this article bit lengthy, but dont worry, you are allowed to read this in parts.According to you, which is the best discussions of all?Which discussions helped you the most? Do share your opinion in the comments section below. Also, if you follow our discussions, which were the most useful discussions according to you?",https://www.analyticsvidhya.com/blog/2015/08/best-ever-data-science-discussions-reddit/
List of useful packages (libraries) for Data Analysis in R,Learn everything about Analytics|Introduction,"If you like what you just read & want to learn more on Big Data,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Get Knowledge from Best Ever Data Science Discussions on Reddit|Faculty for Data Science  Praxis Business School  Kolkata (5+ years of experience)|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R offers multiple packages for performing data analysis.Apart from providing an awesome interface for statistical analysis, the next best thing about R is the endless support it gets from developers and data science maestros from all over the world.Current count of downloadable packages from CRAN stands close to 7000 packages!Beyond some of the popular packages such as caret, ggplot, dplyr, lattice, there exist many more libraries which remain unnoticeable, but prove to be very handy at certain stages of analysis. So, we created a comprehensive list of all packages in R.In order to make the guide more useful, we further did 2 things:Here is a complete guide to powerful R packages, which are categorized into various stages of process of data analysis. Download Here.",https://www.analyticsvidhya.com/blog/2015/08/list-r-packages-data-analysis/
"Faculty for Data Science  Praxis Business School  Kolkata (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|List of useful packages (libraries) for Data Analysis in R|Basics of Ensemble Learning Explained in Simple English|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation Faculty for Data ScienceLocation  KolkataAbout employer Praxis Business SchoolPraxis is a Greek word signifying the combination of the art and science of theoretical learning with the virtues of practical training. This guiding philosophy has helped Praxis develop into a contemporary, industry-facing Business School that has earned Corporate Indias faith. In a rare honor for a young B-school, ICICI Bank has endowed the Deans Chair at Praxis.Praxis Business School was set up in Kolkata in 2007 with an initial academic collaboration with XLRI Jamshedpur and offers the following Full Time Programs:Praxis pioneered the teaching of analytics as a discipline by launching a one-year full-time program in Business Analytics from its campus in Kolkata in 2011, as it believed that India needs to be well-positioned to grasp the huge opportunity this discipline provides to trained professionals. The one-year program has grown in strength and stature, and has been ranked in the top-5 analytics programs in the country by the Analytics India Magazine.Job description: If you are passionate about using analytics to solve business problems and love to tell the story, Praxis Business School will love to have you on its team of accomplished faculty.ResponsibilitiesQualification and Skills RequiredFaculty @ PraxisYour PedigreeYou have an excellent academic career and a postgraduate degree from a reputed institution with adequate experience as an analytics practitioner.Interested people can apply for this job can mail their CV to[emailprotected]with subject asFaculty for Data Science  Praxis Business School  Kolkata",https://www.analyticsvidhya.com/blog/2015/08/faculty-data-science-praxis-business-school-kolkata-5-years-experience/
Basics of Ensemble Learning Explained in Simple English,Learn everything about Analytics|Introduction|What is Ensemble Learning?|Error in Ensemble Learning (Variance vs. Bias)|Some Commonly used Ensemble learning techniques|End Notes,"1. Difference in population|2. Difference in hypothesis|3. Difference in modeling technique|4. Difference in initial seed|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Faculty for Data Science  Praxis Business School  Kolkata (5+ years of experience)|Consultant- BFSI  Innoplexus  Gurgaon /Pune  (4  6 years of Experience)|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.Ensemble learning is a broad topic and is only confined by your own imagination. For the purpose of this article, I will cover the basic concepts and ideas of ensemble modeling. This should be enough for you to start building ensembles at your own end. As usual, we have tried to keep things as simple as possible.Lets quickly start with an example to understand the basics of Ensemble learning. This example will bring out, how we use ensemble model every day without realizing that we are using ensemble modeling.Example: I want to invest in a company XYZ. I am not sure about its performance though. So, I look foradvice on whether the stock price will increase more than 6% per annum or not? I decide to approach variousexperts havingdiversedomain experience:1. Employee of Company XYZ: This person knows the internal functionality of the company and have the insider information about the functionality of the firm. But he lacks a broader perspective on how are competitors innovating, how is the technology evolving and what will be the impact of this evolution on Company XYZs product. In the past, he has beenright 70% times.2. Financial Advisor of Company XYZ: This person has a broader perspective on how companies strategy will fair of in this competitive environment. However, he lacks a viewon how the companys internal policies are fairing off. In the past, he has beenright 75% times.3. Stock Market Trader: This person has observed the companys stock price over past 3 years. He knows the seasonality trends and how the overall market is performing. He also has developed a strong intuition on how stocks might vary over time. In the past, he has beenright 70% times.4. Employee of a competitor: This person knows the internal functionality of the competitor firms and is aware of certain changes which are yet to be brought. He lacks a sight of company in focus and the external factors which can relate the growth of competitor with the company of subject. In the past, he has been right 60% of times.5. Market Research team in same segment: This team analyzes the customer preference of company XYZs product over others and how is this changing with time. Because he deals with customer side, he is unaware of the changes company XYZ will bring because of alignment to its own goals. In the past, they have been right 75% of times.6.Social Media Expert: This person can help us understand how has company XYZ has positioned its products in the market. And how are the sentiment of customers changing over time towardscompany. He is unaware of any kind of details beyond digital marketing. In the past, he has been right 65% of times.Given the broad spectrum of access we have, we can probably combine all the information and make an informed decision.In a scenario when all the 6 experts/teams verify thatits a good decision(assuming all the predictions are independent of each other), we will get a combined accuracy rate ofAssumption: The assumption used here that all the predictions are completely independent is slightly extreme as they are expectedto be correlated. However, we see how we can be so sure by combining various predictions together.Let us now change the scenario slightly. This time wehave 6 experts, all of them are employee of company XYZ working in the same division. Everyone has a propensity of 70% to advocate correctly.What if we combine all these advice together, can we still raise up our confidence to >99% ?Obviously not, as all the predictions are based on very similar set of information. They are certain to be influenced by similar set of information and the only variation in theiradvice would bedue to their personal opinions &collected factsabout the firm.Halt & Think: What did you learn from this example? Was itabstruse ?Mention your arguments in the comment box.Ensemble is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions together will be termed as Ensemble Learning.In this article, we will talk about a few ensemble techniques widely used in the industry. Before we get into techniques, lets first understand how do we actually get different set of learners. Models can be different from each other for a variety of reasons, starting from the population they are built upon to the modeling used for building the model.Here are the top 4 reasons fora model to be different. They can be different because of a mix of these factors as well:The error emerging fromany model can be broken down into three components mathematically. Following are these component :Why is this important in the current context? To understand what really goes behind an ensemble model, we need to first understand what causes error in the model. We will briefly introduce you to these errors and give an insight to each ensemble learner in this regards.Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.Variance on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training. Following diagram will give you more clarity (Assume that red spot is the real value and blue dots are predictions) :Credit : Scott FortmanNormally, as you increase the complexity of your model, you will see a reduction in error due to lower bias in the model. However, this only happens till a particular point. As you continue to make your model more complex, you end up over-fitting your model and hence your model will start suffering from high variance.A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors.Ensemble learning is one way to execute this trade off analysis.Credit : Scott Fortman1.Bagging : Bagging tries to implement similar learners on small sample populations and then takes a mean of all the predictions. In generalized bagging, you can use different learners on different population. As you can expect this helps us to reduce the variance error.2. Boosting : Boosting is an iterative technique which adjust the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes over fit on the training data.3. Stacking : This is a very interesting way of combining models. Here we use a learner to combine output from different learners. This can lead to decrease in either bias or variance error depending on the combining learner we use.Ensemble techniques are beingused in every Kaggle Problem. Choosing the right ensembles is more of an art than straight forward science. With experience, you will developa knack of which ensemble learner to use in different kinds of scenario and base learners.Did you enjoy reading this article? Have you built an Ensemble learner before? How did you go about choosing the right ensemble technique?",https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/
"Consultant- BFSI  Innoplexus  Gurgaon /Pune  (4  6 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Basics of Ensemble Learning Explained in Simple English|Learn Big Data Analytics using Top YouTube Videos, TED Talks & other resources|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Consultant- BFSILocation  Gurgaon / PuneAbout employer  InnoplexusJob description: Business Consultant is expected to engage with client to help them analyze and develop strategies for business growth. The role requires development/ideation of relevant analytics use cases for adding value to the client and execute/manage the delivery of required analytics.The role requires strong understanding of Banking and related services and ability to mine the data to generate the required insights.Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant- BFSI  Innoplexus  Gurgaon /Pune,https://www.analyticsvidhya.com/blog/2015/07/consultant-bfsi-innoplexus-gurgaon-pune-4-6-years-experience/
"Learn Big Data Analytics using Top YouTube Videos, TED Talks & other resources",Learn everything about Analytics|Introduction|Who is expected to benefit most from watching these videos?|TED Talks on Big Data|GetIntroduced to Big Data Terminologies|10 Tutorials on Big Data Analytics|End Notes,"1. Introduction to Big Data by Hilary Mason, Chief Data Scientist at Bitly|2. Big Data, Small World: Kirk Borne at TEDxGeorgeMasonU|3. Kenneth Cukier: Big Data is Better Data|4. What to do with all this Big Data?|5. How to find the worst place to park in New York City using Big Data?|1.Hadoop Crash Course Workshop|2.Fundamentals of MapReduce|3.Enabling R on Hadoop|4.Introduction to Deep Learning on Hadoop|5.Introduction to Apache Cassandra|6.Introduction to PIG|7.An overview of Apache Spark|8.Introduction to Hive and HiveQL|9.Introduction to NoSQL|10.MongoDB Tutorial for Beginners|If you like what you just read & want to learn more on Big Data,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant- BFSI  Innoplexus  Gurgaon /Pune  (4  6 years of Experience)|Data Scientist  Innoplexus  Gurgaon/Pune  (2-5 years of Experience)|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There has been a lot of investment in Big Data by various companies in last few years. This rise in usage of big data analytics has resulted in high demand of skilled big data professionals. While there has been a lot of debate over usefulness of this spend, there is a clear increase in the jobs on Big Data. Here is what a quick search on indeed tells:Given the sharp increase in demand,big data has become a lucrative area to upskill yourself. However, if you are some one like me, who needs a good overview and understanding of practical benefits before learning something in a more formal fashion, you will probably struggle to find structured resources as I did some time back.There are a lot of technologies and terminologies associated with Big Data, which can act as an additional road block to get you started. Names like Hadoop, MapReduce, Spark, MongoDB, Hive take some time to get used to!After receiving an overwhelming response on my previous article on Top YouTube Videos on Machine Learning, Deep Learning, Neural Networkand looking at the lack of structured resources, the answer was simple! Heres another YouTube special for Big Data aspirants  basically my take on immersing yourself on Big Data!Disclaimer: We DO NOT intend to promote any brand or service throughthis article. The videos listed in this article are solely based on their relevance / usefulness to the audience.I have written this article keeping in mind the beginners fraternityof Big Data. Hence, this article is best suited for candidates keen to start their career in big data analytics. If you are already an experienced big data professional, this article might not be what you are looking for! However, you can still consume inspiration from TED Talks listed below.The structure of this article is designed to give a complete overview on various technologies used in Big Data Analytics. TED Talks displayed at the beginning are meant to add a pinch of inspiration to your learning path. These talks offers you to imagine an exciting world driven by numbers, analytics and big data technologies.Duration: 11:30 minsSummary: In this short video, Hilary talks about the rise of big data and how it is going to impact our work environment. She also highlights the tinybut significant changes brought by big data which includes CPUs, Data and Algorithms. Later, she examines the profile of a data scientist in her style. She highlights the applications of big data and its usage in our day to day lives.Duration: 22:00 minsSummary: Dr. Kirk Borne begins by talking about his journey to become a data scientist. Later, he covers some of the best ideas applied behind data mining and how it can be applied to our daily lives.He also talks about the small world phenomenon and 6 degrees separation. Later, he reveals some surprising statistics of big data which promises that the future world will be driven by data.Duration: 16:00 minsSummary: Kenneth lays immense focus on using data available at the granular level. Every byte of data has something or the other to reveal, all it requires is an engineer to discover. He believes that, with the available amount of information, we can find answers to all the questions which were difficult to think of earlier. Data has made us more powerful. Datacan be our greatest power if we dispose it intuitively.Duration: 12:29 minsSummary: Susan believes We are not the passive consumers of data and technology. Rather, we shape the data and make meaning from it. In this short video, she shares her perspective on the rise of big data and the different ways ofusing data for its optimal utilization. Data doesnt create meaning, we do. Data offers us a vast ocean of information which has to be churned to extract useful insights.Duration: 11:53 minsSummary: The title says it all. The speaker makes use of statistics and visualization to infer the worst place to park in NYC. He made sure that he didnt miss out any important information, hence he captured all the important variables in his graphical representations. If you ever wanted to see the real time usage of data, you shouldnt miss it.I would highly recommend these YouTube videos to people who are new tobig data analytics. Watching these quickvideos ( ~ 3 mins) videos would give you a clearoverview of the different big data technologies and the relations between them.1.What is HBase?                       Duration  3 mins2.What is Hadoop?                      Duration  3:12 mins3.What is MapReduce?                  Duration  2:39 mins4.What is HDFS?                       Duration  2:51 mins5.What is Flume?                       Duration  2:59 mins6.What is PIG?                         Duration  3:01 mins7.What is Hive?                        Duration  2:52 mins8.What is Avro?                        Duration  3:00 mins9.What is Oozie?                       Duration  2:28 mins10.What is Zookeeper?                 Duration  3:26 minsDuration  55:32 minsSummary: As the name suggest, this video covers all about Hadoop and related concept in less than an hour. The speaker begin with a quick introduction of Hadoop, followed by explaining hadoop ecosystem and distribution, HDFS in detail. Later, multiple components of Hadoop such as Mapreduce, Yarn, Tez are explained using some interesting stories. Finally, he winds up this crash course by revealing some of the not so popular but super useful ways of accessing data.Duration  32:03 minsSummary: This is a complete tutorial to learn basics of MapReduce. This tutorialseries is divided in 5 parts, each of which covers a specific module of MapReduce. This introductory video on MapReduceprovides a detailed overview on itsimportance, related job opportunities, applications and usage. As you navigate through its following parts, you will cover essential fundamentals of MapReduce. Do check the Up Next section while you are there!Duration  40:25 minsSummary: This tutorial teaches you the knowledge of integrating hadoop with R. The speaker follows a step by step process of Hadoop installation on R. Concepts like RHadoop, RHive and various related R libraries have been discussed. Furthermore, he also discusses on varied usage ofR and how R programming has evolved over the years.Duration  41:14minsSummary: The speaker beautifully explains the concept of deep learning using hadoop. Deep Learning is one of the most talked about topic in data science community. Scientists and researchers are working hard to discover new patterns using deep learning. The concept of deep learning has been explained in a simplistic manner in this video. Topics like deep belief networks, implementation of Hadoop / YARN have also been discussed.Duration  1:15:06 hourSummary: I have found very few videos on Apache Cassandra but this makes up for all. Heres a complete introduction to Apache Cassandra from scratch. The rise of Apache Cassandra is catching eyes of companies and professionals across the world. In this video, the speaker explains the algorithms used, its essential features, benefits and the concept / cause behind launching Apache Cassandra ~6 years back.Duration  30:56 minsSummary: This is a complete tutorial to lean about PIG. In this tutorial, the instructor begins with providing an overview of Pig followed by the comparison between Pig and SQL. Since both are very similar, it makes an interesting comparison. He also explains about using Pig latin. Above all this, the basic steps of Pig installation have also been illustrated.Duration  1:06:14 hourSummary: This tutorial aptly justifies its title by teaching about the spark technology and how it can help in shaping the world. The tutorial begins with a quick refresher of mapreduce followed by spark and the advantage of using this technology. The speaker has beautifully explained these concepts.Duration  1:06:19 hourSummary: Hive is built on top of hadoop to provide data management, querying and analysis.This tutorial discusses hive architecture, hive operations and other related functions.This tutorial not only enriches you with theoretical knowledge, but also displays the practical aspect and demonstrates the same on terminal.Duration  54:51 minsSummary: This is one of the best video I have come across on NoSQL databases. Youll find an introduction to NoSQL databases along with every other essential knowledge of this concept which you must possess. This tutorial covers application, advantages, disadvantages, compatibility, usage, characteristics and various other essential features of NoSQL. Ill recommend this video for everyone.Duration  4:34:47 hourSummary: If you ever longed to learn MongoDB, here the complete resource for you. This tutorial comprehensively covers all the aspect of MongoDB and NoSQL databases. Though, it appears to be quite long ( > 4 hours), you can watch this tutorials in breaks. A prior knowledge of Javascript wouldbe advantageous for learning MongoDB through this tutorial. This tutorial begins with introduction to NoSQL databasesfollowed byexplaining mongodb, how to run mongodb queries, node.js, advanced data processing and method to learn mongodb on cloud ubuntu.Alternate resource: MongoDB course on UdacityIf you have watched the videos listed above  you would be equipped with the essentials of Big Data by now. In this article, I have highlighted the most helpful YouTube videos and TED talks I found on internet. The videos listed are intend to build you big data basics and make your learning path easier.If you wish to reap maximum benefits from these videos, Id insist to make notes and get your hands dirty while watching these videos. In case I have missed out on any important video, feel free to mention it in the comments section below.",https://www.analyticsvidhya.com/blog/2015/07/big-data-analytics-youtube-ted-resources/
"Data Scientist  Innoplexus  Gurgaon/Pune  (2-5 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Learn Big Data Analytics using Top YouTube Videos, TED Talks & other resources|(Senior) Big Data Engineer  Cleartrip  Bangalore  (2+ years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  Gurgaon/PuneAbout employer  InnoplexusJob description: Responsibilities Qualification and Skills RequiredPreferred QualificationsInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  Innoplexus  Gurgaon/Pune,https://www.analyticsvidhya.com/blog/2015/07/data-scientist-innoplexus-gurgaon-2-5-years-experience/
"(Senior) Big Data Engineer  Cleartrip  Bangalore  (2+ years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Innoplexus  Gurgaon/Pune  (2-5 years of Experience)|Beginners Guide To Learn Dimension Reduction Techniques|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  (Senior) Big Data EngineerLocation  BangaloreAbout employer  CleartripAt launch, Cleartrip redefined travel with its simplistic design philosophy and customer centric approach. Later we brought the same approach to solving the hotel discovery problem across India. Of course, there are many more exciting projects in the works. Our young & geeky workforce is spread across Mumbai, Gurgaon, Bangalore and Dubai building disruptive experiences on mobile and desktop. When you are not rubbing shoulders with developers from top Engineering colleges and product managers from top management institutes across the world, you might chill out with them playing sports, partying at local hangouts or enjoying beachside during offsite holidays. No wonder, Cleartrip has been ranked 2nd in professional services category in the survey of best places to work 2015 by Great Places to Work Institute. Theres never been a more thrilling time to join Cleartrip.Job description: Responsibilities Qualification and Skills RequiredQualifications:Interested people can apply for this job can mail their CV to[emailprotected]with subject asBig Data Engineer  Cleartrip  Bangalore",https://www.analyticsvidhya.com/blog/2015/07/big-data-engineer-cleartrip-bangalore-2-years-experience/
Beginners Guide To Learn Dimension Reduction Techniques,Learn everything about Analytics|Introduction|Table of Contents|Why Dimension Reduction is important in machine learning & predictive modeling?|What are Dimension Reduction techniques?|What are the benefits of Dimension Reduction?|What are the common methods to perform Dimension Reduction?|Is Dimension Reduction Good or Bad?|End Note,"||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|(Senior) Big Data Engineer  Cleartrip  Bangalore  (2+ years of Experience)|Consultant- Retail  Equifax  Bangalore  (2 + years of experience)|
Sunil Ray
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Brevity is the soul of witThis powerful quote by William Shakespeare applies well to techniquesused in data science & analytics as well. Intrigued ? Allow me to prove it using a short story.In May  2015, weconducted a Data Hackathon( adata science competition) in Delhi-NCR, India.We gaveparticipants the challenge to identifyHuman Activity Recognition Using Smartphones Data Set. The data set had 561 variables for training model used forthe identification of Human activity in test data set.The participants in hackathon had varied experience and expertise level. As expected, the experts did a commendable job at identifying the human activity. However, beginners & intermediates struggled with sheer number of variables in the dataset (561 variables). Under the pressure of time, these people tried using variables really without understandingthe significance level of variable(s). They lacked the skill to filter information from seemingly high dimensional problems and reduce them to a few relevant dimensions  the skill of dimension reduction.Further, this lack of skill came across in several forms in way of questions asked by various participants:If you have faced similar questions, you are reading the right article. In this article, we will look at various methods to identify the significant variables using the most common dimension reduction techniques and methods.The problem of unwanted increase in dimension is closely related to fixation of measuring / recording data at a far granular level then it was done in past. This is no way suggesting that this is a recent problem. It has started gaining more importance lately due to surge in data.Lately, there has been a tremendous increase in the way sensors are being used in the industry. These sensors continuously record data and store it for analysis at a later point. In the way data gets captured, there can be a lot of redundancy. For example, let us take case of a motorbike rider in racing competitions. Today, his position and movementgets measured by GPS sensor on bike, gyro meters, multiple video feeds and his smart watch. Because of respective errors in recording, the data would not be exactly same. However, there is very little incremental information on position gained from putting these additional sources. Now assume that an analyst sits with all this data to analyze the racing strategy of the biker  he/ she would have a lot of variables / dimensions which are similar and of little (or no) incremental value. This is the problem of high unwanted dimensions and needs a treatment of dimension reduction.Lets look at other examples ofnew waysof data collection:With more variables, comes more trouble! And to avoid this trouble, dimension reduction techniques comes to the rescue.Dimension Reductionrefers to the process of converting a set of data havingvast dimensionsinto data with lesser dimensions ensuringthat itconveys similar information concisely.These techniques are typically usedwhile solving machine learning problems to obtainbetter features for a classification or regression task.Lets look at the image shown below. It shows 2 dimensions x1 and x2, which are let us say measurements of several objectin cm (x1) and inches (x2). Now, if you were to use both these dimensions in machine learning, they will convey similar information and introduce a lot of noise in system, so you are better of just using one dimension.Herewe have converted the dimension of data from 2D (from x1 and x2) to 1D (z1), which has madethe data relatively easier toexplain.In similar ways, we can reduce n dimensions of data set to k dimensions (k < n) . These k dimensions can be directly identified (filtered) or can be a combination of dimensions (weighted averages of dimensions) or new dimension(s) that represent existing multiple dimensions well.One of the most common application of this technique is Image processing. You might have come acrossthis Facebook application  WhichCelebrity Do YouLookLike?. But, have you ever thoughtabout the algorithm used behind this?Heres the answer: To identify the matched celebrity image, we use pixel data and each pixel is equivalent to one dimension. In every image, there arehighnumber of pixels i.e. high number of dimensions. And every dimension is important here. You cant omit dimensions randomlytomake better sense of your overall data set. In such cases,dimension reduction techniques help you to find the significant dimension(s) using various method(s). Well discuss these methods shortly.Lets look at the benefits of applying Dimension Reduction process:There are manymethods to perform Dimension reduction. I have listed themost common methods below:1. Missing Values: While exploring data, if we encounter missing values, what we do? Our first step should beto identify the reason then impute missing values/ drop variables using appropriate methods. But, what if we have too many missing values? Should we impute missing values or drop the variables?I would preferthe latter, because it wouldnot have lot more details about data set. Also, it wouldnothelp in improving the power of model. Next question, is there any threshold of missing values for dropping a variable? It varies from case to case. If the information contained in the variable is not that much, you can drop the variable if it has more than ~40-50% missing values.2. Low Variance: Lets think of a scenario where we have a constant variable (all observations have same value, 5) in our data set. Do you think, it can improve the power of model? Ofcourse NOT, because it has zero variance. In case of high number of dimensions, we should drop variables having low variance compared to others because these variables will not explain the variation in target variables.3. Decision Trees:It is one of my favorite techniques. It can be used as a ultimate solution to tacklemultiple challenges like missing values, outliers and identifying significant variables. It worked well in ourData Hackathon also. Severaldata scientists used decision tree and it worked well for them.4. Random Forest: Similar to decision tree is Random Forest. I would also recommend using the in-built feature importance provided by random forests to select a smaller subset of input features. Just be careful that random forests have a tendency to bias towards variables that have more no. of distinct values i.e. favor numeric variables over binary/categorical values.5. High Correlation:Dimensions exhibitinghigher correlation can lower down the performance of model. Moreover,it is not good to have multiple variables of similar information or variation also known asMulticollinearity. You can usePearson (continuous variables) or Polychoric (discrete variables) correlation matrix to identify the variables with high correlation and select one of them using VIF(Variance Inflation Factor). Variables having higher value ( VIF > 5 ) can be dropped.6. Backward Feature Elimination:In this method, we start with all n dimensions. Compute the sum of square of error (SSR) after eliminating each variable (n times). Then, identifying variables whose removal has produced the smallest increase in the SSR and removing it finally, leaving us with n-1 input features.Repeat this process until no other variables can be dropped. Recently in Online Hackathon organised by Analytics Vidhya (11-12 Jun15), Data scientist who held second position used Backward Feature Elimination in linear regression to train his model.
Reverse to this, we can use Forward Feature Selection method. In this method, we select one variable and analyse the performance of model by adding another variable. Here, selection of variable is based on higher improvement in model performance.7. Factor Analysis:Lets say some variables are highly correlated. These variables can be grouped by their correlations i.e. all variables in a particular group can be highly correlated among themselves but have low correlation with variables of other group(s). Here each group represents a single underlying construct or factor.These factors aresmall in number as compared tolarge number of dimensions. However, these factors are difficult to observe. There are basically two methods of performing factor analysis:8. Principal Component Analysis (PCA): In this technique, variables are transformed into a new set of variables, which are linear combination of original variables. These new set of variables are known as principle components.Theyare obtained in such a way that first principle component accounts for most of the possible variation of original data after whicheach succeeding component has the highest possible variance.The second principal component must be orthogonal to the first principal component. In other words, it does its best to capture the variance in the data that is not captured by the first principal component. For two-dimensional dataset, there can be only two principal components. Below is a snapshot of the data and its first and second principal components. You can notice that second principle component is orthogonal to first principle component.The principal components are sensitive to the scale of measurement, now to fix this issue we should always standardize variables before applying PCA. Applying PCA to your data set loses its meaning. If interpretability of the results is important for your analysis, PCA is not the right technique for your project.Recently, we received this question on our data science forum. Heres the complete answer.In this article, we looked at the simplified version of Dimension Reduction covering its importance, benefits, thecommonly methods and the discretion as to when to choose a particular technique.In future post, I would write about the PCA and Factor analysis in more detail.Did you find the article useful? Do let us know your thoughts about this article in the comment box below. I would also want to know which dimension reduction technique you use most and why?",https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/
"Consultant- Retail  Equifax  Bangalore  (2 + years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Beginners Guide To Learn Dimension Reduction Techniques|Sr. Consultant- Bureau Analytics  Equifax  Bangalore  (3-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Consultant- RetailLocation  BangaloreAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant- Retail  Equifax  Mumbai,https://www.analyticsvidhya.com/blog/2015/07/consultant-retail-equifax-bangalore-2-years-experience/
"Sr. Consultant- Bureau Analytics  Equifax  Bangalore  (3-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant- Retail  Equifax  Bangalore  (2 + years of experience)|Domain Consultant (CPG/FMCG)  Analytics Quotient Bangalore (3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr. Consultant- Bureau AnalyticsLocation  BangaloreAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject as Sr. Consultant- Bureau Analytics  Equifax  Bangalore,https://www.analyticsvidhya.com/blog/2015/07/sr-consultant-bureau-analytics-equifax-bangalore-3-4-years-experience/
"Domain Consultant (CPG/FMCG)  Analytics Quotient Bangalore (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Consultant- Bureau Analytics  Equifax  Bangalore  (3-4 years of experience)|Overview of Analytics Industry in India (my notes and views)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Domain Consultant (CPG/FMCG/)Location  BangaloreAbout employer Analytics QuotientAnalytics Quotient (AQ)  www.aqinsights.com provides analytics-powered solutions to marketing and business problems for leading global marketers. Some of our current key clients include:We are a fast-growing, ambitious and successful start-up and are looking for smart, enthusiastic people to join us. Some key facts about AQ:Job description: ResponsibilitiesQualification and Skills RequiredSALARY & OTHER DETAILSInterested people can apply for this job can mail their CV to[emailprotected]with subject asDomain Consultant (CPG/FMCG)  Analytics Quotient Bangalore",https://www.analyticsvidhya.com/blog/2015/07/domain-consultant-cpgfmcg-analytics-quotient-bangalore-3-years-experience/
Overview of Analytics Industry in India (my notes and views),Learn everything about Analytics|Overview of Analytics / Data Science industry:|How big is analytics industry and how fast is it growing?|Which sectors are using / benefiting from analytics?|What about jobs? Are analytics jobs increasing by the day?||Rise of Analytics Startups globally & in India|The analytics training market in India|A few other trends in analytics industry in India:|End Notes,"Size of analytics services market:|The Analytics Products market:|Data Science start-ups in India|Share this:|Like this:|Related Articles|Domain Consultant (CPG/FMCG)  Analytics Quotient Bangalore (3+ years of experience)|Senior Domain Consultant  Analytics Quotient  Bangalore  (4  7 years of experience)|
Kunal Jain
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common question I get asked around isWhat is your view about Analytics industry in India?and it comes in various shapes and sizes. From the curious ones asking finer details about the industry, the optimists confirming that data science would be the fastest growing field in the next decade topessimists asking whether data scientists would evenbe required 5 years from now?To be honest, I myself dont know answers to a lot of these questions (and that is the reason it is exciting to be working in this domain!) and at times I discuss these questions with various thought leaders in industry. But, I do have my views on these questions. These views are mostly based on my work experience and interaction with people in industry across the globe over last 9 years.Since I get these questions very frequently, I hope that a lot of people would be interested in knowing these views. Hence, I thought I would share my notes and views on the industry through this article. I hope this helps those thousands of people trying to answer these questions for making their own decisions.
Table of Contents:Before we proceed any further, let us understand the setup of analytics industry today. This section becomes the basis for the rest of the article, so you should put close attention to this section.Data Science industry is best understood through following diagram:As you can see, the industry can be looked as a summation of three different verticals  Data Science products, the in house analytics happening in various companies and the third party services / consultancy provided by companies. Among the three, it is very difficult to size the in house analytics setup. How do you put a value to the contribution of data science in the products and services provided by Google?Hence, whenever you hear any metric related to size of data science industry, it would mostly be focused on either products or services. Also, there are a few additional industries associated with data science which are not covered in this framework. These would include data science training institutes, placement agencies and even Analytics Vidhya! We will touch upon these associated industries later in the article.According to Avendus Capital (in 2012), the data analytics market in India is expected to reach $1.15 billion by 2015, and will account for a fifth of Indias knowledge process outsourcing (KPO) market of $5.6 billion. Further, as per recent report published byNASSCOM (2014), this is further expected to double up and become $2.3 billion by 2017-18.Bulk of this revenue would be driven by the top companies like Mu-Sigma, Fractal, AbsolutData, LatentView etc.According to a research from Everest Group, the size of global analytics services is between $2  2.5 billion in 2013. This essentially means that India holds 35%  50% of global analytics services market.If you thought, the numbers quoted above were big, there are bigger numbers coming ahead. And if you thought that the numbers above were not so big, continue reading. The global Data Science / analytics marketplace today stands at $100 billion and is growing at 30% year on year.So, the services industry mentioned above is actually a small portion in overall scheme of things. However, for some reason the focus of data science industry in India has been services and not products  possibly because of presence of the employers.In terms of penetration, different sectors have seen different penetration and adaption of analytics. Here is the distribution of services revenues (globally) by various sectors from the study done by Everest research in 2013.As you can see, the sectors with highest revenues are CPG & Retail, BFSI, Telecom and Healthcare. Similartrend holds true for India as well. I would think that BFSI & telecom would be a larger share in India as opposed to global revenues based on my interaction with people in industry, but that is a view.
For more details, people can look at the list of companies using analytics in India here.I am sure you have heard the headline here. The famous McKinsey report quotes a shortfall of up to 190,000 data scientists and 1.5 Mn data managers in the U.S. alone. The shortage obviously increases when you look at the situation across the globe. Let us add more texture to this.Here is what Google Trends speaks about searches related to analytics jobs across the globe:
While the global trend is quite evident and India is the most significant area in the heatmap of Google searches, the trend in India looks to have lot more noise. Here is how it looks specifically for India:
Looking at regional searches with in India, Gurgaon & Bengaluru seem to have the highest search for analytics jobs, while Bengaluru, Hyderabad and Chennai seem to have most searches for Big Data jobs. This is also in sync with our experience with jobs in industry. As per our estimate, Bengaluru would have 30  40% jobs market share, Delhi NCR would have 25  30% market share and then the remaining cities. Here is a heat map of the same:

This is probably one of the most exciting times to be in a startup, more so for a data science startup. In general, people are more open to taking risk than ever before. Big Data and Data Science projects have received a very favorable response from the venture capital. Today data science is directly or indirectly impacting every major start-up. A while back, we also saw an increase in data science based start-ups in Y Combinator.Just for context, here is a list of top 10 most funded analytics / Big Data startups in Jan 2015 (source: Gill Press on Forbes):Start-up Name              Amount of Funding   What are they doing?Cloudera                      $1040 Million Hadoop-based software, services and trainingPalantir Technologies $950                   Analytics applicationsMongoDB                     $311                    Document-oriented databaseDomo                        $250                    Cloud-based Business intelligenceMu Sigma                     $195                    Data-Science-as-a-ServiceDataStax                      $190                   Apache Cassandra-based platformMapR                        $174                    Hadoop-based software, services and trainingOpera Solutions               $122.2                  Data-Science-as-a-ServiceGuavus                       $107                    Operations intelligence platformUntil few years back, analytics market in India was primarily being driven by Blue Chip companies and Consulting Firms. But, the situations has started to change now. The ferocious wave of startups striking every possible industry, alsoentered the analytics market.While it may be too early to comment on success / failure of these startups right now, what I like is the fact that some of these start-ups are aimed towards creating products rather than the services market. For example, Gramener is creating a software for visualizing data. Here is a list of some exciting startups in India. As would be evident from the list above, Opera Solutions and Mu Sigma would be the most funded start-ups from India.As mentioned in the overview section, this does not fall directly under data science industry, but is closely associated with the industry.Hence, I thought I would add some details on this as well. Lately, there has been an explosion in number of institutes (both recognized and new) offering data science trainings / courses in India. On one side, there are likes of ISB, Great Lakes, Praxis and IIMs offering several executive programsand on the other hand, there are several players providing short term certifications.I think the size of analytics training industry would be close to ~100 Crores p.a. and is increasing at approximately 20% p.a.While there has been an increase in the number of offerings to create industry professionals, till now the industry has not been very open to these candidates. Among the ones who undergo these trainings, only a few get placed in analytics roles with in 3-6 months of finishing the training.P.S. This is by no means saying that people dont get placed. I have seen many transition stories, but it takes time and effort. You can read this article for a more detailed view about transition into analytics.So, there is a kind of imbalance in industry today, where on one hand there are trained people wanting to enter the industry and on the other hand companies are not able to find talent for the open positions they have.So, there is a kind of imbalance in industry today, where on one hand there are trained people wanting to enter the industry and on the other hand companies are not able to find talent for the open positions they have. Normal cycle for filling up an analytics position in industry stands at staggering 6  12 months. And the more senior position you are looking for, the more difficult it is go get good people. Over time, I expect this gap to narrow, but there is clear need of improvement here.This article is meant to provide an overview on the current situation of analytics industry in India. Please note that some of the information mentioned above are my views based on my interaction with people. It may or may not be wrong. If you have spent a few years in industry in India or outside, I would love to hear your take on these subjects (and otherwise). If you can share your views and notes through the comments below, I would love to hear them and people reading this would benefit tremendously.Sources / References:",https://www.analyticsvidhya.com/blog/2015/07/overview-analytics-industry-india/
"Senior Domain Consultant  Analytics Quotient  Bangalore  (4  7 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Overview of Analytics Industry in India (my notes and views)|Food for thought: How to Measure Influence in a Network?|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Domain ConsultantLocation  BangaloreAbout employer Analytics QuotientAnalytics Quotient (AQ) provides analytics-powered insights and solutions to marketing and business problems to leading global marketers. We are headquartered in Bangalore, India. In the USA, our offices are located in Atlanta, GA. Our clients, Fortune 100 companies around the world, have diverse analytics and visualization needs. Our consultants help these companies make better business and marketing decisions by harnessing the power of analytics and visualization.Job description: This position will manage a team of 20-30 people. The position will be responsible for running this business jointly with an onsite Engagement Manager  managing revenues, ideating and developing analytics and visualization projects, managing the team  recruitment, mentorship and planning, setting delivery standards and consistently delivering Client Delight.ResponsibilitiesKey Performance Indicators will includeQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Domain Consultant  Analytics Quotient  Bangalore",https://www.analyticsvidhya.com/blog/2015/07/senior-domain-consultant-analytics-quotient-bangalore-4-7-years-experience/
Food for thought: How to Measure Influence in a Network?,Learn everything about Analytics|A quick exercise|Importance of Influence in todays world|Letstry and measure influence|Create a single score|End Notes,"Case 1 :|Case 2:|Other applications:|||Tweak the score to suit your need||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Senior Domain Consultant  Analytics Quotient  Bangalore  (4  7 years of experience)|Top Data Scientists to Follow & Best Data Science Tutorials on GitHub|
Tavish Srivastava
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lets say you are a customer service executive working for a bank (and the only one for this hypothetical case). You need to make sure that you take decisions in the best interest of the bank. Lets assume that you can read the names of customers in waiting queue before you decide which one to pick. You need to tell, which call would you pick!All set?Okayhere goes the first set of customers waiting to talk to you:The choice should have been easyright? Well done! Mr. Pincus was happy with your serviceHere are next set of customers:Whose call would you answer now? A difficult (and delightful) situation to be in.Coming to the point, we make a lot of decisions based on what we think a persons influence is. At times, these could be simple decisions (like the first set of customers you got). But, at times, this could be mind-numbing (scenario 2). Think of a scenario when you have to answer 10 calls from this list.In todays article, we will come up with a framework and make use of data science to measure influence scientifically.Let us now consider a more practical scenario. youneed tohire for Business Development Ninja for a Technology firm. Theideal candidate should be able to win contracts all by himself and create a network of customers and their referrals.How smartly can you find this candidate?You have received 1000 applications which you need to browse through. After a week of prolonged resume searches, here are somedetails which you could extract:This information can help, but it would take far too much research and time to finalize a candidate..and finally you did some smart work. Check out Case 2.For the same company, you have boughta few networking parameters from LinkedIn on 990 profiles. Now, additionally, you have the following information :a. Total Networking index for every candidate. For every candidate, this score will capture the following:b. Domain specific Networking index: This will be a similar metric but the entire population is restricted to a particular domain. Hence, you can get this index for finance, consulting, operationsandBD for the same user.Think:With this information andthe inputs from case 1, how much easier has your job become now? Is it better or is it worse now? Will such indices make the taskeasier for you to takedecideBefore proceedingbeyond this point, write down your thoughts in the comments section below. Id love to have a discussion on this.If you are thinking that the use of influence is only in case of selecting A vs. B, think again! Here are a few real world applications to drive the importance home:All these cases need you to measure influence and accordingly take decisions. Since there are huge numberof options available in each of these cases, you can not rely on your gut feel any more. You need a scientific criteria to measure influence  so lets get to work!Suppose,you are asked to design this networking index (mentioned in the recruitment case above) and come up with a common score which can help assess the network of users on LinkedIn. The first thing you shoulddo is to find the objective function right. Try doing this exercise on your own before you check outmy approach.We are trying to assess the following aspects through this index:If we can find a metric which can take into account all four, well have thetool / featurewhich hasnt been created yet! My idea is to develop something really simple which can do all these. Here are a few definitions I have created to build this index :1.Geographical Coverage : Divide the entire world into 10*10 equal segment (you can choose any number here), and see the presence of friends in each of these segments. The number in following table denotes the number of friends. In this example, geographical coverage is 6. To eliminate noise we can put a threshold on number of people required in block to make it count (Here I have not used any thresholds).2. Geographical Compression Rate : Make the blocks bigger and recheck the coverage. For instance in following diagram, the broad coverage stays at 6. I define Geographical compression rate as (Broad coverage / Geographical coverage) which is 1 in this case. This number will be somewhere between 4 and 1 in my case. The smaller this number, more spread out are all the points. Hence smaller compression rate is desirable.
3.Total Influence: Currently we are considering only head counts, but all connections are not the same. Hence, the total influence will simply be the score of all the total index (which we will define later) across the entire user network.4.Diversity in domain expertise : Lets broadly categorize the domains into : Finance, Marketing , Operations, Manufacturing and Information Technology. Now we do a similar exercise as geographical coverage. Here I put a threshold of atleast 2 people required to make the field count. Here the domain diversity is 2 as IT is not a field which will count for this user (below threshold).We have all the ingredients now. Lets look at the desirable direction of each co-efficients.1. Geographical Coverage (G) : Higher the better2.Geographical Compression Rate (C) : Lower the better3. Total Influence (I): Higher the better4. Diversity in domain expertise (D): Higher the betterAs you can clearly see, calculation of Total Influence is not very straight forward and needs to be done iteratively. We can start with a common weight across as 1 and then try to iterate all the Total Influence.Now is the time to define our Influence score, (simply put)Influence Individual Score = G*I*D/Cwhere, I = Sum over user network (Influence Individual Score)Here are some variations which can be incorporated inthe methodology. For instance, you need the Influence Individual Score only for Consulting network. You simply restrict the entire universe to only Consulting jobs in profile. Now calculate the same index to get this variation. There can be many other variations to it, for instance you need the influence score for only Asia zone. Again the approach will be to restrict the universe to Asia location.I hope you enjoyed this exercise and hope someday we will actually see these types of products being used in the market. The score will be as powerful as FICO or CIBIL scores (used in financial markets). There have been a few attempts on creating something like this in past, Klout probably being one of the most recognized one. However, the problem is far from solved yet.Did you enjoy reading this article? Have you wondered over this question before? Do you think you can improvise this frameworkfurther to make it more realistic?",https://www.analyticsvidhya.com/blog/2015/07/food-thought-quantifying-users-network/
Top Data Scientists to Follow & Best Data Science Tutorials on GitHub,Learn everything about Analytics|Introduction|What is GitHub?|4 Amazing Facts About GitHub|Data Science Tutorialson GitHub|Top 30Data Scientists to Follow on GitHub|End Notes,"1. Getting started with Data Science|2. Algorithms|3. Machine Learning|4. Deep Learning|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Food for thought: How to Measure Influence in a Network?|Lead/Manager  Equifax  Mumbai  (5-7 years of Experience)|
Analytics Vidhya Content Team
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Twitter started the trend of People to Follow. Thislater got replicated by other platforms such as Facebook, Linkedin, Quora and GitHub. This cool feature lets you connect with the rockstars of various domainsand get an access to what is going on their end without bothering them much. For the influencers, this has become an effective way to communicate with their followers.The lives of people on GitHub doesnt appearto as temptingas you would observe on other platforms, but if you lovecoding, programming and data science,youll surely enjoy the company of 9 million users on this platform!Following influencers is usuallya good practice. Ithas helped me in multiple ways:If you havent tried this yet, its your turn now. I have compiled a list of some awesome Data Scientists on GitHub. In addition, youll also find the list of best data science tutorials available on GitHub. Specially for beginners, if dont know aboutGitHub, heres the quick introduction in simple words.You can best understand GitHub as a social network for coders across the globe. Coders across the world can share their codes and work in collaborative manner using GitHub. GitHub startedin 2008 and is aweb based platform which provides online project hosting using Git.Git is a version control system which helps you save various versions of your project in the original form and allows you to retrieve them later without any problem. Git was createdby Linus Torvalds (he also created Linux) and has been a boon to programmers across the world.It is a free, open source platform, where programmers from all over the world can save, display their codes.GitHub has not only made procuring codes an easy task, but have also rendered immense support to theprogrammers, coders worldwide.To be honest, it is difficult to imagine the programming world without Git today!Now, if you are new to GitHub, you would be asking, where do tutorials come in on a platform meant for version control and sharing of codes. Well, because of its niche community, a lot of people have started creating resource repositories on GitHub. Essentially, since the programmers spend a lot of time on GitHub, why not create list of resources they use regularly.Heres a compiled list of tutorials on various topics in data science. These resources can be very handy. I suggest you to bookmark these (or watch these on GitHub).Awesome Data Science: This is an awesome repository if you are to begin with Data Science. Here youll find every step that you need to take till the end of your journey.Data Science Resources:This is another repositoryof data science tutorials to help you conquer this skill set. You can free to choose any of these, both are equally good.Text Books in Data Science:If you like to read and refer to books, here is a compiled list of best books on machine learning, data mining, statistics, data visualization etc.Data Science Algorithms: Heres a comprehensive overview & explanationof algorithms such as Linear Regression, Logistic Regression, K-Mean Clustering, Random Forest. Youll also find their worksheets for practice.Statistics and ML:Heres a list of tutorials to become efficient in your day to day programming. It covers python pandas, machine learning algorithms, statistics and data visualizationScikit Learn:Scikit learn is a python library for machine learning.This repository has everything to offer to help you learn about machine learning in Python. ( Hint: Dig Deeper )Awesome Machine Learning:Here is an ultimate list of tutorials, resources, guides for machine learning, data analysis, natural language processing, data visualization in all the programming languages like Python, R, Java, Go, C++, Swift. Choose accordingly.Complete Machine Learning:Heres a collection oftutorials and examples for solving problems using machine learning. It consist of beginning to end steps of ML covering stages such asmodel evaluation, implementation of ML algorithms, data visualization etc.Parallel Machine Learning:This tutorial is on using scikit learn and ipythonfor parallel machine learning. Here youll find a2 hours long video from Pycon 2013 with lecture notes and other useful resources.Machine Learning Courses:Heres a list ofBest Machine Learning Courses in the world.Caffe:Caffe is a deep learning framework made with expression,speed, and modularity in mind. This repository consist of installation instructions and other recommended tutorials to help you learn this framework properly.Awesome Deep Learning:Heres a curated list of tutorials on Deep Learning which includes deep learning courses, free books, videos and lectures, papers and other useful resources to follow.Deep Learning in Python:Heres a complete tutorial on implementation of Deep Learning in PythonDeep Learning in Julia:Mocha is a Deep Learning framework for Julia. This tutorial follows a step by step methodology to be able to introduce this framework in the best possible manner.Recurrent Neural Networks:Heres a awesome list of dedicated resources for RNN. If you have longed to curate the resources for RNN, youve like to stop here and take a glance. This guide consists of codes, lectures, books and resources on multiple applications of RNN.Heres is a compiled list of most influential data scientists on Github to follow. These data scientists are experts in their respective field which ranges from python, machine learning, neural nets, data visualization, deep learning, data science etc.1. Sebastian Raschka    (Machine Learning, Data Visualization)2. Randy Olson         (Python  Data Analysis, Matplotlib, Bokeh)3.Hilary Mason        (ChiefData Scientists at Bitly)4.Mike Bostock        (D3, Data Visualisation)5. Prakhar Srivastav    (Python, Algorithms)6. Andreas Mueller     (Machine Learning, Python)7.Wes McKinney      (Author of Python for Data Analysis)8. Jake Vanderplas     (Machine Learning, Data Visualization)9. Mathieu Blondel     (Machine Learning, Neural Networks)10. Gael Varoquaux    (Machine Learning, Statistics, Python)11. Oliver Grisel        (Machine Learning, Deep Learning)12. Andrej             (Deep Learning, Neural Network, SVM)13. Micheal Nielsen    (Neural Networks, Deep Learning)14. Heather Arthur     (Neural Network, Javascript)15. Allen Downey      (Python, Algorithms)16. Davies Liu         (Apache Spark, Python)17. Julia Evans        (Machine Learning, Python)18. Jeff L              (R Programming, Data Analysis)19. John Myles White  (Julia, Machine Learning)20. Thomas Wiecki    (Python, Bayesian Analysis)21. Brian Caffo         (John Hopkins University)22. Roger D Peng      (John Hopkins University)23. Stefan Karpinski   (Julia)24. Pete Skomoroch    (Machine Learning, Big Data, Python)25. Mike Dewar       (Python, D3, Javascript)26. Hadley Wickham  (Statistics, Data Analysis, Data Visualisation)27. Romain Francois   (R Programming)28.Justin Palmer     (D3,Data Visualisation)29. Jason Davies      (D3, Data Visualization)30.Cameron Davidson Pilon   (Python, Algorithms)GitHub is not just about coding and sharing codes. Its utility extends to connecting with expertsand learn from them. The intent behind writing this article is to give you an overview of GitHub and its uses.In this article, I have displayed the list of top 30 data scientists to follow on GitHub. I have also list down some of the best tutorials I felt are awesome. I hope these repositories turns out to be useful for you!If you think, Ive missed out on any useful tutorial or data scientist, feel free to add them in the comments section below.",https://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/
"Lead/Manager  Equifax  Mumbai  (5-7 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Top Data Scientists to Follow & Best Data Science Tutorials on GitHub|CheatSheet: Data Exploration using Pandas in Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Lead/ManagerLocation  MumbaiAbout employer  EquifaxJob description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asLead/Manager  Equifax  Mumbai,https://www.analyticsvidhya.com/blog/2015/07/leadmanager-equifax-mumbai-5-7-years-experience/
CheatSheet: Data Exploration using Pandas in Python,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Lead/Manager  Equifax  Mumbai  (5-7 years of Experience)|Learning path for Tableau  visualization tool with awesome execution capabilities!|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If some one would ask me to mention 2 most important libraries in Python for data science, Ill probably name pandas and scikit-learn. Pandas for the capability to read datasets in DataFrames, exploring and making them ready for modeling / machine learning and Scikit-learn for actually learning from these features created in Pandas.While there are quite a few cheat sheets to summarize what scikit-learn brings to the table, there isnt one I have come across for Pandas. Hence, we thought of creating a cheat sheet for common data exploration operations in Python using Pandas. If you think we have missed any thing in the cheat sheet, please feel free to mention it in comments.The PDF version of the sheet can be downloaded from here (so that you can copy paste codes)You can keep this cheat sheet handy while performing data exploration.Download the PDF Version here.",https://www.analyticsvidhya.com/blog/2015/07/11-steps-perform-data-analysis-pandas-python/
Learning path for Tableau  visualization tool with awesome execution capabilities!,Learn everything about Analytics,"So, if you want to learn Tableau, just follow this awesome path created by Analytics Vidhya.||Share this:|Like this:|Related Articles|CheatSheet: Data Exploration using Pandas in Python|Analytics Consultant  MSBI  Marketelligent  Bangalore  (2-5 years of Experience)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let us take a look at Gartners Magic quadrant for Business Intelligence and Analytics platforms:If there is one company which stands out even in the leaders quadrant, it is Tableau! And it is so for a good reason. Here is what Gartner says about Tableau:Tableaus intuitive, visual-based data discovery capabilities have transformed business users expectations about what they can discover in data and share without extensive skills or training with a BI platform.It further says:Tableau has a strong position on the Ability to Execute axis of the Leaders quadrant, because of the companys successful land and expand strategy that has driven much of its growth momentum. Many of Gartners BI and analytics clients are seeing Tableau usage expand in their organizations and have had to adapt their strategy.Achieving this feat in a short period of existence (slightly more than 10 years) is nothing short of exemplary. It almost feels magical feat to achieve in a capital intensive industry. But Tableau has achieved it and you understand why the minute you start using it.The software is simple to use even for a person who does not have a technical background. It takes away a lot of burden from the end user and performs those task itself. For example, if you are working on a geo-spatial data, most of the tools will require you to map the data with latitude and longitude outside of the system. Not Tableau! It just asks you to input the name of the location and does all the hard work of mapping latitude and longitude itself.Given its increasing presence, it was only matter of time before we got a good learning path for Tableau on Analytics Vidhya. And today, we are happy to announce the same. Again, the philosophy of the learning path is same  free resources collated in a structure, which is easy to follow along with practice sets and exercises.",https://www.analyticsvidhya.com/blog/2015/07/learning-path-tableau-worlds-fastest-growing-visualization-tool/
"Analytics Consultant  MSBI  Marketelligent  Bangalore  (2-5 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Learning path for Tableau  visualization tool with awesome execution capabilities!|Must Watch Data Science Videos from SciPy Conference 2015|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Analytics Consultant  MSBILocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalytics Consultant  MSBI  Marketelligent",https://www.analyticsvidhya.com/blog/2015/07/analytics-consultant-msbi-marketelligent-bangalore-2-5-years-experience/
Must Watch Data Science Videos from SciPy Conference 2015,Learn everything about Analytics|Introduction|Lets warm up with the keynotes:|Machine Learning in Python|Data Visualization in Python|Data Mining in Python|Exploring Statistics in Python|Miscellaneous||End Notes,"A few things to note:|1. Keynote: State of Tools in Python|2. Keynote: My Data Journey with Python|3. Keynote: Data Science at the New York Times|1. Tutorial on Machine Learning with Scikit Learn  Part 1|2. Tutorial on Machine Learning with Scikit Learn  Part 2|3. Deep Learning Tips from the Road|4. Deploying Python Machine Learning Models in Production|5. Examining Malware (Kaggle Problem) with Python|6. Efficient Python for High Performance Parallel Computing|1. A Better Default ColorMap for Matplotlib|2. VisPy Harnessing The GPU For Fast, High Level Visualization|3. Building Python Data Apps with Blaze and Bokeh|4. Anatomy of Matplotlib|5. TrendVis, An Elegant Interface for Dense Sparkline Like Quantitative Visualizations|1. Image Analysis in Python with SciPy and Scikit Image|2. Analyzing and Manipulating Data with Pandas|3. Introduction to NumPy|4. Modern Optimization Methods in Python|5. Geospatial Data with Open Source Tools in Python|6. Eigenvector Spatial Filtering using NumPy and Arc GIS|7. DistArray Distributed Array Computing for Python|1. Computational Statistics in Python  Part I|2. Computational Statistics in Python  Part II|3. Statistical Thinking for Data Science|1. Jupyter Advanced Topics Tutorial|2. Accelerating Python with Numba JIT Complier|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Analytics Consultant  MSBI  Marketelligent  Bangalore  (2-5 years of Experience)|Getting into Top 10 in Kaggle Facebook Recruiting Competition|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",All the Pythonists in the data science community are in for a treat today!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This wasin my first year of engineeringdegree. A hungry, home-food sick student (me) was treated (by a college senior) with a lavish buffet in one of the best five star hotels in Mumbai! You get served with so many dishes that you struggle to decide where to start, what to taste and what to eat!Why is this relevant here? Well, I had a similar feeling when I looked at the videos from the recent SciPy conference.Scientific Computing with Python 2015, popularly known as SciPy 2015 conference, which was held recently at Austin, Texas, USA had a similar experience to offer.This 6 day conference showcasedthe best of research work happening in Python. There weresome fantastic tutorials on data science. The pain point is, if you check the YouTube channel, 115 videos have been uploaded which covers topic ranging from bio science, geo science, astro science and data science. We had to watch every video to come up with this recommended list.Duration- 45:06 minsSummary:In this video, Jake VanderPlas talks about the high level view of the SciPy ecosystem, covering topics including visualization tools like Matplotlib, Seaborn, Bokeh (which plots in HTML5), Data Structures and Arrays tools like xray, and some recent developments including Numba package, Anaconda, IPython and Jupyter and gives a history of how these tools developed over time.Duration- 47:45 minsSummary: Wes McKinney, father of Pandas, returns to SciPy and talks about his own journey and experience from 2007 to the present. He mentions how being a mathematician exposed him to Python. In brief, he talks about what hes personally focused on right now and what he sees as some of the opportunities for the community to continue to grow and flourish.Duration- 38:37 minsSummary: In this video, Chris Wiggins talks about his experience in Data Science being a Biologist and how he ended up with the New York Times, how New York Times used Data Science for its growth. He talks about what he does at the New York Times Data Science group and how Machine Learning plays a significant role in their company.Duration- 3:22:05 hrsSummary: If you have longed to learnMachine Learning in Python, heres the ever wanted cake for you. This tutorial covers various aspects of machine learning using scikit-learn. This tutorial has two parts and both are ~3 hours long. Thereby, promising convincingdissemination of knowledge. This tutorial has everything you need to know about fundamentals of machine learning in Python.Duration- 3:16:12 hrsSummary:This video starts where the previous one ends. In this part, complexed problems pertaining to model selection, cross validation are taken and solved using a step by step methodology. The best part is, a great emphasis has been laid on model building thereby covering a wide range of parameter selection, validation and testing followed by applying the algorithms as learnt in the previous video.Duration- 19:02 minsSummary: This video is sort of a crash course on Deep Learning. The speaker covers the major concepts and topics associated with deep learning. But, skips convolutional networks. However, this video aptly explains the concepts such as conditioning, back propagation, recurrence etc. If you are a beginner, keen to know about deep learning, you cant miss watching this.Duration- 19:06 minsSummary:The speaker aptly explains Production of machine learning models is sumup of deployment, evaluation, management and monitoring. He beautifully describes the use of Machine Learning in Production with a focus onoptimizing model performance.Duration- 21:45 minsSummary: Python tools for text classification can easily be used for malware classification. Phil Roth, data scientist shares his solution of Microsoft Malware Classification Challenge (2015) hosted by Kaggle. He discussed his approach, code and algorithms used to solve this dataset. He made use of Scikit learn for model building and managed to secure 29th rank in this competition.Duration- 3:23:50 hrsSummary: This video covers the essentials of code optimization by taking the example of Monte Carlo games. In 3 hours, the speaker touches upon every bit of code which is essential for optimization and debugging it simultaneously. This video is apt for intermediate and high level python practitioners. Beginners might skip it.Duration- 19:09 minSummary:Presenting the data is as important as producing it. In this video, you will be introduced with colormaps and how you can bring it to usevia matplotlib. This video gives a comprehensive overview of this feature. This video leaves you with enough information to be able to try this at your end.Duration- 18:25 minSummary: In this video, Luke Campagnola will introduce you with VisPy, a python library used for high level visualization. VisPy uses openGL to offload as much computation as possible. It uses more data, gives faster updates and takes lower CPU load. If visualization excites you, watch this and thanks me later!Duration- 3:20:19 hrsSummary:I came across this tutorial by Christine on social media even before the videos were out on the official channel and I knew I had to watch this. This is a SciPy tutorial on building data applications using Blaze and Bokeh. In these 3 hours, you will learn how to use Bokeh to build interactive data visualization for the browser. Also, you will learn Blaze which is used to perform interactive analytics queries for a variety of backends using the dataset from Berkeley Earth and Lahmann.Duration- 3:18:29 hrsSummary: Benjamin Root, one of the developers of matplotlib, briefly explains the concepts used behind matplotlib. He begins with the introduction, covers plotting functions and then moves to the more complex parts of this library, thereby, doing justice with the title anatomy of matplotlib. This is a good starter for beginnerskeen to use python for data visualization.Duration- 18:41 minsSummary: After learning about anatomy of matplotlib, this should be yournext watch. Melissa Cross, flawlessly explains the use of TrendVis (interface for quantitative visualization using matplotlib) by demonstrating it in a stepwise fashion. In 18 minutes, she aptly describes the use of this interface and how to implement it in Python.Duration- 3:03:03 hrsSummary:In this video, Stefan Van der Walt and his team give an introduction to how Scikit Image represents images, how it uses NumPy arrays to do so and what dimensions are involved in the data types etc. They talk about segmentation through interactive examples, image warping, feature detection and how to merge images together.Duration- 3:43:15 hrsSummary:In this video, Jonathan Rocher gives a tutorial on Data Analysis and Munging with Pandas. He shows how Pandas can make our life easier and work efficient in handling tabular data and how it can be used to perform some easy and some not so easy tasks that we do in Excel or in SQL.Duration- 2:42:52 hrsSummary: Eric Jones, in this video, provides a hands-on tutorial and introduction to NumPy in Python with some details on how it works. He touches on topics including NumPy Arrays, Slicing, etc. and shows how to do data visualization and data manipulation using NumPy library.Duration- 3:04:43 hrsSummary: Mike McKerns talks about Optimization methods in Python and about the modernization of these Optimization methods. He goes into the details of these Optimization techniques and shows how to implement them using Python.Duration- 3:03:30 hrsSummary: Kelsey Jordahl gives a tutorial on working with Geospatial data using Open Source tools in Python in this video. This tutorial also provides some hands-on examples and exercises for practice. He emphasizes on the Pythonic ways of working with the Geospatial data by interfacing with NumPy and SciPy stack.Duration- 18:53 minsSummary: In this video, Bryan Chastain talks about Spatial Statistics and Eigenvector Spatial Filtering using NumPy. He talks about techniques for controlling Spatial Autocorrelation like Geographically Weighted Regression, Spatial Weights Matrix, etc. and also compares the processing time with R.Duration- 17:28 minsSummary:Robert Grant talks about Distributed Array Computation in Python, how he and his team built DistArray using the existing libraries in Python like NumPy, IPython Parallel, mpi4py, etc. He shows how DistArray works and how we can perform some manipulation on data using it.Duration- 1:43:07 hrsSummary: In this video, Allen Downey gives a talk on Computational Statistics and Statistical Inference. He explains the concept in three parts, estimating and describing the Effect Size, Quantifying Precision and Hypothesis testing using Python. He uses the libraries SciPy, NumPy and Pandas in his tutorial.Duration- 3:01:25 hrsSummary:This video is a follow up to the previous video Computational Statistics I and in it, Chris Fonnesbeck extends the concept a little with a different objective  some useful computational tools that would allow us to build statistical models competently. This is kind of a useful survey of the statistical tools motivated by lack of utility of stuff we learnt in an Undergraduate/graduate Statistics Program.Duration- 23:50 minsSummary: Chris Fonnesbeck, in this video, talks about issues related to Data Science that as a Statistician, he thinks are important for its success as an emerging new field. He goes into showing us some statistics from the history and also walks us through other statistical concepts and data around us.Duration- 2:48:53 hrsSummary: This video talks about some advanced tools and topics related to Python. Jonathoan Frederic talks about advanced topics in Jupyter and gives some hands-on tutorials for the same. He uses tools like jQuery, CSS, SciPy in the tutorials. The video also contains the talk by Matthias Bussonier who further elaborates on the topic.Duration- 20:22 minsSummary: This video showcases Stanley Seibert talking about the Open Source NumPy-aware optimizing compiler for Python called Numba. He compares Numba with other compilers and goes through various features of Numba and how Numba works. He also talks about some basics of Numba and how to use it.In this article, we have listedthe list of data science videos from SciPy Conference 2015. We found these videos enrichingin their respective subjects and realized that they can be of help for you as well.In case we have missed out on any useful video from SciPy videos playlist, feel free to enlist them in the comments section below.",https://www.analyticsvidhya.com/blog/2015/07/data-science-videos-scipy-2015/
Getting into Top 10 in Kaggle Facebook Recruiting Competition,Learn everything about Analytics|Understanding the Data|Getting started|Results of initial exploratory analysis|Finding the Significant Variables|Developing a simplistic model|Here is mysecret sauce,"List of initial hypothesis:||End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Must Watch Data Science Videos from SciPy Conference 2015|Simple infographic to help you compete in Data Science Competitions!|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Facebook recently wrapped up its Recruitment competition on Kaggle. This was by far the richest data I have seen on Kaggle. The amount of information which was available in this competition was simply beyond imagination. Here is what the problem statement looked like :Interesting enough? If not, here is what will make you jump on the table : You get the Bid level interaction data of these bidders for a significant time to identify who among the bidders is bot. I performed reasonably well on the public board, however the private board did no justice to my entry on Kaggle. Here was my final standing on the Public board :Even though I did not win the competition, I thought it will be interesting to share my approach in public.The data is primarily composed of two tables. One is a simple list of all bidders and the second one contains all the bids made by these bidders. Here are the details of the variables in these two tables :By combining the two datasets on Bidder ID, imagine the richness of this data.To do any kind of prediction and apply machine learning, we need to summarize the data on Bidder ID level. This is because bidder and not bids can be bots. Hence, target variable is defined on bidder level. The simplest way to get do this is simply roll up the bid level information and get the features right away. This was the quick and dirty approach I took at the first go.Here arethe top hypothesis which I had generated before starting with the problem. This approachclearlyhelped me in identifying bots among the bidders :However, it is purely upto your imagination what variables do you want to create while summarizing it. I created following variables for my model :Here are a few insights you will find in the initial analysisHere are a few interesting plots which made things simpler on the modeling side :1. Density plot for mean response time : As you can see from below chart, Robots have some fixed time lag between their bids. However, humans take non-regular time between bids. This looks very intuitive as robots will be programmed to bid and hence show a common pattern.2.Average number of bids : Probability that a bidder is human increases as the number of total bids increase.Also Read  Solution to Kaggle Bike Sharing Demand PredictionFinding variables which are able to distinguish humans from robots is the first thing you will do before building any kind of model. Here is a simple way to do this :Here, what we simply do is find the mean value of each parameter for both humans and Robots and see if they look different. There are many other ways to do the same thing, however this is the simplest of them all the understand. As you can see from the table, most of the variable come out to be significant.Now, once we have a list of significant variables, we simply need to plug them in a machine learning algorithm and check its accuracy with k-folds. Ive used Gradient Boosting to do this exercise. Here is a simple function which you can use to evaluate your K-Folds accuracy by feeding in any number of variables.For using this function simply use following statementCompare the performance of different models and once you have the final selected variables use it with entire dataset.For Beginners: How to choose the right Kaggle competition?I soon realized the the CV for different seeds of the same model is varying too much because we train the model on merely 2000 datapoints. So, I took a shot in experimenting over the problem with a non-traditional approach. Here is what I did :Broke down the bidder IDs with a constant duration of time. For instance if I have time varying from 1 to 100, I divide this time into 10 time points. Now I roll up the bidders using their bidder id and the time point. The assumption being that the observations will be still independent of each other. For instance, I (as a human) might be bidding very aggressively in 2010 but playing safe in 2012. Now I use the same bidder twice with outcome flag as 0 (human).With this manipulation, I was able to raise the number of observations to around 10,000 and was able to make a CV which gave consistently the same score (which was 0.02% lower than Kaggle leadership board).Kaggle Trick: Simple framework to crack a Kaggle ProblemMy model performed similar on Public and Private scoreboard of Kaggle ( 0.92435 on Public board and 0.92680 on Private board). I think the model looks stable enough and not a case of over fitting.However the winner of the competition had the private score far better than the public score (0.91946on Public board and 0.94254 on Private board).If the two samples were similar (Private and Public), is this not a case of unstable model. I reserve my doubts on this issue but will like to encourage people to give their view point on this.Let me know your thoughts on this.Have you participated in any Kaggle problem? Did you see any significant benefits by doing the same? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/07/top-10-kaggle-fb-recruiting-competition/
Simple infographic to help you compete in Data Science Competitions!,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics / data science learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Getting into Top 10 in Kaggle Facebook Recruiting Competition|Comprehensive Guide to Data Visualization in R|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are only 2 possible outcomes for every serious participant in a data science competition  you either win it or you learn from it! Nothing beats the time bound competition to achieve the best with whatever you have or know!We have conducted multiple hackathons (Hackathon 1, Hackathon 2) in an attempt to develop data science community in last couple of months. We found that a lot of people have unreasonable inhibitions stopping them from participating in these competitions. It is difficult to explain them that starting is more important than winning or doing well in a competition!One of the consistent feedback we receive from a lot of aspiring data scientists is that there is no structured path to help them understand where to start in these competitions. Hence, we thought of creating an infographic, which addresses this problem directly. (Aspiring) Data Scientists can take a print on this and keep this in front of their eyes. We sincerely hope that this infographic will help us bring more and more people in the ecosystem. It provides a list of competitions and a few common hacks to get started in these competitions.
If you need any more help in getting started with these data science competitions, feel free to pen in your comments below or ask the wider community.",https://www.analyticsvidhya.com/blog/2015/07/infographic-guide-win-data-science-competitions/
Comprehensive Guide to Data Visualization in R,Learn everything about Analytics|Brief History of Data Visualization:|Data Visualization in R:||BASIC VISUALIZATIONS|Advanced Visualizations|End Notes,"1. Histogram|2. Bar/ Line Chart|3. Box Plot ( including group-by option )|4. Scatter Plot (including 3D and other features)|What is Hexbin Binning ?|Mosaic Plot|Heat Map|How to summarize lots of data ?|Map Visualization|3D Graphs|Correlogram (GUIs)|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Simple infographic to help you compete in Data Science Competitions!|Online Hackathon: Predict the gem of Auxesia for Magazino!|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Line Chart|Bar Chart,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let us look at this chart for a second,This visualization (originally created using Tableau) is a great example of how data visualization can help decision makers. Imagine telling this information to an investor through a table. How long do you think you will take to explain it to him?With ever increasing volume of data in todays world, it is impossible to tell stories without these visualizations. While there are dedicated tools like Tableau, QlikView and d3.js, nothing can replace a modeling / statistics tools with good visualization capability. It helps tremendously in doing any exploratory data analysis as well as feature engineering. This is where R offers incredible help.R Programming offers a satisfactory set of inbuilt function and libraries (such as ggplot2, leaflet, lattice) to build visualizations and present data. In this article, I have covered the steps to create the common as well as advanced visualizations in R Programming. But, before we come to them, let us quickly look at brief history of data visualization. If you are not interested in history, you can safely skip to the next section.Historically, data visualization has evolved through the work of noted practitioners. The founder of graphical methods in statistics is William Playfair. William Playfair invented four types of graphs: the line graph, the bar chart of economic data , the pie chart and the circle graph. Joseph Priestly had created the innovation of the first timeline charts, in which individual bars were used to visualize the life span of a person (1765). Thats right timelines were invented 250 years and not by Facebook!Among the most famous early data visualizations is Napoleons March as depicted by Charles Minard. The data visualization packs in extensive information on the effect of temperature on Napoleons invasion of Russia along with time scales. The graphic is notable for its representation in two dimensions of six types of data: the number of Napoleons troops; distance; temperature; the latitude and longitude; direction of travel; and location relative to specific datesFlorence Nightangle was also a pioneer in data visulaization. She drew coxcomb charts for depicting effect of disease on troop mortality (1858).The use of maps in graphs or spatial analytics was pioneered by John Snow ( not from the Game of Thrones!). It was map of deaths from a cholera outbreak in London, 1854, in relation to the locations of public water pumps and it helped pinpoint the outbreak to a single pump.In this article, we will create the following visualizations:Basic VisualizationAdvanced VisualizationR tip: The HistData package provides a collection of small data sets that are interesting and important in the history of statistics and data visualization.Quick Notes:Histogramis basically a plot that breaks the data into bins (or breaks) and shows frequency distribution of these bins. You can change the breaks also and see the effect it has data visualization in terms of understandability.Let me give you an example.Note: We have used par(mfrow=c(2,5)) command to fit multiple graphs in same page for sake of clarity( see the code below).The following commands show this in a better way. In the code below, the main option sets the Title of Graph and the col option calls in the color pallete from RColorBrewer to set the colors.Notice, if number of breaks is less than number of colors specified, the colors just go to extreme values as in the Set 3 8 colors graph. If number of breaks is more than number of colors, the colors start repeating as in the first row.Below is the line chart showing the increase in air passengers over given time period. Line Charts are commonly preferred when we are to analyse a trend spread over a time period. Furthermore, line plot is also suitable to plots where we need to compare relative changes in quantities across some variable (like time). Below is the code:Bar Plots are suitable for showing comparison between cumulative totals across several groups. Stacked Plots are used for bar plots for various categories. Heres the code:Box Plot shows 5 statistically significant numbers- the minimum, the 25th percentile, the median, the 75th percentile and the maximum. It is thus useful for visualizing the spread of the data is and derivinginferences accordingly. Heres the basic code:Lets understand the code below:In the example below,I have made 4 graphs in one screen.By using the ~ sign, I canvisualizehow the spread (of Sepal Length) is across various categories ( of Species). In the last two graphs I have shownthe example of color palettes. A color palette is a group of colors that is used to make the graph more appealing and helping create visual distinctions in the data.To learn more about the use of color palletes in R, visit here.Scatter plots help in visualizing data easily and for simple data inspection. Heres the code for simple scatter and multivariate scatter plot:Scatter Plot Matrix can help visualize multiple variables across each other.You might be thinkingthat I have not put pie charts in the list of basic charts. Thats intentional, not a miss out. This is because data visualization professionals frown on the usage of pie charts to represent data. This is because of the human eye cannot visualize circular distances as accurately as linear distance. Simply put- anything that can be put in a pie chart is better represented as a line graph. However, if you like pie-chart, use:Herea a complied list of all the graphs that we have learnttill here:You might have noticed that in some of the charts, their titles have gottruncated because Ive pushed too many graphs into same screen. For changing that, you can just change the mfrow parameter for par .We can use the hexbin package in case we have multiple points in the same place (overplotting). Hexagon binning is a form of bivariate histogram useful for visualizing the structure in datasets with large n. Heres the code:We can alsocreate a color palette and then use the hexbin plot function for a better visual effect. Heres the code:A mosaic plot can be used for plotting categorical data very effectively with the area of the data showing the relative proportions.Heat maps enable you to do exploratory data analysis with two dimensions as the axis and the third dimension shown by intensity of color. However you need to convert the dataset to a matrix format. Heres the code:You can use image() command also for this type of visualization as:You can use tableplot function from the tabplot package to quickly summarize a lot of dataThe latest thing in R is data visualization through Javascript libraries. Leaflet is one of the most popular open-source JavaScript libraries for interactive maps. It is based at https://rstudio.github.io/leaflet/You can install it straight from github using:The code for the above map is quite simple:One of the easiest ways of impressing someone byRs capabilities is by creating a3D graph in R without writing ANY line of code and within 3 minutes. Is it too much to ask for?We use the package R Commander which acts as Graphical User Interface (GUI). Here are the steps:The code below is not typed by the user but automatically generated.Note:When we interchange the graph axes, you should see graphs with the respective code how we pass axis labels using xlab, ylab, and the graph title using Main and color using the col parameter.You can also make 3D graphs using Lattice package . Lattice can also be used for xyplots. Heres the code:
Correlogramhelp us visualize the data in correlation matrices. Heres the code:There are three principal GUI packages in R. RCommander with KMggplots, Rattle for data mining and Deducer for Data Visualization. These help to automate many tasks.I really enjoyed writing about the article and the various ways R makes it the best data visualization software in the world. While Python may make progress with seaborn and ggplot nothing beats the sheer immense number of packages in R for statistical data visualization.In this article, I have discussed various forms of visualization by covering the basic to advanced levels of charts & graphs useful to display the data using R Programming.Did you find this article useful ? Do let me know your suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/07/guide-data-visualization-r/
Online Hackathon: Predict the gem of Auxesia for Magazino!,Learn everything about Analytics|Brief about Magazino|What is the gem of Auxesia?|Solution Checker:|Deadline:,"You can download the dataset and learn about its attribute on our discussion portal. You will need to drop a message and login to participate.|Share this:|Like this:|Related Articles|Comprehensive Guide to Data Visualization in R|Getting started with Julia  a high level, high performance language for computing|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Just wanted to announce the launch of online hackathon. We have got an exciting problem for our publishing friend Magazino!Magazino, was started by a few technoprenuers about a year back, who wanted to bring data science to publishing. They believed that current media houses are not doing justice to what should be served to people and with use of Big Data they can get a fair differentiation in their offering.In the last one year, Magazino has done a lot of experimentation with publishing several topics and content. They have made sure that they capture all the data at the back end. Now, they want to put a bit of science to predict which articles get shared on social media, which ones dont! To that extent, they have stored social media performance of ~24,875 articles they have published till date along various attributes.Auxesia is the goddess of growth and prosperity in Greek mythology! Finding articles which can go viral in todays age is like being blessed with a gem from Auxesia. So, you have to find out these gems for Magazino!The evaluating criteria for this problem would be Root Mean Square error (RMSE). The URL of the solution checker is:http://www.datahack.club:8000/You can also access the leaderboard on http://www.datahack.club:8000/learderboardThe contest ends at midnight of 12  13 July, Indian Standard Time (GMT + 5:30). To submit the solution, you need to send an email to [emailprotected]Looking forward to see you in the contest!",https://www.analyticsvidhya.com/blog/2015/07/online-hackathon-predict-gem-auxesia-magazino/
"Getting started with Julia  a high level, high performance language for computing",Learn everything about Analytics|What is Julia?|Why another programming language?|A Summary of Features in Julia|Installation of Julia|A few important packages|How to install & use a package?|The Julia ecosystem:|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Online Hackathon: Predict the gem of Auxesia for Magazino!|My playlist  Top YouTube Videos on Machine Learning, Neural Network & Deep Learning|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Learning new tools and techniques in data science is sort of like running on treadmill  you have to run continuously to stay on top of it. The minute you stop, you start falling behind.As part of this learning, I continuously look out for new developments happening in new tools and techniques.It was in this desire to continuously learn that I came across Julia about a year back. It was in very early stages then  it still is!But, there is something special about Julia, which makes it a compelling tool to learn for all future data scientists. So, I thought to write a fewarticles on it. This is first of these articles, which provides the motivation to learn Julia, its installation, current packages available and ways to become part of Julia community.Julia is a high-level, high-performance dynamic programming language for technical computing, with easy to write syntax. It provides a sophisticated compiler,distributed parallel execution, numerical accuracy, and anextensive mathematical function library.The simplest way to understand its power is to think of it as a language which has a wide range of statistical packages like R, it is easy to write and learn like Python and has execution speed similar to C / C++.If you are still not convinced about what I have mentioned, have a look at benchmarks of a few common benchmarks below:
C compiled by gcc 4.8.2, taking best timing from all optimization levels (-O0 through -O3). C, Fortran and Julia useOpenBLASv0.2.12. The Python implementations ofrand_mat_statandrand_mat_muluse NumPy (v1.8.2) functions; the rest are pure Python implementations.Some of the important features to highlight from data science capabilities are:A more comprehensive list of features can be accessedhereNow that you might be raring to give Julia a try for all the promises made above, let me quickly walk through various options to test drive your new sedan (which has sports car like acceleration):The installation was pretty simple and straight forward. I have tried Juliabox as well as Juno. Option 1 and 2come with a few demo examples before hand. You can just follow the comments (starting with #) to understand and give the code a test run.There are a total of 610 packages on Julia as on date (9th July 2015). If you filter out packages for which tests have failed or which have not been tested, you are only left with 381 packages. Among these I have filtered out the ones related to data science and have more than 15 stars. That leaves us with the following packages:P.S. There is a lot of development happening on the language and the libraries. So this can change very quickly.A few things to note:Installing and using a package in Julia is dead simple. If you want to install / add a package, simply type this in your programming interfaceThis will install the package as well as its dependencies.Once the package is installed, you can load it simply by calling usingSimple!Julia is supported by a close knit community of developers. Here are a few mailing lists, you can be a part of:In addition to these newsletter, you can also look at juliabloggers.com . The site looks like a developing ecosystem as of now though.I hope that you have got a good overview of this powerful language under development. I was pretty excited when I saw it first and I continue to check this language for new developments closely. In the next articles to come, we will understand the data structured available in Julia, its interface with other languages e.g. Python and solve one of the case studies using Julia to understand its power.What do you think of Julia? Are you all set to give it a try? Does the future excite you? Do let us know your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/07/julia-language-getting-started/
"My playlist  Top YouTube Videos on Machine Learning, Neural Network & Deep Learning",Learn everything about Analytics|Introduction|How should you watch them ?|Videos related toMachine Learning|Videos related to Neural Networks|Videos related to Deep Learning|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Getting started with Julia  a high level, high performance language for computing|The guide to quickly learn Cloud Computing in R Programming|
Analytics Vidhya Content Team
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the best way to get better at machine learning and deep learning is to watch a lecture from an expert and work your way along with it. If you do so, you get the best of both the worlds  you learn from the experts across the globe and also get hands on knowledge.In this article, I have provided a list of YouTube videos, which you can use to improve your knowledge in these areas.Youve got to follow a ritual (Just Kidding!). For your ease, I have created a to be followed sequence / order of these videos. I have categorized the videos into: Machine Learning, Neural Networks and Deep Learning. If you are new to this, Id recommend you to follow the sequence for better understanding. If not, feel free to follow your own route and let me know, how it turns out.If you feel that the list is overwhelming, take one byte at a time! Consider this as a collection of references to be consumed over time.Thesevideos range from a few minutes to hour long videos. For your convenience, I have also mentioned the summary against each video for the overview purpose.Go ahead and check them out.1.The Future of Robotics and Artificial Intelligence (Andrew Ng, Stanford University)Duration- 16:26minsSummary: What better way to start this journey than to hear from one of the best machine learning teacher & expert across the globe.In this video, Andrew Ng talks about his childhood dream to build robots that can actually think and work like humans and improve the lives of millions. He talks about the similarity in human brain and the software in machines that can make them function like humans.2.Lecture Series byAndrew Ng (Machine Learning)Duration- N/ASummary: This is the complete playlist for the lectures on Stanford Machine Learning (CS229) by Prof. Andrew Ng at Stanford. I personally think this is better than the coursera classes and enjoyed this thoroughly.In these lectures he covers machine learning concepts which include Linear / Logistic regression, supervised and unsupervised learning, learning theory, reinforcement learning and adaptive control. He discusses techniques like Naive Bayes, Neural Networks, SVM, Bayesian statistics, Regularization, Clustering, PCA and ICA. He also discusses some recent applications of Machine learning such as robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing.If you are a complete newbie and want a lucid introduction, which challenges you at times, go with these videos3. Learning from Data  CaltechDuration-NASummary:In this Caltechs Machine Learning Course  CS 156, Professor Yaser Abu-Mostafa gives in-depth details of various machine learning concepts and techniques. This course is very heavy on mathematics and the theory behind Machine Learning with a bit difficult programming exercises. It balances theory and practice, and covers the mathematical as well as the heuristic aspects. The lectures follow each other in a story-like fashion. You should do this course for its challenging assignments. This lecture series has 18 videos.4.Using Python to Code by VoiceDuration- 28:16minsSummary:In this awesome video, Tavis Rudd talks about his two-year long journey which led him to create this cool feature of coding using Voice Recognition. He did a lot of vocab tweaking and duct-tape coding in Python and Emacs Lisp to develop this system which enables him to code a lot faster. He gives a live demo of the software that he has developed and makes his system do things in seconds through his voice which otherwise may take hours to code!5.Python-Powered Machine Learning in the CloudDuration- 18:00minsSummary:In this video, Mr. Stephen Hoover talks about a cloud based data science platform for data analysis built using Python at his company Civis Analytics which helps analysts to perform their work much faster and with much little effort. He talks about various machine learning aspects of the platform and talks about some open source libraries in Python which help in data analysis like Pandas, NumPy and Scikit-Learn.If you have survived the journey till now, Congratulations! Have a look at the next exciting video and you will be all prepped up for the next 2 sections, i.e. Neural network and deep learning! The fun doubles up, if you were a Mario fan as a kid.6.MarI/O  Machine Learning for Video GamesDuration- 06:00minsSummary: This video shows how a computer program called MarI/O learns to play Super Mario game. This program is made of Neural Networks and Genetic Algorithms. The video showcases the actual biological evolution of the program compared to the human brain. This is one of the awesome applications of Machine Learning which demonstrates the scope of Machine Learning in various activities that humans can perform.1.Getting started with Neural NetworksDuration- N/ASummary: Heres a playlist known by Neural Network Class. It covers the basic to advanced level concepts of neural network ranging from artificial neuron, activation function to recursive network training. You wont find long duration videos in this playlist. The videos are short and crisp with the longest duration of 24mins. Id recommend this to every beginner learningneural networks.2.Neural Network Training Part 1: The Training ProcessDuration- 12:40 minsSummary: This series of video teaches how to train a neural network i.e. neural network training. The way of explaining is really good. This particular video givesthe overview of completetraining process. Once you click on this video, in the Up Next section on YouTube, youll find its subsequent videos covering topics such as neural network error calculation, gradient calculation etc which are indeed helpful.3.Introduction to Artificial Neural NetworksDuration- 54:00minsSummary:ANN exploits the non linearity. It assists the process of input-output mapping says Professor S. Sengupta, IIT Kharagpur, India. He flawlessly explains the concept of artificial neural network in the simplest possible manner. He explains using a pen and paper which actually helped me to understandthese concepts. Towards the end of this video, he touchesupon the applications of ANN. Dont miss out the Up Next section.4.Visualizing and Understanding Deep Neural Networks by Matt ZeilerDuration- 47:40minsSummary:Convolutional neural networks are used to recognize objects, images and videos. In this 47 minute video, youll be introduced with the concept of deconvolutional network followed, the insights for architecture selection in the convolutional networks. The role of visualization is to presentan insights on the performance of each layer using which improvements can be made.5.The Next Generation of Neural NetworksDuration- 1:02:01Summary:A person who needs no introduction,Geoffery Hinton, gives an enriching talk on neural network at GoogleTechTalks. This video will help you to build a solid foundation in deep machine learning. It will also take you through the journey of neural networks from the past to future. Geoff cover topics such as back propagation, digit recognition, Restricted Boltzmann Machine and related topics.6.Neural Bots  Evolving Artificial IntelligenceDuration- 04:40minsSummary: This short video explains artificial intelligence using evolving neural networks by using a designed program Neural Bots. The complete activity of the bots have been visualized using apre-defined set of commands. This will be fun to watch.7.Speech Recognition Breakthrough for the Spoken, Translated WordDuration- 09:04 minsSummary: Uploaded by Microsoft Research, this video is a small talk given by Chief Research Officer, Rick Rashid. Rick demonstrates a speech recognition breakthrough using deep neural networks & machine translation which converts spoken english to chinese languages, simultaneously,reducing the amount of recurring errors at every layer.8.Learning from Bacteria about Social NetworksDuration- 1:04:03hrsSummary: This videos covers the unconventional topic of learning from bacteria about information processing. The speaker begins with the part of rudimentary intelligence which involves cognition, sensing, processing. He also shows the pattern of rethinking bacteria. Finally, the use of social networks can be seen driven by chemical tweeting.9.Genetic algorithm. Learning to jump over ballDuration- 03:00minsSummary:The title of the video is clear enough to explain its content. This video demonstrates the complete process of a gene which learn to jump over the ball.10.A genetic algorithm learns how to fight!Duration- 02:15minsSummary: Just like previous video, it also emphasizes upon the applications and wide range of implementation of neural networks. In this video, a genetic algorithm learn how to fight. Such videos triggered my appetite to learn as I realized the unscalable potential of neural networks.1.Introduction to Deep Learning with PythonDuration- 52:40 minsSummary: This video teaches the implementation of Deep Learning in Python. It begins with a motivating problem of handwritten digit recognition. It also demonstrates the complete codes of python used to solve a dataset based on 60,000 images. The speaker then emphasizesupon the codesand makes sure that he doesnt miss out on explaining the important set of codes and algorithms.2.Intro to Deep Learning with Theano and OpenDeep by Markus BeissingerDuration- 1:09:04 minsSummary: This video gives a good start to understand the concepts of Deep Learning. Markus begins this talk by explaining the story behinddeep learning. Then he gives a quick refresher on linear algebra which is followed by basic neural networks, unsupervised models and RNN-GSN. Later, the explains how to implement simple neural networks in Python using Theano.3.Deep Learning: Intelligence from Big DataDuration- 1:24:06 minsSummary: This talk will introduce you to a new concept which integrates deep learning and big data. Deep Learning has began to derive significant value from big data. At the later half of this video, youll find a useful discussion happening among the research scientists from Google, Facebook and other big giants. Their discussion covers most of the elements of deep learning and big data which are essential to drive its future growth.4.Deep Learning for Computer Vision (Rob Fergus)Duration- 2:00:04 minsSummary: This video got published less than a week back. This is the first tutorial I found on computer vision. This tutorials explains the concepts such as (spatial pooling), normalization, image net classification etc. Towards the end, various amazing applications have been displayed using a collection of useful images.5.Convolutional Neural NetworksDuration- 50:30 minsSummary:The Computer Science Department of University of Oxford released this tutorial few months back. This is by far one of the most sought video on convnets. The speaker discusses the concepts of using convnets for object recognition and language, how to design convolutional layers, how to design pooling layers. In the later half of the video, he discusses the process to build convnets in Torch.6.Unsupervised Feature Learning and Deep Learning  Andrew NgDuration- 48:20 minsSummary: This talk is given by Andrew Ng, Founder of Coursera to address the development of unsupervised feature learning and deep learning that can automatically feature representations from featured data. In this talk, Andrew describes the useful concepts behind unsupervised feature learning and deep learning, describes few algorithms and presents a pertaining case study.7.Geoff Hinton: Recent Developments in Deep LearningDuration- 1:05:20 minsSummary: Geoff Hinton, a pioneer in Machine Learning talks about the recent developments in Deep Learning in this video. Laying emphasis on the mathematical aspects of various algorithms, he talks about tasks such as object recognition, information retrieval, and modeling motion capture data in which Deep networks have been quite successful.8.Interview with Googles AI and Deep Learning Godfather Geoffrey HintonDuration- 27:30minsSummary: This is an audio version of interview with Geoffrey Hinton. In this interview, he describes how does google implements artificial intelligence system. Also, he emphasizes on the learning component of humans, and machines using neural nets.This is a must watch(listen rather) for every machine learning enthusiast.9.Learning Representations: A Challenge for Learning TheoryDuration- 54:31 minsSummary:Yann LeCun from the Computer Science Department of the NYU talks about things where Learning Theory is difficult to apply to and presents it as a challenge for the community to study it. He talks about various Deep Learning concepts and his interest in Learning Representations in particular which he thinks is probably the next step for AI Machine Learning.10.How Deep Learning Will Enable Self Driving CarsDuration- 1:05:30 hrsSummary:This video from the Deep Learning expert at NVIDIA, Mike Houston, talks about the Deep Learning training system called NVIDIA DIGITS, along withthe NVIDIA DRIVE PX car computerwork in enabling cars to drive themselves. He talks about the training tools and platforms that his team uses for building these self-driven cars along with the Deep Learning algorithms which they use for the same.11.Deep Learning for Decision Making and ControlDuration- 56:02 minsSummary:In this video, Sergey Levine is a postdoctoral researcher working with Professor Pieter Abbeel at UC Berkeley talks about the applications of Deep learning in Decision Making and Control. He focusses on applications like Continuous Control Tasks and some other broader applications at the end. He also describes the algorithms that tackle these challenges using Supervised Learning.12.Large-Scale Deep Learning for Building Intelligent Computer SystemsDuration- 1:00:23 minsSummary:Jeff Dean, senior fellow, Google Knowledge Group talks about how we can build more intelligent computer systems using Neural Networks and Deep Learning. He focusses on the abilities of computer systems such as basic speech and vision, language understanding and user behavior prediction and how has he applied all these techniques at Google in its various products.Hope you find this list useful. As mentioned before, it may look overwhelming for starters  take one step at a time and code along side. If you have suggestions for videos, which should be added to this list, please feel free to add them here. I will love to watch them!",https://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/
The guide to quickly learn Cloud Computing in R Programming,Learn everything about Analytics|Introduction,"Download the high resolutionPDF Version available: Download PDF Version|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|My playlist  Top YouTube Videos on Machine Learning, Neural Network & Deep Learning|Consultant Analytics  Equifax  Mumbai  (1.5  3 years of experience)|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Cloud computing is becoming a natural extension for problems / data sets bigger than what laptops and desktops can process. However, for complete starters, getting started on cloud computing platform can look more difficult than what it is.Below in an infographic displayingthe concept of cloud computing, its importance and setup using R programming and RStudio. Since, we term it as aquick mode for learning this concept, chances are you might miss out on conceptual explanations. Yet, not to worry, you can check out the full guide on How to get started with Cloud Computing in R?Keep this guide handy and save it on your desktop.",https://www.analyticsvidhya.com/blog/2015/07/guide-quickly-learn-cloud-computing-programming/
Consultant Analytics  Equifax  Mumbai  (1.5  3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|The guide to quickly learn Cloud Computing in R Programming|Must for Data Scientists & Analysts: Brain Training for Analytical Thinking|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Consultant AnalyticsLocation  MumbaiAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant Analytics_Off shore  Equifax  MumbaiIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/07/consultant-analytics_off-shore-equifax-mumbai-1-6-3-years-experience/
Must for Data Scientists & Analysts: Brain Training for Analytical Thinking,Learn everything about Analytics|Introduction|What youll learn ?|Practice Problem|End Notes,"To explain you better, consider asimplistic scenario:|Lets try to generalize it into a parametric equation|Lets assume a few parameters and understand the equation further:|Lets try to visualize a few relations|Lets summarize our findings|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant Analytics  Equifax  Mumbai  (1.5  3 years of experience)|Learning Path : Best way to learn Machine Learning in 6 easy steps|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Lets make it more complex!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lets start this article with a small exercise. Take a pen and paper and write the answer as it comes to your mind. No thinking twice and you shouldnt take more than 15 seconds to do it.On this paper, please write the answer toWhat are the skills required tobecome a successful data scientist?A lot of you would have written coding, knowledge of analytics tools, statistics etc. All of these are definitely required to be a successful data scientist, but they are not sufficient.One of the most important skill differentiating a good analyst / data scientist from the bad one is the ability to take complex problems, put a framework around it, make simplifying assumptions, analyze the problemand then come up with solutions. And analytics tools are just a medium to do so.In todays article we will takea case study and see this process of problem solving in structured manner.Here youll find practice problems to train your brain think analytically while solving complex problems. This brain training will not only introduce you to a new approach to solve problems but will also help you to think faster while dealing with numbers!My previous article on how to train your mind for analytical thinking?should give you a good head start.Heres is my daily routine:I get ready and leave home for officeat sharp 10:30 AM every working day. Considering the amount of work I got to finish on some days, I try to reach early by driving faster than other days (obviously in safe limits).However, sincelast 5 days, Ive observed that I reach office almost at the same time, irrespective ofmy average speed between traffic lights. This makes me wonder, whether the time taken from my home to office is dependent on my velocity or not? In other words, the total average velocity adjusted by the traffic lights to the same level, and does not depend on the velocity we drive the car!Take the Test: Should I become a Data Scientist ?Two cars start from point A which is the first traffic signal. Point B is a traffic signal with a halt time of 60 sec and drive time of 20 sec. The distance between A and B is 600m. Car1 startsat 5m/sec and Car2 starts at 6m/sec. Who will cross the traffic light first?Here are the assumptions:1. Traffic lights are configured for average speeds, it becomes green 120 seconds (600 m / 5 m/sec) after the first signal turns green.2. Traffic lights are green for 20 seconds and red for 60 seconds (20 * 3)Assume both cars start at 0 sec.Hence, cars reaching point B in 61 sec and one reaching at 140 second show no difference in terms of passing through the second signal. Lets calculate the min and max speeds which will show no difference amongst thetwo lights scenario :It does not matter whether you drive at 18 km/hr or 35 km/hr in this scenario, you will cross the second signal (B) at the same time. In general, it is difficult to drive in such wide range of speeds in peak time traffic and hence my concerns looks logical now. I probably have no control on the time I will take to reach office (obviously this is over simplification of the problem).Now we have 4 signals A,B,C and D. Same two cars start from A at the time 0 sec. Distances between AB , BC and CD are same. The question is now, who will cross the signal D first.Without going into mathematics, the answer is very straight forward. If both will cross B at the same time, A  B pair is the same as B-C pair which is in turn same as C-D pair. Hence both the car will cross D at the same time. The scenario is actually more extreme, the car which maintains an average speed of 18 km/hr and the one at 35 km/hr will cross D at the same time. This further strengthens my hypothesis.Question again boils down to :Am I just a helpless puppet in traffic polices hand while driving to myoffice ? Actual scenario is too difficult to generalize in this article, so lets ground a few assumptions :1. Traffic lights turn green for time t sec and becomes red for time 3t sec2. Average speed of a vehicle on road is v m/sec3. The challenger to the average vehicle drives at a velocity x times v m/sec4. All roads have a length of l metersBy now, we already know, it hardly matters if we solve for one pair of traffic light or more. If the faster driver is able to sneak through the traffic light in a green signal before the average vehicle, it will make a difference or else not.Hence, the difference in time required to make this happen will be 3t. Following is the final equation we are solving for :Time taken by average vehicle : l/v secTime taken by fastervehicle : l/vx secIt simplifies to;Given x , v, l and t are all positive, this can be further simplified to :Here is a JACKPOT! We know that l is always positive, hence to make the above equation practical, both x and (l  3tv) have to be positive. This means if 3tv becomes more than l, you have no chance of beating traffic lights. For instance, if t = 30 sec, v = 5 m/sec and l = 145 m, you simply cannot beat the odds, even if you ride on speed of GUN shot!Say, l = 600 m. The equation becomes :So, here are a few thumb rules to make it possible to beat the Traffic signals :1. Minimize t (cycle of traffic light) : It is possible to beat traffic light in quick traffic localities where it turns Green  Red in quick time.2. Minimize v (the average velocity of the road ) :If the average velocity on road is exceptionally low, we can beat these slow drivers if we drive fast (Duh!)3. Maximize x (Faster multiplier) : If we drive super fast, we can still win the race. But notice if v*t becomes more than 200, you have no chance of gettingDont miss: Introducing the art of structured thinking and analyzingAverage t in Bangalore is about 20 seconds and average speed is 5m/sec. Hence the equation becomes :As seen from the above graph, if x and l are high enough to fall into the shaded region, we have a chance to beat the traffic light.1. There is no point of driving fast on a lane where3 * Green light time * average velocity ismore than the length of the road.2. Beating traffic is possible if following are in our favor :a. High x . We drive really fast (not a safe option)b. High l. For instance driving fast on a highway makes sensec. Low t : No point of driving fast on a high timer traffic signals roadd. Low v : If the average velocity on the road is really low, we can beat them. We already knew that!I hope, you enjoyed solving this traffic problem. Im sure it would have challenged your thinkingwhichwas our motive. Right ?In this article, using a case of traffic light and some elementary physics concepts I have explained the necessary skill required to build a unshakable foundation to become a data scientist.Did you enjoy reading this article? Have you wondered over this question before? Do you think you can improvise these calculations further to make it more realistic?",https://www.analyticsvidhya.com/blog/2015/07/brain-training-analytical-thinking/
Learning Path : Best way to learn Machine Learning in 6 easy steps,Learn everything about Analytics||,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Must for Data Scientists & Analysts: Brain Training for Analytical Thinking|Difference between Machine Learning & Statistical Modeling|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"After immense popularity of our learning paths on various tools, we are delighted to announce our learning path for machine learning. Needless to say, that we have collated all the best resources we know of to take the hassle out of your learning experience.The aim of this page is to provide a comprehensive learning path to people new to machine learning. We recommend you to strictly follow the steps in sequential order. Consider this as your mentor for machine learning.By the end of this learning path, you would have gained complete knowledge about the best resources to learn machine learning. Once youve followed these steps religiously, you can proudly claim yourself to be a machine learning professional.Do you think weve missed out on any ML course? We enjoy taking and implementing feedback. Do let us know your comments in the section below.",https://www.analyticsvidhya.com/blog/2015/07/learning-path-machine-learning/
Difference between Machine Learning & Statistical Modeling,Learn everything about Analytics|Definition:|Differences between Machine Learning and Statistical Modeling:,"A Business Case|They belong to different schools|They came up in different eras|Extent of assumptions involved|Types of data they deal with|Naming Convention|Formulation|Predictive Power and Human Effort|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Learning Path : Best way to learn Machine Learning in 6 easy steps|Data Analyst  Marketelligent  Bangalore  (2  5 years of Experience)|
Tavish Srivastava
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common question, which gets asked atvarious data science forums is:What is the difference between Machine Learning and Statistical modeling?I have been doing research for the past 2 years. Generally, it takes me not more than a day to get clear answer tothe topic I am researching for. However, this was definitely one of the harder nuts to crack. When I came across this question at first, I found almost no clear answer which can layout how machine learning is different from statistical modeling. Given the similarity in terms of the objective both try to solve for, the only difference lies in the volume of data involved and human involvement for building a model. Here is an interesting Venn diagram on the coverage of machine learning and statistical modeling in the universe of data science (Reference: SAS institute)In this article, I will try to bring out the difference between the two to the best of my understanding. I encourage more seasoned folks of this industry to add on to this article, to bring out the difference.Before we start, lets understand the objective behind what we are trying to solve for using either of these tools. The common objective behind using either of the tools isLearning from Data.Both these approaches aim to learn about the underlying phenomena by using data generated in the process.Now that it is clear that the objective behind either of the approaches is same, let us go through their definition and differences.Before you proceed: Machine learning basics for a newbieLets start with a simple definitions :Machine Learning is an algorithm that can learn from data without relying on rules-based programming.Statistical Modelling is formalization of relationships between variables in the form of mathematical equations.For people like me, who enjoy understanding concepts from practical applications, these definitions dont help much. So, lets look at a business case here.Let us now see an interesting example published by McKinsey differentiating the two algorithms :Case : Understand the risk level of customers churn over a period of time for a Telecom companyData Available : Two Drivers  A & BWhat McKinsey shows next is an absolute delight! Just stare at the below graph to understand the difference between a statistical model and a Machine Learning algorithm.What did you observe from the above graph? Statistical model is all about getting a simple formulation of a frontier in a classification model problem. Here we see a non linear boundary which to some extent separates risky people from non-risky people. But when we see the contours generated by Machine Learning algorithm, we witness that statistical modeling is no way comparable for the problem in hand to the Machine Learning algorithm. The contours of machine learning seems to capture all patterns beyond any boundaries of linearity or even continuity of the boundaries. This is what Machine Learning can do for you.If this is not an inspiration enough, machine learning algorithm is used in recommendation engines of YouTube / Google etc. which can churn trillions of observations in a second to come up with almost a perfect recommendation. Even with a laptop of 16 GB RAM I daily work on datasets of millions of rows with thousands of parameter and build an entire model in not more than 30 minutes. A statistical model on another hand needs a supercomputer to run a million observation with thousand parameters.Given the flavor of difference in output of these two approaches, let us understand the difference in the two paradigms, even though both do almost similar job :All the differences mentioned above do separate the two to some extent, but there is no hard boundary between Machine Learning and statistical modeling.Machine Learning is a subfield of computer science and artificial intelligence which deals with building systems that can learn from data, instead of explicitly programmed instructions.Statistical Modelling is a subfield of mathematicswhich deals with finding relationship between variables to predict an outcomeStatistical modeling has been there for centuries now. However, Machine learning is a very recent development. It came into existence in the 1990s as steady advances in digitization and cheap computing power enabled data scientists to stop building finished models and instead train computers to do so. The unmanageable volume and complexity of the big data that the world is now swimming in have increased the potential of machine learningand the need for it.Statistical modeling work on a number of assumption. For instance a linear regression assumes :Similarly Logistic regressions comes with its own set of assumptions. Even a non linear model has to comply to a continuous segregation boundary. Machine Learning algorithms do assume a few of these things but in general are spared from most of these assumptions. The biggest advantage of using a Machine Learning algorithm is that there might not be any continuity of boundary as shown in the case study above. Also, we need not specify the distribution of dependent or independent variable in a machine learning algorithm.Machine Learning algorithms are wide range tools. Online Learning tools predict data on the fly. These tools are capable of learning from trillions of observations one by one. They make prediction and learn simultaneously. Other algorithms like Random Forest and Gradient Boosting are also exceptionally fast with big data. Machine learning does really well with wide (high number of attributes) and deep (high number of observations). However statistical modeling are generally applied for smaller data with less attributes or they end up over fitting.Here are a names which refer to almost the same things :Even when the end goal for both machine learning and statistical modeling is same, the formulation of two are significantly different.In a statistical model, we basically try to estimate the function f inMachine Learning takes away the deterministic function f out of the equation. It simply becomesIt will try to find pockets of X in n dimensions (where n is the number of attributes), where occurrence of Y is significantly different.Nature does not assume anything before forcing an event to occur.So the lesser assumptions in a predictive model, higher will be the predictive power. Machine Learning as the name suggest needs minimal human effort. Machine learning works on iterations where computer tries to find out patterns hidden in data. Because machine does this work on comprehensive data and is independent of all the assumption, predictive power is generally very strong for these models. Statistical model are mathematics intensive and based on coefficient estimation. It requires the modeler to understand the relation between variable before putting it in.However, it may seem that machine learning and statistical modeling are two different branches of predictive modeling, they are almost the same. The difference between these two have gone down significantly over past decade. Both the branches have learned from each other a lot and will further come closer in future. I hope we motivated you enough to acquire skills in each of these two domains and then compare how do they compliment each other.If you are interested in pickingup machine learning algorithms, we have just the right thing coming up for you. We are in process of building a learning path for machine learning which will be published soon.Let us know what you think is the difference between machine learning and statistical modeling? Do you have any case study to point out the differences between the two?",https://www.analyticsvidhya.com/blog/2015/07/difference-machine-learning-statistical-modeling/
"Data Analyst  Marketelligent  Bangalore  (2  5 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Difference between Machine Learning & Statistical Modeling|Senior Analysts (Tableau & SQL)  Marketelligent  Bangalore  (4 + years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data AnalystLocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/data-analyst-marketelligent-bangalore-2-5-years-experience/
"Senior Analysts (Tableau & SQL)  Marketelligent  Bangalore  (4 + years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst  Marketelligent  Bangalore  (2  5 years of Experience)|Data Analyst  Marketelligent  Bangalore  (2  4 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Analysts (Tableau & SQL)Location  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analysts (Tableau & SQL)  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/senior-analysts-tableau-sql-marketelligent-bangalore-4-years-experience/
"Data Analyst  Marketelligent  Bangalore  (2  4 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analysts (Tableau & SQL)  Marketelligent  Bangalore  (4 + years of Experience)|Data Scientist  Marketelligent  Bangalore  (2  4 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data AnalystLocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/data-analyst-marketelligent-bangalore-2-4-years-experience/
"Data Scientist  Marketelligent  Bangalore  (2  4 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst  Marketelligent  Bangalore  (2  4 years of Experience)|Engagement Manager  CPG Analytics  Marketelligent  Bangalore  (4  7 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/data-scientist-marketelligent-bangalore-2-4-years-experience/
"Engagement Manager  CPG Analytics  Marketelligent  Bangalore  (4  7 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Marketelligent  Bangalore  (2  4 years of Experience)|Head-BFSI  Marketelligent  Bangalore  (8  10+ years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Engagement Manager-CPG AnalyticsLocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asEngagement Manager-CPG Analytics  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/engagement-manager-cpg-analytics-marketelligent-bangalore-4-7-years-experience/
"Head-BFSI  Marketelligent  Bangalore  (8  10+ years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engagement Manager  CPG Analytics  Marketelligent  Bangalore  (4  7 years of Experience)|Senior Analyst  Marketelligent  Mumbai  (3  5 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Head-BFSILocation  BangaloreAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asHead-BFSI  Marketelligent  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/head-bfsi-marketelligent-bangalore-8-10-years-experience/
"Senior Analyst  Marketelligent  Mumbai  (3  5 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Head-BFSI  Marketelligent  Bangalore  (8  10+ years of Experience)|Quick Guide: Steps To Perform Text Data Cleaning in Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior AnalystLocation  MumbaiAbout employer  MarketelligentMarketelligent, a Brillio Company has the singular mission of enabling its Clients to make the most of their data. Founded by folks that have over 50 years of combined experience in multinationals across continents and across industries, Marketelligent is the preferred partner for Business Analytics spanning multiple industries  Financial Services, CPG, Media, Telecommunications, etc.Job description: Responsibilities Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analyst  Marketelligent  Mumbai",https://www.analyticsvidhya.com/blog/2015/06/senior-analyst-marketelligent-mumbai-3-5-years-experience/
Quick Guide: Steps To Perform Text Data Cleaning in Python,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Senior Analyst  Marketelligent  Mumbai  (3  5 years of Experience)|Beware  interviewer for analytics job is observing you closely!|
Analytics Vidhya Content Team
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Download the PDF Version of this infographic and referthe python codes to perform Text Mining and follow your Next Steps -> Download Here,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Twitter has become an inevitable channelforbrand management. It has compelled brandsto become more responsive to their customers. On the other hand, the damage it would cause cant be undone. The 140 character tweets hasnow becomeapowerful toolforcustomers / users to directly convey messages to brands.For companies, these tweets carry a lot of information likesentiment, engagement, reviews and features of its products and what not. However, mining these tweets isnt easy. Why? Because, before you mine this data, you need to perform a lot of cleaning. These tweets, once extracted can come with unwanted html characters, bad grammar and poor spellings  making the mining very difficult.Below is the infographic, which displays the steps of cleaning this data related to tweets before mining them. While the example in use is of Twitter, you can of course apply these methods to any text mining problem. Weve used Python to execute these cleaning steps.To view the complete article on effective steps to perform data cleaning using python -> visit here",https://www.analyticsvidhya.com/blog/2015/06/quick-guide-text-data-cleaning-python/
Beware  interviewer for analytics job is observing you closely!,Learn everything about Analytics|Introduction,"Hygiene factors:|Structured thinking:|Numerical abilities:|Curiosity / Motivation:|Problem solving:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Related Articles|Quick Guide: Steps To Perform Text Data Cleaning in Python|Kaggle Bike Sharing Demand Prediction  How I got in top 5 percentile of participants?|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analysts are people with high attention to details! This trait is visible across any endeavor they are involved in.I once went to a Go-Karting event with a bunch of highly competitive analysts. A week after that evening, we had (apart from multiple discussions) an Excel (VBA based) simulation showing details on what happened on each and every turn, when someone overtook the other person, when did people take a pit stop, who couldnt accelerate as much as they could and what not! We almost ended up creating personalized recommendations for each driver.As an analyst, getting into details and studying them carefully, almost becomes your second nature. In an interview, you are likely being interviewed by someone who has been an analyst for a longer duration that you have been. Hence, you should expect a through and close examination of minute details.This, and many more details are covered in our comprehensive Ace Data Science Interviews course. Make sure you check that out before you head for your next interview!Many people new to the industry or who is trying to enter this industry are typically not aware and hence ignore these finer aspects. Hence, I thought Ill create a list of these small things, which are typically overlooked by people coming for an interview. Please see that this is not a list of competencies which you are being judged upon (if you want them, you can read them here). You can almost treat this list as a checklist of things you need to avoid while you are being interviewed.There were some of the behaviors to avoid in an analytics interviewI know of. If you have more to add to this list, please feel free to add them through comments below. These are part of our comprehensive Ace Data Science Interviews course. Thats a guide you dont want to be without for your next data science interview!photo credit: andercismo via photopincc",https://www.analyticsvidhya.com/blog/2015/06/analytics-interview-behaviour-to-avoid/
Kaggle Bike Sharing Demand Prediction  How I got in top 5 percentile of participants?,Learn everything about Analytics|Introduction|Before you start  warming up to participate in Kaggle Competition|Kaggle Bike Sharing DemandChallenge|Solution|End Notes,"Step 1. Hypothesis Generation|2. Understanding the Data Set|3. Importing Data set and Basic Data Exploration|4. Hypothesis Testing (using multivariate analysis)|5. Feature Engineering|6. Model Building|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Beware  interviewer for analytics job is observing you closely!|Senior Consultant  Equifax  Bangalore (3  5 years of experience)|
Sunil Ray
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are three types of peoplewhotake part in a Kaggle Competition:Type 1: Who are experts in machine learning and their motivation is to compete with the best data scientists across the globe. They aim to achieve the highest accuracyType 2: Who arent experts exactly, but participate to get better at machine learning. These people aim to learn from the experts and the discussions happening and hope to become better with time.Type 3: Who are new to data science and still choose to participate and gain experienceof solving a data science problem.If you think you fall in Type 2 and Type 3,go ahead and check how I got close to rank 150. I would strongly recommend you to type out the code and follow the article as you go. This will help you develop your data science muscles and they will be in better shape in the next challenge. The more you practice, the faster youll learn.And if you are a Type 1 player,please feel free to drop your approach applied in this competition in the comments section below. I would like to learn from you!Kaggle Bike Sharing Competition went live for 366 days and ended on 29th May 2015.My efforts would have been incomplete, had I not been supported by Aditya Sharma, IIT Guwahati (doing internship at Analytics Vidhya) in solving this competition.Heres a quick approach to solve any Kaggle competition:InKaggleknowledge competition  Bike Sharing Demand,the participants are asked to forecast bikerental demand of Bike sharing program in Washington, D.C based onhistorical usagepatterns in relation with weather, time and other data.Using these Bike Sharingsystems, people rent a bike from onelocation and return it to a different or same place on need basis. People can rent a bike through membership (mostly regular users) or on demand basis (mostly casual users). This process is controlled by a network of automated kiosk across the city.Here is the step by step solution of this competition:Before exploring the data to understandthe relationship between variables, Id recommend you to focus on hypothesis generation first. Now, this might sound counter-intuitive for solving a data science problem, but if there is one thing I have learnt over years, it is this. Before exploring data, you should spend some time thinking about the business problem, gaining the domain knowledge and may be gaining first hand experience of the problem (only if I could travel to North America!)How does it help? This practice usually helps you form better features later on, which are not biased by the data available in the dataset. At this stage, you are expectedto posses structured thinking i.e. a thinking process which takes into consideration all the possible aspects of a particular problem.Here are some of the hypothesis which I thought could influence the demand of bikes:The dataset shows hourly rental data for two years (2011 and 2012). The training data set is for the first 19days of each month. The test dataset is from 20th day to months end. We are requiredtopredict the total count of bikes rentedduring each hour covered by the test set.In the training data set, they have separately given bike demand by registered, casual users and sum of both is given as count.Training data set has 12variables (see below) and Test has 9 (excluding registered, casual and count).Independent VariablesDependent VariablesFor this solution, Ihave usedR (R Studio 0.99.442) in Windows Environment.Below are the steps to import and perform data exploration.If you are new to this concept, you can refer this guide onData Exploration in RBefore combing test and train data set, I have made the structure similar for both.Above you can see that it has returned no missing values in the data frame.Fewinferences can be drawnby looking at the thesehistograms:Till now, we have got a fair understanding ofthe data set. Now, lets test the hypothesis which we hadgenerated earlier. Here Ihave added some additional hypothesis from the dataset.Lets test them one by one:Lets plot the hourly trend of countover hours and check ifour hypothesis is correct or not.We will separate train and test data set from combined one.Above, you can see the trend of bike demand over hours. Quickly, Ill segregate the bike demand in three categories:Here I have analyzedthe distribution of total bike demand. Lets look atthe distribution of registered and casual users separately.Above you can see that registered users have similar trend as count. Whereas, casual users have different trend. Thus,we can say that hour is significant variable and our hypothesis is true.You might have noticed that there are a lot of outliers while plotting the count of registered and casual users. These values are not generated due to error, so we consider themas natural outliers. They might be a result of groups of people taking up cycling (who are not registered). Totreat suchoutliers, we will use logarithm transformation. Lets look at the similar plot after log transformation.Plot shows registered and casual users demand over days.
While looking at the plot, I can say that the demand of causal usersincreases overweekend.
Here are a few inferences you can draw by looking at the above histograms:You can see that 2012 has higherbike demand as compared to 2011.In addition to existing independent variables, we will create new variables to improve the prediction power of model. Initially, you must havenoticed that we generated new variables like hour, month, day and year.Here we will create more variables, lets look at the some of these:We use the library rpart for decision tree algorithm.
Now, looking at the nodes we can create different hour bucket for registered users.Similarly, we can create day_part for casual users also (dp_cas).As this was our first attempt, we applied decision tree, conditional inference tree and random forest algorithms and found that random forest is performing the best. You can also go with regression, boosted regression, neural network and find which one is working well for you.Before executing the random forest model code, I have followed following steps:Re-transforming the predicted variables and then writing the output of count to the file submit.csvAfter following the steps mentioned above, you can score 0.38675 on Kaggle leaderboard i.e. top 5 percentile of total participants. As you might have seen, we have not appliedany extraordinary science in getting to this level. But, the real competition starts here. I would like to see, if I can improve this further by use of more features and some more advanced modeling techniques.In this article, we have looked at structured approach of problem solving and how this method canhelp you to improve performance. I wouldrecommend you to generate hypothesis before you deep dive in the data set as this technique will not limit your thought process. You can improve your performance by applying advanced techniques (or ensemble methods) and understand your data trend better.You can find the complete solution here : GitHub LinkHave you participated in any Kaggle problem? Did you see any significant benefits by doing the same? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/
"Senior Consultant  Equifax  Bangalore (3  5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Kaggle Bike Sharing Demand Prediction  How I got in top 5 percentile of participants?|Consultant  Equifax  Bangalore (1.5  3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior ConsultantLocation  BangaloreAbout employer EquifaxEquifax India is part of Equifax Inc, a $2 billion group which employs 7000+ people across 18 countries. Headquartered in Bangalore we have worked with many Fortune 500 companies providing Data Analytics and Business Intelligence solutions. Equifax specializes in combining Credit Bureau data with other alternative data sources and adding powerful insights for clients using best in class analytical techniques.Equifax India is one of the fastest growing BI and Analytics outfits in financial services with offices in Bangalore, Dubai and Mumbai. We provide Analytics and BI solutions combining Data, Analytics & Technology to enable banks and retailers to make profitable decisions in day-to-day business operations.At Equifax we truly believe that Our people are our greatest strength and competitive advantage and customer excellence is an amplified reflection of Employee Excellence. Our culture and processes are consciously designed and refined on 3 values  Right-skilling, Being Trusted, and Empowerment.Job description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Consultant  Equifax  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/senior-consultant-equifax-bangalore-3-5-years-experience/
"Consultant  Equifax  Bangalore (1.5  3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Consultant  Equifax  Bangalore (3  5 years of experience)|Campaign Mgmt  Sr. Consultant  Equifax  Mumbai (2  4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ConsultantLocation  BangaloreAbout employer EquifaxEquifax India is part of Equifax Inc, a $2 billion group which employs 7000+ people across 18 countries. Headquartered in Bangalore we have worked with many Fortune 500 companies providing Data Analytics and Business Intelligence solutions. Equifax specializes in combining Credit Bureau data with other alternative data sources and adding powerful insights for clients using best in class analytical techniques.Equifax India is one of the fastest growing BI and Analytics outfits in financial services with offices in Bangalore, Dubai and Mumbai. We provide Analytics and BI solutions combining Data, Analytics & Technology to enable banks and retailers to make profitable decisions in day-to-day business operations.At Equifax we truly believe that Our people are our greatest strength and competitive advantage and customer excellence is an amplified reflection of Employee Excellence. Our culture and processes are consciously designed and refined on 3 values  Right-skilling, Being Trusted, and Empowerment.Job description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant  Equifax  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/consultant-equifax-bangalore-1-5-3-years-experience/
Campaign Mgmt  Sr. Consultant  Equifax  Mumbai (2  4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant  Equifax  Bangalore (1.5  3 years of experience)|Consultant (Qlikview)  Modelytics India Pvt Ltd  Bangalore  (1  2 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Campaign Mgmt  Sr. ConsultantLocation  MumbaiAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asCampaign Mgmt  Sr. Consultant  Equifax  MumbaiIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/06/campaign-mgmt-sr-consultant-equifax-mumbai-2-4-years-experience/
"Consultant (Qlikview)  Modelytics India Pvt Ltd  Bangalore  (1  2 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Campaign Mgmt  Sr. Consultant  Equifax  Mumbai (2  4 years of experience)|Data Analyst  Marketing  JyotKiran Networks Pvt. Ltd Delhi/ NCR (2-3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Consultant (Qlikview)Location  BangaloreAbout employer  Modelytics India Pvt LtdModelytics India Pvt Ltd (www.modelytics.com) was established to offer analytical solutions and functional strategies to various consumer lending institutions in the US and Europe. We offer data driven business solutions and process reengineering services in the marketing & risk management domains. Our clients include some of the leading players in mortgage, credit card & other consumer finance industries in the US & Europe.Job description: Responsibilities The current opening is for the role of a Consultant (Qlikview). The key responsibilities are as follows:Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant (Qlikview)  Modelytics India Pvt Ltd  Bangalore",https://www.analyticsvidhya.com/blog/2015/06/consultant-qlikview-modelytics-india-pvt-bangalore-1-2-years-experience/
"Data Analyst  Marketing  JyotKiran Networks Pvt. Ltd Delhi/ NCR (2-3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant (Qlikview)  Modelytics India Pvt Ltd  Bangalore  (1  2 years of Experience)|7 most commonly asked questions on Correlation|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Analyst  MarketingLocation  Delhi /NCRAbout employer JyotKiran Networks Pvt. LtdJyotKiran Networks Private Limited is a consumer facing e-Commerce startup. As we currently are in stealth mode all we shall disclose publicly for now is that our USP is using a behavioral science based approach and data analytics to provide a better match between consumers and products by better understanding consumers needs and desires. The founding team consists of serial entrepreneurs with an Ivy League educational background and several successful exits of companies they founded. Over the last 20 years, we have built companies that have been acquired for over $500M in value by major MNCs. Additionally, one of the founders is an experienced investor, as a partner in a large US-based VC fund and has within the last 4 years invested in over 20 start-up companies based in India and in USA. We are Stanford University, Oxford University and Columbia Business School alumni and are currently based in New Delhi. You will be part of the excitement at a well-funded start-up company with the stability to challenge the best in the industry.Job description: To get in at the ground level of a start-up. Work with very accomplished founders. Become part of a world-class team, which is driven by innovation and the ability to create game changing businesses. Amazing, personal career growth opportunity.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Marketing  JyotKiran Networks Pvt. Ltd Delhi/ NCR",https://www.analyticsvidhya.com/blog/2015/06/data-analyst-marketing-jyotkiran-networks-pvt-ltd-delhi-ncr-2-3-years-experience/
7 most commonly asked questions on Correlation,Learn everything about Analytics|Introduction|What youll learn ?,"Understanding the Mathematical formulation of Correlation coefficient|Answer  1: Correlation vs. Dependency|Answer  2: Is Correlation Transitive?|Answer  3: Is Pearson coefficient sensitive to outliers?|Answer  4: Does causation imply correlation?|Answer  5: Difference between Correlation and Simple Linear Regression|Answer  6:Pearson vs. Spearman|Answer 7:Correlation vs. co-variance|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Related Articles|Data Analyst  Marketing  JyotKiran Networks Pvt. Ltd Delhi/ NCR (2-3+ years of experience)|Mini Datathon Launch, Mumbai Chapter, Maharashtra, India, 5th July 2015|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The natural trajectory of learning statistics begins with measures of central tendency followed by correlation, regression toother advanced concepts. Amongst these initial concepts, I found correlation easy to understand, yet, got puzzled up when it gotlinked with other statistical concepts & metrices like causation, regression, distribution, pearson correlation coefficientetc. It took me sometime to succeed and get a firm hold on this concept. I succeeded becauseI kept on trying and tried harder, every time I failed.Hence, dont settle, keep trying!To begin with, if you are still struggling to understand the difference between correlation and causation, you should refer to my previous article where Ive explained these concepts in the simplest possible manner.Lets proceed further and learn about the most commonly asked questions asked oncorrelation. If you are learning statistical concepts, you are bound to facethese questions which mostly people try to avoid. For people like me, it should be a good refresher.And if youre looking to learn these questions for your data science interview, we are delighted to point you towards the Ace Data Science Interviews course! The course has tons of videos and hundreds of questions like these to make sure youre well prepared for your next data science interview.Lets begin!The most widely used correlation coefficient is Pearson Coefficient. Here is the mathematical formula to derive Pearson Coefficient.Explanation: It simply is the ratio of co-variance of two variables to a product of variance (of the variables). It takes a value between +1 and -1. An extreme value on both the side means they are strongly correlated with each other. A value of zero indicates a NILcorrelation but not a non-dependence.Youll understand this clearly in one of the following answers.A non-dependency between two variable means a zero correlation. However the inverse is not true. A zero correlation can even have a perfect dependency. Here is an example :In this scenario, where the square of x is linearly dependent on y (the dependent variable), everything to the right of y axis is negative correlated and to left is positively correlated.So what will be the Pearson Correlation coefficient?If you do the math, you will see a zero correlation between these two variables. What does that mean? For a pair of variables which are perfectly dependent on each other, can also give you a zero correlation.Must remember tip: Correlation quantifies the linear dependence of two variables. It cannot capture non-linear relationship between two variables.Good Read: Must Read Books in Analytics / Data ScienceSuppose that X, Y, and Z are random variables. X and Y are positively correlated and Y and Z are likewise positively correlated. Does it follow that X and Z must be positively correlated?As we shall see by example, the answer is (perhaps surprisingly) No. We may prove that if the correlations are sufficiently close to 1, then X and Z must be positively correlated.Lets assume C(x,y) is the correlation coefficient between x and y. Like wise we have C(x,z) and C(y,z). Here is an equation which comes from solving correlation equation mathematically :Now if we want C(x,y) to be more than zero , we basically want the RHS of above equation to be positive. Hence, you need to solve for :We can actually solve the above equation for both C(y,z) > 0 and C(y,z) < 0 together by squaring both sides. Thiswill finally give the result asC(x,y) is a non zero number if following equation holds true:Wow, this is an equation for a circle. Hencethe following plot will explain everything :If the two known correlation arein the A zone, the third correlation will be positive. If they lie in the B zone, the third correlation will be negative. Inside the circle, we cannot say anything about the relationship. A very interesting insight here is that even if C(y,z) and C(z,x) are 0.5, C(x,y) can actually also be negative.The answer is Yes. Even a single outlier can change the direction of the coefficient. Here are a few cases, all of which have the same correlation coefficient of 0.81 :Considerthe last two graphs(X 3Y3 and X 4Y4). X3Y3 is clearly a case of perfect correlation where a single outlier brings down the coefficient significantly. The last graph is complete opposite, the correlation coefficient becomes a high positive number because of a single outlier. Conclusively, this turns out to be thebiggest concern with correlation coefficient, it is highly influenced by the outliers.Check your potential: Should I become a Data Scientist?If you have read our above three answers, I am sure you will be able to answer this one. The answer is No, because causation can also lead to a non-linear relationship. Lets understand how!Below is the graph showing density of water from 0 to 12 degree Celsius. We know that density is an effect of changing temperature. But, density can reach its maximum value at 4 degree Celsius. Therefore, it will not be linearly correlated to the temperature.These two are really close. So lets start with a few things which are common for both.Whats the difference between correlation and simple linear regression?Now lets think of few differences between the two. Simple linear regression gives much more information about the relationship than Pearson Correlation. Here are a few things which regression will give but correlation coefficient will not.The simplest answer here is Pearson captures how linearly dependent are the two variables whereas Spearman captures the monotonic behavior of the relation between the variables.For instance consider following relationship :y = exp ( x )Here you will find Pearson coefficient to be 0.25 but the Spearman coefficient to be 1. As a thumb rule, you should only begin withSpearman when you have some initial hypothesis of the relation being non-linear. Otherwise, we generally try Pearson first and if that is low, try Spearman. This way you know whether the variables are linearly related or just have a monotonic behavior.If you skipped the mathematical formula of correlation at the start of this article, now is the time to revisit the same.Correlation is simply the normalized co-variance with the standard deviation of both the factors. This is done to ensure we get a number between +1 and -1. Co-variance is very difficult to compare as it depends on the units of the two variable. It might come out to be the case thatmarks of student is more correlated to his toe nail in mili-meters than it is to his attendance rate.This is just because of the difference in units of the second variable. Hence, we see a need to normalize this co-variance with some spread to make sure we compare apples with apples. This normalized number is known as the correlation.Questions on correlation are very common in interviews. The key is to know that correlation is an estimate of linear dependence of the two variables.Correlation is transitive for a limited range of correlation pairs. It is also highly influenced by outliers. We learnt thatneither Correlation implyCausation nor vice-versa.Were you able toanswer all questions inthe beginning of this article? Did this article help you with any of your doubts on correlation? If you have any more questions on Correlation, we will be happy to answer them on our discussion portal.",https://www.analyticsvidhya.com/blog/2015/06/correlation-common-questions/
"Mini Datathon Launch, Mumbai Chapter, Maharashtra, India, 5th July 2015",Learn everything about Analytics,"Introduction|Who should join this meetup?||How much is the fees for this meetup?||Why should you attend this meetup?||Contact|Share this:|Like this:|Related Articles|7 most commonly asked questions on Correlation|Infographic: Must Read Books in Analytics / Data Science|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People who are currently working in analytics or eager to enter analytics industry usually have one thing in common which isproblem solving abilities.After going through more than 100 hours of trainings on analytics tools and techniques, people still struggle to implement their knowledge to solvereal time business problems. One such platform which offers problems with a handsome prize money is Kaggle. Every working professional in analytics industry aspires to win atleast one kaggle competition to get credibility in his business domain. But, a few succeed in makingsuch brilliant accomplishments .One major reason for failure that we see is the lack of support in analytics community in India. In order to bridge this gap, Analytics Vidhya proudly announcesMiniDatathonwhere the brains of analytics will fight to solve a kaggle problem. Interesting part is, the people will get completesupport of team of data scientists from Analytics Vidhya.This meetup will be hostedby Mr. Kunal Jain, CEO, Analytics Vidhya who also happens to be a Data Scientist, Growth Hacker.
Anyone who is keento learn analytics or has already stepped intoanalytics and wishes to upscale his/her knowledge and skills is welcome to join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.Food & Fun would be on the house. Please make sure you bring your laptop & your brain along!1. If you are eagerly waiting for an opportunity to compete with the best data analysts, data scientists and check your level of expertise, this can be an ideal platform for you.2.If you are keen to solve kaggle competitions but have are struggling due to lack of guidance and mentorship, you should attend this.3. If you wish to scale up your level of skills and competencies and wish to learn from the best data enthusiastsin the city, you should attend this.4. Needless to say, if you want to meet and network with like minded people i.e. data analysts, data scientists, data managers, this can be an ideal platform for you.Date:5th July 2015Time:10:00amto 4:00pm(IST)Venue: D2 Poddar Chambers,Mathurdas Mill Compound, Lower Parel, Mumbai  400013To participate in this meetup, Register Here.For more information & details on these meetups, feel free to drop us a mail at [emailprotected] or you can also reach us at 0124-4264086.",https://www.analyticsvidhya.com/blog/2015/06/mini-datathon-launch-analytics-vidhya-mumbai-chapter-maharashtra-5th-july-2015/
Infographic: Must Read Books in Analytics / Data Science,Learn everything about Analytics,"P.S  To keep this book-shelfhandy and to avoid hassles of searching it again and again, you can Download the PDF version.|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Mini Datathon Launch, Mumbai Chapter, Maharashtra, India, 5th July 2015|Getting started with Cloud Computing using R Programming|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Drink Coffee, Read Books, Learn More, Be Happy!There are 2 attributes all the members in our team at Analytics Vidhya share:These two attributes lead us to naturally gravitate towards sharing some of the best reads we come across. You can think of this infographic as an ideal list of books to have in bookshelf of every data scientist / analyst. These books cover a wide range of topics and perspective (not only technical knowledge), which should help you become a well rounded data scientist.For your convenience, we have categorized this bookshelf in following categories: Analytics, Data Science, Data Visualization and Web Analytics. The ones classified under category Analytics are actually books which help you understand the perspective of data based decisioning.Of course, it goes without saying  if you think there are more books, which you think should be added to this list, you should add them in this discussion thread.",https://www.analyticsvidhya.com/blog/2015/06/infographic-read-books-analytics-data-science/
Getting started with Cloud Computing using R Programming,Learn everything about Analytics|Introduction|Cloud  an enabling platform for data science:||Why do you need the cloud?||What is Cloud Computing?|What are the cost benefit trade-offs of using cloud computing with R versus other applications?|What are the advantages of using R on the cloud versus on the desktop?||How to use R Programming on the cloud?|How to use R on the cloud using RStudio?||End Notes,"|Using R through the Bioconductor cloud?||What are the various other cloud computing options?||Any examples of Using R with Platforms and Other software as a service?||Any examples of Big Data using R on the cloud?|If you like what you just read & want to continue your analytics earning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Infographic: Must Read Books in Analytics / Data Science|Use of variables in QlikView to create powerful data stories|
Analytics Vidhya Content Team
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Almost any domain / business today is being transformed through SMAC. SMAC is a collective term referring to changes happening in Social, Mobile, Analytics and Cloud. The impact of this change has been across the spectrum  Organizations, people and Products. In todays article, we will enable you to take your analytics capabilities to next level by using Cloud computing.We have explained the concept of cloud computing using R programming and RStudio using a step-wise methodology. Furthermore, you will also learn about the benefits of using R overcloud as compared to the traditional desktop or Local client / Server architecture.Cloud computing has witnessed an unparalleled growth and penetration in last few years. It has enabled organizations to scale quickly and easily. Using cloud services, companies today collect, store and analyze huge amount of data, which was almost non-thinkable before. However, with services from the likes of Amazon, Google and Microsoft, cloud services are now accessible to any analyst.Gone are the days, when you purchase a server for a particular capacity and then need to purchase a new one, when you grow out of the previous capacity. For example, most of the analysis I normally do is on a few GBs of data  sufficient to run on my laptop directly.However, recently Microsoft released ~400 GB of data about Malware and viruses on Kaggle. If, I would have thought of solving this problem on my laptop, I would have run out of my internet plan in just downloading the dataset. Analyzing it is a separate challenge in itself.Even if I would have downloaded the dataset, the only way to do meaning computation through non-cloud way was by buying new machine  not a very practical solution. This is where cloud computing comes in picture!Must Read: Step wise guide tolearn R ProgrammingAs discussed in the case study above, cloud ischeaper for handling big data than storage on local desktops, laptops or servers. Wait.Big Data? Yes! Big Data is an umbrella term that basically denotes data whose Volume and Variety and Velocity is larger than conventional data sources and which requires distributed computing like Hadoop and non-RDBMS storage like NoSQL databases.Must Read: A beginners guide to use big data using MongoDBAccording tothe NIST definition of cloud computing,Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models.Cloud Computing consist of 3 components:IaaS To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software.PaaS Cloud providers deliver a computing platform, typically including operating system, programming language execution environment, database, and web server. Application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layersSaaS  In software as a service (SaaS), users are provided access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as on-demand software.Python is free just like R, but the main reason R scores is that the statistical library of R packages is far more extensive. SAS remains the leading language for corporate analytics on the desktop but it remains expensive for small enterprises and has a significant disadvantage in capital expenditure commitment because of annual license structure instead of one time licence fee.Must Read: A Quick Guide on SAS vs R vs PythonTake the Test: Should I become a Data Scientist?You can create a instance (a virtual machine that you access remotely) on Amazon Cloud, or on MicrosoftAzure or on Google Cloud. You can then simply install R the same way as you use it on your local desktop. You connect to your remote machine through SSH or Remote Desktop.Here is a step by step process forcreating a cloud instance on Amazon Web Services.Note: Amazon has a free tier that enables you to try out the Amazon cloud for free for 1 year. However this is only for micro instances which have very small RAM and very small disk space. For higher RAM and higher storage you need to pay more. To look at the various instances and their per hour pricing you can see visit here.Basically fees is charged in compute units but this website makes it easy to figure out costs.First you need to create your Amazon Id. Once you are done, follow the steps below to create a cloud instance on amazon web services:You can choose on demand instances, or even have reserved instances (booking a virtual machine for a fixed period of time and thus at a considerable discount).Take the Test: Should I become a Data Scientist?RStudio Server edition runs on only Linux. Therefore,we choose Linux instance on the cloud and then configure R Studio Server. We can then connect to the remote RStudio Server through the browser and use it just the same way.Here is a step by step way to run RStudio on the cloud.Bioconductor cloud is an amazing way of kickstarting R on the cloud. You can see the instructions here.You can use cloud options from Google and Windows Azure as well. However most of the space is dominated by Amazon Web Services.Yes we can use Azure Machine Learning with R on the cloud and also use Google Big Query with R.Yes there are many examples. Resource 1and Resource 2.By now, you would have got an overview of how to implement cloud computing using R and R Studio. I really enjoyed writing and curating the useful resources in this article. This article also covers questions which are usually asked by people while learning cloud computing in R. Hence, I have tried to cover all of them in this article. As per my personal experience, Ive found demonstrating cloud in R is relatively easier as compared to other softwares.I hope this article helped you to become familiar with cloud computing. We would love to hear about it from you. Did you find it useful? Feel free to post your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/06/cloud-computing-r-programming/
Use of variables in QlikView to create powerful data stories,Learn everything about Analytics|Introduction|The Natural progression in a programming language|Real life situation  variables to my rescue!|What is a Variable? What are themethods to create it?|How can we access variable?|What are most Common Uses of variable(s)?,"Method 1:|Method -2:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Getting started with Cloud Computing using R Programming|Whats the difference between Causality and Correlation?|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"An application with good Front-end and poor Back-end is like Beauty without brains. You are awed by it initially, but you get irritated by it over time. On the other hand, efficient Back-end with poor Front-end may not excite users to use the application. In todays world, you need excellence on both the aspects.Sadly, a lot of people associate data visualization with only front end engineering. In todays article, I want to bring out what does Back end engineering look like in case of QlikView and how can it add tremendous power to your data stories.If you have learnt coding on any language, you would have experienced this progression yourself.The first stage of learning a language is to understand its syntax, ways to print output, doing mathematical calculations etc. At this stage, you typically dont do any thing complex. This usually isnt very exciting or helpful. After all you dont learn a programming language to calculate 2 + 3!The second stage of learning a language is when you start using variables to accomplish some of the more complex tasks. With use of variables, you can create generic logic, which are more complicated and useful. So it you want to compare 2 columns with a million numbers each, you cant do that unless you use variables (May be you can, not me!). The beauty of the process is that once you learn use of variables, it is impossible to think that there was even a stage 1 involved in the learning process!There are some more stages in learning a new programming language, but for todays article we will focus on what I just mentioned as second stage of any programming language  the use of variables. If you are a complete newbie to QlikView, you should first learn the basics from our learning path Resource to learnQlikview from scratch and become an expertI started using QlikView some time in early 2012 and I fell in love with it. We had a team of passionate developers and over next 6 months (with a lot of dark nights), we were able to replace the traditional clunky excel / access reports on a nice QlikView dashboard for entire Sales process of our organization. We were now the envy of other Intelligence units in our organization and were quoted as a successful transition across the Organization.In September 2012, the Sales Director called me. This was the time of sales planning for next year. So, I expected him to ask the plans for the same. And I was well prepared for this. Over years, we had created an Excel based application which would simulate a whole lot of scenarios and then spit out (literally) the targets for all the sales people.Then came the bomb!The Sales Director said that this time he wants the planning to happen on our QlikView application. He said that he wanted a new sheet in our dashboard, where the sales people could enter parameters for their efforts (e.g. Taking 10 sales calls a day) and the application provides an estimate of where they would end taking seasonality and past trends in account.I had no clue how to do this in QlikView, so I just listened to him. I didnt want all the efforts from last 6 months to end and the people to go back to older ways of running business intelligence. Thankfully, I came across variables and their use to run what-if scenarios, which we went on to build in our application.Variables store data (static or calculated) of any type (numbers, string, boolean, float) and get referenced usingexpressions or directly by using its name. There are a fewground rules of variable naming convention in QlikView:Rule 1: Whenever we create a variable for static value, we prefix variable name with v (lowercase).Rule 2: Whenever we store an expression in a variable, we prefix variable name with e (lowercase).After the first character, we should keeprelevant names so that it is developer friendly. There arevarious methods to create a variable. The commonly used are:Method 1: By going into menu -> settings/variable overviewMethod 2: By using SET and LET statements.Lets look at them one by one:Go to Setting menu > Click Variable OverviewNote: You can define a variablewith anexpression also.Whenever you start a new document and move toedit script, you must have noticed that there are some predefined values which QlikView loads based on your system configuration. Right ?This is because, these are the variables that QlikView creates using Set Statement. You can also use Set statement for defining variable(s) in script.Lets see how do we do it. Look at the below syntax of defining variable:Another method of variable creation is using Let statement.Let statement evaluates the expression on the right side of = and assigns it to the variable on the left side of =.Syntax: Let Variable_Name=Expression;If you will use Setstatement to store expression output in a variable, it will store the expression as a value. Ill showthe detailed effect of defining a formula with Let and Setstatement in the coming up sections.In the screenshot below, you can see all thedefined variables(vPrevYr, vTest, vTest1) after running script. Similarly, new variables can also be defined / found here.Also Read: How to implement incremental load in qlikview?The value of a variable can be accessed using equals (=) sign.If the variable is prefixed with equals =  sign, QlikView tries to evaluate the value as a formula (QlikView expression) and then displays or returns the result rather than the actual formula text.Lets understand it by accessing above created variable vTest (created using Let) and vTest1 (Created using Set).Lets create a text box object and put an expression as =vTestand similarly in another text box, we put=vTest1(as shown below).Justnotice that, the variable (vTest) created using Let has got evaluated well. On the contrary,variable (vTest1) created using Sethasnot been evaluated. This is the difference between creating a variable using Set or Let statement.Now, to evaluate variable vTest1,we can use Dollar Sign Expansion (DSE). It is methodto evaluate aformula. Lets look at this method of using dollar sign expansion. Also, this is also a standard way of accessing a variable in QlikView.Now, Illaccess variable vTest1and checkthe output.
Also Read: The concept of synthetic keys in Qlikview  Simplified !Variables are themainstay of data visualization softwareslike Qlikview, D3.js etc. Optimizing the use of variables always reduces application development time.Lets look at the some common uses of variables:Apart fromthe uses mentioned above, we can also use variable for incremental data load and in various scripting methodology to improve our data models.In this article, we looked at the importanceof variables, methods to create it and the common uses of variable in daily development. Basically, we have looked at two methods (Set & Let and Variable Overview) to create them.If I summarize the most common use of variable, then it would beexpression development using other charts objects like Input box, Slider, Text Box, Buttons and others. My adviseto you would beusing the variable in your dashboard. It reduces the static nature of dashboard and can change the value across the expression by changing it at one place only.I hope this article helped you to get thedetail of variables in QlikView. We would love to hear about it from you. Did you find it useful? Feel free to post your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/06/variables-qlikview-data-stories/
Whats the difference between Causality and Correlation?,"Learn everything about Analytics|Introduction|What are the keypoints in establishing causation?|How can we conclusively derivecause-effect relationship?|Why are observational data not conclusive?|Why dont we do random experiment every time to establish causality?|In that case, how do we establish causality using observational data?|End Notes","Here are the Answers:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Use of variables in QlikView to create powerful data stories|Test your fit as a Data Scientist|
Tavish Srivastava
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Causation and Correlation are loosely used words in analytics. People tend to use these words interchangeably without knowing the fundamental logic behind them. Apparently, people get trapped inthe phonetics of these words and end up using them at incorrect places. But, let me warn you that apart from the similar sounding names, there isnt a lot common in the two phenomena. Their fundamental implications are very different.Lets understand the difference between Causation and Correlation using a few examples below.Analyze the following scenariosand tell us whether there is a causal relation between the two events (X and Y). Answers are provided below.Example 1 : X  Tier of B-school college a student gets offer for=> Y Salary after the graduationHypothesis  Students going to premium B-schools get higher salaries on an average. Are these B-school a cause of getting better jobs?Example 2 : X  Smoking Cigarettes=> Y  Levelof MentalStressHypothesis  People who smoke are found to have higher levelof stress. Is smoking the reason of stress?Example 3 : X  Having Kids=> Y  Maturity levelHypothesis  People get more matured after having kids? Is having kids a cause of attaining higher maturity levels?Example 4 : X  Altitude => Y  TemperatureHypothesis  We witness lower temperature at high altitudes. Which means, the higher you go, the colderit would become. Is higher altitude a cause of lower temperature?I hope the examples described above would have triggered your learning appetiteand have got excited you to learn more on this. Though, this is not a newly discovered topic, but people still havent got firm grip on using these terms.Hence, I have tried to explain the aspects of causation and correlation in the simplest possible manner.In this article, Illexplaindifference between correlation and causation followed by learning whether we just have correlation or we have causation pairs. Understanding of this concept is very essential if you want to keep your foundations strong in this analytics industry, where we now mostly work on black boxes.These techniques (causation and correlation) arent limited to only analytics, but their application spreads across all industries.Example 1 : Causal relation does not exist. For instance, onlyambitious and intelligent people are selected from elite B-schools who further get much higher salary than the average. Hence, even if these students did not studyinTier 1B-School, he/she still might get more than the average salaries. Hence, we have alternate reasoning issue in this case.Example 2 : Causal relation does not exist. We can reject hypothesis based on inverse causality. For instance, higher mental stress can actually influence a person to smoke.Example 3 : Causal relation does not exist. Once again, we can reject hypothesis based on inverse causality. For instance, only mature people are likely to bepreparedto have kids. We can also apply alternate reasoningwith underlying cause as the age. Higher age leads to both, having kids and higher maturity levels.Example 4 : Causal relation does exist. We definitely know that inverse causality is not possible. Also alternate reasoning or mutual independence can be rejected.Result: If you were able to answer all the 4 scenarios correctly, you are ready for the next concept. In case you got any of the scenario wrong, you probably need more practice on finding cause-effect pairs.Here are the key point ( X = > Y) pairs used in establishing causation :1. Alternate Reasoning : If there is an alternate reason(Z) which indeed can influence both X and Y (Z => X & Z => Y are true) , we can reject the hypothesis of X => Y.2. Inverse Causality : If instead of X influencing Y, we have Y influencing X , we can reject X => Y hypothesis based on inverse causality.3. Mutual independence : Sometimes X and Y might just be correlated and nothing else. In such cases we reject hypothesis based on mutual independence.In fields like pharma, it is very important to establish cause-effect pairs. And thats why,there areenough researches done to find cause-effect pairs. Lets understand the following definitions before we get down tomathematics :1. Randomized Experimental data : An experiment is often defined as random assignment of observational units to different conditions, and conditions differ by the treatment of observational units. Treatment is a generic term, which translates most easily in medical applications (e.g. patients are treated differently under different conditions), but it applies to other areas as well.2. Observational data : If we do not have the luxury to do a randomized experiment, we are forced to work on existing data sources. These events have already happened without any control. Hence, the selection is not random.Deriving out causality from Observational data is very difficult and non-conclusive. For a conclusive result on causality, we need to do randomized experiments.1. Observational Data not conclusivebecause the selection in observational data are not randomized. We can never conclude individual cause-effect pair.2. For instance, if the number of students graduating from Tier 1B-school gethigher salary; this willnot conclude causality relationship because the selection was based on initial performance.3. However,if we randomly selectstudents for Tier 1B-schools, this analysis will become more conclusive to establish causality.There are multiple reason you might be asked to work on observational data instead of experiment data to establish causality.First is, the cost involved to do these experiments. For instance, if your hypothesis is giving free I-phone to customers, this activity will have an incremental gain on sales of Mac. Doing this experiment without knowing anything on causality can be an expensive proposal.Second is, not all experiments are allowed ethically. For instance, if you want to know whether smoking contributes to stress, you need to make normal people smoke, which is ethically not possible.There has been good amount of research done on this particular issue. The entire objective of these methodologies is to eliminate the effect of any unobserved variable. In this section, I will introduce you to some of these well known techniques :1. Panel Model (Ordinary regression) : This method comes in very handy if the unobserved dimension is invariant along at least one dimension. For instance, if the unobserved dimension is invariant over time, we can try building a panel model which can segregate out the bias coming from unobserved dimension. For instance, in B Schools=> High Salary example, we assumed that the non observable dimension is invariant over time.Lets try this methodology.1. Following is the equationof regressingy (salary) against both Premium college (subscript T) and unobserved dimensions (subscript U)2. But, because the unobserved dimension is invariant over time, we can simplify the equation as follows :3. We can now eliminate the unobserved factor by differencing over timeNow, it becomes to find the actual coefficient of causality relationship between college and salary.2. Simulated Control : Biggest concern with observation data is that we do not gettreatment and non-treatment data for the same data point. For instance, referring to the smoking example above,a person cant be a smoker and non-smoker at the same time.But, what if, we can find out a look alike for all treatedin the non-treated group. And then compare the response of this treatment among look alikes. This is the most common method implemented currently in the industry.The look alike can be found using nearest neighbor algorithm, k-d tree or any other algorithm. Lets take an instance, we have two people with same age group, gender, income etc. One of them starts smoking and another does not. Now the stress level can be compared over a period of time given no other condition changes among them.While this might sound a very exciting approach theoretically, it is usually difficult to carve out pure simulated / virtual control and this can some time lead to conclusions, which may not be correct. This is actually a topic for a different article in future.3. Instrumental Variable (IV) : This is probably the hardest one which I find to implement. Following are the steps to implement this technique:What have we done here?In observational data, any regression techniqueapplied between cause-effect pair gives a biased coefficient. Using this methodology, we come out with an unbiased estimation. For example(in cigarette  mental stress pair), we mightthink it couldbe influenced by reverse causality.Now, if we can find any information which is connected to cigarette consumption but not mental stress, we might be able to find the actual relationship. Generally IV are regulatory based variables. For example, here we founda tax regulation which increased only cigarette price and lead to lesser consumption of cigarettes across board. We can nowtry finding out the mental stress using the 4-step method discussed above.4. Regression discontinuity design : This is amongst one of my favourite choices. It this makes the observational data really close to experimental design.In the graph shown below, we are findinga dimension which has a spike on treatment and non-treatment population ratio. Suppose, we want to test the effect of scholarship in college on the grades by the end of course for students. Note that,scholarship is offered to students scoring more than 80% in their pre-college examinations. Heres the twist. Because these students are already bright, they might continue being on top in future as well. Hence, this is a very difficult cause-effect relation to crack!But, what if, we compare students who scored just below 80% (say 79.9%) with those who scored just above 80% (say 80.1%) on grades by end of the college. The assumption being that 79.9% student wont be much different from 80.1% student. And the only thing which can change is the effect of scholarship. This is known as Quasi Randomized Selection.Hence, the results are very close to perfect conclusions on causality.The only challenge with this methodology is that getting such a dimension is very difficult which can give a pure break up between treated and non-treated population.Establishing causality is probably the most difficult task in the field of analytics. The probability of getting it wrong is exceptionally high. Key concepts discussed in this article will help you address the question of causality to a good extent.Just to end the article with some humor on the topic, here are a few images to drive the difference in correlation and causality.A spurious correlation:Were you able to find the right cause-effect pairs given at the beginning of this article? Have you applied any of the 4 techniques discussed in this article? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/
Test your fit as a Data Scientist,Learn everything about Analytics|The motivation behind the app:|Purpose of the app:,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Whats the difference between Causality and Correlation?|Data Scientist  InnovAccer  Noida  (0-2 years of experience)|
Kunal Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"While there has been a lot of buzz lately around the demand of data scientists. There are limited resources, which provide a clear answer to following question:Should / Can I become a Data Scientist?orHow do I improve my skills as a data scientist?So, we have created this app, which helps you answer these questions.Analytics Vidhya organized 5 meetups in last 2 months and they were greeted by awesome response. While community building and hackathons are the main themes of these meetups, these meetups provide us another avenue to meet data science aspirants face to face.One question came back again and again across these meetups. It has beenasked on our discussion portals in several ways in past as well.That is when we thought of creating a tool which checks your fit with attributes, which a data scientist requires in his role.As mentioned, this app is built with an aim to help people who want to understand their fit with data science profession. Please note that this app does not judge you on technical knowledge. We believe that if the fundamentals (attitude and aptitude) are in place, technical skills can be learned / adapted easily.So find out how well do you fit with the profession of a data scientist? Which are the skills you need to develop and then go ahead use some of the resources, we have provided as well.Do let us know your experience and feedback with the app.",https://www.analyticsvidhya.com/blog/2015/06/test-fit-data-scientist/
"Data Scientist  InnovAccer  Noida  (0-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Test your fit as a Data Scientist|Data Scientist  Healthcare  InnovAccer  Noida  (0 -2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  NoidaAbout employer InnovAccerInnovAccer is an India based research acceleration company co-founded by IIT and IIM Alumni. The team of coders, data scientists, statistical analysts, and visualization experts at InnovAccer work with researchers across the world to help them create world class datasets, implement cutting-edge statistical models, and build state of the art applications using the latest technology stacks. Within 15 months of its inception, InnovAccer has over 100 customers from 50 customer organizations in over 10 countries including 18 of top 20 business schools like Harvard, Wharton, Stanford, HEC Paris, INSEAD, IESE business school.Job description: ResponsibilitiesQualification and Skills RequiredOther Skill SetWhat to expectSalaryInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  InnovAccer  Noida",https://www.analyticsvidhya.com/blog/2015/06/data-scientist-innovaccer-noida-0-2-years-experience/
"Data Scientist  Healthcare  InnovAccer  Noida  (0 -2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  InnovAccer  Noida  (0-2 years of experience)|Decision Scientist  Machine Learning Engineer  InnovAccer  Noida  (0-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist  HealthcareLocation  NoidaAbout employer InnovAccerInnovAccer is an India based research acceleration company co-founded by IIT and IIM Alumni. The team of coders, data scientists, statistical analysts, and visualization experts at InnovAccer work with researchers across the world to help them create world class datasets, implement cutting-edge statistical models, and build state of the art applications using the latest technology stacks. Within 15 months of its inception, InnovAccer has over 100 customers from 50 customer organizations in over 10 countries including 18 of top 20 business schools like Harvard, Wharton, Stanford, HEC Paris, INSEAD, IESE business school.Job description: ResponsibilitiesQualification and Skills RequiredOther Skill SetIncentivesInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  Healthcare  InnovAccer  Noida",https://www.analyticsvidhya.com/blog/2015/06/data-scientist-healthcare-innovaccer-noida-0-2-years-experience/
"Decision Scientist  Machine Learning Engineer  InnovAccer  Noida  (0-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Healthcare  InnovAccer  Noida  (0 -2 years of experience)|Getting Mongo-ed in NoSQL manager, R & Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Decision Scientist  Machine Learning EngineerLocation  NoidaAbout employer InnovAccerInnovAccer is an India based research acceleration company co-founded by IIT and IIM Alumni. The team of coders, data scientists, statistical analysts, and visualization experts at InnovAccer work with researchers across the world to help them create world class datasets, implement cutting-edge statistical models, and build state of the art applications using the latest technology stacks. Within 15 months of its inception, InnovAccer has over 100 customers from 50 customer organizations in over 10 countries including 18 of top 20 business schools like Harvard, Wharton, Stanford, HEC Paris, INSEAD, IESE business school.Job description: ResponsibilitiesQualification and Skills RequiredOther Skill SetWhat To ExpectSalaryInterested people can apply for this job can mail their CV to[emailprotected]with subject asDecision Scientist  Machine Learning Engineer  InnovAccer  Noida",https://www.analyticsvidhya.com/blog/2015/06/decision-scientist-machine-learning-engineer-innovaccer-noida-0-5-years-experience/
"Getting Mongo-ed in NoSQL manager, R & Python",Learn everything about Analytics|Introduction|Integration of MongoDB with analytics tools:|Structure of the remaining article:|End Notes,"1. CRUD operations & Aggregation in MongoDB using NoSQL Manager|2.1Using RMongo Package|2.2 Using Mongolite Package|3. Using PyMongo|4.1Using JSON Studio for accessing MongoDB |4.2. Using jSonarR|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Decision Scientist  Machine Learning Engineer  InnovAccer  Noida  (0-5 years of experience)|Data Scientist  Machine Learning / Artificial Intelligence  IREF  Mumbai (3-6 Years of Experience)|
Shuvayan Das
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1.a) CREATE|1.b)READ|1. c) UPDATE|1. d) REMOVE|1. e)Aggregation operations|Aggregation|Connect to a MongoDB instance|Insert|Return only chosen fields|Aggregation|MapReduce|Insert records|Querying one or more than one document|Aggregation|Creating an Aggregation Pipeline:|Basic Visualizations|Writing and executing Code for visualizations,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I started my journey on Analytics Vidhya (AV) as a follower. AV, and now especially the discussion portal, always stay open in one of the windows of my working machine. Like thousands of you, who read and love these resources, I owe a lot of my learning in Analytics career to Analytics Vidhya. This relationship evolved further, when my previous article onBeginners Guide to MongoDBgave me a glimpse of what the platform can do for people like me.I am basking in the glory ofthis new found love with my old companion / mentor and taking this journey of contributing back to the community a step forward.In this article, I explain how to use MongoDB in NoSQL Manager, R and Python.MongoDB is the most popular NoSQL database out there. It is used by several big companies like ebay, Criagslist, FourSquare etc. Most of the popular data analysis tools likeR and Python offer incredible packages to integrate themselves with MongoDB. These packages enable people to use Mongo & its powerful features from the windows of their choice and comfort.Some of the popular packages integrating MongoDB to the analytics tool are RMongo, PyMongo, Mongolite, JSON Studio and jSonarR.Here is a brief introduction of these packages:I have structured this article in four parts as described below. We first look at how various operations can be performed using NoSQL manager and then see various packages in R and Python.CRUD stands for: C  Create, R  Read, U  Update, D  Delete. On the other hand, aggregation operationsprocess data records and return computed results. Understanding how these operations can be performed using NoSQL manager would lay the foundation for the remaining article. Once we know this, we will perform similar operations from R and Python.MongoDB stores data in the form of documents:JSON-like field and value pairs called BSON. In the documents, the value of a field can be any of the BSON data types, including other documents, arrays, and arrays of documents. Well perform various CRUD and aggregation operation sequentially.We will create two collections namely IPL (Indian Premier League) Fastest Century and IPL Most Runs. Please note that I am calling thesecollections (and not tables) and I would suggest all wannabe Mongo-ers to do so as well.1. IPL_Fastest_Century                             (IPL 8  Indian Premier League Season 8)2. IPL_Most_RunsNow well insertthese records in the database. Here is the code to insert the first collectionFastest century collectionThe command db.IPL_Fastest_Century.insert () is used to insert records into a collection. If the collection does not exist, it will automatically get created.After we are finished with inserting the records, lets now view the structure of the collection IPL_Fastest_Century.If you notice the structure of the document here, username is another document embedded in the main document. This is a special feature of NoSQL systems like MongoDB and one of its structural characteristics which gives the potential to store and process complex data.Exercisefor you: Create the IPL_Most_Runs collection in a similar way. If you face any difficult, seek my guidancehere.Additional Information: We can also insert an array of documents at once:Note: This is an official MongoDB example taken from the documentation.In MongoDB, the db.collection.find() method retrieves documents from a collection. By default, the db.collection.find() method returns all the documents in a collection. This function has other uses as well which I plan to discuss later.There are four types of READ operations as listed below:These operations are described in the screenshot shown below:Additional Information: How to perform querying on embedded documents?Documents embedded within another document can be dealt with in the following two ways:By now, people who are familiar with SQL must have realized the differences in syntax of MongoDB and SQL.Moreover,the username.lastname functionality shown above, is exclusive to NoSQL DBMS because of itsembedded documents feature. It is alsouseful for extracting information from websites like Facebook as shown in the template below:MongoDB providesupdate() method to update the documents of a collection. The method accepts the following parameters:The screenshot below shows the commandsto useUpdate operation:Use of UPSERT in UPDATE operationThe update() method will not give any outputin case no document matches the condition query. However,if wespecifyupsert:true, update() will add a new document in case it doesnt exist. If the doc exists upsert just does the update. Lets see how!In MongoDB, the db.collection.remove() method removes documents from a collection. We can remove all documents from a collection by removing all documents that match a condition, or limiting the operation to remove just a single document. Lets see how.Note: In the last line of code, the value 1 specifies only a single document which matches the criteria must be removed.The data records in MongoDB can be processed by various operations for aggregation. After processing, it returns the computed results. It collectsvalues from multiple documents, clubs the values and performs a variety of operations on the grouped data to return a single result. MongoDB provides multipleways to perform aggregation. I have explained two of them, namely: Aggregation Pipeline andMap-Reduce Function.Lets see how to do it!Aggregation Pipeline:How does this aggregation pipeline works?MongoDB aggregation pipelines consist of stage based transformation of data. All pipeline stages do not necessarily produce new documents. The stages might appear multiple times in the pipeline.Infact, some stages might take pipeline expressions as operand, which specify the transformation to be done (ex: $and). Mostly, expressions are evaluated only when they are encountered in the pipeline but accumulates expressions such as $sum, $max. These expressions are thenencountered during the $group stage is maintained throughout and the collective result is maintained till the pipeline code ends.Map ReduceHow does Map Reduce operation works?In the map-reduce operation, the map phase is applied to each input record whichmatches the query condition and key-value pairs are then emitted. For the keys that have multiple values, the reduce phase aggregates the collected data and finally the data is stored in a collection. Below is the visual representation of this operation:Must Read:Introduction to MapReduceCodes for Aggregation and Map Reduce in MongoDB

By this, we complete the overview of some basic operations in MongoDB.Note: All the codes done here in the NoSQL manager can be done similarly in the Mongo Shell. I have used NoSQL here for ease of illustration.In the previous section, we got familiar withsome basic coding in MongoDB. But as analysts, most of the work we do, will be done by using a statistical software. We connect the tool of our choice to the database, extract data to generate useful insights.In this section, we will see how to use a package called RMongoDB which provides an interface to the NoSQL MongoDB database using the MongoDB C-driver version 0.8.  source: CRAN.So, lets quickly begin by installing the package. Use the codes below to install this package.The dataset output_rcb which contains only the documents of RCB is shown above. Well now use this dataset to explain operations using MongoDB.Anadvantage of using RMongo package is that, its syntax is similar to MongoDB shell. Hence, operations likerecord, insertion, deletion can be performed in the similar fashion.Also Read: The list of best resources to learn R ProgrammingPerforming aggregation in Rmongo package is similar to performing it in MongoDB.Here, our motive is to match records from the IPL_Most_Runs collection where the Average is more than 40. Then, well group those records by team and sum up the Runs within each group. Lets see how can we do it.We can see that the output is not a dataframe. This is a common problem faced when using these R packages to interact with MongoDB. Since MongoDB stores data internally as BSON, converting them into data frames and performing complex queries such as aggregation, fails to store the results in a data frame except when the data is of primitive type.But simpler queries like dbGetQuery results in a dataframe.A drawback of using this package is, you cannot use it with MapReduce.We got familiar withthe use of RMongo package and few of itslimitation also. There is another package in R called Mongolite which provides another way to connect to a MongoDB instance. This package has been recently releasedon 12th May, 2015. Hence, not many people have started using it.We will use the dataset shown belowfor illustrations:After installing and loading the package into R library, we need to establish a connection to a MongoDB instance and a database in it.This creates a collection called IPL_Most_Sixes in the database test.For this package we have to load a dataframe into R and then insert it into the MongoDB instance.After importing the data into R we insert it into the IPL_Most_Sixes collection created above.Lets view the data in the collection:This returns only the fields PlayerName, Sixes and SR for the records where the Team is RCB. The result is a data frame object.We can also update and remove entries from the database. But, the commands for those operations are similar to what we have already seen in this article, here we are going to look at Aggregation and Map-Reduce in the mongolite package.We can do aggregation similar tothe previous sections using the aggregate function in this package.As. it can be seen,only the required fields are loaded from MongoDB and it willresults in considerable performance enhancement in case of large data sets. Also, the result has been internally converted to a data frame.The map and reduce functions calculate the sum of runs grouped by teams.These results can be stored in a data frame in R for further analysis as shown above.Hence, these two packages (RMongo & Mongolite) in R can be used to deal with data from MongoDB albeit some limitations. There is another package called rmongodb which can be used.However, the results generated from data manipulation using rmongodb are JSON objects and arenot very useful. The syntax is also very complex compared to the other two packages.Till now, we have learnthow to access MongoDB from R . Lets move into Python and see what libraries are there to deal with similar cases.Also Read: Best ways to perform data manipulation using MapReducePyMongo is a Python distribution containing tools for working with MongoDB, and is the recommended way to work with MongoDB from Python.We will be using the dataset shown below:We will create a collection called pymongo and perform operations using the pymongo library.The command db.pymongo creates the database pymongo if it does not exist. Here well insert only one record. As you can see the Stats column is an embedded document in itself.We have inserted 5 records in total into the collection called pymongo. Lets view it in NoSQL managerThe find_one() is used to find a single document in a collection. If we do not pass any arguments to it,the first document will be returned. If we pass any condition, in that case, the first document matching the condition is returned.For querying multiple documents, a for loop is used as shown in the image above. Here, we want to see only those records where the number of ducks are <= 2.Notice the Stats.Ducks part, this is the syntax to be used when we want to query within embedded documents.Also Read: A Quick Guide to learn Python for Data ScienceIn this section we will see how to perform aggregation in Pymongo using Pipelines and Map Reduce.1.Using PipelineThe unwind command opens up the embedded document Stats for further manipulation.Here we are trying to calculatehow many ducks are there for each value of duck. For example,there are 4 records having 2 ducks each, hence a total of 8 ducks.2. Using Map Reduce

The mapper function creates an array of runs for each team and the reducer function sums the runs grouped by each team. The results are stored in the collection runs_team.Aggregations is one of the strong points of the MongoDB framework and can be done quite easily in Mongo Shell, R & Python. There is another package in R calledjSonarR: jSonar Analytics Platform API for RThis package enables users to access MongoDB by running queries and returning their results in R data frames. Usually, data in MongoDB is only available in the form of a JSON document. jSonarR uses data processing and conversion capabilities in the jSonar Analytics Platform and the JSON Studio Gateway , to convert it to a tabular format which is easy to use with existing R packages.- Source : CRAN-jSonarRBefore we dig into the jSonarR package, lets get a basic understanding of using JSON Studio for interfacing with MongoDB. We will take a look at :Here is theInstallation Guide for JSON Studio.Once youve installed JSON Studio, perform the following operations.You can also see the code for pipeline by clicking on View Pipeline Code. You can save the pipeline for future use as well. There is also a chart option which can be used to visualize the output of the aggregation.Below are the illustrations of output of aggregation using basic visualization methods. Lets look at the steps to create them:Note: At times, all the documents in a collection might not be loaded into the chart window. In that case, click on + to bring in all the documents.As you can see above, the bar chart has been created from IPL_Most_Runs data. It is also an interactive chart, RUNS of MI are displayed when we hover above the bar for MI as shown above.Here is another chart:Here the distribution of runs scored by the number of 50s scored is displayed on a scatter plot. The size of the bubble is according to the average of the particular batsman and the grouping has been done on team.We can also execute code in JSON studio. If you have grasped the syntax of MongoDB shown in the first part ,executing code here will not be an issue.The code basically remains the same but only db.IPL_Most_Runs.find() is replaced by db[IPL_Most_Runs].find() as shown below:Select the highlighted portion and click on Execute. The results are JSON objects which get displayed in the results window. You can click on Report to see a tabular format of the output. The results can also be saved into another collection by using the Insert results into: option.These were some of the basic operations that can be done using JSON studio.Though modeling techniques such as regression,clustering etc. cannot be applied using JSON it is definitely a great tool for exploring data, especially when using unstructured data gleaned from MongoDB.Now that we have an understanding of how JSON studio functions, lets quickly take a look at the jSonarR package.IllustrationLets see how we can use the aggregation pipeline we had created in the preceding section to load the resultant data into R and view it as a data frame.Now lets see the structure of the data loaded.We can see that the data has been loaded as a data frame in R. With this data frame, we can do any further analysis as we might want to.Note:All the data has been compiled from IPL Statistics.If you wish to learn more about Json Studio, please visit : http://jsonstudio.com/If you wish to learn more about jSonarR package in R, please visit jSonarR.A comprehensive knowledge of databases is not requiredby an analyst, but knowing how to access a database and getting data into the local environment for further use is certainly beneficial.In this article, we learnthow to deal with MongoDB data using R,Python and Json Studio. This was not aimed at providing the reader with a comprehensive knowledge of any one method but rather as a quickguide to several methods. Choose the one as per your need and comfort.!!I hope this article helped you to get acquainted with basics of MongoDB packages. We would love to hear about it from you. Did you find it useful? Feel free to post your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/06/mongodb-r-python-nosql-manager/
Data Scientist  Machine Learning / Artificial Intelligence  IREF  Mumbai (3-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Getting Mongo-ed in NoSQL manager, R & Python|Machine Learning Basics for a newbie|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist  Machine Learning / Artificial IntelligenceLocation MumbaiAbout employer  IREFJob description: Since home buying is one of the most important decisions in a persons life, lack of tangible information makes the process harrowing and hence the decision tough. IREF intend to revolutionize the process by bringing in transparency in the real estate market. We will be providing depth and breadth of information required to make a home buying decision.Information shared by people resides mostly in cluttered form, to find something informative is like searching a needle in a haystack. We want people to be able to find relevant information easily, suited to their individual needs and their interests. This makes Data Science team an integral part of IREF.We are looking for people smart enough to create JARVIS but would rather work on NERProducts in Development:Qualification and Skills RequiredYou will be assessed on:Interview procedure:Interested people can apply for this job can mail their CV to[emailprotected]with subject as Data Scientist  Machine Learning / Artificial Intelligence  IREF  MumbaiIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/06/data-scientist-machine-learning-artificial-intelligence-iref/
Machine Learning Basics for a newbie,"Learn everything about Analytics|Overview||Introduction|So what exactly is machine learning? My small experiment|Now the common question  How is machine learning different from X?|But, How exactly do we teach machines?|What are the steps used in Machine Learning?|What are the types of Machine Learning algorithms?

|What are the applications of Machine Learning?|End Notes","X = Artificial Intelligence(AI):|X = Statistics:|X = Deep Learning:|X = Data Mining:|Supervised Learning / Predictive models:|Unsupervised learning / Descriptive models:|Reinforcement learning (RL):|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Machine Learning / Artificial Intelligence  IREF  Mumbai (3-6 Years of Experience)|In Conversation with Mr.Stefan Groschupf, Founder and CEO, Datameer|
Kunal Jain
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Use ofArtificial Intelligence in Machine Learning|Use ofStatistics in Machine Learning,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There has been a renewed interest in machine learning in last few years. This revival seems to be driven by strong fundamentals  loads of data being emitted by sensors across the globe, with cheap storage and lowest ever computational costs!However, not every one around understands what machine learning is. Here are a few examples:Here was a little funny (but immensely true) take on the topic we circulated on our Facebook page recently.Coming to the point, given the amount of confusion on the topic, we thought to create an awesome introductory series of articles on machine learning. The idea is to do away with all the jargons, which might have intimidated you in past and create something which can be read by a 5 year old (oksorry, may be a high school pass out)!Just to make sure I dont over-estimate (or under-estimate) the capability of the target audience, I got hold of 10 people who werecompletely new to analytics. None of them had heard about machine learning before (yes, there are people like that!). Here is what they said:That was fun! Perfect group to explain machine learning to. Here is how I started explaining to these people:Machine Learning refers to the techniques involved in dealing with vast data in the most intelligent fashion (by developing algorithms) to derive actionable insights.By this time, they were looking at me as if I have spoken a few things in front of people from Mars! So, I stopped and then asked them a question in return, which they could relate to more:KJ: What do you think happens when you search for something on Google?Group: Google shows up the most relevant web pages related to that search.KJ: Thats good! but what really happens so that Google can show these relevant pages to you?This time it looked like they were thinking a bit more. Then some one from the group spokeGroup member: Google looks at the past clicks from the people to understand which pages are more relevant for those searches and then serves those results on top of search.This wasa far better attempt. I also had to control my urge to preach that how Google does this is far more smarter way than this simple concept. But, I thought I had a good hook to explain machine learning here. So, I continued:KJ: OK, that sounds like a good approach. But, how many searches and what all kind of searches would Google handle regularly?Group: Must be a real big number  may be a trillion searches every yearKJ: So, how do you think Google can serve so many requests with such accuracy? Do you think there are people sitting in Google offices and continuously deciding which search result is relevant and which is not?Group member:Havent really thought about it, but no, that sounds humanly impossible to do.KJ: You are right. This is where machine learning comes into play. Machine learning is a set of techniques, which helpin dealing with vast data in the most intelligent fashion (by developing algorithms or set of logical rules) to derive actionable insights (delivering search for users in this case).A logical nod from the group, looks like mission accomplishedyay! But waitThe minute you start reading about machine learning, you see various rockets bombarding you with high velocity. These are jargons used loosely in the industry. Here are some of them:Artificial Intelligence, Deep Learning, Data Mining and Statistics.For yourclear understanding, I have explained these terms below in the simple manner. You will also understand the importance of these termsin context of machine learning:Itrefers to the procedure ofprogramming a computer (machine) to take rational.Ah!what is rational? Rationalis the basis of taking a decision.I mentioned rational instead of intelligence (as expected) becausewe human beings tend totake decisions which are high on beingrational and feasible rather than being explicitly intelligent. This is because all intelligent decisionsneedntbe rational and feasible (my hypothesis).Hence, the central motive behind usingAI is to achieve the computer (machine) behave in a dandy fashion in lieu of human guidance instead of being doltish!AI may include programs to check whether certain parameters within a program are behaving normally. For example, the machine may raise an alarm if a parameter say X crosses a certain threshold which might in turn affect the outcome of the related process.Machine Learning is a subset of AI where the machine is trained to learn from its past experience. The past experience is developed through the data collected. Then it combines withalgorithms such asNave Bayes, Support Vector Machine(SVM) to deliver the final results.At this high level stage, I assume you would know about statistics. If you dont, heres a quick definition,Statistics is that branch of mathematics which utilizes data, either of the entire population or a sample drawn from the population to carry out the analysis and present inferences. Some statistical techniques used are regression,variance, standard deviation, conditional probability and many others. To know about this topic, read How to understand population distributions using statistics?Lets understand this. Suppose, I need to separate the mails in my inbox into two categories: spam and important. For identifying the spam mails, I can use a machine learning algorithm known as Nave Bayes which will checkthe frequency of the past spam mails to identify the new email as spam. Nave Bayes uses the statistical technique Bayes theorem( commonly known as conditional probability).Hence, we can saymachine learning algorithms uses statistical concepts to execute machine learning.Additional Information:The main difference between machine learning and statistical models come from the schools where they originated. While machine learning originated from the department of computer science and statistical modelling came down from department of mathematics. Also any statistical modelling assumes a number of distributions while machine learning algorithms are generally agnostic of the distribution of all attributes.Deep Learning is associated with a machine learning algorithm (Artificial Neural Network, ANN) which uses the concept of human brain to facilitate the modeling of arbitrary functions. ANN requires a vast amount of data and this algorithm is highly flexible when it comes to model multiple outputs simultaneously.ANN is more complex topic and we may do justice to it in an altogether separate article.During my initial days as an analyst, I always used to muddle the two terms: Machine Learning and Data Mining. But, later I learnt,Data Mining deals with searching specific information.And Machine Learning solely concentrates on performing a given task. Let me cite theexample which helped me to remember the difference; Teaching someone how to dance is Machine Learning. And using someone to find best dance centers in the city is Data Mining. Easy!Also Read: Introduction to Online Machine LearningTeaching the machines involve a structural process where every stage builds a better version of the machine.For simplification purpose, the process of teaching machines can broken down into 3 parts:I shall be covering each of these 3 steps in detail in my subsequent write-ups. As of now, you should understand, these 3 steps ensures the holistic learning of the machine to perform the given task with equalimportance. Success of machine depends on two factors:1. How well the generalization of abstraction data take place.2. How well the machine is able to put its learning into practical usage for predicting the future course of action.AlsoRead: Learn about Scikit-Learn  Machine Learning tool in PythonThere are 5 basic steps used to perform a machine learning task:Be it any model, these 5 steps can be used to structure the technique and when we discuss the algorithms, you shall then find how these five steps appear in every model!Also Read: Getting Smart with Machine Learning  Ada Boost and Gradient BoostPredictive model as the name suggests is used to predict the future outcome based on the historical data. Predictive models are normally given clear instructions right from the beginning as in what needs to be learnt and how it needs to be learnt. These class of learning algorithms are termed as Supervised Learning.For example: Supervised Learning is used when a marketing company is trying to find out which customers are likely to churn. We can also use it to predict the likelihood of occurrence of perils like earthquakes, tornadoes etc. with an aim to determine the Total Insurance Value. Some examples of algorithms used are:Nearest neighbour, Nave Bayes, Decision Trees, Regression etc.Itis used to train descriptive models where no target is set and no single feature is important than the other. The case of unsupervised learning can be:When a retailer wishes to find out what are the combination of products, customers tends to buy more frequently. Furthermore, in pharmaceutical industry, unsupervised learning may be used to predict which diseases are likely to occur along with diabetes. Example of algorithm used here is:K- means Clustering AlgorithmIt is an example of machine learning where the machine is trained to take specific decisions based on the business requirement with the sole motto to maximize efficiency (performance). The idea involved in reinforcement learning is: The machine/ software agent trains itself on a continual basis based on the environment it is exposed to, and applies its enriched knowledge to solve business problems. This continual learning process ensures less involvement of human expertise which in turn saves a lot of time!An example ofalgorithm used in RL is Markov Decision Process.Important Note:There is a subtle difference between Supervised Learning and Reinforcement Learning (RL).RL essentially involves learning by interacting with an environment. An RL agent learns from its past experience, rather from its continual trial and error learning process as against supervised learning where an external supervisor provides examples.A good example to understand the difference is self driving cars. Self driving cars use Reinforcement learning to make decisions continuously  which route to take? what speed to drive on? are some of the questions which are decided after interacting with the environment. A simple manifestation for supervised learning would be to predict fare from a cab going from one place to another.It is very interesting to know the applications of machine learning. Google and Facebook uses ML extensively to push their respective ads to the relevant users. Here are a few applications that you should know:These examples are just the tip of the iceberg. Machine learning has extensive applications practically in every domain. You can check out a few Kaggle problems to get further flavor. The examples included above are easy to understand and at least give a taste of the omnipotence of machine learning.In this article, we started by developing a basic understanding of what machine learning is. We also looked at how it gets confused with several other terms. We also coveredthe processto teach a machine, the essential steps used in machine learning, the algorithms used in machine learning followed by the applications of machine learning.I hope this article helped you to get acquainted with basics of machine learning. We would love to hear about it from you. Did you find it useful? What aspects of machine learning confuse you the most? Feel free to post your thoughts through comments below.This article was originally written by Payel Roy Choudhury, before Kunal did his experiment to set the tone. Payel has completed her MBA with specialization in Analytics from Narsee Monjee Institute of Management Studies (NMIMS) and has worked with Tata Consultancy Services (TCS) in past. She is looking forward to contribute regularly to Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/
"In Conversation with Mr.Stefan Groschupf, Founder and CEO, Datameer",Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Machine Learning Basics for a newbie|Tuning the parameters of your Random Forest model|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With the growing usage of Hadoop, Datameer launched a custom big data analytics application to help people generate insights from data faster than ever before. Datameer provides applications with several benefits such as lightweight architecture, ease of integration, fast execution time and ease of use amongmany other advantages.In order to understand their journey and product philosophy,we decided to connect with the management of Datameer. Fortunately, we got an opportunity to talk toMr.Stefan Groschupf, Founder and CEOof Datameer.Stefan is named one of the most Innovative Germans Under 30 by Stern Magazine in 2003 and his company, Datameer, was voted one of the Worlds Most Innovative Companies by Fast Company in 2013. We discuss the dynamic changes happening in big data industry and how Datameer plans to innovate in this evolving field.Below is the complete transcript of our conversation.AV(Analytics Vidhya): Hi Stefan! Thanks for devoting your valuable time for this interview. To begin with, tell us about your journey from designing open source technologies to co-founding Datameer.SG(Stefan Groschupf): When I was 16, one of the few movies you could see after the wall came down in East Germany was Three Days of the Condor in which Robert Redford used a PDP8 to analyze books. I always found that I couldnt read enough books and the idea of being able to write software that could analyze text really fascinated me. I absolutely love reading and discovering new ways of thinking, new insights. The idea of being able to write software that could analyze text completely blew my mind. Thats why I developed, at some point, data classification clustering algorithms.I did quite a lot of work on open-source plugins for Eclipse and also worked on JBoss. Thats eventually how I started working on Nutch, the technology that spun out Hadoop. Again, I was fascinated early on by the creative process of creating functionality around data, specifically, text data. I worked on network word graphs, with early thesaurus datasets and with Weka, which is one of the first data-mining, open-source frameworks books.Open source is a great way to learn new technologies but also enjoy the creative process.AV: When you started open source technologies, managing big data was a rare thing. Not everyone thought of doing that. What were the challenges that you faced while building up this entity?SG: We founded Datameer early when most investors and companies were thinking about infrastructure (e.g. Cloudera). We believe Hadoop is such a game changer that it requires an enterprise platform on top and that is where we can have the biggest impact to give power to self-service. We had the idea early when the market wasnt quite ready for it but now we have first mover advantage where the market is exploding.We have to be financially disciplined. We have only raised $37 million dollars so far and our next competitor just billed that last year. I think theres a lot of hype in this space for good reason. There are potentially huge returns of investment. I think that we have built a very solid company and we have phenomenal growth year-over-year. We finance that growth by having a great product that people are willing to pay for.Overall, if theres a great opportunity, Silicon Valley or the world of emerging technology will always try to capitalize on it. If you have such a unique shift in the market like big data or data in general, brought to the market, then its clear that there will be a number of start-ups that want to jump on that bandwagon. Guess what? Building a company is really hard. Not everyone will do it.AV: What are Datameers current offerings? How have you positioned your products in the market? SG: Companies understand the value of analyzing big data, but the real subject matter experts like doctors, marketers, or financial analysts need a way to access the data on their own without needing to rely on IT or a data scientist.Datameer gives business users a self-service, end-to-endbig data analyticstool that sits natively on Hadoop. This approach drastically reduces both the cost and the time it takes to get to insight. What once were complex algorithms are now buttons you can click that will automagically identify groups, relationships, patterns, and even build recommendations based on your data.Were built to run where, and how, you want so we can run on the cloud for business departments or on-premise for our onsite Enterprise edition.AV: Since you have targeted multiple industries, how difficult is it to deal with data from multiple industries in a place where data privacy & security is the major concern of companies?SG:As we dont deliver packaged vertical applications, data privacy and security is an implementation detail specific to the industries, organization and use cases where our product is deployed, not something we ship as a one-size-fits-all. However, as Datameer has robust, industry-leading capabilities to support data governance (including data privacy and security), organizations can easily implement their own policies, and the policies of the applicable regulators by configuring Datameer according to their needs.We are very serious about providing an enterprise-grade product for our customers and offer features such as encryption, anonymity, access control and auditing.AV: How does Datameer challenge the frontiers of todays big data analytics industry? Also, what challenges do you face in doing so?SG: Traditional and legacy data warehouses and business intelligence systems are complex, expensive, and time consuming to deploy. Hadoop brings a new way to store and analyze data  since it is linearly scalable on low cost commodity hardware, it removes the limitation of storage and compute from the data analytics equation. Instead of pre-optimizing data in the traditional ETL, datawarehouse, and BI architecture, Hadoop stores all of the raw data and, with Datameer, applies all transformations and analytics on demand.Were also a self-service solution built for business users. Business users are the subject matter experts, and should be the ones working with the data directly. They shouldnt have to go to IT every time they want to ask a new question.Lastly, were an all-in-one tool for data integration, analytics and visualization. In traditional business intelligence, there are typically three different tools and three different teams involved. With these features combined, there is no other tool on the market that allows you to get from raw data to insight in such a short period of time.The biggest challenge is the cultural shift that needs to happen for organizations to become truly data driven. IT has traditionally been the gatekeeper of information and they have been holding data, and the insights from that data, close to their chest. However, for businesses to truly find the benefits of big data, they have to democratize its access. It really needs to be the people who work with the data the most  the doctors, scientists, business analysts, etc.  that have access. It needs to be less of protecting the old way of doing things, just because thats how it has always been done, and more of a new data management process.The learning curve that comes with the advent of any new technology. Businesses for decades have been using traditional and legacy BI technology and now there is an opportunity to transform the process and leverage Hadoops capabilities to get to new insights faster than ever. We have a lot of successful customers in a variety of vertical industries like Citi, Bank of America, AT&T, Vivint, American Airlines, and Cox Auto, just to name a few, that have seen instant results from their big data analytics. Our goal is to show other companies how they can find the same success.AV: How do you envisage the growth of Big Data & Internet of Things (IoT) industry across the world?SG: I think that the IoT phenomenon has the potential to offer unlimited access to new data. Its not the devices per se, but the ability to analyze the data that these devices capture. Were already seeing this in motion  both on an individual level and on a larger, industrial level. For example, the U.S. Womens Olympic Cycling team found their competitive advantage by recording and analyzing their physiological and psychological data. As a result, they went from a five-second deficit at the world championships to earning a Silver medal in the 2012 London Olympics by 8/100th of a second  a triumphant feat that was achieved not only through dedication and athletic ability, but also through enhancing training with insights gained from analyzing big data.At the other end of the spectrum, Fortune 500 energy company is collecting data about the average production of oil, gas, and water from each of its wells and combining it with historical well performance and geospatial data to look at efficiencies and deficiencies based on location and equipment. Extending this knowledge to non-IT users, like production engineers, the company was able to increase production by 60 percent and realized $100 million in incremental revenue.AV: What is coming next from Datameer?SG: We will continue to lower the barrier to entry for people to access insights from their data. I just took a look at the original drawing of the Datameer plans and they are 100 percent the same from when we started a few years ago. It was three boxes  data in, analytics and visualization. We still have a few things to build out, a few boxes we have to tick off that were in the original drawing, but were pretty close. Were on the same path as weve always been on. Thats pretty cool, actually, compared to many other companies that had to find themselves and pivot many times. Were still where we wanted to be.AV: What would be your advice be to aspiring young folks seeking to build their career in data analytics / big data analytics and other related domains? SG: Become a domain expert. As youre looking at your career choices, I think its important to understand what Moores Law and logarithmic growth of knowledge really means. For a Java programmer, its cool to know assembly, but no one is working in assembly anymore. We have entire school programs dedicated to data science. This is cool for the next few years, but it will go away and there will be completely different technology in the future. There will be cool open-source libraries that you just plug in and dont need data scientists anymore. Become a domain expert.Wed like to sincerely thank you for taking your valuable time and letting us know about your uncommon experience. I am sure you will be an inspiration for many budding entrepreneurs.",https://www.analyticsvidhya.com/blog/2015/06/conversation-mr-stefan-groschupf-ceo-datameer/
Tuning the parameters of your Random Forest model,Learn everything about Analytics|Why to tune Machine Learning Algorithms?|What is a Random Forest?|Parameters / levers to tune Random Forests,"1. Features which make predictions of the model better|2. Features which will make the modeltraining easier|Learning through a case study|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|In Conversation with Mr.Stefan Groschupf, Founder and CEO, Datameer|Cheat Sheet for Exploratory Data Analysis in Python|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1.a. max_features:|1.b. n_estimators :|1.c. min_sample_leaf :|2.a. n_jobs :|2.b. random_state :|2.c. oob_score :,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A month back, I participatedin a Kaggle competition called TFI. I started with my first submission at 50th percentile. Having workedrelentlessly on feature engineering for more than 2 weeks, I managed to reach 20th percentile. To my surprise, right aftertuning the parameters of the machine learning algorithm I was using, I was able to breachtop 10th percentile.This is how important tuning these machine learning algorithms are.Random Forest is one of the easiest machine learning tool used in the industry. In our previous articles, we have introduced you to Random Forest and compared it against a CART model. Machine Learning tools are known for their performance.Random forest is an ensemble tool which takes a subset of observations and a subset of variables to build a decision trees. It builds multiple such decision tree and amalgamate them together to get a more accurate and stable prediction. This is direct consequence of the fact that by maximum voting from a panel of independent judges, we get the final prediction better than the best judge.We generally see a random forestas a black box which takes in input and gives out predictions, without worrying too much about what calculations are going on the back end. This black box itself have a few levers we can play with. Each of these levers have some effect on either the performance of the model or the resource  time balance. In this article we will talk more about these levers we can tune, while building a random forest model.Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model. Following are the parameters we will be talking about in more details (Note that I am using Python conventional nomenclatures for these parameters) :There are primarily 3 features which can be tuned to improve the predictive power of the model :These are the maximum number of features Random Forest is allowed to try in individual tree. There are multiple options available in Python to assign maximum features. Here are a few of them :How does max_features impact performance and speed?Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered. However, this is not necessarily true as this decreases the diversity of individual tree which is the USP of random forest. But, for sure, you decrease the speed of algorithm by increasing the max_features. Hence, you need to strike the right balance and choose the optimal max_features.This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.If you have built a decision tree before, you can appreciate the importance of minimum sample leaf size. Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data. Generally I prefer a minimum leaf size of more than 50. However, you should try multiple leaf sizes to find the most optimum for your use case.There are a few attributes which have a direct impact on model training speed. Following are the key parameters which you can tune for model speed :This parameter tells the engine how many processors is it allowed to use. A value of -1 means there is no restriction whereas a value of 1 means it can only use one processor. Here is a simple experiment you can do with Python to check this metric :Output - 1 loop best of 3 : 1.7 sec per loopOutput - 1 loop best of 3 : 1.1 sec per loop%timeit is an awsum function which runs a function multiple times and gives the fastest loop run time. This comes out very handy while scalling up a particular function from prototype to final dataset.This parameter makes a solutioneasy to replicate. A definite value of random_state will always produce same results if given with same parameters and training data. I have personally found an ensemble with multiple models of different random states and all optimum parameters sometime performs better than individual random state.This is a random forest cross validation method. It is very similar to leave one out validation technique, however, this is so much faster. This method simply tags every observation used in different tress. And then it finds out a maximum vote score for every observation based on only trees which did not use this particular observation to train itself.Here is a single example of using all these parameters in a single function :We have referred to Titanic case study in many of our previous articles. Lets trythe same problem again. The objective of this case here will be to get a feel of random forest parameter tuning and not getting the right features. Try following code to build a basic model :Machine learning tools like random forest, SVM, neural networks etc. are all used for high performance. They do give high performance, but users generally dont understand how they actually work. Not knowing the statistical details of the model is not a concern however not knowing how the model can be tuned well to clone the training data restricts the user to use the algorithm to its full potential. In some of the future articles we will take up tuning of other machine learning algorithm like SVM , GBM and neaural networks.Have you used random forest before? What parameters did you tune? How did tuning the algorithm impact the performance of the model? Did you see any significant benefits by doing the same? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/
Cheat Sheet for Exploratory Data Analysis in Python,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Tuning the parameters of your Random Forest model|Beginners Tutorial for Regular Expressions in Python|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The secret behind creating powerful predictivemodels is to understand the data really well.Thereby, it is suggested to maneuver the essential steps of data exploration to build a healthy model.Here is a cheat sheet to help you with various codes and steps while performing exploratory data analysis in Python.We have also released a pdf version of the sheet this time so thatyou can easily copy / paste these codes.You can easily copy / paste these code and keep them handy by downloading the PDF version of this infographic here:Data Exploration in Python.pdf",https://www.analyticsvidhya.com/blog/2015/06/infographic-cheat-sheet-data-exploration-python/
Beginners Tutorial for Regular Expressions in Python,Learn everything about Analytics|Importance of Regular Expressions|What you will learn fromthis article?|What is Regular Expression and how is it used?|What are various methods of Regular Expressions?|Some Examples of Regular Expressions,"re.match(pattern,string): |re.search(pattern,string):|re.findall (pattern,string):|re.split(pattern,string, [maxsplit=0]):|re.sub(pattern,repl,string):|re.compile(pattern,repl,string):|Quick Recap of various methods:|What are the most commonly used operators?|Problem 1: Return the first word of a given string|Problem 2: Return the first two character of each word|Problem 3: Return the domain type of given email-ids|Problem 4:Return date from given string|Problem 5:Return all words of a string those starts with vowel|Problem 6:Validate a phone number (phone number must be of 10 digits and starts with 8 or 9)||Problem 7:Split a string with multiple delimiters||Problem 8: Retrieve Information from HTML file|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Cheat Sheet for Exploratory Data Analysis in Python|Hackathon Problem Description: Do you know whos a Megastar?|
Sunil Ray
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Lets get started!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In last few years, there has been a dramatic shift in usage of general purpose programming languages for data science and machine learning. This was not always the case  a decade back this thoughtwould have met a lot of skeptic eyes!This means that more people / organizations are using tools like Python / JavaScript for solving their data needs. This is where Regular Expressions become super useful. Regular expressions are normally the default way of data cleaning and wrangling in most of these tools. Be it extraction of specific parts of text from web pages, making sense of twitter data or preparing your data for text mining  Regular expressions are your best bet for all these tasks.Given their applicability, it makes sense to know them and use them appropriately.In this article, I will walk you through usage, examples and applications of Regular Expressions. Regular Expression are very popular among programmersand can be applied inmany programming languages like Java, JS, php, C++ etc. For developing ourunderstanding, we have explained this concept using Python programming language. Towards the end, I have solved various problems using regular expressions.Simply put, regular expression is a sequence of character(s) mainly used to find and replace patterns in a string or file. As I mentioned before, they are supported by most of the programming languages like python, perl, R, Java and many others. So, learning them helps in multiple ways (more on this later).Regular expressions usetwo types of characters:a) Meta characters: As the name suggests, these characters have a special meaning, similar to * in wild card.b) Literals (like a,b,1,2)In Python, we have module re that helpswith regular expressions. So you need toimport library re before you can useregular expressions in Python.The most common uses of regular expressions are:Lets look at the methods that library re provides to perform thesetasks.Note: We also have avideo course on Natural Language Processing covering Regular Expressions as well. Do check it out!The re package provides multiple methods to perform queries on an input string.Here are the most commonly used methods, I will discuss:Lets look at them one by one.This methodfinds match if itoccurs at start of the string. For example, calling match() on the string AV Analytics AV and looking for a pattern AV will match. However,if we look for only Analytics, the pattern will not match. Letsperform it in python now.Here is a live coding window to get you started. You can run the codes and get the output in this window itself:Above you can see that start and end position of matching pattern AV in the string and sometime it helps a lot whileperforming manipulation with the string.Itis similar to match() but it doesnt restrict us to find matches at the beginning of the string only. Unlike previous method, here searching for pattern Analyticswill return a match.CodeHere you can see that, search() method is able to find a pattern from any position of the string but it only returns the first occurrence of the search pattern.Ithelps to get a list of all matching patterns. It has no constraints of searching from start or end. If we will use method findall to search AV in given string it will return both occurrence ofAV. Whilesearching a string, I would recommend you to use re.findall()always,it can work like re.search() and re.match() both.CodeThis methods helps to splitstringby the occurrences ofgiven pattern.CodeAbove, we have split the string Analytics by y. Method split() has another argument maxsplit. It has default value of zero. In this case it does the maximum splits that can be done, but if we give value to maxsplit, it will split the string. Lets look at the example below:CodeHere, you can notice that we have fixed the maxsplit to 1. And the result is,it has only two values whereas first example has three values.It helps to search a pattern and replace with a new sub string. If the pattern is notfound,stringis returned unchanged.CodeWe can combine a regular expression pattern into pattern objects, which can be used for pattern matching. It also helps to search a pattern again without rewriting it.CodeTill now,we looked at various methods of regular expression using a constant pattern (fixed characters). But, what if we do not have a constant search pattern and we want to return specific set of characters (defined by a rule) from a string? Dont be intimidated.This can easily be solved by defining an expression with the help of pattern operators (meta and literal characters). Lets look at the most common pattern operators.Regular expressions can specify patterns, not just fixed characters. Here are the most commonly used operators that helps to generate an expression to represent required characters in a string or file. It is commonly used in web scrapping and text mining to extract required information.For more details on meta characters (, ),| and others details, you can refer this link (https://docs.python.org/2/library/re.html).Now, lets understand the pattern operators by looking at the below examples.Solution-1 Extract each character (using \w)CodeAbove, space is also extracted, now to avoid it use \w instead of ..CodeSolution-2 Extract each word(using * or +)CodeAgain, it is returning space as a word because * returns zero or more matches of pattern to its left. Now to remove spaces we will go with +.CodeSolution-3Extract each word(using ^)CodeIf we will use $ instead of ^, it will return the word from the end of the string. Lets look at it.CodeTo explain it in simple manner,I will again go with a stepwise approach:Solution-1Extract allcharacters after @CodeAbove, you can see that .com, .in part is not extracted. To add it, we will go with below code.Solution  2 Extract only domain name using ( )CodeHere we will use \d to extract digit.Solution:CodeIf you want to extract only year again parenthesis ( ) will help you.CodeSolution-1Return each wordsCodeSolution-2Return words starts with alphabets (using [])CodeIn similar ways, we can extract words those starts with constant using ^ within square bracket.CodeWe have a list phone numbers in list li and here we will validate phone numbers using regularSolutionCodeSolutionCodeWe can also use method re.sub()to replace these multiple delimiters with one as space  .CodeI want to extract information from a HTML file (see below sample data). Here we need to extract information available between <td> and </td> except the first numerical index. I have assumed here that below html code is stored in a string str.Sample HTML file (str)Solution:CodeYou can read html file using library urllib2 (see below code).Code// <![CDATA[ require.config({ baseUrl: '/static/', paths: { nbextensions : '/nbextensions', underscore : '/static/components/underscore/underscore-min.js?v=ca26dc8cdf5d413cd8d3b62490e28210', backbone : '/static/components/backbone/backbone-min.js?v=dd2e6c2643968f7932487454302f407d', }, shim: { underscore: { exports: '_' }, backbone: { deps: [""underscore"", ""jquery""], exports: ""Backbone"" } } }); // ]]&gt;In this article, we discuss about the regular expression, methods and meta characters to form a regular expression. We have also looked at various examples to see the practical uses of it. Here I have tried to introduce you with regular expression and cover most common methods to solve maximum of regular expression problems.Did you find the article useful? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/
Hackathon Problem Description: Do you know whos a Megastar?|Predict the category of a working professional,Learn everything about Analytics,"The Dataset||ImportantDetails|Share this:|Like this:|Related Articles|Beginners Tutorial for Regular Expressions in Python|The Hackathon Practice Guide by Analytics Vidhya|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Welcome to the final stage of this contest. If you have reached till here, we assume either you love Analyticsor Amazon  , whichever way it is, go ahead and show what youve got. Just remember, either you will win or you will learn.This hackathon has been designed to help you practice and evaluate your analytics learning and gift yourself the Amazon Voucher worth Rs. 5000(~$100). We have made sure that the difficulty level of this dataset is lower than kaggle, yet challenging. Because, you love challenges. Right?Link: DatasetObjective: The objective of this dataset is to predict the correct category of the working professionals in India.Description:The dataset shows the data of 16,611 working professionals in India. These working professionals belong tovarious categories (Rookie, Champion, Rising Star, Star, Rock Star and Mega Star) . The dataset has been divided into 2 parts: Training(9,887) and Test(6,724) Dataset. Categories of working professionals are given in training dataset. You haveto predict the categories of professionals in the test dataset. Below isdata dictionary to help you understand the variables.Complete rules can be found here",https://www.analyticsvidhya.com/blog/2015/06/dataset-description-megastar/
The Hackathon Practice Guide by Analytics Vidhya,Learn everything about Analytics|Introduction|Modelling Techniques|End Notes,"1. Framework of the Model Building Process||2. Hypothesis Generation|3. Data Exploration and Feature Engineering|1) Logistic Regression|2) Decision Tree|3)Random Forest|4) Support Vector Machine(SVM)|Case Study 1|Case Study 2|Guides:|Text Mining:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Hackathon Problem Description: Do you know whos a Megastar?|Kaggle Competitions: How and where to begin?|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",a) Logit Transformation|Advantages:|Disadvantages:|Advantages:|Disadvantages:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A hackathon is a platform where you get the chance to apply your data science and machine learnin knowledge and techniques. It is a place where you canevaluate yourself by competing against, andlearning from, fellow data science experts.Here is an exclusive guide to help you prepare for participating in hackathons. This guide illustrates the list of important techniques which you should practice before steppinginto the playing ground.Well keep building this guide into a one place exhaustive resource for data science techniques and algorithms.This is how the framework for model building works  you get data from multiple sources and then you perform the extraction and transformation operations. Once your data has been transformed, you apply your knowledge of predictive modeling and business understanding to build predictive models.                         Make sure you always do this in this orderGuides: b) Logit is directly related to OddsGuides:Decision Tree  ExampleTypes of Decision TreesDecision Tree  TerminologyDecision Tree  Advantage and DisadvantagesGuides:Random Forest  Advantages and DisadvantagesGuides:We have a population of 50% males and 50% females. Here, we want to create some set of rules which will guide the gender class for the rest of the population.The blue circles in the plot represent females and the green squares represents male.Males in our population have a higher average height.Females in our population have longer scalp hairs.Text mining is the analysis of data contained in natural language text. Text mining works by transposing words and phrases of unstructured data into numerical values which can then be linked with structured data in a database and analyzed with traditional data mining techniques.Guides:In this guide we talked about various modelling techniques, text analytics and the various stages which are necessary for a perfect model building.",https://www.analyticsvidhya.com/blog/2015/06/hackathon-practice-guide-analytics-vidhya/
Kaggle Competitions: How and where to begin?,Learn everything about Analytics|Introduction|What you will learn in this article?|List of Kaggle Problems|End Notes,"|1. Titanic : Machine Learning from disaster|2.First Step with Julia|3. Digit Recognizer |4. Bag of Words meet Bag of Popcorn|5. Denoising Dirty Documents|6. San Francisco Crime Classification|7. Taxi Trajectory Prediction Time / Location|8. Facebook Recruiting  Human or bot|Case 1 : I have a background of Coding but new to machine learning.|Case 2 : I have been in analytics Industry for more than 2 years, but not comfortable on R / Python|Case 3 : I am good with coding and machine learning, need something challenging to work on|Case 4 : I am a newbie to both machine learning or coding language, but I want to learn|Few hacks to be a fair competition on Kaggle|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|The Hackathon Practice Guide by Analytics Vidhya|Mini Hackathon Launch by Analytics Vidhya, Kolkata Chapter, West Bengal, 14th June 2015|
Tavish Srivastava
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"          Do I have the necessary skills to take part in Kaggle Competitions?Did you ever facethis question? At least I did,as a sophomore, when I used to fear Kaggle just by envisaging the level of difficulty it offers. This fear was similar to my fear of water. My fear of water wouldntallow me totake up swimming classes. Though, later I learnt,Till the moment you dont step into water, you cant make out how deep it is. Similar philosophyapplies to Kaggle. Dont conclude until you try!Kaggle, the home of data science, provides a global platform for competitions, customer solutions and job board. Heres the Kaggle catch, these competitions not only make you think out of the box, but also offers a handsome prize money.Yet, people hesitate to participate in these competitions. Here are some major reasons:I reckon, this issue emanates for Kaggle itself. Kaggle.com doesnt provide anyinformation which can helppeople tochoose the most appropriate problem matching with their skill set. As a result, it has become an arduous task for beginners/intermediates to decide for suitableproblem to begin.In this article, we have opened thedeadlock of choosing the appropriate kaggle problem according to your set of skills, tools & techniques. Here, we have illustrated each kaggle problem with level of difficulty and the level of skills required to solve it.In the latter part, we have defined the correct approach to take up a kaggle problem for the following cases:Case 1 : I have a background of Coding but new to machine learning.Case 2 : I have been in analytics Industry for more than 2 years, but not comfortable on R/PythonCase 3 : I am good with coding and machine learning, need something challenging to work onCase 4 : I am a newbie to both machine learning or coding language, but I want to learnObjective: A classic popular problem to start your journey with machine learning. You are given a set of attributes of passengers onboard and you need to predict who would have survived after the ship sanked.Difficulty levela) Machine Learning Skills Easyb) Coding skills  Easyc) Acquiring Domain Skills -Easyd) Tutorials available  Very comprehensiveObjective: This is a problem to identify characters on Google Street view picture using an upcoming tool Julia.Difficulty level on each of the attributes :a) Machine Learning Skills Easyb) Coding skills Mediumc) Acquiring Domain Skills -Easyd) Tutorial available  ComprehensiveObjective: You are given a data with pixels on handwritten digits and you need to conclusively say what digit is it. This is a classic problem for Latent Markov model.Difficulty level on each of the attributes :a) Machine Learning Skills  Mediumb) Coding skills Mediumc) Acquiring Domain Skills -Easyd) Tutorial available Available but no hand holdingObjective:You are given a set of movie reviews, and you need to find the sentiment hidden in these statement.The objective of this problem statement is to introduce you to Google Package  Word2Vec.It is a fantastic package which helps you convert words into a finite dimension space. This way we can build analogies only looking at the vector. One very simple example is that your algorithm can bring out analogies like : King  Male + Female will give you Queen.Difficulty level on each of the attributes :a) Machine Learning Skills Difficultb) Coding skills Mediumc) Acquiring Domain Skills -Easyd) Tutorial available Available but no hand holdingObjective: You might know about a technology known as OCR. It simply converts handwritten documents to digital documents. However, it is not perfect. Your job here is to use machine learning to make it perfect.Difficulty level on each of the attributes :a) Machine Learning Skills Difficultb) Coding skills Difficultc) Acquiring Domain Skills -Difficultd) Tutorial available NoObjective: Predict the category of crimes that occurred in the city by the bay.Difficulty level on each of the attributes :a) Machine Learning Skills  Very Difficultb) Coding skills  Very Difficultc) Acquiring Domain Skills -Difficultd) Tutorial available NoObjective: There are two problem based on the same datasets. You are given the controllerof a taxi, and you are supposed to predict where is the taxi going to or the time it will take to complete the journey.Difficulty level on each of the attributes :a) Machine Learning Skills Easyb) Coding skills  Difficultc) Acquiring Domain Skills -Mediumd) Tutorial available A few benchmark codes availableObjective: If you have a nag to understand a new domain, you have got to solve this one. You are given the bidding data and are expected to classify the bidder to bot or human. This has the richest data source available out of all problems on Kaggle.Difficulty level on each of the attributes :a) Machine Learning Skills Mediumb) Coding skills Mediumc) Acquiring Domain Skills -Mediumd) Tutorial available No support available as it is a recruiting contestNote: I have not covered the Kagglecontests offering prize moneyin this article as they are all related to a specific domain. Let me know your take on them in the comment section below.We will now look the correct approachfor people havingdifferent set of skill at different stages of lifeto start their Kaggle journey!Step 1:The first kaggle problem you should take up is:Taxi Trajectory Prediction. Reason being, the problem has a complex dataset which includes a JSON format in one of the columns which tells the set of coordinates the taxi has visited. If you are able to break this down, getting some initial estimate ontarget destination or time does not need a machine learning. Hence, you can use your coding strength to find your value in this industry.Step 2: Your next step should be to take up: Titanic. Reason being,you would now already understand how to handle complex datasets. Hence, now is the perfect time to take a shot on pure machine learning problems. With abundance of solutions/scripts available, you will be able to build a good solution.Step 3: You are now ready for something big. Try Facebook Recruiting. This will help you appreciatehow understanding domain can help you get the best out of machine learning.Once you have all these pieces in place, you are good totry any problem on Kaggle.Step 1: You should begin with taking a shot onTitanic. Reason being, you already understand how to build predictive algorithm. You should now striveto learn languages like R and Python. With abundance of solutions/scripts available, you will be able to build different kind of models on both R and Python. This problem will also help you understand a few advanced machine learning algorithms.Step 2: Next step should be Facebook Recruiting. Reason being, given the simplicity of the data structure and the richness of the content, you will be able to join right tables and make a predictive algorithm on this one.This will also help you appreciatehow understanding domain can help you get the best out of machine learning.Suggestions: You are now ready for something very different from your comfort zone. Read problems like Diabetic Retinopathy Detection, Avinto Context Ad Clicks, Crime Classification and find the domain of your interest. Now try applying whatever you have learned so far.Now is the time to try something more complex to code. Try Taxi Trajectory prediction or Denoising Dirty Documents.Once you have all these pieces in place, you can now try any problem on Kaggle.Step 1: You have many options on Kaggle. First option is master a new language like Julia. You can start with First step with Julia. Reason being, this will give you an additional exposure to what can Julia do in addition to Python or R.Step 2: Second option is to develop skills with an additional domain. You can try Avito Context , Search Relevance or Facebook  Human vs. Bot.Step 1: You should begin your kaggle journey withTitanic. Reason being, the first step for you is to learn languages like R and Python. With abundance of solutions/scripts available, you will be able to build different kind of models on both R and Python. This problem will also help you understand a few machine learning algorithms.Step 2: You should then take up:Facebook Recruiting. Reason being,given the simplicity of the data structure and the richness of the content, you will be able to join right tables and make a predictive algorithm on this one.This will also help you appreciatehow understanding domain can help you get the best out of machine learning.Once you are done with these, you can then take up problems as per your interest.This is not a comprehensive list of hacks, but meant to provide you a good start. Comprehensive list deserves a new post by itself:There are multiple benefits I have realized after working on Kaggle problems. I have learnt R / Python on the fly. I believe that is the best way to learn the same. Also interacting with people of discussion forum on various problems will help you get a deeper scoop into machine learning and domain.In this article, we illustrated various Kaggle problems and categorized their essential attributes into the level of difficulty. We also took up various real life cases and elicited the right approach to participate in Kaggle.Have you participated in any Kaggle problem? Did you see any significant benefits by doing the same? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/06/start-journey-kaggle/
"Mini Hackathon Launch by Analytics Vidhya, Kolkata Chapter, West Bengal, 14th June 2015",Learn everything about Analytics,"Introduction|Who should join this meetup?||How much is the fees for this meetup?||Why should you attend this meetup?||Contact|Share this:|Like this:|Related Articles|Kaggle Competitions: How and where to begin?|Data Hackathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 14th June 2015|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People who are currently working in analytics or eager to enter analytics industry usually have one thing in common which isproblem solving abilities.After going through more than 100 hours of trainings on analytics tools and techniques, people still struggle to implement their knowledge to solvereal time business problems. One such platform which offers problems with a handsome prize money is Kaggle. Every working professional in analytics industry aspires to win atleast one kaggle competition to get credibility in his business domain. But, a few succeed in makingsuch brilliant accomplishments .One major reason for failure that we see is the lack of support in analytics community in India. In order to bridge this gap, Analytics Vidhya proudly announcesMiniHackathon where the brains of analytics will fight to solve a kaggle problem. Interesting part is, the people will get completesupport of team of data scientists from Analytics Vidhya.This meetup will be steered by Mr. Kunal Jain, CEO, Analytics Vidhya who also happens to be a Data Scientist, Growth Hacker.Anyone who is keento learn analytics or has already stepped intoanalytics and wishes to upscale his/her knowledge and skills is welcome to join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.Food & Fun would be on the house. Please make sure you bring your laptop & your brain along!1. If you are eagerly waiting for an opportunity to compete with the best data analysts, data scientists and check your level of expertise, this can be an ideal platform for you.2.If you are keen to solve kaggle competitions but have are struggling due to lack of guidance and mentorship, you should attend this.3. If you wish to scale up your level of skills and competencies and wish to learn from the best data enthusiastsin the city, you should attend this.4. Needless to say, if you want to meet and network with like minded people i.e. data analysts, data scientists, data managers, this can be an ideal platform for you.Date: 14th June2015Time: 1:30pm to 6:30pm(IST)Venue:Praxis Business School, City Campus,IMS House, 10 D, HungerFord Street, KolkataFor more information on this event, Register Here >Mini Datathon Launch, Kolkata.For more information & details on these meetups, feel free to drop us a mail at [emailprotected] or you can also reach us at 0124-4264086.",https://www.analyticsvidhya.com/blog/2015/06/mini-hackathon-launch-analytics-vidhya-kolkata-chapter-west-bengal-14th-june-2015/
"Data Hackathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 14th June 2015",Learn everything about Analytics,"Introduction|The Real Story|Who should join this meetup?||How much is the fees for this meetup?|Why should you attend this meetup?||Contact|Share this:|Like this:|Related Articles|Mini Hackathon Launch by Analytics Vidhya, Kolkata Chapter, West Bengal, 14th June 2015|Data visualization guide for SAS|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"After receiving an overwhelming response at Mini Datathon on 17th May 2015, weproudly announce Data Hackathon tofuel your analytics learning appetite. This is how it looked when data science freaks got busy solving the datasets last time(images below). Heres another chance for you to come and be a part of worlds fastest growing analytics community.Having met many industry professionals, beginners, we understand people who are currently working in analytics or eager to enter analytics industry usually have one thing in common which isproblem solving abilities.After going through more than 100 hours of trainings on analytics tools and techniques, people still struggle to implement their knowledge to solvereal time business problems. One such platform which offers problems with a handsome prize money is Kaggle. Every working professional in analytics industry aspires to win atleast one kaggle competition to get credibility in his business domain. But, a few succeed in makingsuch brilliant accomplishments .One major reason for failure that we see is the lack of support in analytics community in India. At Data Hackathon, you will not be left alone to solve any problem.The people will get completesupport of team of data scientists from Analytics Vidhya.This meetup will be steered by Mr. Tavish Srivastava, Co-Founder, Analytics Vidhyawho is currently leading the analytics teamin Citibank.Anyone who is keento learn analytics or has already stepped intoanalytics and wishes to upscale his/her knowledge and skills is welcome to join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.1. If you are eagerly waiting for an opportunity to compete with the best data analysts, data scientists and check your level of expertise, this can be an ideal platform for you.2.If you are keen to solve kaggle competitions but have are struggling due to lack of guidance and mentorship, you should attend this.3. If you wish to scale up your level of skills and competencies and wish to learn from the best data enthusiastsin the city, you should attend this.4. Needless to say, if you want to meet and network with like minded people i.e. data analysts, data scientists, data managers, this can be an ideal platform for you.Date: 14th June 2015Venue:Sigmoid Analytics,Gulmohar Enclave Road, Silver Springs Layout, Munnekollal, BangaloreFor more information on this event, visit here.For more information & details on these meetups, feel free to drop us a mail at [emailprotected] or you can also reach us at 0124-4264086.",https://www.analyticsvidhya.com/blog/2015/06/data-hackathon-analytics-vidhya-bangalore-chapter-karnataka-14th-june-2015/
Data visualization guide for SAS,Learn everything about Analytics|Introduction|1. Doing Comparison|2) Studying relationship|3. Studying Distribution|4) Composition|End Notes:,"a)Bar Chart|b)Column Chart|c) Clustered Bar Chart / Column Chart|Code with PROC FORMAT:|d) Line Chart|e) Bar-Line Chart|a) Bubble Chart|b) Scatter Plot for Relationship|a) Histogram|b) Scatter Plot|a) Stacked Column Chart:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Hackathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 14th June 2015|Cheat sheet: Data Visualisation in Python|
Shuvayan Das
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Lets get started!|Illustration|Code:|Output:|Code:|Output:|> Code explanationfor Bar Chart and Column Chart:|Code:|Output:|Output:|Code:|Output:|Code:|Output:|Data for OS:||Code:|Output:|Code:|Output:|Code:|Output:|Code:|Output:|Code:|Output:|Code:|Output:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A picture is worth a thousand words!In todayscompetitive environment,companies want faster decision making process, thus ensuring they stay ahead in the race.Data Visualisationhelps at two critical stages in data based decision process (as shown in the figure below):In this article, well explore the 4 applications of data visualization and their implementationin SAS. For better understanding, we have taken sample data sets to createthese visualization. Below are the major aspects of data visualization:For purpose of theillustration, we will use a dataset discusstakenfrom the Analytics Vidhya Discuss. The data contains the topic of discussion, category, number of replies to the post and the total number of Views. The data contains the top 20 topics:A bar chart,also known asbar graphrepresents grouped data usingrectangular bars with lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column bar chart.Objective: We want toknow the number of views of each category represented graphically through a bar chart.Column Charts are often self-explanatory. They are simply the vertical version of a Bar Chart where length of the bars equalsthe magnitude of value they represent. Heres a maneuver:Rotate the chart shown above by -90 degrees, itll get converted into a column chart.This type of representation is useful when we want to visualize the distribution of data across two categories.Objective: We want to analyzethe total views of topics in the discussion forum by category and date posted.However, there is a problem with this image, the months are not in chronological order. In order to solve for this, we use PROC FORMAT.A line chart or line graph is a type of chart which displays information as a series of data points called markers connected by straightline segments. A line chart is often used to visualize trends in data over intervals of time  a time series  thus the line is often drawn chronologically. In these cases they are known as run charts.For this illustration,we will be using a data fromPGDBA from IIT + IIM C + ISI vs. Praxis Business School PGPBA.This combination chart combines the features of the bar chart and the line chart. Itdisplays the data using a number of bars and/or lines, each of which represent a particular category. A combination of bars and lines in the same visualization can be useful when comparing values in different categories.Objective:We want to compare the projected sales with the actual sales for different time periods.
Note : The data needs to be sorted by the x-axis variable.A bubble chart is a type of chart that displays three dimensions of data. Each entity with its triplet (v1, v2, v3) of associated data is plotted as a disk that expresses two of the vi values through the disks xy location and the third through its size.  Source: Wikipedia.As we can see,there is a record for which the Sales and Profit are maximum whereas the comparative expenses are less than some other data points.A simple scatter plot between two variables can give us an idea about the relationship between them-linear, exponential etc. This information can be helpful during further analysis.A histogram is a graphical representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable To construct a histogram, the first step is to bin the range of valuesthat is, divide the entire range of values into a series of small intervalsand then count how many values fall into each interval.The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and usually equal size. The rectangles of a histogram are drawn so that they touch each other to indicate that the original variable is continuous.We have used the sashelp.mtcars dataset here.A histogram of the MSRP variable gives us the above figure.This tells us that the variable MSRP is skewed to the right indicating that most of the data points are below $50,000.These kind of simple but meaningful insights can be found out from histograms.In a scatter plot the data is displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.It can be used both to see the distribution of data and accessing the relationship between variables.Note: For Illustration, we will use a dataset discusstakenfrom the Analytics Vidhya DiscussThe SGSCATTER Procedure can also be used for scatter plots.It has the advantage of being able to produce multiple scatter plots. Below is the output using sgcscatter:An importantuse of scatter plot is the interpretation of residuals of linear regression. A scatter plot of the residuals vs the predicted values of the predicted variable helps us in determining whether the data is heteroskedastic or homoskedastic.                         HOMOSKEDASTIC            HETEROSKEDASTICIn astacked bar chart, the stacked bars represent different groups on top of each other. The height of the resulting bar shows the combined result of the groups.For example,if we want to see the total sales per Item grouped by Location across the total data of the OS dataset, we can use the stacked column chart. Below is the illustration:Visualizations become a natural way of understanding data inhuge volume. They convey information in a simplemanner and make it easyto share ideas with others. In this article, we looked at a few basic visualizations that can be done through base SAS. These can be a great way to summarize our data, gain insights, find relationships etc.Did you find this article useful? Are there any other visualisation you have used, which you can share with our audience? Please feel free to share them through comments below.",https://www.analyticsvidhya.com/blog/2015/06/data-visualization-guide-sas/
Cheat sheet: Data Visualisation in Python,Learn everything about Analytics,"Introduction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data visualization guide for SAS|All out beginners guide to MongoDB|
Analytics Vidhya Content Team
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is said A visually presented data speaks for itself. Data, served in the right visual form, brings out hidden trends and insights to enable faster decision making. The importance of right visualization is only set to increase with increasing data.Python, popular for its ease of writing codes, offers some amazing set of libraries support to create visualization. Not only 2D, it has features to create jaw-dropping3Dvisualisations &animations.Here is the cheat sheet for popular visualisation methods used for representing data.You can keep this handy for your use:        Coming up > Data Hackathon Online (Win Rs.5000 Amazon Vouchers)To view this complete article, visit 9 popular ways to perform data visualisation in PythonIf you wish to gain a complete knowledge on data visualisation, heres the ultimate guide on data visualisation.",https://www.analyticsvidhya.com/blog/2015/06/data-visualization-in-python-cheat-sheet/
All out beginners guide to MongoDB,Learn everything about Analytics|Introduction|What can you learn from this guide?|Structural aspects of MongoDB|COMPARISON:Traditional RDBMS vs NoSQL Databases||What are the advantages of using MongoDB ?|What are the limitations of MongoDB?|Installation of Mongo & its admin GUI:|End Notes,"1. Data Model|2. GridFS|3. Sharding|4. Data partitioning|4.Aggregation|5. Indexes|6.Replication|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Cheat sheet: Data Visualisation in Python|Why Business Intelligence Should Be a Piece of the Security Puzzle?|
Shuvayan Das
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",What is a document in Mongo DB?|What are collections in Mongo DB?|a)Normalized Data Models|b) Embedded Data Models|What is a shard key?|a) Range Based Sharding|b) Hash Based Sharding:|What is an Aggregation Pipeline?|a) MapReduce|b) Single Purpose Aggregation Operations|What is a replica?|Setup the MongoDB environment:|Start MongoDB:|Connect to MongoDB:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Necessity is the mother of innovation!This is an old proverb, but it still holds damn good!Last decade has pushed the boundaries of data generation, storage and analysis to an entirely new level. This push towards a digital data driven economy has created its own need. These problems and solutions are typically combined under the umbrella of Big Data.Imagine this  Facebook and Google combined generate more data today, than the entire world would have generated a few years back. With this increase in data generation, comes the problem of data storage and scaling. Allof us want our Facebook feeds to load instantaneously and hate the waiting time  but imagine the architecture you need to deliver that experience. Millions of users making simultaneous queries into your database in real timephew! Add to this the unstructured nature of the data and need of a system, where you can add new features quickly  this would now be looking like an Herculean task.Traditional databases find it hard to cope up with these requirements and the cost of scaling up becomes prohibitive!In this article, well focus on one such innovation in data storage system popularly known as MongoDB. It providesschema-less design, high performance, high availability, and automatic scaling qualities which have now become a need andcannot be satisfactorilymet by traditional RDBMS systems.According to Wikipedia:MongoDB (from humongous) is a cross-platform document-oriented database. Classified as a NoSQL database, MongoDB eschews the traditional table-based relational database structure in favor of JSON-like documents with dynamic schemas (MongoDB calls the format BSON), making the integration of data in certain types of applications easier and faster. Released under a combination of the GNU Affero General Public License and the Apache License, MongoDB is free and open-source software. WikipediaMongoDB is used across severalcompanies inmultiple domains (some of them shown below):In this guide, well start by understandingthebasic structural aspects poweringMongoDB. The idea is to understand how MongoDB works. Specifically, we will look at these aspects:We will also compareTraditional RDBMS vs NoSQL Databases to give you a better understanding of which works better, followed by the advantages and limitationsof MongoDB.Once we have a fair understanding of how MongoDB works, we will provide step by step guide to its installation. In the second part of this series, we will connect MongoDB to our analytics tools to provide ademo. For now, lets start by understanding how MongoDB works.Useful Read: NoSQL Databases explained in simple english!Lets now understand the structural aspects of MongoDB in the order stated above:MongoDB stores data in the form of BSON -Binary encoded JSON documents which supports a rich collection of types. Fields in BSON documents may hold arrays of values or embedded documents. In MongoDB, the database construct is a group of related collections. Each database has a distinct set of data files and can contain a large number of collections. A single MongoDB deployment may have many databases.A record in MongoDB is a document (shown below), which is a data structure composed of field and value pairs. MongoDB documents are similar to JSON objects. The values of fields may include other documents, arrays, and arrays of documents.This is an important differentiation from RDBMS systems where each field must contain only one value.MongoDB stores documents in collections (shown below). Collections are analogous to tables in relational databases. In RDMS all tables in a database must have the same schema, but in MongoDB there is no such requirement. This schema-less design is an innovation which makes MongoDB the most used NoSQL Database. However, documents stored in a collection must have a unique _id field that acts as a primary key.Documents in a collection can be stored either in Normalized for or embedded into another document itself. Lets understand the difference in detail:The relationships between data is stored by links (references) from one document to another (shown below). These references are resolved by the application to fetch the related data.Embedded documents store relationships between data by storing related data in a single document structure (shown below). These denormalized data models allow applications to retrieve and manipulate related data in a single database operation.GridFS is a specification for storing and retrieving files that exceed the BSON-document size limit of 16MB.Instead of storing a file in a single document, GridFS divides a file into parts, and stores each part as a separate document. GridFS uses two collections to store files. One collection stores the file chunks, and the other stores file metadata (shown below).When we query a GridFS store for a file, the client reassembles the chunks as needed. Information can also be accessed from any random section/s of files. This feature is what basically allows for skipping into the middle of a video or audio file.Database systems with large data sets and high throughput applications can challenge the capacity of a single server in multiple ways such as:To address these issues of scale, database systems have two basic approaches:a)Vertical scaling: It adds more CPU and storage resources to increase capacity. But such arrangements are disproportionately expensive. As a result there is a practical maximum capability for vertical scaling.b) Sharding or Horizontal Scaling: By contrast, it divides the data set and distributes the data over multiple servers-shards. Each shard is an independent database and collectively shards make up a single database.MongoDB supports sharding through the configuration of sharded clusters. Process of sharing has been explained in the image below where:MongoDB distributes data at the collection level. Sharding partitions a collections data by the shard key.A shard key is either an indexed field or an indexed compound field that exists in every document in the collection. MongoDB divides the shard key values into chunks and distributes the chunks evenly across the shards. To divide the shard key values into chunks, MongoDB uses either range based partitioning or hash based partitioning.Consider a numeric shard key: If you visualize a number line that goes from negative infinity to positive infinity, each value of the shard key falls at some point on that line. MongoDB partitions this line into smaller, non-overlapping ranges called chunks. It is a range of values from some minimum value to some maximum value (shown below).In a range based partitioning system, documents with close shard key values are most probably in the same chunk, and thus on the same shard.For hash based partitioning, MongoDB computes a hash -A hash value is a numeric value of a fixed length that uniquely identifies data. These values represent large amounts of data as much smaller numeric values of a fields value, and then uses these hashes to create chunks (shown below).With hash based partitioning, two documents with close shard key values are unlikely to be part of the same chunk. This ensures a more random distribution of a collection in the cluster.Aggregations are operations that process data records and return computed results. Unlike queries, aggregation operations in MongoDB use collections of documents as an input and return results in the form of one or more documents. MapReduce is a tool used for aggregating data.An aggregation pipeline is a series of document transformations which are executed in stages. The original input is a collection whereas the output can be a document,cursor or a collection (shown below).The most basic pipeline stages provide filters that operate like queries and document transformations that modify the form of the output document.Other pipeline operations provide tools for grouping and sorting documents by specific field or fields as well as tools for aggregating the contents of arrays, including arrays of documents. In addition, pipeline stages can use operators for tasks such as calculating the average or concatenating a string.MapReduce is a powerful and flexible tool for aggregating data. It can solve problemswhich arecomplex in nature andexpress using the aggregation framework query language.It splits up a problem, sends chunks of it to different machines, and lets each machine solve its part of the problem. When all the machines are finished, all the pieces of the solution are merged back into a full solution.For a number of common single purpose aggregation operations like returning a count of matching documents, returning the distinct values for a field, and grouping data based on the values of a field; MongoDB provides special purpose database commands.All of these operations aggregate documents from a single collection. Though these operations provide simple access to common aggregation processes, they lack the flexibility and capabilities of the aggregation pipeline and MapReduce.Indexes are special data structures that store a small portion of the collections data set in an easy to traverse form. The index stores the value of a specific field or set of fields, ordered by the value of the field.The ordering of the index entries supports efficient equality matches and range-based query operations. In addition, MongoDB can return sorted results by using the ordering in the index. The following diagram illustrates a query that selects and orders the matching documents using an index:Indexes are used for better query performance. They are created on fields which appear often in queries(_id) and for operations that return sorted results. MongoDB automatically creates a unique index on the _id field. Indexes have the following properties in MongoDB:Indexes support the efficient execution of queries. If an appropriate index exists for a query, MongoDB can use the index to limit the number of documents it must inspect.Replication provides redundancy and increases data availability. With multiple copies of data on different database servers, replication protects a database from the loss of a single server allows for recovery from hardware failure and service interruptions.A replica set is a group of mongodb instances that host the same data set. One mongodb, the primary, receives all write operations. All other instances, secondaries, apply operations from the primary so that they have the same data set (shown below).The primary accepts all write operations from clients. A replica set can have only one primary. To support replication, the primary records all changes to its data sets in its oplog (operations log).The secondaries replicate the primarys oplog and apply the operations to their data sets such that the secondaries data sets reflect the primarys data set. If the primary is unavailable, the replica set will elect a secondary to be primary. When a primary does not communicate with the other members of the set for more than 10 seconds, the replica set will attempt to select another member to become the new primary. The first secondary that receives a majority of votes becomes a primary(shown below).Comparing NoSQL andMongoDB is like comparing a Lion with aTiger. Yet,both are predators, one hunts aloneand the other in packs.SQL (tiger) has a rigid data model which needs data to conform to the design of the schema. It is useful for organizing structured data like sales statistics. On the other hand, MongoDB (lion) is a document oriented database, which stores data in the form of documents. Though their approaches are different, both are required for data storage and the selection of the database type depends rather on the organizational need.Useful Read: Basics of SQL and RDBMS  A must have skills for data science professionalAs you can see from the above representation, when the number of queries hitting the server increases, MongoDB is a clear winner. MongoDB is typically used for real-time analytics where latency is low and availability requirements very high.MongoDB has come to the forefront because of the need of organizations to analyze semi-structured, unstructured and geo-spatial data and because the structure of data is rapidly changing in todays world.Traditional RDBMS systems are unable to cope with these demands fully as their inherent structure does not allow them do so.Though changes are being made in RDBMS systems too, to cope with the explosion of data, databases like MongoDB with their document structure are best suited for dealing with todays data.MongoDB has some limitations which are listed below.(Source: www.mongodb.com)Apart from these, prevention of accidental deletion of records due to constraints in RDBMS systems cannot be implemented in MongoDB or other NoSQL systems. Also there might be other problems like the one shown below, for storing multi-layered data without normalization:A user has friends who might be a user himself.People who have liked or commented or both can again be users themselves. This type of duplication makes it way harder to de-normalize an activity stream into a single document.MongoDB also has its fair share of limitations and disadvantages and just like any other technology,with improvements they will be hopefully removed.Follow the 7 steps below andcomplete the installation process of MongoDB:Step 1: Download MongoDB from MongoDBDownload. ClickDownload and save iton your machine. You can alsoselectthe version according to the OS you use.Step 2: In case of Windows, locate the downloaded MongoDB .msi file, which typically is located in the default Downloads folder. Double-click the .msi file. A set of screens will appear to guide you through the installation process.Step 3: MongoDB requires a data directory to store all data. Itsdefault data directory path is\data\db. Create this folder using the following commands from a Command Prompt:By default, this folder gets created in the C: drive.Step 4: Navigate to the bin folder where the mongod.exe file is located and run the following command in the cmd C:\Program Files\MongoDB\Server\3.0\bin\mongod.exe. This should give an output as shown below:The waiting for connections message indicatesMongoDB is running successfully.Notice the part highlighted in white color; if you do not get this message, it means you havent downloadedand installedhotfixprior to running MongoDB.Start 5: To connect to MongoDB, open another command prompt window and type:C:\Program Files\MongoDB\Server\3.0\bin\mongo.exe.Note: The path is the location ofmongo.exefile.This should give the following message in the cmd window(mongo shell):Step 6: Download NoSQL Manager for MongoDB from MongoDBManager. This is much like SQL server management studio and I will use this for the purpose of illustration in the article.Step 7: Click on localhost.This should establish a connection withthe instance of MongoDB and the interface will look like as shown below:More Admin GUI can be found at: mongoDB admin GUIWith this we complete the installation of MongoDB and its admin GUI.Thestructural components of MongoDB like data storage in the form of documents and collections, sharding, replication etc. makes it the most widely used No SQL database today. MongoDB also has APIs for connecting with programming languages like Perl,Ruby,Python and R which further makes it attractive to developers and analysts alike. We will be sharing some of these details in one of the future posts.Did you find this guide useful ?Do let us know your thoughts about this guide in the comments section below.If you want to learn more about MongoDB you can consider Data Wrangling With MongoDB from Udacity. This will require knowledge of Python.References : MongoDB manual.",https://www.analyticsvidhya.com/blog/2015/06/beginners-guide-mongodb/
Why Business Intelligence Should Be a Piece of the Security Puzzle?,Learn everything about Analytics|Introduction|Business Intelligence: Defining Normal |Connecting Security and Business Analytics|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|All out beginners guide to MongoDB|k-Fold Cross Validation made simple|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whenever a business faces a security failure, they turn to their logs, security information and event management (SIEM) software, and security software to tell them what went wrong and when. In most cases, the wealth of data thats available to security teams can pinpoint exactly where the security protocols broke down  and how to fix it.The problem is, its often too little too late. What many security teams fail to realize is that while security analytics are useful after the fact, most businesses also have a wealth of data that they can draw upon to predict an attack before it takes place.Many businesses turn to analytics software like SAS, Python, and R to gain actionable insights into their current state of operations and better align their activities with their strategic priorities. From gaining better insights into customers and sales channel partners to empowering employees to make better, data-driven decisions that improve the overall bottom, the value of business analytical tools cannot be overstated.However, there is one area in which BI software is woefully underused, and thats security. Some experts compare the current state of security in many businesses to that of a governmental strategy room: Security teams are so focused on learning about their enemies, watching their moves, and trying to predict what comes next that they lose perspective on whats happening within their own borders. Instead of focusing on shoring up their defenses from within by identifying something thats out of the ordinary, they are busy trying to stay one step ahead of the outside attackers.This is often because security teams dont always have a complete grasp of what constitutes normal in the business sense. Because most of the data being analyzed by security teams is, understandably, security focused, its only looking at a small subset of the entire picture. They might be able to recognize an attack by a known attacker or the hallmarks of a particular piece of malware, but when events that dont fit pre-existing definitions of risk take place, they are caught unaware.By looking at a wider data set, though, including web traffic, email traffic, customer buying patterns, and other factors, its possible to establish a baseline of what constitutes normal operations and identify anomalies before they take hold. Normal operations create a unique fingerprint for the business, in a sense  and when that fingerprint changes, investigation is warranted.One reason that business intelligence analytics and security analytics, and indeed security as a whole, have remained separate entities is that the processes and tools tend to be separate, with neither group having a full understanding of the other.Bringing together the disparate functions tends to be a major obstacle in many organizations. However, if the security team can reach out to the business intelligence team, not only does that create an organizational bridge that can send positive ripples throughout the organization, but the BI team can provide data analytics insights that might be foreign to the security team.For the same reason, the team that is trained in cyber security can provide insights and guidance that may not be apparent to the rest of the organization. By showing the BI team how to interpret data from a security standpoint, the security analysts can help business analysts better secure data, and identify the changes to the fingerprint that warrant further investigation.Combining BI and security analytics doesnt require the deployment of more tools. Both functions most likely already have a wide range of tools at their disposal, from firewalls and threat protection systems to data collection and analysis programs. Bringing these functions together is making better use of those tools, and leveraging their power in order to both make better business decisions and to protect the data thats collected and vital business functions.While some are concerned that developing the ability to predict attacks before they happen and stop them before the damage is done will have the effect of making security analysts obsolete, experts note that companies will still need knowledgeable individuals to develop and deploy solutions when necessary.The bottom line, then, is that business intelligence should become a larger piece of the puzzle. Its not the only solution to cyber crime, of course, but by more effectively analyzing the data thats already being collected; the puzzle becomes a little less complex.By now, you musthave got an overview of the amazingamalgamation of BI and security analytics. This combination further strengthens thesecurity aspects in an organization thereby ensuring an efficient flow of work and developments.",https://www.analyticsvidhya.com/blog/2015/05/business-intelligence-piece-security-puzzle/
11 Important Model Evaluation Metrics for Machine Learning Everyone should know,"Learn everything about Analytics|Overview|Introduction|Table of Contents|Warming up: Types of Predictive models|1. Confusion Matrix|2. F1 Score|3. Gain and Lift charts
|4. Kolomogorov Smirnov chart
|5. Area Under the ROC curve (AUC  ROC)|6. Log Loss|7. Gini Coefficient|8. Concordant  Discordant ratio|9. Root Mean Squared Error (RMSE)|10. Root Mean Squared Logarithmic Error|11. R-Squared/Adjusted R-Squared|12. Cross Validation|End Notes","|Advantages of using ROC|Adjusted R-Squared|Here is an example of scoring on Kaggle!|The concept : Cross Validation|k-fold Cross validation|How does this help to find best (non over-fit) model?|How do we implementk-fold with any model?|But how do we choose k?|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework|Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science|
Tavish Srivastava
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published in February 2016 and updated in August 2019. with four new evaluation metrics.The idea of building machine learning models works on a constructive feedback principle. You build a model, get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results.I have seen plenty of analysts and aspiring data scientists not even bothering to check how robust their model is. Once they are finished building a model, they hurriedly map predicted values on unseen data. This is an incorrect approach.Simply building a predictive model is not your motive. Its about creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check the accuracy of your model prior to computing predicted values.In our industry, we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model.After you are finished building your model, these 11 metrics will help you in evaluating your models accuracy. Considering the rising popularity and importance of cross-validation, Ive also mentioned its principles in this article.And if youre starting out your machine learning journey, you should check out the comprehensive and popular Applied Machine Learning course which covers this concept in a lot of detail along with the various algorithms and components of machine learning.When we talk about predictive models, we are talking either about a regression model (continuous output) or a classification model (nominal or binary output). The evaluation metrics used in each of these models are different.In classification problems, we use two types of algorithms (dependent on the kind of output it creates):In regression problems, we do not have such inconsistencies in output. The output is always continuous in nature and requires no further treatment.Illustrative ExampleFor a classification model evaluation metric discussion, I have used my predictions for the problem BCI challenge on Kaggle. The solution of the problem is out of the scope of our discussion here. However the final predictions on the training set have been used for this article. The predictions made for this problem were probability outputs which have been converted to class outputs assuming a threshold of 0.5.A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :The accuracy for the problem in hand comes out to be 88%. As you can see from the above two tables, the Positive predictive Value is high, but negative predictive value is quite low. Same holds for Sensitivity and Specificity. This is primarily driven by the threshold value we have chosen. If we decrease our threshold value, the two pairs of starkly different numbers will come closer.In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Sensitivity. Confusion matrix are generally used only with class output models.In the last section, we discussed precision and recall for classification problems and also highlighted the importance of choosing precision/recall basis our use case. What if for a use case, we are trying to get the best precision and recall at the same time? F1-Score is the harmonic mean of precision and recall values for a classification problem. The formula for F1-Score is as follows:Now, an obvious question that comes to mind is why are taking a harmonic mean and not an arithmetic mean. This is because HM punishes extreme values more. Let us understand this with an example. We have a binary classification model with the following results:Precision: 0, Recall: 1Here, if we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a dumb classifier which just ignores the input and just predicts one of the classes as output. Now, if we were to take HM, we will get 0 which is accurate as this model is useless for all purposes.This seems simple. There are situations however for which a data scientist would like to give a percentage more importance/weight to either precision or recall. Altering the above expression a bit such that we can include an adjustable parameter beta for this purpose, we get:Fbetameasures the effectiveness of a model with respect to a user who attaches  times as much importance to recall as precision.Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:Step 1 : Calculate probability for each observationStep 2 : Rank these probabilities in decreasing order.Step 3 : Build deciles with each group having almost 10% of the observations.Step 4 : Calculate the response rate at each deciles for Good (Responders) ,Bad (Non-responders) and total.You will get following table from which you need to plot Gain/Lift charts:This is a very informative table. Cumulative Gain chart is the graph between Cumulative %Right and Cummulative %Population. For the case in hand here is the graph :This graph tells you how well is your model segregating responders from non-responders. For example, the first decile however has 10% of the population, has 14% of responders. This means we have a 140% lift at first decile.What is the maximum lift we could have reached in first decile? From the first table of this article, we know that the total number of responders are 3850. Also the first decile will contains 543 observations. Hence, the maximum lift at first decile could have been 543/3850 ~ 14.1%. Hence, we are quite close to perfection with this model.Lets now plot the lift curve. Lift curve is the plot between total lift and %population. Note that for a random model, this always stays flat at 100%. Here is the plot for the case in hand :You can also plot decile wise lift with decile number :What does this graph tell you? It tells you that our model does well till the 7th decile. Post which every decile will be skewed towards non-responders. Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model. Else you might consider over sampling first.Lift / Gain charts are widely used in campaign targeting problems. This tells us till which decile can we target customers for an specific campaign. Also, it tells you how much response do you expect from the new target base.K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.For the case in hand, following is the table :We can also plot the %Cumulative Good and Bad to see the maximum separation. Following is a sample plot :The metrics covered till hereare mostly used in classification problems. Till here, we learnt about confusion matrix, lift and gain chart and kolmogorov-smirnov chart.Lets proceed and learn fewmore important metrics.This is again one of the popular metrics used in the industry. The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections.Lets first try to understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric.Hence, for each sensitivity, we get a different specificity.The two vary as follows:The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.Lets take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix :As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.Points to Remember:1.For a model which gives class as output, will be represented as a single point in ROC plot.2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.Why should you use ROC and not metrics like lift curve?Lift is dependent ontotal response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift.AUC ROC considers the predicted probabilities for determining our models performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the models capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing butnegative average of the log of corrected predicted probabilities for each instance.Let us calculate log loss for a few random values to get the gist of the above mathematical function:Logloss(1, 0.1) = 2.303Logloss(1, 0.5) = 0.693Logloss(1, 0.9) = 0.105If we plot this relationship, we will get a curve as follows:Its apparent from the gentle downward slope towards the right that the Log Loss gradually declines as the predicted probability improves. Moving in the opposite direction though, the Log Loss ramps up very rapidly as the predicted probability approaches 0.So, lower the log loss, better the model. However, there is no absolute measure on a good log loss and it is use-case/application dependent.Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes certainty of classification into account.Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :Gini = 2*AUC  1Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.This is again one of the most important metric for any classification predictions problem. To understand this lets assume we have 3 students who have some likelihood to pass this year. Following are our predictions :A  0.9B  0.5C  0.3Nowpicture this.if we were to fetch pairs of two from these three student, how many pairs will we have? We will have 3 pairs : AB , BC, CA. Now, after the year ends we saw that A and C passed this year while B failed. No, we choose all the pairs where we will find one responder and other non-responder. How many such pairs do we have?We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the probability of responder was higher than non-responder. Whereas discordant pair is where the vice-versa holds true. In case both the probabilities were equal, we say its a tie. Lets see what happens in our case :AB  ConcordantBC  DiscordantHence, we have 50% of concordant cases in this example. Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the models predictive power. For decisions like how many to target are again taken by KS / Lift charts.RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:RMSE metric is given by:where, N is Total Number of Observations.In case of Root mean squared logarithmic error, we take the log of the predictions and actual values. So basically, what changes are the variance that we are measuring. RMSLE is usually used when we dont want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.We learned that when the RMSE decreases, the models performance will improve. But these values alone are not intuitive.In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of 0.5. So the random model can be treated as a benchmark.But when we talk about the RMSE metrics, we do not have a benchmark to compare.This is where we can use R-Squared metric. The formula for R-Squared is as follows:MSE(model): Mean Squared Error of the predictions against the actual valuesMSE(baseline): Mean Squared Error of mean prediction against the actual valuesIn other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formulafor adjusted R-Squared is given by:k: number of featuresn: number of samplesAs you can see, this metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases.If R-Squared does not increase, that means the feature added isnt valuable for our model. So overall we subtract a greater value from 1 and adjusted r2, in turn, would decrease.Beyond these 11 metrics, there is another method to check the model performance. These 7 methods are statistically prominent in data science. But, with arrival of machine learning, we are now blessedwith more robust methods of model selection. Yes! Im talking about Cross Validation.Though, cross validation isnt a really an evaluation metric which is used openly tocommunicate model accuracy.But, the result of cross validation provides good enough intuitive result to generalize the performance of a model.Lets now understand cross validation in detail.Lets first understand the importance of cross validation. Due to busy schedules, these days I dont get much time to participate in data science competitions. Long time back, I participated in TFI Competition on Kaggle. Without delving into my competition performance, I would like to show you the dissimilarity between my public and private leaderboard score.For TFI competition, following were three of my solution and scores (Lesser the better) :You will notice that the third entry which has the worstPublic score turnedto be the best model on Private ranking. There were more than 20 models above the submission_all.csv, but I still chose submission_all.csvas my final entry (which really worked out well). What caused this phenomenon ? The dissimilarity in my public and private leaderboard is caused by over-fitting.Over-fitting is nothing but when you model become highly complex that it starts capturing noise also. This noise adds no value to model, but only inaccuracy.In the following section, I will discuss how you can know if a solution is an over-fit or not before we actually know the test results.Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.Above diagram shows how to validate model with in-time sample. We simply divide the population into 2 samples, and build model on one sample. Rest of the population is used for in-time validation.Could there be anegativeside of the above approach?I believe, a negative side of this approach is that we loose a good amount of data from training the model. Hence, the model is very high bias. And this wont give best estimate for the coefficients. So whats the next best option?What if, we make a 50:50 split of training population and the train on first 50 and validate on rest 50. Then, we train on the other 50, test on first 50. This way we train the model on the entire population, however on 50% in one go. This reduces bias because of sample selection to some extent but gives a smaller sample to train the model on. This approach is known as 2-fold cross validation.Lets extrapolate the last example to k-fold from2-fold cross validation. Now, we will try to visualize how does a k-fold validation work.This is a 7-fold cross validation.Heres what goes on behind the scene : we divide the entire population into 7 equal samples. Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). Then, at the second iteration we train the model with a different sample held as validation. In 7 iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power. Once we have all the 7 models, we take average of the error terms to find which of the models is best.k-fold cross validation is widely used to check whether a model is an overfit or not. If the performance metrics at each of the k times modelling are close to each other and the mean of metric is highest. In a Kaggle competition, you might rely more on the cross validation score and not on the Kaggle public score. This way you will be sure that the Public score is not just by chance.Coding k-fold in R and Python are very similar. Here is how you code a k-fold in Python :This is the tricky part. We have a trade off to choose k.For a small k, we have a higher selection bias but low variance in the performances.For a largek, we have a smallselection bias but highvariance in the performances.Think of extreme cases :k = 2 : We have only 2 samples similar to our 50-50 example. Here we build model only on 50% of the population each time. But as the validation is a significant population, the variance of validation performance is minimal.k = number of observations(n) : This is also known as Leave one out. We have nsamples and modelling repeated n number of times leaving only one observation out for cross validation. Hence, the selection bias is minimal but the variance of validation performance is very large.Generally a value of k = 10 is recommended for most purpose.Measuring the performance on training sample is point less. And leaving a in-time validation batch aside is a waste of data. K-Fold gives us a way to use every singe datapoint which can reduce this selection bias to a good extent. Also, K-fold cross validation can be used with any modelling technique.In addition, the metrics covered in this article are some of the most used metrics of evaluation in a classification and regression problems.Which metric do you often use in classification and regression problem ? Have you used k-fold cross validationbefore for any kind of analysis? Did you see any significant benefits against using a batch validation? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/05/k-fold-cross-validation-simple/
Principal Data Scientist  JyotKiran Networks Private Limited  Delhi/ Gurgaon/ Noida  ( 2-3+ Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|k-Fold Cross Validation made simple|Data Science Faculty (Full-Time)  Edvancer Eduventures  Mumbai  (2-8 Years of experience)|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Principal Data ScientistLocation  Delhi/Gurgaon/NoidaAbout employer  JyotKiran Networks Private LimitedJob description: Responsibilities:Qualification and Experience:The candidate will have a Ph.D. or a Masters in Computer Science, Physics, Applied Mathematics, Statistics or Machine Learning.Skills Required:Applicants should have strong knowledge of Machine Learning Techniques (Supervised Learning, Unsupervised Learning and Semi-Supervised Learning), Machine Learning Paradigms (Decision Trees, Support Vector Machines, Bayesian Approach and Logistic Regression).Experience with NoSQL, Hadoop, with statistical languages and packages (SAS, R, Matlab etc.) and strong programming skills in C++, Python are a requirement.Applicants should have the ability to work independently as well as collaboratively within a team.",https://www.analyticsvidhya.com/blog/2015/05/principal-data-scientist-jyotkiran-networks-private-limited-delhi-gurgaon-noida-2-3-years-experience/
Data Science Faculty (Full-Time)  Edvancer Eduventures  Mumbai  (2-8 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Principal Data Scientist  JyotKiran Networks Private Limited  Delhi/ Gurgaon/ Noida  ( 2-3+ Years of experience)|Moving into analytics after a break in career? Dont expect a rosy land!|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Faculty/Trainers/SMEsLocation  MumbaiAbout employer www.edvancer.inJob description: Responsibilities:Qualification and Experience:Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/data-science-faculty-full-time-edvancer-eduventures-mumbai-2-8-years-experience/
Moving into analytics after a break in career? Dont expect a rosy land!,Learn everything about Analytics,"The challenges in making a career transition:|Guidelines to improve your chances of getting a break|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage|Share this:|Like this:|Related Articles|Data Science Faculty (Full-Time)  Edvancer Eduventures  Mumbai  (2-8 Years of experience)|Infographic: Quick Guide on SAS vs R vs Python|
Kunal Jain
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Challenges during learning the subject|Challenges after completion of some learning / training|Define your objective and the path:|Define your learning schedule:|Execution of learning plan:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lets look at Vinitas (name changed) story:After completing her M.Sc. in Statistics, Vinita worked as a call center executive for more than threeyears. Two year back, she had to take a break from career to take care of her child. Now the child is 2 years old and Vinita is thinking of taking up a job. She has been reading about analytics and the potential it holds as a career in coming years. She has always loved maths & stats and thought this was her calling. She makes up her mind to pursue analytics as a career.A yearlater:Vinita has undergone a few training and open courses. She has applied to more than 100 jobs after that and hasnt got many calls. The ones she got never moved ahead after the first round. She is feeling helpless and re-thinking about her efforts over the last year. What is going wrong?Can you relate to the problem at hand? A lot of people (especially women) take a 1  3 year break for various reasons during their career. Common reasonsbeing family commitments or pursuing entrepreneurship.Lately, I have come across quite a few people who want tochange their domain to analytics / data science after the break. It is a good progression, if you have been in generic roles in past. All the researchabout shortage of data science professionals acts as a catalyzer in making that decision. However, majority of people end up making no head way in the right direction, despite putting in a lot of hard work.The good news is that once you make a transition, there are a lot of opportunities which come your way. It is just that the industry is hesitant to offer roles to people with no analytics experience.How can a person without past experience make a break into analytics? Ill try and give my perspective to this question in this article. But, before getting to the answer, let us understand the challenges which come in way of a transition.You face these challenges when you start learning the subject. This is just the start of the journey.1. Data Science and statistics arenot easy subjects If you dont have a flair for statistics or quantitative subjects, you should just stop your journey here. Each person has his / her own strengths and if quant subjects are not one of them, this stream is not for you. It is surprising how many people ask Do I need to be good at stats in order to be a good analyst?. If you face this question in your mind, you are better not pursuing this venture.2. Even if you understand it, there is a lot of dirty work you need to do  For the fortunate ones who have a flair for quant subjects, only skill means nothing in a data science career. You need to put in a lot of hard work. So, if you are undergoing a MOOC, with out taking the exercises, you are setting yourself up for a tough ask. Without the exercises and experience of cleaning datasets, you can not expect companies to make you an offer. So, if you understand the subject  practice and practice a lot!3. The number of tools out there is overwhelming  As a beginner, a lot of people are usually confused about which tool to follow. They would ask whether to learn SAS or R or Python, even when they have started journey on one. Hadoop and Spark also sound interesting. Oh,did I mention Matlab and Octave used in the popular Machine Learning course? This confusion can be devastating to people  the debates in industry do not help. It takes a lot of time and counselling to help people understand that success as a data scientist is not about learning a tool, but learning and applying a concept. Any tool they pick up is a good starting point (Of course, I am assuming they are not trying to do data science in Fortran!)If you thought start was difficult, this is going to get more difficult!4. No one is ready to give you a break  About 90% of hiring in industry (at least in India) happens from two channels experienced lateral hires or freshers from top tier institutes. So, if you dont belong to any of this category (which is likely the case, if you are transitioning), you will feel that you are against a wall.5. It is not about the tool or training or certification  A lot of people think that once they are certified professionals of a tool, life would be easy. Sorry to wake you up, that would not be the case. Hiring of candidates never happens on the basis of tool or training you know. It usually happens through test of problem solving and structured thinking  through case studies, role plays and guess estimates. Not many trainings prepare you for them.6. Communication skills matter more than you think  People dont associate communication skills with rejection in data science roles. They expect that if they are good in technical part of the subject, they will ace the interview. Sorry! doesnt happen. Ever been rejected with in an interview, where the interviewer said thank you after listening to your introduction? or over a telephonic call? Make your friend with good communication skills hear your intro and ask for honest feedback!There can be a few challenges after you get your first job, but we will reserve them for some other post.Now that you understand the challenges and would have seen which ones would apply to you, here are ways to improve your chances of getting a break:1. Be clear about who you want to become  There can be a lot of varied roles in data science industry. A MIS professional, data visualization expert, predictive modeler, machine learning expert, data scientist are all roles and words with overlapping functions. Getting into a reporting role would be easier than becoming a predictive modeler than a machine learning expert. So, until and unless you are clear about what / who you want to become, you will stay confused about the path to take and skills to hone.What to do, if you are not clear about the differences or you are not sure what should you become? Talk to people in industry, take mentorship from a few people  data science community loves people asking questions. One of the person I know spent an hour with each of these professionals2. Make your tool selection and then stick with it  I have mentioned before that hiring does not happen on the basis of your knowledge about the tool. Then why am I saying to make a tool selection? Because, you will need one tool to learn and implement the concepts. As long as it is one of the mainstream tools, just go ahead with it. Dont over analyze SAS vs. R or Python.3. Give it at least an year, two in case you are not from Tier I institutes Please understand the transition into anlaytics roles will not happen overnight. You need to give yourself time to learn new tools and skills, apply them to a few problems, improv eon your understanding and get a break. If you dont have resources to learn for this long, take up some work on the side. For women returning from break due to children, you can start picking up things as soon as the child starts a play school. It gives you a gradual transition.4. Decide how much time will you be able to provide daily / weekly?Make sure you get into daily habit of learning and practicing. Regular practice will give you a lot of confidence and makes a lot of mundane essential tasks (e.g. importing libraries or packages, syntax, formats) a cakewalk during technical interviews.5. Identify the right courses and training  Look for the reviews, past placement rates, the time requirements from the courses you are taking. If this information is not available in open, talk to people who have undergone this course or mentors in industry and take their opinion. Remember that each person has his / her own learning style  some would want lucid material and exercises to start, others would want challenges. Know what you want and pick those courses which offer that style.6. Define milestones and commit to them  Once you know the courses to complete, create a learning path with milestones and deadlines. Regularly check yourself against the plan and make sure you are looking at the larger picture and the progress.7. Focus on practical applications: While undergoing courses and trainings, focus on the practical applications of things you are learning. Make sure you do exercises and assignments to understand the applications. Work on a few open data sets and apply your learning. Even if you dont understand the math behind a technique initially, understand the assumptions, what it does and how to interpret the results. You can always develop a deeper understanding at a later date.8. Showcase your work in analytics networks and communities  Create GitHub profile and upload your work, participate in Analytics Vidhya Discuss, take part in Kaggle competitions and follow the discussions, become part of relevant groups on Linkedin, blog about your work and perspective.9. Network heavily  Attend industry events and conferences, popular meetups in your area, participate in hackathons in your area  even if you know only a little. You never know who, when and where will help you out!10. Prepare flawlessly for your interview  What is the use of all this hard work, if you screw up the interview. If you prepare well, analytics interviews can be easy to crack. Read our definitive guide, case studies, puzzles and guess-estimates and plan well before the interview.I hope I have done justice to removeconfusion from mind of people making a transition into analytics. In summary, people coming back from a break might find it a bit overwhelming, but the sooner you get into action, the better it is. The good news is that once you make a successful transition, you see opportunities chasing you around!Do you have any more tips to help people making a transition? Please share them through comments below.If you have any more questions on the topic, feel free to shoot them in our discussion portal.",https://www.analyticsvidhya.com/blog/2015/05/moving-analytics-break-career-expect-rosy-land/
Infographic: Quick Guide on SAS vs R vs Python,Learn everything about Analytics,"Introduction|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Moving into analytics after a break in career? Dont expect a rosy land!|Case study  Building and implementing a predictive model in 3 days|
Analytics Vidhya Content Team
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Note: You can read the comprehensive version of this article here.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the perennial points of debate in data science industry has been  Which is the best tool for the job?. Traditionally, this question was raised for SAS vs. R. Recently, there have been discussions on R vs. Python.A few decades back, when R / SAS launched, it was difficult toenvisage the possibilities future willoffer. And this turned out to be a blessing in disguise. Because, itmade easy for them to focus on one tool!But today ? The situation is different. Even before deciding what technique they should apply, they fall into the pit of searching for the best tool to perform that particular task. And finally, they get nothing out of it.The honest answer is that there is no universal winner in this contest. Each tool has its own strength and weakness. A prudent datascientist would diversify his / her repository of tools and use the one appropriate in each situation. In order to do this, it is critical to know the strengths and weakness of each tool, which is what this infographic offers.By now, you must have realized, there is no clear winner in this race. Every tool has its own importance and own strength areas. These strength areas provide them the leverage to survivein industries and hence factors defined in the infographic plays a significant role in their evaluation.",https://www.analyticsvidhya.com/blog/2015/05/infographic-quick-guide-sas-python/
Case study  Building and implementing a predictive model in 3 days,Learn everything about Analytics,"Where it all started?|The idea:|Available Data|Data Challenges & Solution|Insights|The hustle up to Saturday!|Go live and after-effects|End Notes and Disclosures:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Infographic: Quick Guide on SAS vs R vs Python|Launching Analytics Professional Salary Test, India|
Kunal Jain
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Experience:|Skills:|College Type:|Location:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We launched Analytics Professional salary test last week and got awesome response from our audience. People loved it and shared it across social media channels. We got a few requests from people outside India to create some thing similar for other geographies.Given the response, I thought it would be interesting to share the story about creating this web application.Followingreasons make it a very interesting read:Less thana week ago, Sunil, Sahil, Manish & I were sitting and sipping our chai (Indian white tea) in scorching Delhi heat. A walk to the tea stall after our lunch is almost a part of our daily routine now!Manish took his sip and then looked at Sunil & me with his trademark smile. The look in his eyes told us that he was about to give a suggestion (in form of a question)  this was typical Manish.Only this time, we would implement and go live with his idea in next 3 days!So, what was the idea? Manish said We blog awesome articles onanalytics and have some of the best experts in our team. Why dont we create a case study based on the data we have with us? By the time we walked back to our hack room (i.e. our office), a new idea was already taking shape:We had more than 17,000 data points and profiles of people in India (including their salaries), whom we have interacted with in last one year. How about using this information to find insights in data science industry?After a few minutes of huddle / brainstorming at the Gyaan-board (the white board), we finalized the idea:With the data we already have, we will build a web application which could predict the salary of an analytics professional based on a few inputs he / she provides.Our query into the database showed us that there were 17,413 data points with more than 30 variables in total. This data was related to variousprofessionals related to Data science / Big Data / Machine Learning, Business intelligence and other domains.A closer look into these variables showed us that only about half of the variables were good enough for modelling perspective. This is because we removed all the sensitive data like contact details, Date of birth and variables where more than 30% of information was missing.The available variables could be classified in following classes:There were some additional variables, we could pull from more sources like the interaction these people had on Analytics Vidhya, their level of involvement in learning based on interactions with us, GitHub profile details, Linkedin profile details. We agreed to use these variables for a later build. For now, we were looking at implementing something quick and see, if our audience loves it.Forthose of you, who are thinking that the data was clean and structured, let me warn you! The dataset had a lot of challenges. Here are a few of them, just to provide an overview.After doing all the data exploration or munging, we used regressiontree as a modeling technique. We could have looked at other techniques as well, but regression trees are easy to implement (more on this later) and we could implement them in our wordpress setup without a lot of modifications (Tip: Keep implementation of a project in mind from start). Here are a few insights which came out of the analysis:As expected, higher experience has higher compensation. But, if you look at the distribution mix of work experience, it also shows the average vintage of this domain is near about ~5 years.Below visualization clearly shows that as you upgrade your skills, better compensation is waiting for you so its time to upgrade your skills. You can follow learning path of Python, R, SAS and Qlikview here.Have you graduated from Tier1 colleges? Here is some good news for you. The heat map below shows the willingness of analytics companies to pay premium for talent from Tier 1 institutes.Mumbai and Kolkata have slightly better compensation compared to other top 5 cities. But, if look at the distribution, it clearly shows the penetration of industries is more in Mumbai, Delhi NCR and Bangalore.For the die-hard statisticians, some of the graphs above warrant confidence intervals along with these disclaimers. These plots were created to understand the trends rather than reaching conclusions. Also, the sample size was large, so the intervals would be small compared to the variation we are seeing here.Once we had the decision tree ready, we quickly validated it on a smaller dataset. We found the model to provide right classification in 70% of the cases  not bad given the amount of dirty cuts, we had made in last 1.5 days.This was Thursday afternoon! We had a basic model ready, we did some tweaks to the model in next half a day, but it was a good draft. We decided that we would make this test live on Saturday morning because a lot of job search happens over the weekend. This meant we had less than 1.5 days for all other preparations. Over the next 36 hours, we did the following:Thankfully, we pulled it all together over a few Pizzas and burgers. Our interns extended help for model implementation and to finish all the required testing. The test went live on Saturday morning and reached out to more than 15,000 people over the weekend  not a bad reach for a 3 day hack! We spent most of the Saturday cleaning up the corners we had cut during the process and celebrated the achievement over a movie and dinner in the evening.This is the fastest turn-around I have done on a predictive model  this is what we love about the start-up life. Here is what our Facebook walls looked like over the weekend:While Sahil and Manish were unwinding, Sunil & I were conducting a Hackathon  action never stops!We loved the experience of creating something like Analytics Professional Salary test from scratch in a period of 3 days. It wasnt easy  there were times, when we thought we are pushing too much, but then we did!Is the app perfect? The answer is no. Here are some disclosures. I think there are still areas of improvement. I have a hypothesis that the test is under-predicting because the salary data is coming from people who were searching for jobs and low salary could be one of the reasons for the lookout. But even with its limitations, I think we have created a unique, one of its kind app here. I would love to hear your thoughts on what you think about the app and what additional features would you want it to have. Looking forward to it.",https://www.analyticsvidhya.com/blog/2015/05/created-analytics-professional-salary-test/
"Launching Analytics Professional Salary Test, India",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Case study  Building and implementing a predictive model in 3 days|In Conversation with Mr. Sanjeev Mishra, CEO, Convergytics|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ill keep it short.We arepleased to launch Analytics professional salary test, India  a test, which you can take and see what is the average salary for professionals like you.Just a few things to highlight:Here is a link to Terms & conditions for the testI think this is the first time anyone has created a model / test like this based on so many data points  the only people we can thank is our dear audience! It would not be possible with out them.I hope you will enjoy this awesome survey  an application of predictive modeling for analytics professionals!Do let us know your thoughts & experience. If you like what we have done, please share this survey on social media! NOW!",https://www.analyticsvidhya.com/blog/2015/05/launching-analytics-professional-salary-test-india/
"In Conversation with Mr. Sanjeev Mishra, CEO, Convergytics",Learn everything about Analytics,"Introduction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Launching Analytics Professional Salary Test, India|Getting smart with Machine Learning  AdaBoost and Gradient Boost|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is usually interesting to meet business leaders and hear their views on whether they perceive analytics as a cost center or profit center? While most of the leaderswould want their analytics practiceto be a profit center, a lot of companies struggle to make it into one!Convergyticsaims to help organizations make this journey by delivering high impact analytics.Recently, we got a chance to meet Mr. Sanjeev Mishra, CEO of Convergytics. This meet turned into an amazing discussionon various concepts of analytics and its ever increasing usability across various industries.Below is the transcript of this inspirationalconversation:AV(Analytics Vidhya): Thanks Sanjeev for devoting time to us! To begin with, what made you embark on this journey of exploring analytics?SM(Sanjeev Mishra): Convergytics, as an organization is geared to fill the gap between creation and usage of analytics. We as a company believe in the creation of analytics or business solutions while keeping a focused view on how the solution will be consumed across business processes and having a framework for measuring the impact to correct for future actions. At Convergytics we have frameworks to fuse multiple data sources at each stage of Creation, Consumption and Correction of Analytics which not only helps to understand the customer better but drives sustainable growth.Our VisionToenable & empower organizations to get the bigger picture of their businesses through Big Data. And in the process to transform analytics from a cost-center to a profit-center by offering high-impact analytics that deliver value.Our MissionTohelp organizations adapt to the constantly changing market landscape by going beyond traditional analytics to make actionable data-driven decisions and achieve decision-driven ROI.AV: Getting into a startup is always full of good stories. Tell us about your initial days? What were the challenges you faced initially?SM: It has been an enriching experience. I still remember the days when we use to work from a Garage office in Randhirs (co-founder) home. There were days when we will wait for that one call from a prospective client for the whole day. With every ring, we hadour adrenalin rushing just to realize that it was a call for credit card marketing. Moreover, we never wenttogether from the office since we did not want to miss that ONE call. It was at times frustrating but then it also taught us the value of persistence and patience.AV: Tell us about your products & services? How do you plan to position your products in the market?SM: We do not want to be slotted as a product or a services company. We believe we are here to provide solutions that could be either or a hybrid. Most of the solutions that we have are a combination of services such as for creation of analytics and measurement of the impact / ROI, tools and simulators, accelerate consumption during implementation as self help tools for the clients.We believe we are here to provide solutions that could be either or a hybrid. Most of the solutions that we have are a combination of services such as for creation of analytics and measurement of the impact / ROI, tools and simulators, accelerate consumption during implementation as self help tools for the clients.We focus on domains like Marketing, CRM, Pricing and Merchandising, Social media and Web Analytics. Some of the solutions we offer are:AV: I see you serve diverse industries such as CPG, Retail, Healthcare, Banking and Telecom. Which industry has the highest penetration of analytics and which one is evolving?SM: Retail, Telecom and Banking have been the pioneers of analytics globally. Most of the players in these industries are quite mature from analytics perspective and have in-house analytics teams. They are also complemented with offshore models adopted by many analytics companies in India today. This has happened because traditionally, the existence of analytics has been subject to data availability. But with increased pressure on bottom lines, industries where we do not have data have also been looking at analytics to improve ROI.So industries like CPG, Media, Production houses etc are the new evolving industries. We have conceptualized and created solutions that use lot of publicly available data, syndicated data as well as internal client data that can be used to solve various business problems. Forexample, we have used Social media data quite smartly to create the measure of brand equity in a competitive environment. This enabled the clients to make best use of social data at their disposal even though they might not be very rich in customer data.AV: How do you plan to expand? Will you target more industries or you will focus on expanding your product verticals?SM: As I mentioned earlier, our vision is to move Analytics from being perceived as a cost center to a profit center within organizations. So we do not want to be a Factory of churning statistical models. Our plans are to provide ROI based analytical solutions to the clients. For this, we will continue to focus on industries like Retail, CPG, Media and hospitality but increase the depth with each client. We want to be seen as Business enablers and not just analytics vendors.AV: How do you see the growth of analytics in coming years in India and globally?SM: USA has been the biggest consumer of analytics. Other countries are catching up fast. Europe is a big market but has lot of entry barriers like cost, language, local presence etc. Australia, Singapore and other SE countries like Indonesia, Philippines and Malaysia are more lucrative and evolving. India is still a product based market. It will take some time for India to evolve and embrace analytics. There are very few organizations who have jumped on the Analytics wagon and are reaping the benefits. In my view others will catch up sooner or later.AV: I see you are in a hiring mode right now. What set of skills & competencies you look for in an ideal candidate for data analytics job?SM: We look for few things when we hire, and in that priority:Any candidate that has the inquisitiveness to understand business processes and ask questions on Why we are doing what we are doing is an ideal candidate. In my view there has been too much of hype created around analytics being a field for statisticians and mathematicians. I do not agree with this. Math and statistics are tools. At the end, the client will not be happy if we are not able to give direction to their business, no matter what Rocket science methodology we use.Also See: Comprehensiveguide to prepare for analytics interviewsAV: What would be your advice to aspiring young folks seeking to build their career in data analytics/big data analytics and other related domains?SM: My advice for those who aspire to be a part of this wave is:Analytics is NOT a Math shop. Those who want to build their careers in analytics should realize that understanding domain and business application of solutions is more important. I have seen lot of potentially good candidates who lose the edge because they think statistics is the be all and end all of Analytics. Remember, all the tools and techniques are just tools and anyone with bit of training can use them. If you want to have the edge over others, you should be able to translate numbers into actions for the business and show the clients how to improve ROI/ Revenue.Also See: All you need to know to start a career in analyticsAV: Thanks Sanjeev for interacting with us. It was indeed a pleasure for ourselves to get acquainted with your life experiences & learning lessons. We wish you all the best for your start up and the following future endeavors.P.S  We have more such interesting interviews coming up ! Do subscribe.",https://www.analyticsvidhya.com/blog/2015/05/conversation-mr-sanjeev-mishra-ceo-convergytics/
Getting smart with Machine Learning  AdaBoost and Gradient Boost,Learn everything about Analytics,"Introduction|Where areBoosted algorithms required?|What are Classifier Boosting Algorithms ?|Brief introduction to Regression boosters|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|In Conversation with Mr. Sanjeev Mishra, CEO, Convergytics|Senior Data Analyst  Marketelligent -Bangalore  (2-5 Years of experience)|
Tavish Srivastava
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",More real life examples!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine Learning algorithms are like solving a Rubik Cube. Yougrapple at the beginningto figure out the hidden algorithm, but once learnt, somecan even solve it in less than 7 seconds.Suppose, you are stuck in following situation:Youare served with legion ofdata to generate useful insights. To nail this challenge, you gotyour best team members lined up or you come forwardto lead from the front. You beginwith building a predictivemodel. Then, you check the output statistics and gets disheartened. Reason being, the predictive power of the model is very low. Now, you desperately want to figure a way to increase the predictive power.What will you do?(Share your answers in the comment section below)There are some machine learning engines. These engines make use of certain algorithms and help user reach to the output stage. Some of the most popular engines are Decision Tree and Regression.In this article, well introduce you to some of the best practices used toenhance power of these engines to achievea higher predictability usingan additional booster.Boosted algorithms are used where we have plenty of data to make a prediction. And we seekexceptionally high predictive power. It is used to for reducing biasand variance in supervised learning. It combines multipleweak predictors to a build strong predictor.If you ever want to participate in Kaggle competitions, I would suggest that you bookmark this article. Participants in Kaggle completitions use these boosting algorithms extensively.The underlying engine used forboosting algorithms can be anything. For instance, AdaBoost is a boosting done on Decision stump. There are many other boosting algorithms which use other types of engine such as:1. GentleBoost2. Gradient Boosting (Always my first choice for any Kaggle problem)3. LPBoost4. BrownBoostPerhaps, I can go on adding more engines to this list. But, Iwould like tofocus on these five boosting techniques which are the most commonly used. Lets first learn about  AdaBoost.Classification problem is the one where we need to assign every observation to a given set of class. The easiest classification problem is the one with binary class. This problem can be solved usingAdaBoost. Lets take a very simple example to understand the underlying concept of AdaBoost. You have two classes : 0s and 1s. Each number is an observation. The only two features available is x-axis and y-axis. For instance (1,1) is a 0 while (4,4) is a 1. Now using these two features you need to classify each observation. Our ultimate objective remains the same as any classifier problem : find the classification boundary. Following are the step we follow to apply an AdaBoost.Step 1 : Visualize the data: Lets first understand the data and find insights on whether we have a linear classifier boundary. As shown below, no such boundary exist which can separate 0s from 1s.Step 2 : Make the first Decision stump :You have already read about decision trees in many of our previous articles. Decision stump is a unit depth tree which decides just 1 most significant cut on features. Here it chooses draw the boundary starting from the third row from top. Now the yellow portion is expected to be all 0s and unshaded portion to be all 1s. However, we see high number of false positive post we build this decision stump. We have nine1s being wrongly qualified as 0s. And similarly eighteen 0s qualified as 1s.Step 3 : Give additional weight to mis-classified observations:Once we know the misclassified observations, we give additional weight to these observations. Hence, you see 0s and 1s in bold which were misclassified before. In the next level, we will make sure that these highly weighted observation are classified correctStep 4 : Repeat the process and combine all stumps to get final classifier : We repeat the process multiple times and focus more on previously misclassified observations. Finally, we take a weighted mean of all the boudaries discovered which will look something as below.A classic use case where AdaBoost algorithms is in the problem of Face Detection.You can think of this as a more complex boundary detection as we found in last example. Once we have that boundary, we can now create features and classify if the image has a face or not.However, face recognition is commonly done after a gray scale transformation is done on the RCB image and finally a threshold is assumed to create face boundaries. You can read here to make transformation to gray scale or find threshold to create a black and white image. Once we have the transformation, we now analyze each patch of the image.Similar to classifier boosters, we also have regression boosters. In these problems we have continuous variable to predict. This is commonly done using gradient boosting algorithm. Here is a non-mathematical description of how gradient boost works :Type of Problem  You have a set of variables vectors x1 , x2 and x3. You need to predict y which is a continuous variable.Steps of Gradient Boost algorithmStep 1 : Assume mean is the prediction of all variables.Step 2 : Calculate errors of each observation from the mean (latest prediction).Step 3 : Find the variable that can split the errors perfectlyand find the value for the split. This is assumed to be the latest prediction.Step 4 :Calculate errors of each observation from the mean of both the sides of split (latest prediction).Step 5 : Repeat the step 3 and 4 till the objective function maximizes/minimizes.Step 6 : Take a weighted mean of all theclassifiersto come up with the final model.We have excluded the mathematical formation of boosting algorithms from this article to keep the article simple.Boosting is one of the most powerful tool used inmachine learning. These models suffer the problem of over-fitting if the data sample is too small. Wheneverthe training sample is large enough, you shouldtry boosting with manydifferent engines as discussed in this article.Were you haunted by any questions/doubts whilelearning this concept? Ask our analytics communityand never let your learning process stop.Have you used boosting before for any kind of analysis? Did you see any significant lift compared to the traditional models? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/
Senior Data Analyst  Marketelligent -Bangalore  (2-5 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Getting smart with Machine Learning  AdaBoost and Gradient Boost|Modeler  Marketelligent -Bangalore  (3-5 Years of experience)|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Data AnalystLocation  BangaloreAbout employer marketelligent.comJob description: Responsibilities:Experience and Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/senior-data-analyst-marketelligent-banglore-2-5-years-experience/
Modeler  Marketelligent -Bangalore  (3-5 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Analyst  Marketelligent -Bangalore  (2-5 Years of experience)|Infographic  Quick Guide to learn Python for Data Science|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ModelerLocation  BangaloreAbout employer marketelligent.comJob description: Responsibilities:Experience and Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/modeler-marketelligent-banglore-3-5-years-experience/
Infographic  Quick Guide to learn Python for Data Science,Learn everything about Analytics|Introduction,"Download the PDF Version of this infographic and save it in your computer by clicking here > Data Science in Python.pdf|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Modeler  Marketelligent -Bangalore  (3-5 Years of experience)|9 popular ways to perform Data Visualization in Python|
Analytics Vidhya Content Team
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A situation has been described below. Has it ever happened to you?I wanted to learn Python for Data Science, so I googledI want to learn Python for data science. Google, effortlessly, provided you the link of all resources to learn Python. Then, you get bemused by theinnumerable links available to learn Python. Eventually, you end up contemplating, From where should I begin now?Yes ? Dont worry. Because you will never again face such situations.There are plethora of resources available to learn programming and data science in Python. It is difficult to find a structured approach to master this language. To solve these problems, we launched learning path for data science in Python.Today, we take this once step forward and provide you with an infographic for the same. Feel free to circulate this to your friends or take a print out and keep it on your pin-board!To view the comprehensive version of this learning path, click here: Python learning path resourcesAdditional Information:Once you complete the Beginner Level, read this baby steps guides below and proceed to the next level:1.Baby Steps to Learn Python for Data Analysis2.Baby Steps in Python  Libraries and Data StructuresOnce you reach step 4, follow the baby steps guide shared below:1.Baby Steps in Python  Exploratory Analysis in Python(Using Pandas)2.Baby Steps in Python  How to do data munging in Python (Using Pandas)After this, proceed as per the Infographic.Incase, you feel any difficulty inlearning python, feel free to ask us here.",https://www.analyticsvidhya.com/blog/2015/05/infographic-quick-guide-learn-python-data-science/
9 popular ways to perform Data Visualization in Python,Learn everything about Analytics,"Introduction|What does it take to make visualization in Python?|What are the different visualizations I can make?|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Infographic  Quick Guide to learn Python for Data Science|Senior Consultant  Modelytics  Bangalore  (3-5 Years of experience)|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Import Data Set:|Histogram :|Box Plot|Violin Plot|Bar Chart|Line Chart|Stacked Column Chart|Scatter Plot|Bubble Plot|Pie chart|Heat Map,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The beauty of art lies in the message it conveys. At times, reality is not what we see or perceive. The endless efforts from the likes of Vinci and Picasso have tried to bring people closer to the reality using their exceptional artworks on a certain topic/matter.Data scientists are no less than artists. They make paintings in form of digital visualization (of data) with a motive of manifesting the hidden patterns / insights init. It is even more interesting to know that, the tendency of human perception, cognition and communication increases when he / she gets exposed to visualized form of any content/data.There are multiple tools for performingvisualization in data science. In this article, I have demonstrated various visualization charts usingPython.Not much ! Python has already made it easy for you  withtwo exclusive libraries for visualization, commonly known as matplotlib and seaborn. Heard of them?Matplotlib: Python based plotting library offers matplotlib with a complete 2D support along with limited 3D graphic support. It is useful in producing publication quality figures in interactive environment across platforms. It can also be used for animations as well. To know more about this library, check this link.Seaborn: Seaborn is a library for creatinginformative and attractive statistical graphics in python. This library is based on matplotlib. Seaborn offers various features such as built in themes, color palettes, functions and tools to visualize univariate, bivariate, linear regression, matrices of data, statistical time series etc which lets us to build complex visualizations. To know more about this library, check this link.Last week, A comprehensive guide on Data Visualizationwas published to introduce you tothe most commonly used visualizations techniques. We recommend you to refer that before proceeding further, in case you havent.Below are the python codes with their output. I have used followingdata set to createthese visualization:You can read more about pandasgroupby hereand for dataframe. For plot refer this link.Dataframe.unstack() returns a Data Frame having a new level of column labels whose inner-most level consists of the pivoted index labels. Read more about dataframe.unstack here.You can attempt to plot a heat based on two variables like Gender on x-axis, BMI on Y-axis and Sales values as data points. Please share your code and output in comment section.By now, you must have realized, how beautifully data can be presented using visualization. I find performing visualization in Python much easier as compared to R. In this article, we discussed about deriving various visualizations in Python. In this process, we made use of matplotlib and seaborn in python. In the subsequent articles we will explore map visualization and word cloud in python.Do you have any questions on visualization methods / techniques / selection? Feel free to discuss it with us.",https://www.analyticsvidhya.com/blog/2015/05/data-visualization-python/
Senior Consultant  Modelytics  Bangalore  (3-5 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|9 popular ways to perform Data Visualization in Python|Consultant  Modelytics -Bangalore  (2-3 Years of experience)|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ConsultantLocation  BangaloreAbout employer www.modelytics.comJob description: Responsibilities:Qualification and Experience:Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/senior-consultant-modelytics-banglore-3-5-years-experience/
Consultant  Modelytics -Bangalore  (2-3 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Consultant  Modelytics  Bangalore  (3-5 Years of experience)|List of amazing talks from New York R Conference 2015|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ConsultantLocation  BangaloreAbout employer www.modelytics.comJob description: Responsibilities:Qualification and Experience:Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/consultant-modelytics-banglore-2-3-years-experience/
List of amazing talks from New York R Conference 2015,Learn everything about Analytics|Predictive models & Machine learning related talks:|Data Visualization related talks|Big Data related talks:|Other talks:,"What do these talks have to offer?|Hiring by Human / Machine Learning|Software Architecture and Predictive Models in R|The Development Process for the Caret Package|Dashboarding with Shiny|Interactive Plots in Shiny using R|Storytelling with Data Visualizations|Visual NYC|How Data Science is helping Influence the way Big Retailers Work|Practical Principles for Scalable Statistical Analysis|Leveraging R, Hadoop in Analyzing Multiple Myeloma Patient Timelines|Making R Go Faster and Bigger|DataFrames: The Good, Bad, and Ugly|Reproducible Data Analysis with Revolution R Open|R for every survey analysis|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant  Modelytics -Bangalore  (2-3 Years of experience)|Analytics Manager  Delhivery  Gurgaon  (3-8 Years of experience)|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R is undoubtedly the most popular open source data science tool loved by statisticians and analysts across the globe.It provides one of the best interactive environment for doingstatistical analysis, data visualization and predictive modelling.The language has been supported by thousands of programmers across the world.Recently,New York R Conference held its inaugural version on 24th and 25th April 2015. This conference featured R enthusiasts from across the globe.The videos in these talks go on to reinforce the fact that R has the largest data science community and a thriving ecosystem to offer. From visualizing trends on Ebola virus to using machine learning for recruitment, these talks cover a wide range of topics. These talks tease you enough to get you thinking in these topics and leave you with a flavor of some of the exciting work happening across the globe. Just what you need to create the next multi-billion dollor business idea! So go on and have a look for yourself.In this talk, Julie Yoo talks about hiring using machine learning techniques. She discusses the benefits and challenges of using machine learning. It also talks about how machine learning algorithms are efficient and exact for voice & image recognition.A lot of people have apprehensions in using R for production use.This video talks about software architecture and thequestions you need to ask to prepare a better Software Architecture. It also discuss about the role of R in building data related software products.This video talks about the development process, distribution and testing process of Caret package. It gives a good peek into how a package is developed, the testing process,release process and documentation. A must listen talk, if you are planning to build and release your own packages in R at some point in time!This talk from Winston Chang is about dashboards and how they work. It gives you an introduction to shiny dashboards and R package for creating dashboard style layouts with shiny. It also talks about leafletjs: javascript library for creating interactive maps and use of Leaflet in shiny dashboard. In the second half of the video, it also gives you a tour of shiny dashboard having an interactive map visualization.Shinyis an elegant and powerful web framework for building interactive reports and visualizations using R  with or without web development skills. In this talk, a brief demonstration has been given on Shiny using R to plot Ebola data & make it interactive.In this talk, Vivian explains howstorytelling using data visualization helps to generate insights from the data. She also talks about problems with data visualization, seam story design and discusses these with some examples. She has categorized data visualization process into four categories and talks around these (conception, data collection, Data Analysis and after that visualization).If the previous talks looked like a bit of theory, here is an application.In this video, Kaz & Kristineuse visualization (Shiny dashboard and leaflet) to show the rented house affordability in the city New York. They are illustrating the power of visualization to generate actionable insights.Delivered by Karen Moon, this talkshowcases how small start-ups and companies are using Data Science to create a disruption in large traditional markets. This video, in particular shows how her company is using Data Science and R to predict clothing trends before they hit the masses.With increased data generation and cheap storage options, the need for scalable statistical analysis is obvious, but the solutions are not. This talk addresses this pain point directly.Delivered by Michael Kane, this talktells about different libraries and packages in R and Python which can be used to scale Data Exploration practically. He uses these methods to improve the performance and handle large amount of data.Delivered by Saar Golde, Chief Data Scientist of Knowledgent, this video starts with talking about Big Data in Pharma Research and the challenges faced in Analyzing Real World Evidences. It then points to some solutions for the same using Hadoop and MapReduce technologies.This video talks about Big Data and challenges which come with it like storing, processing and computing. The speaker also talks about efficient techniques and functions in R to deal with big data calculations and how you can enhance the performance.How can a conference on R not talk about DataFrames? And who better to talk about it than Wes McKinney (Father of Pandas in Python). Wesbriefly talks about data frame interfaces, biased information judgments and thoughts on crafting high quality data tools. He also talks about pandas, spark and Julia data frames.In this video, Joseph Rickerttalks about reproducible data analysis and packages of past using library checkpoint. He also talks about how this library helps you to solvethe problem of different versions of packages. When you share your scripts with others it using this library, it automatically installs the necessary packages. This checkpoint library only works with CRAN packages.In this talk, Max Richman tells about his learnings from moving young surveyors from commercial software like SAS and SPSS to R (or python) i.e. FOSSS (Fresh and open source software). Max talks about real world situations like variable recoding & statitistical weighing and how R provides an ease in handling data.So, these were some R-related talks in inaugural version of New York R Conference.The idea of a conference in this fashionis definitely exciting and so were the talks. If you have seen PyCon workshops and talks, you mightfeel that the talks are not as hands on and technical as PyCons, but this was just the inaugural version. Also, the talks were aimed to be high level to cover the breadth of offerings R has to offer. It would be interesting to see how it pans out next year!We hope you enjoyed and learned watching them. Do let us know your thoughts and views in the comments section below. We would love to talk with you !",https://www.analyticsvidhya.com/blog/2015/05/talks-new-york-r-conference-2015/
Analytics Manager  Delhivery  Gurgaon  (3-8 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|List of amazing talks from New York R Conference 2015|Associate  DA  Delhivery  Gurgaon  (0-3 Years of experience)|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Analytics ManagerLocation  GurgaonAbout employer Delhivery.comJob description:The position requires the candidate to lead a team of 2-3 Business Analysts, conduct statistical/machine learning powered data analytics, create mathematical models and generate business insights that will be leveraged to streamline operations, drive revenue growth, formulate strategies and maintain partner relationships for Delhivery.Qualification and Experience:Skills Required:",https://www.analyticsvidhya.com/blog/2015/05/analytics-manager-delhivery-gurgaon-3-8-years-experience/
Associate  DA  Delhivery  Gurgaon  (0-3 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Manager  Delhivery  Gurgaon  (3-8 Years of experience)|Career/Training Guidance from Experienced Data Science Experts|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Associate  DALocation  GurgaonAbout employer Delhivery.comJob description:This position requires the candidate to use advanced statistics and machine learning on large scale multi-dimensional data and generate actionable insights that will be leveraged to drive operations and develop strategies for Delhivery.Qualification and Experience:Skills Required:,https://www.analyticsvidhya.com/blog/2015/05/associate-da-delhivery-gurgaon-0-3-years-experience/
Career/Training Guidance from Experienced Data Science Experts,Learn everything about Analytics,"Introduction|Have you ever been in the same dilemma? Do you have a similar question to ask?|How can you benefit from AVs Mentorship?|Our programs start from basics and make you industry ready by providing you the skills you need. Each program includes multiple courses along with industry projects and mentorship support.||Contact|Share this:|Like this:|Related Articles|Associate  DA  Delhivery  Gurgaon  (0-3 Years of experience)|Mini Datathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 17th May 2015|
Analytics Vidhya Content Team
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Here is the most common list of queries that keeps flowing infrom our readers:1. I am a fresher, how do I start my career in data science?2. Since there are a lot of data science training providers, which is the best training course for me?3. I have experience of 3-15 years in IT/Pharma/Electronics industry, now I want to make a career shift to data science. How can I start accomplish that?4. How can I get to know about the upcoming jobs in data science?and many more..There are a lot of people who are trying hard to make a successful career in data science. But, due to a lack of guidance and support, they somehow get stuck at a certain level. From there, reaching the next level becomes a mystery.This is where Analytics Vidhyas Mentorship comes in. It is part of every single Certified Program we have.Email: [emailprotected]",https://www.analyticsvidhya.com/blog/2015/05/careertraining-guidance-experienced-analytics-experts/
"Mini Datathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 17th May 2015",Learn everything about Analytics,"Introduction|Who should join this meetup?|How much is the fees for this meetup?|Why should you attend this meetup?|Contact|Share this:|Like this:|Related Articles|Career/Training Guidance from Experienced Data Science Experts|Interview with Daniel Graham, General Manager, Teradata|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People who are currently working in analytics or eager to enter analytics industry usually have one thing in common which isproblem solving abilities.After going through more than 100 hours of trainings on analytics tools and techniques, people still struggle to implement their knowledge to solvereal time business problems. One such platform which offers problems with a handsome prize money is Kaggle. Every working professional in analytics industry aspires to win atleast one kaggle competition to get credibility in his business domain. But, a few succeed in makingsuch brilliant accomplishments .One major reason for failure that we see is the lack of support in analytics community in India. In order to bridge this gap, Analytics Vidhya proudly announcesMiniHackathon where the brains of analytics will fight to solve a kaggle problem. Interesting part is, the people will get completesupport of team of data scientists from Analytics Vidhya.This meetup will be steered by Mr. Kunal Jain, CEO, Analytics Vidhya who also happens to be a Data Scientist, Growth Hacker.Anyone who is keento learn analytics or has already stepped intoanalytics and wishes to upscale his/her knowledge and skills is welcome to join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.1. If you are eagerly waiting for an opportunity to compete with the best data analysts, data scientists and check your level of expertise, this can be an ideal platform for you.2.If you are keen to solve kaggle competitions but have are struggling due to lack of guidance and mentorship, you should attend this.3. If you wish to scale up your level of skills and competencies and wish to learn from the best data enthusiastsin the city, you should attend this.4. Needless to say, if you want to meet and network with like minded people i.e. data analysts, data scientists, data managers, this can be an ideal platform for you.Date: 17th May 2015Venue: 6th Floor, Salarpuria Tower, BigBazar Building,Salarpuria Tower, Near Forum Mall, Hosur Main Road,BangaloreFor more information on this event, visit here.For more information & details on these meetups, feel free to drop us a mail at [emailprotected] or you can also reach us at 0124-4264086.",https://www.analyticsvidhya.com/blog/2015/05/mini-datathon-analytics-vidhya-bangalore-chapter-17th-2015/
"Interview with Daniel Graham, General Manager, Teradata",Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Mini Datathon by Analytics Vidhya, Bangalore Chapter, Karnataka, 17th May 2015|WFM Manager Analytics  Sitel.com  Mumbai  (3-4 years of experience)|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data is probably the biggest asset for an organization in todays digital economy. But, with huge data, comes a challenge to mine it quickly and effectively.To solve this problem, Teradata provides data warehousing and analytics solutions across various sectors to help companies make better, faster and smarter decisions.Having launched its IPO in 1987, Teradata is one of the largest companies in providing best in class big data analytics and marketing applications.I personally experienced Teradata solution some time back, when a move to Teradata (from one of the competing solutions) reduced the query execution time to a fraction of previous time. I have ever since been impressed by Teradatas offering and performance.Recently, we got a chance to interact with Daniel Graham, General Manager  Enterprise Systems, Teradata who has been working in the this company for more than a decade. Danis responsible for strategy, go-to-market success, and competitive differentiation for the Active Data Warehouse platform and Extreme Performance Appliance at Teradata.Below is the transcript of the our conversation:Daniel GrahamAV(Analytics Vidhya): Thanks Daniel for devoting time to speak with us. It has been 9 years since you have re-joined Teradata. How has this journey been for you? How has this company transformed from your early days with them in the 1990s?DG(Daniel Graham): The big turning point for Teradata was the spin off from NCR Corporation in 2007.The spinoff allowed Teradata to expand into multiple areas. The Teradata portfolio is much broader now, yet still focused on analytics.Our transition from a single product line to a broad portfolio of products and services has been dramatic.For example, today we have appliances at every price point targeted at specific workloads such as discovery or regulatory compliance. The spin off also freed up funding for innovative software investments such as Active Data Warehousing and Teradata Virtual Storage. But most importantly, the spin off allowed an independent Teradata to acquire other companies.Our acquisitions AsterData, Claraview, Aprimo, ThinkBig, Revelytix, and othersare strategically known as a string-of-pearls. Weve had great success retaining the people and vision of the companies we acquire. Over this same period of time, Teradata Professional Services has expanded exponentially to over 5,000 data quality, performance, security, data mining experts.AV: Give us a brief overview of products offering by Teradata?DG:Our hardware portfolio consists of four Teradata Database appliances, one Aster Database appliance, a Hadoop appliance, a SAS appliance,and a Teradata Cloud offering. These are hardware platforms that are configured to match to specific customer workloads.Our software portfolio begins with the Teradata and Aster Database engines. To this we added the Aprimo Marketing Resource Management, Omni-Channel Marketing, and Digital Marketing solutions.For Hadoop systems, we offer Teradata Loom (a file management and data wrangling solution) and Teradata Rainstor (a big data archival system).We also offer Teradata Analytics for SAP.AV: That is quite extensive for any datawarehousing need. What are your focus areas currently? What can we expect from Teradata in coming days?DG:Today, we are rolling out systems that provide maximum scalability, high performance analytics, and support hundreds of concurrent users.Specifically, the Teradatas Data Warehouse Appliance 2800 is the most robust database for analytic data warehousing. This appliances innovative design is optimized for fast scans and deep-dive analytical workloads. The Teradata Enterprise Data Warehouse 6800 gives leaders in every industry a fresh, big-picture perspective. With a 360-degree view of the business, the organization can gain valuable insights into customers, supply chain, and financial and performance management  from the boardroom to the call center to the loading dock.Teradata is also putting a lot of resources into Teradata QueryGrid to enable massively parallel queries to run across Hortonworks, Cloudera, Aster Database, MapR, Oracle, and MongoDB platforms, among others.This allows a business user to submit a SQL query on one platform and join data together with other platforms  all at high speeds with no extra programming required.We are also expanding Think Big  a Teradata Company  into Europe. Think Big is our Hadoop and NoSQL professional services arm.In this same vein, we also announced Think Bigs Dashboard Engine, the industrys first unstructured data OLAP solution based on Hadoop.AV: How do you ensure that your customers stay on latest offering from Teradata?DG: There is a hardware and a software answer to that question. For software, the best enticement for customers to stay current is new features that they really want, which is guided by customers on our Product Advisory Committee. In addition, every new software release, yields a performance improvement, which is always welcome. For hardware, the benefits are dramatically visible. New Intel microprocessors, larger memories, and faster devices like Infiniband and SSD disks are incredible when it comes to performance boosts. Successful data warehouses always have more users and more demand for more queries, so the need for speed is eternal. Consequently, many of our customers combine a hardware upgrade with a database software release upgrade.AV: Since, privacy and security of data has remained a major concern forcompanies, what innovations/modifications do you plan to bring in data warehousing?DG: Privacy is a policy that our customers must define for themselves. Our Professional Services consultants can help customers define policies and implement some of the decisions. But ultimately, ethical behaviour  aka privacy  is a customer business decision which is often guided by country-level laws.Technologically, Teradata has invested heavily in security over the years with some cool recent developments. Along with all the extensive authentication and permission controls in the Teradata Database, weve implemented row level security and secure zones. Row level security allows for the tagging of individual records, such that only people authorized can see them. For example, we could grant access to the SALES_TRANSACTIONS_DATA to everyone, but each person could only see details from their own region. Sales managers could review deals in their own region, but not neighbouring regions. You dont have to use a lot of SQL Views to do this, just in-database row level security.Second, we recently added secure zones. Secure zones make it appear to the user as if they were in their own isolated database on a separate machine. Imagine a multinational e-commerce retailer. Because of country laws, data from one country cannot move between countries, protecting consumer privacy.With secure zones, we can use the same database design for every country and ensure that no one outside of a country can see any of the data  not even the table names.Secure zones are also a prerequisite to multi-tenancy cloud computing.And I cant forget to mention our partnerships with several best-of-breed security vendors that either encrypt or mask the data in the database  IBM Guardium, Voltage, HP/Vormetrics and Protegrity.For example, our partnership with Protegrity adds encryption in the applications processing layer so that personally identifiable information cannot be seen  not even by the database administrators.AV: Do you plan to capture the untapped market of front end solutions i.e. Data Visualization?DG: Teradata follows a best-of-breed strategy, which means we rely on partners who represent the best in their domain. For example, we partner with MicroStrategy, Qlik, SAS,Tableau, Tibco Spotfire, as well as the large stack players for visualization and BI tools. There are times we overlap a little with these partners. But we have no intention of becoming a BI tools or visualization supplier. Our customers prefer to use best-in-class tools with Teradata just as they have decided that Teradata is best-in-class for data warehouses.We will stick to our core competencies.AV: Qlik, in addition to an amazing data visualization software, also has a data warehousing solution(although less effective). How do you plan to compete against them?DG:Teradata does not compete with Qlik. Teradata sells massively parallel database machines and marketing applications. Qlik sells business intelligence visualization and dashboards. In fact, we have recently co-developed new in-database processing with Qlik. Branded Direct Discovery from Qlik, they can configure what data is placed in memory (e.g., small reference tables, metadata), and then what sources the user will drill through to the database to speed up performance (e.g., large fact tables). We think our joint customers will appreciate this hybrid approach.AV: Follow through questions on market competition, how do you plan to compete / complement against NoSQL data solutions?DG: The majority of NoSQL products are operational databases. These products are good for building online applications whether they are e-commerce, call center apps, or social media applications. These are good sources of data for the data warehouse. For example, a NoSQL website collects buyer clickstreams in response to online offers. Its the data warehouses job to combine that data with past history, customer preferences, payment history, and other demographics. The analysis is passed back to the NoSQL application. The NoSQL application then knows to informthe business user to upsell the consumer and to minimize sales to those that default on payments. Teradata does not compete with NoSQL vendors; we embrace them as partners inside the IT architecture.AV: What is the best part about the culture in your company? How has it evolved over the years?DG: Teradatas culture began in the 1980s in California. Every one of us believes deeply in the vision of leveraging data warehousing, analytics and big data to help our customers better run their businesses. Thirty years of BI/DW market growth reinforces our sense of purpose, vision, and just good fun helping customers. We strive to be our customerstrusted advisor. And that bubbles over into mutual trust and respect. We are that team rowing in the same direction with our customers. This isnt a platitude crafted by HR. Its the way we are inside Teradata. Its a fun place to work.AV: What would be your advice to aspiring young folks seeking to build their career in data analytics/big data analytics and other related domains?DG:Analytics is a force of nature in the digital world. And, we can expect 30 additional years of amazing technology advances. Those individuals with skills and expertise in a vertical industry combined with analytics will be the most attractive to employers. For most people this means proficiency with tools such as Tableau, Qlik, MicroStrategy, SAS and others. For some, a deep dive into predictive analytics  graduate level mathematics  can lead to a career in data science. And in both cases, skills in database and data warehouse concepts are a big plus. I also recommend reading Tom Davenports Competing on Analytics as well as Jill Dyches The New IT. These will illuminate what a career in analytics is like.You can pick up analytic skills at Northwestern, University of Georgia, Ohio State, Stanford, and others with a data science curriculum.AV:Thanks Daniel for that insightful and enriching interview. This interview definitely explains the vision and passion behind these awesome Teradata products and the powerful performance theyenable.Once again, I thank you for your time and sharing your perspective.P.S  We have more such interesting interviews coming up ! Do subscribe.",https://www.analyticsvidhya.com/blog/2015/05/interview-daniel-graham-teradata/
"WFM Manager Analytics  Sitel.com  Mumbai  (3-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Interview with Daniel Graham, General Manager, Teradata|Senior Business Development Executive  TEG Analytics  Bangalore (3-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  WFM Manager AnalyticsLocation  MumbaiAbout employer Sitel.comJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asWFM Manager Analytics  Sitel.com  Mumbai,https://www.analyticsvidhya.com/blog/2015/05/wfm-manager-analytics-sitel-com-mumbai-3-4-years-experience/
Senior Business Development Executive  TEG Analytics  Bangalore (3-5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|WFM Manager Analytics  Sitel.com  Mumbai  (3-4 years of experience)|A Comprehensive guide to Parametric Survival Analysis|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Business Development ExecutiveLocation  BangaloreAbout employer TEG AnalyticsWe are a young company with a vision to bring Analytics to the mainstream. In this regard, we are working with some of the fortune 500 companies and servicing them using our global delivery model, we are also developing intellectual property that will put the power of Analytics in the hands of decision makers without requiring capital investments upfront, and we are promoting an ecosystem of thought leaders and practitioners in Analytics.Job description: The person will be responsible to generate demand for new customer revenue opportunities, to achieve quarterly targets and build pipeline for senior sales folks in the US region.ResponsibilitiesQualification and Skills RequiredExperience -and references to validate it that the candidate has outstanding interpersonal skills with their peers and corporate functional support staffInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Business Development Executive  TEG Analytics  BangaloreIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/05/senior-business-development-executive-teg-analytics-bangalore-3-5-years-experience/
A Comprehensive guide to Parametric Survival Analysis,Learn everything about Analytics,"Introduction|Basics of Survival analysis|What are the distributions used in Parametric Models?|1. Normal Distribution|2. Uniform distribution|3. Exponential Distribution||4. Weibull Distribution|5. Log-normal Distribution|What are the applications of Survival Analysis?|The right distribution for each case:|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Senior Business Development Executive  TEG Analytics  Bangalore (3-5 years of experience)|Why Business Analytics Degrees Arent Just a Fad|
Tavish Srivastava
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Survival analysis is one of the less understood and highly applied algorithm by business analysts. That is a dangerous combination! Not many analysts understand the science and application of survival analysis, but because of its natural use cases in multiple scenarios, it is difficult to avoid!P.S. If you read the first half of this article last week, you can jump here. We have combined the articles to make it more useful for our readers.Survival analysis refers toanalyzing a set of data in a defined time duration before another event occurs. The number of years in which a human can get affected by diabetes / heart attack is a quintessential of survival analysis.Survival analysis is one of the most used algorithms, especially in Pharmaceutical industry.In one oftheprevious article, we have already discussed the use cases of survival analysis. We also talked about non-parametric and semi-parametric survival analysis.We suggestyou to go through these articles first to get a good understanding of this article.In this article, you will learn:Let us first understand how varioustypes of Survival analysis differ from each other.A survival analysis is different from traditional model like regression and classification problems as it models two different parameters. To understand the Survival analysis in detail, refer to our previous articles(1 & 2). However, in this article we will also discusshow the three types of analysis are different from each other.The imageabove will help you understand the difference between the three classes of Survival analysis models. Having already explainedaboutsemi parametric models, we will go a step ahead and understand how to build a Parametric model.In a parametric model, we assume the distribution of the survival curve. Even beforefitting a model, you need to know the shape of the Survival curve and the best function which will fit in this shape. For this you need to build a non-parametric model and understand the shape of hazard function and the survival curve.There are five types of distribution of Survival/hazard functions which are frequently assumed while doing a survival analysis. The name of each of these distribution comes from the type of probability distribution of the failure function. Following are the 5 types of probability distribution curve generally used in parametric models. Each distributionbeen explained below in detail:For each of these distributions, lets first understand the following plots :1. Lifetime Distribution Function (F) : This is the probability of failure happening before a time T.2. Lifetime Probability distribution (f) : A differential of F will give us probability distribution. All the names of distribution function is based on this probability distribution.3. Survival Function (S) : Survival is the inverse of Lifetime. It is one minus Lifetime distribution.4. Hazard Function (Lambda) : Hazard function is the rate of event happening. Hazard function can be derived from the Survival function as follows :5. Cumulative Hazard Function : This is simply the integral of the hazard function and is given as below :Also, by integrating the hazard function equation we get following equation :Following are the two plots we will refer in each case (these are the important ones to select the distribution) :a. Hazard Functionb. Survival FunctionThis type of distribution is assumed when the risk of failure increases considerably with time. Hence, the probability of failure increases suddenly.Check the graphs shown below:Uniform distribution is not a common type to be assumed in real world. The survival curve is just a straight line from 100% to 0%. And the hazard function increases exponentially to force death of every single observation towards the end.Check the graphs shown below:
Exponential distribution is one of the common assumption taken in survival models. The hazard function does not vary with time. This distribution can be assumed in case of natural death of human beings where the rate doesnot vary much over time.Check the graphs shown below:Weibull distribution has a parameter gamma which can be optimized to get different distributions of hazard function. Following are a few scenarios which will illustrate the same:As you can see from the multiple scenarios, gamma can change the weibull hazard function from steep decline to constant function to accelerating increase. Hence, it fits into multiple situations in our practical world.Here is another distribution which can be optimized for different hazard functions. Lognormal distribution can be complimented by Weibull distribution to simulate almost every scenario. Check the scenarios as shown below:As you can notice from the above graphs: With changing value of sigma, the curve changes its nature. This function can generate non-monotonic natures of hazard function. This is a single scenario where weibull curve does not fit well. Hence, they both complement each other well and literally can be used for all scenarios.To understand the applications, lets now take a step back and think of cases where Survival analysis can be used and based on the expected distribution fit the best possible curve.Assignment : Before looking at the answers try to attempt the best fit distribution in each case.Case 1 : Time until next case of scientific innovation.Because innovations are not biased towards any specific reasons, the hazard function is a constant line. Hence, following are the Hazard Function, Survival function and the probability distribution function:Case 2 : Life of patients of Cancer who are not responding to any treatmentCancer gets worse with time and hence the survival rate deteriorates much faster. Following are the Hazard Function, Survival function and the probability distribution function:Case 3 : Life of a patient after surgery OR Financial state of a country/company after a big shockWhenever there is a deteriorating effect shock. For example: Condition of patients after surgery where the risk of anything turningunfavourable,goes down with time. Below we have following type ofthe Hazard Function, Survival function and the probability distribution function:Case 4 : Life of a patient recently detected with Swine Flu or TBDiseases like Swine Flu or TB havea sharp impact. If the patient can survive the initial period of these diseases, the danger of death gradually subsides as the time passes on. Following are the Hazard Function, Survival function and the probability distribution function:Now lets think over what distribution fits well in each of these cases:Case 1 : Both Exponential and Weibull can be used for this case as hazard function is a constant curve.Case 2 : Weibull function with gamma = 2 can be used as the hazard function is a linearly increasing curve.Case 3 : This is kept as an assignment for this article. You wont find a direct answer in this article but with a good basic understanding, you should have no challenge figuring this out.Case 4 : This is the classic case of the use of Log normal distribution. The hazard function shows a peak and hence the log-normal with sigma less than 1 is suitable for this case.This article will help you understand the Survival analysis. It also explains how to estimate distributions given the survival plots. People generally miss out on understanding the application of any concept they choose to learn.In this article,we have also discussed various cases which describes the diverse applications of this Parametric Analysis. Case 3 is given as an assignment. Write your detailed answers in the box below.Were you haunted by any questions/doubts whilelearning this concept? Dont worry, ask our analytics communityand never let your learning process stop by any of the hurdle which comes across your way!Did you find the article useful? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/05/comprehensive-guide-parametric-survival-analysis/
Why Business Analytics Degrees Arent Just a Fad,Learn everything about Analytics|Introduction|Why do companies require Business Analysts? ||What does it take to become a Business Analyst?|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|A Comprehensive guide to Parametric Survival Analysis|Manager -Affine Analytics  Bangalore (8+ years of experience)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data can be defined as a collection of information, used to derive useful insights for informed decision making. Right from analyzing traffic patterns on a small business website to determining strategies for conversion optimization, to more complex industrial algorithms that determine how much laundry detergent a big box retailer should deliver to an individual location; data is playing a vital role in determining B2C or B2B insights and thereby formulating business strategies. The usage of data is becoming more and more mainstream in todays world.More than 90 percent of the data that exists today has been created since 2011  and thanks to our technologically driven society. Billions of bytes of new data are created every day. But what do we do with all of that data?Without interpretation or context, its all but useless. Without knowing what to pay attention to, and what it all means in the larger scheme of things, big data is nothing more than a big pile of information.Thats where Business Analyst comes in. Business Analyst helps in making sense out of raw data.Back in 2012, the Harvard Business Review called the position of data scientist the sexiest job of the century. That description stemmed from the fact that in the latter half of the first decade of the oughts, the notion of big data was only beginning to gain notice. Online businesses, especially social media companies, were collecting reams of data about their users, but werent sure how to use it to their advantage.Analysts began asking questions, looking for patterns, interpreting   data, and developing insights that could be used to grow their businesses. Based on the success of companies like LinkedIn, which relied heavily on analytics to develop the features that attracted millions of new users (and subsequent revenue growth) other companies began looking to scientists who could make sense of their data, and more importantly, show them how to use it.The HBR called the job of data scientist  which has since morphed into business analyst in many cases  sexy, and the employment picture agrees. According to the U.S. Bureau of Labor Statistics, demand for business analysts is expected to increase by 25 percent by 2020. In short, everybody wants one  but not everyone can get one.According to a report on big data from research firm McKinsey & Company, there could be a shortage of nearly 200,000 analysts who have the in-depth skills necessary to interpret big data by 2018. Perhaps even more alarming is the same report predicts a shortage of 1.5 million managers or analysts who know how to actually use the insights gathered from big data to make effective decisions.The Bureau of Labor Statistics notes that most business analysts have a degree in a computer or technology-related field with additional training in business. However, the most successful candidates have a masters degree, which combines their deep knowledge of the computer systems required for gathering and interpreting data with the business expertise necessary for applying it.Colleges and universities across the country are meeting the demand for this specialized knowledge with comprehensive programs that focus on gathering, analyzing and prioritizing business and system data and requirements, and preparing students for work in this field.Some claim, though, that the focus on big data is just a fad, and investing time and energy into business analyst education will prove unnecessary. However, the facts suggest otherwise, and there are more than a few reasons that a business analysis degree is a good investment:Clearly, then, earning a degree in business analytics is a solid investment in your career. Developing not only the technical skills for data collection and analysis, but knowing how to think about the data and interpret it correctly will prove valuable going forward.In this article, we discussed about the importance on business analytics in an enormously populated world of data. We also looked at how business analysts are best at playing with data, thus helping businesses globally to make informed decision making. This article also discusses the job demand of business analyst in the near future.",https://www.analyticsvidhya.com/blog/2015/05/business-analytics-degrees-arent-fad/
"Manager -Affine Analytics  Bangalore (8+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Why Business Analytics Degrees Arent Just a Fad|Consultant  Affine Analytics  Bangalore (5  8 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ManagerLocation  BangaloreAbout employer Affine AnalyticsJob description: ResponsibilitiesQualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asManager -Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/manager-affine-analytics-bangalore-8-years-experience/
"Consultant  Affine Analytics  Bangalore (5  8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager -Affine Analytics  Bangalore (8+ years of experience)|Senior Statistical Analyst  Ahmedabad  (5-7+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ConsultantLocation  BangaloreAbout employer Affine AnalyticsJob description: ResponsibilitiesQualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant  Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/consultant-affine-analytics-bangalore-5-8-years-experience/
"Senior Statistical Analyst  Ahmedabad  (5-7+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant  Affine Analytics  Bangalore (5  8 years of experience)|Senior Data Scientist  ValueLabs  Hyderabad  (8+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Statistical AnalystLocation  AhmedabadAbout employerConfidentialJob description: Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.IQR is a Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.ResponsibilitiesQualification and Skills RequiredEducation and Experience:Interested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Statistical Analyst  Ahmedabad",https://www.analyticsvidhya.com/blog/2015/05/senior-statistical-analyst-iqr-consulting-ahmedabad-5-7-years-experience/
"Senior Data Scientist  ValueLabs  Hyderabad  (8+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Statistical Analyst  Ahmedabad  (5-7+ years of experience)|Analytics Delivery Manager  ValueLabs  Hyderabad  (10 + years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Data ScientistLocation  HyderabadAbout employer ValueLabsJob description: The Senior Data Scientist is primarily responsible for creating business critical insights from large quantities of data using computer-based analytical and modeling techniques like logistic regression, neural networks, and decision trees. In this role the candidate has to listen to the questions that business users, executives and management are asking and devise tangible analytical strategies to solve real business problems. At ValueLabs, you will have the chance to explore large amounts of audience data using state of the art computing resources, analyze it and formulate insights from it for our clients. You will have direct interaction with team members from across disciplines (development, product strategy, sales and client services) and will also be responsible for interacting with our clients to help them realize the possibilities of their dataResponsibilitiesIndividual:Team:Qualification and Skills RequiredWe are seeking a motivated individual with:Interested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Data Scientist  ValueLabs  Hyderabad",https://www.analyticsvidhya.com/blog/2015/05/senior-data-scientist-valuelabs-hyderabad-8-years-experience/
"Analytics Delivery Manager  ValueLabs  Hyderabad  (10 + years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Scientist  ValueLabs  Hyderabad  (8+ years of experience)|Ultimate resource for understanding & creating data visualization|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Analytics Delivery ManagerLocation HyderabadAbout employer ValueLabs Job description: The Analytics Delivery Manager is a key member of the ValueLabs Analytics Delivery team, and is responsible for overseeing a team of Analytic experts.The Analytics Delivery team within ValueLabs Analytics is responsible for blueprinting and delivering projects with the appropriate analytic methodologies and techniques to solve clients business objectives. The team closely collaborates with other analytic stakeholders to understand the business problem in order to determine the most appropriate analytic approach that provides meaningful results to clients. Responsibilities include delivering projects on time and within scope with an in-depth knowledge of big data and cutting edge data mining techniques as well as the use of predictive, classification and alternate analytic algorithms for modeling and segmentation. These analyses are foundational to corroborate or refute stated hypotheses and are incorporated in the final client-facing solutions. The team is responsible for continuously creating and protecting analytic IP resulting from project learningThe Analytics Delivery team is the engine of analytics at ValueLabs; this is a high-performing team of data scientists, data analysts, statisticians, and business analysts. ValueLabs Analytics is looking for a hands-on manager, a person that earns trust and respect of the team. The Manager must be results oriented, highly organized, and must, must be focused on delivering innovative analytics work.ResponsibilitiesQualification and Skills RequiredTechnicalBusinessInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalytics Delivery Manager  ValueLabs Hyderabad",https://www.analyticsvidhya.com/blog/2015/05/analytics-delivery-manager-labs-3-10-years-experience/
Ultimate resource for understanding & creating data visualization,Learn everything about Analytics|Introduction|What is the Impact of Data Visualization?|What are the common methods of visualization?|Advance Visualization Methods|End Notes,"||1. Distribution|2 ) Comparison|3) Relationship|4) Composition|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Analytics Delivery Manager  ValueLabs  Hyderabad  (10 + years of experience)|Product Manager  Affine Analytics  Bangalore (4-5 years of experience)|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are 3 fundamental changes driving penetration ofdata science industry:These forces have also changed the process flow for an analyst. Effective data visualization is more critical component of data science process flow than ever before. The impact can be felt in following areas clearly:Hopefully, this provides enough context to the importance of data visualization.In this article, well understand the importance of data visualization and how it can be used to derive useful insights in diverse situations. Well also look at the various forms of data visualization, beginning from basicto advanced level of visualizations.A good visualization could be the difference between hard to digest piles of data and useful business information.Lets look at the some amazingexamples of data visualization below:Lets look at somecommon methods of visualization which help us to understand distribution, trend, relationship, comparison and composition about the data values. One of the important question, every data scientist facesis to decide, Which visualization method is more effective?While discussing the same problem with my community, I got significant upvotes on the method of first focusing on the Type of messageor information you want to conveyand then select the appropriate visualization method.Here is a really cool cheat sheet on selecting right visualization methods.I found this in HarvardCS-109 extension program (online resource).Here you can notice, how flawlesslythey have divided the chart messages in four categories namely Distribution, Comparison, Relationship and Composition. Then, they haveclassified various visualization methods in these 4 categories.Lets look at these categoriesindividually and discuss the most common and effective methods in detail.It is commonlyused at the initial stage of data exploration i.e. when we get started with understanding the variable. Variables are of two types: Continuous and Categorical. For continuous variable, we look at the centre, spread, outlier. For categorical variable we look at frequency table. Visualization typesused to representthese are:-a) Histogram: It isused for showing the distribution of continuous variables. One of the catch with histogram is number of bins. Lets understand it in detail using example below:-Both histograms are showing different distribution of a given set of data which representsage distributionusingCount of Passengers vs Age. Look at the histogram at right .We can infer thatthere are more infants inage group of 0-4 years compared to age group of 4-16 years. However, if you try to make this inference from left graph, Im sure you would fail to do so. Hence, we should be very careful while selecting number of bins.b) Box-Plot: It is used to display full range of variation from min to max and useful to identify outlier values. It shows Min, Q1, Median, Q3 and Max. Any value outside the lower and upper inferences is considered as an Outlier. Formula for calculating lower and Upper inferences are:-Upper Inference = Q3 + 1.5 * (Q3-Q1), (Q3-Q1) (also known as IQR)Lower Inference = Q1  1.5 * (Q3-Q1)We can also visualizedistribution between two continuous variables or one categorical and one continuous variable using scatter plot or multiple box plots by different categories of categorical variables respectively.It is used to compare values across different categories and over time (trend). Common charts to represent these information are Bar and Line chart. Please note:When we compare values across different categories we should go with Bar chart. If it is over quantitative variable, we should go with line chart.Comparison across various categoriesComparison across quantitative variableWe can also compare multiple metrics using bar chart across different categories using stacked bar charts.If there are multiple categories, it is a good practice to segregate categories in different groups and then compare accordingly. Decision Tree is one of the usefulvisualization techniqueto explore data values as shown below.It is widely used to understand the correlation between two or more continuous variables. Most common method to visualize this information is Scatter Plot. It clearly shows the relationship between two variables. It usually draws a line of fit that best represent the relationship between data points.The line necessarily need not connect with data points.We can also add third variable in scatter plot by using size of the points (known as Bubble Chart) or colors(as shown below).One of the problem with scatter plot is that it may get crowded ifwe have thousands or millions of data points. In this case,we can performalpha blending, which makes each point slightly transparent. So regions appear darker that have more point plotted on them.It is used to show distribution of a variable across categories of another variable. Well known method to represent this is Pie chart. Though, I am not a big fan of pie chart asit is difficult to show distribution across multiple categories and angular comparison is difficult to understand. Hence, I prefer to work withBar Chart.Stacked Bar(image at right) chart is a type of Bar chart. It can compare distribution across different categoriesof two variables.Till here, we looked at most common methods used to visualize information. Lets look at someadvancemethods of visualization. These methods extend the power of storytelling using visualization methods.a) Heat Map: It uses colors to represent numbers in a spreadsheet or in any other visualization methods like Scatter, Geo-spatial, Area Chart. You can set different color gradient for the lowest, highest and mid-range values, with a corresponding transition (or gradient) between these extremes.It represents one more variable to existing visualization method and adds additional information about data values.b) GeoSpatial charts: Data scientists started plotting variables ongeographical address tohelp organisations in makingtheir strategies differently for different cluster based on the spread of data. Here we can also use color index or size as a metric to represent more variables. It is similar to scatter plot. The only difference here is we are plotting data points on map.Advantagesof using Geo-Spatial Visualization are:Above, you can see that how easily we are able to present information on map compare to tabular and Bar Charts.c) Grid:It is used in 2D tabular format. In this method, we use two metrics horizontally and vertically. Then, plot a grid against each category of both metrics. After that, use colors to represent the data.Lets understand it using an example:In above grid, you can represent the skill of a professional across various tools and techniques. In here, Green color represents the expert, Amber to intermediate and Red as beginner. Here we have efficiently represent this information without much hassle.d) WordCloud: A word cloud is a method to represent text data. It is also known as text or tag cloud. It is a graphical representation of frequently used words in a collectionof text files. The height and font style of each word in this pictureis an indication of frequency of occurrence of the word in the entire text data. Such diagrams are very useful in working ontext analytics.It is just like any infographic. It makes an impact, easy to understand and can be shared easily. Make a note,before using word cloud, only focus on frequency of word and noton importance of variable.There is more advance visualization techniques like Venn-diagram, Network Map, Radar Chart and other custom visualization methods to represent data and generate more meaningful insight from them.In this article, we first understood the role of data visualization in data science process flow. We also looked atdiverse applications and usage of data visualization using charts and graphs. Next, we discussed various methods used for representing complex data in a simplified manner. We also covered advanced visualization techniques which features Heatmap, Geospatial, WordCloud and Grid.Do you have questions on visualization methods / techniques / selection? Feel free to discuss it with us and our community here.",https://www.analyticsvidhya.com/blog/2015/05/data-visualization-resource/
Product Manager  Affine Analytics  Bangalore (4-5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Ultimate resource for understanding & creating data visualization|Sr. Business Analyst (R-Automation) -Affine Analytics  Bangalore (2  5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Product ManagerLocation  BangaloreAbout employer Affine AnalyticsJob description: As Product Manager, you will guide a team that is charged with a product line contribution as a business unit. This extends from increasing the profitability of existing products to developing new products for the company. You will build products from existing ideas, and help to develop new ideas based on your industry experience and your contact with customers and prospectsResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asProduct Manager  Affine Analytics  BangaloreIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/05/product-manager-affine-analytics-bangalore-4-5-years-experience/
"Sr. Business Analyst (R-Automation) -Affine Analytics  Bangalore (2  5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Product Manager  Affine Analytics  Bangalore (4-5 years of experience)|Cassandra/Hive Developer -Affine Analytics  Bangalore (1+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr. Business Analyst (R-Automation)Location  BangaloreAbout employer Affine AnalyticsJob description: Qualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asSr. Business Analyst (R-Automation) -Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/sr-business-analyst-r-automation-affine-analytics-bangalore-2-5-years-experience/
"Cassandra/Hive Developer -Affine Analytics  Bangalore (1+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Business Analyst (R-Automation) -Affine Analytics  Bangalore (2  5 years of experience)|Senior Business Analyst  Affine Analytics  Bangalore (2  5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Cassandra/Hive DeveloperLocation  BangaloreAbout employer Affine AnalyticsJob description: Qualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asCassandra/Hive Developer -Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/cassandrahive-developer-affine-analytics-bangalore-1-years-experience/
"Senior Business Analyst  Affine Analytics  Bangalore (2  5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Cassandra/Hive Developer -Affine Analytics  Bangalore (1+ years of experience)|Business Analyst  Affine Analytics  Bangalore (6 Month  2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Business AnalystLocation  BangaloreAbout employer Affine AnalyticsJob description: ResponsibilitiesQualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Business Analyst  Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/senior-business-analyst-affine-analytics-bangalore-2-5-years-experience/
"Business Analyst  Affine Analytics  Bangalore (6 Month  2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst  Affine Analytics  Bangalore (2  5 years of experience)|Inside-sales/ Pre-sales Consultant  Affine Analytics  Bangalore (2-3 years of experience)|
Jobs Admin
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business AnalystLocation  BangaloreAbout employer Affine AnalyticsJob description: ResponsibilitiesQualification and Skills RequiredBehavioral Competences:Interested people can apply for this job can mail their CV to[emailprotected]with subject asBusiness Analyst  Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/business-analyst-affine-analytics-bangalore-6-month-2-years-experience/
"Inside-sales/ Pre-sales Consultant  Affine Analytics  Bangalore (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Affine Analytics  Bangalore (6 Month  2 years of experience)|Interview with Joe Doliner, Co-Founder and CEO of Pachyderm ( Y Combinator Startup)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Inside-sales/ Pre-sales ConsultantLocation  BangaloreAbout employer Affine AnalyticsJob description: Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asInside-sales/ Pre-sales Consultant  Affine Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/05/inside-sales-pre-sales-consultant-affine-analytics-bangalore-2-3-years-experience/
"Interview with Joe Doliner, Co-Founder and CEO of Pachyderm ( Y Combinator Startup)",Learn everything about Analytics,"Introduction|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Inside-sales/ Pre-sales Consultant  Affine Analytics  Bangalore (2-3 years of experience)|How to create Parametric Survival model that gets right distribution?|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There is a reason why analytics, big data & data science are being considered hot fields today. The landscape of tools is changing every 6 months.If we were to ask you:Can you think of complete replacement of Hadoop ecosystem in Big Data Industry? Think and proceed !Pachyderm is one of the big data / analytics startup incubated atY Combinator Winter 2015 Batch. They claim to offer a complete replacement for Hadoop ecosystem. They provide anopen source analytics engine that uses Docker containers for distributed computations. Their product is known to havethe power of MapReduce without the complexity of Hadoop. This very idea caught our attention and wedecided to catch up with the management of Pachyderm to know about this idea and their ways of doing work.Even in his extremely busy schedule, Mr. Joe Doliner, Co-Founder & CEOof Pachydermagreed to do an exclusive interview with us.This conversation was aimed atknowing more about this exciting startup and how it is making using innovation to simplify handling big data. We were enthralled by his knowledge of big data / analytics domain. Talking with Joe gave us a lot of insights on how can creativity be applied to analytics which well share with you!Below is the brief transcript of this conversation:AV(Analytics Vidhya): Thanks Joe for taking time out of your busy schedule. Doinganalytics using docker is a new concept. Tell us more about this concept. How did you get this idea?Joe:Ive been interested in building a better Hadoop for a while. It always seemed needlessly inaccessible to me which is a shame because its such a powerful technology if you can harness it. However, its also a very hard problem and I didnt see a way to solve it until Solomon showed me Docker.I got an early peek at Docker when I was working at RethinkDB. This was really early on, in fact Docker was just a file that faked the interface, but I could already tell it was going to be cool. I really wanted to play with it and had a hunch that it might be the missing piece of the puzzle, so I set to work hacking with it. That code eventually turned into Pachyderm.AV: Tell us about your products & services? How would you plan to position it in the market?Joe:Pachyderm is a set of tools for doing analytics with Docker. Dockers a platform on whicha lot of great things are going to be built on. Its very important that we position ourselves as part of that movement. Already were finding that our earliest adopters are people and companies that have heavily invested in the Docker ecosystem.AV: I am sure you would have faced many hurdles before reaching this stage. What were these hurdles? How did you tackle them?Joe:Geez, there have been a bunch. Getting people, especially investors, to even consider that Hadoop could be replaced has been a big challenge. I wouldnt say that ones fully tackled yet, but the only solution is to ship products that get users excited.AV: What would be the plan B, if this hadnt worked out for you?Joe:This was the plan B, I wanted to be an astronaut.Im actually not sure what plan B would have been. Pachyderm really just started as me hacking on a side project because it seemed interesting. Even if we hadnt gotten into YC or gotten investments from anyone, Im pretty sure Id still be hacking on it. Although Id certainly have less time to do so. Thats the nice thing about personal projects, the only way they dont work out is if you give up on them.AV: As per your website, Pachyderm will eventually be a complete replacement for the Hadoop ecosystem. How do you plan to do so?Joe: By empowering open-source developers. Thats how Hadoop got to where they are today. Hadoop actually started out as a component of another project, Nutch, but it was very hackable and people wound up repurposing it to their needs. Weve built Pachyderm from the ground up to be easily hackable because thats the only way we can succeed. Open-source, when done correctly, lets thousands of developers all work together to create something great. Thats really the only way to create an ecosystem, you cant just hire that many people and even if you could, good luck managing that team.Open-source, when done correctly, lets thousands of developers all work together to create something great. Thats really the only way to create an ecosystem, you cant just hire that many people and even if you could, good luck managing that team.Mr. Joe Doliner, Co-Founder & CEOof PachydermAV: What will be the impact of Docker on current set of softwares available in analytics industry? Do you think Docker can bring a change in the way current analytics industry is being operated?Joe: The first order effect is that a lot of things should get less annoying. Things like setting up standard environments so that results can easily be reproduced are low hanging fruit that Docker can already solve well.The second order effect though is that Docker, and the ecosystem surrounding it, are making distributed systems an order of magnitude easier. Thats going to change the way people handle big data sets. A few years from now, data scientists will be able to click a few buttons and be running R in parallel over petabytes of data. Bringing about this second order effect is Pachyderms reason for existing.AV: Where do you see the market of Big Data industry evolving  3 years down the line? 5 years down the line?Joe:I expect well see it pervading companies more. The last 5 years weve seen big data explode on the back of a few well understood, important use cases. Things like optimizing user revenue and churn, those were the low hanging fruit. But over the next 5 years were going to see the long tail in which companies discover all the other ways data can inform their decisions. This is also going to turn employees who can do a bit of data science on top of their normal responsibilities into really valuable assets, since theyll be able to see the ways that data can help the team. Im probably preaching to the choir on this one, but if youve been thinking about brushing up on your data skills now is a good time to do it.AV: What are the present challenges in your mind and what are your future plans for next 3 years?Joe:As I mentioned before, we live and die by our community, so nailing that experience is our biggest challenge and probably will be for the next several years. Jump starting those developer communities is always hard for small companies. Thats one of the biggest things well be grading ourselves on in the next few years.Hiring is also a big immediate challenge that we face. But thats every tech companys biggest problem.AV: I see a lot of people who are interested to make a shift in big data analytics. What would be your guiding suggestions for them?Joe:First off, Id 100% recommend that they take the plunge. Second Id recommend thinking in terms of projects rather than abstract goals like learn tool X. Ive never been able to learn things when I think about it like that. Youll fair a lot better if set out to learn something cool from a data set and use that as an excuse to learn tool X. Actually early on Pachyderm was an excuse for me to learn Docker and Go. The third thing is to look out for the good communities. Long term the most relevant tools always wind up being the ones with the best communities around them.P.S  Our efforts doesnt end here. We plan to cover some more exciting startups in coming days. Do subscribe!",https://www.analyticsvidhya.com/blog/2015/05/interview-joe-doliner-founder-pachyderm-ycombinator-startup/
How to create Parametric Survival model that gets right distribution?,Learn everything about Analytics,"|Share this:|Like this:|Related Articles|Interview with Joe Doliner, Co-Founder and CEO of Pachyderm ( Y Combinator Startup)|Review  Business Analytics Post Graduate Program  Praxis Business School, Kolkata|
Tavish Srivastava
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"For an updated guide on parametric survival model, visit this post.",https://www.analyticsvidhya.com/blog/2015/05/finding-distribution-parametric-survival-model/
"Review  Business Analytics Post Graduate Program  Praxis Business School, Kolkata",Learn everything about Analytics,"First impressions  a small homely campus|First stop  Interaction with Dean & Associate Dean|The Course designer and Program Director  Prof. Prithwis Mukherjee:|Course Content / Structure:|Other faculty members:|The culmination of the course  placements|Thebest part  interaction with students & Bengali sweets!|Overall Verdict / My recommendation:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How to create Parametric Survival model that gets right distribution?|Swirl Package  Easy way to learn R, in R|
Kunal Jain
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently, I had to travel to Kolkata for a short trip. While I was going there, I thought, I should visit Praxis Business School. While I have talked to multiple stakeholders associated with Praxis in past and have heard good things about the course they run, you can never equate the experience of visit on the campus withtelephonic discussions!So, I called up the management of Praxis Business School and they were more than happy to have me there.In the first impression, Praxis came across as a small homely campus. The class-rooms, the management offices and the staff-room, everything was under a single roof. This also meant high accessibility  I could see faculty and students interacting with each other freely, but more on this later!The first thing I did was to meet Prof. Srinivas Govindrajan (Dean) & Prof. Charanpreet Singh (Associate Dean). Prof. Govindrajan is an IIM Ahmedabad alumni & currently serving as visiting professor at IIM Calcutta and IIM Raipur, in addition to being the Dean at Praxis. Prof. Singh, on the other hand is IIT Kanpur alumni who completed his MBA from University of Iowa and has been part of corporate world for 20 years before joining Praxis. He is a visiting professor at IIM Raipur.Over a cup of tea, we discussed how Analytics became a focus area for Praxis and how the first version of the program came together. Following were a few interesting notes I made from this discussion:In our constant endeavour to create professionals for changing industry requirements, we launched the Business Analytics program in 2011. We pioneered formal training in analytics in the country and are till date the only Business School that teaches a full-time, face-to-face program in analytics and takes ownership of the placement process. We have grown the batch from 8 in 2011-12 to 30+ in 2014-15. Our program evolves constantly  in line with the quickly changing analytics landscape  and our industry connect has grown significantly in the last 4 years. While our program is ranked among the top-5 analytics programs in the country, we have introduced analytics as a specialization in our flagship 2-year business management program as well.-Prof.Charanpreet Singh, Associate Dean, Praxis Business SchoolThere were enough references to Prof. Mukherjee during my discussions with Prof. Govindrajan & Prof. Singh. I was almost looking forward to meet up before meeting him.As soon as I met Prof. Mukherjee, I knew I am talking to a person who not only has immense industry experience under his belt, but is as much an hands on data scientist himself.Prof. Mukherjee is an IIT Kharagpur alumni, was Director at IBM and had served as a partner at PwC before joining Praxis. He enjoys hands on coding himself and has experimented with almost every open source tool available today. From setting up a Hadoop cluster to running recommendation engines for web apps, Mr. Mukherjee has done it all. I almost hope that I have same hunger for knowledge, when I have as much experience as he carries!Coming back to the program,the philosophy Prof. Mukherjee has stuck to whiledesigning the course is to cover a lot of breadth and focus on hands on exercises. Here are some more details I found out while I spent time with Prof. Mukherjee:The entire course at Praxis is divided in 4 trimesters  with the last 3 months being spent in interning. The curriculum during the 3 trimesters was exhaustive with 500+ hours of classroom interaction  much more than any other course has to offer. The course kicks off with a workshop on Excel, basics of R and Intro to SAS and ends up with modules on data visualization, text mining & NoSQL databases.As you can see, the course covers a wide spread of topics in a short amount of time, so if you are planning to take up this course, be prepared for 9 months of intensive learning!Tools covered during the course: SAS, R, Python, RapidMiner, Hadoop, Excel& TableauBusiness Analytics program at Praxis Business School is a perfect amalgamation of statistics and data science along with its application to business. Class room sessions by highly experienced faculty followed by a little self help guarantees a fantastic kick-start to the career as a data analyst/scientist. The course also takes into consideration the advancements in the field of data analytics by constantly revising the curriculum and arranging guest lectures by senior industry executives. The experience at Praxis for me has been very satisfactory and has been the most productive time of my career. Apuroop Pullabhatla, Data analyst at Abzooba, Praxis Analytics Batch of 2015Here is a link to the profile of all core and visiting faculty. It is a good mix of people from some of the best Education institutes and professional Organizations. Around 35% of the course is delivered from people in industry, while 65% of it is delivered by in house faculty.Probably the most important metric for a course like this is the placement ratio. At the time of publishing this, Praxis has managed to place 25 out of 30 students (as on 27th April 2015) out of the current outgoing batch. The process is still undergoing, so the number can change slightly. Some of the companies hiring this season were HSBC, Genpact, Karvy, ICICI Bank, HDFC Bank, KieSqaure Consulting, ICRA Techno Analytics and Happiest Minds. The salaries of placed students has varied from 4 Lakhs to 12 Lakhs p.a. (CTC).I think this is a huge achievement and a good reflection of the ownership shown at Praxis. With increasing awareness about this course, this number should only go up further.As part of my career aspirations of shifting to Analytics , I was looking a for a solid platform which could equip me with the all the necessary tools and techniques to make myself industry ready and opting for praxis business analytics program proved to be the right step.AseemDatta, Assistant Manager(Analytics Wing)  Genpact, Praxis Analytics Batch of 2015Once I had my interaction with the Dean(s)& Program Director, it was time for what I love the most  interacting with students. I spent about half an hour with a batch about to pass out from Praxis and tried understanding their experience. Here is what they had to say:By this time, I was almost running out of time. The campus was good 3 hours away from the airport. I couldnt say no to the delicious Rasagullas offered along with the lunch, so I took them and then went on to catch my flight.I would recommend Praxis as one of the best full time offering currently available in India. It is ideally suited for people with less than 5 years of experience, who can start with hands on roles in analytics and data science.For people with higher experience, it is a tricky situation to be in. The course definitely provides the learning, but analytics industry in India has not accepted high experience people making a career shift into analytics yet! Having said that, the best campus job this year was grabbed by a candidate with 7 years of experience. So it can happen, but you are taking a bit of a chance.Another point worth mentioning here is cost of the course. At a price of INR 4 Lakhs, I think the course offers a lot of value.I walked off from Praxis with a great satisfaction. I see a team dedicated to create one of the best full time offering for people interested in making a career in Analytics. The course has continuously evolved to keep up with high demand of the industry.I thank the management of Praxis to be a generous host and for those juicy Rasagullas and promise to host them, when they visit Delhi!If you have any further question on this course, Ill be more than happy to share my views & perspective about. If I dont have the answer myself, I can surely try to get it as well.",https://www.analyticsvidhya.com/blog/2015/04/review-business-analytics-post-graduate-program-praxis-business-school-kolkata/
"Swirl Package  Easy way to learn R, in R",Learn everything about Analytics,"Why should you learn R?|Is Swirl really for YOU?|How to get started with the Swirl Package?|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Review  Business Analytics Post Graduate Program  Praxis Business School, Kolkata|Comprehensive guide for Data Exploration in R|
Guest Blog
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People usually quote a steep learning curve as one of the reasons against R, when comparing R vs. Python. The reality is that people miss out on some easier ways to learn R.In this article, well introduce you with one such way to learn R in a fun and interactive way.This way is none other than use ofSwirl Package to learn R. Its tagline Learn R in R gives a clear picture of what this package intends to establish. Yes! this inbuilt package can act as your R programming trainer and make youfamiliar with basics ofR in a simplified manner. Moreover, the actions of this trainer will be based on the choices you enter in R console, like the kind of topic to study or the quiz you want to solve.Here are the reasons to learn R:Also See: Comprehensive learning path to learn R from scratchHere are few important points, such as for whom this package might be really useful, in which scenarios this package would work best and when should one look at other options for learning R rather than just using Swirl.Swirl is great for those people who:On the other hand, if you are planning to make a career out of R programming, then learning only with Swirl package would not suffice. You should definitely learn about the business applications that depend on R programming and further you need to understand how it is used in real-time industry scenarios. As such, you should go beyond just learning R with Swirl package if you:All in all, depending on whichever situation you are in, definitely there is no harm in starting your learning on R programming with Swirl package.Lets walk through the steps involved for getting started with Swirl package.Before we begin, its necessary to ensure both R and RStudio versions are already installed on your system. You can follow the video link for completing R installation on Windows platforms. In case, if you are having Linux or Mac platforms, lot of resources are available for helping you with respective R version installations.Though RStudio installation is not mandatory, its highly recommended because of the unique GUI interface which will make your experience with R much more enjoyable. Once installation is done, next step is to install a package then load the same to use its functions. This can be attained by the below commands.After loading the swirl package in your R console, the first function inswirl package you should enter is,It will prompt you to enter your name, and there after all swirl package commands will address you by your name, which kind of gives a feeling of a customized training being provided by the computer. After entering the name, it will further prompt you to select any one of the listed course options to begin your training with.In terms of course selection, you can either enter one of options listed in R console or you can do it manually by downloading the respective course of your interest from Swirl course repository. This task can be achieved using the below commands shown on the help page.Lets select option 1 to get started with the R Programming course of Swirl package. This course will automatically get installed on your system. After the course is installed, it will further prompt you to enter which lesson you want to start with. As you can see below, there are different courses like Basic Building Blocks, Workspace and Files and Base Graphics that exist under R Programming course.If you manage to complete all the 15 lessons present under R Programming course, you can successfully call yourself a basic R programmer which would be your first achievement to have under R skills. Well, this seems to be a straight forward way for anyone to pick up R programming skills just with the help of Swirl package and no other external help.However, this package falls short in providing more data science project examples that are widely used in the Industry. Definitely, one can become a good R programmer, but doing data science is all together a different game compared to picking up a programming language.It is all about how you can leverage R tool skills in solving a business problem as part of your data science project. Well, this would require another blog post all together, for now let us stick with how to acquire R tool skills using Swirl package.Learn more about Swirl package offerings at below link.http://swirlstats.com/In this article, we discussed the significance of Swirl Package in R i.e how this package can be easily adapted by beginners to learn R in a more fun and interactive way. You also learnt the importance of R programming, if you are the right match to use Swirl and how you can get started with Swirl package in RDid you find the article useful? Do let us know your thoughts about this guide in the comments section below.Kiran is a graduate of IIT-Madras with more than five years of professional experience in business analytics. He is currently a faculty at Jigsaw Academy. Prior to Jigsaw, he worked at LatentView Analytics delivering advanced analytics and business consulting solutions to various clients across verticals such as E-Commerce, Insurance, Technology and Financial Services. He has strong proficiency in working with tools such as SAS, R, MySQL, Python, Hadoop, Tableau etc. In free time, he enjoys participating in data mining contests on open platforms like Kaggle and CrowdAnalytix.",https://www.analyticsvidhya.com/blog/2015/04/swirl-package-easy-learn-r/
Comprehensive guide for Data Exploration in R,Learn everything about Analytics,"Introduction|Part 1: How to load data file(s)?|Part 2: How to convert a variableto different data type?|Part 3: How to transposeaData set?|Part 4: How to sort DataFrame?|Part 5: How to create plots (Histogram)?|Part 6: How to generatefrequency tables with R?|Part 7: How to sample Data set in R?|Part 8: How to remove duplicate values of a variable?|Part 9: How to find class level count average and sum inR?|Part 10: How to recognize and Treat missing values and outliers?|Part 11: How to merge / join data sets?|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Swirl Package  Easy way to learn R, in R|Best Data Science talks from PyCon Montreal 2015|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","How to load data file(s)?|How to convert a variableto different data type?|How to transposea table?|How to sort Data?|How to create plots (Histogram, Scatter, Box Plot)?|How to generatefrequency tables?|How to do sampling of Data set?|How to remove duplicate values of a variable?|How to group variables to calculatecount, average, sum?|How to recognize and treat missing values and outliers?|How to merge / join data set effectively?",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Till now we have already covered a detailed tutorials on data exploration using SASand Python. What is the one piece missing to complete this series. I am sure you guessed it right. In this article I will give a detailed tutorial on Data Exploration using R. For readerease, I will follow a very similar format we used in Python tutorial. This is just because of the sheer resemblance between the two languages.Herearethe operationIll cover inthis article (Refer to this article for similar operations in SAS):Input data sets can be in variousformats (.XLS, .TXT, .CSV, JSON ). In R, it is easy to load data from any source, due to its simple syntax and availability of predefined libraries. Here, I will take examples of reading a CSV file and a tab separated file. read.table is also an alternative, however, read.csv is my preference given the simplicity.
Code:All other Read commands aresimilar to the one mentioned above.Type conversions in R work as you would expect. For example, adding a character string to a numeric vector converts all the elements in the vector to character.Use is.xyzto test for data type xyz. Returns TRUE or FALSE
Use as.xyzto explicitly convert it.However, conversion of data structure is more critical than the format transformation. Here is grid which will guide you with format conversion :
It is also some times required to transpose a dataset from a wide structure to a narrow structure. Here is the code you use to do the same :
CodeSorting of data can be done using order(variable name) as an index. It can be based on multiple variables and ascending or descending both order.

CodeData visualization on R is very easy and creates extremely pretty graphs. Here I will create a distribution of scores in a class and then plot histograms with many variations.Lets try to find the assumptions R takes to plot this histogram, and then modify a few of those assumptions.As you can see, the breaks are applied at multiple points. We can restrict the number of break points or vary the density. Over and above this, we can colour the bar plot and overlay a normal distribution curve. Here is how you can do all this :
Frequency tables are the most basic and effective way to understand distribution across categories.Here is a simple example of calculating one war frequency :Here is a code which can find cross tab between two categories :
For sampling a dataset in R, we need to first find a few random indices . Here is how you can find a random sample:
This code will simply take out a random sample of 100 observations from the table mydata.Removing duplicates on R is extremely simple. Here is how you do it:
We generally use Apply functions to do these jobs.
Identifying missing values can be done as follows :And here is a quick fix for the same :As you can see, the missing value has been imputed with the mean of other numbers. Similarly, we can impute missing values with any best value available.This is yet another operation which we use in our daily life.To merge two data frames (datasets) horizontally, use the merge function. In most cases, you join two data frames by one or more common key variables (i.e., an inner join).Appending dataset is another such function which is very frequently used.To join two data frames (datasets) vertically, use the rbind function. The two data frames must have the same variables, but they do not have to be in the same order.In this comprehensive guide, we looked at the Rcodes for various steps in data exploration and munging. This tutorial along with the ones available for Python and SAS will give you a comprehensive exposure to the most important languages of the analytics industry.Did you find the article useful? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-r/
Best Data Science talks from PyCon Montreal 2015,Learn everything about Analytics,"Introduction|1.Other peoples messy data  How to deal with it?  PyCon 2015|2. Interactive data for the web  PyCon 2015|3. Data Science in Advertising  PyCon 2015|4. Machine Learning 101 PyCon 2015|5. Demystifying Docker  PyCon 2015|6. Advanced Git  PyCon 2015|7. Stream Parse: Real time stream with Python and Apache Storm  PyCon 2015|8. Graph Database Pattern in Python  PyCon 2015|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Comprehensive guide for Data Exploration in R|Data Hackathon by Analytics Vidhya & GL, Gurgaon, India, 24th May 2015|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Last week, we wrote an article on workshops inPyCon Montreal 2015- Hands on way to learn Python. Workshops are 3 hour long hands-on tutorials with resources to help audience learn these tools.We hope these videos helped you master Python further.In this article, we will cover another set of amazing talks from PyCon Montreal 2015. These talks are useful enough to trigger the excitement for getting acquainted with some fantastic features in Python.The talks are covered in order of complexity.We would recommend beginnersto strictly follow the sequence of these talks to avoid feeling lost, others may watch as per their ease.The best way to start this journey is to watch this talk. Why? Because, this is the problem every analyst would have faced at some point. Got data in form of PDF documents? Missing values? Undefined variables? And you are expected to build a model over and above this!In this video, Mali explains the funways to handle such data and building fantastic models instead of hating them.In this video, Sarahexplains the extensive use of Bokeh, a data visualization library apart from matplotlib, d3.js, seaborn used in Python. Sarah also defines the use of bokeh using variousmethods to make the data look more interactive on web.Also See: Ultimate Guide to learn Data Exploration in Python using NumPy, Matplotlib, PandaInteresting Watch! In this video, Soups Ranjan, Yelp defines how companies are making use of data science techniques in creating advertisements to delight their customers. He also touches upon the processes of ad selection, filtering, machine learning based CTR predictions etc. and eventually justifies the agenda of this talk.In simple words, Machine learning is an integration of Data Analysis and Automation. If you were looking for the resources tolearn Machine Learning, this is a good option to consider. The instructor beautifully explains the concept of machine learningusing the automation spectrum that covers the wide spread of ML.If you dont know about docker, this video will help. Docker is expected to change the way we look at virtualization and server deployment on the cloud.In this video, Andrew demystifies the concepts of an amazing technology known as Docker and gives a brief idea about its working, usage and the future of this technology. Importantly, he shares 5 tips that you should before you try out docker. Check this out.Also See: Step by Step Guide to become proficientat PythonGit is another tool every data scientist should be hands-on with. For the starters,Github is a website that allows you to upload your git repositories online. Interesting part is, git does not require the use of github. In this video, the advanced level of git has been explained using chapter wise understanding of the commands used in git. This video is more helpful for people who have a basic idea about git & github and are aspiring to conquer the next level.Steamparse lets you parse real time streams of data. It smoothly integrates Python Code and Apache Storm. In this video, Mr. Andrew flawlessly explains the concept of Stormand how beautifully it works with Python. This session gets concluded with a brief overview ofstreamparse & pykafka followed by some useful examples.This talk is focused on understanding the graph database patterns in Python. Elizabethexplains the importance of these patterns followed by explanation of concepts such as Property Graph Definition, Grelim Query Language, Python Pattern for Titan Models, Graph at scale with Cassandra, Elasticsearch, Titan. This talk will give you a fair understanding on this component of Python which you can further refine by personal efforts.In this article, we have featured the list of best data science related talks from PyCon Montreal 2015. If you have not attended PyCon, you should definitely look at these videos. These talks will not only trigger your interest in doing Data Science on Python, but also tell you about some of the tools which Data Scientists will use in future.These talks are meant to whet your appetite on various topics. You can learn these concepts further by accessing the plethora of free resources available online.Incase you find any difficulty in learning any concept, feel free to ask our analytics experts here.",https://www.analyticsvidhya.com/blog/2015/04/pycon-montreal-2015-data-science-talks/
"Data Hackathon by Analytics Vidhya & GL, Gurgaon, India, 24th May 2015",Learn everything about Analytics,"Introduction||Contact|Share this:|Like this:|Related Articles|Best Data Science talks from PyCon Montreal 2015|Application of PageRank algorithm to analyze packages in R|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every data scientist/business analyst or anyone who is tenaciously working hard to rise up the ladder of success in analytics industry, wants to build his/her portfolio bysolving kaggle questions. Unfortunately, not everyone succeeds in solving it. Eventually, people get demotivated and succumbs to inaction.To convert this failure into a success, Analytics Vidhya is organizing Data Hackathonwhere some determined brains from across the country will come and solve a kaggle competition by forming a team.This session will be steered by Mr. Kunal Jain, Data Scientist, Growth Hacker.Why should you attend this?1. If you want to know about the best tips & tricks to successfully solve kaggle questions, you should attend this.2. If you are striving for self-evaluation, knowing your strength and weakness, you should attend this. You will understand and realize the weak points where you are struggling and how you can improve.3. If you are looking to explore analytics in depth, this can be an ideal platform for you.You will get the chance to compete with best brains in data analytics around your area and get the inspiration to move a step ahead of where you are standing now.4. Needless to say, this is going to be a great place for networking.If these reasons excites you enough, you can register for Data Hackathonhere.Here are some important details:Date  24th May2015Venue  Great Lakes Institute of Management, 815, Udyog Vihar, Phase V, Gurgaon, Haryana, India (Nearest Metro Station is Indusland Rapid Metro)P.S  After receiving numerous requests, we will also be organizing Data Hackathon in Bangalore, Kolkata and Mumbai. Below are the links for registration:Bangalore, 17th May 2015: Click hereMumbai, date to be announced: Click hereKolkata, date to be announced: Click hereFor more information & details on these meetups, feel free to drop us a mail at [emailprotected] or you can also reach us at 0124-4264086.",https://www.analyticsvidhya.com/blog/2015/04/data-hackathon-analytics-vidhya-gl-gurgaon-india-24th-may-2015/
Application of PageRank algorithm to analyze packages in R,Learn everything about Analytics,"Introduction|Packages in R|Practice Problems|Solutions||End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Hackathon by Analytics Vidhya & GL, Gurgaon, India, 24th May 2015|Key Takeaways from Andrew Ng and Adam Coates AMA on Reddit|
Tavish Srivastava
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1. Packages which form foundation of R :|2. How many packages were added in Oct14  Apr15|3. How has this impacted the importance of the most critical package?|4 . What are the dependencies of the most critical packages?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In the previous article, we talked about a crucialalgorithm namedPageRank, used bymost of the search engines to figure out the popular/helpful pages on web. We learnt that however, counting the number of occurrences of any keyword can help us get the most relevant page for a query, it still remains a weak recommender system.In this article, we will take up some practice problemswhich will help you understand this algorithm better. We will build a dependency structure between R packages and then try to solve a few interesting puzzles using PageRank algorithm. But before we do that, we shouldbrush up our knowledge on packages in R for better understanding.R is language built ontop of many different packages. These packages contain user-ready functions which make our job easier to do any kind of statistical analysis. These packages are dependent on each other.For instance, a statistical package like Random Forest is dependent onmany packages to seekhelp in evaluating multiple statistical parameters. Before going into dependency structures, lets learn a few commands on R :miniCran : Many private firms like to create a mirror image of all the packages R has to offer on their own server. This package miniCran helps these firms to create a subset of packages required. But why do we even need a package to create a mirror image?The reason is that there is a complex structure between these packages of dependencies. These dependencies tell R which package should get installedbefore installing a new package. miniCran islike a blueprint of all these packages.Function pkgDep : This is a magic function which will bring out all the dependencies for a package. There are three different types of dependencies between packages. Lets understand these three:Lets now try our hand on a few package dependency structures:As you can clearly see from the above example, the package ggplot2 has a import dependency on 22 packages and another 35 suggest packages. Lets try to visualize how this dependency structure looks like. Here is a code which will help you with that :As we can clearly see, packages like lattice or stringr are not directly linked to ggplot2, but shouldbe installed prior to installingggplot2.Hopefully,now we have gained decent knowledgeon these packages, lets do some practice problems on them for deeper understanding.Here are a few questions using which we willunderstand more about R packages :1. Which packages forms the foundation of R? In other words, which packages are the most used or referred package in R?2. How many package were added in the window Oct14 to April 15?3. How has the importance of most critical package changed over the window Oct14 to April 15? Has it increased, decreased or remained same?4. What are the dependencies of the most critical package?We will use PageView algorithm to find the most important packages. The simple philosophy being, the packages which are referred by many different packages on R are the ones forming the foundation of R. Our analysis will be basedon the latest CRAN image available. Here is the code which canhelp you to find the same:As is clearly seen inthe table above, the most important packages are MASS and Rcpp.Evidently, they form the backbone of R.Well run a similar code to find the number of packages in Oct14 and then make a comparison between them. Following is the code to solve this question:498 packages were added in this duration. Wow, its like 70 packages every month!In section 1, we saw MASS as the most critical package with an importance of0.020425142 in Apr15. In case, if it proportionally goes upwith lesser number of package in Oct14, its importance can be calculated as:~ 0.020425142 * nrow(pr)/nrow(pr1)  =0.02212809Now, lets find its actual importance in Oct14:Hence, the importance of MASS has not dropped to the extent it should have because of the increase in the number of packages in the window. This can be just because the new packages would have been using this package MASS equally and hence increasing the importance.This one is the simplest to crack. We already know the function which can bring out all the dependencies of a package. Following code will help you with this one :PageRank comes very handy in any importance determining exercise which has a linkage structure to it. Similar analysis can be done on Python to understand the structure beneath the top level packages which we use so conveniently. This analysis can also be done in social networks to understand the magnitude of influence a user has on social network.To end the discussion, lets view the network structure of the top 10 packages on R :Thinkpot: Can you think ofmore usage of Page Rank algorithm? Share with us useful links to practicePage Rank algorithm in various fields.Did you find this article useful? Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/04/application-pagerank-algorithm/
Key Takeaways from Andrew Ng and Adam Coates AMA on Reddit,Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Application of PageRank algorithm to analyze packages in R|AVturns2: Let the celebrations begin!|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"At Baidu, our goal is to develop hard AI technologies that impact hundreds of millions of users across the world.  Andrew NgIn case you missed it, let me set the context of this erudite discussion which happened on Reddit on 14th April 2015. This was an AMA with Andrew Ng,Chief Scientist at Baidu Research/Coursera Co-Founder/Stanford Professor and Adam Coates, Director of Baidu Silicon Valley AI Labs.The thread manifestedendless appreciations for Andrew Ng for his fantastic Machine Learning course on Coursera. Needless to say, the questions were satisfactorily answered exhibiting a few eye openers too, also mentioned below.Below are a few key takeaways from this AMA. We have quoted the answers from the AMA for each of the sections directly:(Note: The answers have been given together by Andrew and Adam.)1. Advice on career in Machine Learning for people trying to learn Machine LearningBuilding a strong portfolio of projects done through independent researchis valueda lot in industry. For example, at Baidu Research we hire machine learning researchers and machine learning engineers based only on their skills and abilities, rather than based on their degrees and past experience (such as demonstrated in a portfolio of projects) helps a lot in evaluating their skills.I think mastering the basics of machine learning should be the best first step. After that, Id encourage you to find projects to work on and to use this to keep learning as well as to build up your portfolio. If you dont know where to start, Kaggle is a reasonable starting place; though eventually you can thenidentify and work on your own projects. In the meantime, offline engagement such as reachingout to professors, attend local meetups, try to find a community helps a lot.This is often enough to find you a position to do machine learning work in a company, which then further accelerates your learning.2. Whether a PhD is mandatory for a career in MLDoing PhD is one great way to learn about machine learning. But the irony is, many top machine learning researchers do not have a PhD.Given my (Andrews) background in education and in Coursera, I believe a lot in employee development. Thus at most of the teams Ive led (at Baidu, and previously when I was leading Googles Deep Learning team/Google Brain) I invested a lot in training people to become expert in machine learning. I think that some of these organizations can be extremely good at training people to become great at machine learning.I think independent learning through Coursera is a great step. Many other software skills that you may already have are also highly relevant to ML research. Id encourage you to keep taking MOOCs and using free online resources (like deeplearning.stanford.edu/tutorial). With sufficient self-study, that can be enough to get you a great position at a machine learning group in industry, which would then help further accelerate your learning.3. Best follow up courses/self projects afterCoursera ML course1. Many people are applying ML to projects by themselves at home, or in their companies. This helps both with your learning, as well as helps build up a portfolio of ML projects in your resume (if that is your goal). If youre not sure what projects to work on, Kaggle competitions can be a great way to start. Though if you have your own ideas Id encourage you to pursue those as well. If youre looking for ideas, check out also the machine learning projects my Stanford class did last year:http://cs229.stanford.edu/projects2014.htmlIm always blown away by the creativity and diversity of the students ideas. I hope this also helps inspire ideas in others!2. If youre interested in a career in data science, many people go on from the machine learning MOOC to take the Data Science specialization. Many students are successfully using this combination to start off data science careers.https://www.coursera.org/specialization/jhudatascience/14. Use of Machine Learning concepts:A lot of deep learning progress is driven by computational scale, and by data. For example, I think the bleeding edge of deep learning is shifting to HPC (high performance computing aka supercomputers), which is what were working on at Baidu. Ive found it easier to build new HPC technologies and access huge amounts of data in a corporate context. I hope that governments will increase funding of basic research, so as to make these resources easier for universities all around the world to get .5. Important set of skills for Machine LearningThe skillset needed for different problems is different. But broadly, the two sources of knowledge a program can have about a problem are (i) what you hand-engineer, and (ii) what it learns by itself from data. In some fields (such as computer vision; and I predict increasingly so speech recognition and NLP in the future), the rapidly rising flood of data means that (ii) is now the dominant force, and thus the domain knowledge and the ability to hand-engineer little features is becoming less and less important.5 years ago, it was really difficult to get involved in computer vision or speech recognition research, because there was a lot of domain knowledge you had to acquire. But thanks to the rise of deep learning and the rise of data, I think the learning curve is now easier/shallower, because whats driving progress is machine learning+data, and its now less critical to know about and be able to hand-engineer as many corner cases for these domains. Im probably over-simplifying a bit, but now the winning approach is increasingly to code up a learning algorithm, using only a modest amount of domain knowledge, and then to give it a ton of data, and let the algorithm figure things out from the data.6. Excitement ofworkOne of the things both of us (Adam & Andrew) talk about frequently is the impact of research. At Baidu, our goal is to develop hard AI technologies that impact hundreds of millions of users. Over time, I think weve both learned to be more strategic, and to learn to see more steps out aheadbeyond just writing a paperto plot a path to seeing our technology benefit huge numbers of people. These days, this is one of the things that really excite us about our work!7. Single Layer Networks vs Deep Learning NetworksOne of the reasons we looked at single layer networks was so that we could rapidly explore a lot of characteristics that we felt could influence how these models performed without a lot of the complexity that deep networks brought at the time (e.g., needing to train layer-by-layer). There is lots of evidence (empirical and theoretical) today, however, that deep networks can represent far more complex functions than shallow ones and, thus, to make use of the very large training datasets available, it is probably important to continue using large/deep networks for these problems.Thankfully, while deep networks can be tricky to get working compared to some of the simplest models in 2011, today we have the benefit of much better tools and faster computers  this lets us iterate quickly and explore in a way that we couldnt do in 2011. In some sense, building better systems for DL has enabled us to explore large, deep models at a pace similar to what we could do in 2011 only for very simple models. This is one of the reasons we invest a lot in systems research for deep learning here in the AI Lab: the faster we are able to run experiments, the more rapidly we can learn, and the easier it is to find models that are successful and understand all of the trade-offs.Sometimes the best model ends up being a bit more complex than we want, but the good news is that theprocessof finding these models has been simplified a lot!8. Deep Learning vs Recurrent Learning NetworksI think RNNs are an exciting class of models for temporal data! In fact, our recent breakthrough in speech recognition used bi-directional RNNs. Seehttp://bit.ly/deepspeechWe also considered LSTMs. For our particular application, we found that the simplicity of RNNs (compared to LSTMs) allowed us to scale up to larger models, and thus we were able to get RNNs to perform better. But at Baidu we are also applying LSTMs to a few problems were there is are longer-range dependencies in the temporal data.To check out this complete discussions, you can visit the thread here.Also See:On the eve of our second anniversary, an exciting dataset competition is currently going on where you can participate and win exciting amazon vouchers. Also, youget entry to exclusive our whatsapp group community. Here you can start: Click here.For latest happenings on this contest, check out our FB page.",https://www.analyticsvidhya.com/blog/2015/04/learn-andrew-ng-adam-coates-ama-reddit-reflections/
AVturns2: Let the celebrations begin!,Learn everything about Analytics|Plan for the anniversary:,"Our followers and readers:|Analytics Vidhya Team:|And our families:|Share this:|Like this:|Related Articles|Key Takeaways from Andrew Ng and Adam Coates AMA on Reddit|Effective data exploration / processing using FIRST. & LAST. in SAS PDV|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Let the Celebrations begin! Waiting to hear more from you on this special day.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Two years back, on this very day (wellactually night), I posted first article on Analytics Vidhya. Little did I know, what was in store! The excitement about creating one of the best analytics & data science community has only grown exponentially. Analytics Vidhya moved from being a part time leisure blog to (more than) full time engagement for me.In the last 2 years, we have created one of the worlds largest analytics community. This journey would not have been possible without the support of the following three groups (in no particular order):I wish to say my heart felt thanks to all three groups and many more involved in our journey. This journey thrives on the mutual eco-system of all 3 groups. Let me express my gratitude to each of them quickly:You are the source of energy driving this engine. It is that comment you write, that question you ask or that mail you drop, which keeps us going. Our readership has grown beyond the stage where we can name people individually. But every time we meet a reader, we can see the glow in their eyes. I am sure they can see it in our eyes as well.So keep writing to us, talking to us and most importantly keep contributing to the knowledge creation through blogs and discussion portal. In order to hear more from you, here is a page we have created where you can leave your testimonials about Analytics Vidhya. The best testimonial wins an Amazon voucher and a few good ones win Analytics Vidhya merchandise.What can I say about this awesome team we are building! Be it those calls we make to each other at 2 o clock in the night or those chai (tea) sessions we have  we just love working together. I want to thank the entire Analytics Vidhya team for making it the place it is. It is because of this them that we continue to dream and dream big.I think our families are the ones, who feel the heat more than we do. It is this support system which enables us to put in these extra long hours to make sure our readers get those awesome articles in their mailboxes. Thanks for being there for us and understanding our long and erratic work routines to keep this engine running.We have some exciting things lined up forthe day and week to celebrate the anniversary. As usual, it would involve some data crunching and shouting! But most importantly, it involves a bunch of new things we plan to do:",https://www.analyticsvidhya.com/blog/2015/04/analytics-vidhya-turns-2/
Effective data exploration / processing using FIRST. & LAST. in SAS PDV,Learn everything about Analytics,"What is FIRST. & LAST. ?|What are the uses of FIRST. and LAST. ?|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|AVturns2: Let the celebrations begin!|PyCon Montreal 2015 tutorials  Hands-on way to learn Data Science in Python|
Guest Blog
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Business Problem|Problem 1: Find the total customers having only a single day of usage.|Problem 2:Find the total usage of each customer for Home & Roaming Circles.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Efficiency in coding differentiates a good coder from a bad coder. While you dont need to be an awesome coder necessarily to be a good analyst, being a good coder always gives you that extra edge. In this article, I am explaining a few simple tricks with FIRST. & LAST. variables in SAS to help you become better in processing data.The SET and BY statements in a data step tell SAS to process the data by grouping observations together. Whenever we use BY statement with a SET statement, SAS automatically creates two temporary variables for each variable name that appears in the BY statement. One of the temporary variables is called FIRST.variable, where variable is the variable name appearing in the BY statement. The other temporary variable is called LAST.variable. The two variables always equals either 1 or 0 when:SAS uses the value of the FIRST.variable and LAST.variable to identify the first and last observations in a group. SAS places FIRST.variable and LAST.variable in the Program Data Vector (PDV). Then, they becomeavailable for DATA step processing but SAS does not add them to the output data set asthey are temporary in nature.Lets look at some of the many uses of FIRST.variable and LAST.variable in SAS using the business problem below:We have got a data set as shown below from a mobile network provider. Itcontains the following information:We need to performthe following exercises:Note: These problems form a very important part of the exploratory phase of the project and can provide key insights about your customers.Can you think of ways to solve this problem? How easy or difficult does it look?Lets solve them and see!Answer:Since this dataset is small, we can easily make out Henry and Steve have only one record each in this dataset. Lets try to find this answer using theconcept of FIRST. and LAST. Well sort the data by Cust_ID then FIRST.Cust_ID and LAST.Cust_ID and make sure that these two records must beequal to one.Lets verify this, by running the code below and observing the output:Code:Output:Answer:To solvethis problem, I havefirst sorted the data by Cust_ID and then by Circle.Lets see how the variables first. and last. are created for each BYvariable:Code:Intermediate data set:Lets understand the intermediate data set shown above by focussingat customer Alan:Lets look at the below codes to evaluate the total usage by customer and circle:Code:Lets understand the code by looking at the intermediate data set:Output:In this article, we discussed the concept of FIRST. and LAST. in SAS using an example. If you have used it in any other way, or if there is a better way of implementing this concept, please do make suggestions.Did you find the article useful? Do let us know your thoughts about this article in the box below.This article has been contributed by Shuvayan Das. Shuvayan has 4 years of experience with TCS, has undergone training with Jigsaw Academy and is proficient with SAS, SQL & Excel. He was also featured as Newbie of the month in our recent newsletterand happens to beone of the most active users on Analytics Vidhya Discuss.",https://www.analyticsvidhya.com/blog/2015/04/first-last-sas/
PyCon Montreal 2015 tutorials  Hands-on way to learn Data Science in Python,Learn everything about Analytics,"Introduction||List of Workshops at PyCon 2015|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Effective data exploration / processing using FIRST. & LAST. in SAS PDV|PageRank explained in simple terms!|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1. Sarah Guido  Hands on Data Analysis with Python|2. IPython & Jupyter in Depth: High Productivity Interactive and Parallel Python|3. Pandas From The Ground Up|4. Statistical inference with Computational Methods|5. Making Beautiful Graphs in Python and Sharing Them|6. Bayesian statistics made simple|7. Machine Learning with Scikit-Learn (I)|8. Machine Learning with Scikit-Learn (II)|9. Winning Machine Learning Competitions With Scikit-Learn|10.Hands-on with Pydata: How to build a minimal recommendation engine?|11. Practical Graph/Network Analysis Made Simple|12. Twitter Network Analysis with NetworkX|13. Learn Hadoop with Python,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"PyCon(s) carryabenevolent motive of helping the Python community worldwide by providing extensive knowledge resources.I started following PyCon conferences from 2013. My first learning experience from PyCon tutorials & workshops inspired me to follow it back in the year 2014 and this craze continued in 2015 as well.You can check out the training recommendation for tutorials of Pycon 2014 here.Watching these PyCon videos has been an immense learning for me and I am glad that I could spare time to go through them. The event is divided in 2 parts: Tutorials (Workshops) or talks. Workshops aim to provide 3 hour hands on sessions where the instructor also acts as a facilitator. In this article, I have compiled a list of workshop videos you should watch from PyCon 2015.To remove the confusion (because there are quite a few talks on data science here), I have made a recommended roadmap. Thisroadmap is a sequence of tutorials & workshop held at PyCon 2015 watched in a structure.Lets get started!Below is the roadmap of the workshops held at PyCon 2015. We recommend beginners to watch these videos in the listed sequence to help understand these concepts better, intermediates & experts can watch these videos as per their ease.Goodplace to start learning data science in Python! Watch Sara explain these concepts in very simple manner. This video will be helpful for anyone wanting to perform data analysis in Python.I personally use IPython notebooks for the interactive data exploration and recommend it to every data science professional. In case you are wonderingwhat are Python notebooks? this video is just the right place to start. It provides a super basic introduction of notebooks in Python, which then moves on to the explanationof super developed notebooks for Python such as iPython and Jupyter.Data Science in Python couldnt have been effective without Pandas. This video will help you to understand Pandas by performing various exercises as practiced by the instructor. He emphasises more on learning while solving exercises. Apart from that, you will get to know about various functions in panda that you might have missed out till now.This videos demonstrates the methods to make statistical inferences in Python by evaluating the sample, quantifying precision, hypothesis testing and performing similar steps. The instructors also reflects uponthe massively scalable potential of python in statistical analysis.This workshop beautifully explains the concept of data visualization supported by various other features (matplotlib) of python which areused to make your visualizations more apt and appealing.It also feature sets of challenges which will definitely excite your grey cells.Remember the Monty Pythons Deal of No Deal? This video is must watch for anyone who is wanting to learn Bayesian statistics from scratch. This workshop begins with deriving Bayes theorem, then proceeds to the Bayesian statistics followed by solving some real world cases. It gives an awesome in-depth overview of Bayesian statistics.This videoexplains the in depth concepts of Machine Learning. It begins with explaining machine learning with scikit-learn, then explains the concepts of supervised and unsupervised learning and eventually, concludes with model validation. This is useful hub of knowledge for machine learning enthusiasts.This video starts from where the previous one ends. It further deep dives into the concepts of machine learning, thereby, explaining the concepts such as heterogeneous data modelling, text feature extraction, clustering, large scale text classification for sentimental analysis etc. followed by some practice exercises, which willdefinitely compel you to think hard.This video reveals how does winning data scientists think while solving Kaggle competitions? Here, the instructors has intended to create a contest similar to Kaggle competition among the attendeesby forming their groups and helping them with the required tips, tricks, hacks used to solve such questions.By watching this video, you will get acquainted with theconcept of recommendation problem supported by its challenges and solutions. It also covers various practice problems on pandas, DataFrames, setup evaluation functions, test dummy solutions etc.Networks Redefined! This is one of the best video I have come across pertaining to Network Analysis. The instructor tries to explain the these concepts in the most simpler manner. This video illustratesevery possible example which can help you to understand this concept without any difficulty.Lately, I met a lot people who are wanting tolearn more about Twitter Analysis. This is a must watch for the ardent lovers ofsocial media analysts. In this video, twitter network analysis has been explained using theconcepts of network theory/networkX and twitter APIs.This is a good resource available for learning Hadoop with Python. The instructor flawlessly explains the concepts such as MapReduce, Pig, Snakebite for HDFS, HBase, Spark & PySpark in a simple manner. This is a must watch if you are inclined towards big data.14. Introduction to Spark with PythonThis video explains the distributed data processing framework  Spark. Spark is based on resilient distributed datasets. It is generally used for big data processing. The trainer has takenthe coding, altogether to a different level. This video is helpful for group of enthusiasts who make use ofpython for handling big data.In this article, we covered the list of data science related workshops held at PyCon Montreal 2015. We also defined a roadmap to help beginners learn python one step at a time. I believe these are fantastic resources for hands on learning and developing data science skills.If you face any difficultieswhile learning Python, feel free to ask us here. In the next article, well bring up the list of most useful talks related to Data Science in PyCon 2015.Did you find the article useful? Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/04/pycon-montreal-2015-data-science-workshops/
PageRank explained in simple terms!,Learn everything about Analytics,"An artificial web world|Mathematical Formulation of Google Page Rank|Teleportation adjustments|Other uses of PageRank & End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|PyCon Montreal 2015 tutorials  Hands-on way to learn Data Science in Python|Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In my previous article, we talked about information retrieval. We also talked about how machine can read the context froma free text.Lets talk about the biggest web information retrieval engine i.e. Google! Imagine, you were to createGoogle search in a world devoid of any search engine. What could bethe basic rules you will code to build such a search engine? If your answer is to use Term Frequency or TF-IDF kind of framework, consider followingcase:A user enters the query : Harvard Business School. He expects the first link to be http://www.harvard.edu/. But what wouldyour algorithm do? It would try to find out pages which has the word Harvard maximum number of times, as Business and School will come out to be common words. Now, there is a possibility that Harvard keyword might not be repeated multiple times on Harvards own website. However, websites like Business school consultants or articles on business school might have this keyword multiple times. This leads these websites to achieve a rank much higher than the actual business school website.But, do search engines like Google face this challenge today? Obviously not! This is because they take help of an algorithm known as PageRank. In this article, we will discuss the concept of PageRank. In the next article, we will take this algorithm a step forward by leveraging it to find the most important packages in R.Imagine a web which has only 4 web pages, which are linked to each other. Each of the box below represents a web page.The words written in black and italics are the links between pages.For instance, in the web page Tavish, it has 3 outgoing links : to the other three web pages. Now, lets draw a simpler directed graph of this ecosystem.Here is how Google ranks a page : The page with maximum number of incoming links is the most important page. In the current example, we see that the Kunal Jain page comes out as the most significant page.First step of the formulation is to build a direction matrix. This matrix will have each cell as the proportion of the outflow. For instance, Tavish (TS) has 3 outgoing links which makes each proportion as 1/3.Now we imagine that if there were a bot which will follow all the outgoing links, what will be the total time spent by this bot on each of these pages. This can be broken down mathematically into following equation :Here A is the proportions matrix mentioned aboveX is the probability of the bot being on each of these pagesClearly, we see that Kunal Jains page in this universe comes out to be most important which goes in the same direction as our intuition.Now, imagine a scenario where we have only 2 web pages : A and B. A has a link to B but B has no external links. In such cases, if you try solving the matrix, you will get a zero matrix. This looks unreasonable as B looks to be more important than A. But, our algorithm still givessame importance for both. To solve for this problem, a new concept of teleporatation was introduced. We include a constant probability of alpha to each of these pages. This is to compensate for instances where a user teleports from one webpage to other without any link. Hence, the equation is modified to the following equation :Here, b is a constant unit column matrix. Alpha is the proportion of teleportation. The most common value taken for alpha is 0.15 (but can depend on different cases).In this article we discussed the most significant use of PageRank. But, the use of PageRank is no way restricted to Search Engines. Here are a few other uses of PageRank :Thinkpot: Can you think ofmore usage of Page Rank algorithm? Share with us useful links to leverage Page Rank algorithm in various fields.Did you find this article useful? Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/04/pagerank-explained-simple/
"Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas",Learn everything about Analytics,"Introduction||Contents  Data Exploration|Part 1: How to load data file(s)?|||Part 2: How to convert a variableto different data type?|Part 3: How to transposeaData set?|Part 4: How to sort DataFrame?|Part 5: How to create plots (Histogram, Scatter, Box Plot)?|Part 6: How to generatefrequency tables with pandas?|Part 7: How to do sample Data set in Python?|Part 8: How to remove duplicate values of a variable?|Part 9: How to group variables in Python to calculatecount, average, sum?|||Part 10: How to recognize and Treat missing values and outliers?|Part 11: How to merge / join data sets?|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|PageRank explained in simple terms!|Information Retrieval System explained in simple terms!|
Sunil Ray
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","How to load data file(s)?|How to convert a variableto different data type?|How to transposea table?|How to sort Data?|How to create plots (Histogram, Scatter, Box Plot)?|How to generatefrequency tables?|How to do sampling of Data set?|How to remove duplicate values of a variable?|How to group variables to calculatecount, average, sum?|How to recognize and treat missing values and outliers?|How to merge / join data set effectively?||Loading data from CSV file(s):||Loading data from excel file(s):|Loading data from txt file(s):|Convert numeric variables to stringvariables and vice versa|Convert character date toDate:|Histogram:|Scatter plot:|Box-plot:",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Exploring data sets and developing deep understanding about the data is one of the most important skill every data scientist should possess. People estimate that time spent on these activities can go as high as 80% of the project time in some cases.Python has been gaining a lot of ground as preferred tool for data scientists lately, and for the right reasons. Ease of learning, powerful libraries with integration of C/C++, production readiness and integration with web stack are some of the main reasons for this move lately.In this guide, I will use NumPy, Matplotlib, Seaborn and Pandas to perform data exploration. These are powerful librariesto perform data exploration in Python. The idea is to create a ready reference for some of the regular operations required frequently.I am using iPython Notebook to perform data exploration, and would recommend the same for its natural fit for exploratory analysis.In case you missed it, I would suggest you to refer thebaby steps series of Python to understand the basics of python programming.Herearethe operationIll cover inthis article (Refer to this article for similar operations in SAS):Input data sets can be in variousformats (.XLS, .TXT, .CSV, JSON ). InPython, it is easy to load data from any source, due to its simple syntax and availability of predefined libraries.Here I will make use of Pandas. It features a number of functions for reading tabular data as a DataFrame object. Below are the common functions that can be used to read data.CodeOutputCode
df=pd.read_csv(E:/Test.txt,sep=\t) # Load Data from text file having tab \t delimeter print dfOutput
Converting a variable data type to other is important and common procedure we perform after loading data. Lets look at some of the commands to perform these conversions:The later operations are especially useful when you input value from user using raw_input(). By default, the values are read at string.There are multiple ways to do this. The simplest would be to use datetime library and strptime function. Here is the code:
Here, Iwant to transpose TableA into Table B on variable Product. This task can be accomplished by usingdataframe.pivot.Code#Transposing dataframe by a variableOutput

Sorting of data can be done using dataframe.sort(). It can be based on multiple variables and ascending or descending both order.CodeAbove, we have a table with variables ID, Product and Sales. Now, we want to sort it by Product and Sales (in descending order) as shown in table 2.Data visualization always helps to understand the data easily. Python has library like matplotlib and seaborn to create multiple graphs effectively. Lets look at the some of the visualization to understand below behavior of variable(s) .CodeOutput
CodeOutput
Code
Output


Frequency Tablescan be used to understand the distribution of a categorical variable or n categorical variables usingfrequency tables.CodeOutput


To select sample of a data set, we will use library numpy and random. Sampling of data set always helps to understand data quickly.Lets say, from EMP table, I want to select random sample of 5 employee.CodeOutput

Often, we encounter duplicate observations. To tacklethis in python, we can use dataframe.drop_duplicates().CodeOutput
To understand the count, average and sum of variable, I would suggest you to use dataframe.describe() with groupby().Lets look at the code:CodeOutputTo identify missing values , we can use dataframe.isnull(). You can also refer article Data Munging in Python (using Pandas), here we have done a case study to recognize and treat missing and outlier values.CodeOutputTo treat missing values, there are variousimputation methods available. You can refer these articles for methods to detectOutlier and Missing values. Imputation methods for both missing and outlier values are almost similar. Here we will discuss general case imputation methods to replace missing values. Lets do it using an example:Code:
Joining / merging is one of the common operation required to integrate datasets from different sources. They can be handled effectively in Pandas using merge:Code:In this comprehensive guide, we looked at the Pythoncodes for various steps in data exploration and munging. We alsolooked at the python libraries like Pandas, Numpy, Matplotlib and Seaborn to perform these steps.In next article, I will revealthe codes to perform these steps in R.Also See: If you have any doubts pertaining to Python, feel free to discuss with us.Did you find the article useful? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/
Information Retrieval System explained in simple terms!,Learn everything about Analytics,"Introduction|Activity  Information Retrieval in Web search:|Business Case|Solving the puzzle using Text Mining|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Ultimate guide for Data Exploration in Python using NumPy, Matplotlib and Pandas|Internet of Things (IoT) and its impact on data science!|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",What is Information Retrieval System?|     How does an algorithm catch the similarity and retrieve the right set of web pages for us?|Term Frequency (TF) Matrix :|Inverse Document Frequency Matrix(IDF) :|TF-IDF Matrix :,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"While searching for things over internet,I always wondered, what kind of algorithms mightbe running behind these search engineswhich provide us withthe most relevant information? How do they decide which result to show for which set of search keywords.This might be a no brainer for a few people, but definitely an interesting problem for some of the best brains around the world. To find the answer, I read every guide, tutorial, learning material that came my way. Eventually,I learnt about the Information Retrieval System.Information retrieval system is a network of algorithms, which facilitate the search of relevant data / documents as per the user requirement. It not only provides the relevant information to the user but also tracks the utility of the displayed data as per user behaviour, i.e. Is the user finding the results useful or not?In this article, I have explained the basic techniques used for Information Retrieval. The algorithms used by Yahoo and Google are much more complex compared to the ones mentioned in this article, but still you will get a sense of what goes on in the background when you make these searches.Lets understand more about information retrieval system algorithm using the activity and a business case below:Try to search for the queries below and notice the differences in search engine results :Inference: You will notice that the first 3 searches gavesimilar results while 4th and the 5th search result displayed a different result. This is expectedbecause what we are asking in the first 3 queries is quite similar. Hence, the result.That is interesting! But, the question remains:-This was just one part ofinformation retrieval (IR) . IR is no way limited to web searches.Below are the few more cases where IR is used in one form or the other:Lets take a simple example of an online library.We have more than 10,000 books from which we need to search for a book as per thequery entered by customer. In addition, we need to create an information retrieval system which can call out all the books which resembles the customer query. Here are a few names of books :The query entered by customer is: Book for Analytics newbie.Lets solve this case.Imagine if you were a librarian of 70s and a customer comes to you to borrow a book. Given that you have been handling such queries for a long time, you can match the context of the query to the books in the rack. Now imagine, how would this process be, if done by an algorithm.Obviously machines can handle much bigger data with higher accuracy. Lets look at few techniques which will make the work of the machine easier:This is the most obvious technique to find out the relevance of a word in a document. The more frequent a word is, the more relevance the word holds in the context. Here is a frequency count of a set of words in the 5 books :One way to check Term Frequency (TF) is to just count the number of occurrence. But it has been observed that if a word X occurs in document A 1 time and in B 10 times, its generally not true that the word X is 10 times more relevant in B than in A. The difference is generally lesser as compared to the actual ratio. Hence it is good to apply following transformation on TF :Lets do the same calculation here :Now to find the relevance of document in the query, you just need to sum up the values of words in the query.Document 1 : 1.7 + 3.1 + 2.8 + 1 = 8.6Document 2 :2.3 + 3.0+ 0+ 2 =7.3Document 3 : 2.5+ 3.0+ 0+ 2=7.5Document 4 : 2.6+ 3.0 + 0+ 2.3=7.9Document 5 : 2.3+ 3.0 + 0 + 2.5=7.8Result shows,Document 1 will be more relevant to display for the query, but we still make a concrete conclusion. Since, document 4 and 5 are not far away from Document 1. They might turn out to be relevant too. This is because of the stopwords which elevates all the scores with similar magnitude.IDF is another parameter which helps us find out the relevance of words. It is based on the principle that less frequent words are generally more informative.where N represents thenumber of documents and DF representsthe number of documents in which we see the occurrence of this word.We now can clearly see that the words like The for etc. are not really relevant as they occur in almost all the document. Whereas, words like honest, Analytics Big-Data are really niche words which should be kept in the analysis.As we now know the relevance of words (IDF) and the occurrence of words in the documents (TF), we now can multiply the two. Then, find the subject of the document and thereafter the similarity of query with the document.Now it clearly comes out that document 1 is most relevant to the query Book for Analytics newbie.This article is an over simplified version of what really happens in an information retrieval system. In actual, we represent each document as a vector on an n-dimension plane, where n is the count of words in a dictionary built by relevant words in all target documents. Then the query is plotted on the same plane.The document which makes the least angle with the query is given out as the most relevant document. We will cover this rule (cosine rule) and a simple solved example using Python in the next article.Thinkpot: Can you think ofmore strategies to find the relevance of a query in a document? Share with us useful links of related video or article todo information retrieval.Did you find the article useful? Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/
Internet of Things (IoT) and its impact on data science!,Learn everything about Analytics,"What is Internet of Things (IoT)?|What can these devices do?|Why is it gaining significance?|Hype cycle of Internet of Things:|What does it mean for data science professionals?|What are the challenges in realizing Internet of things?|End Note:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Information Retrieval System explained in simple terms!|Comprehensive guide for Data Exploration in SAS (using Data step and Proc SQL)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Examples of consumer applications:|Industrial applications:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Eric Schmidt & Jared Cohen, intheir book The New Digital Age describe typical future morning for a professional like this:There will be no alarm in your wake-up routine  at least, not in the traditional sense. Instead, youll be roused by the aroma of freshly brewed coffee, by light entering your room as curtains open automatically, and by a gentle massage administered by your high-tech bed. Youre most likely to awake refreshed, because inside your mattress theres a special sensor that monitors your sleeping rhythms, determining precisely when to wake so as not to interrupt a REM cycle.A little later:Theres a bit of time left before you need to leave for work  which youll get to by driverless car, of course. Your car knows what time you need to be in the office each moring based on your calendar and, after factoring in traffic data, it communicates with your wristwatch to give you a sixty-minute countdown to when you need to leave the house.How does this world look? Futuristic? Or do you think we are pretty close to this future? I think we are some where is the middle  our world should look like this in 5  8 years from now! All thanks to Internet of things or Web of things, whatever you call it.Internet of things refers to network of objects, each of which has an unique IP address & can connect to internet. These objects can be people, animal and day to day devices like your refrigerator and your coffee machine. These objects canconnect to internet (and to each other) and communicate with each other through this net, in ways which have not been thought before.Imagine a world when every small thing in your home is connected to internet and is speaking to each other  the coffee maker, refrigerator, doors, heating units, your watering system, your weighing scale, your car, mobile phone, watch, TV, your wardrobe, your house cleaning machines, everything on a single network and all interacting and communicating with each other.If this sounds like a scene from the movie Transformers  its not! It is much closer to reality than you would think.There is no rule which defines what these devices can or can not do. So, it is pretty much open to the imagination of the designers and manufacturers. However, here are a few obvious things, which come to my mind:Here is how Google search for Internet of things trends in comparison to Big Data. It already has about one-thirds of searches in comparison to Big Data.So, why is this being searched? I think the best way to answer this is to illustrate a few scenarios and possibilities enabled by this internet of things.I am sure, you get the idea. Basically, Internet of Things (IoT) is set to change how we work and interact with the world in every possible manner.Hype cycle is a typical representation on how technologies evolve over time. They typically go through Innovation trigger followed by inflation of expectations, trough of disillusionment, enlightenment and finally hitting plateau of productivity. This is the time, when a technology has lived its life cycle!Here is the hype cycle of various technologies in July 2014, as per Gartner:As you can see, there are various manifestations of Internet of things, which are 5  10 years from the eventual plateau. For example  Connected Home, Wearable user interfaces, machine to machine communication services.If you are not scratching your head already, have a look at this graph. This is how the data generated through internet of things is expected to explode (at whopping 66% CAGR):So, Cisco expects the data emitted by all mobile devicesto grow 4 times in 3 years! Now, imagine the opportunities it opens up for data science professionals. Here are a few of them, which come to my mind:In simple words, with this explosion in data, we will need more people who can handle this data and make more sense of it.While the picture I painted is definitely rosy, there are a few significant challenges to overcome before we reach there.In this article, I have described the basics of one of the hottest field right now  the Internet of Things (IoT). The space is evolving every week with advancements changing the landscape quickly. It is also a convergence of various hardware and software  something which we have been thinking for long time.I have also touched upon the implications of this for data science professionals and a few challenges in the domain. These are by no means exhaustive, but just the tip of the iceberg. Hopefully, I have told enough to get you excited about this evolving field.",https://www.analyticsvidhya.com/blog/2015/04/internet-of-things-impact-data-science/
Comprehensive guide for Data Exploration in SAS (using Data step and Proc SQL),Learn everything about Analytics,"Introduction|Contents:|Part 1: How to load data file(s) into SAS Data set?|Part 2: How to convert a variableto different data type?|Part 3: How to transposeaData set?|Part 4: How to sort Data set in SAS?|Part 5: How to create plots (Histogram, Scatter, Box Plot) in SAS?|Part 6: How to generatefrequency tables in SAS?|Part 7: How to do sampling of Data set in SAS?|Part 8: How to remove duplicate values of a variable?|Part 9: How to group variables in SAS to calculatecount, average, sum?|Part 10: How to recognizemissing values and outliers?|Part 11: How to imputemissing values and outliers?|Part 12: How to drop and renamevariables in a data set?|Part 13: How to merge / join data set effectively?|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Internet of Things (IoT) and its impact on data science!|Text Mining hack: Subject Extraction made easy using Google API|
Sunil Ray
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","How to load data file(s) into SAS Data set?|How to convert a variableto different data type?|How to transposeaData set?|How to sort Data set in SAS?|How to create plots (Histogram, Scatter, Box Plot) in SAS?|How to generatefrequency tables in SAS?|How to do sampling of Data set in SAS?|How to remove duplicate values of a variable?|How to group variables in SAS to calculatecount, average, sum?|How to recognizemissing values and outliers?||How to imputemissing values and outliers?|How to drop and renamevariables in a data set?|How to merge / join data set effectively?||Importing XLS/ CSV file using PROC Import:||Importing raw text file (Txt) using PROC Import:|Importing using Data step:|Convert numeric variables to Character variables and vice versa|   2. Convert character date toDate|Histogram:|Scatter plot:|Box-plot:|Method 1. Using First. or Last.|Method 2. NODUPKEY with Proc SORT|Method 3. Binning Numerical Variable",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I would like to extend my sincere gratitude to our readers for their overwhelming response on my previous articles on data exploration. These articles featured:variable identification, Univariate and Bivariate analysis, Missing and Outlier identification and treatment and feature engineering.In this guide, I will take a step ahead and show all these steps toexplore data sets practically in SAS. I will also perform some exercises that will help you understand the concept better.You can look at this article as practical implementation of my previous articles (in SAS).I am hoping that this guide can act as a ready reference for our followers trying to navigate SAS on their own.Lets get down to work!Since this is an exhaustive guide, it is a good idea to list down all the things Ill cover:The sources of input data sets can be in variousformats (.XLS, .TXT, .CSV) and sources like databases. In SAS, we can use multiple methods to load data from these sources. Lets look at the commands to load data fromeach dataset type mentioned above:Notes:We can also create a libraryfrom excel files using Libname statement (Eachworksheet in the Excel workbook is treated as a SAS data set. Worksheet name appears with a dollar sign at the end of the name).If SAS has a libref assigned to an Excel workbook, the workbook cannot be opened in Excel. To disassociate a libref, use a LIBNAME statement and specify the libref and the CLEAR option.If your data file is a simple text file, you can use following commands:
It is assumed that the first rowof the data set contains column names. If first rowis not the column name, then we would change getnames=yes to getnames=no. Afterthat,names of the columns would get stored as VAR1 to VARn.You can also make use ofData Step to import data from csv or text file.Syntax: Example: Import data from a csv file using data step, assuming values are separated by comma(,).Above, we looked at multiple methods to load data set in SAS. To load data set from databases like ORACLE, SQL SERVER and others, we would require authorization from both SAS Admin or Database admin.To explore this in detail, you can refer tolinks below:We can convert character to numeric and numeric to character and also change the format of variable like number to date, date to number, number to currency format etc. Lets look at some of the commands to perform these conversions:To perform this, we will use INPUT function. It takes two arguments: the name of a character variable and a SAS informat or user-defined informat to read the data.Syntax: In snapshot below, you can see that variable Avg is in character format. Now to convert it into number, well use Input function.See below codes:Similarly, if we want to convert a numeric variable to character, itcan be done usingPUT function.Syntax: For more details on Input and Put function, you can refer below links:
Let us say, we want to transpose TableA into Table B on variable Product. This task can be accomplishedin SAS using PROC Transpose:For more detail on PROC Transpose, refer below link:
Sorting of data can be done using procedure PROC SORT. It can be based on multiple variables and ascending or descending both order.Syntax: Above, we have a table with variables ID, Product and Sales. Now, we want to sort it by Product and Sales (in descending order) as shown in table 2. This can be done using Proc Sort as shown below.
Lets understand plots using the example shown above. We have employee details with their EmpID, Gender, Age and Sales Detail. Wewant to understand:These tasks can be accomplishedby usingScatter, Box and Histogram representation.Now to understand thedistribution and check whether the data is distributed normally or not, we will plot a Histogram.In SAS, histograms can be produced using PROC UNIVARIATE, PROC CHART, or PROC GCHART. Here we willuse PROC UNIVARIATE with the HISTOGRAM statement.It is used to find the relation b/w two continuous variables. Here we will use PROC SGPLOT to plot scatter graph.Box-Plot is used to understand the distribution of continuous variables. This is also known as five number summary plot of Min, First Quartile, Median, Third Quartile and Max. We will again use PROC SGPLOT to display the Box-plot.For more details on PROC Univariate and PROC SGPLOT, you can refer below links:
Frequency Tablescan be used to understand the distribution of a categorical variable or n categorical variables usingfrequency tables. We will use PROC FREQ procedure to perform this.PROC FREQ is capable of producing statistical test and other statistical measures in order to analyze categorical data based on the cell frequencies in 2-way or higher tables.I have added another variable BMI to above mentioned employee table. Now, to understand the distribution between GENDER and BMI, I will use PROC FREQ procedure with CHISQ statistical test.

For more detail on PROC FREQ, you can refer below link:To select an unbiased sample from a larger data set in SAS,weuse procedure PROC SURVEYSELECT. Here we will go with PROC SURVEYSELCT.Lets say, from EMP table, I want to select random sample of 3 employee.

Often, we encounter duplicate observations. To tacklethis, SAS has multiple options like FIRST., LAST., NODUPKEY with PROC SORT ,PROC SQL and others. Lets understand these options one by one:To use First. or Last. option, data set must be sorted by variable(s) on which we want to identify the unique records. First.and Last. automatic variables created by SAS when using by-groupprocessing. It has value of 0 and 1.Above, you can see that how value of First. and last. is populated.Now, lets see how can we use these two values to identify unique records.Above, we have used first. to filter first observation and to filter last observation, we can use Last.We can use NODUPKEY option with Proc Sort to remove duplicate values.We can use conditional statements and logical operators to bin numerical variables. In Emp data set, we have variable Age. Here we will bin variable Age as <25, >=25 and <35, >=35.
To understand the count, average and sum of variable, I would suggest you to use PROC SQL with group by. There are other methods also like Proc FREQ and PROC Means to perform.Lets look at the syntax of these Procedures:PROC SQL:
PROC Means:To identify outliers in a variable, we can go with Proc Univariate procedure and use PROC FREQ to identify missing values. Lets look at the output below to understandthese two procedures:
Above, you can see that PROC Univariate as shown top andbottom 5 values whereas PROC FREQ shows the distribution of unique values of variable.There are various imputation methods available for missing and outlier imputation. You can refer these articles for methods to detectOutlier and Missing values. Imputation methods for both missing and outlier values are almost similar. Here we will discuss general case imputation methods to replace missing values. Lets do it using an example:Lets say we have an employee data set comprising ofmultiple variables like Empid, Name, Gender, Sales, Age, Region, Product and other. Here, we want to predict the sales of employee. But,one of the concern is variable Age has missing values and variable Age appeared as significant variable.Now to deal with this missing values, I have written below SAS statements:Identify Values to Impute Using General Case Method (Average of Age):Imputation Using Data Step

Above, you have seen one of the methods to deal with it. You can also use multiple methods using SAS statements. I would suggest you to practice all the discussed method in my previous post on missing values and outliers.Lets say, during data exploration stage, we want to exclude variables those are not required in the data modelling exercise or want to rename few variables also. These two operations can be performed using DROP and RENAME options using DATA STEP.Lets say, we want to drop variable AGE and rename variable Gender as Sex. This can be performed using below statement.
Merging / Joining can be of various types. It depends on the business requirement and relationship between data sets.In SAS, we can perform this in various ways using DATA STEP, PROC SQL and PROC Format. Now, question is, which is the most appropriate methodto perform merging and joining?You can refer on of my post on this topic for detailedinfo.here:Introduction to Merging.In this guide, we looked at the SAS statements for various steps in data exploration and munging like loading of data, converting data type, transposing tables, sorting, plotting, removing duplicate values, binning, grouping, identifying missing & outlier values, dropping & renaming variables, merging & joining tables and imputing values for missing and outlier values. We alsolooked at the basic SAS statement to perform this and havegiven links to look at more advance methods.In one of the next article, I will revealthe codes to perform these steps in Python. Stay Tuned!Did you find the article useful? Do let us know your thoughts about this guidethrough comments below.",https://www.analyticsvidhya.com/blog/2015/04/data-exploration-sas-data-step-proc-sql/
Text Mining hack: Subject Extraction made easy using Google API,Learn everything about Analytics,"Why is subject extractionnot a common analysis?|Why do we even need subject extraction?|What are the challenges in subjectextraction?|Possible Framework to build a Subject ExtractionDictionary|How to automate the process of Subject Extraction Dictionary Creation?|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Comprehensive guide for Data Exploration in SAS (using Data step and Proc SQL)|Basics of SQL and RDBMS  must have skills for data science professionals|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lets doa simple exercise. You need to identify the subject and the sentiment in following sentences:Was this exercise simple? Even if this looks likea simple exercise, now imagine creating an algorithm to do this? Howdoes that sound?The first example is probably the easiest, where you know Google is the subject. Alsowe see a positive sentiment about the subject. Automating the two components namely subject mining and sentiment mining are both difficult tasks, given the complex structure of English language.Basic sentiment analysis is easy to implement because positive / negative word dictionary is abundantly available on internet. However, subject mining dictionaries are very niche and hence user needs to create his own dictionary to find the subject. In this article, we will talk about subject extraction and ways to automate it using Google API.Also See: Basics of creating a niche dictionaryThe most common projects using text mining are those with sentiment analysis. We rarely hear about subject mining analysis. Why is it so?The answer is simple. Social media, the major source of unstructured data, does a good job with subject mapping. Users themselves make the subject of most comment very obvious by hash tagging. These hash tags help us search for a subject related comments on social media. And most of our analysis is based on a single subject which makes subject mining redundant. It would be awesome if we hash tag all our statements . In that case text mining would have become a cake walk because indirectly I am creating a structured text on social media.Only, if the world was ideal!Hash tags could also help to find sarcasm to some extent. Consider the following tweet :Indian batting line was just fine without Sachin. #Sarcasm #CWC2015 #Sachin #Indian-Cricket-TeamThink of this sentence without hash-tags. It would be incomplete and would give a different meaning. Mining for hash tag(#sarcasm) will indicatethat the sentence is most probably a negative sentence. Also, multiple subjects can be extracted from the hash tags and added to this sentence.Hopefully, you can now realize the importance of these hash tags in data management and data mining of social networks. They enable the social media companies to understand our emotions, preferences, behavioretc.However, social media do a good job with subject tagging, we still have a number of other sources of unstructured informations. For instance, consider the following example :You run a grocery stores chain. Recently you have launched a co-branded card which can help you understand the buying patterns of your customers. Additionally this card can be used at other retail chains. Given, that you now will have transaction information of your customers at other retail chains, you will be in a better position to increase the wallet share of the customer at your store. For instance, if a customer buys all vegetables at your store but fruits at other, you might consider giving the customer a combo of fruits & vegetables.In this scenario, you need to mine the name of retail store and some description around the type of purchase from the transaction description. No hash-tags or other clues, hence you need to do the hard work!There are multiple other scenarios where you will need to do subject mining. Why do we call it a hard work? Here are the major challenges you might face whilesubject mining :Now even if we build a system which can populate the first/first 2 words as the subject, we cant find a common subject for the above two. Hence, we need to build a dictionary which can identify a common subject i.e. Pizza Hut for both these sentences.There are two critical steps in building a subject mining dictionary:For second part, here arethe sub-steps you need to follow :Now all we need to do is to match subjects for each of these pairs. We search pairs and not single words because we need enough context to search for the phrase. For example Express might mean American Express or Coffee Express, two words can give enough context whereasmore than two words will make the dictionary too big.Here are some examples of this process :Wall Mart has the best offersTesco stores arenot good with discountsNew Wall Mart stores are supposed to open this yearTesco Stores have the coolest loyalty programs and discountsMost Frequent words: After removing stop-words : 1. Wall 2. Mart 3.Tesco 4. StoresMost Associated words: 1. Wall & Mart , 2. Mart & Wall , 3. Tesco & Stores , 4. Stores & TescoNow well use these words to search for the right subject.Second step of subject mining is creating keyword to subject pairs. This step is generally done manually, but lets take a shot at automating this process. Here is what we intendtodo :Lets first create a function which can retrieve the first four links from Google on a search and then find if we have a common link. Here is code to do the same :Now, lets create a list of keywords which our code can search. (Notice that each of these keywords are quite different but Google will help us standardize them)Its now time to test our function :And Bingo! You see that our code was given different inputs but our code has done fairly well to spot the right set of subjects. Also notice that this dictionary is not limited by any scope of the subject. Two of its searches are Fast Food chains. Third one is an analytics website. Hence, we are creating a more generalized dictionary in this case. Now all we need to do is build rules using these keywords and map them to the matched links.Here is the entire code :The approach mentioned in this article can be used to create a generalized dictionary which is not restricted to any subject. Frequently, weuse the super powers of Google to auto correct the input keywords to getthe most appropriate results. If this result is unanimous, it tells us Google has found a decent match fromthe entire web world. This approach minimizes the human effort of creating such tedious subject extraction dictionaries.Thinkpot: Can you think ofmore cases whereGoogle APIs are used? Share with us useful links of related video or article to leverage Google APIDid you find the article useful? Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/03/text-mining-subject-extraction-google-api/
Basics of SQL and RDBMS  must have skills for data science professionals,Learn everything about Analytics,"What is SQL?|What is CAP Theorem?|Properties of Databases:|Set of Commands in SQL|Where do weuse SQL? |Does SQL influences other languages as well?|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Text Mining hack: Subject Extraction made easy using Google API|Big Data / Analytics based startups at Y Combinator, Winter 2015 batch|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you meet 10 people who have been in data science for more than 5 years, chances are that all of them would know of or would have used SQL at some time in some form! Such is the extent of influence SQL has had on any thing to do with structured data.In this article,we will learn basics of SQL and focus on SQL for RDBMS. As you will see, SQL is quite easy to learn and understand.SQL stands for Structured Query Language. It is a standard programming language for accessing a relational database. It has been designed for managing data in Relational Database Management Systems (RDBMS) like Oracle, MySQL, MS SQL Server, IBM DB2.SQL is one of the first commercial languages used for Edgar F. Codds relational model, alsodescribed in his influential 1970 paper, A Relational Model of Data for Large Shared Data Banks.Earlier, SQL was a de facto language for the generation of information technology professionals. This was due to the fact that data warehouses consisted of one or the other RDBMS. The simplicity and beauty of the language enabled data warehousing professionals to query data and provide it to business analysts.However, the trouble with RDBMS is that they are often suitable only for structured information. For unstructured information, newer databases like MongoDB and HBase (from Hadoop) prove to bea better fit.Part of this is a trade-off in databases, which is due to the CAP theorem.CAP theorem states that at best we can aim for two of three following properties. CAP stands for:Consistency  This means that data in the database remains consistent after the execution of an operation.Availability  This means that the database system is always on to ensure availability.Partition Tolerance  This means that the system continues to function even if the transfer of information amongst the servers is unreliable.The various databases and their relations withCAP theorem isshown below:A database transaction, however, must be ACID compliant. ACID stands forAtomic, Consistent, Isolated and Durable as explained below:Atomic : A transaction must be either completed with all of its data modifications, or may not.Consistent : At the end of the transaction, all data must be left consistent.Isolated : Data modifications performed by a transaction must be independent of other transactions.Durable : At the end of transaction, effects of modifications performed by the transaction must be permanent in system.To counter ACID, the consistent services provide BASE (Basically Available, Soft state, Eventual consistency) features .SELECT- The following is an example of a SELECT query that returns a list of inexpensive books. The query retrieves all rows from the Library table in which the price column contains a value lesser than 10.00. The result is sorted in ascending order by price. The asterisk (*) in the select list indicates that all columns of the Book table should be included in the result set.UPDATE  This query helpsin updating tables in a database. One can also combine SELECT query with the GROUP BY operator for aggregating statistics for a numeric variable by a categoric variable.JOINS- SQL is thus heavily used not only for querying data but also for joining the data returned by such queries or tables. Merging data in SQL is done using joins. The following infographic often used for explaining SQL Joins:CASE-We have case/when/then/else/end operator in SQL. It works like else if in other programming languages:
Nested Sub Queries Queries can be nested suchthat the results of one query can be used in another query via a relational operator or aggregation function. A nested query is also known as a subquery.
SQL has been widely used to retrieve data, merge data, perform group and nested case queries for decades. Even for data science, SQL has been widely adopted. Following are some examples of analytics specific use of SQL:The drawback with relational databases is that they cannot handle unstructured data. To deal with the emergence, new databases have come up and they aregiven NoSQL as an alternative name to DBMS. But SQL is not dead yet.Also See: A mapping of SQL to MongoDBBelow are some languages where SQL is found to have significant influence:Hive  Apache Hive provides a mechanism to project structure onto the data in Hadoop and to query that data using a SQL-like language called HiveQL (HQL). It is a data warehouse infrastructure built on top of Apache Hadoop for providing data summarization, ad hoc query, and analysis of large datasets. Even HQL, a language for querying used in Hadoop heavily uses influences of SQL. You can find out more here.SQL-Mapreduce Teradata uses Aster database that uses SQL with MapReduce for huge datasets in the Big Data era. SQL-MapReduce is a framework created by Teradata Aster to allow developers to write powerful and highly expressive SQL-MapReduce functions in languages such as Java, C#, Python, C++, and R and push them into the discovery platform for high performance analytics. Analysts can then invoke SQL-MapReduce functions using standard SQL or R through Aster Database .Spark SQL  Apaches Spark project is for real-time, in-memory, parallelized processing of Hadoop data. Spark SQL builds on top of it to allow SQL queries to be written against data. In Clouderas Impala- Data stored in either HDFS or HBase can be queried, and the SQL syntax is the same as Apache Hive.Also See:Find outmore on ways to Query Hadoop using SQL here.In this article we discussed about SQL, its uses, CAP Theorem and influence of SQL on other languages.A basic knowledge of SQL is very relevant into todays world where Python, R, SAS are dominant languages in data science. SQL remains relevant in the BIG DATA era. The beauty of the language remains its simplicty and elegant structure.Thinkpot :Do you think SQL has become an inevitable weapon for data management? Would you recommend any other database language?Share you views/opinion/comments with us in the comments section below. We would love to hear it from you!",https://www.analyticsvidhya.com/blog/2015/03/basics-sql-rdbms/
"Big Data / Analytics based startups at Y Combinator, Winter 2015 batch",Learn everything about Analytics,"|What is Y Combinator?|Analytics and Big Data based startups in Y Combinator Winter 2015 batch:|End Note:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Basics of SQL and RDBMS  must have skills for data science professionals|Consultant  Equifax  Mumbai (1.5 to 4 years of experience)|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Pachyderm|Yhat|SmartSpot|MashGin:|Atomwise:|Kuhcoon|SlideMail|Pomello|AnalyticsMD|Rescue Forensics|HigherMe,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have even a mini / micro / nano doubt about how analytics and Big Data are re-shaping this world, you should look at the list of start-ups in Y Combinator Winter 2015 batch.Image source: Y CombinatorFor people who dont know what Y Combinator is, Y Combinator is probably Americas most looked up to seed stage accelerator for tech startups. Some of the companies incubated at Y Combinator include the likes of Reddit, Dropbox and Airbnb.The startups move to Silicon Valley for 3 months and work with Y Combinator partners to get the company into the best possible shape. At the end of these 3 months, startups present their companies to a select audience on a Demo Day. Recently, the Winter 2015 batch completed their demo day  and here is a sneak peak into how analytics is changing the world!The intricacies of big data analysis compelled the team of Pachyderm to create a product that can make big data analysis simpler and doable. Their aim was to enable people who dont know Java or Mapreduce can easily run analysis on large amounts of data. Pachyderm is an open source MapReduce engine that uses docker containers for distributed computations. It claims to have the power of MapReduce without the complexity of Hadoop.This startup leverages data science teams to quickly transform new ideas into data driven products. They aim to create a support structure that can assist data science professionals to convert their brainstorming discussions into actionable insights. They also have a data science operation system, ScienceOps, used for managing predictive and advances decision making APIs and workflows.This startup makes use of embedded data analytics techniques that allows a person to meet his new workout partner, a mirror. This mirror is a multi functional device. It counts the reps and sets, motivates the person to maximize reps, keeps a track of time-outs, tracks the workout progress, guides the person how to do every exercise in the best possible manner and many more functions to define.Mashgin is automating retail checkout using computer vision. They have developed a high-precision object recognition algorithm to identify multiple items simultaneously and in any orientation. Using this technology, they have built a kiosk that aims to reduce the queues in various cafetaria.How is MashGin solving this problem?Instead of applying neural networks and deep learning on food objects directly (which results in low precision and low recall), they first re-construct the objects in 3D and then apply their machine learning algorithms to identify the objects.Atomwise predicts potential drug cures with the use of supercomputers, artificial intelligence and a specialized algorithm that runs through millions of molecular structures, potentially reducing the cost and time involved in making new drug discoveries. Most drug research takes months or years and millions of dollars. Atomwise utilizes AI and machine learning to enable research that costs thousands of dollars and takes just days.They are working with companies like Merck, Notable Labs (another YC backed firm) for cancer cure, with IBM to find a cure to Ebola and with Dalhousie University in Canada to search for a measles treatment.Kuhcoon steers fully automated, performance driven social ad-campaigns. This startup aims to inspire people to make use of their precious time into more productive tasks than wasting time on managing ad campaigns and striving for higher ROIs. Their runaway success is largely because of bringing data analytics, machine learning to use, which gave birth to data driven campaigns, programmatic optimization, automated a/b testing, live dashboard reporting, automated ad rotation . Moreover, they make use of bigdata to optimize the ad spend.This startup says What if your email app could think? Needless to say, by use of machine learning, slidemail automatically converts emails to calendar events. It adapts to how you use your email. It helps to sort and categorize the emails automatically. Above all, probably the best thing is, delete the app and all your data is gone with it. They dont store any user data. As of now, it is available in App Store.Pomello is also one of the evolving startup which makes use of a data driven approach to help companies find the right talent for their unique company culture.They undertake a survey based method to collect the data and map the team by team organizational culture and integrate it with the recruitment process.They claim to improve employee performance, lower attrition and reduce the time to hire candidates.Streamlines hospital operations using real time optimization. analyticsMD applies machine-learning based forecasting and Industrial Engineering algorithms to anticipate demand and recommend specific actions. These actions can range from in-the-moment decisions (Call in an additional staff member now) to decisions that look several weeks ahead (Adjust an appointment length). Simply put, AnalyticsMD uses machine learning to monitor, predict, and optimize how hospitals work.Rescue Forensics provides actionable human trafficking Intelligence. Theybelieve that data is the future of human trafficking investigationsand theyuse data as a tool to identify and dismantle human trafficking networks operating on the internet.their client list already includes the likes of FBI and several Police departments.Simply by making use of data and videos, HigherMe helps various retail and service employers to find the right candidate for the job. The team of HigherMe, which involves coding wizards, employee detectives, product creators, data drivers makes this recruitment process much faster by bringing into use the latest technical expertise.So, there you have it! I was all pumped to see so many analytics and big data based startups coming out of Y Combinator. All of these ideas and startups are exciting and on their track to change traditional industries fundamentally or create new ones and all of this is based on analytics and big data!What do you think about these startups? Do you feel as pumped up as I did when I read about them first? Do share your thoughts through comments below.",https://www.analyticsvidhya.com/blog/2015/03/big-data-analytics-based-start-ups-combinator-winter-2015-batch/
"Consultant  Equifax  Mumbai (1.5 to 4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data / Analytics based startups at Y Combinator, Winter 2015 batch|Importance of actionable insights in analytics (with case from ICC Cricket World Cup)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  ConsultantLocation  MumbaiAbout employer EquifaxJob description: ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant  Equifax  Mumbai,https://www.analyticsvidhya.com/blog/2015/03/consultant-equifax-mumbai-1-5-4-years-experience/
Importance of actionable insights in analytics (with case from ICC Cricket World Cup),Learn everything about Analytics,"Example of insights shown during a cricket match:|The problem with current insights:|Ways to improve insights (in Cricket matches):|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Consultant  Equifax  Mumbai (1.5 to 4 years of experience)|Data Scientist  Vserv  Mumbai  (2-4 years of experience)|
Kunal Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","It is an insight, but not an actionable insight!|The current insights provide no help tothe team management to utilize this information.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever been to a meeting, where everyone in the room has good stats to share, but no one knows how to use them? I am sure, you have! Dont worry, you are not alone and by end of this article, you will have some tips to make your analysis better.Inthis article, I will challenge how analytics is being showcasedduring the cricket matches. While the display is a good way to evangelizeuse ofanalytics on grand scale, itis missing the very point it should be proving!The message I want to leave through this article is far broader than just application to cricket. Example of cricket is being used only as a case study.P.S. If you dont follow cricket, you can still follow the article for the points I am trying to tell. Here is a beginners guide to Cricket.Here is anexample of what spectators glued to the screen would typically see at the start of the match:There will be similar set of so called insights for the second team as well. The analysts performing this analysis have also attached probabilities with each event!Here is a live example, which was shown in a recent match (South Africa vs. New Zealand):These look good and exciting! What is the problem with this? Can you spot problems with these so called keys to success?While these insights are good to see, they do not helpthe teams play better. Let me explain  an insight saying that team A wins 85% of matches when Player X scores more than 80 runs is useless to the team coach and the captain.The team management would want analytics to do much more than just pulling out this insight and then praying that Player X has a good outing in every match!If I was the analyst running this model  I would go further and say what is the best strategy to make sure Player X goes on to make a big score  Which position should he batat? What kind of areas and which bowlers should he target? Which bowlers should he negotiate?Lets take another example  analysts have mapped out the strong zone and the weak zone for each batsman. You would think this is clearly actionable. The bowler just needs to ball in the right areas.But it isnt! Why? Strong zone and weak zone for a batsman would change from bowler to bowler, with differentfield settings and differentwhether conditions. It would also depend on the current form of both  the bowler and the batsman.Other way to look at these insights is this they have a lot of numbers and stats, but dontreally tell what they mean.As shown already, the current level of analysis shown during cricket matches is rudimentary at best! There are tons of ways to improve this analysis. Ill share a few high level thoughts, which, when implemented would surely provide better use of analytics:There are many more ways to improve, but this should hopefully help you to understand what I mean by actionable insights.The idea behind this article was to bring out some ways in which you can improve your analysis and to show case them through a real life case study. I have learnt some of these practices the hard way over time, but you dont need to do that! A single minded focus on yielding actionable insights for your users can completely change the way analytics can add value to them. On the other hand, a sketchy job can lead to wrong outcomes and mis-guided views. All the best for the next piece of analysis you do.Disclaimer:I have shared some of the gaps on the analysis showcased to the audience during recent cricket matches. I am sure individual teams in the tournament would be taking help of analysts for more sophisticated analysis. I do not have access to that analysis and hence do not know, how many of the shortcomings mentioned here get covered through those pieces of analytics.",https://www.analyticsvidhya.com/blog/2015/03/actionable-insights-analytics-cricket-world-cup/
"Data Scientist  Vserv  Mumbai  (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Importance of actionable insights in analytics (with case from ICC Cricket World Cup)|Hacking Google Maps to create distance features in your model / applications|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  MumbaiAbout employer VservJob description: Data scientist will be responsible to design and impalement efficient, adaptable, & reusable code and algorithms. The role requires tuning components for high performance & scalability using big data technologies like Pig, Hive, HBase, Vertica, Redisetc and scripting languages Python, Java, scala, R etc. This will involve working with both product and tech team to help achieve a state-of-the-art environment that meets current and future business objectives.ResponsibilitiesQualification and Skills RequiredDNA  Essential SkillsCompetencies RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  Vserv  Mumbai",https://www.analyticsvidhya.com/blog/2015/03/data-scientist-vserv-2-4-years-experience/
Hacking Google Maps to create distance features in your model / applications,Learn everything about Analytics,"A commonapproach|A manual Approach|How toautomate this approach?||End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Vserv  Mumbai  (2-4 years of experience)|How to re-use data models in Qlikview using Binary Load?|
Tavish Srivastava
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Thisarticle is going to be different from the rest of my articles published on Analytics Vidhya  both in terms of content and format. I usually layout my article such that after a read, the reader is left to think about how this article can be implemented on grounds.In this article, I will start with a round of brainstorming around a particular type of business problem and then talk about a sample analytics based solution to these problems. To make use of this article make sure that you follow my instructionscarefully.Lets start with a few business cases:What is so common in all the problems mentioned above? Each of these problems deal with getting the distance between multiple combination of source and target destinations.Exercise : Think about at-least 2 such cases in your current industry and then at least 2 cases outside your current industry and write them in the comment section below.I have worked in multiple domains and saw this problem being solved in similar fashion which gives approximate but quick results.Exercise: Can you think of a method to do the same using your currently available data and resources?Here is the approach :You generally have a PIN CODE for both source and destination. Using these PIN CODES, we find the centroid of these regions. Once you have both the centroids, you check their latitude and longitude. You finally calculate the eucledian distance between these two points. We approximate our required distance with this number. Following figure will explain the process better :The two marked areas refersto different PIN CODES and the distance 10 kms is used as an approximate distance between the two points.Exercise: Can you think of challenges with this approach ?Here are a few I can think of :Say you have two branches and a single customer, how will you make a call between the two branches (which one is closer)? Here is a step by step approach :Obviously, this process cannot be done manually for millions of customers and thousands of branches. But this process can be well automated (however, Google API have a few caps on the total number of searches). Here is a simple Python code which can be used to create functions to calculate the distance between two points on Google Map.Exercise:Create a table with a few sources and destinations. Use these functions to find distance and time between those points. Reply Done without support if you are able to implement the code without looking at the rest of the solution.Here is how we can read in a table of different source-destination combinations :Notice that we have all types of combinations here. Combination 1 is a combo of two cities. Combo 4 is a combination of two detailed address. Combo 6 is a combination of a city and a monument. Lets now try to get the distances and time & check if they make sense.All the distance and time calculations in this table look accurate.Exercise: What are the benefits of using this approach over the PIN CODE approach mentioned above? Can you think of a better way to do this task?Here is the complete Code :GoogleMaps API come with a few limitations on the total number of searches. You can have look at the documentation, if you see a use case of this algorithm.Did you find the article useful? Share with usfind more use cases of GoogleMaps API usage apart from the one mentioned in this article?Also share with us any links of related video or article to leverage GoogleMaps API.Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/03/hacking-google-maps-create-distance-features-model-applications/
How to re-use data models in Qlikview using Binary Load?,Learn everything about Analytics,"What will you learn?|What is Binary Load?|Problem Case:|||Implementation of Binary Load:|Benefits of Binary Load|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Hacking Google Maps to create distance features in your model / applications|Why most data science trainings fail to deliver? How to overcome these failures?|
Sunil Ray
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"After receiving a lot of queries on use optimization techniques in QlikView, I am compelled to write this article. The beauty of optimization is that it can be applied at any stage in QlikView application development cycle.There are multiple methods to optimize loading data in QlikView. In this article, Ill discuss one of theoptimization methods of loading data loading data from another QlikView document. Before proceeding further, I would suggest you to go throughIncremental data load in QlikView and Loading data from QVDfilesto get familiar with common loading data techniques.Now that we are familiar with variousmethods of loading data from different sources in QlikView, wewill load data from a QlikView (.QVW file) itself. This method is known as Binary Load.Binary Loadreplicates the data model of an existingQlikView document into another QlikView documentwithout accessing the original data source. Once the data model getsreplicatedintoanother QlikView document, it can be manipulated further, integrated into a bigger data model (byadding more tables to it or even reducing it by removing few tables).Syntax:                     Binary <Qlikview_filename.qvw>Some key facts to remember about Binary Load:During one of my assignment with a leading insurance company, I hadcreated a sales dashboard for the sales team. After sometime, the fraud detection team asked foranother dashboard, which should featurethe sales metrics including the fraud metrics.This is an idealuse case for implementing binary load. I already had the data model in Sales dashboard. I can use this existing data model to create a new document and add more tables of fraud metrics for fraud detection.Now, lets implement this binary load to a new QlikView document. This can be done through following steps:Now, the rest of the task is of datavisualization.Lets look at somebenefits of binary load:In this article, we looked atbinary loadusing anexample of cloning data model of one QlikView document to another. We also looked at steps to implement binary load and its benefits. I recommend you to apply binary load whenever you are required use the data model of an existing QlikView document.In the next post, welllook at advanced use of Binary load with incremental data loading scenarios.photo 1 credit:Qlikviewaddict.com",https://www.analyticsvidhya.com/blog/2015/03/qlikview-binary-load/
Why most data science trainings fail to deliver? How to overcome these failures?,Learn everything about Analytics|Limitations ofthe data science trainings & how to overcome them:,"1. Limited / no attention to structured thinking:|2. Focus on tools instead of learning fundamentals of the subject:|3. Not enough emphasis on feature engineering / data cleaning:|4. Trainings dont prepare you for real life implementation problems:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How to re-use data models in Qlikview using Binary Load?|Building additional features & variables through open data sources|
Kunal Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",How to emphasize / learn structured thinking:|How to solve for this?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"At the outset, it looks like there is nodearth of data science / analytics trainings available today. Our training listing page probablyhas more than 300 trainings listed and they come in all shape and size: short term / long term, tool specific courses, self paced / instructor led trainings, Online / offline and the list goes on!But, how many of them enable you for a real life analytics career? Probably, only a handful of them! I think that there are some major flaws in the way data science trainings are being designed and delivered currently. Until we correct them, it is very difficult to create meaningful impact in careers of people undergoing these trainings.A good analogy to drive the point is to look at offerings of Apple vs. others. If you compare the products by writing down specs, you would not understand what Apple is delivering against its competitors. Similarly on the trainings comparison, whether a training is self paced vs. instructor led, delivered online or offline, which tool is it teaching are just specifications. You need to look at how they prepare you for an analytics career!This is where a lot of trainings would fall through the cracks. In this article, I will bring out some of the common short-comings of various analytics programs and leave you with thoughts on how to overcome these short-comings.P.S. My aim is not to point of the flaws in the eco-system. I too am part of it. The idea is to make sure people undergoing these trainings are making the right choices and decisions.Most of the trainings I know of, dont emphasize the need of structured thinking enough. They assume that people from various backgrounds would be able to take amorphous business problems and put a data science framework around these problems to solve them.On the other hand, most of the good analytics companies would train people on structured thinking as part of their induction.What is the reason for this gap to exist?I think part of it is down to the fact that the need and expertise of structured thinking aredifficult to quantify currently. It is something which can not becommunicated in form of certifications. But, the people undergoing these trainings would feel the heat, the minute they face an interview for data science positions.You can start by reading these articles:The best way to cover for these shortcomings is to practice structured thinking and practice it in day to day activities. The more structured your thoughts, the easier it is tosolve business problems.Stakeholders in the training ecosystem need to understand that learning data science is different from learning its tools. A certification in SAS or R does not prepare you for solving real life problems. On the other hand, if you understand the fundamentals of the subject and can put frameworks in place, you can always learn and apply the tools very easily.What do I mean by fundamentals of data science? By fundamentals, I meant that the person undergoing training understands what are we trying to do with regressions, their underlying assumptions and their shortcomings.The best way to solve for this shortcoming is to be curious. If you dont understand something, just ASK! You can ask your mentors / instructors or even use our discussion platform.Most of the training would provide you small datasets to play around and apply your programming techniques. Learning clustering? Take the standard IRIS dataset. Learning time series? Take the airline passenger data.While these toy datasets are good to get the hang of concepts, they fail to provide understanding of real life challenges. They fail to make the trainee understand the importance of hypothesis building and spending time cleaning your data.Kaggle is probably the best source to learn the importance of feature engineering. Start from the Titanic competition and move upwards on the level of complexity! Participate in as many competitions as possible (but one at a time) and see what other data scientists are doing by following the forums.How many people passing out of these trainings / courses would appreciate the difference in approach required to implement a data science solution in a manufacturing setup vs. e-Commerce setup vs. BFSI companies? Not many. While a lot of this might come from experience, including them as part of curriculum definitely makes a better data science professional.Again, the easiest solution to this is by being curious. Ask your mentors and instructors, read out case studies, network with people in industry  talk to them about their challenges and learnings. The least you should do is make the most of your instructors experience.Internships can also turn out to be a useful way to get a real hang of these problems. If you can get such an internship, nothing like it!Choosing trainings by just comparing on a few features can lead you to wrong outcomes. Hopefully, you would see what I am trying to tell. Ask these questions while selecting thetrainings. Chances are that you wont find a perfect training out there. But, you will beaware of these short-comings and can then cover up these short-comings on your own.Hope you find these questions useful. If you are facing challenges in figuring out the right data science training, feel free to reach out to us. We will love to help you in any manner we can.",https://www.analyticsvidhya.com/blog/2015/03/data-science-trainings-failures-overcome/
Building additional features & variables through open data sources,Learn everything about Analytics,"Power of Analytics|Social Media|Google API|Video Sharing Website|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Why most data science trainings fail to deliver? How to overcome these failures?|Feature Engineering: How to transform variables and create new ones?|
Tavish Srivastava
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently, while travelling, I met a few people who perceivedanalytics as a passive industry. They considered it to be a limited growth industry. On the contrary, I always wonder about the enormous source of accessible data availableat our fingertips  big thanksto the search engines! Exploitation of twitter feeds for sentimental analysis is no longer atough row to hoe.Lets understand this unseenpower of analytics by an example.Suppose, you run an international chain of retail stores, say Bresco. You run all sorts of loyalty programs to collect customer data. You have also tied up with commercial banks. As a result,you get all the necessary data about yourcustomers ranging from bank account details, card details, demographic information, food preferences etc. Now, the collected data canhelp you in creatinga virtual image of the customers. Based on the image, predicting what type of food they would have next, or their future purchases can do wonders for your store.In this article, well look atfreely available sources of information and discuss how they can be usedin context of analytics.Social Networks bring out two very critical pieces of information, we could not have known otherwise.First,the unrealizedcustomer preference. Using the behavioural information of customers social media, we can predict what customer prefers. This information can aid existing information about the customer as well. For instance, if a customer transact a lot on restaurants, we can say that the customer is a Foodie and likes to visitdifferent restaurants. But this might be just a requirement of his job and not his preference. Yet, if such an inference comes out from his social network, we can be more certain of what customer really likes and what not.Customer preference can be carved out from the customers network (if he has more people who have been referred to as Foodies, this person might be a foodie as well), the photos he has been tagged / Check ins in (if he is tagged in multiple restaurants, he might be a Foodie), his comments, hash tags etc. Social Media can bring out such information, which can help us make ourproducts more customer centric.ALSO SEE: Here is an article which can give you a kick start using Twitter Sentiment analysis.Second,the customer network information. Social media can bring out the type of people network a customer owns. Imagine, we have a social media management team who can resolve 10,000 customer complaints in a day. But, we started getting 1,00,000 complaints everyday on social media. How should we prioritize addressing these complaints? A very simple way to do this is to quantitatively assess the network strength of the customer and choose the stronger ones. For instance, complaints coming from person X will be more important than person Y, ifthe people Xinteracts with are more influential than those of Y.ALSO SEE: Here is an article which can give you a kick start usingnetwork analysis.Google can help us create features in multiple ways. Here we will take help of Google in two different ways:First, the direct information which can be extracted from Google. A few example are as follows:Second, the capability of Google being leveraged directly in our analytics projects. Google hasalways been the undisputed leader in data science. We can leverage Googles strong algorithms directly. Below are a few ways:1. Google has the facilityof auto-correcting spellings.In text mining concepts, this is like an unnatural power which can be directly leveraged. For example, I have a list of cricketers from the year 1970 to 2015. I want to aggregate all the records made by all cricketers. But, the information is manually typed, hence requires cleaning. One of the record states Mahendra Singh Thoni! Should we combine this with Mahendra Singh Dhonis record or not? Of course the answer is yes, but we cannot go to each record and check. So we make an automated system which uses Google API and search for the keyword and picks up the top 5-10 searches. If all these searches respond to a single key (which in this case is MS Dhoni) we will impute the information by new key. Here is a video which can help you write python codes to bring out all the search links for a keyword.2. Google also has the capability to know the popularity of different pages. Using this we can check the popularity of different pages in different countries. This can dictate us a few key trends for each country.3. Googles capability to recognize language can also be exploited to impute information in countries like Germany or Japan, where information isdirectly fed in the local language. Though, this can be translated using Google Translator to standardize the entire data.Just like Google in search, Youtube is the undisputed worldwide leader in video sharing websites. Youtube API can be used to find the popularity of videos and thereby the popularity of the topic of videos. All the likes, dislikes, comments information can tied up together to understand the trends in preference of customers.ALSO SEE: Here is an article which will get you kick started with harnessing You tube information.My objective of writing this article was to ignite interest in upcoming data sources which can be readily used in different industries without much investments.The information sources stated above are easily accessible and carry massive potential of transforming analytics industry.Did you find the article useful? Share with us all the new sources of information which you have used in your projects. Also share with us any links of related video or article to leverage these data sources. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/03/building-features-variables-open-data/
A Comprehensive Guide to Data Exploration,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Steps of Data Exploration and Preparation|2. Missing Value Treatment|3. Techniques of Outlier Detection and Treatment|4. The Art of Feature Engineering|End Notes,"Lets get started.|Variable Identification|Univariate Analysis|Bi-variateAnalysis|Why missing values treatment is required?|Why my data has missing values?|Which are the methods to treat missing values ?|What is an Outlier?|What are the types of Outliers?||What causes Outliers?|What is the impact of Outliers on a dataset?||How to detect Outliers?||How to remove Outliers?|What is Feature Engineering?|What is the process of Feature Engineering ?|What is Variable Transformation?|When should we use Variable Transformation?|What are the common methods of Variable Transformation?|What is Feature /Variable Creation & its Benefits?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|20 Powerful Images which perfectly captures the growth of Data Science|The Ultimate Plan to Become a Data Scientist in 2016|
Sunil Ray
|102 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","For further read, here is a list of transformation / creation ideas which can be applied to your data.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are no shortcuts for data exploration. If you are in a state of mind, that machine learning can sail you away from every data storm, trust me, it wont. After some point of time, youll realize that you are strugglingat improving models accuracy. In such situation, data exploration techniques will come to your rescue.I can confidently say this, because Ive been through such situations, a lot.I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.Ive created this tutorial to help you understand the underlying techniques of data exploration. As always, Ive tried my best to explain these concepts in the simplest manner. For better understanding, Ive taken up few examples to demonstrate the complicated concepts.Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot oftime and efforts here. Withmy personal estimate, data exploration, cleaning and preparation cantakeup to 70% of your total project time.Below are the steps involved to understand, clean and prepare your data for building your predictive model:Finally, we will need to iterate over steps 4  7 multiple times before we come up with our refined model.Lets now study each stage in detail:-First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.Lets understand this step more clearly by taking an example.Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.Below, the variables have been defined in different category:At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable typeis categorical orcontinuous.Lets look at these methods and statistical measures for categorical and continuous variables individually:Continuous Variables:-In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:Note:Univariate analysisis also used to highlight missing and outlier values. In theupcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer coursedescriptive statistics from Udacity.Categorical Variables:- For categorical variables, well use frequency table to understand distribution ofeach category. We can also read as percentage of valuesunder each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.Bi-variate Analysis finds out the relationship between two variables. Here, we look forassociation and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.Lets understand the possible combinations in detail:Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way tofind out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.Scatter plot shows the relationship between two variable but does not indicatesthe strength of relationship amongst them. To find the strength of the relationship, we useCorrelation. Correlation varies between -1 and +1.Correlation can be derived using following formula:Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))Various tools have function or functionality to identify correlation between variables. In Excel, function CORREL() is used to return the correlation between two variables and SAS uses procedure PROC CORR to identify the correlation. These function returns Pearson Correlation value to identify the relationship between two variables:In above example, we have goodpositive relationship(0.65) between two variables X and Y.Categorical & Categorical:To find the relationship between two categorical variables, we can use following methods:Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by:

From previous two-way table, the expected count for product category 1 to be of small size is0.22. It is derived bytaking the row total for Size(9) times the column total for Product category (2) then dividing by the sample size (81). This is procedure is conducted for each cell. Statistical Measures used to analyze the power of relationship are:Different data science language and tools have specific methods to perform chi-square test. In SAS, we can use Chisqas an option with Proc freq to perform this test.Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are smallin number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks.We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.Till here, we have understoodthe first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables.Now, we will look at the methods ofMissing values Treatment.More importantly, we will also look at why missing values occur in our data and why treating them is necessary.Missing data in the training data set can reduce the power / fit of a model or can lead toa biased model because we have not analysedthe behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.Notice the missing values in the image shown above: In the left scenario, we have not treated missing values. The inference from this data set is that the chances of playing cricket by males is higher than females. On the other hand, if you look at the second table, which shows data after treatment of missing values (based on gender), we can see that females have higher chances of playing cricket compared to males.We looked at the importance of treatment of missing values in a dataset. Now, lets identify the reasons for occurrence of these missing values. Theymay occur at two stages:
After dealing with missing values, the next task is to deal with outliers. Often, we tend to neglect outliers while building models. This is a discouraging practice. Outliers tend to make your data skewed and reduces accuracy. Lets learn more about outlier treatment.Outlier is a commonly usedterminology by analysts and data scientists asit needsclose attention else itcan result in wildly wrongestimations. Simply speaking,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.Lets take an example, we do customer profiling and find out that the average annual income of customers is $0.8 million. But, there are two customers having annual income of $4 and $4.2 million. These two customers annual income is much higher thanrest of the population. These two observations will be seen asOutliers.Outlier can be of two types:Univariate andMultivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.Let us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look atthe box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two valuesbelowand one above the average in a specific segment of weight and height.Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:Lets understand various types of outliers in more detail:Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:To understand the impact deeply, lets take an example to checkwhat happens to a data set with and without outliersin thedata set.Example:As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 5.45. But with the outlier, average soarsto30. This would change the estimate completely.Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detectoutliers. Some of them are:Most of the waysto dealwith outliers are similar to the methods ofmissing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we willdiscuss thecommon techniques used to deal with outliers:Deleting observations:We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning values:Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allowsto deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.Imputing:Likeimputationof missing values, we can also imputeoutliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.Treat separately:If there are significant number of outliers, we should treat them separatelyin the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and thencombine the output.
Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Lets now proceed to the final stage of data exploration. It is Feature Engineering.Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.For example, lets say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.This exercising of bringing out information from data in known as feature engineering.You perform feature engineering once you have completed the first 5 steps in data exploration Variable Identification,Univariate, Bivariate Analysis,Missing Values ImputationandOutliers Treatment. Feature engineering itself can be divided in 2 steps:These two techniques are vital in data exploration andhavea remarkableimpact on the power of prediction.Letsunderstand each of this step in more details.In data modelling, transformation refers tothe replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.Lets look at the situations when variable transformation is useful.Below are the situations where variable transformation is arequisite:There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Lets look at these methods in detailbyhighlightingthe pros and cons of these transformation methods.Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, wehave date(dd-mm-yy) as an input variable in adata set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:There are various techniques to create new features. Lets look at the some of the commonly used methods:As mentioned in the beginning, quality and efforts invested in data exploration differentiates a good model from a bad model.This ends our guideon data exploration and preparation. In this comprehensive guide, we looked at the seven steps of data exploration in detail. The aim of this series was to provide an in depth and step by step guide to an extremely important process in data science.Personally, I enjoyed writing this guideand would love to learn from your feedback. Didyou findthis guideuseful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2015/03/feature-engineering-variable-transformation-creation/
How Apple Watch would re-define Apples products in next 3 years?,Learn everything about Analytics,"So, why am I writing another post on this? |Feature 1: Launch of Apple Watch heralds deep personalization for Apples customers:|Feature 2: You are now constantly on!|Feature 3: Heart beat tracking is now a reality:|Lets put these features together:|So, how do I feel about Apple Watch?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Feature Engineering: How to transform variables and create new ones?|Framework and Applications of ARIMA time series models|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"By the time I publish this post, the internet and blogosphere would be swamped with Apple Watch written all over it.For a simple reason  my perspective is different. I think some of the advancements from Apple havent been discussed as much as they should have been. Please note that this is a data engineers perspective on Apple Watch.The purpose of this article is not to review the watch (Sorry, the Apple Watch!) as a product. This article aims to bring out how Apple Watch would re-define Apples data warehouses and present a(nother) competitive edge to Apple in years to come.Here are a few features of Apple Watch I want to callout as a data scientist:Think of iPhones 2 years back  there used to be 1 or may be 2 models to choose from. So, every customer with an iPhone 5 was similar  the device you possessed didnt tell Apple any thing about you till they start mining your behaviour from App usage. Apple had no way to know upfront whether you are a fashion buff? A sports freak? A working professional?In last 2 years, Apple drove basic personalization in their products  Apple iPhone 5c and 5s launched simultaneously, so did iPhone 6 and 6 Plus. But, this was still 2 new devices at a time and a total of 4  8 devices to buy from at any time.All of this changes with Apple Watch  different models, different kind of bands, different sizes with Prices varying from $350  $10,000 provides very specific information about the user to Apple. There are 38 different models to choose your design from!If that does notdrive the point home,overlay the data from your iPhone (Watches would need to be paired with iPhones) and app usage behaviour. We are clearly talking a granular segmentation, probably no other player in the industry has been able to achieve. How wouldthis impact you? Well, in almost every manner! Would you worry about a notification for a transaction of $5 from Credit card if you possess one of the Gold plated Apple Watches? The chances are slim!If that has got you thinking, the next few features will give you an idea about Apples Genius behind this beautiful piece of instrument!There are 2 aspects of this feature worth mentioning:What does this mean? More and more data about your habits and your lifestyle, which was not possible through your phones.One of the challenges here would be the battery life  18 hours. If only Apple would have made it in days, they would have just scored a home run in collecting customer data. I expect this to improve dramatically in the next iteration of Watch  something to watch out for!According to CDC,1 in every 4 deaths in the United States is because of heart disease. The percentage would be far higher in middle and low income countries. How far are we from the time, before a company releases data about heart rates for a Kaggle competition which makes heart ailments more predictable? It is unlikely that Apple would do this, but Apple already has in house expertiseto make use of this data.Tracking heart beat every second can yield results which are probably difficult to think right now. How far are we from the time when your health app tells you that exercising 5 minutes extra is likely to extend your life by x days? How does that gamification sound?Now, let me just give a few examples of some data driven products which Apple Watch would enable in future:Frankly, the possibilities are limitless!The journey would take some time. Every new product has its own growth cycle and so would Apple Watch. Also. Apple still needs to make a few tweaks in their product:Overall, I am damn excited about the product as a data scientist (Of course, it is beautiful too and tells the time as well, as Tim Cook says it). It might take some tweaks before it becomes the dream of a data scientist. Ill also closely watch how Apple handles privacy issues with customers and app developers.Apple has started its journey for deep personalization through data, and I cant wait to see how they arrive in style! I think that the data asset created by Apple through Watch would re-define their products and experience in years to come.",https://www.analyticsvidhya.com/blog/2015/03/apple-watch-re-define-apple-products-3-years/
A Complete Tutorial on Time Series Modeling in R,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Basics  Time Series Modeling|2. Exploration of Time Series Data in R|3. Introduction to ARMA Time Series Modeling|4. Framework and Application of ARIMA Time Series Modeling|Projects|End Notes,"Stationary Series|Why do I care about stationarity of a time series?|Random Walk|Lets spice up things a bit,|Dickey Fuller Test of Stationarity|Loading the Data Set|Detailed Metrics|Important Inferences|Auto-Regressive Time Series Model|Moving Average Time Series Model|Difference between AR and MA models|Exploiting ACF and PACF plots|Overview of the Framework|Step 1: Visualize the Time Series|Step 2: Stationarize the Series|Step 3: Find Optimal Parameters|Step 4: Build ARIMA Model|Step 5: Make Predictions|Applications of Time Series Model|Where did we start ?|What do you see in the chart shown above?|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Business Analytics Programs in India (2015  16)|10 Machine Learning Algorithms Explained to an Army Soldier|
Tavish Srivastava
|50 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the mostimportant factor which ensures success in a business. Its difficult to keep up with the pace of time. But, technology has developed some powerful methods using which we cansee things ahead of time. Dont worry, I am not talking about Time Machine. Lets be realistic here!Im talking about the methods of prediction & forecasting. One such method, which deals with time based data is Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to makeinformed decision making.Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyzesales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.So, if you arent sure about complete process of time series modeling, this guide would introduce you to various levels of time series modelingand its related techniques.Time toget started!Lets begin from basics. This includes stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity. If these terms are already scaring you, dont worry  they will become clear in a bit and I bet you will start enjoying the subject as I explain it.There are three basic criterion for a series to be classified as stationary series :1. The mean of the series should not be a function of time rather should be a constant. The image below has the lefthand graph satisfying the condition whereas the graph in red has a time dependent mean.2. The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)3. The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the red series.The reason I took up this section first was that until unless your time series is stationary, you cannot build a time series model. In cases where the stationary criterion are violated, the first requisite becomesto stationarize the time series and then try stochastic models to predict this time series. There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.This is themost basic concept of the time series. You might know the concept well. But, I found many people in the industry who interprets random walk as a stationary process. In this section with the help of some mathematics, I will make this concept crystal clear for ever. Lets take an example.Example: Imagine a girl moving randomly on a giant chess board. In this case, next position of the girl is only dependent on the last position.(Source: http://scifun.chem.wisc.edu/WOP/RandomWalk.html )Now imagine, you are sitting in another room and are not able to see the girl. You want to predict the position of the girl with time. How accurate will you be? Of course you will become more and more inaccurate as the position of the girl changes. At t=0 you exactly know where the girl is. Next time, she can only move to 8 squares and hence your probability dips to 1/8 instead of 1 and it keeps on going down. Now lets try to formulate this series :where Er(t) is the error at time point t. This is the randomness the girl brings at every point in time.Now, if we recursively fit in all the Xs, we will finally end up to the following equation :Now, lets try validating our assumptions of stationary series on this random walk formulation:1. Is the Mean constant ?We know that Expectation of any Error will be zero as it is random.Hence we get E[X(t)] = E[X(0)] = Constant.2. Is the Variance constant?Hence, we infer that the random walk is not a stationary process as it has a time variant variance. Also, if we check the covariance, we see that too is dependent on time.We already know that a random walk is a non-stationary process. Let us introduce a new coefficient in the equation to see if we can make the formulation stationary.Introduced coefficient : RhoNow, we will vary the value of Rho to see if we can make the series stationary. Here we will interpret the scatter visually and not do any test to check stationarity.Lets start with a perfectly stationary series with Rho = 0 . Here is the plot for the time series :Increase the value of Rho to 0.5 gives us following graph :You might notice that our cycles have become broader but essentially there does not seem to be a serious violation of stationary assumptions. Lets now take a more extreme case of Rho = 0.9We still see that the X returns back from extreme values to zero after some intervals. This series also is not violating non-stationarity significantly. Now, lets take a look at the random walk with rho = 1.This obviously is an violation to stationary conditions. What makes rho = 1 a special case which comes out badly in stationary test? We will find the mathematical reason to this.Lets take expectation on each side of the equation X(t) = Rho * X(t-1) + Er(t)This equation is very insightful. The next X (or at time point t) is being pulled down to Rho * Last value of X.For instance, if X(t  1 ) = 1, E[X(t)] = 0.5 ( for Rho = 0.5) . Now, if X moves to any direction from zero, it is pulled back to zero in next step. The only component which can drive it even further is the error term. Error term is equally probable to go in either direction. What happens when the Rho becomes 1? No force can pull the X down in the next step.What you just learnt in the last section is formally known as Dickey Fuller test. Here is a small tweak which is made for our equation to convert it to a Dickey Fuller test:We have to test if Rho  1 is significantly different than zero or not. If the null hypothesis getsrejected, well get a stationary time series.Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.Lets nowconsider an example to show you what a time series looks like.Here well learn to handle time series data onR. Our scopewill be restricted to data exploring in a time series type of data set and not go to building time series models.I have used an inbuilt data set of R called AirPassengers. The dataset consists ofmonthly totals of international airline passengers, 1949 to 1960.Following is the code which will help you load the data set and spill out a few top level metrics.Here are a few more operations you can do:Exploring data becomes most important in a time series model  without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.Lets nowtake up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.ARMA models are commonly used in time series modeling.In ARMA model, AR stands forauto-regression and MA stands formoving average. If these words sound intimidating to you, worry not  Illsimplify these concepts in next few minutes for you!We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose fromthe available time series models.First, Ill explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.Lets understanding AR models using the casebelow:The current GDP of a country say x(t) is dependent on the last years GDP i.e. x(t  1). The hypothesis being thatthe total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.Hence, we can formally write the equation of GDP as:x(t) = alpha * x(t  1) + error (t)This equation is known as AR(1) formulation.The numeral one (1) denotes that the next instance is solely dependent on the previous instance. The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.For instance, lets say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day,the temperature roseand the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing thatthe people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during thecold days. In following days, theproportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explainsthe inertia property of AR series:Lets take another caseto understand Moving average time series model.A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood atzero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market.Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so highthat the bag ran out of stock. As a result, some 100 odd customers couldntpurchase this bag. Lets call this gap as the error at that time point. With time,the bag hadlost itswoo factor. But still few customers were leftwho wentempty handed the previous day. Following is a simple formulation to depict the scenario :x(t) = beta* error(t-1) + error (t)If we try plotting this graph, it will look something like this :Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference getsexploited irrespective ofhaving the AR model or MA model. The correlation plot can give us the order of MA model.Once we have got thestationary time series, we must answertwo primary questions:Q1. Is it an AR or MA process?Q2. What order of AR or MA process do we need to use?The trick to solve these questions is availablein the previous section. Didnt you notice?The first question can be answered usingTotal Correlation Chart (also known as Auto  correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now lets reflect on what we have learnt above.In a moving average series of lag n, we will not get any correlation between x(t) and x(t  n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series, if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag(x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :              ACF                                   PACFThe blue line aboveshows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.                   ACF                                PACFClearly, the graph abovehas a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots.Now, Ill introduce you toa comprehensive framework to build a time series model. In addition, well also discuss about the practicalapplications of time series modelling.A quick revision, Till here weve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.This framework(shown below)specifies the step by step approach onHow to do a Time Series Analysis:As you would be aware, the first three stepshave already been discussed above. Nevertheless, the same has been delineated briefly below:It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.Once we know the patterns, trends, cycles and seasonality , we cancheck if the series is stationary or not. Dickey  Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesnt ends here!What if the series is found to be non-stationary?There are three commonly used technique to make a time series stationary:1. Detrending : Here, we simply remove the trend component from the time series. For instance, the equation ofmy time series is:x(t) = (mean + trend * t) + errorWell simply remove the part in the parentheses and build model for the rest.2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,x(t)  x(t-1) = ARMA (p , q)This differencing is called as the Integration part in AR(I)MA. Now, we have three parametersp : ARd : Iq : MA3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly.More on this has been discussed in the applications part below.The parameters p,d,q can be found using ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to d.With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should beour choice. We can also try some models with a seasonal component. Just in case, we noticeany seasonality in ACF/PACF plots.Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.Now, well use the same example that we have used above. Then,using time series, well make future predictions. We recommend you to check out the example before proceeding further.Following is the plot of the number of passengers with years. Try and make observations on this plotbefore moving further in the article.Here are my observations :1. There is a trend component which grows the passenger year by year.2. There looks to be a seasonal component which has a cycle less than 12 months.3. The variance in the data keeps on increasing with time.We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, lets test the resultant series.Augmented Dickey-Fuller TestWe see that the series is stationary enough to do any kind of time series modelling.Next step is to find the right parameters to be used in theARIMA model. We already know that the d component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :#ACF PlotsClearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Lets see how ACF and PACF curve come out after regressing on the difference.Clearly, ACF plot cuts off after the first lag. Hence, we understoodthat value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.Lets fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Withthis, we come to this end oftutorial on Time Series Modelling. I hope this will help you to improve your knowledgeto work on time based data. To reap maximum benefits out of this tutorial, Id suggest you to practice these R codes side by side and check your progress.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/03/framework-application-build-arima-model/
Thanking all successful women in the world of data analytics,Learn everything about Analytics,"Foreword||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Framework and Applications of ARIMA time series models|2 Simple Hacks to improve information density in your QlikView dashboards|
Analytics Vidhya Content Team
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Hilary Mason(@hmason)||Corina Cortes(@CorinnaCortes)|Claudia Perlich(@claudia_perlich)||Monica Rogati(@mrogati)||Daphne Koller(@DaphneKoller)||Lillian Pierson(@BigDataGal)|Radhika Kulkarni(LinkedIn)|Alice Zheng(LinkedIn)|||Sunita Sarawagi(LinkedIn)||,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Women likeLady Ada Lovelace, Marie Curie, Mother Teresa, Indra Nooyi, Oprah Winfrey, Marissa Mayer, Sheryl Sandberg and many others have passionately served their communities across the world. They have enlightened other people to challengethe status quo and prove their worth.On this International Womens day, we thank some of the most successful women in the world of Data Analytics. We hope that they continue to contribute to the world of data science and inspire millions through their work.Hereare some of successful women who have proved their mettle in data analytics:Carla Gentry(@data_nerd)Carla Gentryis a Data Scientist & Founder ofAnalytical Solution. She holds agraduate degree in Maths & Economics from University of Tennessee. She carries an invaluable experience of over 15 years which includes working forFortune 500 companies like Hershey, Kraft,Johnson & Johnson, Kelloggs and Firestone.She is one of the most popular personality in Big Data community to follow on Twitter.In 2013, the Information Week named her one of 10 IT Leaders to Follow on Twitter.Hilary Mason loves two things: Data & Cheeseburgers! She is the Founder of Fast Forward Labs. Also, she is the co-founder of hackNY.org & DataGotham. She completed her graduation in Computer Science from Grinnell College, US. She carries an invaluableexperienceofover 15 years which involves working at companies such as Accel Partners, Bitly, Path101 etc. In 2012, she was the recipient of TechFellow Engineering Leadership Award. She has also co-authored a book named Data Driven: Creating a Data Culture.Corinna Cortes is the Head of Google Research, NY. Corinna received her MS degree in Physics from University of Copenhagen and joined AT&T Bell Labs as a researcher. She has also worked at AT&T Labs, formerly AT&T Bell Labs for more than 10 years as a researcher. She received her PhD in Computer Science from the University of Rochester. Her research areas include Artificial Intelligence and Machine Learning, General Science and Algorithms & Theory. She is a competitive runner and mother of two.Claudia is currently working as a Chief Data Scientist at Dstillery, where she is working closely over a multitude of machine learning algorithms and data processing tasks. She received her MS in Computer Science from University of Colorado. She holds aPhD from Leonard N. Stern School of Business, New York University. Claudia got featured in the list of 100 most creative people of 2014 by FastCompany. Her hobbies include horse riding, weight lifting, snowboarding, gardening etc.Monica has strong background inapplied machine learning (CMU CS PhD), data science, wearables and health, social network analysis, recommender systems, statistical text mining and multilingual information retrieval (search).Her passion lies in turning data into products, actionable insights and meaningful stories. Currently, She is working as a Vice President of Data at Jawbone. Prior to Jawbone, Shewas one of the early members of the LinkedIn data science team and has worked there for more than 5 years as Senior Data Scientist. She is also one of the Fast Companys 100 most creative people in business.Daphne is the President and Co-Founder of Coursera. She holds aPhD in Computer Science from Stanford University. Later, she taught at Stanford for 18 years as a Professor of Computer Science. She has beenfelicitated withvarious awards & accolades which includes ACM Infosys Awards, MacArthur Foundation Fellowship and many more.Her research focus lies inusing probabilistic models and machine learning to understand complex domains that involve large amounts of uncertainty. Duringidle time, she likes to listen to music & spend time with her daughters.Lillian is a self-proclaimed nomadic datapreneur. She is the Founder & Chief Data Scientist at DataMania, an information services start-up. Prior to DataMania, she founded BigDataGal.com. She strongly believes Data science and big data technologies are quickly changing the world. These technologies have the power to solve business, social, lifestyle, and environmental problems to a degree that is unparalleled throughout human history. She is the author of famous book Data Science for Dummies. Shes an avid traveller and has travelled over 28 countries across the world.Currently, She is the VP of Advanced Analytics R&D at SAS Institute. Radhika oversees software development in many analytical areas including Statistics, Operations Research, Econometrics, Forecasting and Data Mining. She received MSc Degree in Mathematics from IIT Delhi, India. She holds a PhD in Operations Research from Cornell University.Radhikais a Member of the Board of Directors for IDeaS, a SAS Company.Alice is the Director of Data Science at Dato. She holds Ph.D. and B.A. degrees in Computer Science, and a B.A. in Mathematics, all from University of California, Berkley.She is known to be an expert in machine learning algorithms. She loves juggling with data and empowers others in doing so. Prior to joining Dato, she worked for 6 years at Microsoft as a machine learning researcher. Her focus is on easing the dependence on expertise by making learning algorithms more automated, their outputs more interpretable, and the labeling tasks simpler.Sunita Sarawangi is a graduate in Computer Science from IIT Kharagpur, India. Later, she received her PhD in databasesfrom University of California at Berkley. Sunitas work is well known indatabases & data miningand has several publications & patents registered ather name.Her current research interests are web information extraction, data integration, graphical models and structured learning. She was awarded theBest paper award at 1998 ACM-SIGMOD International Conference on Management of Data.She serves on the board of directors of ACM SIGKDD and VLDB foundation.Kristin P. BennettKristinis a founding associate editor of ACM Transactions on Knowledge Discovery and Data Mining. She serves on the advisory board of the Journal of Machine Learning Research. Her research areas includes Mathematical Programming,Operations Research, Machine Learning, Bioinformatics and Data Mining communities. She has gained experience developing data mining approaches for chemistry, biology, and public health related applications. Currently, She is a Professor at Mathematical Science and Computer Science at Rensselaer.Once again, we would like to thank these successful women in the field of data analytics. They have unfailingly contributed to the development of data analytics globally.This is not an exhaustive list,so if you know some one who should be on the list, please feel free to add them through the comments below.",https://www.analyticsvidhya.com/blog/2015/03/successful-women-data-analytics/
2 Simple Hacks to improve information density in your QlikView dashboards,Learn everything about Analytics,"Foreword||Method 1: Using Conditional Enable Option||Method 2:Creating Container in QlikView|||End Note|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Thanking all successful women in the world of data analytics|Data Analyst  Crayon Data  Chennai (2-8 years of experience)|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Few months back, I got anopportunity to work ona QlikView project with an insurance company. The objective wasto create a sales dashboard to bring out business insights, which could help improve sales of the company. Needless to say, the dashboard required use of multiple metrics and dimensions.One of the common ways to include multiple dimensions in QlikView is to arrange them using list boxes. While this simple approach works, it ends up taking a lot of space in the dashboard. This leads to lesser area being available to represent your information.In this situation, how do you accomplish the task? Some challenges were associated with this project as discussed below:There are multiple ways to overcome these challenges. Ill discuss two such methods in this article:Lets look at both the options in detail.Lets look at the pre and post effect of this method onsheet space in QlikView (refer to the snapshot below):Did you notice the difference in the two snapshots? Snapshot at the right has morespace available for effective data representation. How did we manage to get this extraspace? How didwe reduce the number of list boxes?Wellthis isnt rocket science! If you observe closely, you willnoticethree new sub tabs appearing on top left corner: Policy, FPA and SM. Wehave simply categorized the dimensions in three categories and enabled them conditionally.Lets look at the steps to perform this:Step-1: Create a variable Var_Active and assign it the value 0 (zero). Go to Settings Menu > Variable Overview. Then, click on ADD button to create new variable.Step-2:Categorize the dimensions, identify the number of unique categories and create text boxes for each one. Here we havethree different categories as Policy, FPA and SM represented by three text boxes.Step-3:Set action for all three text boxes to assign the unique value to variable Var_Active.This is how you can do:Select Text box > Go to properties > Actions Tab > Click on ADD > Select Externalaction type and Set Variableas an action parameter. After that put the Var_Active in variable box and assign unique value for Value text box as 0.Similar steps can be performed for FPA and SM text box also. Simply, assign unique value 1 forFPA and 2 for SM.Step-4:Select list boxes individually and apply condition enable option using unique value of their respective category.Below are the steps:Select List box (Branch City) > Go to Properties > Layout Tab > Option button on for Conditional Show > Put expression Var_Active=0. Here, we have assignedvalue 0 to Var_Active asit belongs to category Policy'(value for Var_Active is set at 0).Similarly, we can do it for other list boxes and assign respective variable value based on their category.Step-5:This is an optional step. It enhances the visual effect. When we select a text box, its color changes,which indicates it as an active category. Below are the steps to do it:Select text box (Policy) > Go to properties window > General tab > Click on color button >Select calculated option under base color bucket. Then, write the expression as: =If(Var_Active=0,rgb(0,128,0),rgb(214,231,248)).This expression can be read as: If variable value is 0 then color should go as green else default color. Similar expression can be written for other text boxes also. Only conditional argument will change as Var_Active=1for FPA and Var_Active=2for SM.Now, we have sub tabs for list boxes seen as Policy, FPA and SM. It will show list boxes related to the active category. Similarly, we can use command button to perform this conditional enable or disable list boxes. This feature can also be used with chart objects.QlikView has another object that can be used to show multiple chart objects at the same place where charts objects can be accessed using tabs knownasCONTAINER.Containeris a QlikView object which holds multiple charts in the same box as seen below. All charts will appear in the same window but only one chart will appear active at a given time and navigation to different charts can be done using the bottom tabs. Position of the tabs can be changed to top, left and right as well. Look at the snapshot below, here we have four different charts namely Region wise sales, Region wise Profit, Manpower Distribution, YoY Growth available in one container.Users can navigate through the charts by clicking on their respective tabs.Method to create container is as easy as drag and drop objects. It is just a two steps process. Lets take a look:Select Container from New Sheet Object List > Select objects from list of available objects on General tab.Now,we have Container with selected list of objects. We can change the position of tab or container type by selecting different option under Presentation tab.In this article, we have looked at twomethods foreffective use of sheet space:These two techniques arequite effective in utilizing your sheet space.Haveyou used these techniques everwhile working on QlikView? Do yo know of any other hacks to utilize sheet space effectively? Share your valuable experience with our readers through comments below.",https://www.analyticsvidhya.com/blog/2015/03/qlikview-container-conditional-enabling/
Data Analyst  Crayon Data  Chennai (2-8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|2 Simple Hacks to improve information density in your QlikView dashboards|Learning R couldnt get easieR and betteR!|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data AnalystLocation  ChennaiAbout employer CrayonDataJob description: We are looking to build a team of inspired, self-driven, analytical, flexible and focused individuals who bring great energy into a new enterprise with their experience, skills and winning attitude. If you get excited about building new things and arent daunted by the challenge of building something from scratch, then Crayon is your Simpler Choice.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Crayon Data  ChennaiIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/03/data-analyst-crayon-chennai-2-8-years-experience/
Learning R couldnt get easieR and betteR!,Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Analyst  Crayon Data  Chennai (2-8 years of experience)|Introduction to ARMA Time Series Models  Simplified|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We launched our learning paths in January this year  and they have done phenomenally well. They solve a problem every person wanting to learn data science faces today  lack of a clear, crisp and structured road map. After looking at the learning path for Python, a lot of you asked for a learning path on R.I am pleased to share that not only have we released a learning path on R. We got it created from one of the best R experts I know of  none other than Ajay Ohri!So, I would not take thethunder away fromlearning path on R go check it out for yourself.Keep leaRning!",https://www.analyticsvidhya.com/blog/2015/03/learning-r-easier-better/
A Complete Tutorial on Time Series Modeling in R,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Basics  Time Series Modeling|2. Exploration of Time Series Data in R|3. Introduction to ARMA Time Series Modeling|4. Framework and Application of ARIMA Time Series Modeling|Projects|End Notes,"Stationary Series|Why do I care about stationarity of a time series?|Random Walk|Lets spice up things a bit,|Dickey Fuller Test of Stationarity|Loading the Data Set|Detailed Metrics|Important Inferences|Auto-Regressive Time Series Model|Moving Average Time Series Model|Difference between AR and MA models|Exploiting ACF and PACF plots|Overview of the Framework|Step 1: Visualize the Time Series|Step 2: Stationarize the Series|Step 3: Find Optimal Parameters|Step 4: Build ARIMA Model|Step 5: Make Predictions|Applications of Time Series Model|Where did we start ?|What do you see in the chart shown above?|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Business Analytics Programs in India (2015  16)|10 Machine Learning Algorithms Explained to an Army Soldier|
Tavish Srivastava
|50 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the mostimportant factor which ensures success in a business. Its difficult to keep up with the pace of time. But, technology has developed some powerful methods using which we cansee things ahead of time. Dont worry, I am not talking about Time Machine. Lets be realistic here!Im talking about the methods of prediction & forecasting. One such method, which deals with time based data is Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to makeinformed decision making.Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyzesales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.So, if you arent sure about complete process of time series modeling, this guide would introduce you to various levels of time series modelingand its related techniques.Time toget started!Lets begin from basics. This includes stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity. If these terms are already scaring you, dont worry  they will become clear in a bit and I bet you will start enjoying the subject as I explain it.There are three basic criterion for a series to be classified as stationary series :1. The mean of the series should not be a function of time rather should be a constant. The image below has the lefthand graph satisfying the condition whereas the graph in red has a time dependent mean.2. The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)3. The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the red series.The reason I took up this section first was that until unless your time series is stationary, you cannot build a time series model. In cases where the stationary criterion are violated, the first requisite becomesto stationarize the time series and then try stochastic models to predict this time series. There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.This is themost basic concept of the time series. You might know the concept well. But, I found many people in the industry who interprets random walk as a stationary process. In this section with the help of some mathematics, I will make this concept crystal clear for ever. Lets take an example.Example: Imagine a girl moving randomly on a giant chess board. In this case, next position of the girl is only dependent on the last position.(Source: http://scifun.chem.wisc.edu/WOP/RandomWalk.html )Now imagine, you are sitting in another room and are not able to see the girl. You want to predict the position of the girl with time. How accurate will you be? Of course you will become more and more inaccurate as the position of the girl changes. At t=0 you exactly know where the girl is. Next time, she can only move to 8 squares and hence your probability dips to 1/8 instead of 1 and it keeps on going down. Now lets try to formulate this series :where Er(t) is the error at time point t. This is the randomness the girl brings at every point in time.Now, if we recursively fit in all the Xs, we will finally end up to the following equation :Now, lets try validating our assumptions of stationary series on this random walk formulation:1. Is the Mean constant ?We know that Expectation of any Error will be zero as it is random.Hence we get E[X(t)] = E[X(0)] = Constant.2. Is the Variance constant?Hence, we infer that the random walk is not a stationary process as it has a time variant variance. Also, if we check the covariance, we see that too is dependent on time.We already know that a random walk is a non-stationary process. Let us introduce a new coefficient in the equation to see if we can make the formulation stationary.Introduced coefficient : RhoNow, we will vary the value of Rho to see if we can make the series stationary. Here we will interpret the scatter visually and not do any test to check stationarity.Lets start with a perfectly stationary series with Rho = 0 . Here is the plot for the time series :Increase the value of Rho to 0.5 gives us following graph :You might notice that our cycles have become broader but essentially there does not seem to be a serious violation of stationary assumptions. Lets now take a more extreme case of Rho = 0.9We still see that the X returns back from extreme values to zero after some intervals. This series also is not violating non-stationarity significantly. Now, lets take a look at the random walk with rho = 1.This obviously is an violation to stationary conditions. What makes rho = 1 a special case which comes out badly in stationary test? We will find the mathematical reason to this.Lets take expectation on each side of the equation X(t) = Rho * X(t-1) + Er(t)This equation is very insightful. The next X (or at time point t) is being pulled down to Rho * Last value of X.For instance, if X(t  1 ) = 1, E[X(t)] = 0.5 ( for Rho = 0.5) . Now, if X moves to any direction from zero, it is pulled back to zero in next step. The only component which can drive it even further is the error term. Error term is equally probable to go in either direction. What happens when the Rho becomes 1? No force can pull the X down in the next step.What you just learnt in the last section is formally known as Dickey Fuller test. Here is a small tweak which is made for our equation to convert it to a Dickey Fuller test:We have to test if Rho  1 is significantly different than zero or not. If the null hypothesis getsrejected, well get a stationary time series.Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.Lets nowconsider an example to show you what a time series looks like.Here well learn to handle time series data onR. Our scopewill be restricted to data exploring in a time series type of data set and not go to building time series models.I have used an inbuilt data set of R called AirPassengers. The dataset consists ofmonthly totals of international airline passengers, 1949 to 1960.Following is the code which will help you load the data set and spill out a few top level metrics.Here are a few more operations you can do:Exploring data becomes most important in a time series model  without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.Lets nowtake up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.ARMA models are commonly used in time series modeling.In ARMA model, AR stands forauto-regression and MA stands formoving average. If these words sound intimidating to you, worry not  Illsimplify these concepts in next few minutes for you!We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose fromthe available time series models.First, Ill explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.Lets understanding AR models using the casebelow:The current GDP of a country say x(t) is dependent on the last years GDP i.e. x(t  1). The hypothesis being thatthe total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.Hence, we can formally write the equation of GDP as:x(t) = alpha * x(t  1) + error (t)This equation is known as AR(1) formulation.The numeral one (1) denotes that the next instance is solely dependent on the previous instance. The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.For instance, lets say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day,the temperature roseand the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing thatthe people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during thecold days. In following days, theproportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explainsthe inertia property of AR series:Lets take another caseto understand Moving average time series model.A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood atzero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market.Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so highthat the bag ran out of stock. As a result, some 100 odd customers couldntpurchase this bag. Lets call this gap as the error at that time point. With time,the bag hadlost itswoo factor. But still few customers were leftwho wentempty handed the previous day. Following is a simple formulation to depict the scenario :x(t) = beta* error(t-1) + error (t)If we try plotting this graph, it will look something like this :Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference getsexploited irrespective ofhaving the AR model or MA model. The correlation plot can give us the order of MA model.Once we have got thestationary time series, we must answertwo primary questions:Q1. Is it an AR or MA process?Q2. What order of AR or MA process do we need to use?The trick to solve these questions is availablein the previous section. Didnt you notice?The first question can be answered usingTotal Correlation Chart (also known as Auto  correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now lets reflect on what we have learnt above.In a moving average series of lag n, we will not get any correlation between x(t) and x(t  n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series, if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag(x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :              ACF                                   PACFThe blue line aboveshows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.                   ACF                                PACFClearly, the graph abovehas a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots.Now, Ill introduce you toa comprehensive framework to build a time series model. In addition, well also discuss about the practicalapplications of time series modelling.A quick revision, Till here weve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.This framework(shown below)specifies the step by step approach onHow to do a Time Series Analysis:As you would be aware, the first three stepshave already been discussed above. Nevertheless, the same has been delineated briefly below:It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.Once we know the patterns, trends, cycles and seasonality , we cancheck if the series is stationary or not. Dickey  Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesnt ends here!What if the series is found to be non-stationary?There are three commonly used technique to make a time series stationary:1. Detrending : Here, we simply remove the trend component from the time series. For instance, the equation ofmy time series is:x(t) = (mean + trend * t) + errorWell simply remove the part in the parentheses and build model for the rest.2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,x(t)  x(t-1) = ARMA (p , q)This differencing is called as the Integration part in AR(I)MA. Now, we have three parametersp : ARd : Iq : MA3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly.More on this has been discussed in the applications part below.The parameters p,d,q can be found using ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to d.With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should beour choice. We can also try some models with a seasonal component. Just in case, we noticeany seasonality in ACF/PACF plots.Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.Now, well use the same example that we have used above. Then,using time series, well make future predictions. We recommend you to check out the example before proceeding further.Following is the plot of the number of passengers with years. Try and make observations on this plotbefore moving further in the article.Here are my observations :1. There is a trend component which grows the passenger year by year.2. There looks to be a seasonal component which has a cycle less than 12 months.3. The variance in the data keeps on increasing with time.We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, lets test the resultant series.Augmented Dickey-Fuller TestWe see that the series is stationary enough to do any kind of time series modelling.Next step is to find the right parameters to be used in theARIMA model. We already know that the d component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :#ACF PlotsClearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Lets see how ACF and PACF curve come out after regressing on the difference.Clearly, ACF plot cuts off after the first lag. Hence, we understoodthat value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.Lets fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Withthis, we come to this end oftutorial on Time Series Modelling. I hope this will help you to improve your knowledgeto work on time based data. To reap maximum benefits out of this tutorial, Id suggest you to practice these R codes side by side and check your progress.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/03/introduction-auto-regression-moving-average-time-series/
A Comprehensive Guide to Data Exploration,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Steps of Data Exploration and Preparation|2. Missing Value Treatment|3. Techniques of Outlier Detection and Treatment|4. The Art of Feature Engineering|End Notes,"Lets get started.|Variable Identification|Univariate Analysis|Bi-variateAnalysis|Why missing values treatment is required?|Why my data has missing values?|Which are the methods to treat missing values ?|What is an Outlier?|What are the types of Outliers?||What causes Outliers?|What is the impact of Outliers on a dataset?||How to detect Outliers?||How to remove Outliers?|What is Feature Engineering?|What is the process of Feature Engineering ?|What is Variable Transformation?|When should we use Variable Transformation?|What are the common methods of Variable Transformation?|What is Feature /Variable Creation & its Benefits?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|20 Powerful Images which perfectly captures the growth of Data Science|The Ultimate Plan to Become a Data Scientist in 2016|
Sunil Ray
|102 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
","For further read, here is a list of transformation / creation ideas which can be applied to your data.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are no shortcuts for data exploration. If you are in a state of mind, that machine learning can sail you away from every data storm, trust me, it wont. After some point of time, youll realize that you are strugglingat improving models accuracy. In such situation, data exploration techniques will come to your rescue.I can confidently say this, because Ive been through such situations, a lot.I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.Ive created this tutorial to help you understand the underlying techniques of data exploration. As always, Ive tried my best to explain these concepts in the simplest manner. For better understanding, Ive taken up few examples to demonstrate the complicated concepts.Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot oftime and efforts here. Withmy personal estimate, data exploration, cleaning and preparation cantakeup to 70% of your total project time.Below are the steps involved to understand, clean and prepare your data for building your predictive model:Finally, we will need to iterate over steps 4  7 multiple times before we come up with our refined model.Lets now study each stage in detail:-First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.Lets understand this step more clearly by taking an example.Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.Below, the variables have been defined in different category:At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable typeis categorical orcontinuous.Lets look at these methods and statistical measures for categorical and continuous variables individually:Continuous Variables:-In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:Note:Univariate analysisis also used to highlight missing and outlier values. In theupcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer coursedescriptive statistics from Udacity.Categorical Variables:- For categorical variables, well use frequency table to understand distribution ofeach category. We can also read as percentage of valuesunder each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.Bi-variate Analysis finds out the relationship between two variables. Here, we look forassociation and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.Lets understand the possible combinations in detail:Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way tofind out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.Scatter plot shows the relationship between two variable but does not indicatesthe strength of relationship amongst them. To find the strength of the relationship, we useCorrelation. Correlation varies between -1 and +1.Correlation can be derived using following formula:Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))Various tools have function or functionality to identify correlation between variables. In Excel, function CORREL() is used to return the correlation between two variables and SAS uses procedure PROC CORR to identify the correlation. These function returns Pearson Correlation value to identify the relationship between two variables:In above example, we have goodpositive relationship(0.65) between two variables X and Y.Categorical & Categorical:To find the relationship between two categorical variables, we can use following methods:Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by:

From previous two-way table, the expected count for product category 1 to be of small size is0.22. It is derived bytaking the row total for Size(9) times the column total for Product category (2) then dividing by the sample size (81). This is procedure is conducted for each cell. Statistical Measures used to analyze the power of relationship are:Different data science language and tools have specific methods to perform chi-square test. In SAS, we can use Chisqas an option with Proc freq to perform this test.Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are smallin number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks.We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.Till here, we have understoodthe first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables.Now, we will look at the methods ofMissing values Treatment.More importantly, we will also look at why missing values occur in our data and why treating them is necessary.Missing data in the training data set can reduce the power / fit of a model or can lead toa biased model because we have not analysedthe behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.Notice the missing values in the image shown above: In the left scenario, we have not treated missing values. The inference from this data set is that the chances of playing cricket by males is higher than females. On the other hand, if you look at the second table, which shows data after treatment of missing values (based on gender), we can see that females have higher chances of playing cricket compared to males.We looked at the importance of treatment of missing values in a dataset. Now, lets identify the reasons for occurrence of these missing values. Theymay occur at two stages:
After dealing with missing values, the next task is to deal with outliers. Often, we tend to neglect outliers while building models. This is a discouraging practice. Outliers tend to make your data skewed and reduces accuracy. Lets learn more about outlier treatment.Outlier is a commonly usedterminology by analysts and data scientists asit needsclose attention else itcan result in wildly wrongestimations. Simply speaking,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.Lets take an example, we do customer profiling and find out that the average annual income of customers is $0.8 million. But, there are two customers having annual income of $4 and $4.2 million. These two customers annual income is much higher thanrest of the population. These two observations will be seen asOutliers.Outlier can be of two types:Univariate andMultivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.Let us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look atthe box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two valuesbelowand one above the average in a specific segment of weight and height.Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:Lets understand various types of outliers in more detail:Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:To understand the impact deeply, lets take an example to checkwhat happens to a data set with and without outliersin thedata set.Example:As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 5.45. But with the outlier, average soarsto30. This would change the estimate completely.Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detectoutliers. Some of them are:Most of the waysto dealwith outliers are similar to the methods ofmissing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we willdiscuss thecommon techniques used to deal with outliers:Deleting observations:We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning values:Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allowsto deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.Imputing:Likeimputationof missing values, we can also imputeoutliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.Treat separately:If there are significant number of outliers, we should treat them separatelyin the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and thencombine the output.
Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Lets now proceed to the final stage of data exploration. It is Feature Engineering.Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.For example, lets say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.This exercising of bringing out information from data in known as feature engineering.You perform feature engineering once you have completed the first 5 steps in data exploration Variable Identification,Univariate, Bivariate Analysis,Missing Values ImputationandOutliers Treatment. Feature engineering itself can be divided in 2 steps:These two techniques are vital in data exploration andhavea remarkableimpact on the power of prediction.Letsunderstand each of this step in more details.In data modelling, transformation refers tothe replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.Lets look at the situations when variable transformation is useful.Below are the situations where variable transformation is arequisite:There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Lets look at these methods in detailbyhighlightingthe pros and cons of these transformation methods.Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, wehave date(dd-mm-yy) as an input variable in adata set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:There are various techniques to create new features. Lets look at the some of the commonly used methods:As mentioned in the beginning, quality and efforts invested in data exploration differentiates a good model from a bad model.This ends our guideon data exploration and preparation. In this comprehensive guide, we looked at the seven steps of data exploration in detail. The aim of this series was to provide an in depth and step by step guide to an extremely important process in data science.Personally, I enjoyed writing this guideand would love to learn from your feedback. Didyou findthis guideuseful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2015/02/outliers-detection-treatment-dataset/
How to choose the right data science / analytics / big data training?,Learn everything about Analytics,"|Framework to choose right analytics training:|Overview of the framework:|Step 1: Which tool to learn?|Step 2: Which techniques should you be learning?|Step 3: How should you learn?|Step 4: Where to learn?|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How to detect Outliers in your dataset and treat them?|Exploration of Time Series Data in R|
Kunal Jain
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Over the last 2 years, this is the most common query I receive from our readers:Which data science / analytics training should I go for?The query comes in varied shapes and size, but the inherent question is still the same.I can empathize with people facingthese questions  the number of tools, analytical techniques under applicationand trainings provider, all have increased many-fold in last fewyears. If the trends and projections are to be believed, this is probably just the start of a growth phase.Lets take an example, as a person switching from software industry, do you learn SAS or do you learn R? Or should you learn Big Data tools and techniques? How about machine learning? Data Visualization tools? Even if you zero in on one of these, the next question which arises is where and how to undergo these trainings?I am sure most ofthe person in this situation feel like the person in the imageabove. This is where a framework can help you.I aimto provide a frameworkto you to decide:You can apply it at variousstages of your analytics career to find out what should you be learning next.The answer to first 2 questions in this framework are in form of levels or steps. You start from level 0 and move one step at a time. So if you are a complete fresher start from Level 0 of tools and level 0 of techniques. But, if you are a fresher with statistics background, start with Level 1 of tools (assuming you know Excel) and Level 1 of techniques (move to level 2 if you know predictive modeling)Once you have finalized the tools and techniques to learn, move on to step 3 and step 4 of the process.Level 0: Excel.If you dont know excel, you should learn it first. You should be able to play with Pivot tables, do simple data manipulations and apply lookups in Excel.Level 1: SAS / R / PythonThis is going to be your work horse. You can choose any of these languages. For a more detailed comparison, have a look at this article.Level 2: QlikView / Tableau / D3.jsYou should add up your repository with one of the visualization tools.Level 3: Big Data toolsThis in itself can be multiple levels  start with Hadoop stack  HDFS, HBase, Pig, Hive, SparkLevel 4:NoSQL DatabasesAgain, you can read an overview of NoSQL databases here and start by learning the most popular one  MongoDB.Exception 1: If you come from MIS / reporting background, you can start from learning visualization tools like QlikView and Tableau (Level 2) and then go to Level 1Exception 2: If you come from software engineering / web development and know one of the 2 languages  Java or Python, you can start from Big Data tools as well (level 3)Now that you know, which tool would you want to learn, let us look at the techniques to learn. Again the structure is similarLevel 0: Basics of statistics  Descriptive and Inferential statisticsLevel 1: Basic predictive modeling  ANOVA, Regression, Decision trees, Time SeriesLevel 2: All otherremaining machine learning techniques except Neural netsLevel 3: Neural nets and deep learningHow should you learn is dependent on 2 factors:This image explains the selection:On one extreme, you have option to join open courses  where you spend low (almost zero) resources, but need high self learning motivation. On the other hand, you have courses run by big universities like Stanford / MIT / North Western, where you will need to spend money and will get help and mentor-ship from experts over longer duration. You can choose the style of your learning depending on where you fit in.Please note that irrespective of which method and blend you choose, you will need to aid these trainings by hands on projects and practice. No resources or trainings can cover that for you. Here are a few examples of these projects.For people relying completely on self learning, our learning paths can be of great help. There is one for Python, SAS, Weka and Qlikview each and several more under development.Now that you know, what to learn and how to learn, you can shortlist various options available. You should talk to people who have undergone that training / course and gather some reviews. You can also use our training listing page and apply filters to shortlist the trainings available for various tools and techniques. We have more than 300 trainings listed here and arein process of adding more trainings and courses.So, there you go! You should have a way to find out your way through this data science course juggle.Hope you find this framework immensely useful. I have tried to put a framework to the most common query I get from our audience. The idea is to enable you to make the right decision to the extent possible. If you think, you are in a situation which doesnt get addressed by the framework above, please feel free to ask those questions through comments / discussion portal.P.S. These are my views. A lot of these recommendations are based on my experience and what I think is the right choice. As you can expect, some of these questions dont have a right or wrong answer. They are subjective in nature. So, if you have a different opinion about something I have mentioned, please feel free to let me know.",https://www.analyticsvidhya.com/blog/2015/02/choose-data-science-analytics-big-data-training/
A Complete Tutorial on Time Series Modeling in R,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Basics  Time Series Modeling|2. Exploration of Time Series Data in R|3. Introduction to ARMA Time Series Modeling|4. Framework and Application of ARIMA Time Series Modeling|Projects|End Notes,"Stationary Series|Why do I care about stationarity of a time series?|Random Walk|Lets spice up things a bit,|Dickey Fuller Test of Stationarity|Loading the Data Set|Detailed Metrics|Important Inferences|Auto-Regressive Time Series Model|Moving Average Time Series Model|Difference between AR and MA models|Exploiting ACF and PACF plots|Overview of the Framework|Step 1: Visualize the Time Series|Step 2: Stationarize the Series|Step 3: Find Optimal Parameters|Step 4: Build ARIMA Model|Step 5: Make Predictions|Applications of Time Series Model|Where did we start ?|What do you see in the chart shown above?|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Business Analytics Programs in India (2015  16)|10 Machine Learning Algorithms Explained to an Army Soldier|
Tavish Srivastava
|50 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the mostimportant factor which ensures success in a business. Its difficult to keep up with the pace of time. But, technology has developed some powerful methods using which we cansee things ahead of time. Dont worry, I am not talking about Time Machine. Lets be realistic here!Im talking about the methods of prediction & forecasting. One such method, which deals with time based data is Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to makeinformed decision making.Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyzesales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.So, if you arent sure about complete process of time series modeling, this guide would introduce you to various levels of time series modelingand its related techniques.Time toget started!Lets begin from basics. This includes stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity. If these terms are already scaring you, dont worry  they will become clear in a bit and I bet you will start enjoying the subject as I explain it.There are three basic criterion for a series to be classified as stationary series :1. The mean of the series should not be a function of time rather should be a constant. The image below has the lefthand graph satisfying the condition whereas the graph in red has a time dependent mean.2. The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)3. The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the red series.The reason I took up this section first was that until unless your time series is stationary, you cannot build a time series model. In cases where the stationary criterion are violated, the first requisite becomesto stationarize the time series and then try stochastic models to predict this time series. There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.This is themost basic concept of the time series. You might know the concept well. But, I found many people in the industry who interprets random walk as a stationary process. In this section with the help of some mathematics, I will make this concept crystal clear for ever. Lets take an example.Example: Imagine a girl moving randomly on a giant chess board. In this case, next position of the girl is only dependent on the last position.(Source: http://scifun.chem.wisc.edu/WOP/RandomWalk.html )Now imagine, you are sitting in another room and are not able to see the girl. You want to predict the position of the girl with time. How accurate will you be? Of course you will become more and more inaccurate as the position of the girl changes. At t=0 you exactly know where the girl is. Next time, she can only move to 8 squares and hence your probability dips to 1/8 instead of 1 and it keeps on going down. Now lets try to formulate this series :where Er(t) is the error at time point t. This is the randomness the girl brings at every point in time.Now, if we recursively fit in all the Xs, we will finally end up to the following equation :Now, lets try validating our assumptions of stationary series on this random walk formulation:1. Is the Mean constant ?We know that Expectation of any Error will be zero as it is random.Hence we get E[X(t)] = E[X(0)] = Constant.2. Is the Variance constant?Hence, we infer that the random walk is not a stationary process as it has a time variant variance. Also, if we check the covariance, we see that too is dependent on time.We already know that a random walk is a non-stationary process. Let us introduce a new coefficient in the equation to see if we can make the formulation stationary.Introduced coefficient : RhoNow, we will vary the value of Rho to see if we can make the series stationary. Here we will interpret the scatter visually and not do any test to check stationarity.Lets start with a perfectly stationary series with Rho = 0 . Here is the plot for the time series :Increase the value of Rho to 0.5 gives us following graph :You might notice that our cycles have become broader but essentially there does not seem to be a serious violation of stationary assumptions. Lets now take a more extreme case of Rho = 0.9We still see that the X returns back from extreme values to zero after some intervals. This series also is not violating non-stationarity significantly. Now, lets take a look at the random walk with rho = 1.This obviously is an violation to stationary conditions. What makes rho = 1 a special case which comes out badly in stationary test? We will find the mathematical reason to this.Lets take expectation on each side of the equation X(t) = Rho * X(t-1) + Er(t)This equation is very insightful. The next X (or at time point t) is being pulled down to Rho * Last value of X.For instance, if X(t  1 ) = 1, E[X(t)] = 0.5 ( for Rho = 0.5) . Now, if X moves to any direction from zero, it is pulled back to zero in next step. The only component which can drive it even further is the error term. Error term is equally probable to go in either direction. What happens when the Rho becomes 1? No force can pull the X down in the next step.What you just learnt in the last section is formally known as Dickey Fuller test. Here is a small tweak which is made for our equation to convert it to a Dickey Fuller test:We have to test if Rho  1 is significantly different than zero or not. If the null hypothesis getsrejected, well get a stationary time series.Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.Lets nowconsider an example to show you what a time series looks like.Here well learn to handle time series data onR. Our scopewill be restricted to data exploring in a time series type of data set and not go to building time series models.I have used an inbuilt data set of R called AirPassengers. The dataset consists ofmonthly totals of international airline passengers, 1949 to 1960.Following is the code which will help you load the data set and spill out a few top level metrics.Here are a few more operations you can do:Exploring data becomes most important in a time series model  without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.Lets nowtake up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.ARMA models are commonly used in time series modeling.In ARMA model, AR stands forauto-regression and MA stands formoving average. If these words sound intimidating to you, worry not  Illsimplify these concepts in next few minutes for you!We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose fromthe available time series models.First, Ill explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.Lets understanding AR models using the casebelow:The current GDP of a country say x(t) is dependent on the last years GDP i.e. x(t  1). The hypothesis being thatthe total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.Hence, we can formally write the equation of GDP as:x(t) = alpha * x(t  1) + error (t)This equation is known as AR(1) formulation.The numeral one (1) denotes that the next instance is solely dependent on the previous instance. The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.For instance, lets say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day,the temperature roseand the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing thatthe people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during thecold days. In following days, theproportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explainsthe inertia property of AR series:Lets take another caseto understand Moving average time series model.A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood atzero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market.Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so highthat the bag ran out of stock. As a result, some 100 odd customers couldntpurchase this bag. Lets call this gap as the error at that time point. With time,the bag hadlost itswoo factor. But still few customers were leftwho wentempty handed the previous day. Following is a simple formulation to depict the scenario :x(t) = beta* error(t-1) + error (t)If we try plotting this graph, it will look something like this :Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference getsexploited irrespective ofhaving the AR model or MA model. The correlation plot can give us the order of MA model.Once we have got thestationary time series, we must answertwo primary questions:Q1. Is it an AR or MA process?Q2. What order of AR or MA process do we need to use?The trick to solve these questions is availablein the previous section. Didnt you notice?The first question can be answered usingTotal Correlation Chart (also known as Auto  correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now lets reflect on what we have learnt above.In a moving average series of lag n, we will not get any correlation between x(t) and x(t  n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series, if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag(x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :              ACF                                   PACFThe blue line aboveshows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.                   ACF                                PACFClearly, the graph abovehas a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots.Now, Ill introduce you toa comprehensive framework to build a time series model. In addition, well also discuss about the practicalapplications of time series modelling.A quick revision, Till here weve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.This framework(shown below)specifies the step by step approach onHow to do a Time Series Analysis:As you would be aware, the first three stepshave already been discussed above. Nevertheless, the same has been delineated briefly below:It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.Once we know the patterns, trends, cycles and seasonality , we cancheck if the series is stationary or not. Dickey  Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesnt ends here!What if the series is found to be non-stationary?There are three commonly used technique to make a time series stationary:1. Detrending : Here, we simply remove the trend component from the time series. For instance, the equation ofmy time series is:x(t) = (mean + trend * t) + errorWell simply remove the part in the parentheses and build model for the rest.2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,x(t)  x(t-1) = ARMA (p , q)This differencing is called as the Integration part in AR(I)MA. Now, we have three parametersp : ARd : Iq : MA3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly.More on this has been discussed in the applications part below.The parameters p,d,q can be found using ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to d.With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should beour choice. We can also try some models with a seasonal component. Just in case, we noticeany seasonality in ACF/PACF plots.Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.Now, well use the same example that we have used above. Then,using time series, well make future predictions. We recommend you to check out the example before proceeding further.Following is the plot of the number of passengers with years. Try and make observations on this plotbefore moving further in the article.Here are my observations :1. There is a trend component which grows the passenger year by year.2. There looks to be a seasonal component which has a cycle less than 12 months.3. The variance in the data keeps on increasing with time.We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, lets test the resultant series.Augmented Dickey-Fuller TestWe see that the series is stationary enough to do any kind of time series modelling.Next step is to find the right parameters to be used in theARIMA model. We already know that the d component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :#ACF PlotsClearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Lets see how ACF and PACF curve come out after regressing on the difference.Clearly, ACF plot cuts off after the first lag. Hence, we understoodthat value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.Lets fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Withthis, we come to this end oftutorial on Time Series Modelling. I hope this will help you to improve your knowledgeto work on time based data. To reap maximum benefits out of this tutorial, Id suggest you to practice these R codes side by side and check your progress.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/02/exploration-time-series-data-r/
"MongoDB World 2015, New York, USA, June 1st & 2nd, 2015",Learn everything about Analytics,"Introduction|Why should you attend?|Who is organizing this event?|Contact|Share this:|Like this:|Related Articles|Exploration of Time Series Data in R|7 Steps of Data Exploration & Preparation  Part 2|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In isolation, data hardly makes any sense. Data makes sense when it is embraced by some useful tools & techniques to generate valuable insights. Wouldnt it be great if youcan build your own transformative application for analyzing data and you no longer have to adhere to the commands & limitations of the commercially available softwares? Sounds interesting! Right?Imagine an applicationwhose potentialvaries from computing the data received from artificial satellites to making sense from micro-atomic data. Its time to expand your imagination. All this is going to be possible at MongoDB World 2015.1. You will be able to enhance your knowledge quotient by understanding the latest technology of MongoDB 3.0 & WiredTiger.2. You will be able to discover the fantastic ecosystem of MongoDB while fulfilling your desire of building the critical solutions using MongoDBs network of technology.3. Fun meetups with community experts & professionals. Make acquaintance with like-minded people. Ask your questions that have been baffling you over time. Needless to say, it is an amazing opportunity to grow your network.MongoDB is organizing this event.MongoDB is the only database that harnesses the innovations of NoSQL (flexibility, scalability, performance) and builds on the foundation of relational databases (expressive query language, secondary indexes, strong consistency).For more information on this event, you can visit here.",https://www.analyticsvidhya.com/blog/2015/02/mongodb-world-2015-york-usa-june-1st-2nd-2015/
A Comprehensive Guide to Data Exploration,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Steps of Data Exploration and Preparation|2. Missing Value Treatment|3. Techniques of Outlier Detection and Treatment|4. The Art of Feature Engineering|End Notes,"Lets get started.|Variable Identification|Univariate Analysis|Bi-variateAnalysis|Why missing values treatment is required?|Why my data has missing values?|Which are the methods to treat missing values ?|What is an Outlier?|What are the types of Outliers?||What causes Outliers?|What is the impact of Outliers on a dataset?||How to detect Outliers?||How to remove Outliers?|What is Feature Engineering?|What is the process of Feature Engineering ?|What is Variable Transformation?|When should we use Variable Transformation?|What are the common methods of Variable Transformation?|What is Feature /Variable Creation & its Benefits?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|20 Powerful Images which perfectly captures the growth of Data Science|The Ultimate Plan to Become a Data Scientist in 2016|
Sunil Ray
|102 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","For further read, here is a list of transformation / creation ideas which can be applied to your data.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are no shortcuts for data exploration. If you are in a state of mind, that machine learning can sail you away from every data storm, trust me, it wont. After some point of time, youll realize that you are strugglingat improving models accuracy. In such situation, data exploration techniques will come to your rescue.I can confidently say this, because Ive been through such situations, a lot.I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.Ive created this tutorial to help you understand the underlying techniques of data exploration. As always, Ive tried my best to explain these concepts in the simplest manner. For better understanding, Ive taken up few examples to demonstrate the complicated concepts.Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot oftime and efforts here. Withmy personal estimate, data exploration, cleaning and preparation cantakeup to 70% of your total project time.Below are the steps involved to understand, clean and prepare your data for building your predictive model:Finally, we will need to iterate over steps 4  7 multiple times before we come up with our refined model.Lets now study each stage in detail:-First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.Lets understand this step more clearly by taking an example.Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.Below, the variables have been defined in different category:At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable typeis categorical orcontinuous.Lets look at these methods and statistical measures for categorical and continuous variables individually:Continuous Variables:-In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:Note:Univariate analysisis also used to highlight missing and outlier values. In theupcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer coursedescriptive statistics from Udacity.Categorical Variables:- For categorical variables, well use frequency table to understand distribution ofeach category. We can also read as percentage of valuesunder each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.Bi-variate Analysis finds out the relationship between two variables. Here, we look forassociation and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.Lets understand the possible combinations in detail:Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way tofind out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.Scatter plot shows the relationship between two variable but does not indicatesthe strength of relationship amongst them. To find the strength of the relationship, we useCorrelation. Correlation varies between -1 and +1.Correlation can be derived using following formula:Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))Various tools have function or functionality to identify correlation between variables. In Excel, function CORREL() is used to return the correlation between two variables and SAS uses procedure PROC CORR to identify the correlation. These function returns Pearson Correlation value to identify the relationship between two variables:In above example, we have goodpositive relationship(0.65) between two variables X and Y.Categorical & Categorical:To find the relationship between two categorical variables, we can use following methods:Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by:

From previous two-way table, the expected count for product category 1 to be of small size is0.22. It is derived bytaking the row total for Size(9) times the column total for Product category (2) then dividing by the sample size (81). This is procedure is conducted for each cell. Statistical Measures used to analyze the power of relationship are:Different data science language and tools have specific methods to perform chi-square test. In SAS, we can use Chisqas an option with Proc freq to perform this test.Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are smallin number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks.We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.Till here, we have understoodthe first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables.Now, we will look at the methods ofMissing values Treatment.More importantly, we will also look at why missing values occur in our data and why treating them is necessary.Missing data in the training data set can reduce the power / fit of a model or can lead toa biased model because we have not analysedthe behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.Notice the missing values in the image shown above: In the left scenario, we have not treated missing values. The inference from this data set is that the chances of playing cricket by males is higher than females. On the other hand, if you look at the second table, which shows data after treatment of missing values (based on gender), we can see that females have higher chances of playing cricket compared to males.We looked at the importance of treatment of missing values in a dataset. Now, lets identify the reasons for occurrence of these missing values. Theymay occur at two stages:
After dealing with missing values, the next task is to deal with outliers. Often, we tend to neglect outliers while building models. This is a discouraging practice. Outliers tend to make your data skewed and reduces accuracy. Lets learn more about outlier treatment.Outlier is a commonly usedterminology by analysts and data scientists asit needsclose attention else itcan result in wildly wrongestimations. Simply speaking,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.Lets take an example, we do customer profiling and find out that the average annual income of customers is $0.8 million. But, there are two customers having annual income of $4 and $4.2 million. These two customers annual income is much higher thanrest of the population. These two observations will be seen asOutliers.Outlier can be of two types:Univariate andMultivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.Let us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look atthe box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two valuesbelowand one above the average in a specific segment of weight and height.Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:Lets understand various types of outliers in more detail:Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:To understand the impact deeply, lets take an example to checkwhat happens to a data set with and without outliersin thedata set.Example:As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 5.45. But with the outlier, average soarsto30. This would change the estimate completely.Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detectoutliers. Some of them are:Most of the waysto dealwith outliers are similar to the methods ofmissing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we willdiscuss thecommon techniques used to deal with outliers:Deleting observations:We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning values:Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allowsto deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.Imputing:Likeimputationof missing values, we can also imputeoutliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.Treat separately:If there are significant number of outliers, we should treat them separatelyin the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and thencombine the output.
Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Lets now proceed to the final stage of data exploration. It is Feature Engineering.Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.For example, lets say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.This exercising of bringing out information from data in known as feature engineering.You perform feature engineering once you have completed the first 5 steps in data exploration Variable Identification,Univariate, Bivariate Analysis,Missing Values ImputationandOutliers Treatment. Feature engineering itself can be divided in 2 steps:These two techniques are vital in data exploration andhavea remarkableimpact on the power of prediction.Letsunderstand each of this step in more details.In data modelling, transformation refers tothe replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.Lets look at the situations when variable transformation is useful.Below are the situations where variable transformation is arequisite:There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Lets look at these methods in detailbyhighlightingthe pros and cons of these transformation methods.Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, wehave date(dd-mm-yy) as an input variable in adata set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:There are various techniques to create new features. Lets look at the some of the commonly used methods:As mentioned in the beginning, quality and efforts invested in data exploration differentiates a good model from a bad model.This ends our guideon data exploration and preparation. In this comprehensive guide, we looked at the seven steps of data exploration in detail. The aim of this series was to provide an in depth and step by step guide to an extremely important process in data science.Personally, I enjoyed writing this guideand would love to learn from your feedback. Didyou findthis guideuseful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2015/02/7-steps-data-exploration-preparation-building-model-part-2/
How to get the most out of Massive Open Online Courses (MOOCs)?,Learn everything about Analytics,"|How is a Fitbit tracker related to Massive Open Online Courses (MOOCs)? |1. Decide what is there for you in that course?|2. Create an eco-system  find mentors and buddies|3. Participate in discussions|4. Blog aboutyour learning by applying them to various other problems|5. Participate in projects, assignments and competitions|6. Stay on the schedule|7. Decide the minimum benchmark, you will achieve irrespective|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|7 Steps of Data Exploration & Preparation  Part 2|Step by Step guide to learn Time Series Modeling|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",I bought my first fitbit recently and I am loving it! ,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"While tracking my activities is obviously good, what makes it a great product is the eco-system it provides. Everyday, I am competing with my friends to finish those extra steps and shed those extra kilos! We all challenge each other and buck up each other (sometimes in not so polite way as well) to make the group a fitter group!Well, staying fit without an eco-system and learning new tools by yourself have a few common challenges:So, in this post, I am sharingsome bestpractices, I have learnt the hard way. I have undergone more than 50 open courses on Data Science across various platforms (including Coursera, edX, Udacity, SAS and several others) in last 18 months or so. I still sign up for most of the new courses which are floated.One of the things, people get wrong is that they assume that entire course is for everyone to be undergone in same way. This is certainly not true. It is true that the creators of the courses try hard to make the courses as generic for the larger audience, but you still need to decide and define what you are looking for in each of these courses.For example a course on machine learning would typically start by whetting your appetite about machine learning. If I was undergoing that course, I dont need that section. So Ill typically skip that section (P.S. It may be a good idea to do the exercises  more on this later).Before you start the course, you should have following things defined:This is where I think Fitbit does a fab job! You need to almost replicate it for MOOCs. Here is how I typically find them.Mentors: Typically people in your network on Linkedin, experts in the fields, bloggers on the subject. Just reach out to them and ask for help as a mentor. I usually define the expectations from them up front as well  how much time I expect from them? what kind of questions / projects I would be working upon? I also try and offer them some thing in return  it could be research / tutorial for the blogger, a presentation for the expert presenting in a conference  you get the idea!Buddies: Best is to reach out to people who are asking questions similar to the ones you have. Again, key is to interact with them before finalizing. You would want to have some one who is determined to finish the course, participating in projects and trying out things outside the templates in the MOOC. A good buddy can make or break your learnings from a MOOC. Once found, you should spend time discussing your understanding and doubts with these buddies.Another practice, which can be of help in some places is to join meet-ups and find people with similar interest.Discussions are probably one of the best way to learn while experimenting. There is a reason why every coder in this world spends time on stackoverflow to get his answers. We run a similar discussion portal for data science professionals here. All the MOOCs will typically have their own discussion portals as well. If they are active, you should fire your questions there as well.While on the disucssion portal, offer what ever help you can to fellow participants.Another way to showcase your learning, is to blog about them. This is how I started my journey! Once you have learnt about a particular technique  apply it to another dataset and come up with a solution. Then see what other people have done. iPython notebooks and Githubcan help immensely here. You just need a functional blog, where you can note what you are doing, what you are doing and how you are doing.Projects and assignments are lifeline of all the data science MOOCs. If you miss on them, you miss on a lot of learning. Again, different platforms have different approach. Udacity, particularly is more project focused than others. So, make it a point to do all the assignments diligently  even if you think they are too easy / basic or they are too tough. Just have a go at it.If you dont add few numbers of an array or a list thinking that it is too easy, you will struggle with the data cleaning later in a project. If you can afford, you should also participate in a few competitions outside of the MOOC using the same tools you have learnt. So, if you have learn Logistic regression, apply that on Kaggle Titanic survivor competition.There are 2 commitments you need to make in order to make sure you dont drop out during the process. The first of them is to stay on the schedule. Finish the content and assignments on every week with in the week itself. Dont play the catch up game, otherwise you will fall out the next week.This might sound easier that what is actually is. You will need to put in a lot of efforts to make sure that youare on the top of the schedule.The second commitment you should make to yourself is the minimum score you would achieve. This is a personal choice and should be in sync with the need of the course. If this is an important course, you should at least commit 70% score.I hope this post would help numerous people who struggle to complete open courses. They are a fabulous platform to learn new skills. If you have any other tips and tricks to help MOOC learning, please feel free to shareOn the lighter side, if you use a Fitbit to track your fitness  lets compete!",https://www.analyticsvidhya.com/blog/2015/02/massive-open-online-courses-moocs/
A Complete Tutorial on Time Series Modeling in R,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Basics  Time Series Modeling|2. Exploration of Time Series Data in R|3. Introduction to ARMA Time Series Modeling|4. Framework and Application of ARIMA Time Series Modeling|Projects|End Notes,"Stationary Series|Why do I care about stationarity of a time series?|Random Walk|Lets spice up things a bit,|Dickey Fuller Test of Stationarity|Loading the Data Set|Detailed Metrics|Important Inferences|Auto-Regressive Time Series Model|Moving Average Time Series Model|Difference between AR and MA models|Exploiting ACF and PACF plots|Overview of the Framework|Step 1: Visualize the Time Series|Step 2: Stationarize the Series|Step 3: Find Optimal Parameters|Step 4: Build ARIMA Model|Step 5: Make Predictions|Applications of Time Series Model|Where did we start ?|What do you see in the chart shown above?|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Business Analytics Programs in India (2015  16)|10 Machine Learning Algorithms Explained to an Army Soldier|
Tavish Srivastava
|50 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time is the mostimportant factor which ensures success in a business. Its difficult to keep up with the pace of time. But, technology has developed some powerful methods using which we cansee things ahead of time. Dont worry, I am not talking about Time Machine. Lets be realistic here!Im talking about the methods of prediction & forecasting. One such method, which deals with time based data is Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to makeinformed decision making.Time series models are very useful models when you have serially correlated data. Most of business houses work on time series data to analyzesales number for the next year, website traffic, competition position and much more. However, it is also one of the areas, which many analysts do not understand.So, if you arent sure about complete process of time series modeling, this guide would introduce you to various levels of time series modelingand its related techniques.Time toget started!Lets begin from basics. This includes stationary series, random walks , Rho Coefficient, Dickey Fuller Test of Stationarity. If these terms are already scaring you, dont worry  they will become clear in a bit and I bet you will start enjoying the subject as I explain it.There are three basic criterion for a series to be classified as stationary series :1. The mean of the series should not be a function of time rather should be a constant. The image below has the lefthand graph satisfying the condition whereas the graph in red has a time dependent mean.2. The variance of the series should not a be a function of time. This property is known as homoscedasticity. Following graph depicts what is and what is not a stationary series. (Notice the varying spread of distribution in the right hand graph)3. The covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the red series.The reason I took up this section first was that until unless your time series is stationary, you cannot build a time series model. In cases where the stationary criterion are violated, the first requisite becomesto stationarize the time series and then try stochastic models to predict this time series. There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.This is themost basic concept of the time series. You might know the concept well. But, I found many people in the industry who interprets random walk as a stationary process. In this section with the help of some mathematics, I will make this concept crystal clear for ever. Lets take an example.Example: Imagine a girl moving randomly on a giant chess board. In this case, next position of the girl is only dependent on the last position.(Source: http://scifun.chem.wisc.edu/WOP/RandomWalk.html )Now imagine, you are sitting in another room and are not able to see the girl. You want to predict the position of the girl with time. How accurate will you be? Of course you will become more and more inaccurate as the position of the girl changes. At t=0 you exactly know where the girl is. Next time, she can only move to 8 squares and hence your probability dips to 1/8 instead of 1 and it keeps on going down. Now lets try to formulate this series :where Er(t) is the error at time point t. This is the randomness the girl brings at every point in time.Now, if we recursively fit in all the Xs, we will finally end up to the following equation :Now, lets try validating our assumptions of stationary series on this random walk formulation:1. Is the Mean constant ?We know that Expectation of any Error will be zero as it is random.Hence we get E[X(t)] = E[X(0)] = Constant.2. Is the Variance constant?Hence, we infer that the random walk is not a stationary process as it has a time variant variance. Also, if we check the covariance, we see that too is dependent on time.We already know that a random walk is a non-stationary process. Let us introduce a new coefficient in the equation to see if we can make the formulation stationary.Introduced coefficient : RhoNow, we will vary the value of Rho to see if we can make the series stationary. Here we will interpret the scatter visually and not do any test to check stationarity.Lets start with a perfectly stationary series with Rho = 0 . Here is the plot for the time series :Increase the value of Rho to 0.5 gives us following graph :You might notice that our cycles have become broader but essentially there does not seem to be a serious violation of stationary assumptions. Lets now take a more extreme case of Rho = 0.9We still see that the X returns back from extreme values to zero after some intervals. This series also is not violating non-stationarity significantly. Now, lets take a look at the random walk with rho = 1.This obviously is an violation to stationary conditions. What makes rho = 1 a special case which comes out badly in stationary test? We will find the mathematical reason to this.Lets take expectation on each side of the equation X(t) = Rho * X(t-1) + Er(t)This equation is very insightful. The next X (or at time point t) is being pulled down to Rho * Last value of X.For instance, if X(t  1 ) = 1, E[X(t)] = 0.5 ( for Rho = 0.5) . Now, if X moves to any direction from zero, it is pulled back to zero in next step. The only component which can drive it even further is the error term. Error term is equally probable to go in either direction. What happens when the Rho becomes 1? No force can pull the X down in the next step.What you just learnt in the last section is formally known as Dickey Fuller test. Here is a small tweak which is made for our equation to convert it to a Dickey Fuller test:We have to test if Rho  1 is significantly different than zero or not. If the null hypothesis getsrejected, well get a stationary time series.Stationary testing and converting a series into a stationary series are the most critical processes in a time series modelling. You need to memorize each and every detail of this concept to move on to the next step of time series modelling.Lets nowconsider an example to show you what a time series looks like.Here well learn to handle time series data onR. Our scopewill be restricted to data exploring in a time series type of data set and not go to building time series models.I have used an inbuilt data set of R called AirPassengers. The dataset consists ofmonthly totals of international airline passengers, 1949 to 1960.Following is the code which will help you load the data set and spill out a few top level metrics.Here are a few more operations you can do:Exploring data becomes most important in a time series model  without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.Lets nowtake up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.ARMA models are commonly used in time series modeling.In ARMA model, AR stands forauto-regression and MA stands formoving average. If these words sound intimidating to you, worry not  Illsimplify these concepts in next few minutes for you!We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose fromthe available time series models.First, Ill explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.Lets understanding AR models using the casebelow:The current GDP of a country say x(t) is dependent on the last years GDP i.e. x(t  1). The hypothesis being thatthe total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.Hence, we can formally write the equation of GDP as:x(t) = alpha * x(t  1) + error (t)This equation is known as AR(1) formulation.The numeral one (1) denotes that the next instance is solely dependent on the previous instance. The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.For instance, lets say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day,the temperature roseand the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing thatthe people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during thecold days. In following days, theproportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explainsthe inertia property of AR series:Lets take another caseto understand Moving average time series model.A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood atzero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market.Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so highthat the bag ran out of stock. As a result, some 100 odd customers couldntpurchase this bag. Lets call this gap as the error at that time point. With time,the bag hadlost itswoo factor. But still few customers were leftwho wentempty handed the previous day. Following is a simple formulation to depict the scenario :x(t) = beta* error(t-1) + error (t)If we try plotting this graph, it will look something like this :Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference getsexploited irrespective ofhaving the AR model or MA model. The correlation plot can give us the order of MA model.Once we have got thestationary time series, we must answertwo primary questions:Q1. Is it an AR or MA process?Q2. What order of AR or MA process do we need to use?The trick to solve these questions is availablein the previous section. Didnt you notice?The first question can be answered usingTotal Correlation Chart (also known as Auto  correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now lets reflect on what we have learnt above.In a moving average series of lag n, we will not get any correlation between x(t) and x(t  n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series, if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag(x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :              ACF                                   PACFThe blue line aboveshows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.                   ACF                                PACFClearly, the graph abovehas a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots.Now, Ill introduce you toa comprehensive framework to build a time series model. In addition, well also discuss about the practicalapplications of time series modelling.A quick revision, Till here weve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.This framework(shown below)specifies the step by step approach onHow to do a Time Series Analysis:As you would be aware, the first three stepshave already been discussed above. Nevertheless, the same has been delineated briefly below:It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.Once we know the patterns, trends, cycles and seasonality , we cancheck if the series is stationary or not. Dickey  Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesnt ends here!What if the series is found to be non-stationary?There are three commonly used technique to make a time series stationary:1. Detrending : Here, we simply remove the trend component from the time series. For instance, the equation ofmy time series is:x(t) = (mean + trend * t) + errorWell simply remove the part in the parentheses and build model for the rest.2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,x(t)  x(t-1) = ARMA (p , q)This differencing is called as the Integration part in AR(I)MA. Now, we have three parametersp : ARd : Iq : MA3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly.More on this has been discussed in the applications part below.The parameters p,d,q can be found using ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to d.With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should beour choice. We can also try some models with a seasonal component. Just in case, we noticeany seasonality in ACF/PACF plots.Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.Now, well use the same example that we have used above. Then,using time series, well make future predictions. We recommend you to check out the example before proceeding further.Following is the plot of the number of passengers with years. Try and make observations on this plotbefore moving further in the article.Here are my observations :1. There is a trend component which grows the passenger year by year.2. There looks to be a seasonal component which has a cycle less than 12 months.3. The variance in the data keeps on increasing with time.We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, lets test the resultant series.Augmented Dickey-Fuller TestWe see that the series is stationary enough to do any kind of time series modelling.Next step is to find the right parameters to be used in theARIMA model. We already know that the d component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :#ACF PlotsClearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Lets see how ACF and PACF curve come out after regressing on the difference.Clearly, ACF plot cuts off after the first lag. Hence, we understoodthat value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.Lets fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Withthis, we come to this end oftutorial on Time Series Modelling. I hope this will help you to improve your knowledgeto work on time based data. To reap maximum benefits out of this tutorial, Id suggest you to practice these R codes side by side and check your progress.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/02/step-step-guide-learn-time-series/
"Data Science Conclave  2015, Chennai, India, 20th  21st February 2015",Learn everything about Analytics,"Analytics Vidhya Advantage|Why should you attend?|Who all will be there?|Contact|Share this:|Like this:|Related Articles|Step by Step guide to learn Time Series Modeling|Gartner Business Intelligence and Analytics Summit, Las Vegas, NV, USA, Mar 30th  1st April 2015|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A decade back, organizations across the world have struggled while dealing with enormous amountof data. Worst part was, Data Analysts used to find themselves in a miserable situation when it came to presenting the data. Thanks to the progressing technology, a big pat on the back of data science for making our lives simpler. Data Science has enabled businesses to grow and develop like never before. The rapidity in the businesses could be invited simply bymaking the use of their sales, customer, workforce, finance, IT data in an interesting and productive manner.All readers of Analytics Vidhya can avail a 10% discount on their tickets by using the code AV. Click on the linkto register.1. You will be able to understand the importance of data science and its imperative role in your business whileovercoming the business challenges.2. You will be able to achieve your business goals painlessly by understanding the pragmatic impact of data science, big data & advanced analytics.3.Identification of the unique needs of your business will get easier by getting tailored recommendations based on the analysis so performed.4. The pedagogy of this event will be case study based. Various case studies pertaining to success, growth & failures in analytics will be discussed in order to give more of a real life view of the impact of data science.5. You will get the exclusive opportunity to connect with subject matter experts, industry stalwarts and thought leaders.Eminent speaker from organizations such as Oracle, Target, SAP will feature in this event. In addition, alma mater of IIT Madras will also be there to demonstrate their persona of intellect.For more details, click here.",https://www.analyticsvidhya.com/blog/2015/02/data-science-conclave-2015-chennaiindia-20th-21st-february-2015/
"Gartner Business Intelligence and Analytics Summit, Las Vegas, NV, USA, Mar 30th  1st April 2015",Learn everything about Analytics,"Why should you attend?|Who all will be there?|Contact|Share this:|Like this:|Related Articles|Data Science Conclave  2015, Chennai, India, 20th  21st February 2015|Predictive Analytics World, San Francisco, CA, USA, March 29th  April 2nd, 2015|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Gaining analytics advantage has now become one of the top priority of organization these days. Professionals in analytics industry arekeen to upgrade their sets of skills to deliver the requiredadvantage of analytics. Tappingthe new tools, techniques, digital management & governance strategies allows companies to improve their operational & workforce efficiency.1. You will be able to develop customized Business Intelligence skills necessary to gain success in the analytics domain.2. You will be able to understand the holistic impact of advanced analytics on business.3. You will get the opportunity of networking with leading industry experts & professionals.4. If you are keen to grow your advanced analytics skill set, this event is just for you.There will be a massive gathering of leading industry experts. Below are the few delegates:-The Guest Keynote at this event will be delivered by is a globally well-renowned persona. Hes a producer, director and screenwriter. Take a Guess!Are you keen to know more about this incredibly amazing event? Have questions in your mind. No worries! Click here, andget more information on this event.",https://www.analyticsvidhya.com/blog/2015/02/gartner-business-intelligence-analytics-summit-las-vegas-nv-usa-mar-30-april/
"Predictive Analytics World, San Francisco, CA, USA, March 29th  April 2nd, 2015",Learn everything about Analytics,"Whats in it for you?||Who all will be there?|Contact|Share this:|Like this:|Related Articles|Gartner Business Intelligence and Analytics Summit, Las Vegas, NV, USA, Mar 30th  1st April 2015|Chief Data Strategy Forum, Boston, MA, USA, March 30th  31st, 2015|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever tried to know how much sales are you expecting next month? How much revenue is following up? Ofcourse! Using various available prediction softwares & techniques, your team can accomplish this task. But, have you ever thought of boosting the prediction accuracy by redesigning your prediction models with high precision and low error? If yes! this is the event you must look forward to in this year.Internationally recognized industry stalwarts will feature in this event. Below are a few of them:-Are you keen to know more about this incredibly amazing event? Have questions in your mind. No worries! Solution just follows. Justvisit here, andget more information on this event.",https://www.analyticsvidhya.com/blog/2015/02/predictive-analytics-world-san-francisco-ca-usa-march-29-april-2/
"Chief Data Strategy Forum, Boston, MA, USA, March 30th  31st, 2015",Learn everything about Analytics,"Why should you attend?|Who are the Expert Speakers?|Contact:|Share this:|Like this:|Related Articles|Predictive Analytics World, San Francisco, CA, USA, March 29th  April 2nd, 2015|7 Steps of Data Exploration & Preparation  Part 1|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data is considered to be a strategic asset for an organization. The significance of data in world of business has gone up over the last decade. Chief Data Officer, the newly emerged professional, is considered to know the spells which can easily unravel the invaluable insights made from data. Chief Data Strategy Forum 2015 will showcase all the leading data experts from well known brands, sharing their perspectives on the future of data centric business intelligence and its implications on improved decision making.Below are the speakers featuring exclusively at this event:Are you keen to know more about this incredibly amazing event? Have questions in your mind. No worries! the solution is right here. Justvisithere, andget more information on this event.",https://www.analyticsvidhya.com/blog/2015/02/chief-data-strategy-forum-boston-ma-usa-march-30-31/
A Comprehensive Guide to Data Exploration,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Steps of Data Exploration and Preparation|2. Missing Value Treatment|3. Techniques of Outlier Detection and Treatment|4. The Art of Feature Engineering|End Notes,"Lets get started.|Variable Identification|Univariate Analysis|Bi-variateAnalysis|Why missing values treatment is required?|Why my data has missing values?|Which are the methods to treat missing values ?|What is an Outlier?|What are the types of Outliers?||What causes Outliers?|What is the impact of Outliers on a dataset?||How to detect Outliers?||How to remove Outliers?|What is Feature Engineering?|What is the process of Feature Engineering ?|What is Variable Transformation?|When should we use Variable Transformation?|What are the common methods of Variable Transformation?|What is Feature /Variable Creation & its Benefits?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|20 Powerful Images which perfectly captures the growth of Data Science|The Ultimate Plan to Become a Data Scientist in 2016|
Sunil Ray
|102 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","For further read, here is a list of transformation / creation ideas which can be applied to your data.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are no shortcuts for data exploration. If you are in a state of mind, that machine learning can sail you away from every data storm, trust me, it wont. After some point of time, youll realize that you are strugglingat improving models accuracy. In such situation, data exploration techniques will come to your rescue.I can confidently say this, because Ive been through such situations, a lot.I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.Ive created this tutorial to help you understand the underlying techniques of data exploration. As always, Ive tried my best to explain these concepts in the simplest manner. For better understanding, Ive taken up few examples to demonstrate the complicated concepts.Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot oftime and efforts here. Withmy personal estimate, data exploration, cleaning and preparation cantakeup to 70% of your total project time.Below are the steps involved to understand, clean and prepare your data for building your predictive model:Finally, we will need to iterate over steps 4  7 multiple times before we come up with our refined model.Lets now study each stage in detail:-First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.Lets understand this step more clearly by taking an example.Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.Below, the variables have been defined in different category:At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable typeis categorical orcontinuous.Lets look at these methods and statistical measures for categorical and continuous variables individually:Continuous Variables:-In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:Note:Univariate analysisis also used to highlight missing and outlier values. In theupcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer coursedescriptive statistics from Udacity.Categorical Variables:- For categorical variables, well use frequency table to understand distribution ofeach category. We can also read as percentage of valuesunder each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.Bi-variate Analysis finds out the relationship between two variables. Here, we look forassociation and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.Lets understand the possible combinations in detail:Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way tofind out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.Scatter plot shows the relationship between two variable but does not indicatesthe strength of relationship amongst them. To find the strength of the relationship, we useCorrelation. Correlation varies between -1 and +1.Correlation can be derived using following formula:Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))Various tools have function or functionality to identify correlation between variables. In Excel, function CORREL() is used to return the correlation between two variables and SAS uses procedure PROC CORR to identify the correlation. These function returns Pearson Correlation value to identify the relationship between two variables:In above example, we have goodpositive relationship(0.65) between two variables X and Y.Categorical & Categorical:To find the relationship between two categorical variables, we can use following methods:Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by:

From previous two-way table, the expected count for product category 1 to be of small size is0.22. It is derived bytaking the row total for Size(9) times the column total for Product category (2) then dividing by the sample size (81). This is procedure is conducted for each cell. Statistical Measures used to analyze the power of relationship are:Different data science language and tools have specific methods to perform chi-square test. In SAS, we can use Chisqas an option with Proc freq to perform this test.Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are smallin number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks.We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.Till here, we have understoodthe first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables.Now, we will look at the methods ofMissing values Treatment.More importantly, we will also look at why missing values occur in our data and why treating them is necessary.Missing data in the training data set can reduce the power / fit of a model or can lead toa biased model because we have not analysedthe behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.Notice the missing values in the image shown above: In the left scenario, we have not treated missing values. The inference from this data set is that the chances of playing cricket by males is higher than females. On the other hand, if you look at the second table, which shows data after treatment of missing values (based on gender), we can see that females have higher chances of playing cricket compared to males.We looked at the importance of treatment of missing values in a dataset. Now, lets identify the reasons for occurrence of these missing values. Theymay occur at two stages:
After dealing with missing values, the next task is to deal with outliers. Often, we tend to neglect outliers while building models. This is a discouraging practice. Outliers tend to make your data skewed and reduces accuracy. Lets learn more about outlier treatment.Outlier is a commonly usedterminology by analysts and data scientists asit needsclose attention else itcan result in wildly wrongestimations. Simply speaking,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.Lets take an example, we do customer profiling and find out that the average annual income of customers is $0.8 million. But, there are two customers having annual income of $4 and $4.2 million. These two customers annual income is much higher thanrest of the population. These two observations will be seen asOutliers.Outlier can be of two types:Univariate andMultivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.Let us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look atthe box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two valuesbelowand one above the average in a specific segment of weight and height.Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:Lets understand various types of outliers in more detail:Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:To understand the impact deeply, lets take an example to checkwhat happens to a data set with and without outliersin thedata set.Example:As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 5.45. But with the outlier, average soarsto30. This would change the estimate completely.Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detectoutliers. Some of them are:Most of the waysto dealwith outliers are similar to the methods ofmissing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we willdiscuss thecommon techniques used to deal with outliers:Deleting observations:We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning values:Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allowsto deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.Imputing:Likeimputationof missing values, we can also imputeoutliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.Treat separately:If there are significant number of outliers, we should treat them separatelyin the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and thencombine the output.
Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Lets now proceed to the final stage of data exploration. It is Feature Engineering.Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.For example, lets say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.This exercising of bringing out information from data in known as feature engineering.You perform feature engineering once you have completed the first 5 steps in data exploration Variable Identification,Univariate, Bivariate Analysis,Missing Values ImputationandOutliers Treatment. Feature engineering itself can be divided in 2 steps:These two techniques are vital in data exploration andhavea remarkableimpact on the power of prediction.Letsunderstand each of this step in more details.In data modelling, transformation refers tothe replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.Lets look at the situations when variable transformation is useful.Below are the situations where variable transformation is arequisite:There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Lets look at these methods in detailbyhighlightingthe pros and cons of these transformation methods.Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, wehave date(dd-mm-yy) as an input variable in adata set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:There are various techniques to create new features. Lets look at the some of the commonly used methods:As mentioned in the beginning, quality and efforts invested in data exploration differentiates a good model from a bad model.This ends our guideon data exploration and preparation. In this comprehensive guide, we looked at the seven steps of data exploration in detail. The aim of this series was to provide an in depth and step by step guide to an extremely important process in data science.Personally, I enjoyed writing this guideand would love to learn from your feedback. Didyou findthis guideuseful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2015/02/data-exploration-preparation-model/
"Data Visualization  Transforming / Building Traditional Charts using D3.js, New Delhi, India, February 19th, 2015",Learn everything about Analytics,"Introduction|About this Meetup|Contact|Share this:|Like this:|Related Articles|7 Steps of Data Exploration & Preparation  Part 1|R Tutorials and Networking with Data Scientists, New Delhi, India, March 8th 2015|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A very common questions that people ask these days is what are the tricks & ways to handle data visualization in D3.js effectively. This has remained aherculean task for practitioners to present the data in an easily interpreted manner. While dealing with huge volumes of data, analysts sometimes find themselves in the middle of a road when it comes to presenting it. This meetup will focus upon new ways to handle data,therebygenerating interesting insights using D3.js software.D3.js is a JavaScript library for manipulating documents based on data. D3 helps you bring data to life using HTML, SVG, and CSS. D3s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.The pedagogy used in this meetup will be case study based, which will help you in learning from the real life scenarios & happenings.In this meetup, the following case studies will be covered:-1. Transforming traditional bar charts into something more meaningful2. Candle Stick ChartingThis meetup willexplore how d3.js can be used to transform a traditional bar chart into something that produces cognitive ease and displays more information than possible in a traditional bar chart.Incase you have any questions or doubts related to this meetup, you can visit here.",https://www.analyticsvidhya.com/blog/2015/02/data-visualization-transforming-building-traditional-charts-d3-js-delhi-india-february-19th-2015/
"R Tutorials and Networking with Data Scientists, New Delhi, India, March 8th 2015",Learn everything about Analytics,"Introduction|About R Meetup|Contact|Share this:|Like this:|Related Articles|Data Visualization  Transforming / Building Traditional Charts using D3.js, New Delhi, India, February 19th, 2015|Learning path for Weka  GUI based way to learn Machine Learning|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you know how many people are opting to learn R everyday? Do you know how powerful is R as compared to other statistical softwares? Apart from being an open source software, what are the other benefits that you can reap while working with R? If these questions strike a bell in your mind, this is a must attend meet up for you. Find answers to all these stimulating questions. This meetup is dedicated to bringing together all the practitioners in the analytics domain who wish to upgrade their skills & knowledge of R.R is an open source programming language for statistical computing, data analysis, and graphical visualization. R has an estimated one million users worldwide, and its user base is growing. While it is most commonly used within academia, in fields such as computational biology and applied statistics, it is gaining currency in commercial areas such as quantitative finance and business intelligence.The strength of R lies in its ability to accommodate various add-on packages which gives it an edge over other statistical tools used forresearch.Additionally,Rs strengths as a language are its powerfully built-in tools for inferential statistics, its compact modeling syntax, its data visualization capabilities, and its ease of connectivity with persistent data stores (from databases to flatfiles). For all its strengths, though, R has an admittedly steep learning curve; the first steps towards learning and using R can be challenging for some.This event is organized Ajay Ohri.Ajay Ohri is the founder of analytics startup decisionstats.com. He is a well known writer in the analytics industry. He has religiously contributed to the analytics space by writing books such as R for Business Analytics & the latest R for Cloud Computing.Incase you have any questions or doubts related to this meetup, you can visit here.",https://www.analyticsvidhya.com/blog/2015/02/tutorials-networking-data-scientists-delhi-india-march-8th-2015/
Learning path for Weka  GUI based way to learn Machine Learning,Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|R Tutorials and Networking with Data Scientists, New Delhi, India, March 8th 2015|Confluencia15 Management Summit, IIT Roorkee, 21st & 22nd February 2015|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you feel like a lost bird when you started learning Machine learning? Do you learn coding on a language first? Or you focus on understanding the math / logic behind machine learning algorithms? Or you look for ways to handle the large datasets efficiently? Statistics?If you have come from a non-quant background, there is a good chance that you would feel like this:Worry not! The data science community has found ways to induct you in less harsher ways in its ecosystem. You will still need to do the hard work, but you can focus on learning one thing at a time.One of the tools, which helps you learn machine learning through a Graphical User Interface (GUI) is Weka. Weka was a project at University of Waikato and can be a good stepping stone for beginners. Once you are more comfortable with machine learning and various algorithms, you can always switch back to a more evolved language like R or Python.So, here is a learning path on Weka, which was designed by Abhinav Unnam, who interned with us last summer. He himself started his machine learning journey through Weka. If you have any question related to starting machine learning journey through Weka, please feel free to ask them here or on our discussion portal",https://www.analyticsvidhya.com/blog/2015/02/learning-path-weka-gui-based-learn-machine-learning/
"Confluencia15 Management Summit, IIT Roorkee, 21st & 22nd February 2015",Learn everything about Analytics,"Introduction: Department of Management Studies, IIT Roorkee|List of prospective speakers :||Contact:|Share this:|Like this:|Related Articles|Learning path for Weka  GUI based way to learn Machine Learning|SAS Global Forum, The Kay Bailey Hutchison Convention Center, Dallas, TX, Apr 26  29|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Theme: Prism of Possibilities: Big Data and E-commerce in India.|,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Confluencia2015 is being organized by Department of Management Studies, IIT Roorkee. This event will showcase the big icons of indian analytics industry sharing a common platform. Undoubtedly, this is one of the most awaited event for analytics enthusiasts and professionals.The Department of Management Studies was set up in the then University of Roorkee (converted into IIT Roorkee in 2001) in year 1998 with two year full time residential program to cater to the emerging needs for techno-managers in the growing Indian Economy.Brief of Department of Management Studies [DoMS], IIT Roorkee:For more details and participation, feel free to contact the Student Co-ordinators, Confluentia15, IIT Roorkee:08006424038 | 07060334617 | 07060334271",https://www.analyticsvidhya.com/blog/2015/02/confluencia15-management-summit-iit-roorkee-21st-22nd-february-2015/
"SAS Global Forum, The Kay Bailey Hutchison Convention Center, Dallas, TX, Apr 26  29",Learn everything about Analytics,"Why should you attend?|Schedule:|Share this:|Like this:|Related Articles|Confluencia15 Management Summit, IIT Roorkee, 21st & 22nd February 2015|How to avoid Over-fitting using Regularization?|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","Saturday, April 25|Sunday, April 26|Monday, April 27|Tuesday, April 28|Wednesday, April 29",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you use SAS in your Organization, SAS Global Forum is the place to be. A showcase of latest SAS products, case studies from various SAS customers with an aim to enhance its customer relationship.It is a complete package of knowledge offering 4 days of education, activities and energy. Hundreds of presentations, expert-led courses and Hands-On Workshops. Thousands of attendees from all over the world. Countless opportunities to learn, network and share.This is your chance to get your organization in front of SAS professionals from across the globe. From students to experts, every skill level is represented  so you can network with the best and brightest.For more details, you can visit: SAS Global Forum",https://www.analyticsvidhya.com/blog/2015/02/sas-global-forum-kay-bailey-hutchison-convention-center-dallas-tx-apr-26-29/
How to avoid Over-fitting using Regularization?,Learn everything about Analytics,"Among competinghypotheses, the one with the fewest assumptions should be selected. Other, morecomplicated solutions may ultimately prove correct, butin the absence of certaintythe fewer assumptions that are made, the better.|Business Situation:|Methods to avoid Over-fitting:|Regularization basics|Understanding Regularization Mathematically|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|SAS Global Forum, The Kay Bailey Hutchison Convention Center, Dallas, TX, Apr 26  29|Apprentice Leader  Mu Sigma  Bangalore ( 3-10 years of experience)|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Occams Razor, a problem solving principle states thatIn the world of analytics, where we try to fit a curve to every pattern, Over-fitting is one of the biggest concerns. However, in general models are equipped enough to avoid over-fitting, but in general there is a manual intervention required to make sure the model does not consume more than enough attributes.Lets consider an example here, we have 10 students in a classroom. We intend to train a model based on their past score to predict their future score. There are 5 females and 5 males in the class. The average score of females is 60 whereas that of males is 80. The overall average of the class is 70.Now, there are several ways to make the prediction:The first case here is called under fit, the second being an optimum fit and last being an over-fit.Have a look at the following graphs,Image source: pingax.comThe trend in above graphs looks like a quadratic trend over independent variable X. A higher degree polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset.We will briefly touch up on various techniques we use to avoid over-fitting. And then focus on a special technique called Regularization.Following are the commonly used methodologies :A simple linear regression is an equation to estimate y, given a bunch of x. The equation looks something as follows :In the above equation, a1, a2, a3  are thecoefficients and x1, x2, x3 .. are the independent variables. Given a data containing x and y, we estimate a1, a2 , a3 based on an objective function. For a linear regression the objective function is as follows :Now, this optimization might simply overfit the equation if x1 , x2 , x3 (independent variables ) are too many in numbers. Hence we introduce a new penalty term in our objective function to find the estimates of co-efficient. Following is the modification we make to the equation :The new term in the equation is the sum of squares of the coefficients (except the bias term) multiplied by the parameter lambda. Lambda = 0 is a super over-fit scenario and Lambda = Infinity brings down the problem to just single mean estimation. Optimizing Lambda is the task we need to solve looking at the trade-off between the prediction accuracy of training sample and prediction accuracy of the hold out sample.There are multiple ways to find the coefficients for a linear regression model. One of the widely used method is gradient descent. Gradient descent is an iterative method which takes some initial guess on coefficients and then tries to converge such that the objective function is minimized. Hence we work with partial derivatives on the coefficients. Without getting into much details of the derivation, here I will put down the final iteration equation :Here, theta are the estimates of the coefficients. Alpha is the learning parameter which will guide our estimates to convergence. Now lets bring in our cost terms. After taking the derivative of coefficient square, it reduces down to a linear term. Following is the final iteration equation you get after embedding the penalty/cost term.Now if you look carefully to the equation, the starting point of every theta iteration is slightly lesser than the previous value of theta. This is the only difference between the normal gradient descent and the gradient descent regularized. This tries to find converged value of theta which is as low as possible.In this article we got a general understanding of regularization. In reality the concept is much deeper than this. In few of the coming articles we will explain different types of regularization techniques i.e. L1 regularization, L2 regularization etc. Stay Tuned!Did you find the article useful? Have you used regularization to avoid over-fitbefore? Share with us any such experiences. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/
Apprentice Leader  Mu Sigma  Bangalore ( 3-10 years of experience),Learn everything about Analytics,"Interested people can apply for this job can mail their CV to[emailprotected]with subject asApprentice Leaders  Mu Sigma  Bangalore ( 3-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|How to avoid Over-fitting using Regularization?|Geo-Searching & Analytics Using AWS Cloud Search|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Apprentice LeaderLocation  BangaloreAbout employer  Mu SigmaMu Sigma is a category defining Decision Sciences and Big Data Analytics Company helping enterprisesinstitutionalize data-driven decision making.With over 3500 decision scientists and experience across 10 industry verticals, Mu Sigmahas been consistently validated as the preferred Decision Sciences and analytics partner. Mu Sigmaprovides an integrated decision support ecosystem of products, services and cross-industry best practiceprocesses transforming the way decisions are enabled in enterprises for more than 140 Fortune 500clients. For more details, visit: www.mu-sigma.comJob description:ResponsibilitiesQualification and Skills RequiredAdditional Information:-Mu Sigma Special : What will you get in return?",https://www.analyticsvidhya.com/blog/2015/02/apprentice-leaders-mu-sigma-bangalore-3-10-years-experience/
Geo-Searching & Analytics Using AWS Cloud Search,Learn everything about Analytics,"Solution on the cloud|Steps to build a search domain on AWS:|Example Data set:|Geo location search:|Share this:|Like this:|Related Articles|Apprentice Leader  Mu Sigma  Bangalore ( 3-10 years of experience)|Interview with Industry expert  Ajay Ohri, Founder, decisionstats.com|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the common challenge faced by Analytics professionals is geo & radius related questions such as:In all these scenarios, we need to implement the radius search to get us an answer. We can use our favorite analytical package such as R, SAS or even PG SQLs. However, when we move to implement these type of analytics to a live application, which requires fast response, the infrastructure engineering could become tricky.AWS Cloudsearch, a cloud search service from Amazon Web Services, could help in some of these problems. AWS Cloud search is a hosted search platform, which can be used to search large collections of data such as web pages, document files, forum posts, or product information. Search indexing technologies such as Lucene have existed for a long time but AWSs Cloud search relies on the AWS platform, which is used to power Amazons own shopping search engine. This means that the kinks that are required to be ironed out from the dev server to live operations have been worked out.
AWS Cloudsearch indexes and searches both structured data and plain text. Some of the features are:We can get search results in JSON or XML, sort and filter results based on field values, and sort results alphabetically, numerically, or according to custom expressions.We can follow these broad steps to build a search domain in AWS Cloud Search:vHomeInsurance collected detailed data on home insurance & property values across the US and compared it within specific geographic and regional areas. Here is an example of home prices using a heat map showing the house prices across the US. Consumers & analysts need to understand where to live within those pockets based on home prices given the property values, home insurance & other factors.For example, if you want to live in Atlanta, you want to identify the cheapest home insurance in Atlanta and nearby locations. AWS Cloud search helps you do that using geo-location searching. Here is another example data set in California and places close to Los Angeles for home insurance & property values.We index the above table using the cloud search API within the search domain in cloud search. The indexed fields can include home insurance rates, home value, number of homes, city, state, Zipcode, population & lat long details.AWS Cloud Search uses Cosine search for its geo location search. A brief explanation on the cosine search is available below and details can be found here.Law of cosines is more preferable than haversine when calculating distance between two latitude-longitude points.It gives well-conditioned results down to distances as small as around 1 metre. In view of this, it is probably worth, in most situations, using either the simpler law of cosines or the more accurate ellipsoidal Vincenty formula in preference to haversine.Law of cosines:Lat/Lon in degrees:The formula above can be used to find the distance between one geo location to all the document locations in the search domain and return the documents which are in the specified radius.Here is an example search query to find the cheapest areas for home insurance in Los Angeles within a 50 miles radius:treshold=&t-dis=..radiusFor example Los angeles latitude=33.7866,longitude=-118.2987,radius=50(miles). If you pass these values to the above query, it will return all the documents which are less than 50 miles range to Los angeles.The simplicity and consistency of the AWS cloud search can enable you to do Geo analytics on the fly instead of developing a custom infrastructure and an entire team to support and maintain the search infrastructure.Hope this article helps you to solve your needs on geo-location analytics with geo radius questions.-This article has been contributed by vHomeInsurance.com. vHomeInsurance.com (www.vhomeinsurance.com) analyzes home insurance rates, home values and other factors to help home owners make better decisions about their insurance.",https://www.analyticsvidhya.com/blog/2015/02/geo-searching-analytics-aws-cloud-search/
"Interview with Industry expert  Ajay Ohri, Founder, decisionstats.com",Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage|Share this:|Like this:|Related Articles|Geo-Searching & Analytics Using AWS Cloud Search|Better, faster and more helpful Analytics Vidhya is now live!|
Kunal Jain
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently I caught up with Ajay Ohri, founder of decisionstats.com over a cup of coffee. Ajay has been a friend and a mentor to me and a well known data science evangelist in India. We share a lot of common interest, the most obvious one being writing!For the people, who dont know Ajay, here is a brief intro:Ajay Ohri is the founder of analytics startup decisionstats.com. He is a well known writer in the analytics industry. He has religiously contributed to the analytics space by writing books such as R for Business Analytics & the latest R for Cloud Computing.I am also delighted to announce that Ajay has cordially agreed to join Analytics Vidhya as a mentor. We are excited about having Ajay on board and are sure that his experience would help our readers immensely in their data science learning.Here is a brief transcript of the discussion between me & Ajay:Kunal: Congratulations! Ajay on the launch of your second book R for Cloud Computing! I personally have a lot of respect for authors, who religiously devote themselves to their love for writing.First of all, we sincerely thank you for taking time out of your busy schedule for this interview.You come from DCE / IIM fraternity. What prompted you to get into the entrepreneurial boots leaving behind your well established job?Ajay: There is no fraternity nor a sorority for DCE/IIM people. I loved being part of both DCE and IIM Lucknow, and I am grateful for what these institutes have done to help me kickstart my career. I am thankful, I got access to the best coaches in this country to help shape my thinking. Having said that, the real benefit of the so called top institutes is the alumni and the friends network you get.I think I got tired and dissatisfied at the pace of my learning in analytics, the displeasure with which I was forced to indulge in managerial soft skills (a.k.a. politics). I was always wanting to get more , learn more, and to be honest & write more. A corporate job would place enormous restrictions on what I could blog or write upon. I learnt a lot at all the places I worked andam very grateful to everyone I worked with in my corporate stint. Even the bosses who pushed me and made me cry, they taught me the importance of hard work and focus.The final kick into startups was Citis troubles in 2007 and GM going bust in 2008. So you see, it was not all career planning. A lot of events and luck went into it. Things I thought were a lucky break, turned out not to be so good. Things I thought were awful, actually were the best things for my career. I guess a time frame of 5 years is necessary to rationally and impassionately look at events and say whether they were really good or bad, and if so in what magnitude.Having said that, I owe my financial independence since 2007 to my clients who take a bet on my brain, people across the world who took time to mentor me on social media. I dont think I am very intelligent to begin with, but I certainly have been trained by the best.Kunal: You started data science career much before people would have heard about it and it became one of the hottest field around. What were the challenges that you faced during the initial stages of your professional career?Ajay: Cool question man. Yeah it used to be called business analytics, then data analytics and now its data science. What will they call it next?Initial challenges: R was raw (this was 2007) , SAS was expensive, even Open Office was not so good as it is now. Getting a pipeline of work, leads for clients, converting leads to contracts and chasing people to pay me after work done were initial challenges. Initial challenges in blogging were: what do I write, when do I write, I did not know anything at first . I picked up blogging, SEO, SEM, Web Analytics, social media, networked like crazy.Money was a constant challenge. I even went through a divorce because my ex wife did not (rightly) support my crazy move to startups . I have been blessed by God though that my clients always managed to keep the proverbial wolf from my door.Differentiating myself from the competition was a constant challenge. I stuck to my guns, was true and frank as my personality was, constantly turned down boring work, and kept my prices low and honest and my deadlines timely. This is how I met those challenges. Open source and R really helped me here as they also took off at the same time. So I guess I got a bit lucky.Kunal: You have been an avid blogger. What stimulated you to write books?Ajay: Well, Google Adsense basically. It doesnt pay you even peanuts. So, I started selling my own banner ads on my blog. The interviews were then a new feature in 2008 in analytics. Then kind people like John Sall, founder SAS Institute helped me out by answering my emails for interviews. Those breaks helped me build a brand and following.But I never got satisfied in the Build more views business. The trouble with that was analytics was and will remain a rather niche area for reading. How do I spread my views further. Springer stepped in. I sent them a book proposal. They approved it. 2 years later Book1  R for Business Analytics got out. It was ok. A Chinese university got a contract to translate it. So, 2 more years later book 2- R for Cloud Computing.I am 38 now. I still try to learn a new technical thing every day. One new thing every day. I share everything I know because I get more knowledge in return.Kunal: What inspired you to write the second book R for Cloud Computing?Ajay: Writing the second book was to just an excuse to do more research on a geeky and cool subject cloud. I loved the freedom that the cloud represents to people from poor countries and how we can now match even super computers by paying by hour. Also R was RAM constrained so the cloud is a natural fit to R. I also got a lot of traffic to a particular blog post on setting up R on cloud. That made me think there was an audience out there wanting to learn more and learn easily. My book style has always been  This is how we will do this task, Step 1 , Screenshot, Step 2 Screenshot etc peppered with trouble shooting comments.I try to aim for simplicity and thats something which comes only if you have learnt to study and clear engineering exams at the last hour. These books were just diaries of my jugaad. Each book costs two years of my life but it is a life well spent. I love analytics and think of it as both elegant and beautiful.Kunal: What was the hardest thing about writing the second book?Ajay:The technology about the cloud changed so quickly and so often by so many players I had to rewrite it a lot more than the first book. Even now it is changing. Amazon and Google and even IBM keep coming up with new innovations. Microsoft just bought Revolution Analytics totally changing the analytics game at the globally strategic level.Kunal: What is your favorite past time?Ajay:Writing poetry on http://poemsforkush has been my favourite past time and watching movies with my DCE / IIM friends. I loved Birdman and I think Benedict Cumberbatch was awesome in Imitiation Game (as he is in any movie he plays).I firmly support Tyrion Lannister for winning the Game of Thrones and have a huge crush on Claire in House of Cards.Kunal: What has been the turning point of your life?Ajay:My sons birth in 2007 provided me the focus, the urgency and the need to build a digital legacy one that he can enjoy and use when he grows up 15 years from now.Kunal: If you had a chance to go back in time, what are the things you would have done differently?Ajay: I would have learnt R earlier. I would have drunk less beer. I would have exercised more. I would have applied more sunscreen. I would have balanced my emotions better with my logic and my instincts.I definitely would write more poetry and more code and write fewer emails , read more computer science and read less celebrity news. I would also worry less about money and just shut up and do my job more.Kunal: You also run one of the most successful meetup I know about. How did that start and what keeps it ticking?Ajay:I enjoyed Meetups during my time in North America. There were no R meetups in India in 2012. So I started up one. I keep it ticking. People are slowly coming to learn about Meetups. In India I often find people grow impatient and try to make money out of everything they work hard on. Some things can be worked on for passion and community also. I think we have the largest R meetup in India.Kunal: What will be your advice for the people who are looking forward to enter analytics industry?Ajay:Learn R, Python, Java. Learn to Blog, Tweet and use LinkedIn. Keep Learning one new thing every day.Kunal: Any advice for young budding entrepreneurs of the analytics industry in India?Ajay:Just do it Man! Dont be greedy for easy money. Dont tell lies. Dont believe the lies you hear repeated most often. Keep a side job that makes steady money. Keep some time spare for thinking new ideas. Experiment, Fail, Learn, Repeat &Try to think of products and not just training and outsourcing services.Kunal: What is coming next from Ajay Ohri?Ajay:More books (I think Python  one need not be dependent on R  , more blog posts, more consulting, more poems. I try to learn new things and I try and spread. So teaching and meetups and conferences will be there. I am getting into spatial and big data analytics. Also, more analytics for economic well being of the poor people who are a big part of India (still). More teaching and efforts to reduce barriers to learning analytics.I will like to die writing with my head on my laptop on my writing desk and on my epitaph they should throw a party and have a beer and say- he did what he loved.It was indeed a wonderful experience to share. We are excited to have Ajay on board and wish good luck for hisfuture endeavors. We wish that hissuccess momentum keeps rolling to achieve even greater milestones!",https://www.analyticsvidhya.com/blog/2015/02/interview-expert-ajay-ohri-founder-decisionstats-com/
"Better, faster and more helpful Analytics Vidhya is now live!",Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Interview with Industry expert  Ajay Ohri, Founder, decisionstats.com|How to create Box-Plot chart in Qlikview?|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you are reading this article, you would have seen the new look of Analytics Vidhya by now! I cant tell how much we love this new avatar  you have to feel it! The colors of the site closely resemble the colors of our logo and so does the character  it is dynamic, exciting and full of useful content.With this change, we takeone more step inmaking Analytics Vidhya incredibly useful to its readers. In case you have missed the action  we launched our discussion platform and learning paths in January.Here are a few features to note about the new site:I could go on and tell you about these features and then some more, but that wouldnt help.I would love to hear what you think about the new look of Analytics Vidhya. Can you find the content more easily? Do you want some more addition? Do you want some changes in the layout? How does it look on your mobile devices? How fast does it load?You are the best judge to all these questions.Do share your experience with us  we love to hear what you say.",https://www.analyticsvidhya.com/blog/2015/02/better-faster-helpful-analytics-vidhya-live/
How to create Box-Plot chart in Qlikview?,Learn everything about Analytics,"Business Situation:|Different ways of Visualization:||What is a Box Plot?|How to read a Box Plot?:|Insights for the case study using Box Plot:||Method to Create A Box Plotin Qlikview:||End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Better, faster and more helpful Analytics Vidhya is now live!|Introduction to Online Machine Learning: Simplified|
Sunil Ray
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",Step 1:|Step 2:|Step 3:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The use of this article is best illustrated by a case study. So lets dive straight in.Recently, we entered 2015 and before starting new projects andinitiatives, every company would want to review their performance in last year. Lets think about a sales oriented company ABC. They have branches across India and these branches fall under four regions. Now, the company wants to analyse profitability at branch level and compare region level performance. They are looking for answers like which regions are following good practices and can they be replicated in other regions.So, you want to compare performance of a group of branches to come out with insights on relative performance like Region N branches are more profitable as compared to region S or 75% of Region W branches are more profitable than Region E.Nowat individual level, we can easily evaluate profitability and visualize it with the help of Pivot Tables or Bar Chart to show top and bottom branches. But the question is, how would we visualizecomparison of branch profitabilityatregional level? In other words, we can say how to compare distribution of branch profitability at regional level?Lets look at the available options to visualize this information. We can represent it through a pie chart, stacked column chart and also through table but all of these visualizations have some limitation.So, whenever we come across with suchsituations, we should look at Box Plot chart. It is the best way to visualize distribution of a group and compare across different levels. This chart is also known as Five Number Summary in statistics.A box plot is a graphical method of displaying variation in a set of data. In most cases, a histogram provides a sufficient display; however, a box plot can provide additional detail while allowing multiple sets of data to be displayed in the same graph. As I have mentioned before, it is also known as Five Number Summary, reason being it uses five statistic metrics to represent it.
There are some other types of Box plot also known as Modified Box Plot, those used to highlight outliers. In this article we will look at Box Plot only.Lets visualize the above discussed scenario using Box Plot and understand the insights gained from it.While looking at the above visualization, the following inferences can be generated:-In similar way, we can generate various inferences compare to different levels of dimensions.Now that the use of Box plot is clear, it is dis-appointing that creating box plot is not a simple click away on QlikView. You need to use a combo chart with a few hacks to create a Box Plot. Let us look at them step by step:In this article, precisely, we have looked at the methods to create Box Plotin Qlikview and how to generate insight while looking at box plot. Additionally, We have also looked at five statistical measures Minimum, Maximum, Median, First and third quartile and how these are useful to visualize the information effectively. Do you use Box Plotsin your roles and visualizations? If yes, how do you create them? Do you think this tip is useful?Do let me know your thoughts on using this chart in Qlikview.",https://www.analyticsvidhya.com/blog/2015/01/create-box-plot-qlikview/
Introduction to Online Machine Learning: Simplified,Learn everything about Analytics,"How does On-line learning differ from batch learning algorithms?|Example Case to understand the concept|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How to create Box-Plot chart in Qlikview?|Learning path for SAS  from beginner to a Business Analyst|
Tavish Srivastava
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data is being generated in huge quantities everywhere. Twitter generates 12 + TB of data every day, Facebook generates 25 + TB of data everyday and Google generates much more than these quantities everyday. Given that such data is being produced everyday, we need to build tools to handle data with high1. Volume : High volume of data are stored today for any industry. Conventional models on such huge data are infeasible.2. Velocity : Data come at high speed and demand quicker learning algorithms.3. Variety : Different sources of data have different structures. All these data contribute to prediction. A good algorithm can take in such variety of data.A simple predictive algorithm like Random Forest on about 50 thousand data points and 100 dimensions take 10 minutes to execute on a 12GB RAM machine. Problems with hundreds of millions of observation is simply impossible to solve using such machines. Hence, we are left with only two options : Use a stronger machine or change the way a predictive algorithm works. First option is not always feasible. In this article we will learn about On-line Learning algorithms which are meant to handle data with such high Volume and Velocity with limited performance machines.If you are a starter in the analytics industry, all you would have probably heard of will fall under batch learning category. Lets try to visualize how the working of the two differ from each other.Batch learning algorithms take batches of training data to train a model. Then predicts the test sample using the found relationship. Whereas, On-line learning algorithms take an initial guess model and then picks up one-one observation from the training population and recalibrates the weights on each input parameter. Here are a few trade-offs in using the two algorithms.In cases where we deal with huge data, we are left with no choice but to use online learning algorithms. The only other option is to do a batch learning on a smaller sample.We want to predict the probability that it will rain today. We have a panel of 11 people who predict the class : Rain and non-rain on different parameters. We need to design an algorithm to predict the probability. Let us first initialize a few denotions.i are individual predictorsw(i) is the weight given to the i th predictorInitial w(i) for i in [1,11] are all 1We will predict that it will rain today if,Sum(w(i) for all rain prediction) > Sum(w(i) for all non rain prediction)Once, we have the actual response of the target variable, we now send a feedback on the weights of all the parameters. In this case we will take a very simple feedback mechanism. For every right prediction, we will keep the weight of the predictor same. While for every wrong prediction, we divide the weight of the predictor by 1.2 (learning rate). With time we expect the model to converge with a right set of parameters.We created a simulation with 1000 predictions done by each of the 11 predictors. Here is how our accuracy curve came out,Each observation was taken at a time to re adjust the weights. Same way we will make predictions for the future data points.Online learning algorithms are widely used by E-commerce and social networking industry. It is not only fast but also has the capability to capture any new trend visible in with time. A variety of feedback systems and converging algorithms are presently available which should be selected as per the requirements. In some of the following articles, we will also take up a few practical examples of Online learning algorithm applications.Did you find the article useful? Have you used online learning algorithms before? Share with us any such experiences. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2015/01/introduction-online-machine-learning-simplified-2/
Learning path for SAS  from beginner to a Business Analyst,Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Introduction to Online Machine Learning: Simplified|Decision Tree Algorithms  Simplified|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This is now becoming a theme! But some thing we are very excited about and our audience is loving.Those who are late in the journey, Learning paths are your guide to master a new tool or technique with help of very structured path and (mostly) freely available resources.We launched the learning paths for Python and QlikView in last 2 weeks, and this week we are launching learning path for SAS. SAS is one of the easiest language to learn and is job market share leader.Hope this learning path proves immensely helpful to all the people who want to learn SAS. So, go out and check this learning path.",https://www.analyticsvidhya.com/blog/2015/01/learning-path-sas-beginner-business-analyst/
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. What is a Decision Tree ? How does it work ?|2. Regression Trees vs Classification Trees|3. How does a tree decide whereto split?|4. What are the key parameters of tree modelingand how can we avoid over-fitting in decision trees?|5. Are tree based models better than linear models?|6. Working with Decision Trees in R and Python|7. What are ensemble methods in tree based modeling ?|8. What is Bagging? How does it work?|9. What is Random Forest ? How does it work?|10. What is Boosting? How does it work?|11. Which is more powerful:GBM or Xgboost?|12. Working with GBM in R and Python|13. Working with XGBoost in R and Python|14. Where to practice ?||End Notes,"Types of Decision Trees|Important Terminology related toDecision Trees|Advantages|Disadvantages|Gini
|Chi-Square|Information Gain:|Reduction in Variance|Setting Constraints on Tree Size|Tree Pruning|How does it work?|Advantages of Random Forest|Disadvantages of Random Forest|Python & R implementation|How does it work?|GBM in R (with cross validation)|GBM in Python|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Case Study For Freshers (Level : Medium)  Call Center Optimization|Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|
Analytics Vidhya Content Team
|64 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), its important to learn these algorithms and use them for modeling.This tutorial is meant to help beginners learn tree based modeling from scratch. After the successful completion of this tutorial, one is expected to become proficient at using tree based algorithms andbuild predictive models.Note: This tutorial requires no prior knowledge of machine learning. However, elementary knowledge of R or Python will be helpful. To get started you can follow full tutorial in R and full tutorial in Python. You can also check out the Introduction to Data Science course covering Python, Statistics and Predictive Modeling.Decision treeis a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used inclassification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator ininput variables.Tree based modeling in R and PythonExample:-Lets say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket inleisure time. Now, I want to create a model topredict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.This is where decision tree helps, it will segregate the students based on all values of three variable andidentify the variable, which creates thebest homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To dothis, decision tree uses various algorithms, which we will discuss in the following section.Types of decision tree is based on the type of target variable we have. It can be of two types:Example:-Lets say we have a problem to predict whether a customer will pay his renewal premium with an insurance company(yes/ no). Here we know that income of customer is asignificant variable but insurance company does not have income details for all customers. Now, as we know thisis an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable.Lets look at the basic terminology used with Decision trees:These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leavesare the the bottom & roots are the tops (shown below).Both the trees work almost similar to each other, lets look at the primary differences & similaritybetween classification and regression trees:The decision of making strategic splits heavily affects a trees accuracy. The decision criteria is different for classification and regression trees.Decision trees use multiplealgorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.The algorithm selection is also based on type of target variables. Lets look at the four most commonlyused algorithms in decision tree:Gini says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.Steps toCalculate Gini for a splitExample:  Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In thesnapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini .Split on Gender:Similar for Split on Class:Above, you can see that Gini score for Split on Gender is higher than Split on Class,hence, the node split will take place on Gender.You might often come across the term Gini Impurity which is determined by subtracting the gini value from 1. So mathematically we can say,Gini Impurity = 1-GiniIt is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it bysum of squares of standardizeddifferences between observed and expected frequenciesof target variable.Steps toCalculate Chi-square for a split:Example: Lets work with above example that we have used to calculate Gini.Split on Gender:Split on Class:Perform similar steps of calculation for split on Class and you will come up with below table.Above, you can see that Chi-squarealso identify the Gender split is more significant compare to Class.Look at the image below and think which node can be described easily. I am sure, your answer isC because it requires lessinformation as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say thatC is a Pure node, B is less Impure and A is more impure.Now, we can build aconclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory isa measure to define this degree of disorganization in a systemknown as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50%  50%), it has entropy of one.Entropy can be calculated using formula:-Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.Steps to calculate entropy for a split:Example:Lets use this method to identify best split for student example.Above, you can see that entropy forSplit on Gender is the lowest among all,so the tree will split onGender. We can derive information gain from entropy as 1- Entropy.Till now, we have discussed the algorithms for categorical target variable. Reduction in variance is an algorithm used forcontinuoustarget variables (regression problems). This algorithm uses the standard formulaof variance to choose the bestsplit. The split with lower variance is selected as thecriteria to split the population:Above X-bar is mean of the values, X is actual and n is number of values.Steps to calculate Variance:Example:- Lets assign numerical value 1 for play cricket and 0 for not playing cricket. Now follow the steps to identify the right split:Above, you can see that Gender split has lower variance compare to parent node, so the split would take place on Gender variable.Until here, we learnt about the basics of decision trees and the decision making process involved to choose the best splits in building a tree model. As I said, decision tree can be applied both on regression and classification problems. Lets understand these aspects in detail.Overfitting is one of the key challenges faced while modeling decision trees. Ifthere is no limit set of a decision tree, it will give you 100% accuracy on training set becausein the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting ispivotal while modeling a decision tree and it can be done in 2 ways:Lets discuss both of these briefly.This can be done by using various parameters which are used to define a tree.First, lets look at the general structure of a decision tree:The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It isimportant to understandthe roleof parameters used in tree modeling. These parameters are available in R & Python.As discussed earlier, the technique of setting constraint is agreedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Lets consider the followingcase when youre driving:There are 2 lanes:At this instant, you are the yellow car and you have 2 choices:Lets analyze these choice. In the former choice, youll immediatelyovertake the car ahead and reachbehind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!This is exactly the difference between normal decision tree & pruning. A decision tree with constraints wont see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.So we know pruning is better. But howto implement it in decision tree? The idea is simple.Note that sklearns decision tree classifier does not currentlysupportpruning.Advanced packages like xgboost have adoptedtree pruning in their implementation.But the library rpart in R, provides a function to prune. Good for R users!If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees? Many of us have this question. And, this is a valid one too.Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Lets look at some key factors which will help you to decide which algorithm to use:For R users and Python users, decision tree is quite easy to implement. Lets quickly look at the set of codes that can get you started with this algorithm. For ease of use, Ive shared standard codes where youll need to replace your data set name and variables to get started.In fact, you can build the decision tree in Python right here! Heres a live coding window for you to play around the code and generate results:For R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc.In the code above:For Python users, below is the code:The literary meaning of word ensemble is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree basedmodels.Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, how much on an average are the predicted values different from the actual value. Variance means, how different will the predictions of the model be at the same point if different samples aretaken from the same population.You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors.Ensemble learning is one way to execute this trade off analysis.Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking. In this tutorial, well focus on Bagging and Boosting in detail.Baggingis a technique used to reduce the variance of our predictionsby combiningthe resultof multipleclassifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:
The steps followed in bagging are:Note that, herethe number of models built is not a hyper-parameters.Higher number of models are always better or may give similarperformance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.There are various implementations of bagging models. Random forest is one of them and well discuss it next.Random Forest is considered to be a panaceaof all data science problems. On a funny note, when you cant think of any algorithm (irrespective of situation), use random forest!Random Forestis a versatile machine learning method capable of performing both regression and classification tasks.It also undertakesdimensional reduction methods, treats missing values, outlier valuesand other essential steps of data exploration,and does a fairly good job.It isa type of ensemble learning method, where a group of weak models combineto form a powerful model.In Random Forest, we growmultipletrees as opposedto a single tree in CART model (see comparison between CART and Random Forest here, part1 and part2).To classify a new object based on attributes, each tree gives a classification and we say the tree votes for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.It works in the following manner.Each tree is planted & grown as follows:Tounderstand more in detail about this algorithm using a case study, please read thisarticle Introduction to Random forest  Simplified.Random forests have commonly known implementations in R packages and Python scikit-learn. Lets look at the codeof loading random forest model in R and Python below:RCodeDefinition: The term Boosting refers to a family of algorithms whichconverts weak learner to strong learners.Lets understand this definition in detail by solving a problem of spam email identification:How would you classifyan email as SPAM or not? Like everyone else, our initial approach would beto identify spam and not spam emails using following criteria. If:Above, weve defined multiple rules to classifyan email into spam or not spam.But, do you think these rules individually are strong enough to successfully classifyan email? No.Individually, these rules arenot powerful enough to classify an email into spam or not spam.Therefore, these rules are called as weak learner.To convert weak learner to strong learner, well combine the prediction of each weak learner using methods like:For example: Above,we have defined 5 weak learners. Out of these 5, 3 arevoted asSPAM and 2 are voted as Not a SPAM. In this case, by default, well consider an email as SPAM because wehave higher(3) vote for SPAM.Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, How boosting identify weak rules?To find weak rule, we applybase learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.Heres another question which might haunt you, How do we choose different distribution for each round?For choosing the right distribution, here are the following steps:Step 1: Thebase learner takes all the distributions and assign equal weight or attention to each observation.Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, weapply the next base learning algorithm.Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs fromweak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting payshigherfocus on examples which are mis-classied or have higher errors by preceding weak rules.
There are many boosting algorithms which impart additional boost to models accuracy. In this tutorial, well learn about the two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost.Ive always admired the boosting capabilities that xgboost algorithm. At times, Ive found that it providesbetter result compared to GBM implementation, but at times you might find that the gains are just marginal. When I explored more about its performance and science behind its high accuracy, I discovered many advantages of Xgboost over GBM:Before we start working, lets quickly understand the important parameters and the working of thisalgorithm. This will be helpful for both R and Python users. Below is the overall pseudo-code of GBM algorithm for 2 classes:This is an extremely simplified (probably naive) explanation of GBMs working. But, it will help every beginners to understand this algorithm.Lets considerthe important GBMparameters used to improve model performance in Python:Apart from these, there are certain miscellaneous parameters which affect overall functionality:I know its a long list of parameters but I have simplified it for you inan excel file which you can download from thisGitHub repository.For R users, using caret package, there are 3 main tuning parameters:Ive shared the standard codes in R and Python. At your end, youll be required to change the value of dependent variable and data set name used in the codes below. Considering the ease of implementing GBM in R, one can easily perform tasks like cross validation and grid search with this package.XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Its feature to implement parallel computing makes itat least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.R Tutorial: For R users, this is a complete tutorial on XGboost which explains the parameters along with codes in R. Check Tutorial.Python Tutorial: For Python users, this is a comprehensive tutorial on XGBoost, good to get you started. Check Tutorial.Practice is the one and true method of mastering any concept. Hence, you need to start practicingif you wish to master these algorithms.Till here, youve got gained significant knowledge on tree based models along with these practical implementation. Its time that you start working on them. Here are open practice problems where you can participate and check your live rankings on leaderboard:Tree based algorithm are important for every data scientist to learn. In fact, tree models are known to provide the best model performance in the family of whole machine learning algorithms. In this tutorial, we learnt until GBM and XGBoost. And with this, we come to the end of this tutorial.We discussed about tree based modeling from scratch. We learnt the important of decision tree and how that simplistic concept is being used in boosting algorithms. For better understanding, I would suggest you to continue practicing these algorithms practically. Also, do keep note of the parameters associated with boosting algorithms. Im hoping that this tutorial would enrich you with complete knowledge on tree based modeling.Did you find this tutorial useful ? If you have experienced, whats the best trick youve used while using tree based models ? Feel free to share your tricks, suggestions and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/01/decision-tree-algorithms-simplified/
"Engineering Manager  Big Data  Guavus  Gurgaon (4-12 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Decision Tree Algorithms  Simplified|Principal Engineer  Big Data  Apigee  Bangalore (5-8+ years of experience)|
Jobs Admin
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Engineering Manager  Big DataLocation  GurgaonAbout employer  GuavusJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asEngineering Manager  Big Data  Guavus  Gurgaon,https://www.analyticsvidhya.com/blog/2015/01/engineering-manager-big-data-guavus-gurgaon-8-12-years-experience/
"Principal Engineer  Big Data  Apigee  Bangalore (5-8+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engineering Manager  Big Data  Guavus  Gurgaon (4-12 years of experience)|Analyst / Senior Analyst  KiE Square Consulting  Delhi/Mumbai (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Principal Engineer  Big DataLocation  BangaloreAbout employer  ApigeeJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asPrincipal Engineer  Big Data  Apigee  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/principal-engineer-big-data-apigee-bangalore-5-8-years-experience/
"Analyst / Senior Analyst  KiE Square Consulting  Delhi/Mumbai (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Principal Engineer  Big Data  Apigee  Bangalore (5-8+ years of experience)|Data Analyst  Robosoft Technologies  India (2+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Analyst / Senior AnalystLocation  Delhi/MumbaiAbout employer  KiE Square ConsultingKiE Square Consulting is an Analytics Advisory firm with a deep-rooted focus on leveraging analytics to decentralize decision making and create sustainable competitive advantage. Founded by alumni of IIM Ahmedabad with over a decade of experience in US, Europe and India of leveraging business consulting, research, technology and analytic science to help companies strengthen their knowledge-intensive service functions leading to sustainable competitive advantage.Job description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalyst / Senior Analyst  KiE Square Consulting  Delhi/Mumbai",https://www.analyticsvidhya.com/blog/2015/01/analyst-senior-analyst-kie-square-consulting-delhimumbai-2-4-years-experience/
"Data Analyst  Robosoft Technologies  India (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst / Senior Analyst  KiE Square Consulting  Delhi/Mumbai (2-4 years of experience)|Analytics Manager (Statistical)  Accenture  Mumbai (6+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,We are looking for a Data Analyst to help us analyze game data and communicate insights that will help guide game development and game marking to create better games.Designation  Data AnalystLocation  IndiaAbout employer  Robosoft TechnologiesResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Robosoft Technologies  India,https://www.analyticsvidhya.com/blog/2015/01/data-analyst-robosoft-technologies-india-2-years-experience/
"Analytics Manager (Statistical)  Accenture  Mumbai (6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst  Robosoft Technologies  India (2+ years of experience)|Data Scientist / Machine learning Expert  Bidgely  Bangalore (2-8+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytics Manager (Statistical)Location  MumbaiAbout employer  AccentureJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalytics Manager (Statistical)  Accenture  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/analytics-manager-statistical-accenture-mumbai-6-years-experience/
"Data Scientist / Machine learning Expert  Bidgely  Bangalore (2-8+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Manager (Statistical)  Accenture  Mumbai (6+ years of experience)|Data Scientist  BloomReach  Bangalore|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Scientist / Machine learning ExpertLocation  BangaloreAbout employer  BidgelyJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject Data Scientist / Machine learning Expert  Bidgely  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/data-scientist-machine-learning-expert-bidgely-bangalore-2-8-years-experience/
"Data Scientist  BloomReach  Bangalore|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist / Machine learning Expert  Bidgely  Bangalore (2-8+ years of experience)|Asst Manager/Manager  Web Analytics  BookMyShow  Mumbai (2-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  BangaloreAbout employer  BloomReachJob description:As a BloomReach Data Scientist, you will analyze how users behave and engage with BloomReach products on the web to drive key product decisions. You should be scrappy, hands-on and passionate about mining insights from terabytes of data using quantitative analysis and turning these insights into products, features and stories. You will be working with some of the richest datasets in the world, cutting-edge technology and world-class researchers/engineers.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE",https://www.analyticsvidhya.com/blog/2015/01/data-scientist-bloomreach-bangalore-2/
"Asst Manager/Manager  Web Analytics  BookMyShow  Mumbai (2-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  BloomReach  Bangalore|Sr. Data Scientist  Tiger Analytics  Chennai (3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Asst Manager/Manager  Web AnalyticsLocation  MumbaiAbout employer  BookMyShowJob description:As Assistant Manager/ Manager  Web Analytics, you will work with the Conversion Rate Optimization (CRO) team at BookMyShow. The CRO team at BookMyShow is focused on improving ticket conversions on the website and mobile apps. Analytical bend of mind and high interest in data analysis are critical for success in this role.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject Asst Manager/Manager  Web Analytics  BookMyShow  Mumbai",https://www.analyticsvidhya.com/blog/2015/01/asst-managermanager-web-analytics-bookmyshow-mumbai-2-6-years-experience/
"Sr. Data Scientist  Tiger Analytics  Chennai (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Asst Manager/Manager  Web Analytics  BookMyShow  Mumbai (2-6 years of experience)|Product Analyst  Electronic Arts  Hyderabad (2-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr. Data ScientistLocation  ChennaiAbout employer  Tiger AnalyticsJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr. Data Scientist  Tiger Analytics  Chennai,https://www.analyticsvidhya.com/blog/2015/01/sr-data-scientist-tiger-analytics-chennai-3-years-experience-2/
"Product Analyst  Electronic Arts  Hyderabad (2-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Data Scientist  Tiger Analytics  Chennai (3+ years of experience)|Model Performance metrics: How well does my model perform?  Part 2|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Product AnalystLocation  HyderabadAbout employer  Electronic ArtsJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/product-analyst-electronic-arts-hyderabad-2-5-years-experience/
11 Important Model Evaluation Metrics for Machine Learning Everyone should know,"Learn everything about Analytics|Overview|Introduction|Table of Contents|Warming up: Types of Predictive models|1. Confusion Matrix|2. F1 Score|3. Gain and Lift charts
|4. Kolomogorov Smirnov chart
|5. Area Under the ROC curve (AUC  ROC)|6. Log Loss|7. Gini Coefficient|8. Concordant  Discordant ratio|9. Root Mean Squared Error (RMSE)|10. Root Mean Squared Logarithmic Error|11. R-Squared/Adjusted R-Squared|12. Cross Validation|End Notes","|Advantages of using ROC|Adjusted R-Squared|Here is an example of scoring on Kaggle!|The concept : Cross Validation|k-fold Cross validation|How does this help to find best (non over-fit) model?|How do we implementk-fold with any model?|But how do we choose k?|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework|Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science|
Tavish Srivastava
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published in February 2016 and updated in August 2019. with four new evaluation metrics.The idea of building machine learning models works on a constructive feedback principle. You build a model, get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results.I have seen plenty of analysts and aspiring data scientists not even bothering to check how robust their model is. Once they are finished building a model, they hurriedly map predicted values on unseen data. This is an incorrect approach.Simply building a predictive model is not your motive. Its about creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check the accuracy of your model prior to computing predicted values.In our industry, we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model.After you are finished building your model, these 11 metrics will help you in evaluating your models accuracy. Considering the rising popularity and importance of cross-validation, Ive also mentioned its principles in this article.And if youre starting out your machine learning journey, you should check out the comprehensive and popular Applied Machine Learning course which covers this concept in a lot of detail along with the various algorithms and components of machine learning.When we talk about predictive models, we are talking either about a regression model (continuous output) or a classification model (nominal or binary output). The evaluation metrics used in each of these models are different.In classification problems, we use two types of algorithms (dependent on the kind of output it creates):In regression problems, we do not have such inconsistencies in output. The output is always continuous in nature and requires no further treatment.Illustrative ExampleFor a classification model evaluation metric discussion, I have used my predictions for the problem BCI challenge on Kaggle. The solution of the problem is out of the scope of our discussion here. However the final predictions on the training set have been used for this article. The predictions made for this problem were probability outputs which have been converted to class outputs assuming a threshold of 0.5.A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :The accuracy for the problem in hand comes out to be 88%. As you can see from the above two tables, the Positive predictive Value is high, but negative predictive value is quite low. Same holds for Sensitivity and Specificity. This is primarily driven by the threshold value we have chosen. If we decrease our threshold value, the two pairs of starkly different numbers will come closer.In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Sensitivity. Confusion matrix are generally used only with class output models.In the last section, we discussed precision and recall for classification problems and also highlighted the importance of choosing precision/recall basis our use case. What if for a use case, we are trying to get the best precision and recall at the same time? F1-Score is the harmonic mean of precision and recall values for a classification problem. The formula for F1-Score is as follows:Now, an obvious question that comes to mind is why are taking a harmonic mean and not an arithmetic mean. This is because HM punishes extreme values more. Let us understand this with an example. We have a binary classification model with the following results:Precision: 0, Recall: 1Here, if we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a dumb classifier which just ignores the input and just predicts one of the classes as output. Now, if we were to take HM, we will get 0 which is accurate as this model is useless for all purposes.This seems simple. There are situations however for which a data scientist would like to give a percentage more importance/weight to either precision or recall. Altering the above expression a bit such that we can include an adjustable parameter beta for this purpose, we get:Fbetameasures the effectiveness of a model with respect to a user who attaches  times as much importance to recall as precision.Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:Step 1 : Calculate probability for each observationStep 2 : Rank these probabilities in decreasing order.Step 3 : Build deciles with each group having almost 10% of the observations.Step 4 : Calculate the response rate at each deciles for Good (Responders) ,Bad (Non-responders) and total.You will get following table from which you need to plot Gain/Lift charts:This is a very informative table. Cumulative Gain chart is the graph between Cumulative %Right and Cummulative %Population. For the case in hand here is the graph :This graph tells you how well is your model segregating responders from non-responders. For example, the first decile however has 10% of the population, has 14% of responders. This means we have a 140% lift at first decile.What is the maximum lift we could have reached in first decile? From the first table of this article, we know that the total number of responders are 3850. Also the first decile will contains 543 observations. Hence, the maximum lift at first decile could have been 543/3850 ~ 14.1%. Hence, we are quite close to perfection with this model.Lets now plot the lift curve. Lift curve is the plot between total lift and %population. Note that for a random model, this always stays flat at 100%. Here is the plot for the case in hand :You can also plot decile wise lift with decile number :What does this graph tell you? It tells you that our model does well till the 7th decile. Post which every decile will be skewed towards non-responders. Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model. Else you might consider over sampling first.Lift / Gain charts are widely used in campaign targeting problems. This tells us till which decile can we target customers for an specific campaign. Also, it tells you how much response do you expect from the new target base.K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.For the case in hand, following is the table :We can also plot the %Cumulative Good and Bad to see the maximum separation. Following is a sample plot :The metrics covered till hereare mostly used in classification problems. Till here, we learnt about confusion matrix, lift and gain chart and kolmogorov-smirnov chart.Lets proceed and learn fewmore important metrics.This is again one of the popular metrics used in the industry. The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections.Lets first try to understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric.Hence, for each sensitivity, we get a different specificity.The two vary as follows:The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.Lets take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix :As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.Points to Remember:1.For a model which gives class as output, will be represented as a single point in ROC plot.2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.Why should you use ROC and not metrics like lift curve?Lift is dependent ontotal response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift.AUC ROC considers the predicted probabilities for determining our models performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the models capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing butnegative average of the log of corrected predicted probabilities for each instance.Let us calculate log loss for a few random values to get the gist of the above mathematical function:Logloss(1, 0.1) = 2.303Logloss(1, 0.5) = 0.693Logloss(1, 0.9) = 0.105If we plot this relationship, we will get a curve as follows:Its apparent from the gentle downward slope towards the right that the Log Loss gradually declines as the predicted probability improves. Moving in the opposite direction though, the Log Loss ramps up very rapidly as the predicted probability approaches 0.So, lower the log loss, better the model. However, there is no absolute measure on a good log loss and it is use-case/application dependent.Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes certainty of classification into account.Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :Gini = 2*AUC  1Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.This is again one of the most important metric for any classification predictions problem. To understand this lets assume we have 3 students who have some likelihood to pass this year. Following are our predictions :A  0.9B  0.5C  0.3Nowpicture this.if we were to fetch pairs of two from these three student, how many pairs will we have? We will have 3 pairs : AB , BC, CA. Now, after the year ends we saw that A and C passed this year while B failed. No, we choose all the pairs where we will find one responder and other non-responder. How many such pairs do we have?We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the probability of responder was higher than non-responder. Whereas discordant pair is where the vice-versa holds true. In case both the probabilities were equal, we say its a tie. Lets see what happens in our case :AB  ConcordantBC  DiscordantHence, we have 50% of concordant cases in this example. Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the models predictive power. For decisions like how many to target are again taken by KS / Lift charts.RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:RMSE metric is given by:where, N is Total Number of Observations.In case of Root mean squared logarithmic error, we take the log of the predictions and actual values. So basically, what changes are the variance that we are measuring. RMSLE is usually used when we dont want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.We learned that when the RMSE decreases, the models performance will improve. But these values alone are not intuitive.In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of 0.5. So the random model can be treated as a benchmark.But when we talk about the RMSE metrics, we do not have a benchmark to compare.This is where we can use R-Squared metric. The formula for R-Squared is as follows:MSE(model): Mean Squared Error of the predictions against the actual valuesMSE(baseline): Mean Squared Error of mean prediction against the actual valuesIn other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formulafor adjusted R-Squared is given by:k: number of featuresn: number of samplesAs you can see, this metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases.If R-Squared does not increase, that means the feature added isnt valuable for our model. So overall we subtract a greater value from 1 and adjusted r2, in turn, would decrease.Beyond these 11 metrics, there is another method to check the model performance. These 7 methods are statistically prominent in data science. But, with arrival of machine learning, we are now blessedwith more robust methods of model selection. Yes! Im talking about Cross Validation.Though, cross validation isnt a really an evaluation metric which is used openly tocommunicate model accuracy.But, the result of cross validation provides good enough intuitive result to generalize the performance of a model.Lets now understand cross validation in detail.Lets first understand the importance of cross validation. Due to busy schedules, these days I dont get much time to participate in data science competitions. Long time back, I participated in TFI Competition on Kaggle. Without delving into my competition performance, I would like to show you the dissimilarity between my public and private leaderboard score.For TFI competition, following were three of my solution and scores (Lesser the better) :You will notice that the third entry which has the worstPublic score turnedto be the best model on Private ranking. There were more than 20 models above the submission_all.csv, but I still chose submission_all.csvas my final entry (which really worked out well). What caused this phenomenon ? The dissimilarity in my public and private leaderboard is caused by over-fitting.Over-fitting is nothing but when you model become highly complex that it starts capturing noise also. This noise adds no value to model, but only inaccuracy.In the following section, I will discuss how you can know if a solution is an over-fit or not before we actually know the test results.Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.Above diagram shows how to validate model with in-time sample. We simply divide the population into 2 samples, and build model on one sample. Rest of the population is used for in-time validation.Could there be anegativeside of the above approach?I believe, a negative side of this approach is that we loose a good amount of data from training the model. Hence, the model is very high bias. And this wont give best estimate for the coefficients. So whats the next best option?What if, we make a 50:50 split of training population and the train on first 50 and validate on rest 50. Then, we train on the other 50, test on first 50. This way we train the model on the entire population, however on 50% in one go. This reduces bias because of sample selection to some extent but gives a smaller sample to train the model on. This approach is known as 2-fold cross validation.Lets extrapolate the last example to k-fold from2-fold cross validation. Now, we will try to visualize how does a k-fold validation work.This is a 7-fold cross validation.Heres what goes on behind the scene : we divide the entire population into 7 equal samples. Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). Then, at the second iteration we train the model with a different sample held as validation. In 7 iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power. Once we have all the 7 models, we take average of the error terms to find which of the models is best.k-fold cross validation is widely used to check whether a model is an overfit or not. If the performance metrics at each of the k times modelling are close to each other and the mean of metric is highest. In a Kaggle competition, you might rely more on the cross validation score and not on the Kaggle public score. This way you will be sure that the Public score is not just by chance.Coding k-fold in R and Python are very similar. Here is how you code a k-fold in Python :This is the tricky part. We have a trade off to choose k.For a small k, we have a higher selection bias but low variance in the performances.For a largek, we have a smallselection bias but highvariance in the performances.Think of extreme cases :k = 2 : We have only 2 samples similar to our 50-50 example. Here we build model only on 50% of the population each time. But as the validation is a significant population, the variance of validation performance is minimal.k = number of observations(n) : This is also known as Leave one out. We have nsamples and modelling repeated n number of times leaving only one observation out for cross validation. Hence, the selection bias is minimal but the variance of validation performance is very large.Generally a value of k = 10 is recommended for most purpose.Measuring the performance on training sample is point less. And leaving a in-time validation batch aside is a waste of data. K-Fold gives us a way to use every singe datapoint which can reduce this selection bias to a good extent. Also, K-fold cross validation can be used with any modelling technique.In addition, the metrics covered in this article are some of the most used metrics of evaluation in a classification and regression problems.Which metric do you often use in classification and regression problem ? Have you used k-fold cross validationbefore for any kind of analysis? Did you see any significant benefits against using a batch validation? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/01/model-perform-part-2/
"Senior Data Scientist  Azuga  Bangalore (5-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Model Performance metrics: How well does my model perform?  Part 2|Lead Business Analyst  BST MIS, Analysis and Reporting  Amex  Gurgaon (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Data ScientistLocation  BangaloreAbout employer  AzugaJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Data Scientist  Azuga  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/senior-data-scientist-azuga-bangalore-5-10-years-experience-2/
"Lead Business Analyst  BST MIS, Analysis and Reporting  Amex  Gurgaon (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Scientist  Azuga  Bangalore (5-10 years of experience)|Analytics Professionals  Accenture  Bangalore (2-8 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Lead Business Analyst  BST MIS, Analysis and ReportingLocation  GurgaonAbout employer  AmexJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asLead Business Analyst  BST MIS, Analysis and Reporting  Amex  Gurgaon",https://www.analyticsvidhya.com/blog/2015/01/lead-business-analyst-bst-mis-analysis-reporting-amex-gurgaon-2-4-years-experience/
"Analytics Professionals  Accenture  Bangalore (2-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead Business Analyst  BST MIS, Analysis and Reporting  Amex  Gurgaon (2-4 years of experience)|Data Scientist (Big Data)  Mobileum  Gurgaon (3-10 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytics ProfessionalsLocation  BangaloreAbout employer  AccentureJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalytics Professionals  Accenture  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/analytics-professionals-accenture-bangalore-2-8-years-experience/
"Data Scientist (Big Data)  Mobileum  Gurgaon (3-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Professionals  Accenture  Bangalore (2-8 years of experience)|Machine Learning Scientist  Amazon  Bangalore (2-3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Scientist (Big Data)Location  GurgaonAbout employer  MobileumJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist (Big Data)  Mobileum  Gurgaon,https://www.analyticsvidhya.com/blog/2015/01/data-scientist-big-data-mobileum-gurgaon-3-10-years-experience/
"Machine Learning Scientist  Amazon  Bangalore (2-3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Big Data)  Mobileum  Gurgaon (3-10 years of experience)|Business Analyst  Marketing  Groupon  New Delhi (0.5-2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Machine Learning ScientistLocation  BangaloreAbout employer  AmazonJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asMachine Learning Scientist  Amazon  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/machine-learning-scientist-amazon-bangalore-2-3-years-experience/
"Business Analyst  Marketing  Groupon  New Delhi (0.5-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Scientist  Amazon  Bangalore (2-3+ years of experience)|Senior Analyst  LatentView  Chennai (3-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Analyst  MarketingLocation  New DelhiAbout employer  GrouponJob description:ResponsibilitiesQualification and Skills RequiredWhy Groupon:Interested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/business-analyst-marketing-groupon-delhi-0-5-2-years-experience/
"Senior Analyst  LatentView  Chennai (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Marketing  Groupon  New Delhi (0.5-2 years of experience)|Data Scientists  Gurgaon/Pune (4-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior AnalystLocation  ChennaiAbout employer  LatentViewLatentView is a business consulting firm, founded by IIT / IIM Alumni, delivering analytics solutions to business problems based on data-driven insights. LatentView delivers solutions around customer analytics, predictive modeling, and decision sciences that will provide you excellent opportunities to apply principles of mathematics, statistics, econometrics, computer science and operations research to solve real world business problems. LatentView has won numerous business and technical awards, including Deloitte Fast 50 Asia, RedHerring Asia 500, KDD Cup for excellence in data mining, among others.Job description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analyst  LatentView  Chennai",https://www.analyticsvidhya.com/blog/2015/01/senior-analyst-latentview-chennai-3-5-years-experience-2/
"Data Scientists  Gurgaon/Pune (4-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  LatentView  Chennai (3-5 years of experience)|Big Data Specialist  Zoomcar  Bangalore (4-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistsLocation  Gurgaon/PuneAbout employer ConfidentialJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientists  Gurgaon/Pune,https://www.analyticsvidhya.com/blog/2015/01/data-scientists-zs-associates-gurgaonpune-4-6-years-experience/
"Big Data Specialist  Zoomcar  Bangalore (4-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientists  Gurgaon/Pune (4-6 years of experience)|Sr Analyst/Manager Analytics  Zoomcar  Bangalore (2-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Big Data SpecialistLocation  BangaloreAbout employer  ZoomcarZoomcar is Indias first 100% self-drive car rental service. Were a membership-based service that allows customers to rent cars out by the hour, day, week or month. At present, we have a fleet of nearly 600 vehicles across more than 85+ locations in Bangalore, Pune & Delhi NCR. We offer more than 10 models covering hatchbacks, sedans, suvs and luxury cars. Were in rapid expansion mode as we look to grow pan-India in 2015 and beyond!Job description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asBig Data Specialist  Zoomcar  Bangalore",https://www.analyticsvidhya.com/blog/2015/01/big-data-specialist-zoomcar-bangalore-4-6-years-experience/
"Sr Analyst/Manager Analytics  Zoomcar  Bangalore (2-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Specialist  Zoomcar  Bangalore (4-6 years of experience)|QlikView learning path  the only resource you need to master QlikView|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Sr Analyst/Manager AnalyticsLocation  BangaloreAbout employer  ZoomcarZoomcar is Indias first 100% self-drive car rental service. Were a membership-based service that allows customers to rent cars out by the hour, day, week or month. At present, we have a fleet of nearly 600 vehicles across more than 85+ locations in Bangalore, Pune & Delhi NCR. We offer more than 10 models covering hatchbacks, sedans, suvs and luxury cars. Were in rapid expansion mode as we look to grow pan-India in 2015 and beyond!Job description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr Analyst/Manager Analytics  Zoomcar  Bangalore",https://www.analyticsvidhya.com/blog/2015/01/sr-analystmanager-analytics-zoomcar-bangalore-2-5-years-experience/
QlikView learning path  the only resource you need to master QlikView,Learn everything about Analytics,"Today, we launch the second learning path  Data Visualization with QlikView.|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Sr Analyst/Manager Analytics  Zoomcar  Bangalore (2-5 years of experience)|Business Analytics Lead  MIS Reporting Sr Analyst  JPMorgan Chase  Mumbai (3-6+ years of experience)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We launched our learning paths last week with Data Science in Python. The Python learning path received awesome response from not only our audience, but data science community world wide. The learning path received more views in a day than what some of our best written articles get in a month!The philosophy behind thelearning path remains the same  we have taken out confusion from your learning and created a very clear, crisp and structured path.Data Visualization with QlikView has been designed by Sunil, our QlikView expert and I am hyper excited about what he has penned down. Not only he has collated best articles, videos and trainings on QlikView, he has created a bunch of exercises to make sure you get enough practice.So, without taking away any more thunder from Sunils work, I ask you to go and have a look at this awesome learning path.",https://www.analyticsvidhya.com/blog/2015/01/qlikview-learning-path-resource-master-qlikview/
"Business Analytics Lead  MIS Reporting Sr Analyst  JPMorgan Chase  Mumbai (3-6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|QlikView learning path  the only resource you need to master QlikView|Operations Business Analyst 2  Lam Research  Mumbai (3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Analytics Lead  MIS Reporting Sr AnalystLocation  MumbaiAbout employer  JPMorgan ChaseJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/business-analytics-lead-mis-reporting-sr-analyst-jpmorgan-chase-mumbai-3-6-years-experience/
"Operations Business Analyst 2  Lam Research  Mumbai (3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analytics Lead  MIS Reporting Sr Analyst  JPMorgan Chase  Mumbai (3-6+ years of experience)|Hadoop Developer  Gracenote  Mumbai (1-2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Operations Business Analyst 2Location  MumbaiAbout employer  Lam ResearchJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/operations-business-analyst-2-lam-research-mumbai-3-years-experience/
"Hadoop Developer  Gracenote  Mumbai (1-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Operations Business Analyst 2  Lam Research  Mumbai (3 years of experience)|Application Developer -Data Science  Lumata  Noida|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Hadoop DeveloperLocation  MumbaiAbout employer  GracenoteJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asHadoop Developer  Gracenote  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/hadoop-developer-gracenote-mumbai-1-2-years-experience/
"Application Developer -Data Science  Lumata  Noida|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Developer  Gracenote  Mumbai (1-2 years of experience)|Engineer / Senior Engineer  Big Data / Hadoop  American Express  Bangalore /Gurgaon (2-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Application Developer -Data ScienceLocation  NoidaAbout employer  LumataJob description:ResponsibilitiesLumata is looking for a smart engineer to work on our product suites data mining and forecasting platform. This roles entails writing production quality code and not just building prototypes. The role involves working with product owners and research scientists to build production quality innovative forecasting and optimization solutions for building real time predictive models and algorithms to solve challenging problems such as churn management, improving campaigns, optimizing offers, and so forth; and processing terabytes of data to extract insights using data mining, statistics, and machine learning techniquesQualification and Skills RequiredBehaviours & AttributesExtra CreditInterested people can apply for this job can mail their CV to[emailprotected]with subject asApplication Developer -Data Science  Lumata  Noida",https://www.analyticsvidhya.com/blog/2015/01/application-developer-data-science-lumata-noida/
"Engineer / Senior Engineer  Big Data / Hadoop  American Express  Bangalore /Gurgaon (2-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Application Developer -Data Science  Lumata  Noida|Hadoop Architect  DataMetica  Pune (10+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Engineer / Senior Engineer  Big Data / HadoopLocation  Bangalore/GurgaonAbout employer American ExpressJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asEngineer / Senior Engineer  Big Data / Hadoop  American Express Bangalore /Gurgaon,https://www.analyticsvidhya.com/blog/2015/01/engineer-senior-engineer-big-data-hadoop-american-express-bangalore-gurgaon-2-5-years-experience/
"Hadoop Architect  DataMetica  Pune (10+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engineer / Senior Engineer  Big Data / Hadoop  American Express  Bangalore /Gurgaon (2-5 years of experience)|Customer Analytics  R Systems  New Delhi (3-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Hadoop ArchitectLocation  PuneAbout employer  DataMeticaJob description:We are looking for key addition to our engineering team for building scalable distributed data solutions using Hadoop. The Hadoop Architect would be driving solutions and product development which are relevant to the industry today.ResponsibilitiesQualification and Skills RequiredThe Hadoop Architect should have a solid background in the fundamentals of computer science, distributed computing, large scale data processing as well as mastery of database designs and data warehousing. The person should have a high degree of self motivation, an unwavering commitment to excellence, excellent work ethic, positive attitude, and is fun to work with.Interested people can apply for this job can mail their CV to[emailprotected]with subject asHadoop Architect  DataMetica  Pune",https://www.analyticsvidhya.com/blog/2015/01/hadoop-architect-datametica-pune-10-years-experience/
"Customer Analytics  R Systems  New Delhi (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Architect  DataMetica  Pune (10+ years of experience)|Data Scientist  GSN  Bangalore (5+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Customer AnalyticsLocation  New DelhiAbout employer  R SystemsJob description:We are looking for data scientists/customer analytics resources having experience in analyzing and modelling customer behavior to increase customer profitability and revenue. Resources with segmentation and predictive analytics experience preferably in pre-paid card, debit cards or credit card industry would be ideal candidates.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asCustomer Analytics  R Systems  New Delhi",https://www.analyticsvidhya.com/blog/2015/01/customer-analytics-systems-delhi-3-5-years-experience/
"Data Scientist  GSN  Bangalore (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Customer Analytics  R Systems  New Delhi (3-5 years of experience)|Data Engineer  GSN  Bangalore|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  BangaloreAbout employer  GSNJob description:Are you skilled at using statistics and machine learning to solve complex business problems? Do you want to directly impact the strategy and game development of a best-in-class video game company using big data? Would you like to be part of a fun, fast-paced team working in a strong data-driven culture? We are looking for an outstanding Data Scientist to join our Analytics team.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  GSN  Bangalore",https://www.analyticsvidhya.com/blog/2015/01/data-scientist-gsn-bangalore-5-years-experience/
"Data Engineer  GSN  Bangalore|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  GSN  Bangalore (5+ years of experience)|Analytical Delivery Leads  Analyttica  Bangalore (2-10 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data EngineerLocation  BangaloreAbout employer  GSNJob description:We are building a next generation big data analytics platform for social gaming. Our BI platform runs on Vertica and contains over one trillion data points. We ingest close to 1 billion records a day from various internal and external data sources including databases, mobile devices, web services and 3rd party APIs. You will be responsible for building highly scalable multithreaded applications to gather, store and maintain massive datasets. The applications you build will help power a growing library of dashboards, reports, and interactive data tools consumed directly by our management team.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Engineer  GSN  Bangalore",https://www.analyticsvidhya.com/blog/2015/01/data-engineer-gsn-bangalore/
"Analytical Delivery Leads  Analyttica  Bangalore (2-10 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Engineer  GSN  Bangalore|Decision Tree  Simplified!|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytical Delivery LeadsLocation  BangaloreAbout employer  AnalytticaJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAnalytical Delivery Leads  Analyttica  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/analytical-delivery-leads-analyttica-bangalore-2-10-years-experience/
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. What is a Decision Tree ? How does it work ?|2. Regression Trees vs Classification Trees|3. How does a tree decide whereto split?|4. What are the key parameters of tree modelingand how can we avoid over-fitting in decision trees?|5. Are tree based models better than linear models?|6. Working with Decision Trees in R and Python|7. What are ensemble methods in tree based modeling ?|8. What is Bagging? How does it work?|9. What is Random Forest ? How does it work?|10. What is Boosting? How does it work?|11. Which is more powerful:GBM or Xgboost?|12. Working with GBM in R and Python|13. Working with XGBoost in R and Python|14. Where to practice ?||End Notes,"Types of Decision Trees|Important Terminology related toDecision Trees|Advantages|Disadvantages|Gini
|Chi-Square|Information Gain:|Reduction in Variance|Setting Constraints on Tree Size|Tree Pruning|How does it work?|Advantages of Random Forest|Disadvantages of Random Forest|Python & R implementation|How does it work?|GBM in R (with cross validation)|GBM in Python|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Case Study For Freshers (Level : Medium)  Call Center Optimization|Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|
Analytics Vidhya Content Team
|64 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), its important to learn these algorithms and use them for modeling.This tutorial is meant to help beginners learn tree based modeling from scratch. After the successful completion of this tutorial, one is expected to become proficient at using tree based algorithms andbuild predictive models.Note: This tutorial requires no prior knowledge of machine learning. However, elementary knowledge of R or Python will be helpful. To get started you can follow full tutorial in R and full tutorial in Python. You can also check out the Introduction to Data Science course covering Python, Statistics and Predictive Modeling.Decision treeis a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used inclassification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator ininput variables.Tree based modeling in R and PythonExample:-Lets say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket inleisure time. Now, I want to create a model topredict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.This is where decision tree helps, it will segregate the students based on all values of three variable andidentify the variable, which creates thebest homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To dothis, decision tree uses various algorithms, which we will discuss in the following section.Types of decision tree is based on the type of target variable we have. It can be of two types:Example:-Lets say we have a problem to predict whether a customer will pay his renewal premium with an insurance company(yes/ no). Here we know that income of customer is asignificant variable but insurance company does not have income details for all customers. Now, as we know thisis an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable.Lets look at the basic terminology used with Decision trees:These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leavesare the the bottom & roots are the tops (shown below).Both the trees work almost similar to each other, lets look at the primary differences & similaritybetween classification and regression trees:The decision of making strategic splits heavily affects a trees accuracy. The decision criteria is different for classification and regression trees.Decision trees use multiplealgorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.The algorithm selection is also based on type of target variables. Lets look at the four most commonlyused algorithms in decision tree:Gini says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.Steps toCalculate Gini for a splitExample:  Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In thesnapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini .Split on Gender:Similar for Split on Class:Above, you can see that Gini score for Split on Gender is higher than Split on Class,hence, the node split will take place on Gender.You might often come across the term Gini Impurity which is determined by subtracting the gini value from 1. So mathematically we can say,Gini Impurity = 1-GiniIt is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it bysum of squares of standardizeddifferences between observed and expected frequenciesof target variable.Steps toCalculate Chi-square for a split:Example: Lets work with above example that we have used to calculate Gini.Split on Gender:Split on Class:Perform similar steps of calculation for split on Class and you will come up with below table.Above, you can see that Chi-squarealso identify the Gender split is more significant compare to Class.Look at the image below and think which node can be described easily. I am sure, your answer isC because it requires lessinformation as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say thatC is a Pure node, B is less Impure and A is more impure.Now, we can build aconclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory isa measure to define this degree of disorganization in a systemknown as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50%  50%), it has entropy of one.Entropy can be calculated using formula:-Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.Steps to calculate entropy for a split:Example:Lets use this method to identify best split for student example.Above, you can see that entropy forSplit on Gender is the lowest among all,so the tree will split onGender. We can derive information gain from entropy as 1- Entropy.Till now, we have discussed the algorithms for categorical target variable. Reduction in variance is an algorithm used forcontinuoustarget variables (regression problems). This algorithm uses the standard formulaof variance to choose the bestsplit. The split with lower variance is selected as thecriteria to split the population:Above X-bar is mean of the values, X is actual and n is number of values.Steps to calculate Variance:Example:- Lets assign numerical value 1 for play cricket and 0 for not playing cricket. Now follow the steps to identify the right split:Above, you can see that Gender split has lower variance compare to parent node, so the split would take place on Gender variable.Until here, we learnt about the basics of decision trees and the decision making process involved to choose the best splits in building a tree model. As I said, decision tree can be applied both on regression and classification problems. Lets understand these aspects in detail.Overfitting is one of the key challenges faced while modeling decision trees. Ifthere is no limit set of a decision tree, it will give you 100% accuracy on training set becausein the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting ispivotal while modeling a decision tree and it can be done in 2 ways:Lets discuss both of these briefly.This can be done by using various parameters which are used to define a tree.First, lets look at the general structure of a decision tree:The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It isimportant to understandthe roleof parameters used in tree modeling. These parameters are available in R & Python.As discussed earlier, the technique of setting constraint is agreedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Lets consider the followingcase when youre driving:There are 2 lanes:At this instant, you are the yellow car and you have 2 choices:Lets analyze these choice. In the former choice, youll immediatelyovertake the car ahead and reachbehind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!This is exactly the difference between normal decision tree & pruning. A decision tree with constraints wont see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.So we know pruning is better. But howto implement it in decision tree? The idea is simple.Note that sklearns decision tree classifier does not currentlysupportpruning.Advanced packages like xgboost have adoptedtree pruning in their implementation.But the library rpart in R, provides a function to prune. Good for R users!If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees? Many of us have this question. And, this is a valid one too.Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Lets look at some key factors which will help you to decide which algorithm to use:For R users and Python users, decision tree is quite easy to implement. Lets quickly look at the set of codes that can get you started with this algorithm. For ease of use, Ive shared standard codes where youll need to replace your data set name and variables to get started.In fact, you can build the decision tree in Python right here! Heres a live coding window for you to play around the code and generate results:For R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc.In the code above:For Python users, below is the code:The literary meaning of word ensemble is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree basedmodels.Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, how much on an average are the predicted values different from the actual value. Variance means, how different will the predictions of the model be at the same point if different samples aretaken from the same population.You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors.Ensemble learning is one way to execute this trade off analysis.Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking. In this tutorial, well focus on Bagging and Boosting in detail.Baggingis a technique used to reduce the variance of our predictionsby combiningthe resultof multipleclassifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:
The steps followed in bagging are:Note that, herethe number of models built is not a hyper-parameters.Higher number of models are always better or may give similarperformance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.There are various implementations of bagging models. Random forest is one of them and well discuss it next.Random Forest is considered to be a panaceaof all data science problems. On a funny note, when you cant think of any algorithm (irrespective of situation), use random forest!Random Forestis a versatile machine learning method capable of performing both regression and classification tasks.It also undertakesdimensional reduction methods, treats missing values, outlier valuesand other essential steps of data exploration,and does a fairly good job.It isa type of ensemble learning method, where a group of weak models combineto form a powerful model.In Random Forest, we growmultipletrees as opposedto a single tree in CART model (see comparison between CART and Random Forest here, part1 and part2).To classify a new object based on attributes, each tree gives a classification and we say the tree votes for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.It works in the following manner.Each tree is planted & grown as follows:Tounderstand more in detail about this algorithm using a case study, please read thisarticle Introduction to Random forest  Simplified.Random forests have commonly known implementations in R packages and Python scikit-learn. Lets look at the codeof loading random forest model in R and Python below:RCodeDefinition: The term Boosting refers to a family of algorithms whichconverts weak learner to strong learners.Lets understand this definition in detail by solving a problem of spam email identification:How would you classifyan email as SPAM or not? Like everyone else, our initial approach would beto identify spam and not spam emails using following criteria. If:Above, weve defined multiple rules to classifyan email into spam or not spam.But, do you think these rules individually are strong enough to successfully classifyan email? No.Individually, these rules arenot powerful enough to classify an email into spam or not spam.Therefore, these rules are called as weak learner.To convert weak learner to strong learner, well combine the prediction of each weak learner using methods like:For example: Above,we have defined 5 weak learners. Out of these 5, 3 arevoted asSPAM and 2 are voted as Not a SPAM. In this case, by default, well consider an email as SPAM because wehave higher(3) vote for SPAM.Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, How boosting identify weak rules?To find weak rule, we applybase learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.Heres another question which might haunt you, How do we choose different distribution for each round?For choosing the right distribution, here are the following steps:Step 1: Thebase learner takes all the distributions and assign equal weight or attention to each observation.Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, weapply the next base learning algorithm.Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs fromweak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting payshigherfocus on examples which are mis-classied or have higher errors by preceding weak rules.
There are many boosting algorithms which impart additional boost to models accuracy. In this tutorial, well learn about the two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost.Ive always admired the boosting capabilities that xgboost algorithm. At times, Ive found that it providesbetter result compared to GBM implementation, but at times you might find that the gains are just marginal. When I explored more about its performance and science behind its high accuracy, I discovered many advantages of Xgboost over GBM:Before we start working, lets quickly understand the important parameters and the working of thisalgorithm. This will be helpful for both R and Python users. Below is the overall pseudo-code of GBM algorithm for 2 classes:This is an extremely simplified (probably naive) explanation of GBMs working. But, it will help every beginners to understand this algorithm.Lets considerthe important GBMparameters used to improve model performance in Python:Apart from these, there are certain miscellaneous parameters which affect overall functionality:I know its a long list of parameters but I have simplified it for you inan excel file which you can download from thisGitHub repository.For R users, using caret package, there are 3 main tuning parameters:Ive shared the standard codes in R and Python. At your end, youll be required to change the value of dependent variable and data set name used in the codes below. Considering the ease of implementing GBM in R, one can easily perform tasks like cross validation and grid search with this package.XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Its feature to implement parallel computing makes itat least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.R Tutorial: For R users, this is a complete tutorial on XGboost which explains the parameters along with codes in R. Check Tutorial.Python Tutorial: For Python users, this is a comprehensive tutorial on XGBoost, good to get you started. Check Tutorial.Practice is the one and true method of mastering any concept. Hence, you need to start practicingif you wish to master these algorithms.Till here, youve got gained significant knowledge on tree based models along with these practical implementation. Its time that you start working on them. Here are open practice problems where you can participate and check your live rankings on leaderboard:Tree based algorithm are important for every data scientist to learn. In fact, tree models are known to provide the best model performance in the family of whole machine learning algorithms. In this tutorial, we learnt until GBM and XGBoost. And with this, we come to the end of this tutorial.We discussed about tree based modeling from scratch. We learnt the important of decision tree and how that simplistic concept is being used in boosting algorithms. For better understanding, I would suggest you to continue practicing these algorithms practically. Also, do keep note of the parameters associated with boosting algorithms. Im hoping that this tutorial would enrich you with complete knowledge on tree based modeling.Did you find this tutorial useful ? If you have experienced, whats the best trick youve used while using tree based models ? Feel free to share your tricks, suggestions and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/
"Qlikview (DAS)  Technical Lead  Mindtree  Bangalore/Chennai (7+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Decision Tree  Simplified!|AVP  Business Insights  HDFC Life  Mumbai|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Qlikview (DAS)  Technical LeadLocation  Bangalore/ChennaiAbout employer  MindtreeJob description:Provides technical leadership and assumes overall accountability for a successful implementation phase on all client projects. Oversees employee and partners, ensures solutions are properly architected and specified and partners with the project manager to ensure qualityResponsibility Qualification and Skills RequiredInterested people can apply for this job at this PAGE",https://www.analyticsvidhya.com/blog/2015/01/qlikview-das-technical-lead-mindtree-bangalorechennai-7-years-experience/
"AVP  Business Insights  HDFC Life  Mumbai|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Qlikview (DAS)  Technical Lead  Mindtree  Bangalore/Chennai (7+ years of experience)|Senior Data Scientist  CoreCompete  India (2-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  AVP  Business InsightsLocation MumbaiAbout employer  HDFC LifeJob description:Responsibility Qualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asAVP  Business Insights  HDFC Life  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/avp-business-insights-hdfc-life-mumbai/
"Senior Data Scientist  CoreCompete  India (2-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|AVP  Business Insights  HDFC Life  Mumbai|SAS Administrator  CoreCompete  India (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior Data ScientistLocation  IndiaAbout employer  CoreCompeteJob description:Our Data Scientists are a unique breed of individuals. Exceptionally qualified and multi-faceted, they bring a combinationAnalytical ExpertiseDomain ExpertiseTechnical ExpertiseValue OrientationQualification and Skills RequiredInterested people can apply for this job at this PAGE",https://www.analyticsvidhya.com/blog/2015/01/senior-data-scientist-corecompete-india-2-5-years-experience/
"SAS Administrator  CoreCompete  India (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Scientist  CoreCompete  India (2-5 years of experience)|SAS Analyst  Truven Health Analytics  Chennai (1-2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  SAS AdministratorLocation  IndiaAbout employer  CoreCompeteJob description:Join our seasoned team of SAS professionals to take your career in SAS to the next level. As a SAS administrator, you will face the most complex and up-to-date challenges. You will be responsible for helping our clients innovate using our Agile Analytics on Amazon service offering.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE",https://www.analyticsvidhya.com/blog/2015/01/sas-administrator-corecompete-india-2-4-years-experience/
"SAS Analyst  Truven Health Analytics  Chennai (1-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|SAS Administrator  CoreCompete  India (2-4 years of experience)|Marketing Data Analyst  Intel  Bangalore (7+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  SAS AnalystLocation  ChennaiAbout employer  Truven Health AnalyticsJob description:The Analyst, SAS provides SAS programming and data analysis support as part of team conducting outcomes, econometrics, and/or market research studies for pharmaceutical customers.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSAS Analyst  Truven Health Analytics  Chennai",https://www.analyticsvidhya.com/blog/2015/01/sas-analyst-truven-health-analytics-chennai-1-2-years-experience/
"Marketing Data Analyst  Intel  Bangalore (7+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|SAS Analyst  Truven Health Analytics  Chennai (1-2 years of experience)|Product Analyst  Analyttica  Bangalore (3-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Marketing Data AnalystLocation  BangaloreAbout employer  IntelJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/marketing-data-analyst-intel-bangalore-7-years-experience/
"Product Analyst  Analyttica  Bangalore (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Marketing Data Analyst  Intel  Bangalore (7+ years of experience)|Senior and Chief Data Scientists  Analyttica  Bangalore (5-15 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Product AnalystLocation  BangaloreAbout employer  AnalytticaJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asProduct Analyst  Analyttica  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/product-analyst-analyttica-bangalore-3-5-years-experience/
"Senior and Chief Data Scientists  Analyttica  Bangalore (5-15 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Product Analyst  Analyttica  Bangalore (3-5 years of experience)|Text Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior and Chief Data ScientistsLocation  BangaloreAbout employer  AnalytticaJob description:ResponsibilitiesQualification and Skills RequiredCompetency Requirements: TechnicalInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior and Chief Data Scientists  Analyttica  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/senior-chief-data-scientists-analyttica-bangalore-5-15-years-experience/
"Text Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior and Chief Data Scientists  Analyttica  Bangalore (5-15 years of experience)|Web Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Text Analytics ConsultantLocation  BangaloreAbout employer Angel and GenieJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asText Analytics Consultant  Angel and Genie  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/text-analytics-consultant-angel-genie-bangalore-4-5-years-experience/
"Web Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Text Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|Launch of learning path  Data Science in Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Web Analytics ConsultantLocation  BangaloreAbout employer Angel and GenieJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asWeb Analytics Consultant  Angel and Genie  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/web-analytics-consultant-angel-genie-bangalore-4-5-years-experience/
Launch of learning path  Data Science in Python,Learn everything about Analytics,"Why create learning paths?|Our first Learning path  Python for data science|End Note:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Web Analytics Consultant  Angel and Genie  Bangalore (4-5 years of experience)|Data Scientist  Target  Bangalore (3-5+ years of experience)|
Kunal Jain
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",We are jumping on our feets right now!,"Setting up your machine|Learn the basics of Python language|Learn Regular Expressions in Python|Learn Scientific libraries in Python  NumPy, SciPy, Matplotlib and Pandas|Effective Data Visualization|Learn Scikit-learn and Machine Learning|Practice, practice and Practice|Deep Learning|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|",No header found,"We cant find any other way to express our excitement. We said that 2015 is going to be a year when Analytics Vidhya will become the place to learn analytics and data science. We launched our discussion forums 2 weeks back and there are awesome discussions already happening there. Check them out here to see what you are missing.Today, we launch another milestone in data science learning  the learning paths. The aim of creating these learning paths is to take out the confusion from learning process. We livein a world of information overload.We come across people daily who are following too many things and chasing too many directions in their attempt to learn data science. It is hard to blame them, when there is so much content to absorb. These people start doing a MOOC  2 videos down the line, they switch on to a blog, then a notebook and end up reaching nowhere.These learning paths will provide crystal clear direction to your learning so that you can focus on learning the subject rather than worrying about what to learn. If you are a complete beginner on a topic, you couldnt have asked for a better roadmap  you will find the entire learning journey mapped in front of you. If you are some one who has already spent some time learning  you can pick up the journey from where you are and the remaining journey should be faster, clearer and more focused!The first learning path which we are launching is on Python for data Science. The learning path starts from why you should learn Python and goes on to provide the resource you need to become a Python machine learning expert! The Python learning path consists of 8 steps:If you are completely new to Python, you should just follow the steps as mentioned in the path. On the other hand, one of my friends has already learnt the basics of the language  he can directly pick things from step 3 onwards. If you already know Python and want to pick up machine learning, start from step 5 or 6.We are super excited about launching learning paths. We think that these should help our readers immensely. So check out the learning path and let us know how you plan to use them. There will be more learning paths, we will add in coming days. If you have any suggestions on this awesome initiative, please feel free to reach out to me and shout in the comments below.",https://www.analyticsvidhya.com/blog/2015/01/launch-learning-paths-data-science-python/
"Data Scientist  Target  Bangalore (3-5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Launch of learning path  Data Science in Python|Data Analytics Consultant  Clematis Technology  Pune (3-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  BangaloreAbout employer  TargetJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/data-scientist-target-pune-3-5-years-experience/
"Data Analytics Consultant  Clematis Technology  Pune (3-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Target  Bangalore (3-5+ years of experience)|Marketing Data Scientist  Adobe  Noida (5-7 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Analytics ConsultantLocation  PuneAbout employer  Clematis TechnologyJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr Associate  Analytics Consulting  Sigmoid Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/data-analytics-consultant-clematis-technology-pune-3-4-years-experience/
"Marketing Data Scientist  Adobe  Noida (5-7 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analytics Consultant  Clematis Technology  Pune (3-4 years of experience)|Lead Data Scientist  M.Paani  Mumbai (3-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Marketing Data ScientistLocation  NoidaAbout employer  AdobeJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asMarketing Data Scientist  Adobe  Noida,https://www.analyticsvidhya.com/blog/2015/01/marketing-data-scientist-adobe-noida-5-7-years-experience/
"Lead Data Scientist  M.Paani  Mumbai (3-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Marketing Data Scientist  Adobe  Noida (5-7 years of experience)|Junior Data Scientist  M.Paani  Mumbai|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Lead Data ScientistLocation  MumbaiAbout employer  M.PaaniJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asLead Data Scientist  M.Paani  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/lead-data-scientist-m-paani-mumbai-3-5-years-experience/
"Junior Data Scientist  M.Paani  Mumbai|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead Data Scientist  M.Paani  Mumbai (3-5 years of experience)|Data Scientist / PM Analytics  Adobe  Noida (3-5+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Junior Data ScientistLocation  MumbaiAbout employer  M.PaaniJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asJunior Data Scientist  M.Paani  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/junior-data-scientist-m-paani-mumbai/
Data Scientist / PM Analytics  Adobe  Noida (3-5+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Junior Data Scientist  M.Paani  Mumbai|Manager / Sr. Manager  Analytics  Fullerton India  Mumbai|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist / PM AnalyticsLocation  NoidaAbout employer  AdobeJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist / PM Analytics  Adobe  NoidaIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/01/data-scientist-pm-analytics-adobe-noida-3-5-years-experience/
"Manager / Sr. Manager  Analytics  Fullerton India  Mumbai|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist / PM Analytics  Adobe  Noida (3-5+ years of experience)|Lead  SEO and Analytics  Financial Express  Chennai|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Manager / Sr. Manager  AnalyticsLocation  MumbaiAbout employer  Fullerton IndiaJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asManager / Sr. Manager  Analytics  Fullerton India  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/manager-sr-manager-analytics-fullerton-india-mumbai/
"Lead  SEO and Analytics  Financial Express  Chennai|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager / Sr. Manager  Analytics  Fullerton India  Mumbai|Data Analyst  Indegene  Bangalore (1-2 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Lead  SEO and AnalyticsLocation  ChennaiAbout employer  Financial ExpressJob description:The Digital Marketing Lead is responsible for Google Analytics Implementations using GA API/ Google Tag Manager API for FE business portals, products and solutions. The person should be a strategist and a forward thinker in finding opportunities which involves supporting and improving company portals business analytics and insights.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asLead  SEO and Analytics  Financial Express  Chennai",https://www.analyticsvidhya.com/blog/2015/01/lead-seo-analytics-financial-express-chennai/
"Data Analyst  Indegene  Bangalore (1-2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead  SEO and Analytics  Financial Express  Chennai|Web Analyst  Cognizant  Bangalore (8-12 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data AnalystLocation  BangaloreAbout employer  IndegeneJob description:ResponsibilitiesQualification and Skills RequiredCHARACTERISTICS:Interested people can apply for this job can mail their CV to[emailprotected]with subject asData Analyst  Indegene  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/data-analyst-indegene-bangalore-1-2-years-experience/
"Web Analyst  Cognizant  Bangalore (8-12 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst  Indegene  Bangalore (1-2 years of experience)|Model performance metrics: How well does my model perform?  Part 1|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Web Analyst  Data ScientistLocation  BangaloreAbout employer  CognizantJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asWeb Analyst  Cognizant  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/web-analyst-cognizant-bangalore-8-12-years-experience/
11 Important Model Evaluation Metrics for Machine Learning Everyone should know,"Learn everything about Analytics|Overview|Introduction|Table of Contents|Warming up: Types of Predictive models|1. Confusion Matrix|2. F1 Score|3. Gain and Lift charts
|4. Kolomogorov Smirnov chart
|5. Area Under the ROC curve (AUC  ROC)|6. Log Loss|7. Gini Coefficient|8. Concordant  Discordant ratio|9. Root Mean Squared Error (RMSE)|10. Root Mean Squared Logarithmic Error|11. R-Squared/Adjusted R-Squared|12. Cross Validation|End Notes","|Advantages of using ROC|Adjusted R-Squared|Here is an example of scoring on Kaggle!|The concept : Cross Validation|k-fold Cross validation|How does this help to find best (non over-fit) model?|How do we implementk-fold with any model?|But how do we choose k?|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework|Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science|
Tavish Srivastava
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published in February 2016 and updated in August 2019. with four new evaluation metrics.The idea of building machine learning models works on a constructive feedback principle. You build a model, get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results.I have seen plenty of analysts and aspiring data scientists not even bothering to check how robust their model is. Once they are finished building a model, they hurriedly map predicted values on unseen data. This is an incorrect approach.Simply building a predictive model is not your motive. Its about creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check the accuracy of your model prior to computing predicted values.In our industry, we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model.After you are finished building your model, these 11 metrics will help you in evaluating your models accuracy. Considering the rising popularity and importance of cross-validation, Ive also mentioned its principles in this article.And if youre starting out your machine learning journey, you should check out the comprehensive and popular Applied Machine Learning course which covers this concept in a lot of detail along with the various algorithms and components of machine learning.When we talk about predictive models, we are talking either about a regression model (continuous output) or a classification model (nominal or binary output). The evaluation metrics used in each of these models are different.In classification problems, we use two types of algorithms (dependent on the kind of output it creates):In regression problems, we do not have such inconsistencies in output. The output is always continuous in nature and requires no further treatment.Illustrative ExampleFor a classification model evaluation metric discussion, I have used my predictions for the problem BCI challenge on Kaggle. The solution of the problem is out of the scope of our discussion here. However the final predictions on the training set have been used for this article. The predictions made for this problem were probability outputs which have been converted to class outputs assuming a threshold of 0.5.A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :The accuracy for the problem in hand comes out to be 88%. As you can see from the above two tables, the Positive predictive Value is high, but negative predictive value is quite low. Same holds for Sensitivity and Specificity. This is primarily driven by the threshold value we have chosen. If we decrease our threshold value, the two pairs of starkly different numbers will come closer.In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Sensitivity. Confusion matrix are generally used only with class output models.In the last section, we discussed precision and recall for classification problems and also highlighted the importance of choosing precision/recall basis our use case. What if for a use case, we are trying to get the best precision and recall at the same time? F1-Score is the harmonic mean of precision and recall values for a classification problem. The formula for F1-Score is as follows:Now, an obvious question that comes to mind is why are taking a harmonic mean and not an arithmetic mean. This is because HM punishes extreme values more. Let us understand this with an example. We have a binary classification model with the following results:Precision: 0, Recall: 1Here, if we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a dumb classifier which just ignores the input and just predicts one of the classes as output. Now, if we were to take HM, we will get 0 which is accurate as this model is useless for all purposes.This seems simple. There are situations however for which a data scientist would like to give a percentage more importance/weight to either precision or recall. Altering the above expression a bit such that we can include an adjustable parameter beta for this purpose, we get:Fbetameasures the effectiveness of a model with respect to a user who attaches  times as much importance to recall as precision.Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:Step 1 : Calculate probability for each observationStep 2 : Rank these probabilities in decreasing order.Step 3 : Build deciles with each group having almost 10% of the observations.Step 4 : Calculate the response rate at each deciles for Good (Responders) ,Bad (Non-responders) and total.You will get following table from which you need to plot Gain/Lift charts:This is a very informative table. Cumulative Gain chart is the graph between Cumulative %Right and Cummulative %Population. For the case in hand here is the graph :This graph tells you how well is your model segregating responders from non-responders. For example, the first decile however has 10% of the population, has 14% of responders. This means we have a 140% lift at first decile.What is the maximum lift we could have reached in first decile? From the first table of this article, we know that the total number of responders are 3850. Also the first decile will contains 543 observations. Hence, the maximum lift at first decile could have been 543/3850 ~ 14.1%. Hence, we are quite close to perfection with this model.Lets now plot the lift curve. Lift curve is the plot between total lift and %population. Note that for a random model, this always stays flat at 100%. Here is the plot for the case in hand :You can also plot decile wise lift with decile number :What does this graph tell you? It tells you that our model does well till the 7th decile. Post which every decile will be skewed towards non-responders. Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model. Else you might consider over sampling first.Lift / Gain charts are widely used in campaign targeting problems. This tells us till which decile can we target customers for an specific campaign. Also, it tells you how much response do you expect from the new target base.K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.For the case in hand, following is the table :We can also plot the %Cumulative Good and Bad to see the maximum separation. Following is a sample plot :The metrics covered till hereare mostly used in classification problems. Till here, we learnt about confusion matrix, lift and gain chart and kolmogorov-smirnov chart.Lets proceed and learn fewmore important metrics.This is again one of the popular metrics used in the industry. The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections.Lets first try to understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric.Hence, for each sensitivity, we get a different specificity.The two vary as follows:The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.Lets take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix :As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.Points to Remember:1.For a model which gives class as output, will be represented as a single point in ROC plot.2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.Why should you use ROC and not metrics like lift curve?Lift is dependent ontotal response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift.AUC ROC considers the predicted probabilities for determining our models performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the models capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing butnegative average of the log of corrected predicted probabilities for each instance.Let us calculate log loss for a few random values to get the gist of the above mathematical function:Logloss(1, 0.1) = 2.303Logloss(1, 0.5) = 0.693Logloss(1, 0.9) = 0.105If we plot this relationship, we will get a curve as follows:Its apparent from the gentle downward slope towards the right that the Log Loss gradually declines as the predicted probability improves. Moving in the opposite direction though, the Log Loss ramps up very rapidly as the predicted probability approaches 0.So, lower the log loss, better the model. However, there is no absolute measure on a good log loss and it is use-case/application dependent.Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes certainty of classification into account.Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :Gini = 2*AUC  1Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.This is again one of the most important metric for any classification predictions problem. To understand this lets assume we have 3 students who have some likelihood to pass this year. Following are our predictions :A  0.9B  0.5C  0.3Nowpicture this.if we were to fetch pairs of two from these three student, how many pairs will we have? We will have 3 pairs : AB , BC, CA. Now, after the year ends we saw that A and C passed this year while B failed. No, we choose all the pairs where we will find one responder and other non-responder. How many such pairs do we have?We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the probability of responder was higher than non-responder. Whereas discordant pair is where the vice-versa holds true. In case both the probabilities were equal, we say its a tie. Lets see what happens in our case :AB  ConcordantBC  DiscordantHence, we have 50% of concordant cases in this example. Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the models predictive power. For decisions like how many to target are again taken by KS / Lift charts.RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:RMSE metric is given by:where, N is Total Number of Observations.In case of Root mean squared logarithmic error, we take the log of the predictions and actual values. So basically, what changes are the variance that we are measuring. RMSLE is usually used when we dont want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.We learned that when the RMSE decreases, the models performance will improve. But these values alone are not intuitive.In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of 0.5. So the random model can be treated as a benchmark.But when we talk about the RMSE metrics, we do not have a benchmark to compare.This is where we can use R-Squared metric. The formula for R-Squared is as follows:MSE(model): Mean Squared Error of the predictions against the actual valuesMSE(baseline): Mean Squared Error of mean prediction against the actual valuesIn other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formulafor adjusted R-Squared is given by:k: number of featuresn: number of samplesAs you can see, this metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases.If R-Squared does not increase, that means the feature added isnt valuable for our model. So overall we subtract a greater value from 1 and adjusted r2, in turn, would decrease.Beyond these 11 metrics, there is another method to check the model performance. These 7 methods are statistically prominent in data science. But, with arrival of machine learning, we are now blessedwith more robust methods of model selection. Yes! Im talking about Cross Validation.Though, cross validation isnt a really an evaluation metric which is used openly tocommunicate model accuracy.But, the result of cross validation provides good enough intuitive result to generalize the performance of a model.Lets now understand cross validation in detail.Lets first understand the importance of cross validation. Due to busy schedules, these days I dont get much time to participate in data science competitions. Long time back, I participated in TFI Competition on Kaggle. Without delving into my competition performance, I would like to show you the dissimilarity between my public and private leaderboard score.For TFI competition, following were three of my solution and scores (Lesser the better) :You will notice that the third entry which has the worstPublic score turnedto be the best model on Private ranking. There were more than 20 models above the submission_all.csv, but I still chose submission_all.csvas my final entry (which really worked out well). What caused this phenomenon ? The dissimilarity in my public and private leaderboard is caused by over-fitting.Over-fitting is nothing but when you model become highly complex that it starts capturing noise also. This noise adds no value to model, but only inaccuracy.In the following section, I will discuss how you can know if a solution is an over-fit or not before we actually know the test results.Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.Above diagram shows how to validate model with in-time sample. We simply divide the population into 2 samples, and build model on one sample. Rest of the population is used for in-time validation.Could there be anegativeside of the above approach?I believe, a negative side of this approach is that we loose a good amount of data from training the model. Hence, the model is very high bias. And this wont give best estimate for the coefficients. So whats the next best option?What if, we make a 50:50 split of training population and the train on first 50 and validate on rest 50. Then, we train on the other 50, test on first 50. This way we train the model on the entire population, however on 50% in one go. This reduces bias because of sample selection to some extent but gives a smaller sample to train the model on. This approach is known as 2-fold cross validation.Lets extrapolate the last example to k-fold from2-fold cross validation. Now, we will try to visualize how does a k-fold validation work.This is a 7-fold cross validation.Heres what goes on behind the scene : we divide the entire population into 7 equal samples. Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). Then, at the second iteration we train the model with a different sample held as validation. In 7 iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power. Once we have all the 7 models, we take average of the error terms to find which of the models is best.k-fold cross validation is widely used to check whether a model is an overfit or not. If the performance metrics at each of the k times modelling are close to each other and the mean of metric is highest. In a Kaggle competition, you might rely more on the cross validation score and not on the Kaggle public score. This way you will be sure that the Public score is not just by chance.Coding k-fold in R and Python are very similar. Here is how you code a k-fold in Python :This is the tricky part. We have a trade off to choose k.For a small k, we have a higher selection bias but low variance in the performances.For a largek, we have a smallselection bias but highvariance in the performances.Think of extreme cases :k = 2 : We have only 2 samples similar to our 50-50 example. Here we build model only on 50% of the population each time. But as the validation is a significant population, the variance of validation performance is minimal.k = number of observations(n) : This is also known as Leave one out. We have nsamples and modelling repeated n number of times leaving only one observation out for cross validation. Hence, the selection bias is minimal but the variance of validation performance is very large.Generally a value of k = 10 is recommended for most purpose.Measuring the performance on training sample is point less. And leaving a in-time validation batch aside is a waste of data. K-Fold gives us a way to use every singe datapoint which can reduce this selection bias to a good extent. Also, K-fold cross validation can be used with any modelling technique.In addition, the metrics covered in this article are some of the most used metrics of evaluation in a classification and regression problems.Which metric do you often use in classification and regression problem ? Have you used k-fold cross validationbefore for any kind of analysis? Did you see any significant benefits against using a batch validation? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2015/01/model-performance-metrics-classification/
Comprehensive Introduction to merging in SAS,Learn everything about Analytics,"The need for joining / merging datasets:|DATA STEPS|ONE-to-ONE relationship|PROC SQL|End Note:-|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Model performance metrics: How well does my model perform?  Part 1|Domain Consultant  CRM Analytics  Housing.Com  Bangalore (2-6 years experience)|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In my previous article, Combining data sets in SAS  Simplified, we discussed three methods to combine data sets  appending, concatenating and Interleaving. In this article, we will look at the most common and frequently used method of combining data sets  MERGING or JOINING.Before jumping into the details, let us understand why we actually need joining / merging. Whenever we have some information split and available into two or more data sets and we want to combine them into a single dataset, we need to merge / join these tables. One of the main things to be kept in mind is that the merging should be based on common criteria or field. For example, In a retail company, we have a daily Transaction table (Table contains Products Detail, Sales Detail and Customers Detail) and Inventory table (which has Product Detail and Available quantity). Now, to have the information on Inventory or the availability of a product, what we need to do? Combine the Transaction table with Inventory table based on Product_Code and subtract the sold quantity from available quantity.Merging / Joining can be of various types and it depends on the business requirement and relationship between data sets. First, let us look at various kinds of relation between data sets can have.In SAS, we can perform Joining/ Mergingthrough various ways, here we will discuss the most common ways  Data Step and PROC SQL. In Data step, we use Merge statement to perform joins, where as in PROC SQL, we write SQL query. Let us discussData step first:Note:- Data sets must be sorted by common variable(s) and the name, type and length of the common variable should be same for all input data sets.Lets look at some scenarios for each of the relationships between input data sets.Scenario:-1 In below input data sets, you can see that there is one to one relationship between these two tables on Student_ID. Now we want to create data setMARKS, where we have all unique student_ids with respective marks of maths and physics. If student_id is not available in Mathematics table, then math_marks should have missing value and vice versa.Solution using Data Steps:-How it works:-You can perform a dry run to evaluate the result data set.Scenario2:-Based on input data sets of scenario-1, we want to create below output data sets.Solution using Data Steps:-Lets write code similar to scenario-1 with IN option.Above, you can see that we have used IN option with both input data sets and assigned values of these to temporary variables MATH and PHYS because these are temporary variable so we can not see those in output data set.I have shown you the table (PDV data) having variable value for all observation along with temporary variables. Now, based on these variable value, we can write a code for sub setting and JOIN operations as we need:ONE-to-MANY relationshipScenario  3Here we have two data sets, Studentand Examand we want to create a output data set Marks.Above you look at input data sets, there is one-to-many relationship between Student and Exam. Now if you want to create output data set Marks having individual observation for eachexam of students, those belongs to the STUDENT data set i.e Left Join.Solution using Data Steps:-In similar way, we can perform operation for Inner, Right and Full join for one-to-many relationship using IN operator.MANY-to-MANY relationshipScenario 4: Create a output data sets having all combination based on common field.You can also see that both input data sets has Many-to-Many relationship.Data steps does not perform MANY-to-MANY relationship, because it does not provideoutput as Cartesian product. When we merge table A and table B using data stepsthan output is similar as below snapshot.Above we have seen, how can we use data steps to merge two or more data sets having any of the relationship except MANY to MANY. Now we will look at PROC SQL methods to have solution for similar requirements.To understand join methodology in SQL, we need to understand Cartesian product first. Cartesian product is a query that has multiple tables in from clause and produces all possible combination of rows from input tables. If we have two tables with 2 and 4 records respectively, then using Cartesian product, we have a table with 2 X 4=8 records.SQL joins works for each of the relationship between data sets (One-to-One, One-to-Many and Many-to-Many). Lets look at how it works with types of joins.Syntax:-Select Column-1, Column-2,Column-n from table1 INNER/ LEFT/ RIGHT/ FULL JOIN table2ONJoin-condition <Other clauses>;Note:-Lets solve above requirements using PROC SQL.Scenario-1 :-This was an example of FULL Join, where all Student_IDs was required in output data set with respective MATH and PHYSICS marks.Above in output data set, you can see that Student_ID is missing for those student have appeared only for Physics exam. To solve it we will use a function COALESCE. It returns the value of first non missing argument from given variables.Syntax:-COALESCE (argument-1, argument-2,..argument-n)Lets modify above code:-
Scenario 2:-This was an example of INNER, Left and right Join. Here we are solving for Inner Join.Similarly we can do for left and right join.Scenario -3This was a problem of left join for ONE-to-MANY relationship.Scenario -4This was a problem of Many-to-MANY relationship. We have already discussed that SQL can produces Cartesian product that contains all combination of records betweentwo tables.Above we have looked at Proc SQL to join/ merge data sets.In this series of articles regarding combining data sets in SAS, we looked at various methods to combine data sets like appending, concatenating, interleaving , Merging. Particularly in this article, we discussed that depending on relationship between data sets, various kinds of joins and how can we solve it based on different scenarios. We have usedtwo methods (Data Steps and PROC SQL) to achieve results. We will look at efficiency of these methodsin one of the future article.Haveyou found this series useful? We have simplified a complex topic like Combining data setsand have tried to present it in understandable manner. If you need any more help with Combining data sets, please feel free to ask your questions through comments below.P.S. Have you joined Analytics Vidhya Discuss yet? If not, you are missing out on awesome data science discussions. Here are some of the discussions happening on SAS:1.Selecting variables and transferring them to new dataset in SAS2. Import first 20 records of excel to SAS3. Where statement not working in SAS",https://www.analyticsvidhya.com/blog/2015/01/introduction-merging-sas/
"Domain Consultant  CRM Analytics  Housing.Com  Bangalore (2-6 years experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Comprehensive Introduction to merging in SAS|Statistical Analyst  SAS/R/SPSS  Trendwise Analytics  Bangalore|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Domain Consultant  CRM AnalyticsLocation  BangaloreAbout employer  Housing.ComJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asDomain Consultant  CRM Analytics  Housing.Com  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/domain-consultant-crm-analytics-housing-com-bangalore-2-6-years-experience/
"Statistical Analyst  SAS/R/SPSS  Trendwise Analytics  Bangalore|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Domain Consultant  CRM Analytics  Housing.Com  Bangalore (2-6 years experience)|Hadoop Developers  Trendwise Analytics  Bangalore (1-3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Statistical Analyst  SAS/R/SPSSLocation  BangaloreAbout employer  Trendwise AnalyticsJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asStatistical Analyst  SAS/R/SPSS  Trendwise Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/statistical-analyst-sasrspss-trendwise-analytics-bangalore/
"Hadoop Developers  Trendwise Analytics  Bangalore (1-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Statistical Analyst  SAS/R/SPSS  Trendwise Analytics  Bangalore|Manager  Meritus  Hyderabad, Telangana|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Hadoop DevelopersLocation  BangaloreAbout employer  Trendwise AnalyticsJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asHadoop Developers  Trendwise Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/hadoop-developers-trendwise-analytics-bangalore-1-3-years-experience/
"Manager  Meritus  Hyderabad, Telangana|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Developers  Trendwise Analytics  Bangalore (1-3 years of experience)|Sr Associate  Analytics Consulting  Sigmoid Analytics  Bangalore (2+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ManagerLocation  Hyderabad, TelanganaAbout employer  MeritusJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asManager  Meritus  Hyderabad, Telangana",https://www.analyticsvidhya.com/blog/2015/01/manager-meritus-hyderabad-telangana/
"Sr Associate  Analytics Consulting  Sigmoid Analytics  Bangalore (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Meritus  Hyderabad, Telangana|Image processing and feature extraction using Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr Associate  Analytics ConsultingLocation  BangaloreAbout employer  Sigmoid AnalyticsJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr Associate  Analytics Consulting  Sigmoid Analytics  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/sr-associate-analytics-consulting-sigmoid-analytics-bangalore-2-years-experience/
Image processing and feature extraction using Python,Learn everything about Analytics,"Importing an Image|Understanding the underlying data|Converting Images to a 2-D matrix|Blurring an Image|Complete Code|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Sr Associate  Analytics Consulting  Sigmoid Analytics  Bangalore (2+ years of experience)|Engineer Big Data  GDS  Noida|
Tavish Srivastava
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"No doubt,the above picture looks like one of the in-builtdesktop backgrounds. All credits to my sister, who clicks weird things which somehow become reallytempting to eyes. However, we have been born in an era of digital photography, we rarely wonder how are these pictures stored in memory or how are the various transformations made in a photograph.In this article, I will take you through some of the basic features of image processing. The ultimate goal of this data massaging remains the same : feature extraction. But here we need more intensive data cleaning. But data cleaning is done on datasets , tables , text etc. How is this done on an image? We will look at how an image is stored on a disc and how we can manipulate an image using this underlying data?Importing an image in python is easy. Following code will help you import an image on Python :This image has several colors and many pixels. To visualize how this image is stored, think of every pixel as a cell in matrix. Now this cell contains three different intensity information, catering to the color Red, Green and Blue. So a RGB image becomes a 3-D matrix. Each number is the intensity of Red, Blue and Green colors.Lets look at a few transformations:As you can see in the above image, we manipulated the third dimension and got the transformation done. Yellow is not a direct color available in our dictionary but comes out as combination of red and green. We got the transformation done by setting up intensity of other colors as zero.Handling the third dimension of images sometimes can be complex and redundant. In feature extraction, it becomes much simpler if we compress the image to a 2-D matrix. This is done by Gray-scaling or Binarizing. Gray scaling is richer than Binarizing as it shows the image as a combination of different intensities of Gray. Whereas binarzing simply builds a matrix full of 0s and 1s.Here is how you convert a RGB image to Gray scale:As you can see, the dimension of the image has been reduced to two in Grayscale. However, the features are equally visible in the two images. This is the reason why Grayscale takes much lesser space when stored on Disc.Now lets try to binarize this Grayscale image. This is done by finding a threshold and flagging the pixels of Grayscale. In this article I have used Otsus method to find the threshold. Otsus method calculates an optimal threshold by maximizing the variance between two classes of pixels, which are separated by the threshold. Equivalently, this threshold minimizes the intra-class variance.Following is a code to do this transformation:Last part we will cover in this article is more relevant for feature extraction : Blurring of images.Grayscale or binary image sometime captures more than required image and blurring comes very handy in such scenarios. For instance, in this image if the shoe was of lesser interest than the railway track, blurring would have added a lot of value. This will become clear from this example. Blurring algorithm takes weighted average of neighbouring pixels to incorporate surroundings color into every pixel. Following is an example of blurring :In the above picture, after blurring we clearly see that the shoe has now gone to the same intensity level as that of rail track. Hence, this technique comes in very handy in many scenarios of image processing.Lets take a practical example of such application in analytics industry. We wish to count the number of people in a towns photograph. But this image has a few buildings also. Now the intensity of the people behind the buildings will be lower than building itself. Hence, it becomes difficult for us to count these poeple. Blurring in such scenarios can be done to equalize the intensities of buildings and people in the image.Here is the complete code :The world of image processing is already so rich that multi-billion dollar companies today rely on these image processing tools for various purposes. These image processing techniques are being used heavily in researches and automization of industry processes. In few of the coming articles we will take a deep dive into feature extraction from an image. This will include detecting corners, segmenting the image, seperating object from the background etc.Did you find the article useful? Share with us any practical application of image processing you have worked on.Do let us know your thoughts about this article in the box below.P.S. Have you joined Analytics Vidhya Discuss yet? If not, you are missing out on awesome data science discussions. Here are 2 of my best picks among recent discussions:1. How to do feature selection and transformation?2. Algorithm for time series forecasting",https://www.analyticsvidhya.com/blog/2015/01/basics-image-processing-feature-extraction-python/
"Engineer Big Data  GDS  Noida|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Image processing and feature extraction using Python|Celebrity and Data Analyst  Utsav Fashion  Delhi (3  5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Engineer Big DataLocation  NoidaAbout employer  GDSJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/engineer-big-data-gds-noida/
"Celebrity and Data Analyst  Utsav Fashion  Delhi (3  5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engineer Big Data  GDS  Noida|Celebrity and Analytic Lead  Utsav Fashion  Delhi (3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Celebrity and Data AnalystLocation  DelhiAbout employer  Utsav FashionJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asCelebrity and Data Analyst  Utsav Fashion  Delhi,https://www.analyticsvidhya.com/blog/2015/01/celebrity-data-analyst-utsav-fashion-delhi-3-5-years-experience/
"Celebrity and Analytic Lead  Utsav Fashion  Delhi (3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Celebrity and Data Analyst  Utsav Fashion  Delhi (3  5 years of experience)|Analytics Manager  GDS  Noida|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Celebrity and Analytic LeadLocation  DelhiAbout employer  Utsav FashionJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asCelebrity and Analytic Lead  Utsav Fashion  Delhi,https://www.analyticsvidhya.com/blog/2015/01/celebrity-analytic-lead-utsav-fashion-delhi-3-years-experience/
"Analytics Manager  GDS  Noida|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Celebrity and Analytic Lead  Utsav Fashion  Delhi (3+ years of experience)|Data Scientist  GDS  Noida|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analytics ManagerLocation  NoidaAbout employer  GDSJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/analytics-manager-gds-noida/
"Data Scientist  GDS  Noida|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Manager  GDS  Noida|Analyst- Digital Marketing Web Analytics  Deloitte  Hyderabad (2-3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  NoidaAbout employer  GDSJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/data-scientist-gds-noida/
"Analyst- Digital Marketing Web Analytics  Deloitte  Hyderabad (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  GDS  Noida|Data ArchitectStructured and Unstructured Data Analytics  Infosys  Bhubaneswar/Mysore (3-8+ years of experience)|
Jobs Admin
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Analyst- Digital Marketing Web AnalyticsLocation  HyderabadAbout employer  DeloitteJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE,https://www.analyticsvidhya.com/blog/2015/01/analyst-digital-marketing-web-analytics-deloitte-hyderabad-2-3-years-experience/
"Data ArchitectStructured and Unstructured Data Analytics  Infosys  Bhubaneswar/Mysore (3-8+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst- Digital Marketing Web Analytics  Deloitte  Hyderabad (2-3 years of experience)|Engineer  Hadoop Test/ QE  Altiscale  Chennai (2-3+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ArchitectStructured and Unstructured Data AnalyticsLocation  Bhubaneswar/MysoreAbout employer  InfosysJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData ArchitectStructured and Unstructured Data Analytics  Infosys  Bhubaneswar/Mysore,https://www.analyticsvidhya.com/blog/2015/01/data-architect-structured-unstructured-data-analytics-infosys-bhubaneswarmysore-3-8-years-experience/
"Engineer  Hadoop Test/ QE  Altiscale  Chennai (2-3+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data ArchitectStructured and Unstructured Data Analytics  Infosys  Bhubaneswar/Mysore (3-8+ years of experience)|Associate Analyst/Analyst  McKinsey  Gurgaon (2-4 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Engineer  Hadoop Test/ QELocation  ChennaiAbout employer  AltiscaleJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asEngineer  Hadoop Test/ QE  Altiscale  Chennai,https://www.analyticsvidhya.com/blog/2015/01/engineer-hadoop-test-qe-altiscale-chennai-2-3-years-experience/
"Associate Analyst/Analyst  McKinsey  Gurgaon (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engineer  Hadoop Test/ QE  Altiscale  Chennai (2-3+ years of experience)|Data Scientist  Adecco  Bangalore (3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Associate Analyst/AnalystLocation  GurgaonAbout employer  McKinseyJob description:The Analyst will be responsible for working with statisticians, actuaries, experts and consultants to understand the clients requirements, to design and to develop intelligent solutions using US Health Insurance databases.ResponsibilitiesQualification and Skills RequiredResult Orientation:Leadership:Interested people can apply for this job can mail their CV to[emailprotected]with subject asAssociate Analyst/Analyst  McKinsey  Gurgaon",https://www.analyticsvidhya.com/blog/2015/01/associate-analystanalyst-mckinsey-gurgaon-2-4-years-experience/
"Data Scientist  Adecco  Bangalore (3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Associate Analyst/Analyst  McKinsey  Gurgaon (2-4 years of experience)|Scikit-learn in Python  the most important Machine Learning tool I learnt last year!|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  BangaloreAbout employer  AdeccoJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asData Scientist  Adecco  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/data-scientist-adecco-bangalore-3-years-experience/
Scikit-learn in Python  the most important Machine Learning tool I learnt last year!,Learn everything about Analytics,"What is scikit-learn?|Components of scikit-learn:|Community / Organizations using scikit-learn:|Quick Example:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Adecco  Bangalore (3 years of experience)|ACN  Digital  Analytics  CMT  09-Consultant  Accenture  Bangalore (4-8 years of experience)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article went through a series of changes!I was initially writing ona different topic (related to analytics). I had almost finished writing it. I had put in about 2 hours and written an average article. If I had made it live, it would have done OK! But something in me stopped me from making it live. I was just not satisfied with the output. The article didnt convey how I am feeling about 2015 and how useful Analytics Vidhya could become for your analytics learning this year.So, I put that article in Trash and started re-thinking which topic would do the justice. This is what I ended up with  let me write awesome articles and guides about what was my biggest learning in 2014  The Scikit-learn library in Python. This was my biggest learning becauseit is now the tool I use for any machine learning project I work upon.Creating these articles would not only be immensely useful for readers of the blog, but would also challenge me in writing about something I am still relatively new at. I would also love to hear from you on the same  what was your biggest learning in 2014 and would you want to share it with readers of this blog?Scikit-learn is probably the most useful library for machine learning in Python. It ison NumPy, SciPy and matplotlib, this library contains a lot of effiecient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.Please note that scikit-learn is used to build models. It should not be used for reading the data, manipulating and summarizing it. There are better libraries for that (e.g. NumPy, Pandas etc.)Scikit-learn comes loaded with a lot of features. Here are a few of them to help you understand the spread:One of the main reasons behind using open source tools is the huge community it has. Same is true for scikit-learn as well. There are about 35 contributors to scikit learn till date, the most notable being Andreas Mueller (P.S. Andys machine learning cheat sheet is one of the best visualizations to understand the spectrum of machine learning algorithms).There are various Organiations of the likes of Evernote, Inria and AWeber which are being displayed on scikit learn home page as users. But I truly believe that the actual usage is far more.In addition to these communities, there are various meetups across the globe. There was also a Kaggle knowledge contest, which finished recently but might still be one of the best places to start playing around with the library.Machine Learning cheat sheet  see Original image for better resolutionNow that you understand the eco-system at a high level, let me illustrate the use of scikit learn with an example. The idea is to just illustrate the simplicity of usage of scikit-learn. We will have a look at various algorithms and best ways to use them in one of the articles which follow.We will build a logistic regression on IRIS dataset:Step 1: Import the relevant libraries and read the datasetimport numpy as npimport matplotlib as pltfrom sklearn import datasetsfrom sklearn import metricsfrom sklearn.linear_model import LogisticRegressionWe have imported all the libraries. Next, we read the dataset:dataset = datasets.load_iris()Step 2: Understand the dataset by looking at distributions and plotsI am skipping these steps for now. You can read this article, if you want to learn exploratory analysis.Step 3: Build a logistic regression model on the dataset and making predictionsmodel.fit(dataset.data, dataset.target)expected = dataset.targetpredicted = model.predict(dataset.data)Step 4: Print confusion metrixprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted))This was an overview of one of the most powerful and versatile machine learning library in Python. It was also the biggest learning I did in 2014. What was your biggest learning in 2014? Please share it with the group through comments below.Are you excited about learning and using Scikit-learn? If Yes, stay tuned for the remaining articles in this series.A quick reminder: If you have not checked out Analytics Vidhya Discuss yet, you should do it now. Users are joining in quickly  so take up that username you want before it gets picked up by some one else!",https://www.analyticsvidhya.com/blog/2015/01/scikit-learn-python-machine-learning-tool/
"ACN  Digital  Analytics  CMT  09-Consultant  Accenture  Bangalore (4-8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Scikit-learn in Python  the most important Machine Learning tool I learnt last year!|Domain Consultant  Analytics Quotient  Bangalore (2-3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ACN  Digital  Analytics  CMT  09-ConsultantLocation  BangaloreAbout employer  AccentureJob description:Resourceful and motivated individual for leading the CMT Analytics team. As a Consultant in CMT Analytics, we help the clients with Analytical solution at every stage with Complex machine/M2M learning algorithms & techniques. He/she should be strong in advance/predictive analytics.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at this PAGE",https://www.analyticsvidhya.com/blog/2015/01/acn-digital-analytics-cmt-09-consultant-accenture-bangalore-4-8-years-experience/
"Domain Consultant  Analytics Quotient  Bangalore (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|ACN  Digital  Analytics  CMT  09-Consultant  Accenture  Bangalore (4-8 years of experience)|Business Intelligence Architect (SAP BW / Qlikview)  Datwyler  Pune (7+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Domain ConsultantLocation  BangaloreAbout employer Analytics QuotientJob description:ResponsibilitiesQualification and Skills RequiredSALARY & OTHER DETAILSInterested people can apply for this job can mail their CV to[emailprotected]with subject asDomain Consultant  Aqinsights  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/domain-consultant-aqinsights-bangalore-2-3-years-experience/
"Business Intelligence Architect (SAP BW / Qlikview)  Datwyler  Pune (7+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Domain Consultant  Analytics Quotient  Bangalore (2-3 years of experience)|Sr. Data Scientist  Infoexcel  Hyderabad (4-9 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business Intelligence Architect (SAP BW / Qlikview)Location  PuneAbout employer  DatwylerJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asBusiness Intelligence Architect (SAP BW / Qlikview)  Datwyler Sealing  Pune,https://www.analyticsvidhya.com/blog/2015/01/business-intelligence-architect-sap-bw-qlikview-datwyler-pune-7-years-experience/
"Sr. Data Scientist  Infoexcel  Hyderabad (4-9 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Intelligence Architect (SAP BW / Qlikview)  Datwyler  Pune (7+ years of experience)|Manager  Analytics Engineering  Gap Inc.  Mumbai (4+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr. Data ScientistLocation  HyderabadAbout employer  InfoexcelJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr. Data Scientist  Infoexcel Consulting  Hyderabad,https://www.analyticsvidhya.com/blog/2015/01/sr-data-scientist-infoexcel-hyderabad-4-9-years-experience/
"Manager  Analytics Engineering  Gap Inc.  Mumbai (4+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Data Scientist  Infoexcel  Hyderabad (4-9 years of experience)|DBA  NoSQL  Machinepulse  India (2-5+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Manager  Analytics EngineeringLocation  MumbaiAbout employer  Gap Inc.Job description:The Analytics Engineering Manager for the Advanced Analytics Group at GID, leads a team of engineers that designs, implements and tests tools for the Data Scientist Team and its customers. The GID Advanced Analytics team is chartered with building analytical models that help in both decision making and customer targeting using both Mathematical / Statistical and Machine Learning approaches.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asManager  Analytics Engineering  Gap Inc.  Mumbai",https://www.analyticsvidhya.com/blog/2015/01/manager-analytics-engineering-gap-mumbai-4-years-experience/
"DBA  NoSQL  Machinepulse  India (2-5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Analytics Engineering  Gap Inc.  Mumbai (4+ years of experience)|Senior Analyst  Consulting  Infiniti-Research  Bangalore (3-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  DBA  NoSQLLocation  IndiaAbout employer MachinepulseJob description:The successful candidate will have a track record of operational excellence managing large scale production databases utilizing one or more of the following (MongoDB, Cloudera, Hortonworks, HBase, Cassandra). You will have excellent communication skills, a keen attention to detail, and a passion for learning new technology and building high quality, scalable software.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asDBA  NoSQL  Machinepulse  India",https://www.analyticsvidhya.com/blog/2015/01/dba-nosql-machinepulse-india-2-5-years-experience/
"Senior Analyst  Consulting  Infiniti-Research  Bangalore (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|DBA  NoSQL  Machinepulse  India (2-5+ years of experience)|Senior Analyst  Sourcing Intelligence and Analytics  Infiniti-Research  Bangalore (3-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Analyst  ConsultingLocation  BangaloreAbout employer Infiniti-ResearchJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analyst  Consulting  Infiniti-Research  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/senior-analyst-consulting-infiniti-research-bangalore-3-6-years-experience/
"Senior Analyst  Sourcing Intelligence and Analytics  Infiniti-Research  Bangalore (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  Consulting  Infiniti-Research  Bangalore (3-6 years of experience)|Sr Analyst / Manager  Data Science  Gap Inc.  Mumbai (3-6 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Analyst  Sourcing Intelligence and AnalyticsLocation  BangaloreAbout employer Infiniti-ResearchJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analyst  Sourcing Intelligence and Analytics  Infiniti-Research  Bangalore,https://www.analyticsvidhya.com/blog/2015/01/senior-analyst-sourcing-intelligence-analytics-infiniti-research-bangalore-3-6-years-experience/
"Sr Analyst / Manager  Data Science  Gap Inc.  Mumbai (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  Sourcing Intelligence and Analytics  Infiniti-Research  Bangalore (3-6 years of experience)|Business Leader, Campaign Analytics  Master Card  Gurgaon (7-12 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr Analyst / Manager  Data ScienceLocation  MumbaiAbout employer  Gap Inc.Job description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job can mail their CV to[emailprotected]with subject asSr Analyst / Manager  Data Science  Gap Inc.  Mumbai,https://www.analyticsvidhya.com/blog/2015/01/sr-analyst-manager-data-science-gap-mumbai-3-6-years-experience/
"Business Leader, Campaign Analytics  Master Card  Gurgaon (7-12 years of experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr Analyst / Manager  Data Science  Gap Inc.  Mumbai (3-6 years of experience)|Welcome 2015 with new, better and more helpful Analytics Vidhya|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Business Leader, Campaign AnalyticsLocation  GurgaonAbout employer  Master CardJob description:ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job at thisPAGEIf you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2015/01/business-leader-campaign-analytics-master-card-gurgaon-7-12-years-experience/
"Welcome 2015 with new, better and more helpful Analytics Vidhya","Learn everything about Analytics|If 2014 was big (for us), the plans for 2015 are grand!","So, the first thing we want to do in 2015 is to listen more, participate more in discussions, in knowledge creation and in turn help out the larger community.||Analytics Vidhya Discuss:|||Coming soon  Website re-design:|End Notes:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Business Leader, Campaign Analytics  Master Card  Gurgaon (7-12 years of experience)|Big Data Scientist  HP  Bangalore|
Kunal Jain
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Over the last 12 months, we went from a small, little known blog on analytics to one of the most engaging and helpful community in Data Science across the globe.Before I pull out any rabbits frommy hat, I want to thank our followers and audience. Theyhave been the driving force behind our existence, our growth and our inspiration. Nothing beats the satisfaction we get in seeing / meeting / talking to a satisfied reader!One of the things, which set us on this high growth trajectory isthat we have listened to our audience! We have listened to every comment on our blog, every mail in our inbox, every social media interaction and the list goes on! And, it has helped us immensely. Whenever we had to decide between A or B, or take any important decision  it has always been guided by what our audiencehave said.Whenever we are stuck with these questions, we always question ourselves  What is the best decision for our audience? And it has served us well. It goes without saying, that this is not changing in future.One of the things, which set us on this high growth isthat we have listened! We have listened to every comment on our blog, every mail in our inbox, every social media interaction and the list goes on. And it has helped us immensely!
Analytics Vidhya Discuss is a question and answer platform for data science professionals (or those who want to enter data science world). You can ask any question related to data science here. Here are a few examples:We are super excited and I am sure you would also be, once you look at our new Question and Answer platform  discuss.analyticsvidhya.com. This platform is one of the best platforms we have seen for asking questions and getting answers. It is super intuitive, easy to scroll and find relevant topics. We think that this platform should go on to become the place to discuss analytics and data science.So, what are you waiting for? Go and test out Analytics Vidhya DiscussWe started our journey as a blog and the focus at the start was to test out whether we can help people with their analytics learning. We never thought we would add so much content in so little time. We also hacked a blog platform and used it for training listings and job alerts. While all of this has helped our readers and us, it has also resulted in a cluttered website.So, we are re-designing the site.The site will get a new look shortly. We love the way new look is coming out and are working hard in tuning, fine-tuning and re-tuning it. It feels like we are dressing up a teen who is out to impress his / her friends in a party. Where do we put the social sharing buttons? How is the site looking on mobile devices? Different browsers? How do we ask users to login?Do lookout for the new look and let us know your feedback.We are starting the yearin style  by launching Analytics Vidhya Discuss and re-designing Analytics Vidhya. The idea is to make it better and more helpful. We dedicate both these changes to our audience for their support and patronage and hope that Analytics Vidhya Discuss turns out to be our biggest gift to data science community across the globe.Do check out these additions and let us know your feedback. As usual, we will be waiting to hear more from you.",https://www.analyticsvidhya.com/blog/2015/01/2015-new-helpful-analytics-vidhya/
