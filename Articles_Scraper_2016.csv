Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
Abinitio  Sr profile (The Lead/Architect/Designer)- Bangalore/Gurgaon ( 10+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Software Engineer  Machine Learning- Bangalore (3+ Years of Experience)|Abinitio  Sr Architect- Bangalore/Gurgaon (12+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : Role:Abintiio Data Warehouse Tech Lead/ ArchitectResponsibilities :ETL Design and development using AbinitioTake ownership of architecting and designing solutionsJob Requirements (Technical + Managerial skills required)System Analysis & DesignStrong Abinitio ETL design skillsInvolved in Ab Initio Design, Configuration experience in Ab Initio ETL, Data Mapping, Transformation and Loading in complex and high-volume environment and data processing at Terabytes level.Capacity of designing solutions around AbInitio, with advanced skills in high performance and parallelism in AbInitioDataWarehousing implementation experienceData Modeling experience.Knowledge of Oracle 8i/9iStrong analytical & problem solving skillsStrong UNIX, korn shell scripting experience.Strong interpersonal / communication skillsExperience in leading a team of ETL developersExperience in co-ordinating with offshore on development / maintenance projectsSkill RequiredAbinitioOracle 8i/9i RDBMS, PLSQL, Database Stored ProceduresScripting skills with Unix Korn shell, PerlGood experience in understanding business requirements and translating these to schema requirements, good understanding of data profiling, meta data, ETL and reporting
College Preference : no-bar
Min Qualification : ug
Skills : etl, oracle, reporting, unix
Location : Bengaluru, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/abinitio-sr-profile-the-leadarchitectdesigner-bangaloregurgaon-10-years-of-experience/
Abinitio  Sr Architect- Bangalore/Gurgaon (12+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Abinitio  Sr profile (The Lead/Architect/Designer)- Bangalore/Gurgaon ( 10+ Years of Experience)|Senior Business Analyst-Bangalore (4+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 12  years
Requirements : 
Task Info : Role:Abintiio Data Warehouse Senior ArchitectResponsibilities :Current state analysis, technical evaluation, architecture and design of scalable large scale ETL solution in a multi-platform environment.Enterprise ETL architecture in the DW contextFuture state and current state (including the legacy platforms) gap analysisOptimization and Consolidation of technology roadmap and executionA go to person for any complex problems requiring an architectural solutionJob Requirements (Technical + Managerial skills required)System Analysis & DesignStrong Abinitio ETL design skillsInvolved in Ab Initio Design, Configuration experience in Ab Initio ETL, Data Mapping, Transformation and Loading in complex and high-volume environment and data processing at Terabytes level.Capacity of designing solutions around AbInitio, with advanced skills in high performance and parallelism in AbInitioDataWarehousing implementation experienceData Modeling experience.Knowledge of Oracle 8i/9iStrong analytical & problem solving skillsStrong UNIX, korn shell scripting experience.Strong interpersonal / communication skillsExperience in leading a team of ETL developersExperience in co-ordinating with offshore on development / maintenance projectsSkills RequiredAbinitioOracle 8i/9i RDBMS, PLSQL, Database Stored ProceduresScripting skills with Unix Korn shell, PerlGood experience in understanding business requirements and translating these to schema requirements, good understanding of data profiling, meta data, ETL and reporting
College Preference : no-bar
Min Qualification : ug
Skills : etl, oracle, reporting
Location : Bengaluru, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/abinitio-sr-architect-bangaloregurgaon-12-years-of-experience/
Senior Business Analyst-Bangalore (4+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Abinitio  Sr Architect- Bangalore/Gurgaon (12+ Years of Experience )|Abinitio Architect & Abinitio Technical Lead- Gurgaon/ Bangalore (10+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : A typical assignment handled by an SBA and his role:Starting with understanding the business problem, the SBA works with the Consultants & Managers to create the solution framework, followed by the analysis framework. This is followed by the BA managing his part of the work.The SBA is deeply knowledgeable of different analytics techniques and can critique among different techniques to come up with the best fit one. This entails the SBA spending a good amount of time researching and learning about new tools and techniques not only from his seniors in the company but also through great deal of self-learning. The SBA also needs to understand the client business and his domain quite well to be able to design meaningful solutions.The last but very important aspect of an SBAs work is to be able to manage a part of the work in global delivery model, ensuring smooth communication across borders.Primary Role  Senior Business AnalystFormulating the Analytical Framework for AnalysesManage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysisAnalyse big data using statistics, econometrics, mathematics, operations research, and text mining techniquesLeverage technology to enhance the competitive position of clients by identifying appropriate solutionsDeveloping sophisticated analytical solutions to solve business problems using advanced statistical methodsDrawing actionable insights from the conducted analysisMaking business inference out of the analyses done to provide value to the Client teamsEngaging with the Clients, understanding their requirements and fulfilling them to their utmost satisfactionAiding Consultants & Managers in managing the teamHelping the Consultants/Engagement Manager with Business Development initiativesAdditional Responsibilities at client siteoAiding the onsite consultant on running analysis at client siteoAiding the onsite consultant on managing client communicationoAiding communication between onsite and offshore teamsoActing as onsite SME and Data Expert CompetencesRequired:Technical Capabilities oStatistical Packages (Atleast R, RapidMiner, Weka, SAS, SPSS, KNIME)oAnalytical Techniques (Atleast regression, clustering, machine learning algorithms)oData Management (Atleast SQL and One reporting tool)OthersoAbility to think strategically and analytically in order to effectively assess each assignmentoExcellent written and oral communication skillsoAbility to work in tight deadlines & under pressureoExcellent interpersonal & organizational skillsoGood listening and comprehension skillsoExcellent time management skills
College Preference : no-bar
Min Qualification : ug
Skills : clustering, machine learning, r, regression, sas, sql, statistics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/senior-business-analyst-bangalore-4-years-of-experience/
Abinitio Architect & Abinitio Technical Lead- Gurgaon/ Bangalore (10+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst-Bangalore (4+ Years of Experience )|Data Scientist  Marketing  Bangalore (4-9+ years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : More than 10 years of work experience in ETL technologies mostly on AbinitioStrong Data warehousing & Business Intelligence concepts and good Analytical skillsAt Least 2 year working experience as Abinitio ArchitectExpertise in Abinitio components, Parallelism, Dependency Analysis, MetaProgramming, PDL, EME commands, generic graphs etc.Should have extensive knowledge about Abinitio Architecture, bestpractices and performance tuning techniques.Experience in ICFF, Data profiling, ConductIT, ExpressIT (ACE / BRE),Continuous flow, XML processing are big plus.Should have experience in configuring EME and MFS.Strong experience in SQL and Unix shell scriptingExperience in any of the scheduling tools like Autosys, Control-M etc.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business intelligence, etl, programming, sql, unix, xml
Location : Bengaluru, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/abinitio-architect-abinitio-technical-lead-gurgaon-bangalore-10-years-of-experience/
Data Scientist  Marketing  Bangalore (4-9+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Abinitio Architect & Abinitio Technical Lead- Gurgaon/ Bangalore (10+ Years of Experience )|Manager- Bangalore (8+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  9 years
Requirements : 	Ability to learn new analytical methods and technologies and apply in practical business problems
	Ability to independently drive innovative programs
	Ability to work in an agile mode
	Ability to consultant as a subject matter expert on complex client projects
	Good interpersonal, problem solving, reasoning and analytical skills
	Train junior team members on new capabilities
	Excellent verbal and written communications skills
Task Info : Data scientist will be part of Solutions Team based out of India and will work closely work with partners, principals, consultants and data engineers to build different scales of analytical solutions involving variety of data sources and technology for CPG and Retail clientsQualification and Skills RequiredMarketing:Experienced in consumer and brand analytics, campaign analysis, concept testing of products, competitor analysis, promotion analysis, etc.Exposure with consumer survey questionnaire designs and/or data structure (if using syndicated data) is preferred.Experience in the following areas: segmentation, clustering, discrete choice modeling, design of experiment, associative mining, latent class modeling, etc.Expertise in SAS/R must Desired skills and experienceBachelors (+9 years of relevant exp), Masters (+7 years of relevant exp), Ph.D. (+4 years of relevant experience) in Engineering, Operation Research, Mathematics, Statistics, Marketing, preferredExposure to product assortment planning, consumer choice models, KANO, Shapley and Turf(Total Unduplicated Reach & frequency) is added advantageCPG and/or Retail experience preferredDemonstrate ability to work with ambiguous problem definitions, recognize dependencies and deliver impactful solutions through logical problem solving and technical ideationsAbility to learn new analytical methods and technologies and apply in practical business problemsAbility to independently drive innovative programsAbility to work in an agile modeAbility to consultant as a subject matter expert on complex client projectsGood interpersonal, problem solving, reasoning and analytical skillsTrain junior team members on new capabilitiesExcellent verbal and written communications skillsWhat will make you successful?Ability to manage stakeholdersAbility to make sense of ambiguityAbility to apply first principles and structured approaches to problem solving than over solely relying on excessively on past domain expertise alone. Innovation / Look for Ideas way to solve problem.Ability to work with diverse skills/ background of people
College Preference : no-bar
Min Qualification : ug
Skills : clustering, marketing analytics, modeling, r, sas, segmentation, testing
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-scientist-marketing-bangalore-4-9-years-of-experience/
Manager- Bangalore (8+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Marketing  Bangalore (4-9+ years of experience)|Consultant- Bangalore ( 3+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  years
Requirements : 
Task Info : A typical assignment handled by a Manager and his role:To coordinate and manage the data collection, implementation and deployment of quantitative analytical applications/frameworks to enhance the quality of clients management information for business decision-making. The Manager (Management Analytics) will use industry-standard project management techniques to meet deadlines, budget constraints and technical requirements. The Manager will also be required to work on growing the business is getting from the Client assigned to this person.The Manager (Management Analytics) is required to have technical expertise in basic statistical (i.e., quantitative) analytical techniques and business knowledge in different industries. In addition, they must guide and mentor Consultants, Business Analysts and Analysts in different but engagement-related disciplines. This person will need to draw on complex quantitative analysis expertise based on education and business experience in the following areas: critical reasoning, team management, attention to detail, pre-submission quality assurance of project report, and organization of client meetings.Primary Role  ManagerProvide analytical architecture analysis, design, development, and enhancementsDevelop analytical framework with the help of offshore team for the project.Prepare the data dictionary for the list of variables from the clients data.Finalize the list of variables for modeling based on business and technical aspects.Define and identify appropriate analytical model for the project.Develop business insights based on the output of quantitative models relevant to clients business needs.Prepare the benchmarking report (technical report) for to analyze and compare the results of different models/variables.Presentation of final results to the client and discuss further opportunities within/outside the project. Project Definition & ManagementServe as Project Lead or Project Manager for the entire life cycle of the projects.Maintain project documentation and reports.Plan deliverables and milestones for different projects that you are responsible for.Provide business analysis and business area assessment.Lead Project kick-off and Project Closure including (on per need basis) board-level progress and status reporting.Facilitate meetings within the team and also with developers, users and external vendors.Track and report team hours.Analyze, drive and re-engineer existing business processes to reshape the organization structure to better adapt future business needs.Develop business opportunity within the client responsible for.Develop business opportunity in nearby areas outside the current clientele.Define KPIs and SLAs for projects.Support dashboards design for BIs. Business Development: Talk with the different teams within the Client and work with them to support their Analytics initiatives and in turn grow the business for AffineCompetences Required:A Masterss degree in any field providing extensive exposure to quantitative analysis and quantitative methods, such as Statistics, Engineering, a quantitative field of Business, or similar job-relevant fieldAt least 8 years relevant experience collecting, analyzing and presenting analytics-based recommendations to businessesDirection and overseeing of project teams consisting of Affine and/or client personnelProficiency in analytical tools such as but not limited to R/SAS/SQLDemonstrated understanding of statistical techniques (Hypothesis Testing, Regression Analysis etc.)Must have worked on large datasets as the person will be responsible for manipulating, extracting, transforming, and analyzing the large data to develop solutions and insightsDomain knowledge in Retail/CPG, and/or Banking and Finance, and/or Pharmaceutical, Technology/Media, and/or eCommerce, etc.
College Preference : no-bar
Min Qualification : pg
Skills : banking, ecommerce, r, regression, retail banking, sas, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/manager-bangalore-8-years-of-experience/
Consultant- Bangalore ( 3+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager- Bangalore (8+ Years of Experience )|Text Mining/ Text Analytics/ NLP- Sr. Resource- Gurgaon ( 6+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : A typical assignment handled by a Consultant and his role:To coordinate and manage the implementation and deployment of analytical applications/frameworks, while using project management techniques in order to meet deadlines, budget constraints and technical requirements. Consultants are required to have technical expertise in basic statistical analytical techniques and business knowledge in different industries. Also, they should be able to guide and mentor the analysts. Consultants holding bachelors degree are more likely to bring the following skill sets of critical reasoning, team management, and attention to detail, quality assurance and organizing client meetings. They are also required to be inquisitive and persuasive for effective project delivery.Primary Role  ConsultantProvide analytical architecture analysis, design, development, and enhancementsDevelop analytical framework with the help of offshore team for the project.Prepare the data dictionary for the list of variables from the clients data.Finalize the list of variables for modeling based on business and technical aspects.Define and identify appropriate analytical model for the project.Develop business insights based on the output of model relevant to clients business needs.Prepare the benchmarking report (technical report) for to analyze and compare the results of different models/variables.Presentation of final results to the client and discuss further opportunities within/outside the project.Project Definition and Management:Serve as Project Lead or Project Manager for the entire life cycle of the projects.Maintain project documentation and reports.Plan deliverables and milestones for different projects that you are responsible for.Provide business analysis and business area assessment.Lead Project kick-off and Project Closure including (on per need basis) board-level progress and status reporting.Facilitate meetings within the team and also with developers, users and external vendors.Track and report team hours.Analyze, drive and re-engineer existing business processes to reshape the organization structure to better adapt future business needs.Develop business opportunity within the client responsible for.Develop business opportunity in nearby areas outside the current clientele.Define KPIs and SLAs for projects.Support dashboards design for BIs.Competences Required:An undergraduate degree in Computer Science, Electrical Engineering or similar job-relevant field from a premier institute.Direction and overseeing of project teams consisting of five personnel or more.Proficiency in analytical tools like R/SAS/SQL.Demonstrate an understanding of statistical techniques (Hypothesis Testing, Regression analysis etc.)Must have worked on large datasets as the consultant will be responsible for manipulating, extracting, transforming, and analyzing the large data to develop solutions and insights.Should have domain knowledge in either of the following  Retail/CPG, Banking & Finance, Pharmaceutical, Technology/Media, eCommerce etc.
College Preference : tier1-any
Min Qualification : ug
Skills : banking, ecommerce, r, regression, retail banking, sas, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/consultant-bangalore-3-years-of-experience/
Text Mining/ Text Analytics/ NLP- Sr. Resource- Gurgaon ( 6+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant- Bangalore ( 3+ Years of Experience )|Graph DB & Data Visualizer- Gurgaon (3+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  years
Requirements : 
Task Info : What we want you to do:Understand customer business use cases and be able to translate them to analytical data applications and models with a vision on how to implement.Design, develop, and test applications for text processing, such as name or entity matching, text categorization/routing, named-entity extraction, sentiment analysisIdentify enhancements that can help to improve the productivity of the team and to improve the precision and recall scores for the categoriesResearching and recommending machine learning algorithms and data sets for advancing the state-of-the-art techniques for ""entity resolution"" (Named Entity Recognition, co-reference, anaphora, etc.)Develop and implement the solution using Java technology stacks.Skills We Think You Need to Do This:6+ professional experience with core skillsets include semantic technologies (RDF,OWL), knowledge representation, natural language processing, Text Mining, search algorithm development and development in Java/J2EE/Scala.Good understanding and implementation of graph analytics and graph algorithm.Experience with Big Data execution using Hadoop / Horton Works.Experience with text mining using GATE or UIMA.Experience with Open source NLP libraries e.g. Corenlp, Opennlp, mallet, etc.Good Knowledge of indices such as Apache Solr, Lucien and Elastic Search will be plus.Good Knowledge of real time data streaming and offline data Streaming
College Preference : no-bar
Min Qualification : ug
Skills : graph analysis, java, nlp, text mining
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/text-mining-text-analytics-nlp-sr-resource-gurgaon-6-years-of-experience/
Graph DB & Data Visualizer- Gurgaon (3+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Text Mining/ Text Analytics/ NLP- Sr. Resource- Gurgaon ( 6+ Years of Experience )|Semantic Web Experts- Gurgaon (2+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Graph DB-We are creating a niche machine learning based problem solving platform targeted at investment decision making.The platform uses big data architecture with semantic computing to process structured & unstructured data.Skills Required:Neo4JRDFSolrBig data technologies like Hadoop, NoSQL, MapReduce, MongoDB, Cloud Computing etc.Analytical Tools like Advanced Excel, SAS, R, Matlab, Weka etc.Data Visualization developer:Role & Responsibilities:The Data Visualization Developer will be responsible for the design and development of visualizations of new high-impact visual applications to make data actionable and facilitate decision making for the clientsApplies design and data analysis techniques to organize the presentation of data in visually innovative ways in order to make it easier to understand, insightful, and actionable by end users.Lead the complete lifecycle of visual analytical applications; from development of mock-ups & storyboards to complete production ready application.Supports the Business Intelligence/Analytics data analyst and product manager with the understanding of business and data requirements, and iteratively designs visualizations for initial concepts and prototypesCollaboratively identifies the best means to visually depict the intermediate as well as final data analytics results in ways to provide effective process for mining new insights and assisting in decision making for solving complex problems.Manages a repository of re-usable data visualization templates and views.Skills and experience required:Experience in data visualization & data analysisExperience in Javascript, D3, HTML5, CSS3,Angular JS, JSP, XMLStrong experience designing and presenting complex data sets in visually compelling formats.Experience in developing many different types of visualizations, including visual analytics; real-time visualization for situation awareness; visualizations for interactive data exploration; narrative / editorially-guided visualizations;Advanced skills in working with business intelligence visualization tools such as Tableau, Pentaho, or experience with other data discovery, visualization, and/or infographic technologies (e.g. Tibco Spotfire, Qlikview, etc.)Proficiency with SQL is required.Experience with other data manipulation technology is helpfulPassion for transforming large amount of data into compelling visual stories to facilitate analysis and decision-makingAbility to balance attention to detail with prioritizing work in order to meet deadlines.
College Preference : no-bar
Min Qualification : ug
Skills : cloud, d3.js, data analysis, datavisualization, excel, hadoop, java script, matlab, nosql, pentaho, qlikview, r, sas, sql, tableau, xml
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/graph-db-data-visualizer-gurgaon-3-years-of-experience/
Semantic Web Experts- Gurgaon (2+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Graph DB & Data Visualizer- Gurgaon (3+ Years of Experience )|Top 35 Articles and Resources from Analytics Vidhya for the year 2016|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : Headquartered in US, the organisation is a strategy consulting and a big data & analytics focused firm. We bring together the best analytical minds worldwide, deliver a robust offering of knowledge services, and work as genuine partners in enhancing and implementing our clients growth strategies. Our clients vary from Fortune 500 companies to private equity to government entities.It is a Niche Machine Intelligence based problem solving company. We solve problem in the areas of investment and strategic decision making. We work on some of the niche technologies like Semantic Web, Graph Analytics, Text Analytics and MLEmployees get to work on technologies such as Neo4J, RDF, Solr and many other Big Data Technologies JOBDESCRIPTIONHands on experience in developing Semantic Web and Linked Data applicationsSkills:RDF/OWLOntology DevelopmentJava/Python/XMLSPARQLGraph DBJava or Python framework (e.g. Apache Jena) to develop Semantic Web and Linked Data applications
College Preference : no-bar
Min Qualification : ug
Skills : java, python, xml
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/semantic-web-experts-gurgaon-2-years-of-experience/
Top 35 Articles and Resources from Analytics Vidhya for the year 2016,"Learn everything about Analytics|Introduction|Bye bye 2016, Welcome 2017|How to consume this article?|Comprehensive Guides|Articles|Books / Courses|Career|Skill tests|End Notes","|1.A Complete Tutorial to learn Data Science in R from Scratch|2.A Complete Tutorial to learn Data Science in Python from Scratch|3.A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)|4. A Comprehensive beginners guide to Create Time Series Forecast (with Codes in Python)|5.Practical Guide to Principal Component Analysis (PCA) in R & Python|6. Complete Guide to Parameter Tuning in XGBoost (with codes in Python)|7.A Complete Tutorial on Ridge and Lasso Regression in Python|8.Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python|9.A Comprehensive Guide to Data Exploration|10. A Comprehensive beginners guide to start ML with Amazon Web Services (AWS)|1.12 Useful Pandas Techniques in Python for Data Manipulation|2.How to use XGBoost Algorithm in R in easy steps|3.7 Important Model Evaluation Error Metrics Everyone should know|4.Bayesian Statistics explained to Beginners in Simple English|5.Tutorial on 5 Powerful R Packages used for imputing missing values|6.Quick Guide to Build a Recommendation Engine in Python|7.Practical Guide to deal with Imbalanced Classification Problems in R|8.Practical Guide to implementing Neural Networks in Python (using Theano)|9.How to use Multinomial and Ordinal Logistic Regression in R?|10.How to perform feature selection (i.e. pick important variables) using Boruta Package in R?|1. Free Must Read Books on Statistics & Mathematics for Data Science|2. 19 Data Science Tools for people who arent so good at Programming|3.Most Active Data Scientists, Free Books, Notebooks & Tutorials on Github|4.18 New Must Read Books for Data Scientists on R and Python|5.Top Certification Courses in SAS, R, Python, Machine Learning, Big Data, Spark ( 2015-16 )|1. The Ultimate Plan to Become a Data Scientist in 2016|2.40 Interview Questions asked at Startups in Machine Learning / Data Science|3.10 Analytics / Data Science Masters Program by Top Universities in the US|4.India Exclusive: Analytics and Big Data Salary Report 2016|5.This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume|1.Skill test  Machine Learning|2. Skill test  Statistics(First and Second)|3. Skill test  R & Python|4.Skill test  Regression|5.Skill test  Tree-based algorithms|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Semantic Web Experts- Gurgaon (2+ Years of Experience)|Data Science Intern  Analytics Vidhya  Gurgaon|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Reflection time! Yes  it is that time of the year, when you stand and look back. You take a small pause, soak the environment around you, take a deep breath and look atthe path you just traveled. You feel a sense of accomplishment, fulfilment and satisfaction. Then, you turn around and look at the path ahead  set your eyes firmly back on your vision and resume your journey knowing you will be closer to the vision in the months to come!As a reflection of 2016, we have curated the best of resources from Analytics Vidhya. Have a look and see, if you have missed on any of these nuggets of gold. For better experience, we have divided the article in varioussections  comprehensive guides, articles, career related articles and skill tests.As a beginner, youwill love this post  it has the summary of all the hard work we have put through the year. I wish some one had provided such resources at the start of my career. As a professional, you can pick and choose what interests you.For us, 2016 has been phenomenal  we grew 3x in terms of traffic, our user base grew 10x (though on a small base), our hackathons continue to be the intense problem solving sessions and skill tests continue to provide community a testing ground to assess themselves. We started meetups, webinars and AMAs to provide our community with industry interactions.I can only thank you all for the love, support, feedback and suggestions you have provided. I also want to thank the team at Analytics Vidhya, our families, the unnamed volunteers who help us relentlessly and our supporters for this phenomenal year. We couldnt have asked for a better year.As I look forward, 2017 looks extremely exciting and happening for us. We just launched a revamped job portal and there are several new initiatives being planned. You will see them shaping up during the year. We hope to hear more and interact more with each one of you, we hope to provide you with unmatched learning opportunities and we will leave no stone unturned to provide a boost to your career this year.With that thought, we wish you a very happy new year. Stay warm and enjoy the new year eve with your family, friends and fellowAVians.If you look at this article and feel there is a ton of info  there is! We worked hard through the year to get you the best of resources. So, if you have not read these articles before and are going through them for the first time, take them bit by bit. Start with comprehensive guides from scratch and move one step at a time.If you have ready some of the articles before, you will find it relatively easy  but the principle still applies!For a complete beginner in R, if there is one resource you can read  read this resource. The article assumes no background in machine learning, provides basics of R, performs exploratory analysis & data manipulation on a dataset and ends up with building a predictive model on a dataset. I assure you this is one of the best hands-on guides to learn data science & machine learning in R.Tool: RTechniques: Complete case study on a datasetLevel: BeginnerIf you want to start your machine learning and data science journey in Python, this is the place to start. The guide assumes no prior knowledge in Python. It starts with basics of Python language, provides details ofpopular libraries in data science and data structures in Python. Once the basics are covered, a case study is used to show data exploration, data munging and predictive model building.Tool: PythonTechniques: Complete case study on a dataset includingLogistics Regression, Decision Tree and Random Forest.Level: BeginnerThis guide will teach you Tree based algorithms from scratch. Algorithms like decision tree, random forest and gradient boosting are widely used to solve several data science problems. Hence, it is important for any analyst to have athorough understanding of them. In this guide, you will learn about these algorithms and how they are being used in modeling. This guide assumes no prior knowledge of machine learning, but one must have familiarity with R or Python.Tools: R & PythonTechniques: Tree based algorithmsLevel: IntermediateTime series is animportant concept in data science. This guide will walk you through various techniques of time series with end-to-end problem solving along with codes in Python. You will learn about what makes time series special, loading & handling time series in Pandas, how to check stationarity of time series, how to make time series stationary and forecasting a time series. By the end of this guide, you will be able to forecast using time series techniques.Tools:PythonTechniques: Time series forecastingLevel: IntermediateSometimes you might come across a dataset which happens to have too many variables. To find right variables for computation purpose can be both confusing and cumbersome. To tackle this problem you have Principal Component Analysis (PCA) at your rescue. Principal component analysis is a method ofextracting important variables from a large set. In this guide, you will learn what are principle components, normalization of variables, implementation of PCA in R or Python and predictive modeling using PCA. This guide assumes some prior knowledgeof statistics.Tools:R & PythonTechniques:Principal Component AnalysisLevel: IntermediateXGBoost is considered as one of the most powerful algorithms by any data scientist. Building a model using XGBoost is easy but to improve the accuracy of the model using XGBoost can be a challenging. Here is a guide for you on parameter tuning using XGBoost in Python. You will learn about the advantages of using XGBoost, various parameters of XGBoost and tuning parameters using examples. One must have working knowledge of Python for data science for this guide.Tools:PythonTechniques:XGBoostLevel: IntermediatePeople often restrict their understanding of regression to only linear and logistic regression. But regression is much more thanthat. This is a complete guide on Ridge and Lasso regression, which usefundamental regularization techniques. In this guide you will learn about the intricacies of Ridge and Lasso regression techniques, peep into the statistics behind dealing with a regression problem and the advantages of using Ridge & Lasso over linear regression. I am certain by the end of this guide you will be able to use ridge and lasso regression in action.Tools:PythonTechniques:Ridge & Lasso regressionLevel: IntermediateGradient Boosting algorithms are easy to apply, but difficult to tune. This guide will take you through the science behind using GBM in Python. You will learn how boosting works, GBM parameters and hands-on experience for tuning parameters using machine learning problem dataset. After you have a basic understanding of parameter tuning in GBM, the guide will also walk you through the general approach for parameter tuning.Tools:PythonTechniques:Gradient Boosting ModelLevel: IntermediateYour predictive models can only be as good as your understanding of the data. Data exploration helps you understand the domain, build those awesome features and marry your domain thinking with the data. This guide teaches you the steps for data exploration & preparation, missing value treatment, techniques of outlier detection & treatment and art of feature engineering. I bet with the help of this guide you will be able to improve your model performance in the next machine learning competition.Tools:AgnosticTechniques:Exploratory Data Analysis, Missing value imputation, Outlier detectionLevel:BeginnerCloud computation is an integral part of any data scientist work flow. If you have to handle data which is much larger than what your laptop / desktop can handle  cloud is the way to go. Heres a complete guide on how to use AWS. This guide will make you familiar with the terminologies used and the interface of AWS. Then you will learn how to configure and launch an instance.Once you are familiar with how AWS works, its time to build your first machine learning model on AWS using Python. The guide will also be helpful for any R user, all you have to do is change the line of codes. By following this guide you can start building models on AWS.Tools:R & Python, CloudTechniques:NALevel:BeginnerPandas is full-featured Python library for data analysis, manipulation and visualization. With its high readability and general purpose use, it has proved to be most useful for data science operations. In this article, you will learn about 12 useful techniques for data manipulation in Python using Pandas. To help you see these techniques in action the article uses a machine learning problem dataset. Learn about Boolean indexing, imputing missing values, multi-indexing, creating pivot tables, merge dataframes and many more useful techniques for data exploration using Pandas. It also provides few tips for each technique to work faster.Tools:PythonTechniques:data exploration, visualizationLevel:IntermediateIn multiple recent competitions, XGBoost has dominated the competitions. This guide will teach you how to use XGBoost in R for model building, what are the different parameters of XGBoost, its functionality and testing the results. By the end of this guide, you will be available to build a simple XGBoost model on your own.Tools:RTechniques:XGBoostLevel:IntermediateThis article will provide you in-depth understanding of evaluation metrics like confusion matrix, gain & lift chart,Kolmogorov Smirnov Chart, AUC  ROC, Gini Coefficient, Concordant  Discordant Ratio, Root Mean Squared Error and cross validation.Tools:AgnosticTechniques:Model EvaluationLevel:BeginnersBayesian Statistics still remains as one of the most important concepts in statistical analysis. But sadly,analysts and data scientists dont seem to have acomplete understanding of Bayesian Statistics. Mathematical explanation can be intimidating for some people. To make things simpler for you here is a guide on Bayesian statistics explained in simple English.Tools:AgnosticTechniques:Bayesian StatisticsLevel:IntermediateImputing missing values in a predictive model can be agonizing. If you are a R practitioner, then this guide is a boon for you. This article will take you through 5 packages in R used for imputing missing values. Learn about MICE, Amelia, missForest, Hmisc and mi in detail. For better understanding, each package has been explained with a practical application.Tools:RTechniques:Missing value imputationLevel:BeginnerToday Recommendation engines are used by almost all websites then be it Facebook, Amazon, Youtube, etc. Building a recommendation is both fun and challenging. This article will teach you how to build recommendation engine using GraphLab in Python. In this guide, you will learn about the different types of recommendation engines. Once you know what about how recommendation engine works, then build one yourself. You will learn how to create arecommendation engine for MovieLens Data, popularity based model and a collaborative flittering model. It will also take you through evaluating a recommendation engine.Tools:PythonTechniques:Recommendation enginesLevel:IntermediateDealing with imbalanced classification datasets can be tricky. In this article you will learn about how to tackle imbalanced classification problems. Learn about imbalanced classification and why machine learning algorithms struggle with accuracy on imbalanced data. Then learn about the various methods for dealing with imbalanced datasets. To provide you hands-on experience the guide will also take you through performing imbalanced classification in R.Tools:RTechniques:Imbalanced classificationLevel:IntermediateArtificial neural network has been a hot topic this year. Self-driving cars, speech recognition, image recognition all these applications of deep learning has gained a lot of attention of data science enthusiasts. In this article, you will get acquainted with theimplementation of neural networks in Python using Theano package. It will first provide you an overview of Theano, implementing simple expressions, Theano variable & function types, modeling a single neuron and modeling a two-layer network.Tools:Python  TheanoTechniques:Artificial Neural NetoworksLevel:IntermediateHere is a guide for you on multinomial and ordinal regression used for dealing with multi-level dependent variables. Learn about multinomial and ordinal regression in detail. Once you have atheoretical understanding of both the techniques, see how multinomial and ordinal regression are implementing in R. This article requires one to have expertise in R.Tools:RTechniques:Multinomial and Ordinal regressionLevel:BeginnerFor any ML model, variable selection is an important concept. Sometimes removing correlated variables can hinder the model performance. R happens to have aparticular package which is mainly for Variable selection. This article will walk you through Boruta Package and how it works. You will also learn how to implement Boruta package in R. I am sure you are wondering how Boruta package wins over the traditional feature selection algorithms. The only way to find out is going this article. The prerequisites for this article is working knowledge of R.Tools:R  BorutaTechniques:Feature selectionLevel:IntermediateAny data scientist must have asound understanding of statistics and mathematics. Here is the list of all the books that will ensure you have a firm base in statistics & mathematics. These are free books available on the web and can be accessed by anyone. So, dont keep waiting and find out which book you should lay your hands first.For thosewho are not familiar with programming, might consider it as aroadblock in their successful career path for data science. But need not worry, because here we have 19 data science tools for non-programmers which will ensure you are not left behind.These tools are devoid of programming and provide user-friendly GUI (Graphical User Interface) so that anyone with minimal knowledge of algorithms can simply usethem to build predictive models. Get started now.This article willhelp you decide which data scientists to follow on Github. In this article, we have also shared some Github repositories, free books & notebooks which you can refer to improve your knowledge of data science & machine learning. To simplify things for the tutorials/repositories on Github have been separated for R and Python users.The language war on R vs Python has created too much uproar among data scientists. It doesnt matter if you are R or Python practitioner, I bet you will find this article helpful. We have provided you ample of resources on tutorials, courses, repositories which you can follow to learn data science & machine learning. But I think books are found most helpful by everyone. Here is a curated list of must read books for data scientists on R & Python.The uncountable courses and certification for each SAS, R, Python, machine learning, big data can be confusing. To help you choose the best course as per your requirements here is a list of top-rated courses in India from 2016. We have evaluated each & every course and we present to you our analysis for each of them.The courses have been ranked as per the evaluation parameter. Go on and find out which course is best for you.The path to become a data scientists is definitely not easy. Here in this article, we have shared the ultimate guide which you must follow to become a data scientist in a year. The article month by month approach to help you achieve your dream. The tasks have been divided into monthly targets from starting with data science to becoming an adept in data science & machine learning. I am sure before every interview you must be scrolling through Glassdoor to find out which are the commonly asked questions in machine learning & data science startups. The task can be cumbersome and futile. To help you, here is the list of 40 interview questions you are most likely to encounter in your next interview at any machine learning & data science startup. Trust me this your best interview guide before any machine learning interview.Applying for Data Science & Analytics masters in US universities? Before jumping the gun to filling out college applications, find out which universityyou should apply and will yield a better ROI. Here is a comprehensive list of Top 10 universities in US with best MS in Data Science programs. Of course, each one of them have their own advantage over the other. Read on to know where you will be the best fit and what segregates the same program over different university.This article was created to give you an insight about the actual market salary report as per your skills & experience. Since India happens to be the 2nd largest analytics market for demand for analytics professionals the salary package are also lucrative. The report is focused on India and reveals the takeaway salary of data science professionals. If you are a beginner and want to find out how the salary packages of analytics professionals, this is your best resource. We keep getting queries from professionals who want to shift their career to data science & analytics. But mid career shift can be intimidating. How to create an attractive resume can be one of the biggest worries. With this article, we provide you a means to build your resume and prove your mettle in the job market. This article provides you a machine learning project which you can work and add to your CV. This article provides you a step by step guide on how to work on the machine learning project.This year we launched several skilltest to help you assess your knowledge and understanding of basic concepts. Skilltest  Machine Learning was designed for any machine learning practitioner. The test covered various concepts of machine learning. The questions designed were based on the practical problems one encounters on day to day basis for machine learning. Here there are 40 questions along with their detailed solutions.Statistics is one of the founding pillars of data science. A sound understanding of statistics will help you have asuccessful career path in data science. We conducted two skilltests with basics and advanced levels. If you are new to statistics go through Skill test 1 to find out which are the must know concepts for basics of statistics. Once you are thorough with the basics of statistics, go through the Skill test 2 to learn the advanced concepts of statistics which will be helpful for you in data science.The best way to master any concept or language is to keep testing ones knowledge by frequent assessments. Skill test R for Data Science and Skill test Python for Data Science were two skill tests exclusively designed for R & Python practitioners. These tests containaround 40 questions each based on much know concepts for each R & Python. If you missed out on these tests, check out the questions and find out how many can your answer correctly.Regression is a vast concepts and is used both for statistical analysis & predictive modeling. Here are 45 questions on regressions and its various techniques which you should be able to answer. We dont want you to have only half knowledge, so we have provided you detailed solutions for each question. This your best resource to master regression.Tree Based algorithms like Random Forest, Decision Tree, and Gradient Boosting are commonly used machine learning algorithms. They are often used in data science problems. Answer these 45 questions of tree-based algorithms and analyze your knowledge of the basic concepts. If you wish to find a complete resource for must know concepts for tree based algorithms then this is your best resource.I hope you found the resources useful. My sense of accomplishment only increased when I curated these articles. I hope that we have been helpful in your journey to learn this year and we promise to do so in coming year as well.We wish you a very happy new year. May the new year bring the best of health, wealth and knowledge for you. In the meanwhile, if you have any suggestions / feedback, do share them with us. If you have any questions, feel free to drop your comments below.This is not all  check out our upcoming competitionsin Jan 2017 and also our new Job portal. See you around.",https://www.analyticsvidhya.com/blog/2016/12/top-35-articles-resources-analytics-vidhya-year-2016/
Data Science Intern  Analytics Vidhya  Gurgaon,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Top 35 Articles and Resources from Analytics Vidhya for the year 2016|[Announcement] Launching Analytics Vidhya glossary & new revamped Job portal|
Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  years
Requirements : Good Communication skills
Hard working and passionate for data science
Smart enough to learn languages and techniques quickly.
Task Info :What is the hiring process?Because, we got more than 1000 application, a simple process would not work anymore. So, there is a new format of selection:The first round will be technical skill assessment  and what could be more appropriate than our DataHack platform. This round would include multiple hackathons and contests you will need to showcase your skill at.The best people from the first round(s) would be then interviewed for internship at Analytics Vidhya.What should you expect?A team of best data scientists and thought leaders from industryDisciplined entrepreneurship within team. Each person is owner of his own work  you set the milestones, the pace and the achievements.Complete flexibility  we would love to have in our office whenever you feel like. Otherwise, work from where you prefer.High standards, deep passion for data science and a commitment to find out ways to make things work.What is the Role?Here are the specific things you will do during the internship:Learn Python / R, tools and techniques used in data science.Research on these tools and techniques to solve business problems and problems faced by analytics professionals across the globe.Once you have the solution, share it with the rest of the world through Analytics Vidhya.Continuously learning new skills and evangelizing them with in our community. Helping members of our community.College Preference : no-bar
Min Qualification : open
Skills : machine learning, statistics
Location : Gurugram",https://www.analyticsvidhya.com/blog/2016/12/data-science-intern-analytics-vidhya-gurgaon/
[Announcement] Launching Analytics Vidhya glossary & new revamped Job portal,Learn everything about Analytics|Introduction,"Analytics Vidhya Glossary|Analytics Vidhya Job Portal|End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Data Science Intern  Analytics Vidhya  Gurgaon|Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As 2016 comes to a close, we are thinking about one and one thing only- how to make Analytics Vidhya more useful for our community members. This year has been nothing short of phenomenal for us. We grew multiple times in traffic, launched different types of hackathons, moved from a single room home office to our new office where we have little more space for our dreams.This is also just the start  we have only started scratching the surface of our vision. And we promise, 2017 is going to be even bigger  for you and for us. To set the tone for the year to come  we launch Analytics Vidhya Glossary and new revamped Job portal.Machine learning can be a tough topic to wrap your head around and remember all the concepts on your tips.As a beginner, it is overwhelming to remember various concepts and recall them every time you come across them! Add statistics, business intelligence and big data to it and you get that drowning feeling in this technical world.If you have felt this way, dont worry you are not alone. We have all been there. It is for community members like you, that we have created a glossary of commonly used machine learning and statistics terms. We have explained the terms in the glossary in simple language. The idea is to provide you a handy reference guide which you can refer at any point in time.Over the next few days, we will continue to add more terms and terminology in this Glossary section. If youwant any concept / keyword to be added to the list, all you have to do is post it in the comments section and it will be added.Here is another reason for you to rejoice before the year ends. We have just revamped our job portal completely and we are opening it up for our existing users. If you are a new user, you will need to wait for a few days!Once you login with your Analytics Vidhya login and password  you should be able to apply to various jobs with your Analytics Vidhya identity. The employers would be able to not only see your past experience and project work  they will also see your participation on Analytics Vidhya portal.In order to help our partner companies hire the best talent, we are also running an introductory offer on our listing model. They can avail this fabulous discount and kick start their hiring.Just a word of caution  over the next couple of days, we will be fine tuning the portal. In case you have any problem, feel free to reach out to us directly on [emailprotected]Update: We are experiencing problem with the job portal  it will be up again in a few hours. Regret the inconvenienceWe cant wait to see you use both the additions on the portal  the glossary page and new revamped job portal. We have our eyes on the comments sections and ears on what you say  do let us know your feedback on both the new things on the portal through the comments below! Looking forward to hear from you.",https://www.analyticsvidhya.com/blog/2016/12/announcement-launching-analytics-vidhya-glossary-new-revamped-job-portal/
Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|[Announcement] Launching Analytics Vidhya glossary & new revamped Job portal|Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  12 years
Requirements : 
Task Info : JOB RESPONSIBILITIES (EXHAUSTIVE LIST)1. Lead the BI journey.2. SAP BPC administration3. IT / BI interface of FP&A team with IT and rest of the organization4. Support / Improve SAP COPA related processesPerson Specifications:1. Age Group- 30 to 352. Qualifications: CA/ ICWA/ SAP & BI Certifications. Desirable: Preferably from FMCG/Automobile and located in Gurgaon3. Experience (Essential work experience required for this role): Minimum 8 years4. Technical Competencies required: Strong accounting knowledge Hands on experience in SAP modules (FICO / MM / S&D / COPA / PS) Hands on experience in SAP BPC Implemented / contributed significantly to Business Intelligence / Automation journey of business reporting (finance, sales, manufacturing, etc) Understanding on BI tools / Dashboarding flair Working Exposure in MIS/Finance/ FP&A Department (Essential) Good in Financial Modeling & Templates design. Expertise in Using MS Office Excel (expert level), Word, Powerpoint (expert level) Strong Analytical Skills (Essential)Desirable:Executed to have led BI implementation journey5. Behavioural Competencies requiredHigh on Business AcumenAbility to handle timely delivery pressure (Essential)Good Communication SkillsUrge of Learning6. Targetted CompaniesFMCG / Automobile  Hero, Honda, Yamaha, Bajaj, Unilever, Nestle, etc etc
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, excel, financial modeling, sap
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/team-manager-group-manager-bi-gurgaon-8-12-years-of-experience/
Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience)|Director / VP (Analytics)  Gurgaon (8  14 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : About the company-Started 5 years ago, today it is worlds no.1 internship platform. We are transforming lives of millions of college students, one meaningful internship at a time.Imagine a world full of freedom and possibilities. A world where you can discover your passion and turn it into your career. A world where your practical skills matter more than your university degree. A world where you do not have to wait till 21 to taste your first work experience (and get a rude shock that it is nothing like you had imagined it to be). A world where you graduate fully assured, fully confident, and fully prepared to stake the claim on your place in the world. We are making this dream a reality. Join us!What will you do?Build, validate and deploy machine learning models for various business problems (recommendation, relevancy, automate decisioning, fraud prevention to name a few)Work on data enrichment and integrity frameworks and processes to add to the power of modelsThe role would start as an individual lead but overtime you may need to build and lead a team depending on scope of workWho are we looking for?Someone with2-4 years experience of building and deploying Machine Learning models, preferably in internet industrySound knowledge of R, Python and different data mining tools (Advanced Excel, MySQL etc.)Deep knowledge of various predictive modeling and machine learning algorithms and underlying Maths and Stats behind themBachelors or Masters degree from top Engineering colleges (IITs) or Mathematics/Statistics institutes (ISI, IISc., Indian Institute of Mathematics etc.)Strong analytical, numerical, interpersonal skills and business acumenLocation  Gurgaon (Address)Compensation  INR 10-15 Lacs per annumStart date  Immediately
College Preference : tier1-any
Min Qualification : ug
Skills : excel, machine learning, predictive modeling, python, r
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-scientist-machine-learning-gurgaon-2-4-years-of-experience/
Director / VP (Analytics)  Gurgaon (8  14 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience)|Data Scientist  Delhi NCR (4+ years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  14 years
Requirements : 
Task Info : Experience:8-14 yearsWe are looking to hire a person who can lead our analytics strategy. This is a key role as the person would be defining the analytics road map for the organization.This would include:o Building predictive models using the current data assetso Integrating with other data pipelines / platforms to define the value from current / additional sources (e.g. Bureau data)o Defining the future hiringo Expected to make a direct impact on the approval and disbursal rate through use of smart data based lendingoIdeally someone with 8  14 years of experience in banking analytics, with some time in risk analyticso The person would be expected to recruit a junior person for data cleansing and part of model building
College Preference : no-bar
Min Qualification : ug
Skills : banking, predictive model, risk
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/director-vp-analytics-gurgaon-8-14-years-of-experience/
Data Scientist  Delhi NCR (4+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Director / VP (Analytics)  Gurgaon (8  14 Years of Experience)|Senior Analyst  Ahmedabad (2 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : We are a multinational business process and information technology services company. We are a global leader in designing, transforming and running business process operations, including those that are complex and industry-specific.Now, we are in process of re-defining the use of big data and machine learning for managing human resources in a company.Hence, we are looking for a curious data scientist, who can find his / her way through big data.The person would be responsible for setting up an HR analytics practice, proving the value to the organization and scaling it up. The person should be a thought leader, open to try out new approaches and ready to work on problems, not many have solved.Responsibilities:Set up data practices to identify and implement data based opportunities in the organization.Create machine learning models to predict human behaviour specifically attrition of employees, performance expectation and prediction.Implement, evangelize and scale big data based solutions within the organizationSkills:You have a deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms.With 4+ years of experience delivering world-class data science outcomes, you solve complex analytical problems using quantitative approaches with your unique blend of analytical, mathematical and technical skills.You are accomplished in the use of Advanced Machine Learning Algorithms with the techniques like R, SVM, Survival, Neural Network, Random Forest etc..
College Preference : no-bar
Min Qualification : ug
Skills : classification, clustering, machine learning, optimization, predictive modeling, statistical modeling
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-scientist-delhi-ncr-4-years-of-experience/
Senior Analyst  Ahmedabad (2 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Delhi NCR (4+ years of experience)|NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.This is a Data Analytics Company that has transformed business across verticals through focus on Strategic Consulting Services. It provides Business Analytics Support using an integrated onsite/offshore Business model.A Senior Analyst is in a role that requires thorough understanding of Marketing and Customer Data, Relational Database Models and working knowledge of SQL queries. It is implied that the individual has a strong analytical bent of mind and an urge to solve problems through the effective usage of Microsoft Excel, PowerPoint, Relational Database Models, SQL and SAS.In-depth understanding and some working knowledge of Marketing and Customer data is necessary. The individual is expected to own and take initiative with a zeal to execute projects with minimum guidance from Consultants.Competencies and Skills. A strong analytical bent of mind with a penchant for problem solving and trouble shooting. Knowledge and understanding of quantitative and statistical analysis is required. Must have a thorough understanding & some working knowledge of marketing and customer data, relational database models, and experience in SQL queries. Expertise in Microsoft Excel, Microsoft PowerPoint is needed and knowledge of SAS is preferable. Listen clearly, communicate effectively, and observe minutely. Be a good team player. Begin to take preliminary steps towards Client facing and handling.Education and Experience: 2 yrs. of Analytics/related experience in IT/ITES and basic SQL Skills mandatory against a Bachelors Degree. (BE/B.Tech or Honors in Economics, Math, Statistics) Masters degree from a premier Institute is mandatory if an individual doesnt have any past experience. Candidate is expected to have proficient knowledge of Excel, Macros, VBA, SAS & SQL. Good Understanding of Marketing and Customer Profiling.
College Preference : tier1-any
Min Qualification : pg
Skills : excel, sas, sql
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/senior-analyst-ahmedabad-2-years-of-experience/
NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  Ahmedabad (2 years of experience)|Business Consultant  Bangalore (2+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  8 years
Requirements : 
Task Info : Responsible for developing engineering solutions and services for Xpresso, the companys core text analytics engine in collaboration with the team. Implements professional concepts in accordance with company objectives to solve complex problems in the field of NLP and machine learning in creative, innovative and effective ways. Investigates, creates and develops new methods and technologies for project advancement. Regularly exercises technical discretion in design, execution and interpretation of experiments that contribute to project goals. Contributes to project process within scientific discipline through innovative research. Prepares technical reports, summaries, protocols and quantitative analyses. Should be able to customize solutions built on top of Xpresso for different clients from different domains. Should lead the effort in domain modelling, data extraction and preparation. Prepares and installs solutions by determining and designing system specifications, standards, and programming. Improves operations by conducting systems analysis; recommending changes in policies and procedures.Develops NLP and other text analytics components and services across the organization. These components will be used for developing sentiment analysis, emotion analysis, content filtering, topic extraction and auto-tagging.You will work on sentiment analysis, information retrieval and other related areas. Develops software solutions by studying information needs; conferring with users; studying systems flow, data usage, and work processes; investigating problem areas; following the software development lifecycle. Experience with part of speech taggers, NER, resolving semantic disambiguity, syntactic parsers is very essential. Collaborate closely with fellow architects, engineers and product management to ensure functionality is designed properly so that its stable, scalable, usable, of high quality and meets customer needs. Should collaborate with the testers and ensure quality and quantity of testing, dataset preparation and annotation etc. He will also lead the effort in fixes, improvements and enhancements based on the testing outcomes. Research, experiment, and build prototypes using new technologies such as machine learning and natural language processing applicable to the proposed solutions. Partner with external teams to understand frameworks or tools being developed elsewhere to identify areas where the team can leverage, co-develop, or share technologies. Participate in scientific conferences and make contributions to publications, research journal write up and patents. Take part in companys Intellectual Property submissions, lucid problem conceptualizations, institutional innovations and subsequent benchmarking. Requirements:Excellent programming skills (particularly Java)Experience with NLP tools, algorithms and techniquesUpdated with the latest technologies and research in the fieldKnowledge of Machine LearningSkills with architecting large-scale high performance distributed systemsTechnical discretion in design, execution and interpretation of experiments that contribute to project goalsWe are a well funded, revenue generating company looking to augment our highly talented text analytics R&D team. You will be working on our core XPRESSO text analytics engine that takes in unstructured text from various sources and finds key hidden insights. We are on our way to building the best analytics engine there is. We are able to look at a statement such as This washer uses a lot of power and understand that this represents a negative sentiment in an unsupervised way. If you are able to see why this is hard to do (sentiment lexicons fail on this), well like to talk with you.You should be totally comfortable producing production level code in addition to evaluating the relative merits of various academic papers. We come up with and implement bleeding edge NLP algorithms on a daily basis and you should be excited at the prospect of doing the same.As part of the organization, youll have the perfect opportunity to grow and develop your skills in a highly relevant industry vertical (NLP, machine learning). You will also get the opportunity to directly interact with some of the most renowned academics in the field of NLP.Research is centered around the domain of data sciences on structured and unstructured data, using big data analytics, social media analytics, natural language processing, machine learning algorithms, statistical modeling, predictive analytics, information retrieval, artificial intelligence, text analytics and processing, knowledge-base buildup, pattern recognition, and semantic parsing.As part of your role as a Senior Software Engineer, NLP, you will:work with the product team to flush out product level detailsdevelop methodologies for evaluating precision and recallParticipate in scientific conferences and make contributions to publications, research journal write ups and patents.Requirements:MS/PhD in Natural Language Processing or related fieldExcellent Java skillsOut of box thinking abilitySkills with architecting large-scale high performant distributed systemsTechnical discretion in design, execution and interpretation of experiments that contribute to project goals.Desired Skills and ExperienceMS/PhD in Natural Language Processing or related fieldExcellent Java skillsOut of box thinking abilitySkills with architecting large-scale high performant distributed systemsTechnical discretion in design, execution and interpretation of experiments that contribute to project goals.
College Preference : no-bar
Min Qualification : pg
Skills : java, machine learning, nlp
Location : Kolkata, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/nlp-engineer-pune-kolkata-3-8-years-of-experience-2/
Business Consultant  Bangalore (2+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience )|Data Crawling Engineer  Pune/Kolkata (4 to 6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Desired Skills and ExperienceMinimum of 1-2 years on the below skillsExcel  mandatory (Intermediate Level)Power point and Presentation  mandatory (Develop content, flow, visualization)SAS/R  mandatory (Expert/Intermediate Level)SQL mandatory (Expert level)Logistic and Linear Regression  mandatory (Intermediate/Expert level)Segmentation & Profiling  mandatory (Intermediate/Expert level)Experience with BI and Visualization tools like Tableau  Good to haveAdditional Skills needStrong oral and written communication skills, as well as strong documentation and data management and analysis skills.Communicates key process information, deadlines, task definition, etc., to groupsCreative approach to problem solving.Manage analytical projects to deliver intelligence and data discovery capabilitiesProvides information and analysis essential for planning and strategy development.Must have the ability to develop, own and drive processes; complete complex analysis; successfully partner with other professionals; and have strong project management skillsRecommends and supports on-going business decisions and processes.As an independent professional, management provides direction primarily on new projects or assignments, as well as review of activities and priorities.  The ideal candidate will be proactive contributor on team projects, with an ability to successfully demonstrate favorable results through coaching and influencing others.
College Preference : no-bar
Min Qualification : ug
Skills : excel, linear regression, logistic regression, r, sas, segmentation, sql, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/business-consultant-bangalore-2-years-of-experience/
Data Crawling Engineer  Pune/Kolkata (4 to 6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Consultant  Bangalore (2+ Years of Experience )|Technical Project Manager  Pune/Kolkata (3 to 7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Responsibilities1.Deep knowledge of the relevant Internet protocols such as HTTP, DNS, and familiarity with the web authoring standards such as HTML, CSS as well as de-facto data encoding, markup, and representations like JSON, XML etc.2.Extensive knowledge of and experience with crawling web pages and building systems to do so.3.Extensive knowledge of and experience with working with Web APIs, in particular, REST services and JSON-encoded responses4.Experience with building systems at Internet scales, that scale to terabytes or more of data, millions of API calls, and thousands of requests/second5.Experience with data extraction, scraping/parsing/processing semi-structured data, ETL toolsets, and mechanisms6.Experience building robust, fault-tolerant, scalable and cost-effective distributed systems7.Experience with Amazon Web Services or other Cloud platformsSkillsets Required4-6 years of experience in a related occupation that provided the required skills and abilities.Skills:Individuals must have demonstrable proficiency, knowledge and experience with the following specific areas:Machine-learning, text-mining, Java, Python or natural language processing techniques.Computational linguistics for artificial intelligence.Lexical analysis design and implementation.
College Preference : tier1-entire
Min Qualification : open
Skills : 
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-crawling-engineer-punekolkata-4-to-6-years-of-experience/
Technical Project Manager  Pune/Kolkata (3 to 7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Crawling Engineer  Pune/Kolkata (4 to 6 Years of Experience)|Data Science Consultant- Text Mining  Gurgaon/Pune (3 to 7 years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : Technical lead or manager
Task Info : The Project Manager will take the lead role in overseeing the delivery of AI-based products for clients in retail and fashion industry. She/he will manage the local engineering team supporting the development and delivery of the AI products. An ideal candidate can simultaneously execute against multiple priorities, which include: project planning, analysis structuring/execution, team management, client management/communication, and oversight of the completion of time-sensitive deliverables. The position requires strong data pipeline and machine learning skills, intellectual curiosity, comfort with ambiguity, and an ability to leverage information to tell compelling storiesResponsibilities:Manage complex technical projects and a team of software engineersRoll up your sleeves and code alongside your team when neededProvide frequent and constructive feedbackAble to coach and mentor junior and new college graduatesParticipate in the hiring processLead teams to develop and deliver AI products tailored to the specificities of each clientSkillsets RequiredPrevious experience as a technical lead or manager.Agile development experience.Command of modern programming language: Python, Ruby, Java, Objective-C, C++, etc.Comfortable working in a UNIX environment.
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/technical-project-manager-punekolkata-3-to-7-years-of-experience/
Data Science Consultant- Text Mining  Gurgaon/Pune (3 to 7 years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Technical Project Manager  Pune/Kolkata (3 to 7 Years of Experience)|NLP Engineer  Pune/Kolkata (3-8 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : strong understanding of Natural Language Processing
Task Info : Pioneers in sales force and marketing analytics  applying optimization techniques to sales force and marketing investment problems. We have grown significantly and are now recognized global leaders in sales and marketing consulting. A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, the organization has set up a Data Scientist track which has three major focus areas:Research the evolving datasets and advanced analytical techniques to develop new offerings/solutionsDeliver client impact by collaboratively implementing these solutionsProvide thought leadership by developing the Point of View in this spaceThe Data Science Consultant will play a critical role in designing, developing and implementing analytical techniques on large, complex, structured and unstructured data sets (including big data) to help client make better decisions in sales and marketing space, with guidance from Data Science senior leadership.Specific responsibilities include: Client impactDeveloping advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective mannerImplementing statistical and data mining techniques e.g. hypothesis testing, machine learning, and retrieval processes on a large amount of data to identify trends, figures and other relevant informationCollaborating with clients and other stakeholders to effectively integrate and communicate analysis findingsProviding guidance and project management support to the Associates and/or Associate Consultants on the teamResearch & firm contributionEvaluating emerging datasets and technologies that may contribute to our analytical platformOwning the development of select assets/accelerators for efficient scaling of capabilityContributing to the thought leadership of the firm by helping in researching the evolving topics and publishing themQualifications:Educational background of Masters or PhD in Computer Science (OR Statistics/Operations Research) from a premier instituteKey competenciesHave a strong understanding of Natural Language Processing and a genuine appreciation for NLP research.Exposure in creating lexical semantic resources for NLP-based applications.Experience in applying ontologies to NLP applications.Experience with multi-lingual text mining and information retrieval.Experience in Social Media, Web 2.0, and Marketing domains.Understand the data from various sources and build models that can provide actionable insightsStrong programming skills in at least one language  Java/Python/RExposure to tools/platforms  Hadoop eco system and DB systemsAgile project planning and project management skillsExcellent communication skillsDomain knowledge/expertise (Healthcare/Travel/Hi-tech/Insurance) is preferredApproach to workResearch focused mindset with strong attention to detailExcellent critical thinking and problem solving skills; ability to devise a structure for solving unstructured problems / for conducting exploratory analyticsHigh motivation, good work ethic and maturity
College Preference : tier1-any
Min Qualification : pg
Skills : java, marketing analytics, nlp, python, r, text mining
Location : Gurugram, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-science-consultant-text-mining-gurgaonpune-3-to-7-years-of-experience/
NLP Engineer  Pune/Kolkata (3-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Science Consultant- Text Mining  Gurgaon/Pune (3 to 7 years of Experience)|AWS/ Cloud Engineer  Pune/Kolkata (3 to 7 years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  years
Requirements : 
Task Info : Lead cutting edge research in NLP and machine learning technology or algorithm Produce benchmark results for existing NLP development platforms or offering from Vendors innovating with NLP, such as Microsoft, Google, Facebook, Amazon and Startups, by building and testing application. Write real-time prototypes of the newly designed algorithms Develop solutions for real world, large scale problems, such as context sensitive Languages understanding1.Well versed with existing open source frameworks such as NLTK and Stanford Core NLP, and have a good understanding of the underlying machine learning techniques and algorithms used to facilitate these tasks Deep understanding of NLP problems and experience going from raw data to choosing the right statistical model and benchmarking performance and accuracy Industrial experience applying machine learning techniques to NLP problems that involve large-scale data Experience with NLP problems such as language modelling, language identification, sentiment analysis, named entity recognition, lemmatization, summarization Strong working knowledge of machine learning techniques such as HMMs, maximum entropy models, boosting/decision tree, neural networks (CNN, RNN and etc.) Programming experience with C++, Objective C or similar object oriented languages Programming experience with Python Strong communications skills Ability to lead an Engineering team2. Responsible for developing engineering solutions and services for Xpresso, core text analytics engine in collaboration with the team.3. Implement professional concepts in accordance with company objectives to solve complex problems in the field of NLP and Machine Learning in creative, innovative and effective ways.4. Investigate, create and develop new methods and technologies for project advancement.Skillset Required:Excellent programming skills (particularly Java and python)Experience with NLP tools, algorithms and techniquesMSc or PhD in Computer Science, Computational Linguistics or equivalent 3-5 years of demonstrable NLP research and development experience in a commercial product Experience with voice recognition is a plusUpdated with the latest technologies and research in the fieldKnowledge of Machine LearningSkills with architecting large-scale high performance distributed systemsTechnical discretion in design, execution and interpretation of experiments that contribute to project goals
College Preference : no-bar
Min Qualification : pg
Skills : algorithms, java, machine learning, nlp, python
Location : Calcutta, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/nlp-engineer-punekolkata-3-8-years-of-experience/
AWS/ Cloud Engineer  Pune/Kolkata (3 to 7 years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|NLP Engineer  Pune/Kolkata (3-8 Years of Experience)|Data Science Consultant  Real World Evidence  Gurgaon/Pune (2 to 4 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : Technical lead or manager
Task Info : In this position as a back-end engineer, you will be working on deploying and maintaining production code for one of our products. You will also be in charge of implementing versions of the previously mentioned product tailored to the specific needs of our customers. You will be closely working with our R&D team and our product team. You are expected to excel in Java programming and to ship highly stable and reusable text. You will own your projects and be responsible for their timely delivery. You are comfortable working in an agile environment. You are a team player who also knows how to work independently. You are eager to share knowledge and best practice and mentor junior colleagues.Requirements:Expertise in JavaExpertise and experience in solution architectureExperience with cloud deploymentFamiliarity with agile development, scrumExperience in managing a projectProficiency in PythonFamiliarity with SQL and no-SQL databasesFamiliar with AWS stackValuable plus (not required):Machine learningData miningFull-stack engineerNatural Language Processing
College Preference : no-bar
Min Qualification : ug
Skills : aws, data mining, java, machine learning, nlp, nosql, python
Location : Calcutta, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/aws-cloud-engineer-punekolkata-3-to-7-years-of-experience/
Data Science Consultant  Real World Evidence  Gurgaon/Pune (2 to 4 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|AWS/ Cloud Engineer  Pune/Kolkata (3 to 7 years of Experience)|Who is the Superhero of Cricket battlefield? An In-Depth Analysis|
Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : Strong attention to detail
Task Info : A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, the organization has set up a Data Scientist track which has three major focus areas:Research the evolving datasets and advanced analytical techniques to develop new offerings/solutionsDeliver client impact by collaboratively implementing these solutionsProvide thought leadership by developing the Point of View in this spaceThe Data Science Consultant will play a critical role in designing, developing and implementing analytical techniques on large, complex, structured and unstructured data sets (including big data) to help client make better decisions in sales and marketing space, with guidance from Data Science senior leadership.Specific responsibilities include: Client impactDeveloping advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective mannerImplementing statistical and data mining techniques e.g. hypothesis testing, machine learning, and retrieval processes on a large amount of data to identify trends, figures and other relevant informationCollaborating with clients and other stakeholders to effectively integrate and communicate analysis findingsProviding guidance and project management support to the Associates and/or Associate Consultants on the teamResearch & firm contributionEvaluating emerging datasets and technologies that may contribute to our analytical platformOwning the development of select assets/accelerators for efficient scaling of capabilityContributing to the thought leadership of the firm by helping in researching the evolving topics and publishing themQualifications:Educational background of Masters or PhD in Computer Science (OR Statistics/Operations Research) from a premier instituteKey competenciesExperience with working with RWE data sources such as medical claims, electronic health records, registriesHave a strong understanding of RWE use cases such as Treatment patterns, Cost of medications, Prevalence and impact of various risk factors or relevant topicsExposure to instance based learning / online learning algorithms such as Mini-batch dictionary learning, Incremental PCA.Exposure to machine learning approaches applied in real-world evidenceUnderstand the data from various sources and build models that can provide actionable insightsStrong programming skills in at least one language  Java/Python/RExposure to tools/platforms  Hadoop eco system and DB systemsAgile project planning and project management skillsExcellent communication skillsApproach to workResearch focused mindset with strong attention to detailExcellent critical thinking and problem solving skills; ability to devise a structure for solving unstructured problems / for conducting exploratory analyticsHigh motivation, good work ethic and maturity
College Preference : tier1-any
Min Qualification : pg
Skills : hadoop, java, machine learning, python, r, real world evidence (RWE)
Location : Gurugram, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/data-science-consultant-real-world-evidence-gurgaonpune-2-to-4-years-of-experience/
Who is the Superhero of Cricket battlefield? An In-Depth Analysis,Learn everything about Analytics|Introduction|Player Selection Criterion|Current Standing across all the formats|Framework for comparison|Test Matches|One day International (ODI)|T20|What does the analysis say about the four players?,"1.Consistency|2. Player Dominance|3. Patience (Average balls faced per inning)|4. Winning Contribution|5. Dismissal Pattern|The Test Verdict|1. Consistency|2. Player Dominance|3. Hitting Strength|4. Winning Contribution|5. Dismissal Pattern|The ODI Verdict|2. Player Dominance|3. Hitting Strength|4. Winning Contribution|5. Dismissal Pattern|The T20 Verdict|End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Science Consultant  Real World Evidence  Gurgaon/Pune (2 to 4 Years of Experience)|Artificial Intelligence Demystified|
NSS
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",C) Chasing/Scoring,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The cricket battlefield is competitive and challenging. Players have become extremely professional and disciplined about their training. Companies have optimized the weight of the bat, material of the bat, length of the handle, design of the shoes and what not to ensure top performance from the players. The cut-throat competition doesnt makeit easy either. In spite of all the hoopla, a few players still make it to the top. They raise the bar of the game every time they walk to the field.Who rules the cricket pitch across the globe for ODI, Test and T20?Have you ever pondered over this question? Well  we have and to find out the answers we decided to do an in-depth analysis of best reigning batsmen in the cricket fraternity today. Whether you are a cricket fan or not, this questions must have piqued you to find answers further. Here is a major analysis of this question the world has been asking.P.S. If you are not a cricket fan, todays article might not be for you. And if you are, you are in for a treat!Before we begin, I am sure the first questions that must have popped in your head is how to choose a consideration set for players. Who are the best batsmen in the world? The player selection parameters can be confusing and difficult. So, how did we reach an agreeable state for player selections? And which four players made to our list?For the analysis purpose, we used following criteria to select theplayers:So after applying these filters, we were left with four remarkable test batsman (as per ICC Ranking):Steven Smith, Virat Kohli, Joe Root and Kane Williamson. We are keeping Amla out of the analysis because of his age. Although he is one of the most dominating cricketer of world cricket with a good average across Test and ODI.I also want to mention the multi-talented cricketer (hope you guessed it right), ABD The ABD. He can hit a ball in any part of the groundbut again his current statistics and age are not satisfying our selection criteria.Lets start with the obvious  The current ICC rankings of all the four players across all the formats of the game.For the analysis, all the four players will be judged on the following parameters across all the three formats of the game, i.e, ODI, Test, T20.Now, lets look at the each format and compare the four remarkable players on the above metrics.The consistency of a player is defined by two criteria:A) Consistency in ICC Rankings over the yearsB) Average over the inningsSo let us first look at the trend of ICC Rankings of all the four players over the years starting from 2013.A) ICC Rankings Over last 3 yearsIt looks like Joe Root was on Turbo mode from 2013 to 2015 surpassing other players but what catches the attention is the growth of Kohli from 14th rank in 2015 to 2nd position in 2016. This is especially intriguing keeping in mind that Kohli is at the top positions in other formats too. Everybody knows the aggressiveness which Virat Kohli has on the field but test matches demand an entirely different outlook and we think Kohli understood it well in 2015.B) Average Over InningsIt certainly cant be denied that a good average is an indicator of a good player and a consistent average is what makes a player dependable. Let us look at the average of all the four players over the course of their innings to see if we can generate an inference about their performance.As it can be seen Joe Root and Kane Williamson started off with an excellent opening in their career but their performance graph dipped later on. After 70 innings into their careers, Steven Smith rules the average turf. Meanwhile, it is worth noticing that Kane Williamson has shown a constant increase in his average score over the innings and I would say he is one hell of a player to bet your money on. Whereas, Kohli attained a constant average score early into his career and has maintained it so far. But these are just the overall average of each player and only shows you the outside picture. Interesting insights emerge when we break this down into two categories:C) Chasing Vs.ScoringD) Away Vs.HomeC) Chasing Vs. ScoringVarious exciting insights can be drawn from the above graph, like:D) Away Vs. Home
The only insight that is clearly visible is Steven Smith beating down the other three players both in Home conditions and Overseas.Who wins on Consistency: Steven SmithPlayer Dominance is the percent of the time a player has been able to put up a show as to mark his presence in every game he has played. Player Dominance here is measured by two factors:A) Conversion RateB) Man of the Match / Number of Matches playedA) Conversion RateIt is the ability of a player to play big knocks and provide the team with the winning runs. Not everybody is capable of playing bigger knocks. Let us look at the conversion rate of all the four players and see which player has the capability to score big runs.
It is clearly evident that Virat Kohli is the player who once settled, goes for the big runs. So it is advisable to restrict him before 50 otherwise he is more likely to convert that 50 into 100.B) Man of the Match/ Number of matches playedThis ratio is representative of the degree of dominance of the player in the matches played.
From the graph, it is evidently clear that Steven Smith steals the show in terms of number of Man of the Match per game played with Joe Root behind in the second position.So, after combining the two results it is Steven Smith who is more dominating in comparison to the other three players.Who wins on Dominance: Steven Smith and Virat Kohli.Test matches require a lot of patience to hold on to the pitch the entire facing hundreds of balls. Let us look at the graph below to see which player fares better than the rest in terms of the number of balls faced per innings.It is easy to observe that Kane Williamson is the most patient cricketer of all the four but difference is not big.Who wins on Patience: Kane WilliamsonWinning Contribution is determined by metrics MoM / Number of times team won.MoM / Number of times team wonAs can be seen from the graph that Joe Root has the higher percentage of winning contribution than the rest.Who wins on Winning Contribution: Joe RootThough this is not a judging criterion but it will be exciting to know about the dismissal patterns of all the four players. This will let us know their preference of bowlers.It seems Steven Smith is better at protecting wickets against pace bowlers while Joe Root is the one who can play spinners well.Note: The ratio is calculated w.r.t innings rather than matchesAnother insight is Virat is much more capable than the other players at not getting bowled. With getting 5 times bowled out of 89 innings, this is an interesting insight.Seeing the all the parameters for Test Matches, you can see that all the four players are at the top on different parameters although it is clear that Steven Smith wins 2 of all four parameters so Steven is the man for test matches.Now is the turn for limited hours format. We will be evaluating players on the same criteria similar to test matches. So lets begin.The consistency of a player is defined by two criteria:A) Consistency in ICC Rankings over the yearsB) Average over the inningsSo let us first look at the trend of ICC Rankings of all the four players over the years starting from 2013.A) ICC RankingsDo you see the flat line in dark blue color? Yes, that indicates Virat Kohli occupying the top spot since 2013. This symbolizes the consistency of Virat Kohli in ODI format. While the other three players have almost caught up in 2016, they still have a lot to catch up.B) Average Over InningsIt certainly cant be denied that a good average is an indicator of a good player and a consistent average is what makes a player dependable. Lets look at the average of all the four players over the course of their innings to see if we can generate an inference about their performance.As it can be seen Joe Root started off to an excellent start in his career but his graph dipped midway. After 40 innings, Joe Root has shown a constant increase in his average over innings. Meanwhile, it is worth noticing that Kane Williamson has shown a constant increase in average over the innings and he is one hell of a player to bet your money on. Whereas, Kohli attained a constant average during his mid-career and has maintained it so far and currently it is higher than all of them.C) Chasing / ScoringVarious exciting insights can be drawn from the above graph. Like:D) Home / Away
 Now there are some interesting insights that can be derived from the above graph:Who wins on consistency: Virat KohliPlayer Dominance is the percent of the time a player has been able to put up a show as to mark his presence in every game he has played. Player Dominance here is measured by two factors:A) Conversion Rate
B) Man of the Match / Number of Matches playedA) Conversion RateIt is the ability of a player to play big knocks and provide the team with the winning runs. Not everybody is capable of playing bigger knocks. Lets look at the conversion rate of all the four players and see which player has the capability to score big runs.It is clearly evident that Virat Kohli is the player who once gets settled goes onto making big runs. So it is advisable to restrict him before 50 otherwise he is more likely to convert that 50 into 100.So, after combining the above tworesults we reach a conclusion that Virat Kohli happens to bemore dominating in comparison to the other three players.B) Man of the Match / Number of matches playedThis ratio is representative of the degree of dominance of the player in the matches played.From the graph, it is clearly evident that Virat Kohli steals the show for player dominance with the highest ratio for Man of the match per game played.And Kane Williamson stands at thesecond position.Who wins on Player dominance: Virat KohliHitting Strength of a player is calculated by two factors:A) Strike RateB) Number of 6s and 4sA) Strike RateStrike Rate plays an important role in the One Day format since it is equally important to score big runs as well score them fast. Lets look at the below strike rate graph of all the four players to see who likes to take it aggressive on the pitch.Well, once again it is Virat Kohli leading the race followed by Steven Smith.B) Number of 6s and 4sIt is difficult to decide the winner amongst the cricketer who likes to send the balls out of the ground. Kane Williamson and Virat Kohli are the players who like to score their runs mostly in 4s and 6s.Who wins on Hitting Strenght: Virat KohliWinning Contribution is determined by the ratio of number of Man of the Matches to the Number of times team won.MoM / Number of times team wonAs can be seen, it is Virat Kohli who has contributed the most out of all the four players to the number of times a team has won the match.Who wins on Winning Contribution: Virat KohliThough this is not a judging criterion but it will be exciting to know about the dismissal patterns of all the four players. This will let us know their preference of bowlers.Joe Root happens to be the most vulnerable of the four players against fast bowlers whereas Virat Kohli is the best batsman against spin bowlers.Note: The ratio is calculated w.r.t innings rather than matchesVirat Kohli has constantly featured at the top position across various parameters and has emerged as the most influential batsman amongst all the four players in ODIs.The consistency of a player is defined by two criteria:A) Consistency in ICC Rankings over the yearsB) Average over the inningsSo let us first look at the trend of ICC Rankings of all the four players over the years starting from 2013.A) ICC RankingsThere are two interesting trends that can be seen in the graph above:B) Average Over InningsThere is no denying that a good average is an indicator of a good player and a consistent average is what makes a player dependable. Let us look at the average of all the four players over the course of their innings to see if we can generate inference about their performance.As it can be seen that Joe Root started off to an impressive start at the beginning of his career and remained almost constant at a score of around 40 for the rest of his T20 career. But the most exciting trend is exhibited by Virat Kohli who after his 20th inning has shown a constant and enormous increase in his T20 average. Meanwhile, Kane Williamson and Steven Smith have a lot to catch up in this format.Various exciting insights can be drawn from the above graph.D) Home / AwayThe only insight that is clearly visible is Virat Kohli taking down every other player on the overseas ground whereas it is Kane Williamson who outperforms everybody in home ground.Who wins on consistency: Virat KohliPlayer Dominance is the percent of the time a player has been able to put up a show as to mark his presence in every game he has played. Player Dominance here is measured by metric Man of the Match / Number of Matches played in T20 cricket.Man of the Match/ Number of matches playedThis ratio is representative of the degree of dominance of the player in the matches played.Virat Kohli has time and again proved that he is a master at the limited over format. Here too, the blue bars signifies the mastery Virat Kohli has over limited format game.Who wins on Player Dominance: Virat KohliHitting Strength of a player is calculated by two factors:A) Strike RateB) Number of 6s and 4sA) Strike RateStrike Rate plays an important role in the T20 format since it is equally important to score big runs as well score them very fast. Lets look at the below strike rate graph of all the four players to see who likes to take it aggressive on the pitch.With a marginal difference, Joe Root leads the race with Virat Kohli behind. The greater the area in the above graph more is the strike rate.Number of 6s and 4sInsights:Who wins on Hitting Strength: Joe RootWinning Contribution is determined by factor MoM / Number of times team wonMoM / Number of times team wonAs can be seen from the graph that Virat Kohli has a higher percentage of winning contribution than the rest followed by Joe Root.Who wins on Winning Contribution: Virat KohliThough this is not a judging criterion but it will be exciting to know about the dismissal patterns of all the four players. This will let us know their preference of bowlers.It seems Steven Smith is better at protecting wickets against pace bowlers while Kane Williams0n and Virat Kohli are the one who can play spinners well.Note: The ratio is calculated w.r.t innings rather than matches.It is Undoubtedly Virat Kohli. Once again this can be attributed much to the IPL influence with Virat getting regular practice in 20 over formats. But nonetheless, the answer is Virat Kohli.As per the above analysis, we can conclude that Virat Kohli happens to rule the ODI and T20 cricket pitch. He has shown consistent performance increase and has proved to be one of the promising players till now.Disclaimer: The above analysis is not subject to any personal bias but is purely on the basis of the data and facts available.I hope you enjoyed reading this article. What are your views on this analysis? Do you agree or disagree? Share your opinions with us in the comments section.Through this article,we have tried to show you how analytics can be used for different applications. If you have any doubts/ confusions then feel free to post your questions below.",https://www.analyticsvidhya.com/blog/2016/12/who-is-the-superhero-of-cricket-battlefield-an-in-depth-analysis/
Artificial Intelligence Demystified,Learn everything about Analytics|Introduction||What is Artificial Intelligence?|10 Major Milestones in the History of AI|Buzzwords associated with AI|1.Machine Learning|2.Deep Learning|The Future of AI|Does AI have the power to automate you?,"The 5 steps in the Machine Learning Process|Machine Learning Techniques|Types of Machine Learning|Popular Machine Learning Algorithms|3.Natural Language Processing|How does Siri use Natural Language Processing?|4.Pattern Recognition|How do self-driving cars use pattern recognition?|How energy physics uses pattern recognition?|4.Image Analysis|How Facebook uses Image Analysis?|Careers and Opportunities in AI|End Notes|About the Author|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Who is the Superhero of Cricket battlefield? An In-Depth Analysis|30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",What is Deep Learning?|Where & How is Deep Learning used?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence has become a very popular term today. There is sure to be at least one article in the newspaper daily on the revolutionary advancements made in the field. But, there seems to be some confusion about what AI really is.Is it Robotics? Will the Terminator movie actually come true? Or is it something that has crept into our daily lives without us even realizing it?This article will give you a broad understanding on the buzzwords associated with AI, its applications, the careers & opportunities it has and its future.Artificial Intelligence is simply the ability of a computer to exhibit intelligence. This intelligence can either mimic human intelligence or observe real world problems and intelligently find solutions for it.Did you know? Chef Watson- a part of IBMs Watson program-can now cook for you! This AI cooking app uses algorithms to choose a quirky set of ingredients (this can be done by the user too) and comes up with the perfect recipe. So, Bon Apptit!Machine Learning is a field in Data Science, where machines can learn themselves, without being explicitly programmed by humans. By analyzing past data called training data, the Machine Learning model forms patterns and uses these patterns to learn and make future predictions. The precision of predictions made using ML models has been increasing every day.Machine Learning is used in practically every field these days, even though some of the uses may not always be very obvious. The main techniques of Machine Learning are:Today, Machine Learning is probably the most important field in AI. Hence, several Machine Learning algorithms have been devised each solving a particular type of problem. Each algorithm falls into one of the 3 types of learning. The most popular Machine Learning algorithms are :Did you know? A Scottish cartoonist has taken Machine Learning to an all new level by creating an intelligent program that can write scripts for Friends! Through gathering Big Data (dialogues from all the 10 seasons) and using recurrent neural networks, he could create all-new episodes for this popular sitcom series!Deep Learning is a branch of Artificial Intelligence that is producing life-changing results. Deep Learning means neural networks with a large number of hidden layers. It is an attempt to replicate the functioning of a human brain. Just like the exact functioning of a human brain is unknown, not much is known about the exact working of Deep Learning too. It is like a black box, i.e. where the input and output can be seen and are known, but the internal working is a mystery! Interestingly, Data Scientists believe that if we crack the working of Deep Nets, we will be closer to understanding how a human brain works!Today, Deep Learning has applications in Natural Language Processing, Image Recognition (explained in the later section), Spam Filtering, Fraud Detection, etc. This is just a fraction of what Deep Learning can do! Googles search engine, Facebooks photo tagging feature, Baidus speech recognition  all involve Deep Learning behind the scenes. As these companies invest more and more in this area, the advancements in the field are mind-boggling!1. Google:Apart from optimizing search results, Google uses Deep Learning in a variety of immensely vital but slightly lesser-known fields. Google Brain and Google DeepMind, the two brainchildren of Google are working quite furiously to achieve greater heights in AI. Google has been actively researching and exploring virtually all aspects of machine learning, including deep learning and more classical algorithms.AlphaGo, a project of Googles DeepMind, is perhaps one of the most popular breakthroughs in Deep Learning. Go is a game of stones on-board where you try to make points of territory. Its a game of intense complexity- it is 10100 times more complex than Chess! The algorithm in Alpha-Go combines Monte-Carlo Tree Search with Deep Neural Networks and uses Reinforcement Learning approach to better its result.How AlphaGo works: AlphaGo is built using two different neural-network brains that cooperate to choose its moves. These brains are multi-layer neural networks which are almost identical in structure to the ones used for classifying pictures for image search engines like Google Image Search. They start with several hierarchical layers of 2D filters that process a Go board position just like the way an image-classifying network processes an image. Roughly speaking, these filters identify patterns and shapes. After this filtering, 13 fully-connected neural network layers produce judgments about the position they see. Broadly, these layers perform classification or logical reasoning.The networks are trained by repeatedly checking their results and feeding back corrections that adjust the numbers to make the network perform better. This process has a large element of randomness, so its impossible to know exactly how the network does its thinking, only that it tends to improve after more training.Did you know? In March 2016, AlphaGo beat the legendary Go player-Lee Sedol-with a score of 4-1, a feat previously believed to be at least a decade away.2. Facebook:Facebook AI Research (FAIR) focuses on using Deep Learning to improve the social networking experience. FB is trying to build more than 1.5 billion AI agents, one agent for every Facebook user. The social media giant formed the Applied Machine Learning team called FBLearner Flow. It combines several machine learning models to process several billion data points drawn from the activities of its 1.5 billion users to make predictions about user behaviour and keep them glued to Facebook for hours!For example: the algorithms created from FBLearner Flows models help to define your news feed, the advertisements you see, the people you may know and many more!Therefore, in the AI war between Facebook and Google, there isnt a winner, as the research concentrations and applications are quite different in nature.Natural Language Processing is the process by which computers translate human language into a language that the computer can understand. Siri, Cortana and Alexa are all examples of NLP that we use every day. So how does Artificial Intelligence fit into NLP? Heres how. Consider this. You want to learn a new language. How do you go about doing so? You start by learning new words in the language and understanding the usage. But, you will not really understand what works and what doesnt, unless you are exposed to the language and learn from the usage. This is exactly how Deep Learning is used in NLP. The computer learns by using a technique called embeddings, which Deep Learning implements. In this technique, words and phrases are mapped to vectors of real numbers. This mapping is carried out by Neural Networks.NLP forms the heart and soul of Siri. When a user asks Siri something, the sequence of actions taking place is as follows. Through voice recognition, Siri first uses a discretization algorithm to turn your voice into digital data. Next, your question is routed through Apple servers, and a flowchart is run on it to find a possible solution. This step is easy enough for simple sentences like What is the weather like today?. But it becomes difficult when sentences like Will Larry be attending the meeting today? are asked because it is quite difficult for a machine to understand such a complex thought process. This is where NLP comes into play. NLP breaks the command down into tokens and uses syntactic analyzers to parse through and understand the sentence. In addition to this, Machine Learning algorithms are used to optimize the results and learn from the past results. Finally, the results are produced to the user.Did you know? Robots can now socialize! Kismet, an emotionally intelligent robot from MITs AI Lab affective computing experiment, can interact by recognizing human body language and voice tone.As the name suggests, Pattern Recognition is a part of Artificial Intelligence which deals with recognizing patterns in data. Its used for quality and process control. Applications includesself-driving cars, neuroscience, cancer treatment and energy physics.The much talked about Self-Driving Cars collects and analyzes Big Data from sensors and maps to identify pedestrians, vehicles and other objects based on their shape, size and pattern. After predicting what all the objects around it might do next, it is then designed to safely drive around them. The technologies used are radar, lidar, GPS, odometry, and computer vision.It is used to associate the energy depositions in a multi-component, non-magnetic high-energy particle detector. Higgs detection is a great example of pattern recognition in particle-physics.Did you know? Self-driving cars have many versions. Google has removed steering wheels and pedals and is improving on the different levels of autonomy that can be achieved. Whereas, Tesla and Baidu are making advancements in this technology by slowly adding autonomous features that enable efficient driving in different environments. Tesla has come up with a conventional car having Autopilot (i.e. self-driving) capabilities at a safety level which is much greater than that of a human driver.Image Analysis involves extracting meaningful information from images. The idea is to imitate the human visual cortex using Machine Learning algorithms like neural networks. Handwriting recognition, automatic image recognition and geomorphologic (form or surface features of the earth or another celestial body) terrain feature classification are some popular forms.The ImageNet challenge is a competition started in 2010. Here research teams submit programs that classify and detect objects and scenes. Since then, there has been excellent progress in image processing. In 2010, a good visual recognition program had around 40% classification error rate. In 2015,a deep convolutional neural net program for image recognition had about a 3.5% classification error rate!Image Analysis forms a big part of Facebooks auto-tagging feature. A facial recognition software is used to detect the categories of users friends to match the newly uploaded pictures with the ones that have been tagged elsewhere. This software uses Machine Learning algorithms like neural nets. The algorithm is fed with large amounts of training data and the machine then learns to classify and recognize people in the uploaded images and suggests to you friends who could be there with you in the photo. So Facebook is heavily investing in AI. They recently acquired FacioMetrics, a facial image analysis start-up-to delve deeper into AI research.These are the just some of the main advantages and applications of AI. The field is huge and has a lot more to this!Until a few years ago, Artificial Intelligence was mainly used by the military and the government with the help of a select few professionals in the field. But now more and more people are educating themselves and are becoming proficient in the field. They are now realizing the improvement that AI can make in a business. Today, AI is used practically in every field. Some of the possible career opportunities in AI include-While there are many other AI jobs, these are the most talked about ones under a broad umbrella. This is a wonderful time for anyone to start working in AI. The field is just getting started. Even if you are a beginner, learning new things every day and scaling up is the key.Artificial Intelligence is undoubtedly changing the world. It is making lives easier. But, as the efficiency of AI increases, so does the growing concern that it is changing the world too much, with the fear that machine intelligence would soon surpass human intelligence. The fear that the Terminator and Matrix (movies on AI) will become a reality is increasing too. So, to what extent are these fears warranted? Is there any truth to them at all?Ominously, the answer is yes. Dont get us wrong. We dont mean that there will be a machine uprising in the near future making humans obsolete, an inferior species. As of now, Artificial General Intelligence is a myth. It does not exist. AI still does not have the human cognitive abilities and may not so in the near future. But, we cannot entirely write off the possibility of this happening. It is certainly possible, even if it is very unlikely. Maybe in a few decades, or by the end of this century or many centuries later. Artificial General Intelligence and Superintelligence could become a reality.Superintelligence is the ability of a machine to seamlessly perform every task that a human can perform, and better! Thanks to their perfect recall (computers have an eidetic memory as opposed to humans), and ability to multitask, they will fare far better than humans at practically everything. The book Superintelligence: Paths, Dangers, Strategies by Nick Bostrom talks about exactly this- Superintelligence as a possible concept.Recently, there has been a lot of talk around AI automating humans and disrupting millions of jobs. As of now, machines are good at tasks that involve Big Data and a great amount of iteration. Machines dont have intuition and cant match humans ability to take decisions in tricky situations.Example- machines can analyse huge amounts of data far more accurately and quickly than a human can, but the final decision that a Data Scientist is always a mix of data and intuition, which comes with experience.AI has been surrounded by quite a lot of controversy. On one hand, companies (not only limited to tech giants) are investing millions in AI research and development. On the other hand, Stephen Hawking has voiced his concern that AI could be the end of mankind. Elon Musk & Bill Gates have also agreed with this.However, in the debate of AI being a boon or a bane, we believe boon will always win. This isnt because we are being ignorant about the catastrophic situations that will unfold if superintelligence is achieved. It is because steps are already being taken to prevent the potential hazards AI may bring along with it. AIs progress will continue only if it is in alignment with general human interest. So, dont fear it! Go ahead and just enjoy the revolution.So which camp do you belong to? The pro-AI camp or the AI-against one? Do you think AI will disrupt more jobs than it actually creates? We would love to hear your opinion.Rahul is a Data Scientist at UpX Academy. An alum of SP Jain School of Management, Rahul loves everything AI, ML and predictive analytics. An in-house data analytics expert at UpX Academy, Rahuls goal number#1 is to make UpX Academys students fall in love with data science irrespective of the background that they come from. He can be reached out at [emailprotected]",https://www.analyticsvidhya.com/blog/2016/12/artificial-intelligence-demystified/
"30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016",Learn everything about Analytics|Introduction||How to decide your own learning path?|How to use this article?|Table of Contents|Machine Learning For Beginners|Machine Learning: Advanced|Applications of Machine Learning,"1.How to become a Data Scientist in 6 months|2.Statistical Machine Learning Course|3. Machine Learning Course|4. Practical Machine Learning Tutorial with Python|5. Introduction  Learn Python for Data Science|6.Machine Learning Tutorial|7. Data Analysis in Python with Pandas|8. Machine Learning  CS50 2016|9.Analyzing and Manipulating Data with Pandas Beginner|10.What is Artificial Intelligence|11.Machine Learning Tutorial for Beginnerswith Azure ML|1.Machine Learning Recipes|2. Machine Learning with Text in Scikit-Learn|3.Machine Learning for Hackers|4.Introduction to Machine Learning on Apache Spark MLlib|5. Time Series Analysis with Python Intermediate|1.Breakthroughs in Machine Learning|2. Machine Learning & Art|3.How computers are learning to be creative|4. Machine learning to decode the genome|5.Machine Learning at Pinterest|6.How GrabTaxi uses machine learning to predict taxi availability|7.Getting Started with Amazon Machine Learning|8.Amazon Go  worlds most advanced shopping technology|9.10 Machine Learning based Products You Must See|10.Knowledge Graphs for a Connected World|11.The AI Gaming Revolution|12.Better Medicine Through Machine Learning|13. Build Smart Application with your own Superpower|14. Machine Learning: Googles Vision|End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Artificial Intelligence Demystified|Data Pre-Processing: A Crucial Element of Analytics  Driven Embedded Systems|
Analytics Vidhya Content Team
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"2016 has been the year of Machine Learning and Deep Learning. We have seen the likes of Google, Facebook, Amazon and many more come out in open and acknowledge the impact machine learning and deep learning had on their business.Last week, I published top videos on deep learning from 2016. I was blown away by the response. I could understand the response to some degree  I found these videos extremely helpful. So, I decided to do a similar article on top videos on machine learning from 2016.People often ask us How should I get started with data science & machine learning?There can not be just one answer to this question.The anatomy of machine learning is quite vast. One needs to decide their own framework and time period to get comfortable with machine learning. Through this article, I want to help you reach that comfort zone.In this article, I have compiled popular and most viewed machine learning videos, tutorials and courses from 2016. I want to help you to get started with machine learning and gain expertise building predictive models using machine learning. You are free to decide your own framework and time period to watch these videos. It is always advisable to take baby steps but feel free to mould it as per your wish.In order to help you, I have arranged the article and suggested a way to go through them. Hope you find it useful.In this article, I have segregated the videos for beginner & advanced. The first section of the article will introduce you to machine learning basics and the underlying theory to make you get comfortable with the machine learning.If you are just getting started with machine learning then this section should be your first stop. Go through these videos one by one, and put the learninginto practice with apractice problem.For those who already have a basic understanding of machine learning, you should start with the advance machine learning videos. These videos will introduce you to various machine learning libraries, modeling techniques and other advanced concepts of machine learning. Once you are thorough understanding with advanced concepts of machine learning, try your hands on Black Friday practice problem.After you have an understanding of how machine learning works, embark upon the next section on the applications of machine learning. These videos will blow your mind on how machine learning is changing the world. Find out how companies today are using machine learning to makes things simpler for us.Duration: 56:24 minsIn this video, Tetiana Ivanova shares her journey ofbecoming a data scientist in just 6 months. Participating in hackathons got her started with machine learning. If you have been wondering whether to go for analytics post-graduate programor become self-taught data science professional, you must watch this video. Tetiana shares her real life experience of making the career move, the hardships and truth behind the facade of a higher education. Either you are a beginner or someone transitioning his / her career to data science then I would recommend that you must watch this video. This video will leave you inspired.Duration: N/AIn this course from Carnegie Mellon University, it will take you through basics of machine learning and statistical modeling.You will learn about parametric & non-parametric regression, clustering, boosting, graphical methods, minimax theory, dimensionality reduction, etc. This course is best suited for students with asound background in statistics & mathematics. Alongside, there are assignments & solution which would further improve your concepts. Feel free to skip the initial few minutes of the first video.Duration: N/AThis course on machine learning from theUniversity of Waterloo will take you through machine learningbasics and advanced concepts.Its a conceptual course which will educate you on mathematical relations in ML algorithms.It has been taught by multiple professors including Shai Ben David, author of bookUnderstanding machine learning. It covers topics such as linear regression, bayesian, trees, clustering, neural networks, ensemble, hidden markov model and much more. Check out the other course material here. Feel free to skip the first 8 mins of the video.Duration: N/AThis course is designed for all the Python practitioners looking for a comprehensive introduction to machine learning. It covers theoretical & practical concepts on supervised, unsupervised and deep learning algorithms. In this series of videos, you will learn about linear regression, K-Nearest Neighbors, Support Vector Machines (SVM), flat clustering, hierarchical clustering, and neural networks. For each algorithm, the speaker discusses the real-life applications with the help of actual datasets. Then you will learn about the workings of each of the algorithms by recreating them in code. This will provide you a complete understanding of exactly how the algorithms work, how they can be tweaked to our advantage.Duration: N/AHeres yet another tutorial to learn Python for data science. If your hectic work schedule has been making it difficult for you to start with learning data science then these videos are at your rescue.In these 7 min videos, you will learn how to get started with data science. It will introduce you to sentimental analysis, recommendation system, predicting stock prices, create neural network using python & tensorflowandintroduction to genetic algorithms. The speaker is clear in his approach and will let you learn data science in your tea breaks. This tutorial requires basic understanding of Python.Duration: 3 hoursThis is an exclusive tutorial by Sebastian Raschka and Andreas Muller from the SciPy Conference held in July 2016. In this tutorial, Sebastianintroduces machine learning & Scikit Learn with sample applications. Then he will go on to explain the different computational tools for Python: NumPy, SciPy and matplotlib. Sebastian explains data representation using Iris dataset for machine learning. Andreas introduces you to classification and regression techniques in supervised learning. Sebastian then explains clustering technique for unsupervised learning and will make you familiar with the interface of scikit-learn, one of the widely used python libraries. It will also provide you hands-on experience in building apredictive model using Titanic dataset.Duration: N/APandas is full-featured Python library for data analysis, manipulation and visualization. With high its readability and general purpose use, Python is often a popular choice for beginners to start with data science. This tutorial is for Python users who want to understand the vast data and get started with data science. The series will introduce you to Pandas and what all you can do using Pandas library.In this 31 video series, the speaker will take you through each step of Pandas involved in data analysis.Duration: 1 hour 30 minsThis is a video from CS50 course taught at Harvard and Yale University. In this video, the speaker introduces you to machine learning and its applications.For all coders out there this is one of the best tutorials for you to get started with machine learning using Python. It is a simple introduction to machine learning and it is affecting our lives today. Learn how machine learning is being applied for building search engines, image recognition, voice recognition, and natural language processing. This tutorial will teach you image classification with Python and text clustering. Skip the first 9:05 mins of the video.Duration: 3 hour 30 minsAs I described above, Pandas is one of the popular libraries in Python.This tutorial will take you through analyzing and manipulating data in Python using Pandas. Pandas ecosystem is growing more & more and it user friendliness makes data analysis simpler. This tutorial is aimed at any beginner who wants to get started with data analysis in Python. It uses climate datasets to learn about Pandas.Duration: 9:21 minsArtificial Intelligence is a means to make machines smart enough to take actions on their own. There is a lot of buzz around AI but people often ask the question what is AI exactly? Here is a brief video which takes you to the origin of AI. Learn how AI has evolved into a mainstream topic and how its various applications are changing the world. AI has created a possibility for machines to differentiate between a dog and a human. Learn what are expert systems and how image recognition, robotics, deep learning are all inter-connected with AI.Duration: 2 hours 35 minsAfter the success of Hadoop basedAzure HDinsight Microsoft launched Azure Machine Learning (Azure ML) early this year. In this tutorial led by analytics experts from MicrosoftCorporation, you will learn about Azure machine learning and how it is contributing to large scale consumerization of Machine Learning. The tutorial has been divided into four modules. In this first module, you will learn about machine learning advantage and AzureML studio. The second module will learn about AzureML and build recommender solution using AzureML. In the third module, the speakers will walk you through monetizing AzureML with Microsoft Azure marketplace.Azure Marketplace hosts various exciting APIs that use ML, including the Bing Speech Recognition Control, Microsoft Translator, Bing Synonyms API and Bing Search API. As of today, Azure Marketplace has 25+ machine learning APIs. Watch this tutorial to get hands on experience on AzureML.Duration: N/AMachine learning is making systems so smart that they are getting closer & closer to replacing humans. In this these 10 min videos you will about the various applications of machine learning. By watching the first video itself you will be able to write your first code. In these videos you will learn about decision tree visualization, scikit- learn, tensorflow, how to build your own classifier, what is the most accurate features for your model and many more interesting concepts. The language used is Python. The videos are very informative and a must watch for any intermediate in data science.Duration: 2 hours 40 minsAlthough numeric data is easy to work with in Python, most knowledge created by humans is actually raw, unstructured text. By learning how to transform text into data that is usable by machine learning models, you drastically increase the amount of data that your models can learn from. In this tutorial, you will learn to build and evaluate predictive models from real-world text using scikit-learn. By the end of this tutorial, you will be able to confidently build predictive models from text-based data along with feature extraction, model building and model evaluation. This tutorial was delivered in PyCon 2016.Duration: N/AEver wondered how Netflix recommends shows based on your choice or how Amazon recommends you products they think you might also like. For any machine learning practitioner, these questions are no brainer. In this tutorial, the speaker introduces you to machine learning and how it is being used to solve various problems, build AI based games and many other application of ML. Well not just an introduction to these applications but you will learn howto build movie recommender system, chatbots, AI game and AI reader & writer. There are 5 min videos with key takeaways. These videos are meant for any machine learning hacker and requires one to have thorough understanding of machine learning concepts.Duration: 42:19 minsSpark MLlib is a library for performing machine learning and associated tasks on massive datasets. With MLlib, fitting a machine learning model to a billion observations can take only a few lines of code. Along with this one can leverage hundreds of machines. In this tutorial, a senior data scientist from Cloudera introduces you through Apache Spark from scratch. You will learn about how Spark works and its execution model. The speaker has used several examples to explain the interactivity which Spark offers.It also covers the use of Sparks DataFrames API for fast data manipulation, as well as ML Pipelines for making the model development and refinement process easier.Duration: 3 hoursIn this tutorial, you will learn why should you use time series and what is the importance of time series analysis. The speaker provides a quick 10 mins introduction to Pandas to provide a refresher. Then you will see time series in action and learn how to deal with calendar dates in Pandas. The speaker will teach you how to understand the different time-stamped data like US-GIS, NIH, FRB, etc. Learn about the common time series analytical tools, prediction and classification in time series.Duration: 28:26 minsMachine Learning is producing smarter gadgets and machines. Siri and Cortona is a result of major advances in machine learning. But, what goes behind creating these products. Lets learn that in this video from Google data science team. The team first takes you through speech recognition and how it has been made possible. Then understand how machine learning is used on graphs to make image classification and smart replies a reality. It is an interesting video which reveals all the backend operations a machine performs for three major machine learning applications developed by Google.Duration: 42:35 minsThe recent progress of machine learning is impressive, and the applications seem endless. Neural networks are incredible tools allowing artist not just to analyze but also manipulate and generate images, movies and music. In this video, the speaker explains how The Google Cultural institute in finding ways to use machine learning for art and culture. In my last article on deep learning videos, we saw a video to create neural art. In this video, the speakers takeyouthrough all the fun stuff one can do with machine learning like Train Mario games, create artful collages using machine learning, create digital interactive images & videos. It is a very interesting video andI would recommend everyone to must watch it.Duration: 17:34 minsHere is one of the amazing videos I have come across on applications of machine learning. Deep Learning is a subfield of artificial intelligence. Through deep learning, the aim of data scientists is to interpret the same functionality of a human brain into machines. In this Ted talk by Blaise Agera y Arcas, PrincipalScientist at Google he shares how machine learning algorithms and neural nets are used to build machine perception. In this video he shows how neural nets trained to recognize images can be run in reverse, to generate the same images. He explains this with several visual examples.Duration: 9:53 minsMachine Learningcan also be applied for understanding human genome, revealing a whole new world of personalized medication. In this video, Anshul Kundaje assistant professor of genetics and of computer science at Stanford explains how machine learning can be used for this purpose. He explains how the genomes of healthy individuals can be compared with their family members diagonised with a particular disease to identify disease associated genetic variants. I think this can be a revolutionary step in detecting early chances of diseases like Alzheimers and Cancer.Duration: 23:54 minsIn this video, Jure Leskovec, Chief Scientist at Pinterest explains how machine learning is used at Pinterest. Its motivating to see how ML is transforming ways of businesses on internet. Here,Jure explains different segmentsof Pinterest driven by machine learning which affectsnew user experience, interest recommendation, type of content, user actions prediction, pin ranking and visual features. Jure also shares insights about what worked for them and what lessons they learned. I think its an interesting take on how machine learning is changing our day-to-day lives.Duration: 11:24 minsPersonally, to me it is surprising to seehow machine learning can solve business problems at different levels. One such example is how Grab Taxi uses machine learning to tackle the problem of taxi availability. To handle this problem, Grab started a unique initiative of bidding for a ride by the drivers and the fastest bidder wins and is assigned theride. Watch the full video to find out how theyused machine learning to build a predictive model on drivers bidding probability and used real time data to solve the problem.Duration: 54:43 minsAmazon Machine Learning (Amazon ML) is a service that makes it easy for developers of all skill levels to use machine learning technology. Amazon Machine Learnings powerful algorithms create machine learning models by finding patterns in your existing data. In this tutorial, you will learn how to use machine learning with the data you already have to arrive at accurate and actionable predictions, i.e, to create smart applications. You will learn how to use and integrate Amazon ML into your applications to take advantage of predictive analysis in the cloud.Duration: 1:49 minsI think this is one of the most fascinating technologies that will blow off every hot & shot organized retail. Amazon is using computer vision, machine learning, deep learning algorithms and sensor fusions to give you an out of the world shopping experience. Just to be clear, I am not promoting Amazon Go but trying to show you what all machine learning can do.Duration: 18:35 minsTo see what magic of machine learning can create, watch this video featuring robotics trained with artificial intelligence & computer vision. These devices behave like any other human. Meetthese super smart robots who can perform any task you thought a machine wont be able to perform. This video is arevelation of how robotics may replace human beings in the few years.Duration: 48:41 minsTheConnected graph is theanswer to optimal business strategies and the key for growth in todays world. The ability to utilize the connected data to understand the relationship between any user or customer. In this video, the speaker explains the graph database technology which uses AI, machine learning, and deep learning. You will learn about the basics of connected graphs and how they work. How AI forms the basis of these connected graphs and see knowledgeable connected graphs in action with popular use cases.Duration: 10:30 minsArtificial intelligence powered computers have come a long way. The machines are so smart today that they can beat humans in any new game. AlphaGo gained significant attention when it won against the professional human Go player. But the questions is what goes in their brain and how are they able to perform that well. In this video, you will learn about Heuristics, production systems and deep neural networks which has madeAI games a reality.Duration: 17:56 minsEach & every industry is realizing the potential of Machine Learning. In this video, the speaker a machine learning practitioner in healthcare explains how machine learning can be used to detect diseases at their early stage. The speaker suggests the same technology which is used in self-driving cars, AI powered games can be used in detecting the early signs of a disease. If you are wondering how this is possible, then it is basically by using massive data available with hospitals and analyzing the patterns within the data. Imagine how many lives can be saved if hospitals start using machine learning on regular basis.Duration: 42:32 minsGoogle Photos and Google Translation is yet another example of machine learning applications. In this video from Google you will learn how developers can use machine learning at their leverage to build much powerful apps at their end.Learn how Academy Award-winning and recognized studios takes advantage of cloud economics and Googles on-demand computing to realize their creative visions and expand this digital medium for storytellingDuration: 44:44 minsHow Google uses machine learning?Hear from the Googles machine learning team, how they have been using machine learning to produce products one couldnt have imagined. Learn more about Googles take on machine learning and AI, how machine learning has streamlined Googles end products. Also, ithas deployed practical AI throughout its products and has brought an end user closer to the technology.I hope you enjoyed reading this article. Now that you know, all the popular and must videos in machine learning from 2016 tell me how are you planning to watch to them.I would also like to know your feedback on this article. Kindly, drop in your comments below and share your opinions.If you have watched any of these videos, share your reviews with me and other users in the comments section. I hope this article was a great value add to your knowledge.Through this article, I wanted to ensure that youre ahead of your learning goal. If there is any particular tool or technique you would like to focus on in 2017 then tell us which are they. Just for your knowledge, we are giving away reference guides for Machine Learning, Python, R, Big-Data, Deep Learning and many more. To download these guides, click here.",https://www.analyticsvidhya.com/blog/2016/12/30-top-videos-tutorials-courses-on-machine-learning-artificial-intelligence-from-2016/
Data Pre-Processing: A Crucial Element of Analytics  Driven Embedded Systems,Learn everything about Analytics|Introduction|Overview|Why Data Pre-processing?|Benefits,"About the Author||Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016|45 questions to test a Data Scientist on Regression (Skill test  Regression Solution)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The goal of the Internet of Things (IoT) is to acquire data from various embedded systems and impart analytical processes on that data to improve performance, efficiency, and business outcomes. The ability to create analytics that process massive amounts of business and engineering data is enabling designers in many industries to develop intelligent products and services. Designers can use analytics to describe and predict a systems behavior, and further combine analytics with embedded control systems to automate actions and decisions.Whether cloud-based or embedded, the first step in developing analytics is to access the wealth of available data to explore patterns and develop deeper insights. This abundance of data is explored through data pre-processing, a crucial, yet often understated step in the creation of analytics-driven embedded systems.Datasets can be large in size, come from many different sources and represent many different attributes. Therefore, the software tools used for exploratory analysis and analytics development should be capable of accessing all the data sources and formats that comprise the dataset. As real world data tends to be incomplete, noisy and inconsistent, data preparation is avery important procedure for both data warehousing and data mining.Data preparation includes data cleaning, data integration, data transformation and data reduction. Data cleaning steps can be used to fill in missing values, smooth the noisy data, identify outliers and correct data inconsistencies. Data integration combines data from multiple sources to form a coherent data store. Data transformation routines segregate data into appropriate forms for mining. Data reduction can be used to obtain a reduced representation of the data while minimizing the loss of information content.When exploring this wealth of information data pre-processing cleans and prepares the data before predictive models are developed. Predictions from incorrect data can be difficult to debug, or worse, can lead to inaccurate or misleading results that impact system performance and reliability. The goal here is to find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model. Some common techniques include feature selection to reduce high-dimension data, feature extraction and transformation for dimensionality reduction, and domain analysis such as signal, image, and video processing.The information gathered from data pre-processing is then taken and implemented across a number of analytics-driven embedded systems. An example of this is the innovation in using Big Data and analytics to make cars smarter. Automotive OEMs are collecting enormous amounts of data from real-world driving situations (think millions of miles of driving), recording data such as engine performance, video, radar, and other signals. This data is used to generate important metrics such as fuel economy and performance at the fleet level. Engineering teams are also using this real-world data to design, develop, and test new types of automotive systems, such as advanced driver assistance systems (ADAS).In some implementations, analytics are performed in the cloud with the intent to improve the performance of existing embedded systems. For example, BuildingIQ is a leading provider of advanced energy management software and designs climate-control systems to reduce energy consumption in commercial buildings. The analytics incorporated into these systems include engineering data from power meters, thermometers, pressure sensors, and other HVAC sensors combined with business data from weather forecasts, real-time energy prices, and demand response data. The result is a cloud-based service that is able to adjust the buildings existing HVAC embedded systems and lower energy consumption by up to 25%.In other cases, analytics are better run directly in an embedded system. For instance, a design team at Scania, the Swedish truck manufacturer, embeds analytics into their emergency braking systems to provide real-time crash avoidance to reduce accidents and meet stringent EU safety regulations. Engineering data from cameras and radar are processed in real time for object and road marking detection, which is subsequently fused to signal collision warning alerts and automatic brake requests.As the Scania case shows, theres a growing need to put more of the data pre-processing and data reduction on the sensor or embedded device itself in order to optimize speed and power. The accelerating Internet of Things trend towards smarter and more connected sensor networks is only adding to that pressure. This has the benefit of shrinking the amount of data that is transferred over the network, which reduces the cost of transmission and can lower the power consumption of wireless devices. For this reason, good system designers should conduct local preprocessing wherever they are able and only upload the useful information or predictive signal.As the number of possible applications increase, well be able to benefit from the impact of analytics-driven embedded systems across a wider variety of industries, where data pre-processing will continue to play an important step in how we shape our world and how we operate in it.Paul PilottePaul Pilotte has more than 20 years of experience in technical marketing and development in technical computing, security software, data communications, and test-equipment markets. He is currently a Technical Marketing Manager at MathWorks focusing on MATLAB toolboxes for statistics, optimization, symbolic math, and computational finance. He holds a Bachelors and Masters degrees in electrical engineering from MIT and an MBA from Babson College.",https://www.analyticsvidhya.com/blog/2016/12/data-pre-processing-a-crucial-element-of-analytics-driven-embedded-systems/
45 questions to test a Data Scientist on Regression (Skill test  Regression Solution),Learn everything about Analytics|Introduction|Overall Scores|Helpful Resources for Regression,"End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Pre-Processing: A Crucial Element of Analytics  Driven Embedded Systems|Hadoop developer-2|
Ankit Gupta
|51 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Regression is much more than just linear and logistic regression. It includes many techniques for modeling and analyzing several variables. This skill test was designed to test your conceptual and practical knowledge of various regression techniques.A total of 1845 number of people participated in the test. I am sure they all will agree it was the best skill assessment test on regression they have come across.If you are one of those who missed out on this skill test, then you did miss out on the real time test. But here the questions with detailed solutions, find out how many you could have answered correctly.Below are thedistribution of scores, this will help you evaluate your performance:You can assess your performance here. Around 530 people participate in the skilltestand the highest score was 38.Here are a few statistics about the distribution.Mean Score: 23.15Median Score: 23Mode Score: 23In case you want to revise your knowledge, here are a few resources for brushing up your knowledge on Regression.Going Deeper into Regression Analysis with Assumptions, Plots & Solutions5 Questions which can teach you Multiple Regression (with R and Python)7 Types of Regression Techniques you should knowSimple Guide to Logistic Regression in RA Complete Tutorial on Ridge and Lasso Regression in PythonUsing Platt Scaling and Isotonic Regression to Minimize LogLoss Error in RQ1. Which of the following step / assumption in regression modeling impacts the trade-off between under-fitting and over-fitting the most.A. The polynomial degreeB. Whether we learn the weights by matrix inversion or gradient descentC. The use of a constant-termSolution: AChoosing the right degree of polynomial plays a critical role in fit of regression. If we choose higher degree of polynomial, chances of overfit increase significantly.Q2. Suppose you have the following data with one real-value input variable & one real-value output variable. What is leave-one out cross validation mean square error in case of linear regression (Y = bX+c)?A. 10/27B. 20/27C. 50/27D. 49/27Solution: DWe need to calculate the residuals for each cross validation point. After fitting the line with 2 points and leaving1 point for cross validation.Leave one out cross validation mean square error = (2^2 +(2/3)^2 +1^2) /3 = 49/27Q3. Which of the following is/ are true about Maximum Likelihood estimate (MLE)?A. 1 and 4B. 2 and 3C. 1 and 3D. 2 and 4Solution: CThe MLE may not be a turning point i.e. may not be a point at which the first derivative of the likelihood (and log-likelihood) function vanishes.* The MLE may not be unique.Q4. Lets say, a Linear regression model perfectly fits the training data (train error is zero). Now, Which of the following statement is true?A. You will always have test error zeroB. You can not have test error zeroC. None of the aboveSolution: CTest error may be zero if there no noise in test data. In other words, it will be zero, if the test data is perfect representative of train data but not always.Q5. In a linear regression problem, we are using R-squared to measure goodness-of-fit. We add a feature in linear regression model and retrain the same model.Which of the following option is true?A. If R Squared increases, this variable is significant.B. If R Squared decreases, this variable is not significant.C. Individually R squared cannot tell about variable importance. We cant say anything about it right now.D. None of these.Solution: CR squared individually cant tell whether a variable is significant or not because each time when we add a feature, R squared can either increase or stay constant. But, it is not true in case of Adjusted R squared (increases when features found to be significant).Q6. Which one of the statement is true regarding residuals in regression analysis?A. Mean of residuals is always zeroB. Mean of residuals is always less than zeroC. Mean of residuals is always greater than zeroD. There is no such rule for residuals.Solution: ASum of residual in regression is always zero. It the sum of residuals is zero, the Mean willalso be zero.Q7. Which of the one is true about Heteroskedasticity?A. Linear Regression with varying error termsB. Linear Regression with constant error termsC. Linear Regression with zero error termsD. None of theseSolution: AThe presence of non-constant variance in the error terms results in heteroskedasticity. Generally, non-constant variance arises because of presence of outliers or extreme leverage values.You can refer this article for more detail about regression analysis.Q8. Which of the following indicates a fairly strong relationship between X and Y?A. Correlation coefficient = 0.9B. The p-value for the null hypothesis Beta coefficient =0 is 0.0001C. The t-statistic for the null hypothesis Beta coefficient=0 is 30D. None of theseSolution: ACorrelation between variables is 0.9. It signifies that the relationship between variables is fairly strong.On the other hand, p-value and t-statistics merely measure how strong is the evidence that there is non zero association. Even a weak effect can be extremely significant given enough data.Q9. Which of the following assumptions do we make while deriving linear regression parameters?A. 1,2 and 3.B. 1,3 and 4.C. 1 and 3.D. All of above.Solution: DWhen deriving regression parameters, we make all the four assumptions mentioned above. If any of the assumptions is violated, the model would be misleading.Q10. To test linear relationship of y(dependent) and x(independent) continuous variables, which of the following plot best suited?A. Scatter plotB. BarchartC. HistogramsD. None of theseSolution: ATo test the linear relationship between continuous variables Scatter plot is a good option. We can find out how one variable is changing w.r.t. another variable. A scatter plot displays the relationship between two quantitative variables.Q11. Generally, which of the following method(s) is used for predicting continuous dependent variable?A. 1 and 2B. only 1C. only 2D. None of these.Solution: BLogistic Regression is used for classification problems. Regression term is misleading here.Q12. A correlation between age and health of a person found to be -1.09. On the basis of this you would tell the doctors that:A. The age is good predictor of healthB. The age is poor predictor of healthC. None of theseSolution: CCorrelation coefficient range is between [-1 ,1]. So -1.09 is not possible.Q13. Which of the following offsets, do we use in case of least square line fit? Suppose horizontal axis is independent variable and vertical axis is dependent variable.A. Vertical offsetB. Perpendicular offsetC. Both but depend on situationD. None of aboveSolution: AWe always consider residual as vertical offsets. Perpendicular offset are useful in case of PCA.Q14. Suppose we have generated the data with help of polynomial regression of degree 3 (degree 3 will perfectly fit this data). Now consider below points and choose the option based on these points.A. Only 1B. 1 and 3C. 1 and 4D. 2 and 4Solution: CIf we fit higher degree polynomial greater than 3, it will overfit the data because model will become more complex. If we fit the lower degree polynomial less than 3 which means that we have less complex model so in this case high bias and low variance. But in case of degree 3 polynomial it will have low bias and low variance.Q15. Suppose you are training a linear regression model. Now consider these points.Which of the above statement(s) are correct?A. Both are FalseB. 1 is False and 2 is TrueC. 1 is True and 2 is FalseD. Both are TrueSolution: C1.With small training dataset, its easier to find a hypothesis to fit the training data exactly i.e. overfitting.2. We can see this from the bias-variance trade-off. When hypothesis space is small, it has higher bias and lower variance. So with a small hypothesis space, its less likely to find a hypothesis to fit the data exactly i.e. underfitting.Q16. Suppose we fit Lasso Regression to a data set, which has 100 features (X1,X2X100). Now, we rescale one of these feature by multiplying with 10 (say that feature is X1), and then refit Lasso regression with the same regularization parameter.Now, which of the following option will be correct?A. It is more likely for X1 to be excluded from the modelB. It is more likely for X1 to be included in the modelC. Cant sayD. None of theseSolution: BBig feature values = smaller coefficients = less lasso penalty = more likely to have be keptQ17. Which of the following is true about Ridge or Lasso regression methods in case of feature selection?A. Ridge regression uses subset selection of featuresB. Lasso regression uses subset selection of featuresC. Both use subset selection of featuresD. None of aboveSolution: BRidge regression will use all predictors in final model whereas Lasso regression can be used for feature selection because coefficient values can be zero. For more detail click here.Q18. Which of the following statement(s) can be true post adding a variable in a linear regression model?A. 1 and 2B. 1 and 3C. 2 and 4D. None of the aboveSolution: AEach time when you add a feature, R squared always either increase or stays constant, but it is not true in case of Adjusted R squared. If it increases, the feature wouldbe significant.Q19. The following visualization shows the fit of three different models (in blue line) on same training data. What can you conclude from these visualizations?A. 1 and 3B. 1 and 3C. 1, 3 and 4D. Only 5Solution: CThe trend of the data looks like a quadratic trend over independent variable X. A higher degree (Right graph) polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset. But if you see in left graph we will have training error maximum because it under-fits the training data.Q20. Which of the following metrics can be used for evaluating regression models?A. 2 and 4.B. 1 and 2.C. 2, 3 and 4.D. All of the above.Solution: DThese (R Squared, Adjusted R Squared, F Statistics , RMSE / MSE / MAE ) are some metrics which you can use to evaluate your regression model.Q21. We can also compute the coefficient of linear regression with the help of an analytical method called Normal Equation. Which of the following is/are true about Normal Equation?A. 1 and 2B. 1 and 3.C. 2 and 3.D. 1,2 and 3.Solution: DInstead of gradient descent, Normal Equation can also be used to find coefficients. Refer this article for read more about normal equation.Q22. The expected value of Y is a linear function of the X(X1,X2.Xn) variables and regression line is defined as:Y = 0 + 1 X1 + 2 X2+ n XnWhich of the following statement(s) are true?Note: Features are independent of each others(zerointeraction).A. 1 and 2B. 1 and 3C. 2 and 3D. 1,2 and 3Solution: DQ23. How many coefficients do you need to estimate in a simple linear regression model (One independent variable)?A. 1B. 2C. Cant SaySolution: BIn simple linear regression, there is one independent variable so 2 coefficients (Y=a+bx).Q24. Below graphs show two fitted regression lines (A & B) on randomly generated data. Now, I want to find the sum of residuals in both cases A and B.Note:Which of the following statement is true about sum of residuals of A and B?A) A has higher than BB) A has lower than BC) Both have sameD) None of theseSolution: CSum of residuals always zero.Q25. If two variables are correlated, is it necessary that they have a linear relationship?A. YesB. NoSolution: BIt is not necessary. They could have non linear relationshipQ26. Correlated variables can have zero correlation coeffficient. True or False?A. TrueB. FalseSolution: AQ27. Suppose I applied a logistic regression model on data and got training accuracy X and testing accuracy Y. Now I want to add few new features in data. Select option(s) which are correct in such case.Note: Consider remaining parameters are same.A. Only 2B. Only 1C. Only 3D. Only 4Solution: AAdding more features to model will always increase the training accuracy i.e. low bias. But testing accuracy increases if feature is found to be significant.Q28. The graph below represents a regression line predicting Y from X. The values on the graph shows the residuals for each predictions value. Use this information to compute the SSE.A. 3.02B. 0.75C. 1.01D. None of theseSolution: ASSE is the sum of the squared errors of prediction, so SSE = (-.2)^2 + (.4)^2 + (-.8)^2 + (1.3)^2 + (-.7)^2 = 3.02Q29. Height and weight are well known to be positively correlated. Ignoring the plot scales (the variables have been standardized), which of the two scatter plots (plot1, plot2) is more likely to be a plot showing the values of height (Var1  X axis) and weight (Var2  Y axis).A. Plot2B. Plot1C. BothD. Cant saySolution: APlot 2 is definitely a better representation of the association between height and weight. As individuals get taller, they take up more volume, which leads to an increase in height, so a positive relationship is expected. The plot on the right has this positive relationship while the plot on the left shows a negative relationship.Q30. Suppose the distribution of salaries in a company X has median $35,000, and 25th and 75th percentiles are $21,000 and $53,000 respectively.Would a person with Salary $1 be considered an Outlier?A. YesB. NoC. More information is requiredD. None of these.Solution: CQ31. Which of the following option is true regarding Regression and Correlation ?Note: y is dependent variable and x is independent variable.A. The relationship is symmetric between x and y in both.B. The relationship is not symmetric between x and y in both.C. The relationship is not symmetric between x and y in case of correlation but in case of regression it is symmetric.D. The relationship is symmetric between x and y in case of correlation but in case of regression it is not symmetric.Solution: DQ32. Can we calculate the skewness of variables based on mean and median?A. TrueB. FalseSolution: BThe skewness is not directly related to the relationship between the mean and median.Q33. Suppose you have n datasets with two continuous variables (y is dependent variable and x is independent variable). We have calculated summary statistics on these datasets. All of them give the following result:Are all the given datasets same?A. YesB. NoC. Cant SaySolutiom: CTo answer this question, you should know about Anscombes quartet. Refer this link to read more about this.Q34. How does number of observations influence overfitting? Choose the correct answer(s).Note: Rest all parameters are sameA. 1 and 4B. 2 and 3C. 1 and 3D. None of thesesSolution: AIn particular, if we have very few observations and its small, then our models can rapidly overfits data. Because we have only a few points and as were increasing in our model complexity like the order of the polynomial, it becomes very easy to hit all of our observations.On the other hand, if we have lots and lots of observations, even with really, really complex models, it is difficult to overfit because we have dense observations across our input.Q35. Suppose you have fitted a complex regression model on a dataset. Now, you are using Ridge regression with tuning parameter lambda to reduce its complexity. Choose the option(s) below which describes relationship of bias and variance with lambda.A. In case of very large lambda; bias is low, variance is lowB. In case of very large lambda; bias is low, variance is highC. In case of very large lambda; bias is high, variance is lowD. In case of very large lambda; bias is high, variance is highSolution: CIf lambda is very large it means model is less complex. So in this case bias is high and variance in low.Q36. Suppose you have fitted a complex regression model on a dataset. Now, you are using Ridge regression with tuning parameter lambda to reduce its complexity. Choose the option(s) below which describes relationship of bias and variance with lambda.A. In case of very small lambda; bias is low, variance is lowB. In case of very small lambda; bias is low, variance is highC. In case of very small lambda; bias is high, variance is lowD. In case of very small lambda; bias is high, variance is highSolution: BIf lambda is very small it means model is complex. So in this case bias is low and variance is high because model will overfit the data.Q37. What is/are true about ridge regression?A. 1 and 3B. 1 and 4C. 2 and 3D. 2 and 4Solution: ASpecifically, we can see that when lambda is 0, we get our least square solution. When lambda goes to infinity, we get very, very small coefficients approaching 0.Q38. Out of the three residual plots given below, which of the following represent worse model(s) compared to others?Note:A. 1B. 2C. 3D. 1 and 2Solution: CThere should not be any relationship between predicted values and residuals. If there exist any relationship between them means model has not perfectly capture the information in data.Q39. Which of the following method(s) does not have closed form solution for its coefficients?A. Ridge regressionB. LassoC. Both Ridge and LassoD. None of bothSolution: BThe Lasso does not admit a closed-form solution. The L1-penalty makes the solution non-linear. So we need to approximate the solution.If you want to read more about closed form solutions, refer this link.Q40. Consider the following datasetWhich bold point, if removed will have the largest effect on fitted regression line as shown in above figure(dashed)?A) aB) bC) cD) dSolution: DLinear regression is sensitive to outliers in the data. Although c is also an outlier in given data space but it is closed to the regression line(residual is less) so it will not affect much.Q41. In a simple linear regression model (One independent variable), If we change the input variable by 1 unit. How much output variable will change?A: By 1B. No changeC. By interceptD. By its SlopeSolution: DEquation for simple linear regression: Y=a+bx. Now if we increase the value of x by 1 then the value of y would be a+b(x+1) i.e. value of y will get incremented by b.Q42. Logistic Regression transforms the output probability to be in a range of [0, 1]. Which of the following function is used by logistic regression to convert the probability in the range between [0,1].A.SigmoidB. ModeC. SquareD. ProbitSolution: ASigmoid function is used to convert output probability between [0,1] in logistic regression.Q43: Which of the following statement is true about partial derivative of the cost functions w.r.t weights / coefficients in linear-regression and logistic-regression?A. Both will be differentB. Both will be sameC. Cant sayD. None of theseSolution: BRefer this linkQ44. Suppose, we are using Logistic regression model for n-class classification problem. In this case, we can use One-vs-rest method. Choose which of the following option is true regarding this?A. We need to fit n model in n-class classification problem.B. We need to fit n-1 models to classify into n classes.C. We need to fit only 1 model to classify into n classes.D. None of these.Solution: AIf there are n classes, then n separate logistic regression has to fit, where the probability of each category is predicted over the rest of the categories combined.Take a example of 3-class(-1,0,1) classification. Then need to train 3 logistic regression classifiers.Q45. Below are two different logistic models with different values for 0 and 1.Which of the following statement(s) is true about 0 and 1 values of two logistics models (Green, Black)?Note: consider Y = 0 + 1*X. Here, 0 is intercept and 1 is coefficient.A. 1 for Green is greater than BlackB. 1 for Green is lower than BlackC. 1 for both models is sameD. Cant Say.Solution: B0 and 1: 0 = 0, 1 = 1 is in X1 color(black) and 0 = 0, 1 = 1 is in X4 color (green)I hope you enjoyed taking the test and you found the solutions helpful. The test focused onconceptual knowledge ofregression and its various techniques.We tried to clear all your doubts through this article but if we have missed out on something then let me know in comments below. If you have any suggestions or improvements you think we should make in the next skilltest, let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/
Hadoop developer-2,Learn everything about Analytics,"Share this:|Like this:|Related Articles|45 questions to test a Data Scientist on Regression (Skill test  Regression Solution)|Cheatsheet: Scikit-Learn & Caret Package for Python & R respectively|
Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : able to build hadoop architecture
College Preference : tier1-entire
Min Qualification : pg
Skills : 
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2016/12/hadoop-developer-2/
Cheatsheet: Scikit-Learn & Caret Package for Python & R respectively,Learn everything about Analytics|Introduction,"|About Scikit-learn|About Caret|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.package in the comments below.|Share this:|Like this:|Related Articles|Hadoop developer-2|Launching Analytics Vidhya Secret Santa  Kick start 2017 with this gift!|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"For any Python or R practitioner, this article will prove to be a boon. We provide you cheatsheets for the mostwidely used machine library in Python & R each. Read on to know whats in store for you.Python has a rich and healthy ecosystem of various libraries for data analysis. But one of them stands out as the best and most effective library. No points for guessing, it is Scikit-Learn, one of the robust libraryfor machine learning in Python.Scikit-learn was initially developed by David Cournapeau as a Google summer of code project in 2007. In the same year, Matthieu Brucher joined the project. In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent Michel of INRIA got involved with the project and made the first public release, February the 1st 2010. Since then, several new contributions have been made to the project.Scikit-Learn provides arange of supervised & unsupervised algorithms and is built over SciPy. To get a hands-on experience on Scikit-Learn in Python for machine learning, heres a step by step guide.The R platform has proved to be one of the most powerful for statistical computing and applied machine learning. CARET (Classification And Regression Training) is one of the biggest projects in R. Caret package is all you to know for solving any supervised machine learning problem.Caretpackage is created and maintained by Max Kuhn from Pfizer. Development started in 2005 and was later made open source and uploaded to CRAN. Heres a practice guide for implementing machine learning with Caret package in R.Here are cheatsheets for Scikit-Learn and Caret package to help to gain prowessin Python & R respectively. To download the PDFs of these cheatsheets, click here.",https://www.analyticsvidhya.com/blog/2016/12/cheatsheet-scikit-learn-caret-package-for-python-r-respectively/
Launching Analytics Vidhya Secret Santa  Kick start 2017 with this gift!,Learn everything about Analytics,"How do you play Analytics Vidhya Secret Santa?|Share this:|Like this:|Related Articles|Cheatsheet: Scikit-Learn & Caret Package for Python & R respectively|Getting ready for AI based gaming agents  Overview of Open Source Reinforcement Learning Platforms|
Kunal Jain
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The first things which come to my mind, when I think about Christmas are holidays, family time and festivities! Yes  it is that time of the year.For us at Analytics Vidhya, our community is our extended family. Helping you learn is like breathing for us and ultimately seeing you grow in your career is our growth. If you were to judge by amount of time spent, then Analytics Vidhya would be our primary home. So, it is only natural that we do something special with our community members to mark the arrival of Santa Claus!Dont worry, we are not thinking of doing hackathons during this holiday season (although that could be fun!). We are actually playingSecret Santa withour community members (sshhh! Dont tell any one yet).Well, it is quite simple! Here are the steps:Once you do so, Santa will pull out a small little gift from Analytics Vidhya, especially for you.Hope you enjoy these gifts from our Secret Santa. If you like them, do leave gifts for us in the comments section below! Take me to Santa Claus
",https://www.analyticsvidhya.com/blog/2016/12/launching-analytics-vidhya-secret-santa-kick-start-2017-with-this-gift/
Getting ready for AI based gaming agents  Overview of Open Source Reinforcement Learning Platforms,Learn everything about Analytics|Introduction|Table of Contents|1. What is Reinforcement learning?|2. Examples of Reinforcement Learning|3. What is a Reinforcement Learning Platform?|4. Major Reinforcement Learning Platforms||5. Cheatsheet of Major Platforms|A few other notable platforms|Acknowledgements|End Notes,"i) Deepmind Lab|ii) OpenAI Gym|iii) OpenAI Universe||iv) Project Malmo|v) VizDoom|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Launching Analytics Vidhya Secret Santa  Kick start 2017 with this gift!|21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are living in exciting times. We are all set to create an army of smart machines and robots. Creating these machines has been a dream and one of the biggest challenges humans have faced. What adds to this excitement is that no one knows how these smart machines and robots will impact us in return. Will they end up taking people out of their jobs? Or Will they create new avenues and opportunities, which we humanscant think of as of now! One thing is for sure though  there is a lot of automation about to happen here!Researchers have created a roadmap for machine intelligence research. Following suite, some major platforms have been built to give way to this research. In this article, I explain Reinforcement learning in simple terms and comparemajor platforms for testing reinforcement learning algorithms. Kindly note, I have included only those platforms which have projects dedicated for the environment and not those which have integrated support for reinforcement learning algorithms.These platforms should enable a generation of research and new findings / developments in Artificial Intelligence and Machine Learning.Let us start with a simple analogy. If you have a pet at home, you may have used this techniquewith your pet.A clicker (or whistle) is a technique to let your pet know some treat is just about to get served! This is essentially reinforcing your pet to practice good behavior. You click the clicker and follow up with a treat. And with time, your pet gets accustomed to this sound and responds every timehe/she hears the click sound. With this technique, you can train your pet to do good deeds when required.Now lets make these replacements in the example:The above example explains what reinforcement learning looks like. This is actually a classic example of reinforcement learning.To apply this on an artificial agent, you have a kind of a feedback loop to reinforce your agent. It rewards when the actions performed is right and punishes in-case it was wrong. Basically what you have in your kitty is:Source: UTCS RL Reading GroupNow, I am sure you must be thinking how the experiment conducted on animals can be relevant to people practicing machine learning. This is what I thought when I came across reinforcement learning first.A lot of beginners tend to think that there are only 2 types of problems in machine learning  Supervised machine learning and Unsupervised machine learning. I dont know where this notion comes from, but the world of machine learning is much more than the 2 types of problems mentioned above. Reinforcement learning is one such class of problems.Lets look at some real-life applications of reinforcement learning. Generally, we know the start state and the end state of an agent, but there could be multiple paths to reach the end state  reinforcement learning finds an application in these scenarios. This essentially means that driverless cars, self navigating vaccum cleaners, scheduling of elevators are all applications of Reinforcement learning.Here is a video of a game bot trained to play flappy bird.Before we look into what a platform is, lets try to understand a reinforcement learning environment.A reinforcement learning environment is what an agent can observe and act upon. The horizon of an agent is much bigger, but it is the task of the agent to perform actions on the environment which can help it maximize its reward. As per A brief introduction to reinforcement learning by Murphy (1998),The environment is a modeled as a stochastic finite state machine with inputs (actions sent from the agent) and outputs (observations and rewards sent to the agent).Lets take an example,This is a typical game of mario. Remember how you played this game. Now consider that you are the agent who is playing the game.Now you have access to a land of opportunities, but you dont know what will happen when you do something, say smash a brick. You can see a limited amount of environment, and until you traverse around the world you cant see everything. So you move around the world, trying to perceive what entails ahead of you, and at the same time try to increase your chances to attain your goal.This whole story is not created by itself. You have to render it first. And thatis the main task of the platform, viz to create everything required for a complete experience  the environment, the agent and the rewards.DeepMind Lab is a fully 3D game-like platform tailored for agent-based AI researchA recent release by Google Deepmind, Deepmind lab is an integrated agent-environment platform for general artificial intelligence research with a focus on first person perspective games. It was built to accomodate the research done at DeepMind. Deepmind lab is based on an open-source engine ioquake3 , which was modified to be a flexible interface for integration with artificial systems.Things I liked
Things I did not like
Resources to explore further:(OpenAI Gym is) A toolkit for developing and comparing reinforcement learning algorithmsOpenAI Gym is aplatform for creating, evaluating and benchmarking artificial agents in a game environment. The best thing I like about gym is that along with the toolkit, there is a community support built around it, viz an evaluation platform, code sharing platform and a discussion platform. Gym platform consists multiple categories of environment along with sample solutions provided by the communityThings I likedThings I did not likeResources to explore further:Universe is a software platform for measuring and training an AIs general intelligence across the worlds supply of games, websites and other applicationsThis is essentially an extension to OpenAI gym, with support for literally anything you can do on a computer. Universe is built to emulate how a human interacts with a computer. It uses Virtual Network Computing to access a computer remotely, packages any program and converts it into a gym environment.

Things I likedThings I did not likeResources to explore further:The Malmo platform is a sophisticated AI experimentation platform built on top of Minecraft, and designed to support fundamental research in artificial intelligence.Project Malmo is a research initiative by Microsoft research, with an aim to build AI agents to do complex tasks. Minecraft is a perfect scenario for building AI agents, and that is why they chose it.Things I likedThings I did not likeResources to explore further:Doom-based AI Research Platform for Reinforcement Learning from Raw Visual InformationI personally found this the most interesting platform to build AI agents, as you can have a multi-agent support with a competitive environment to test the agent. The platform runs on Doom, a first person shooting game, with a variety of levels and modes.Things I likedThings I did not likeResources to explore further:Thanks to AV community and reddit community for the helpful discussions. Special thanks to johny_cauchy, kendingpku and Kaixhin for their feedback.In this article, we briefly looked at what reinforcement learning is. I have listed all themajor platforms for RL research. Most of these rely on gaming environments to simulate real life conditions. If you know other platforms for reinforcement learning, do let me know in the comments below!Have your worked on any of these platforms? Share your experience by dropping in your comments. If you have any doubts / suggestions / feedback I would love you hear it. Feel free to post your comments.",https://www.analyticsvidhya.com/blog/2016/12/getting-ready-for-ai-based-gaming-agents-overview-of-open-source-reinforcement-learning-platforms/
"21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016",Learn everything about Analytics|Introduction|Who can benefit from these videos?|Table of Content|Tutorials for Beginnerson Deep Learning|Deep Learning  Advanced|Application of Deep Learning|ReinforcementLearning|End Notes,"1.Deep Learning Simplified|2. Bay Area Deep Learning School 2016, Stanford  Day 1|3. Bay Area Deep Learning School 2016, Stanford  Day 2|4. Tutorial : Deep Learning|5. Deep Learning with Neural Networks & Tensorflow Introduction|6. Neural Networks for Machine Learning|7. Intro to TensorFlow|8. Neural Networks|9. Neural Network that Changes Everything|10. Wide & Deep Learning with TensorFlow  Machine Learning|11. Introduction to Deep Learning|12.Deep Learning Demystified|1.Deep Learning Summer School, Montreal 2016|2. Deep Learning Tutorial  Advanced|3.Deep Learning in Practice Speech Recognition and Beyond|1. Googles Deepmind Explained|2. Self-Driving Cars and Deep Learning GPUs  NVIDIA|3. 9 Cool Deep Learning Applications|4. Deep Learning Program learns to Paint|1.Introduction to Reinforcement Learning with Function Approximation  Tutorial|2. Deep Reinforcement Terrain Learning|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Getting ready for AI based gaming agents  Overview of Open Source Reinforcement Learning Platforms|Exclusive AMA with Data Scientist  Sebastian Raschka|
Analytics Vidhya Content Team
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Until afew years back, deep learning was considered of a lesser importance as compared to machine learning. The emergence of neural networks & big-data has made various tasks possible.Back in 2009, deep learning was only an emerging field and only a few people recognized it as fruitful area of research. But soon it gained momentum and is used today for several applications. Speech recognition, image recognition, finding patterns in a dataset, object classification in photographs, character text generation, self-driving cars and many more. Hence it is important to be familiar with deep learning and its concepts.To make it easier for you to learn deep learning, I have curated list of youtube videos, tutorials and courses on deep learning from 2016. The list includes talks & tutorials from deep learning summer school, summits, and conference.I hope you will find this helpful.The videos have been shortlisted for beginners, intermediates and experts in deep learning.The article has been divided into sections for each one of you. If you a novice or intermediate in deep learning then start with the first section. If you want to master deep learning then this article will be your best resource. First, make a schedule and start learning deep learning. I bet in few weeks you will at least be able to build your first model in deep learning.For experts in deep learning, the advanced sectioncontains good videos for you to enhance your knowledge. The 5 mins videos in the beginners will be a good refresher for you.For all deep learning / data science enthusiasts you will love the applications of deep learning and examples segregated in a separate section. There are videos on Google Deepmind, learn to paint using deep learning and how deep learning ismaking self-driving cars a reality.There is also a small section on reinforcement learning with one of its applications.Read on!Duration: N/AIf the complicated terminologies makes it difficult for you to learn deep learning, then this tutorial will prove to be a boon for you. This is a simplified tutorial on deep learning and its basic concepts. You will learn about neural networks, deep net, deep belief nets, convolutional neural networks, H2O.ai and This tutorialwill give you a basic understanding aboutdeep learning. You will also learn about different kind of models, why & when to choose each one of them. Then it will provide you hands on experience on deep learning with different use cases. You will also learn about different platforms where you can build your own deep nets and the different libraries available for deep learning. The tutorial is devoid of any mathematical calculations or coding and is best for anyone looking to get a basic idea about deep learning.Duration: 10 hours 33 minshttps://www.youtube.com/watch?v=eyovmAtoUx0As Andrew Ng correctly puts it, Deep Learning is changing the industry landscape and there is a lot of interesting deep learning applications. This video showcases Day 1 of Bay Area Deep Learning School 2016. It covers talks on Introduction on Feedforward Neutral Network by Hugo Larochelle, Deep Learning for Computer Vision by Andrej Karpathy, Deep Learning for NLP by Richard Socher, Tensorflow Tutorial by Sherry Moore, Foundations of Deep Unsupervised Learning by Ruslan Salakhutdinov and Nuts and Bolts of Applying Deep Learning by Andrew Ng. All the deep learning experts have explained theunderlying concepts of deep learning in asimplified manner to give you a basic understanding of deep learning. They share use case problems to explain the real-life application of deep learning for each topic.Duration: 10 hours 33 minshttps://www.youtube.com/watch?v=9dXiAecyJrYThis is the Day 2 video of Bay Area Deep Learning school. It showcases talks on Foundation of Deep Reinforcement Learning by John Schulman, Introduction to Theano: A Fast Python library for Modelling & Training by Pascal Lamblin, Speech Recognition and Deep Learning by Adam Coates & Vinay Rao, Machine Learning with Torch & Autograd by Alex Wiltschko, Sequence to Sequence by Deep Learning by Quoc Le, Foundation and Challenges of Deep Learning by Yoshua Bengio. These deep learning practitionersare most searched deep learning practitioners and serve with companies like Google Brain, Twitter to name a few.Duration: 2 hours 29 minsIn this tutorial on Deep Learning Yoshua Bengio and Yann Lecun explains the breakthroughs brought by deep learning in the recent years. After their in-depth research of 30 years, Yoshua & Yann share the insights on how deep learning has transformed machine learning & AI. In this tutorial, you will learn how deep learning allows computational modelscomposed of multiple processing layers to learnrepresentation of data. These methods have improved speech recognition, visual object recognition, object detection and domains like genomics. This tutorial will take you through the basics of deep learning, discuss its various applications and what challenges it poses in front of us.Duration:N/AIf you have been wondering how neural network works and why recently there is so much of uproar created by them. In this tutorial on introduction to neural networks,you will learn how neural network are able to create powerful models with huge datasets. Understand the structure of neural networks and how each input layer combines together to generate an output. This is the only first video of the complete tutorial, forTensorFlow Basicswatch part 2 of the tutorial. To know how to build a neural network model, continue watching part 3 and so on.Duration: N/AThe main idea behind studying artificial neural networks is to understand the style of parallel computation of neurons and their adaptive connections. In this course by Prof. Geoffrey Hintontaught at University of Toronto you will learn how neural networks and machine learning can bring a revolution in technology. It includes topics such as perceptrons, back propagation, CNN, RNN, gradient descent, bayesian optimization of hyperparameters and many more topics. This is one of the best courses available out there on deep learning. If you are a deep learning enthusiast, you cant just afford to miss it.Duration: N/AOne of the most popular machine learning library right now is TensorFlow. Though it was built for conducting machine learning and deep neural network research primarily. But because of itsversatility, Tensorflow can be used in variety of applications. Here in this interesting tutorial on TensorFlow you will learn to build a handwritten digit image classifier in Python in under 40 lines of code. You will also learn how to generate music in TensorFlow, what is Tensorboard, build a neural network and pros & cons of using TensorFlow over other deep learning libraries. This brief tutorial on TensorFlow is a must watch for any novice in deep learning.Duration: N/AArtificial neural network are capable of learning and they need to be trained. There are basically 3 steps for building a machine learning model  build it, train it and test it. Once the model is built it can be trained to become better & better at pattern recognition. In these quick 5 min videos, you will learn to build neural network, build autoencoders and build a recurrent neural network. The codes for each video is also available in the description on youtube.Duration: 14:16 minsConvolutional neural networks is a combination of deep neural networks and kernel convolutions. In this video, it is explained how convolutional neural networks is a step change in image classification accuracy. If you are a deep learning enthusiast with very little knowledge of neural networks, watch this video. It will explain how deep learning is used to estimate the price of a house.Duration: 3:24 minsWide and deep learning combines the power of memorization and generalization used for training wide linear models and deep neural networks. In this video, learn about the implementation of this in easy to use API in TensorFlow. They are useful for large scale regression and classification problems with sparse inputs like recommendation system, search and ranking problems. Explore the possibilities of wide and deep learning with this video.Duration: 11 minsThis video will provide you a mathematical explanation to deep learning. It will take you through a basic introduction on how machinesfindthegrouping of different variables and takedecisions. If you are a mathematics person then this will be an apt explanation for you to parameters of model building. It very easily explains neural networks and how the varied input variables affect an output.Duration: 22:18 minsThis tutorial on deep learning is a beginnersguide to getting started with deep learning. In this tutorial, you will learn how deep learning is beneficial for finding patterns. Learn about neural networks with a simplified explanation in simple english. It will first introduceyou tothe structure of neuron and how they work. Then it proceeds further to explain how all neuronsform a pattern within each other. Then learn about the various applications of deep learning in real life.Duration: N/AThe Deep Learning summer school of Montreal saw experts and practitioners of deep learning from all ages group. This tutorial is aimed at individuals having basic understanding of deep learning and neural networks. Here it showcases talks on Recurrent Neural Networks by Yoshua Bengio, Theoretical neuroscience & deep learning theory by Surya Ganguli, Reasoning summit and attention by Sumit Chopra, Large Scale Deep Learning with Tensorflow by Jeff Dean, Learning Deep Generative Models by Ruslan Salakhutdinov, GPU programming for Deep Learning by Ryan Olson and many more informative talks. If you missed out on the summer school and all the informative content that was shared, here is the list of all the talk.Duration: 1 hour 36 minsIn past few years, the techniques of image classification, segmentation, and object detection have evolved tremendously withDeep Learning. This tutorialwill take you through the advance concepts of Deep Learning focusing mainly on computer vision and image processing using Theano & Lasagne. Alongside, the speaker also discusses important tips & tricks such as dealing with less training data etc. To understand concepts, prior knowledge of algebra, calculus and machine learning is required.Duration: 34:46 minsAndrew Ng needs no introduction, his contributions to deep learning are recognized well. He was one of the first ones to recognize the potential deep learning beholds for the world. In this one-on-one conversation with Andrew Ng, he shares his experience of working with deep learningand the technology advancements which has been brought by deep learning. He talks about how theemergence of big-data is disrupting all the industries today. Watch this complete video to know more about the future of deep learning and data science.Duration: 13:44 minsIt was a historic moment when Googles AlphaGo beat the world champion Lee Sedol in the ancientboardgame GO. It triggered anew wave of technology advancement when a machine succeeded over a human.Google Deepmind claimed to bring the next generation of AI and aims to develop programs which will be smart enough to take actions on their own. This video explains when and why Deepmind was founded. And what revolution it can bring in AI.Duration: 1 hour 7 minsThe CEO of NVIDIA Jen-Hsun Huang shares how the deep learning and research has changed the face of self-driving cars making it a reality. He opens the talk by introducing the worlds first AI supercomputer for self-driving cars designed by NVIDIA. He explains how deep neural networks & big data has been used to solve the problem of GPUs. This video will blow your mind that how deep learning and AI is making the impossible become a reality.Duration: 4:43 minsWondering what are some of the interesting the real-life applications of Deep learning and machine learning? This video showcases the real-life applications of deep learning. You will come across some intriguing applications like toxicity detection for different chemical structure, mitosis detection for large images, sequence generation, how a computer program itself plays pong and many more interesting applications.Duration: 4:43 minsArtificial neural networks are inspired by the human brain and the aims to study the connection between the neurons. In the above videos, we have seen several applications of deep learning. But Neural art happens to be the most amazing and surprising application of deep learning. In this video, you will learn to how to paint using deep learning or re-create famous painting using artificial neural networks.All the user needs to do is provide an input photograph and a target image from which the artistic style will be learned.Duration: 2 hours 18 minsReinforcement learning is a technique developed by machine learning and research communities for making optimal sequential decision making. This tutorial will provide you athorough understanding of the underlying formal problem (Markov decision processes) and its core solution methods, including dynamic programming, Monte Carlo methods, and temporal-difference learning. It is focussed on how these methods are combined with parametric approximation to find good approximate solutions to problems that are otherwise too large to be addressed at all. The speaker will also take you through the recent developmentsin function approximation, eligibility traces, and off-policy learning.Duration: 3 minsIn this video, a combination of deep learning and reinforcement learning is depicted which is thought to be useful in solving many extremely difficult tasks. Google DeepMind built a system that can play Atari games at a superhuman level using deep reinforcement learning.This video shows an interesting use of deep reinforcement learning to teach terrain animals map their movements and avoid obstacles in the way.This article contains acurated list of videos on deep learning and reinforcement learning. The videos have been shortlisted on the basis of the year, view count and relevance. There is ample of content on the web and we aimed to provide the most relevant videos.Go through the list and shortlist the videos which you find suitable for you. I have tried to add all the relevant videos from 2016. But if I have missed out on any video which you think deserves a mention in the list, feel free to add them below in the comments sections. If you are a visual learner, let me know what are your thoughts on the article.My aim is to help a larger audience to learn deep learning. Looking forward to your suggestions.",https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/
Exclusive AMA with Data Scientist  Sebastian Raschka,Learn everything about Analytics|Introduction,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016|10 Super exciting Data Science / Machine Learning / Artificial Intelligence based startups in India|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Sebastian RaschkaAt Analytics Vidhya, we are always in pursuit of providing you learning and networking opportunities. We bring you closer to the bestdata scientists from around the globe. Recently, we hosted Sebastian Raschka, one of the top data scientist & authorof the bookPython Machine Learning.Sebastian is also an open source contributor and the methods implemented by him are successfully used in machine learning competitions worldwide.Sebastian enjoys interacting with people and motivating them to pursue their interest in machine learning.He humbly agreed to do anAMA sessionwith our community members.Here we present to you the extracts from the AMA. If you missed this AMA then you did miss out on a great opportunity. But read on to find answers to all the questions lingering in your minds.Q1. How did you get interested in Biology? Why Computational Biology?Sebastian:Thats a good one to start. Ive always been interested in technology and natural sciences. I wouldnt say that I was much into programming during my high school years, I was building my first websites using HTML, CSS, and JavaScript, organized LAN parties, and build some video game mods, though.In Germany, college works a bit differently compared to other countries like the US, where you can explore a bit and pick your majors and minors. So, where I grew up [in Germany], we had to pick one particular field for our undergrad studies. I picked biology (my thesis was in developmental genetics), since I saw biology, in a certain sense, as the field that satisfied my curiosity figuring out how life works  on a molecular level. However, Ive always been a technology tinkerer and doing experimental work in a wet lab environment isnt really my thing. I love programming, statistics, algorithms, and computational data analyses/data mining/data science too much. Thus, I eventually decided joining a purely computational lab for my graduate studies.Q2.How ismachine learningapplied in Computational Biology?Sebastian: I LOVE machine learning. However, machine learning, in a certain, sense is just a tool that helps us automating tasks such as predictive modeling and discovering hidden structures in data. Since (computational) biology is all about making predictions and interpreting certain phenomena, theres a wide variety of tasks where machine learning can be usefully applied. For instance, say we want to discover/develop a new drug that regulates a certain biological process. Often, we are looking for a small molecule that binds to a certain protein that triggers a certain mechanism. So, we often end up looking for a binding partner of a protein. Here, we can leverage machine learning to assess how well certain molecules bind to our protein of interest.Ballester, Pedro J, and John BO Mitchell. A machine learning approach to predicting protein ligand binding affinity with applications to molecular docking. Bioinformatics 26.9 (2010): 1169-1175.Although I wasnt necessarily using machine learning to device a algorithms for predicting a native protein-ligand complex, I recently developed a novel approach for this task that provides us with a new feature, and I got some promising results using ML ensemble methods to build a powerful scoring function for protein-ligand scoring based on different features of a protein-ligand complex.Continuing with the example above, we certainly want to make sure that our potential molecule of interest isnt toxic if we want to use it as a pharmaceutical drug. Also here, many machine learning approaches have been developed to predict the toxicity of chemical molecules, for example:Now, lets say we predicted a bunch of potentially promising drug-like molecules. These are just predictions, and, ultimately, we want to have them tested in an experimental assay. In my work, I am lucky that I get to collaborate with many experimental biologists, who can do these tests and report the results back to me. Or in other words: I get to make the predictions, my collaborators test them, and I get to analyze the results. Using machine learning jargon, I ultimately end up with at dataset for supervised learning, which I can use to make predictions about potentially active or inactive compounds, and I can use it to infer subsets of features that are essential for activity using machine learning.
Now, this was just one example of using ML in computational biology, but there are many, many cases where ML comes in handy! Read here(Detecting the native ligand orientation by interfacial rigidity).Understanding the physical attributes of protein ligand interfaces, the source of most biological activity, is a fundamental problem in biophysics. Knowing the characteristic features of interfaces also.Q3.Who is a data scientist? Does one need formal training to become a data scientist?Sebastian: A few years ago, I attended a talk about big data analytics where the speaker spent ~30 min on clarifying that any kind of science or research IS data science, since any kind of research involved some sort of data. Of course, I agreed with his point but lets just roll the term data science roll over our tongue.These days, there are many different origin stories for the term data science:The term Data Science was coined at the beginning of the 21st Century. It is attributed to William S. Cleveland who, in 2001, wrote Data Science: An Action Plan for Expanding the Technical Areas of the Field of Statistics. Read here.Here what Jeff Hammerbacher says: I told this story at my presentation at Interface 2013. After a team offsite in February 2008, I decided that we needed to combine the Data Analyst and Research Scientist job titles in our team into a single job title. I proposed Data Applications Scientist initially; after some discussion with the team, we settled on Data Scientist in early March 2008. Read here.However, in my opinion, the gist is that a data scientist is a person that possesses a certain number of skills: programming, statistics, machine learning, data visualization, and communication skills. A person, given a question and a bunch of data, knows how to leverage computational tools to slice and dice the date to answer (or raise) a certain question.Ultimately, I would classify all researchers and scientists as data scientists if we just consider the term. However, used a job description, I think a data scientist is a person who uses computational tools, statistics, machine learning, etc. to ask & address questions from provided datasets.Q4.What is the best way to stay up to speed with new tools and techniques in data science?Sebastian: Staying up to date with the latest developments is certainly not a trivial task given the rapid development of new technologies. I am not sure if thats the silver bullet, but personally, I do a daily (often fairly quick) sweep through my twitter timeline, sub-reddits (python, machinelearning, datascience), and Wired to see whats going on in the world, in order to keep up to date in terms whats out there. Regarding learning new tools, I tend to be focussed on projects themselves and prefer tools I already know to solve my problems at hand satisfactorily.Although, I love tinkering with new tools, I try to spend my limited free time to educate myself more in terms of broader concepts (machine learning theory, statistics, etc.), since tools are just manifestations of these and somewhat volatile. If I have a problem to solve, I would first formulate the number of steps (or analyses). Then, I would consider tools in my repertoire that help me implement these tasks. If these tools are not sufficient, I will consider new / alternative tools. Please note that I am not saying that picking up new tools is not worthwhile, what I want to get at is that the day has only 24 hours, and I am trying not to get distracted by tinkering if it isnt necessary.It is a good idea though to keep up with the status quo to be ready to learn new tools / techniques if needed. For example, I used (and use) Pandas DataFrames a lot in my projects. For instance, I suddenly got logfiles that couldnt fit into memory in a certain project. Here, I just adopted Blaze /Dask for the accompanying analysis tasks, knowing that it was out there. Maybe, to summarize my main point(s). I think that learning a tool just for the sake of learning tool may not be worthwhile if you dont use it for your task at hand. But knowing that such a tool exists when you eventually need it is a good thing.Q5.Do you see a time where the complete machine learning can be automated?Sebastian: Currently, I do not have a particular time in future in mind where machine learning would be completely automated. Many people are working on this, including a good friend of mine, Randy Olson, who is developing TPOT at UPenn. I just visited his research group last week, and they have a lot of exciting things in development.However, I still see automation tools as a companion for a data scientist / machine learning person, not a replacement. Concretely, it is currently something that you may fire up and run in the background and compare your own results. While the building machine learning pipelines can be more easily automated, I think the real challenge is in dealing with different questions and data sources. Here, the scientist still has to define the question, scope, and the general approach. Machine learning automation tools can then be used for the tedious tasks in a project, such as hyperparameter tuning and comparing different data processing approaches.Q6.Can you please suggest any best resource tounderstand the completemathematical picture of advanced / basic data science algorithms?Sebastian: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is my personal favorite resource for that (it got many updates over the years, and the PDF is available for free online if youd like to take a look.Q7. What is your daily day routine like? how much time you dedicate to learning every day? What do you do when you find some free time(if any)? What are the best ways to keep up with the data science learning tempo? If you were 23(age) right now how would you go about learning the plethora of resources on Data Science  Like if you had to make a timeline what would you do when in some order in coming years? What are some things in life you learned the hard way and would not like to see others face it in their lives the way you did  Some words of caution?Sebastian: In my Ph.D. program, it is expected to spend like 8 hours a day in the lab (~40 hour/week). Since I am currently the only grad student in our lab, there are tons of things to do at work: helping/co-supervising our undergrads, doing sys admin stuff, working with our collaborators and doing my own research of course. Although theres always something new I learn at work every day, however, one thing that is very dear to me is making a daily news sweep to see whats going on in the (tech) world. Often, I tend to spend like 30min on reading one or more current articles that interest me, and save the rest to my growing for later pile :wink:. Whats also important to me is working a bit on my hobby projects and current areas of studies. Currently, I am taking Geoff Hintons Neural Network class on Coursera again and try to code things up in Python in parallel to exercise my coding skills and check if I understand everything correctly. In addition to reading about concepts, coding them up has always helped me to get a better grasp on things! Regarding the word of caution: First of all, make sure that you get enough sleep and have a healthy social life! :slightly_smiling_face:. I noticed that too much work can really wear me out over time, so finding some balance between time in front of the computer & exercising and spending time with friends is very important to me to recharge batteriesRegarding the word of caution: First of all, make sure that you get enough sleep and have a healthy social life! I noticed that too much work can really wear me out over time, so finding some balance between time in front of the computer & exercising and spending time with friends is very important to me to recharge batteries sometimes!Also, I tend to get more and more selective regarding things I want to learn. Our time is limited, and I realized that I cannot learn everything I want to learn, so I try to stick to the things that are most relevant to my current projects at work, and things that I think are particularly interesting to me. In other words, I try to be selective and focus one a few things deeply rather than spreading myself to thin.Q8.Deep Learning Vs machine learning? Will deep learning take over ML?Sebastian: I dont think so! There are a lot of areas where deep learning has become the status quo, like natural language processing and image classification. However, deep learning models are inherently very complex models, and they require a lot of data (and currently, that data has to be in the right format / representation). There are many tasks where we can benefit from classical machine learning. Typically, I would start addressing a question / solving a problem using the simplest approach first. Even if we have sufficient data to train deep learning models, there are many factors besides mere generalization error or accuracy. For instance, interpretability, time to train the model, etc.Q9.As a newbie in Deep Learning, which specific package I should Master: Deep learning with Keras or Deep learning with H2O?Sebastian: I havent used H2O, yet, but I really love Keras. It has a very clean API build on top of TensorFlow, and it is not really easy to use and very flexible. I would say that Keras is also the more popular one (but I may be wrong), and I could see that theres a better chance for long-term development of the package since the community seems to be larger. In any case, I would maybe just pick one of the two and focus more on deep learning concepts rather and see how you could implement them in one of the packages.Tools change, new tools are being developed, and who knows what package will be the best one in a few years. Thus, focussing on concepts and being a bit flexible in terms of packages may not be a bad thing to do. Often, a single package is also not enough to do anything you want to do. By that, I mean it can be useful to focus on the techniques/models you want to implement and pick a package that is best at a certain task if that makes sense.Q10.I have worked on a few ML projects and I wanted to know if there is any specific strategy that you follow for model selection? Also what strategies do you follow to boost up your model performance by optimizing hyper-parameter?Sebastian: It depends a bit on the size of the dataset and the time budget, and the overall goal of the project (what performance score would already be good enough, (how important is interpretability?). For model selection / hyperparameter search, I typically just do grid search or randomized search since these tasks can be easily run in parallel, on multiple processors.Q11.Can you tell us how you deal with the deficiency of a good python visualization library?Sebastian: I am actually quite happy with matplotlib and use it most of the time. Sure, the API is probably not that friendly for a newcomer, but I learned to deal with it over the years. But yeah, I have a huge bunch of snippets, notes, and templates for using matplotlib that I need to consult rather frequently.However, I find matplotlib really flexible and I can usually always get it to do what I want (and with the newly added styles, you can also make the plots relatively pretty). I also use seaborn quite frequently, especially the heatmap function. And sometimes, I also use Rs ggplot2 (although, not that frequently anymore). At SciPy 2016, Brian Granger presented a new, promising take on data viz in Python, Altair (by Brian Granger and Jake Vanderplas). I toyed around with it and really like it, however, I havent deeply integrated it into my workflow though.Q12. How do you check whether the output your model that is being suggested is good enough as in how do you check for the effectiveness of your model?Sebastian: I keep my test dataset completely independent and try to avoid leaking information into the training/model selection loops. However, one thing I do in addition is to understand what the model is doing, i.e., by looking at feature importances. Metrics like accuracy are one thing, but making sure that the model does something reasonable (i.e., trusting the model) is important as well!For instance, Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin gave some good examples in their paper Why Should I Trust You?: Explaining the Predictions of Any Classifier. For instance, they had trained a system that could achieve a perfect score in classifying wolves vs. huskies. However, as it turned out, the model was using snow in the background of the image as a distinguishing feature.Another example was that they trained an SVM with a bag of words model to classify Christianity vs Atheism from the 20-newsgroup dataset. They found that the best performing model was picking up information in the email subject headers such as Nntp-Posting-Host, which has clearly nothing to do with either Atheism or Christianity. So, looking at performance metrics may not be enough to say that a model is effective on a test dataset, and having an understanding what the model actually does in a certain sense is important as well!Q13.Any chance you can guide us to some good matplotlibtutorials?Sebastian: Mostly, my notes are copy & pasted from the projects I am working on and are not really sharable in this state. However, I was generalizing them a bit in an attempt to make a little gallery in IPython notebooks: read here. I should add some more stuff to it some time, but Id also appreciate PRs!Q14. What would you suggest after finishing your book?Sebastian: Wow, thanks for your interest and finishing my book. I hope it was useful.
Id say the number one thing to do is to use some of the learned skills in some projects (projects at work, or hobby projects you are excited about). This way, youd get some experience and a better feeling for using these techniques, and hopefully get interesting to expand on these, i.e., by diving into some of the literature more deeplyor reading about models that I havent covered in my book.Q15.Do you think in depth knowledge about theimplementation of ML algorithms from scratch is really helpful during projects ?Sebastian: To a certain extendI think it is really beneficial to get a grasp on whats going on under the hood. You probably dont need to know the exact code implementation, but understanding how the algorithms work can be super useful. E.g., take a simple example of linear regression, knowing that theres a closed-form solution vs. learning the weight coefficients via e.g., stochastic gradient decent can make a real difference. While the former gives you exact results, it may not be feasible on huge datasets due to the expansive matrix inversion. Vice versa, you may want to be careful with iterative methods such as stochastic gradient descent and prefer more sophisticated optimization algorithms.Although linear regression is a convex optimization algorithm, choosing a suboptimal learning rate in SGD can be catastrophical, and I would also suggest scaling the features to mean=0 standard dev=1 in SGD and so forth. So I would say understanding a bit how the algorithms work and how they are roughly implemented is really useful knowledge!Q16.Is there a template that exists that can be generally applied to any ML problem as a 1st go before custom tweaks are done?Sebastian: In a classification problem, for instance, there are usually two things I would try first: A simple linear model like logistic regression (the tweak here would be adjusting the regularization strength) and random forests. Random forests are typically very robust out of the box (given a large enough number of trees), and if the random forest does not do well, there may be something about the dataset or features that I recommend to revisit before trying other algorithms.Q17.How to startwithMachine learning and what advice would you give to adata scientist aspirant?Sebastian: Thats a very common question so I hope you dont mind to provide my personal opinion on getting started resources in form of a link. Regarding the advice to beginners, I would try to stay somewhat focussed. There are a lot of intro resources out there, and most of them are good, but it can sometimes also be a distraction to try to read all/many of them. Also, I think that working on personal projects while learning is really useful to keep you interested and apply your skill in practical situation. In addition, the advantage of working on projects is that you have something to add to your portfolio or CV, and to demonstrate that you have experience in working as a data scientist. If you are interested, I also gave a ~45 min getting started with data science talk at MSU Data Science related to that topic.Q18.Is working through your awesome machine learning book enough to get started in a junior data position (provided solid math and stats skills)?Sebastian: I believe that the techniques I wrote about in my book could be a solid foundation. However, also related to my previous answer, I would recommend exercising / demonstrating your practical experience in form of projects or applications, using these techniques in projects in addition to working through a book. Based on my own experience and based on what Ive heard from friends and colleagues, having something like a blog and/or GitHub portfolio can be really beneficial for ones career, plus others can benefit from your knowledge, and you often get useful feedback that help you learn.KJ: Thank you Sebastian, for taking out the time for this AMA. I am sure our community will benefit a lot from this interaction. It was great hosting you. All the best for your future endeavours.For those of you, who want to continuously learn from top data scientists and learn by doing data science  check out our latest hackathons here.",https://www.analyticsvidhya.com/blog/2016/12/exclusive-ama-with-data-scientist-sebastian-raschka/
10 Super exciting Data Science / Machine Learning / Artificial Intelligence based startups in India,Learn everything about Analytics|Introduction|Framework to shortlist the startups|List of Companies|1.Edge Networks|2.Fluid AI|3.Flutura|4.Heckyl|5.Mad Street Den|6.Niki.ai|7.ShopR360|8.SigTuple|9.SocialCops|10.VPhrase   |End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Exclusive AMA with Data Scientist  Sebastian Raschka|Cheatsheet  Excel Functions & Keyboard Shortcuts|
NSS
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data technologies have been around for some time now. But, the increase in data generation and availability of servers on the cloud has enabled an entire generation of startups working on ideas which were unthinkable a few years back. The change in landscape is aptly summarised by the quote below:When ever I see data today, I think OPPORTUNITYAt Analytics Vidhya, we love start ups and we love data! So, mixing the two provides us with the heady mix which we thrive on. That is where this article was born. Today, we will look at 10 exciting startups in the Analytics / Data Science / Machine Learning / Artificial Intelligence based in India, which are looking to disrupt the world in coming years.This list has been curated based on certain parameters, which act as indicators for success of startups. The parameters on which startups have been evaluated are:Incorporated in 2012 by Arjun Pratap, Edge Networks dreams to change the way HR industry works right now. With an ever increasing number of job seekers, the process of finding a right match for a particular job profile today has become extremely cumbersome. With Data Science and Artificial Intelligence at its core, Edge Networks has developed their product HIREalchemy to match people with the required job. The solution provided by them facilitates talent acquisition, internal workforce optimization and talent analytics. Edge Networks was featured in Nasscoms Emerge 50 2016 list.What if I tell you, there is a company out there which is working to convert any screen into a gesture controlled AI powered assistant. Then be it in malls, banks etc. these screens will be able to address you when you approach a product kept next to it just like a human staff does? Fluid AI is one such company which is on the verge of a revolution for personalisation in Finance, Government, Web and Marketing. Founded in 2009 by two brothers Abhinav Aggarwal and Raghav Aggarwal, Fluid AI is leading the virtual customer assistance market. It aims to cater to various sectors to mimic human interaction with the customer & help reduce operational cost for a company. Fluid AI serves clients like Vodafone, Toyota, Deloitte, Emirates, NBD, Barclays, Rolls Royce, Accenture and Axis Bank. This is one companythat you must keep an eye on.Every now & then we see new analytics startups trying to generate insights from structured and unstructured data. But Flutura founded by Derick Jose, Srikanth Muralidhara and Krishnan Raman is different. Flutura believes in actions and not insights. Flutura works on M2M model via its product Cerebra where it collects data on thousands of data points of various different machines. And it then leverages these data points to convert it into actionable strategies like pre-scheduling repairs for machines, order spare parts, etc. This model increases the life of machines, saves cost on operational loss and increases efficiency. Fluturahas been recognized as one of the Top 20 Most Promising Big Data Companies globally by California-based Tech magazine, CIO Review and was also recognized by TechSparks2013 as one of the Top 3 startups out of India.Trading is an uncertain world and the best example of Butterfly Effect. Any small incident in some part of the globe can result in huge gains or losses in the trading industry. What if there was a way to keep track of all these news, people emotions, trending sentiments, etc all in a single place that can optimize your trading strategy ?Founded in 2010 by four former Merrill Lynch executives Abhijit Vedak, Jaison Mathews, Mukund Mudras, Som Sagar, Heckyl is revolutionizing the trading industry for brokerage firms, short-term traders, investors and fund managers. Heckyl does this through its integrated trading terminal which also provides visuals and heat maps of sentiments and market data to help traders find the right trading opportunities.Some of the clients are Angel Broking, Sharekhan, Motilal Oswal. Heckylis a Mumbai based startup.You plan to buy a red knee length floral dress from some online fashion websites and what you get in return is either a red dress, a red short dress and every other combination but not the one you had wanted. This is where Mad Street Dens flagship product Vue.ai comes in which relies heavily on Machine Learning and Artificial Intelligence. Vue.ai provides visual search using captured photos, targeting customers through emails and messages with their own style preferences. The e-commerce companies can customize their homepages according to the preferences of their customers along with automated tagging of products. The interesting feature of Mad Stree Dens algorithms is the use of neuromorphic principles which facilitate organic learning using less data.Mad Street Den was founded by couple Anand Chandrasekaran and Ashwini Asokan in 2013 with from their office in Chennai.Founded in 2015 by Sachin Jaiswal, Nitin Babel, Shishir Modi and Keshav PrawasiNiki.ai aims to be the one-stop solution for acustomers order.The startup leverages natural language processing and machine learning technologies to converse with customers over a simple chat interface, and places their orders with their partner businesses within seconds.The startup, in August, launched its Facebook Messenger bot. With an Android app and now Nikibot, Niki helps people in India hail a cab, order food, and pay for laundry or the electricity bill, among other things. The bot lets users pay for services directly in chat via Paytm without having to leave Facebook Messenger.The company works on a channel partnership model and generates revenue for every order processed on its platform. It is now also working with brands to provide them chatbots for their use case and application.Niki.ai has Ratan Tata as one of its backers and is a must app to try at once.Founded in 2015 by Rajul Tandon and Pranav BhruguwarShopR360 provides video analytics solutions to malls, retailers, hypermarkets and quick service restaurants. Their technology enables to distinguish staff from customers and easily integrates with existing CCTV infrastructure at no additional cost. Their solutions help to increase staff productivity, map customers journey in retail stores, measure footfall, bounce rates and dwell times. As a result, increase conversion rate and help in strategic placements of products in stores.ShopR360 uses a combination of wi-fi/bluetooth sensors, CCTV and Optical Character Recognition. ShopR360 featured in Nasscoms Emerge 50 list of 2016.In a nation where there is an enormous shortage of trained medical practitioners, SigTuple founded by ex-American Express Employees Tathagato Rai Dastidar, Rohit Kumar Pandey, Apurv Anand in 2015 took up the task to assist medical practitioners in fast diagnosis of diseases using Image processing and Classification with AI and Machine Learning at its core. SigTuple aims to build affordable medical diagnosis solutions using a microscope, a cellphone app and cloud-based engine for analysis and reporting.SigTuple is backed by Flipkart founders and Accel Partners.In their own words- Socialcops are on a mission to confront the worlds most critical problems using data. Their platform helps in driving healthcare policy, smarter cities and education outcomes.Socialcops offers three products  Collect, Search and Visualize. Collect is an android app where human sensors feed data into it on a regular basis. This is currently being employed in places like Jharkhand for purposes where no data exists. Through Search, data collected can be structured and sort through at a very fast pace. Visualise helps in making decisions from the collected data.Today, SocialCops is working on to improve the healthcare segment of India by acting as dataaggregator at grass root level.Socialcops in lieu of their contribution to make the world a better living place have been mentioned in Fortune 40 under 40 and Forbes 30 under 30.Vphrase, founded by Neerav Parekh in 2015 is a Natural Language Processing company with PHRAZOR as its product which can convert and structured content such as graphs into words as if they were written by a human analyst. VPhrase is on the verge of an innovation in terms of reports that are prepared and consumed by the employees of companies. PHRAZOR can generate automated reports, write articles, weather reports. The possibilities are endless. The thing that makes VPhrase all the more exciting is its capability to analyse graphs and create a word report in multiple languages.VPhrase is backed by seed funding platform Venture Catalysts.It has not been long for AI and Machine Learning in India. Yet, various exciting Startups have been incorporated which are pushing the boundaries of technology and human comfort meanwhile solving real world problems. Apart from the 10 above startups, there are many more startups in analytics industry which are waiting to leave a mark. If you are aware of a company which is pushing boundaries of AI, share with me in the comments below.I hope you enjoyed reading this article as I much as I did writing it. I would like to know your thoughts on the above startups, share your opinion in the comments below.",https://www.analyticsvidhya.com/blog/2016/12/10-super-exciting-data-science-machine-learning-artificial-intelligence-based-startups-in-india/
Cheatsheet  Excel Functions & Keyboard Shortcuts,Learn everything about Analytics|Introduction,"|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.package in the comments below.|Share this:|Like this:|Related Articles|10 Super exciting Data Science / Machine Learning / Artificial Intelligence based startups in India|Practical guide to implement machine learning with CARET package in R (with practice problem)|
Sunil Ray
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What is the most commonly used tool in data industry? You might have guessed it because of the title of the article  it is Excel. It is by far the most widely used tool for several reasons. Easy UI / UX for any one to start with, great features for doing simple exploratory and statistical analysis and almost universal availability make it a must have tool in your repository.However, not many people realize the power of Excel. At times, it is surprising to see people using R / Python for simple financial and business analysis, just because they are not comfortable with Excel. They fail to realize how much power Pivot tables, conditional formating and simple formulas can deliver. Check out this article for some of these tricks:simple yet powerful tricks for analyzing data.With that in mind, we thought we would share a cheat sheet for excel formulas and shortcuts. Below are the cheat sheets!Here is the cheatsheet for all shortcuts (on Windows):",https://www.analyticsvidhya.com/blog/2016/12/cheatsheet-excel-functions-keyboard-shortcuts/
Practical guide to implement machine learning with CARET package in R (with practice problem),Learn everything about Analytics|Introduction|Table of Contents|1. Getting started|2. Pre-processing using Caret|3. Splitting data using caret|4. Feature selection using Caret|5.Training models using Caret|6. Parameter tuning using Caret|7. Variable importance estimation using caret|8. Predictions using Caret|Additional Resources|End Notes,"6.1.Using tuneGrid|6.2. Using tuneLength|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.package in the comments below.|Share this:|Like this:|Related Articles|Cheatsheet  Excel Functions & Keyboard Shortcuts|Analytics Roadshow with UpGrad & IIIT-Bangalore (3 Dec 16  11 Feb 17)|
Saurav Kaushik
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the biggest challenge beginners in machine learning face is which algorithms to learn and focus on. In case of R, the problem gets accentuated by the fact that various algorithms would have different syntax, different parameters to tune anddifferent requirements on the data format. This could be too much for a beginner.So, then how do you transform from a beginner to a data scientist building hundreds of models and stacking them together?There certainly isnt any shortcut but what Ill tell you today will make you capable of applying hundreds of machine learning models without having to:All this has been made possible by the years of effort that have gone behind CARET ( Classification And Regression Training) which is possibly the biggest project in R. This package alone is all you need to know for solve almost any supervised machine learning problem. It provides a uniform interface to several machine learning algorithms and standardizes various other tasks such as Data splitting, Pre-processing, Feature selection, Variable importance estimation, etc.To get an in-depth overview of various functionalities provided by Caret, you can refer to this article.Today, well work on the Loan Prediction problem-III to show you the power of Caret package.P.S. While caret definitely simplifies the job to a degree, it can not take away the hard work and practice you need to put in to become a master at machine learning.To put in simple words, Caret is essentially a wrapper for 200+ machine learning algorithms. Additionally, it provides several features which makes it a one stop solution for all the modeling needs for supervised machine learning problems.Caret tries not to load all the packages it depends upon at the start. Instead, it loads them only when the packages are needed. But it does assume that you already have all the algorithms installed on your system.To install Caret on your system, use the following command. Heads up: It might take some time:Now, lets get started using caret package on Loan Prediction 3 problem:In this problem, we have to predict the Loan Status of a person based on his/ her profile.We need to pre-process our data before we can use it for modeling. Lets check if the data has any missing values:Next, let ususe Caret to impute these missing values using KNN algorithm. We will predict these missing values based on other attributes for that row. Also, well scale and center the numerical data by using the convenient preprocess() in Caret.It is also very easy to use one hot encoding in Caret to create dummy variables for each level of a categorical variable. But first, well convert the dependent variable to numerical.Now, creating dummy variables using one hot encoding:Here, fullrank=T will create only (n-1) columns for a categorical column with n different levels. This works well particularly for the representing categorical predictors like gender, married, etc. where we only have two levels: Male/Female, Yes/No, etc. because 0 can be used to represent one class while 1 represents the other class in same column.Well be creating a cross-validation set from the training set to evaluate our model against. It is important to rely more on the cross-validation set for the actual evaluation of your model otherwise you might end up overfitting the public leaderboard.Well use createDataPartition() to split our training data into two sets : 75% and 25%. Since, our outcome variable is categorical in nature, this function will make sure that the distribution of outcome variable classes will be similar in both the sets.Feature selection is an extremely crucial part of modeling. To understandtheimportance of feature selection and various techniques used for feature selection, I strongly recommend that you to go through my previous article. For now, well be using Recursive Feature elimination which is a wrapper method to find the best subset of features to use for modeling.This is probably the part where Caret stands out from any other available package. It provides the ability for implementing 200+ machine learning algorithms using consistent syntax. To get a list of all the algorithms that Caret supports, you can use:To get more details of any model, you can refer here.We can simply apply a large number of algorithms with similar syntax. For example, to apply, GBM, Random forest, Neural net and Logistic regression :You can proceed further tune the parameters in all these algorithms using the parameter tuning techniques.Its extremely easy to tune parameters using Caret. Typically, parameter tuning in Caret is done as below:It is possible to customize almost every step in the tuning process. The resampling technique used for evaluating the performance of the model using a set of parameters in Caret by default is bootstrap, but it provides alternatives for using k-fold, repeated k-fold as well as Leave-one-out cross validation (LOOCV) which can be specified using trainControl(). In this example, well be using 5-Fold cross-validation repeated 5 times.If the search space for parameters is not defined, Caret will use 3 random values of each tunable parameter and use the cross-validation results to find the best set of parameters for that algorithm. Otherwise, there are two more ways to tune parameters:To find the parameters of a model that can be tuned, you can useAccuracy was used to select the optimal model using the largest value.The final values used for the model were n.trees = 10, interaction.depth = 1, shrinkage =0.05 and n.minobsinnode = 3Thus, for all the parameter combinations that you listed in expand.grid(), a model will be created and tested using cross-validation. The set of parameters with the best cross-validation performance will be used to create the final model which you get at the end.Instead, of specifying the exact values for each parameter for tuning we can simply ask it to use any number of possible values for each tuning parameter through tuneLength. Lets try an example using tuneLength=10.Tuning parameter shrinkage was held constant at a value of 0.1Tuning parameter n.minobsinnode was held constant at a value of 10Accuracy was used to select the optimal model using the largest value.The final values used for the model were n.trees = 50, interaction.depth = 2, shrinkage =0.1 and n.minobsinnode = 10.plot(model_gbm)Here, it keeps the shrinkage and n.minobsinnode parameters constant while alters n.trees and interaction.depth over 10 values and uses the best combination to train the final model with.Caret also makes the variable importance estimates accessible with the use of varImp() for any model. Lets have a look at the variable importance for all the four models that we created:Clearly, the variable importance estimates of different models differs and thus might be used to get a more holistic view of importance of each predictor. Two main uses of variable importance from various models are:For predicting the dependent variable for the testing set, Caret offers predict.train(). You need to specify the model name, testing data. For classification problems, Caret also offers another feature named type which can be set to either prob or raw. For type=raw, the predictions will just be the outcome classes for the testing data while for type=prob, it will give probabilities for theoccurrence of each observation in various classes of the outcome variable.Lets take a look at the predictions from our GBM model:Caret also provides a confusionMatrix function which will give the confusion matrix along with various other metrics for your predictions. Here is the performance analysis of our GBM model:Caret is one of the most powerful and useful packages ever made in R. It alone has the capability to fulfill all the needs for predictive modeling from preprocessing to interpretation. Additionally, its syntax is also very easy to use. If you use R, Ill encourage you to use Caret.Caret is a very comprehensive package and instead of covering all the functionalities that it offers, I thought itll be a better idea to show an end-to-end implementation of Caret on a real hackathon J dataset. I have tried to cover as many functions in Caret as I could, but Caret has a lot more to offer. For going in depth, you might find the resources mentioned abovevery useful. Several of these resources have been written by Max Kuhn (the creator of caret package) himself.",https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/
Analytics Roadshow with UpGrad & IIIT-Bangalore (3 Dec 16  11 Feb 17),Learn everything about Analytics|The Events|Few words from the Experts|The action in On!,"Share this:|Like this:|Related Articles|Practical guide to implement machine learning with CARET package in R (with practice problem)|Medium.com  Top 14 handles & publications to follow for Data Science|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"At Analytics Vidhya, we love evangelizing Analytics and Data Science. One of the biggest reason behind creating Analytics Vidhya was to address the knowledge gap between Analytics practitioners and the people who needed guidance from these practitioners. We love to promoteAnalytics ecosystem in whatever way we can  blogs, hackathons, discussions, webinars and meetups are just different forms and manifestation of the same objective.I am pleased to announce that today we take one more step in this direction. We are partnering with various thought leaders in our industry. We, along with UpGrad, IIIT-Bangalore, Genpact and Gramener have decided to do an Analytics roadshow, where we will conduct several events creating these interaction points across India.As part of this roadshow, we are doing several events. Here is a brief overview of the same:P.S. I am doing a workshop tonight on Why (or why not) to become a data scientist. You can register for it hereHere are a few words form the experts on the roadshow:Im consistently asked two questions: how do you analyze data, and how do I get started? Im thrilled that with the UpGrad- IIIT Bangalore Analytics Roadshow, Gramener will help students and professionals get the insiders view of a career in data analytics.S Anand, CEO GramenerWe have engaged with more than 20,000 analytics aspirants and our biggest realization is that there is a lot of noise around analytics but no clear signal for aspiring professionals who want to take up analytics as a career. With this roadshow we aim to help professionals get a sharp understanding on the Who, What, Where, When, Why & How (5W1H) of analytics. Phalgun Kompalli, Co-Founder  UpGradAnalytics can play a similar role to what IT did for India in 2000; it can help propel the next wave of economic growth and employment generation, but we need to train people to grab this opportunity. In this pursuit, IIIT Bangalore is organizing this roadshow and also offering a PG Diploma program in Data Analytics with UpGrad to help professionals get an academic rigorous and industry relevant learning experience Prof. Sadagopan, Director IIIT BangaloreIf you have been thinking of interacting with industry experts to understand what the industry entails, how does it feel to be part of the industry, what are the challenges you face in this industry  here is your chance to grab it. We are coming to your city and providing you a platform to interact with the best. Come be a part of this roadshow.",https://www.analyticsvidhya.com/blog/2016/12/analytics-roadshow-with-upgrad-iiit-bangalore-3-dec-16-11-feb-17/
Medium.com  Top 14 handles & publications to follow for Data Science,Learn everything about Analytics|Introduction|Top data science evangelist to follow on Medium|Publications|End Notes,"1. Adam Gietgey|2. Monica Rogati|3. Sam DeBrule|4.Oliver Cameron|5.Nathan Benaich|6.Cameron Godbout|7.Carlos E.Prez|1.IBM Cognitive Business|2.Data.gov.sg Blog|3. Airbnb Data Science|4. Actionable Data|5. NYU Data Science|6. Cloudera|7. Udacity|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Analytics Roadshow with UpGrad & IIIT-Bangalore (3 Dec 16  11 Feb 17)|Webinar  Why (or why not) become a Data Scientist, Speaker Kunal Jain, 7 Dec 2016|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Medium is an awesome product!The easy interface, no distraction and high readability are some of the drivers of popularity of Medium. I can go on reading for hours on Medium.I used Medium as one of the ways to read interesting high quality posts on current topics and perspective of people. I didnt expectarticles on niche technical subjects there. I was pleasantly surprised that not only there were articles on machine learning and data science, some of them were top notch. There were articles, where people have explained machine learning in very simple and yet very powerful effective manner. There were articles on deep learning and tensorflow.Given my experience, I thought I will create and share a list of popular accounts / handles to follow on Medium.Find the curated list of data science evangelist and publications to follow on Medium for data science & machine learning.The handles have been selected on the basis of followers and regularity of articles.Followers: 10.6KI came across Adams articles when I was starting with machine learning. The simplistic approach to machine learning caught my attention. In these articles, Adam has focussed on novice and non-technical professionals looking for a simplified overview of machine learning.Machine Learning is fun is asequence of articles in which Adam introduces machine learning with simple examples.Learn about neural networks and how to predict the sequence of words in a story in the next article. The following articles will take you through Deep Learning and image recognition detailed explanation.To make these accessible by anyone the context has been kept generic and lot of technical concepts have been skipped. I would recommend these articles to anyone who is curious to know about machine learning but failed to understand it in detail because of too many technical jargons. If you are a beginner in machine learning and looking for a simple to understand & easy to digest guide on machine learning, then feel free to get started with machine learning with these articles.Followers: 6.2KMonica Rogati is the VP of Data at Jawbones. She is an expert in applied machine learning, data science and recommender system. Her passion lies in converting data into products. On Medium, she shares her perspective on the recent advancements in data science and how they will affect the startup ecosystem. The conversational tone of her blogs caught my attention.In her blogs, she shares her personal experiences from being a young college student to her expert opinion on how the next generation of AI and data products would be.Followers: 2.1KEvery week Sam shares curated articles and best reads on machine learning. You can read about the latest developments in machine learning, news, implementation of machine learning in different sectors and how is AI changing the technology landscape.Recently, his non-technical guide to Machine Learning & AI is an awesome list of curated resources.Followers: 4.2KOliver leads the self-driving car team at Udacity and heexplains extensive applicationsof Deep Learning. He sharesblogs on Deep Learning and how it is living up to the hype it has created. Learn about self-driving cars, photonic neural network, neural art, lip reading with deep learning and many more. He shares his perspective and experienceof working with deep learning in these blogs. I find them one of the most informative articles on machine learning you can read on Medium.Followers: 6KIf you want to know the latest news related to machine learning & artificial intelligence then follow Nathan for all the weekly updates. He shares all the recent trends, technology and academic contributions to machine learning & artificial intelligence.He shares the advancement of AI in healthcare, academia, self-driving cars and research. He also discussed what the future of AI would be like and the AI startups acquired by big tech giants. Follow him to remain updated on all the recent happenings in AI space.Followers: 775Cameron is a deep learning & NLP enthusiast. He writes about neural networks, implementation of deep learning for sales automation. On his blog, you can come find some good comprehensive articles like a cheatsheet of Deep Learning, beginners guide to neural networks, tensorflow in a nutshell. If you are a deep learning practitioner follow this handle for a refresher.Followers: 535Carlos studies the patterns & strategies of Deep Learning. In the past year, deep learning has gained abundant momentum and all the tech giants are talking about it. Google Deepmind remains ahead of the game. Carlos discusses the new strategies adopted by GoogleDeepmind, GoogleBrain, OpenAI, Facebook Fair, Microsoft based on their research papers.In 2011, IBM Watson left everyone stunned when it first won its first competition against Brad Ruther and Ken Jennings taking away a whopping $1 million. That was just the beginning, IBM Watson since then hasnt stopped surprising us. The IBM cognitive business focusses on AI and cognitive science. The handle is managed by the IBM Watson team and they share some insightful articles. You can read building an AI smarter city, measuring your calories intake with, how visual recognition helps prevent skin cancer, how tourism portals personalize your trip and many more interesting implementations of Watson.This is an official blog of Singapores government open data portal. The blog features open source problems like disruption in train schedule, how to segregate the non-hygienic outlets in the city, connectivity of 4G network in different parts of the city and many more interesting problems. Then they the approach taken to solve these problems. I would recommend this blog to anyone looking for real-life problems and how are they solved using data science.This is the official blog of Airbnb Data Science team. Data is one of the most important partof Airbnb and in this blog they sharethe use of machine learning at Airbnb. In these articles, they explain how do they detect host preferences, use of NPS to predict booking, data infrastructure and data exploration at Airbnb. This blog will give you insights about how data science is used at Airbnb. If you are curious to know how can you make it to Airbnb Data science team then I bet you will take away key notes from the blog.This is an official blog of Pivotal Data where the data science team shares articles in big data, data science & analytics. The articles feature some of the best real-life problems solved by Pivotal Data. The Data scientists at Pivotal shares stories of how they solved the logistics problem in Middle East, the fresh take on insurance companies being data-driven and how big data is changing the face of the companies. The blog has a lot of informative articles on big data.This is the official blog for data science home at NYU.The articlesfocus on topics like how to use big data to measure what kind of speech have more impact, how powerful social media is becoming in order to transmit news and similar topics. The blog also covers articles on future of data science and what impact it will have on the next generation. And it also has ample of resources to explore further on data science.This is an official blog by Cloudera and features articles on machine learning, neural networks, bigdata and analytics. These are some of the must reads like how hadoop is acting as a game changer in financial services, introduction to Clouderas datalennials, deriving value from IoT, howto handle big data. The articles have been authored by data science practitioners at Cloudera.This is an official blog of Udacity core team. Every week they curate best machine learning & virtual reality articles. The articles will interest anyone & everyone interested in machine learning. Machine learning is evolving and every industry is using machine learning today. In this blog, you can find articles on machine learning in healthcare, music, art, media, technology, food & beverage, sports, entertainment, history, games, and many more. Its a good refresher on how machine learning is touching our lives everywhere.I hope you enjoyed reading this article. If you are a frequent reader on Medium then Im sure you will find these handles helpful.If you follow any handle on Medium which I have missed out and you think it deserves a mention. Then feel free to share it in the comments below. Have any doubts or confusions let me know. Also, I would love to hear your feedback on this.",https://www.analyticsvidhya.com/blog/2016/12/medium-com-top-14-handles-publications-to-follow-for-data-science/
"Webinar  Why (or why not) become a Data Scientist, Speaker Kunal Jain, 7 Dec 2016",Learn everything about Analytics|Introduction|Speaker Biography|What is the Webinar about?|What will you learn?|Why should you attend it?,"Share this:|Like this:|Related Articles|Medium.com  Top 14 handles & publications to follow for Data Science|45 questions to test Data Scientists on Tree Based Algorithms (Decision tree, Random Forests, XGBoost)|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are coming up with a Webinar in association with UpGrad & IIIT-B as part of their Analytics Roadshow.The webinar is aimed towards beginners and data science enthusiasts out there, thinking whether a career as a data scientist is good for them or not. You can expect an overview of life as a data scientist through this webinar. Attend the webinar to know more about it.Topic: Why (or Why not) to become a Data ScientistDate: 7 Dec 2016Time: 8:30Pm to 9Pm (IST) RegisterHere
Kunal is Founder & CEO of Analytics Vidhya. Kunal is an engineer be education, a data scientist by heart and a data science evangelist by profession. He has more than 10 years of experience across various geographies and currently leads Analytics Vidhya full time. Analytics Vidya is Indias largest analytics and data science community.Webinar is about the life of a data scientists. Data Scientists has been claimed as the sexiest job of the21st century by Harvard Business Review.Find out what are the skills required to become a data scientist. It will also cover the tools & languagesone must know for the successful career in data science.This is your chance to learn & ask questions from Kunal directly. Kunal has over 10 years of experience in analytics & data science. By attending this webinar find out if you are cut out for this role.",https://www.analyticsvidhya.com/blog/2016/12/webinar-why-or-why-not-become-a-data-scientist-speaker-kunal-jain-7-dec-2016/
"45 questions to test Data Scientists on Tree Based Algorithms (Decision tree, Random Forests, XGBoost)",Learn everything about Analytics|Introduction|Overall Results|Helpful Resources on Tree Based Algorithms|Questions and Solutions|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Webinar  Why (or why not) become a Data Scientist, Speaker Kunal Jain, 7 Dec 2016|Deep Learning Session with Microsoft at SP Jain School of High Technology, Mumbai, 5 Dec 2016|
Faizan Shaikh
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tree Based algorithms like Random Forest, Decision Tree, and Gradient Boosting are commonly used machine learning algorithms. Tree based algorithms are often used to solve data science problems. Every data science aspirant must be skilled in tree based algorithms.We conducted this skill test to help you analyze your knowledge in these algorithms.A total of 1016 participants registered for this skill test. The test was designed to test the conceptual knowledge of tree based algorithms. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how you could have answered correctly.Here are the leaderboard ranking for all the participants.Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 400 people participated in the skill test and the highest score obtained was 36 . Here are a few statistics about the distribution.Mean Score : 16.98Median Score : 19Mode Score : 19You can see that got a bi-modal distribution of scores. We were not expecting that as the first 8 questions were relatively easy and could be solved grounds up without too much knowledge about decision trees.If you did well, here is another test coming up  Skill test  Regression , which would test you on knowledge of solving regression problems.Here are a few resources you can refer to to improve your knowledge on tree based algorithms.A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)Introduction to Random forest  SimplifiedComplete Guide to Parameter Tuning in Gradient Boosting (GBM) in PythonQ 1) The data scientists at BigMart Inc have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product based on these attributes and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store during a defined period.Which learning problem does this belong to?Solution: ASupervised learning is the machine learning task of inferring a function from labeled training data. Here historical sales data is our training data and it contains the labels / outcomes.Q2) Before building our model, we first look at our data and make predictions manually. Suppose we have only one feature as an independent variable (Outlet_Location_Type) along with a continuous dependent variable (Item_Outlet_Sales).
We see that we can possiblydifferentiate in Sales based on location (tier 1 or tier 3). We can write simple if-else statements to make predictions.Which of the following models could be used to generate predictions (may not be most accurate)?Solution: DAll the options would be correct. All the above models give a prediction as output and here we are not talking about most or least accurate.Q3) The below created if-else statement is called a decision stump:Our model: if Outlet_Location is Tier 1: then Outlet_Sales is 2000, else Outlet_Sales is 1000Now let us evaluate the model we created above on following data:Evaluation Data:We will calculate RMSEto evaluate this model.The root-mean-square error (RMSE) is a measure of the differences between values predicted by a model or an estimator and the values actually observed.The formula is :What would be the RMSEvalue for this model?Solution: BSo by calculating RMSE value using the formula above, we get ~824 as our answer.Q4) For the same data, let us evaluate our models. The root-mean-square error (RMSE) is a measure of the differences between values predicted by a model or an estimator and the values actually observed.The formula is :Which of the following will be the best model with respect to RMSE scoring?Solution: ACalculate the RMSE value for each if-else model:We see that the model in option A has the lowest value and lower the RMSE, better the model.Q5) Now lets take multiple features into account.If have multiple if-else ladders, which model is best with respect to RMSE?Solution: DWe see that option D has the lowest valueQ6) Till now, we have just created predictions using someintuition based rules. Hence our predictions may not be optimal.What could be done to optimize the approach of finding better predictions from the given data?Solution: CWe will take that value which is more representative of the data. Given all three options, central tendency, mean value would be a better fit for the data.Q7) We could improve our model byselecting the feature which gives a better predictionwhen we use it for splitting (It is a process of dividing a node into two or more sub-nodes).In this example, we want to find which feature would be better for splitting root node (entire population or sample and this further gets divided into two or more homogeneous sets).Assume splitting method is Reduction in Variance i.e. wesplit using a variable, which results in overall lower variance.What is the resulting variance if we split using Outlet_Location_Type?Solution: AOption A is correct. The steps to solve this problem are:P.S. You will need to take weigthed mean.Q8) Next, we want to find which feature would be better for splitting root node (where root node represents entire population). For this, we will set Reduction in Variance as our splitting method.The split with lower variance is selected as the criteria to split the population.Among Between Outlet_Location_Type and Item_Fat_Content, which was a better feature to split?Solution: AOption A is correct because Outlet_Location_Type has more reduction in variance. You can perform calculation similar to last question.Q9) Look at the below image: The red dots represent original data input, while the green line is the resultant model.How do you propose to make this model better while working with decision tree?Solution: CA. As we can see in the image, our model is not general enough, it takes outliers/ noise into account when calculating predictions which makes it overfit the data.B. If we can set the number of nodes, we could easily get an optimal tree. But to select this value optimally beforehand is very hard, as it requires extensive cross-validation to be generalizable.C. Tuning Tree parameters is the best method to ensure generalizabilityQ10) Which methodology does Decision Tree (ID3) take to decide on first split?Solution: AThe process of top-down induction of decision trees (TDIDT) is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data. Readhere.Q11) There are 24 predictors in a dataset. You build 2 models on the dataset:1. Bagged decision trees and
2. Random forestLet the number of predictors used at a single split in bagged decision tree is A and Random Forest is B.Which of the following statement is correct?Solution: A Random Forest uses a subset of predictors for model building, whereas bagged trees use all the features at once.Q12) Why dowe prefer information gain over accuracy when splitting?Solution: DAll the above options are correctQ13) Random forests (While solving a regression problem) have thehigher variance of predicted result in comparison to Boosted Trees (Assumption: both Random Forest and Boosted Tree arefully optimized). Solution: C
It completely depends on the data, the assumption cannot be made without data.Q14) Assume everything else remains same, which of the following is the right statement about the predictions from decision tree in comparison with predictions from Random Forest?Solution: DThe predicted values in Decision Trees have low Bias but high Variance when compared to Random Forests. This is because random forest attempts to reduce variance by bootstrap aggregation. Refer topic 15.4 of Elements of Statistical LearningQ15) Which of the following tree based algorithm uses some parallel (full or partial) implementation?Solution: D
Only Random Forest and XGBoost have parallel implementations.Random Forest is very easy to parallelize, where as XGBoost can have partially parallel implementation. In Random Forest, all trees grows parallel and finally ensemble the output of each tree .Xgboost doesnt run multiple trees in parallel like Random Forest, you need predictions after each tree to update gradients. Rather it does the parallelization WITHIN a single tree to create branches independently.Q16) Which of the following could not beresultof two-dimensional feature space from natural recursive binary split?Solution: A1 is not possible. Therefore, Option A is correct. For more details, refer to Page 308 from ELSI (Elements of Statistical Learning).Q17) Which of the following is not possible in a boosting algorithm?Solution: ABoosted algorithms minimize error in previously predicted values by last estimator. So it always decreases training error.Q18) Which of the following is a decision boundary of Decision Tree?
Solution: CDecision Boundaries of decision trees are always perpendicular to X and Y axis.Q19) Lets say we have m numbers of estimators (trees) in a boosted tree.Now, how many intermediate trees will work on modified version (OR weighted) of data set?Solution: BThe first tree in boosted trees works on the original data, whereas all the rest work on modified version of the data.Q20) Boosted decision trees perform better than Logistic Regression on anomaly detection problems (Imbalanced Class problems).Solution: AOption A is correctQ21) Provided n < N and m < M. A Bagged Decision Treewith a dataset of N rows and M columns uses____rows and ____ columns for training an individual intermediate tree.Solution: CBagged trees uses all the columns for only a sample of the rows. So randomization is done on the number of observations not on number of columns.Q22) Given 1000 observations, Minimum observation required to split a node equals to 200 and minimum leaf size equals to 300 then what could be the maximum depth of a decision tree?Solution: BThe leaf nodes will be as follows for minimum observation to split is 200 and minimum leaf size is 300:So only after 2 split, the tree is created. Therefore depth is 2.Q23) Consider a classification tree for whether a person watches Game of Thrones based on features like age, gender, qualification and salary. Is it possible to have following leaf node?Solution: AA node can be split on a feature, as long as it gives information after split. So even though the above split does not reduce the classification error, it improves the Gini index and the cross-entropy. Refer Pg. 314 of ISLR.Q24) Generally, in terms of prediction performance which of the following arrangements are correct:Solution: CGenerally speaking, Boosting algorithms willperform better than bagging algorithms. In terms of bagging vs random forest, random forest works better in practice because random forest has less correlated trees compared to bagging. And its always true that ensembles of algorithms are better than single modelsQ25) In which of the following application(s), a tree based algorithm can be applied successfully?Solution: EOption E is correct as we can apply tree based algorithm in all the 3 scenarios.Q26) When using Random Forest for feature selection, suppose you permute values of two features  A and B. Permutation is such that you change the indices of individual values so that they do not remain associated with the same target as before.For example:You notice that permuting values does not affect the score of model built on A, whereas the score decreases on themodel trained on B.Which of the following features would you select from the following solely based on the above finding?Solution: BThis is called mean decrease in accuracy when using random forest for feature selection. Intuitively, if shuffling the values is not impacting the predictions, the feature is unlikely to add value.Q27) Boosting is said to be a good classifier because:Solution: BA. Trees are sequential in boosting. They are not parallelB. Boosting attempts to minimize residual error which reduces margin distributionC. As we saw in B, margins are minimized and not maximized.Therefore B is trueQ28)Which splitting algorithm is better with categorical variable having high cardinality?Solution: BWhen high cardinality problems, gain ratio is preferred over any other splitting technique. Refer slide number 72 of this presentation.Q29) There are A features in a dataset and a Random Forest model is built over it. It is given that there exists only one significant featureof the outcome  Feature1. What would be the %of total splits that will not consider the Feature1 as one of the features involved in that split (It is given that m is the number of maximum features for random forest)?Note: Considering random forest select features space for every node split.Solution: AOption A is correct. This can be considered as permutation of not selecting a predictor from all the possible predictorsQ30) Suppose we have missing values in our data.Which of the following method(s) can help usto deal with missing values while building a decision tree?Solution: DAll the options are correct. Refer this article.Q31) To reduce under fitting of a Random Forest model, which of the following method can be used?Solution: BOnly option B is correct, becauseA: increasing the number of samples for a leaf will reduce the depth of a tree, indirectly increasing underfittingB: Increasing depth will definitely decrease help reduce underfittingC: increasing the number of samples considered to split will have no effect, as the same information will be given to the model.Therefore B is True.Q32)While creating a Decision Tree, can we reuse a feature to split a node?Solution: AYes, decision tree recursively uses all the features at each node.Q33) Which of the following is a mandatory data pre-processing step(s) for XGBOOST?Solution: DXGBoost is doesnt require most of the pre-processing steps, so only converting data to numeric is required among of the above listed stepsQ34) Decision Trees are not affected by multicollinearity in features:Solution: AThe statement is true. For example, if there are two 90% correlated features, decision tree would consider only one of them for splitting.Q35) For parameter tuning in a boosting algorithm, which of the following search strategies may give best tuned model:Solution: CFor a a given search space,Both random search or grid search may give best tuned model. It depends on how much time and resources can be allocated for search.Q36) Imagine a two variable predictor space having 10 data points. Adecision tree is built over it with 5 leaf nodes.The number of distinct regions that will be formed in predictors space?Solution: DThe predictor space will be divided into 5 regions. Therefore, option D is correct.Q37) In Random Forest, which of the following is randomly selected?Solution: DOption A is False because, number of trees has to decided when building a tree. It is not random.Options B and C are trueQ38) Which of the following are thedisadvantage of Decision Tree algorithm?Solution: DOption A is False, as decision tree are very easy to interpretOption B is True, as decision tree are high unstable modelsOption C is True, as decision tree also tries to memorize noise.So option D is True.Q39) While tuning the parameters Number of estimators and Shrinkage Parameter/Learning Rate for boosting algorithm.Which of the following relationship should be kept in mind?Solution: BIt is generally seen that smaller learning rates require more trees to be added to the model and vice versa. So when tuning parameters of boosting algorithm, there is a trade-off between learning rate and number of estimatorsQ40) Lets say we have m number of estimators (trees) in a XGBOOST model.Now, how many trees will work on bootstrapped data set?Solution: CAll the trees in XGBoost will work on bootstrapped data. Therefore, option C is trueQ41) Which of the following statement is correct about XGBOOST parameters:Solution: D1 and 4 are wrong statements, whereas 2 and 3 are correct. Therefore D is true. Refer this article.Q42) What can be the maximum depth of decision tree (where k is the number of features and N is the number of samples)? Our constraint is that we are considering a binary decision tree with no duplicate rows in sample (Splitting criterion is not fixed).Solution: CThe answer is N-1. An example of max depth would be when splitting only happens on the left node.Q43) Boosting is a general approach that can be applied to many statistical learning methods for regression or classification.Solution: ABoosting is an ensemble technique and can be applied to various base algorithmsQ44) Predictions of individual trees of bagged decision trees have lower correlation in comparison to individual trees of random forest.Solution: BThis is False because random Forest has more randomly generated uncorrelated trees than bagged decision trees. Random Forest considers only a subset of total features. So individual trees that are generated by random forest may have different feature subsets. This is not true for bagged trees.Q45) Below is a list of parameters of Decision Tree. In which of the following cases higher is better?Solution: DFor all three options A, B and C, it is not necessary that if you increase the value of parameter the performance may increase. For example, if we have a very high value of depth of tree, the resulting tree may overfit the data, and would not generalize well. On the other hand, if we have a very low value, the tree may underfit the data. So, we cant say for sure that higher is better.I hope you enjoyed taking the test and you found the solutions helpful. The test focused onconceptual knowledge of tree based algorithms.We tried to clear all your doubts through this article but if we have missed out on something then let me know in comments below. If you have any suggestions or improvements you think we should make in the next skilltest, let us know in the comments below.Dont forget to register for Skilltest Regression coming up on 17 Dec16. You will be tested on regression and its various forms. All the Best!",https://www.analyticsvidhya.com/blog/2016/12/detailed-solutions-for-skilltest-tree-based-algorithms/
"Deep Learning Session with Microsoft at SP Jain School of High Technology, Mumbai, 5 Dec 2016",Learn everything about Analytics|Introduction|What is CNTK?|Whatis the session about?|What will you learn?|Why should you attend this event?,"Share this:|Like this:|Related Articles|45 questions to test Data Scientists on Tree Based Algorithms (Decision tree, Random Forests, XGBoost)|21 Reason why you should NOT become a Data Scientist|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"SP Jain School of High Technology is organizing a power-packed session on Deep Learningwith Microsofts Open Source Cognitive Toolkit (CNTK). The session will be conducted personally by none other than Mr. Sayan Pathak, the Global Principal Program Manager Lead at Microsoft Corporation based out of their HQ in Redmond.Venue: SP Jain Global School of Management,1st Floor,Kohinoor City Mall, Premier Rd,Kurla, MumbaiDate  5 Dec 2016Time  10:30 amLocation  Mumbai RegisterHere
Cognitive Toolkit is deep learning toolkit by Microsoft. It supports feed-forward, convolutional, and recurrent network for speech, image and text workloads also in combination. Popular network types are supported either natively or can be described asa CNTK configuration. It can also scale to multiple servers.The session will explore Microsofts open source computation-graph based on deep learning toolkit CNTK. It is used to train and evaluate deep neural networks found inMicrosoft products like Cortana.Learn how CNTK is used to observe deep learning needs, what they can and cannot do. Understand the workflow of the CNTK and howto implement different algorithms using CNTK. To provide you hands-on experience the session will be followed by a tutorial.",https://www.analyticsvidhya.com/blog/2016/12/deep-learning-session-with-microsoft-at-sp-jain-global-school-of-management-mumbai-5-dec-2016/
21 Reason why you should NOT become a Data Scientist,Learn everything about Analytics|Introduction|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Deep Learning Session with Microsoft at SP Jain School of High Technology, Mumbai, 5 Dec 2016|Introduction to Feature Selection methods with an example (or how to select the right variables?)|
Saurav Kaushik
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time for some Friday Fun!In last few years, the growth of Data Scientists has been following the growth in data . You can see it all around  people attending webinars, info sessions, undergoing multiple certifications and what not!While everyone is going North, I thought Friday isa good time to take a moment and head South!Source: PinterestHere is my list of 21 reasons why not to become a Data Scientist. If you have a few others to add, please do so!1. Trump: When every data scientist in the world got it wrong!Source: UproaxxIf no one could predict such a macro phenomena, how would they know who is going to buy what?2. At times youll get it horribly wrong. Did I just say wrong? Epic FAILURES!Remember the Twitter bot from Microsoft? She started saying great things about Hitler with 1 day of training on live data.3. Ever participated in Kaggle? This is how I feel when you have to spend days getting that small increase in the 5thdecimal place.Source: megaaa.kyu.mobiNot to mention the days when I dont get even a slight increase in my scores irrespective of what I try.4. And in case, I got awesome results from a black box model  this is how it feels!I have no clue what happened.5. Torture the data and itll confess what you want it to.Source: PintrestAt times, you can use data to create any point you want!6. Why study so manyalgorithms when XGBoost always does the trick for you!Source: Giphy7. Automation! My job is to make machines replace me.Who on earth in sane mind wants that?8. I must learn the languages that are going to pass out in 105 years anyway.Or at times the ones, which many people have not even heard of!9. Looks like Im the only one who calculates the p-value of getting an increment everyday.10. I get bashed by the CEO daily while everybody stands and watches.Source: KnowyourmemeWhat people dont realise is that Im the only person whom they take advice from!11.Nothing is impossible until you start to explain Data Science at a social gathering.Source: Giphy12.Human thinking ends at 3-D. My work starts at 100-D.Source: Giphy13. Carrying cool laptops is a dream. Carrying servers is a necessity.Source: ASUS14.  Only god knows the future. Whom am I to predict.15. Astrologers have been doing it for years.Source: Imgflip16. Why spoil weekends over a hackathon/competition? Coldplay?17. I am expected to teach domain expertise to domain experts.Source: France Etiquette Limited18. Any my expertise depends on where Im giving the interview.Source: Pinterest19.Dont doubt me. Alternative hypothesis stands true!Source: Pinterest20. No one knows who a Data Scientist is?But everyone is searching for one!21. The world is a strange place. And believe me, its not at all like what you think.Disclaimer: This is just a fun take on life as a Data Scientist. We dont intend to hurt any sentiments or endorse any products / services mentioned above. We truly believe, data science is here to stay, else we would not have bet our careers on it I hope you enjoyed reading this article as much as I did while writing it. If you are a data scientist, you will definitely relate to some of the points above. Which one did you relate to the most? Also, any point you think we missed in this list?Share them with us in the comments below.",https://www.analyticsvidhya.com/blog/2016/12/21-reason-why-you-should-not-become-a-data-scientist/
Introduction to Feature Selection methods with an example (or how to select the right variables?),Learn everything about Analytics|Introduction|Table of Contents|1. Importance of Feature Selection in Machine Learning|2. Filter Methods|3. Wrapper Methods|4. Embedded Methods||5. Difference between Filter and Wrapper methods|6. Walkthrough example|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|21 Reason why you should NOT become a Data Scientist|Building a machine learning / deep learning workstation for under $5000|
Saurav Kaushik
|36 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the best ways I use to learn machine learning is by benchmarking myself against the best data scientists in competitions. It gives you a lot of insight into how you perform against the best on a level playing field.Initially, I used to believe that machine learning is going to be all about algorithms  know which one to apply when and you will come on the top. When I got there, I realized that was not the case  the winners were using the same algorithms which a lot of other people were using.Next, I thought surely these people would have better / superior machines. I discovered that is not the case. I saw competitions being won using a MacBook Air, which is not the best computational machine. Over time, I realized that there are 2 things which distinguish winners from others in most of the cases: Feature Creation and Feature Selection.In other words, it boils down to creating variables which capture hidden business insights and then making the right choices about which variable to choose for your predictive models! Sadly or thankfully, both these skills require a ton of practice. There is also some art involved in creating new features  some people have a knack of finding trends where other people struggle.In this article, I will focus on one of the 2 critical parts of getting your models right  feature selection. I will discuss in detail why feature selection plays such a vital role in creating an effective predictive model.Read on!Machine learning works on a simple rule  if you put garbage in, you will only get garbage to come out. By garbage here, I mean noise in data.This becomes even more important when the number of features are very large. You need not use every feature at your disposalfor creatingan algorithm. You can assist your algorithm by feeding in only those features that are really important. I have myself witnessed feature subsets giving better results than complete set of feature for the same algorithm. Or as Rohan Rao puts it  Sometimes, less is better!Not only in the competitions but this can be very useful in industrial applicationsas well. You not only reduce the training time and the evaluation time, you also have less things to worry about!Top reasons to use feature selection are:Next, well discuss various methodologies and techniques that you can use to subset your feature space and help your models perform better and efficiently. So, lets get started.Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. For basic guidance, you can refer to the following table for defining correlation co-efficients.One thing that should be kept in mind is that filter methods do not remove multicollinearity. So, you must deal with multicollinearity of features as well before training models for your data.In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.One of the best ways for implementing feature selection with wrapper methods is to use Boruta package that finds the importance of a feature by creating shadow features.It works in the following steps:For more information on theimplementation of Boruta package, you can refer to this article :For theimplementation of Boruta in python, refer can refer to this article.Embedded methods combine the qualities of filter and wrapper methods. Its implemented by algorithms that have their own built-in feature selection methods.Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.For more details and implementation of LASSO and RIDGE regression, you can refer to this article.Other examples of embedded methods are Regularized trees, Memetic algorithm, Random multinomial logit.The main differences between the filter and wrapper methods for feature selection are:Lets use wrapper methods for feature selection and see whether we can improve the accuracy of our model by using an intelligently selected subset of features instead of using every feature at our disposal.Well be using stock prediction data in which well predict whether the stock will go up or down based on 100 predictors in R. This dataset contains 100 independent variables from X1 to X100 representing profile of a stock and one outcome variable Y with two levels : 1 for rise in stock price and -1 for drop in stock price.To download the dataset, click here.Lets start with applying random forest for all the features on the dataset first.library('Metrics')library('randomForest')library('ggplot2')library('ggthemes')library('dplyr')#set random seedset.seed(101)#loading datasetdata<-read.csv(""train.csv"",stringsAsFactors= T)#checking dimensions of datadim(data)## [1] 3000 101#specifying outcome variable as factordata$Y<-as.factor(data$Y)data$Time<-NULL#dividing the dataset into train and testtrain<-data[1:2000,]test<-data[2001:3000,]#applying Random Forestmodel_rf<-randomForest(Y ~ ., data = train)preds<-predict(model_rf,test[,-101])table(preds)##preds## -1 1##453 547#checking accuracyauc(preds,test$Y)##[1] 0.4522703Now, instead of trying a large number of possible subsets through say forward selection or backward elimination, well keep it simple by using the top 20 features only to build a Random forest. Lets find out if it can improve the accuracy of our model.Lets look at the feature importance:importance(model_rf)#MeanDecreaseGini##x1 8.815363##x2 10.920485##x3 9.607715##x4 10.308006##x5 9.645401##x6 11.409772##x7 10.896794##x8 9.694667##x9 9.636996##x10 8.609218##x87 8.730480##x88 9.734735##x89 10.884997##x90 10.684744##x91 9.496665##x92 9.978600##x93 10.479482##x94 9.922332##x95 8.640581##x96 9.368352##x97 7.014134##x98 10.640761##x99 8.837624##x100 9.914497Applying Random forest for most important 20 features onlymodel_rf<-randomForest(Y ~ X55+X11+X15+X64+X30 +X37+X58+X2+X7+X89 +X31+X66+X40+X12+X90 +X29+X98+X24+X75+X56, data = train)preds<-predict(model_rf,test[,-101])table(preds)##preds##-1 1##218 782#checking accuracyauc(preds,test$Y)##[1] 0.4767592So, by just using 20 most important features, we have improved the accuracy from 0.452 to 0.476. This is just an example of how feature selection makes a difference. Not only we have improved the accuracy but by using just 20 predictors instead of 100, we have also:I believe that his article has given you a good idea of how you can perform feature selection to get the best out of your models. These are the broad categories that are commonly used for feature selection. I believe you will be convinced about the potential uplift in your model that you can unlock using feature selection and added benefits of feature selection.Did you enjoy reading this article? Do share your views in the comment section below.",https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/
Building a machine learning / deep learning workstation for under $5000,Learn everything about Analytics|Introduction|Configuration of the Workstation:|Pros and Cons of building a Workstation?|A few high level decisions:|Choosing the hardware:|Assembling the server|Software Installation:|Setting up Deep Learning tools:|End Notes,"Share this:|Like this:|Related Articles|Introduction to Feature Selection methods with an example (or how to select the right variables?)|In talk with Manvender Singh, CEO  UpX Academy  Taking Data Science certification to new heights|
Kunal Jain
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Building a machine learning / deep learning workstation can be difficult and intimidating. There are so many choices out there. Would you go for NVidia developer box and spend $15,000? or could you build something better in a more cost-effective manner. Which hardware is right for your requirements? How much RAM do you need? The questions are endless and there might be no right or wrong answers.We just finished building our first workstation at Analytics Vidhya. It was a special moment for all of us at Analytics Vidhya. Not because the server is a monster (which it is) and we can now let our fantasies run on a much larger machine. But, because we were all proud of what we have built together. Building this server was symbolic of something much bigger. It meant that as an organization, we are coming out of our monthly cash flow challenges and starting to build for an exciting future.While building this work station, we faced several trade-offs. We made several choices people come across while creating a workstation for data science. In this article, I will highlight some of the choices we made while building this server, the steps in getting it up and running and the thrill of running first large dataset on it. If you are only interested in knowing the configuration, you can read it below. If you are interested in understanding the trade-offs and the reasons for the hardware / software we chose, you can continue to read the rest of the article.I am sure some of you would be asking, why build a monster workstation in todays world? Why not rent a machine on the cloud?The true answer is it depends on your requirements. In our case, this was the best way to enable the team to do machine learning at scale. Just to give some context, Analytics Vidhya currently has 5 people today, who might work on large datasets or huge computations. These datasets could come from competitions, real world problems from clients looking to crowd source solutions from our community or running solutions of our winners to verify them.Here are the reasons, why building a workstation worked best for us:Obviously nothing comes free in life. So, while we had these advantages, there were a few down sides as well. Here are the compromises we had to make:Given the pros were working out better for us and I think we would still be able to use the server a few years down the line, we went ahead with the decision to build a workstation.Once we decided that we would build a workstation, there were a few more decisions we had to take. Here were the decisions we needed to take:Once these high level decisions were taken, we started the search on finding hardware components of the server.Smart Trooper CabinetGiven the spread of Hardware, length of GPU and the cost constraints  we decided to go with Storm Trooper Cabinet.With all the hardware ready, it was time to assemble the server. If you have not done this before, it might look a bit intimidating from outside, but it is not. The components are meant to fit in only in the slots they are supposed to fit in. So, here was the monster ready to breatheWith the hardware ready, we were excited to the core. The next thing was installing the right software. Before doing so, make sure your server is connected via a LAN cable and is attached to power. It is advisable to add a wireless keyboard so that you can start installation of desired software. Once you are ready, it is time to awaken the monster  BOOM!Time to feed the machine with the right software:You can add other packages you may need depending on your need, but this should give you a base setup ready. Time to add Deep Learning tools.Deep learning is an active field of research. With a widevariety of available libraries to choose from and their individual dependencies, it is very hard to maintain a continuously working DL system. Hence, it is usually a good practice to set up your own virtual environment. This is what we did.The easiest solution wasto use containers, which are essentially boxes of self-contained softwares. Its like virtualization of OS, but along with the ease of access from local machine.The first step we did was to setup nvidia driversThen we installed docker (https://docs.docker.com/engine/installation/linux/ubuntulinux/).Integrating GPU on docker has some additional steps:Now comes the main part, installing DL libraries on docker. We built the docker from scratch, following this excellent guide (https://github.com/saiprashanths/dl-docker/blob/master/README.md)Thats it! We have completed the installation steps. There were a few more things to, i.e. handle data persistency. DL requires huge amount of data to be processed and the best wayto share this data to docker system is add as a data volume (https://docs.docker.com/engine/tutorials/dockervolumes/).And Voila! We have a complete running Deep Learning system!We shared ourdecisions and choice of hardware / software as there are so many options available today. It is difficult to find the right option for you among all the options. If you are looking to build your own workstation, this guide should be immensely helpful to you. If you are not looking to build one, there is still a lot to learn from the process. I myself learnt a lot of finer aspects about hardware selection while going through this journey.If you had to build one workstation under $4,500  how would you go about doing so? Would you do something different? If so, why? Looking forward to hear from you.",https://www.analyticsvidhya.com/blog/2016/11/building-a-machine-learning-deep-learning-workstation-for-under-5000/
"In talk with Manvender Singh, CEO  UpX Academy  Taking Data Science certification to new heights",Learn everything about Analytics|Introduction,"Share this:|Like this:|Related Articles|Building a machine learning / deep learning workstation for under $5000|Mystory: I became a Data Scientist after 8 years working as a Software Test Engineer|
Kunal Jain
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have been following the startup ecosystem in Data Science education sector very closely for some time now. Recently,I came across UpX Academy, an education venture focussed on offering data science knowledge to professionals & students.UpX Academy is a Tech Mahindra venture that provides online instructor-led courses in Big Data & Data Science.In conversation with Manvender SinghI reached out to UpX academy and got a chance to interview their CEO  Manvender Singh.Manvender is the Founder & CEO of UpX Academy. Manvender is an MBA from ISB Hyderabad and before starting UpX Academy, he was the founder of Aristotle Prep, a popular GMAT test prep company.Here are excerpts of my conversation with Manvender Singh.KJ: Thanks Manvender for taking out time for an exclusive interview with Analytics Vidhya. Tell us the idea behind UpX Academy?Manvender:Its a pleasure talking to the audience at Analytics Vidhya!It was back at ISB in my Business Analytics class where I realized the importance of Big Data Analytics, Machine Learning, and Artificial Intelligence. I felt that these are the areas where the future of technology truly is! It was the same time when Apples Siri & Google Now had started to gain traction, increasing my fascination for these technologies even more!Although a few companies were offering courses in these domains, but on having a look at the overall curriculum and pedagogy, I felt that this was not how a student would learn the most.So, once I joined Tech Mahindra as an Entrepreneur-in-Residence, I started working on setting up UpX Academy. Our tagline is Move up in life. Through our courses, we essentially help professionals move up in their careers by equipping them with in-demand skills that will keep them ahead of the curve for the next 10 years.KJ: Could you please elaborate on the gap between student expectations and the current training landscape that the market provides?
Manvender:Recently, there has been a lot of buzz around Big Data Analytics, Data Science, Machine Learning, etc. A lot of people want to enter these fields but most of the trainings offered tend to be primarily theoretical and do not teach students how to practically apply these concepts to real-world scenarios. However, the industry is not interested whether you know Hadoop as a concept or you know the various Machine Learning techniques. What the industry is really interested in, is knowing how effectively you can apply these concepts to real-world projects. So while designing the curriculum at UpX Academy, we ensured that theory was well integrated with a lot of projects, case studies and hands-on exercises.KJ: Who are the target audience for the courses from UpX Academy?Manvender:Firstly, Id like to break the myth that to learn Big Data & Data Science, one needs to have a strong coding knowledge. If you check the student profile page on our website, you will find that our students have a diverse range of work experience. We have software developers to team leaders to senior project managers enrolled in our courses. In fact, in our current Data Science batch, quite a few students are MBAs with little technical background.For those who are from technology background and want to remain in technology, we recommend them to go for Big Data courses and for those who want to solve business problems through data, we recommend them to go for Data Science courses.We only focus on providing courses on Big Data & Data Science and thats what makes us a specialist in these areas. Unlike other institutes, we run only one batch at a time. So all our resources and time are focused on a single batch which results in a high completion rate and a great learning experience!KJ: So, which courses are you currently offering and which ones can we expect to be introduced in the near future?Manvender:Like I said earlier, we are specialists in Big Data & Data Science. So, in Big Data, we have three tracks. The first is the Foundation track that covers Hadoop, second is the Specialization track that covers Hadoop & Spark and the third one is the Super Specialization track that covers Hadoop, Spark, NoSQL databases like MongoDB and HBase.Similarly, in Data Science, we have three tracks. The Foundation one covers Data Analytics with Python & R. In Data Science Specialization, we add Machine Learning to the foundation track & in Super Specialization, we cover Data analytics with Python and R, Machine Learning and Tableau. In fact, we are one of the very few companies in India which offers online instructor-led courses in Machine Learning.Being backed by Tech Mahindra, which is one of the biggest IT companies in India, our courses are highly industry-relevant. With every batch, we update our curriculum which further ensures that we teach our students what the industry demands.Very soon, we are planning to launch industry-specific courses in Retail, Finance & HR Analytics.In our programs, we have brought together three critical components that I believe make for a great learning experience.KJ: Can you describe these three critical components which make UpX truly unique?Manvender:The first component is the instructor. We are very selective in the faculty we choose. Our instructors belong to top institutes such as London Business School, Carnegie Mellon, Indian School of Business, etc. and have worked for top organizations such as Microsoft, Google, etc.Secondly, while most other programs teach a lot of theory, we have a very high hands-on component in our classes. As I mentioned earlier too, the focus of our courses is on helping our students apply the concepts to real-world scenarios. For example, if they are being taught Linear Regression in a class, then students have access to all the files that the instructor is working on and they also work on those files along with the instructor.Also, the projects students do are very different here. Usually, in online training, most training institutes provide projects on small datasets with 5 MB or 10 MB of data. However, our capstone projects have data in 100s of MB and which can go upto GBs at times! At UpX, students also get access to cloud lab which enables them to work on these Big Data sets for their projects.So when a student walks out of this program, he has a portfolio of projects that he has worked on from multiple domains. This incredibly helps students, who do not have formal experience in these technologies, to feel confident when appearing for interviews.The third component that makes us special is our focus on peer learning. We divide our students into study groups of 8-10. Students from the same cities are put in the same group. This enables them to solve problems in teams, just like the way it works in real world. After all, Data Scientists often work on projects in a team. We also have discussion forums, where the students can post their queries and have discussions with the other enrolled students.Another very unique thing about us is our industry immersion sessions. We implemented this based on the feedback we received from our first batch. The students told us that although they understood the concepts and their implementation, they wanted to know how the concepts are actually used in the real-world. So, we introduced industry-immersion sessions wherein the students get to interact with industry experts from top companies like Twitter, Facebook, Vodafone, Barclays, PwC, Zomato, etc. to make them understand how their organizations use Big Data & Data Science. This gives a very good perspective to students who are looking to transition their careers into these tracks and want to know how to crack interviews and what it takes to break into these companies.Lastly, the classes take place over the weekends in the mornings. This means that industry professionals can now acquire skills in Big Data & Analytics without having to compromise on their work!KJ: Since your main focus is providing hands-on experience through case studies and projects. We would like to know the type of projects you offer and the source of these materials.Manvender:So, while some of the case studies have been developed in-house, the projects have been curated from top sources such as World Bank, US Health Department, Carnegie Mellon, Stanford and many more. All these datasets are real-world and are not dummy ones.The students get to choose from a wide variety of datasets from domains such as retail, entertainment, finance, social media, healthcare, etc. on which they want to do their projects on.KJ: Is there any course material that students are provided with?Manvender:Absolutely! Before every class, we give pre-reads, installation guides and ask students to watch a lot of videos to make them well-equipped for the class. After every class, we give them assignments that help them cement the concepts which they have learned. In addition to this, we also give the class recordings, a host of knowledge repositories, PowerPoint presentations etc. to which you can refer to.KJ: After the students complete the course, is there any certification provided that they are trained analytics professional?Manvender:While most other companies provide certifications to everyone who attends their course, we have a little different approach. We award certifications to only those who complete the course and clear the final certification exam that we conduct. What makes the certification exam unique is the fact that it is purely case-based and theres not even a single theory question that is asked. What I can guarantee is that if you clear this exam, then you can definitely crack the interviews that you are aspiring for. We provide a certification of completion to all those who successfully finish the course and projects.KJ: Is there any placement assistance provided to the students?Manvender:Yes, we do assist in placements. We do this in 3 steps.Firstly, we have resources that will help you in resume preparation for Big Data and Data Science roles. Unfortunately, most people working in the IT sector do not know how to build a great resume. We have a set of resources which you can use to take their resume to an altogether different level.Secondly, for interview preparation, we have repositories that will help you get prepared. These repositories contain all the technical questions that you are likely to be asked in Big Data / Data Analytics interviews that you will be appearing for.The third way in which we provide placement assistance is through the industry immersion sessions that we do. For example, if someone has 10 years of experience in Java and when he goes for interviews, he would most likely be asked why should we hire you as your background has primarilyKJ: Thank you Manvender for taking out the time for an interview with us.Manvender:Thank you for having me here. We look forward to seeing many of your readers attend our webinar- How to crack Big Data & Data Science roles, where experts from LBS & ISB will be explaining the opportunities in Big Data, Machine Learning & Data Science. The event will be held on the 11th of December in partnership with Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2016/11/in-talk-with-manvender-singh-ceo-upx-academy-taking-data-science-certification-to-new-heights/
Mystory: I became a Data Scientist after 8 years working as a Software Test Engineer,Learn everything about Analytics|Background|Discovering Analytics|The Start of my Analytics Journey|Overcoming the initial skepticism|Challenges Faced During Transition|Job Hunting|Getting through the Interviews|Great Lakes PGP-BABI experience|Advice to aspiring analytics professionals,"Have a story to share? Career transitions? Challengingprojects? First big project? Our community will love it! Email your stories to us  [emailprotected]|Share this:|Like this:|Related Articles|In talk with Manvender Singh, CEO  UpX Academy  Taking Data Science certification to new heights|25+ websites to find datasets for data science projects|
Guest Blog
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I am Bindhya Rajendran, an Electronics and Communication Engineer, with more than 8 years of experience in Quality assurance and an aspiring Analytics professional. I started my career as a Software Test Engineer where my primary role involved embedded system software testing, which further involved working with real-time data from sensors and other devices like robot arms/chemical deposition chambers etc., primarily used in the manufacturing ofequipments for a semi-conductor. All this highly interested me and I was happy on being associated closely with a field related to my study.Unfortunately or fortunately, I had to move to Hyderabad due to some personal reason and there I got an opportunity to work with ADP, a leading HCM (Human capital management) solution provider where I was exposed to web based software solutions.Starting 2016, I joined Great Lakes PGP-BABI course and 3 months into thecourse; I took this internship offered by a predictive care start-up Touchkin. The offered project involved developing a model based on a passive mobile data collected during the pilot study. This gave me a great opportunity to apply my newly acquired skills to use and actually get hands on experience.Earlier this month, I have taken up a new role of data analytics specialist at BOSCH Engineering and Business solution Pvt. Ltd for the smart cities global solutions project which comprises of Intelligent Traffic management, Intelligent Parking Solutions and Intelligent Traffic solution modules.I followed Analytics Vidhya through this transition and it was very helpful to learn from other professionals. Hence, I thought that sharing my story here might help other people like me in similar situation.It was during my tenure with ADP where analytics solutions were provided as part of the Talent module (HR analytics) offering. I got attracted to this emerging and attractive field of analytics and the vast opportunity it offered to candidates who work with data day in and out.I realized the true strength of data analytics which could be used to predict what will happen in future using various trends. The discovery of unknown insights that has been lying hidden in the data excited me. In addition to that, the experience of undergoing my capstone project during Great Lakes based on social sensing convinced me further for this domain.As I got more curious about the field, I started looking out for the skills required and the different courses offered. It was in one such general talk with my Product Owner who himself is an alumnus of Great Lakes suggested me to take a look at the offerings. As I went through the course and review across board I found that the course offered exactly all the aspects I was looking out for . This included some domain and base courses, advanced analytics, data mining, predictive modeling and also tool exposure in a balanced format.Besides PGP-BABI course work, I was also looking for various opportunities that could help me enhance my analytics learning and keep me updated about the current trends. With the help of PGP-BABI lecturer Mr. Uma Shankar Sir, I also got an opportunity to participate in the GOMC  Google online Marketing Challenge (2016). We did a successful Adword campaign for an online partner business wherein the focus was basically on Brand building strategywith theuse of Google analytics. Our team submission was rated GOOD.I also participated in the3 days long Deep learning Conference and workshop  held by Fifth Elephant and acquired insights on emerging trends in deep learning Analytics domains and NLP and thoroughly enjoyed it.The thing I was most confused about was definitely the domain I should get into. As I was basically from the software testing industry for almost 8 years, I had no idea about areas like finance, BFSI, Retail, Operations orMarketingdomains. Initially, I was concerned about how would I transit with such a big learning gap. However, soon I discovered that I could fit well into emerging domains like IOT and HR analytics. Analytics is definitely predominant in few industries but in current times its becoming an eminent part of emerging fields like health, human resource management, pharma, IOT and other smart solutions as well.Some non-programmers might face difficulty with some of the tools which are more coder friendly, but its just a matter of some practice to overcome that. Also, there are varieties of tools in the industry that can help a non-programmer cope with analytics. Focusing on concepts rather than tools is also very important as the number of new tools coming into the industry is increasing day by day.I didnt go for searching for jobs directly. We had a chance of interacting with Kunal (Founder of Analytics Vidhya) in one of the industry sessions and he advised to apply for internships.SoI applied for Internships through Internshala portal.I got 2 offers; I chose to go with Touchkin as it was related more to my area of interest and the social cause really appealed to my personal sense of satisfaction. My current offer with BOSCH Engineering and Business solution was based on my application on their portal a month or two back when my internship was almost completing and I started looking for a permanent position.I had one telephonic round, 1 Skype, 2 face to face technical interviews and finally the HR round.The telephonic round was very generic scenario based and a little about the current role.Skype round was in more details about models worked on and some generic performance and KPI metrics for machine learning Techniques.F2F technical round 1 was deep dive into each model that I have worked on its approach taken, issues and challenges faced along with inference derived from each model.F2F technical round 2 was an iterative session with the senior management focused on scenario based on the current project that I am being hired for and in detail discussion of the roles and responsibilities of the new role offered.HR round was focusedon my personal interests, motivation and family background.The faculty at Great Lakes is exceptional with very unique blend of industry as well as academic experience, which is an advantage for students like me with an engineering background. The course gets really exciting after the second residencyuntil then its basically the foundation or domain insights. I feel it was a worth career move to take up PGP-BABI with Great Lakes while having 8 years of industry work experience in hand. Thanks to the Great faculty at Great Lakes who made my experience and learning grow to a level I never imagined of.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2016/11/mystory-i-became-a-data-scientist-after-8-years-working-as-a-software-test-engineer/
25+ websites to find datasets for data science projects,Learn everything about Analytics|Introduction|How can you use these sources?|End Notes,"Simple & Genericdatasets to get you started|Huge Datasets  things are getting serious now!|Datasets for predictive modeling & machine learning:|Image classification datasets|Text Classification datasets|Datasets forRecommendation Engine|Websites which Curate list of datasets from various sources:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Mystory: I became a Data Scientist after 8 years working as a Software Test Engineer|Fine-tuning a Keras model using Theano trained Neural Network & Introduction to Transfer Learning|
Kunal Jain
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If there is one sentence, which summarizes the essence of learning data science, it is this:The best way to learn data science is to apply data science.If you are a beginner, you improve tremendously with each new project you undertake. If you are an experienced data science professional, you alreadyknow what I am talking about.However, when I give this advice to people, theyusually ask something in return  Where can I get datasets for practice?They dont realize the amount of data sets available in open. They fail to realize the amount of learning they can get out from working on these projects to get a boost in their career.If you think that the situation above applies to you  Dont worry! you are just at the right place. This article willprovide you a list of websites / resources from which you can use data to do your own (pet) projects oreven create your own products.There is no end to how you canuse these data sources.The application and usage is only limited by your creativity and application.The simplest way to use them is to create data stories and publishing them over web. This would not only improve your data and visualization skills, but also improve your structured thinking.On the other hand, if you are thinking / working on a data based product, these datasets could add power to your product by providing additional / new input data.So, go ahead, work on these projects and share them with the larger world to showcase your data prowess!I have divided these sources in various sections to help you categorizedata sources based on application. We start with simple, generic and easy to handle datasets and then move to huge / industry relevant datasets. We then provide links to dataset for specific purpose  Text Mining, Image classification, Recommendation engine etc. This should provide you a holistic list of data resources.If you can think of any application of these datasets or know of any popular resources which I have missed, please feel free to share them with me in the comments below.I hope that this list of resources would prove extremely useful for people looking out for doing pet projects or side projects. For the starters, this is definitely a gold mine. Make sure you pick a few side projects and continue to work on them.If you can think of any application of these datasets or know of any popular resources which I have missed, please feel free to share them with me in the comments below.Looking forward to hearing from you.",https://www.analyticsvidhya.com/blog/2016/11/25-websites-to-find-datasets-for-data-science-projects/
Fine-tuning a Keras model using Theano trained Neural Network & Introduction to Transfer Learning,Learn everything about Analytics|Introduction|Table of Contents|A bit about the superpowers|When would you prefer Theano over Keras?|Simple neural network model using Theano|Getting to know Transfer Learning and Fine Tuning|Case study with Keras and Theano|Where to go from here|Additional Resources|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|25+ websites to find datasets for data science projects|Solutions for Skilltest Machine Learning : Revealed|
Faizan Shaikh
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have seen the in-depth detailedimplementation of neural networks in Keras and Theano in the previous articles. I think both the libraries are fascinating with their pros one over the other. I decided to make this more interesting and do a comparison between two superpowers of Deep Learning.In this article, Ive covered a basic overview of Keras and Theano. Then I will be explaining in details with the help of use cases why & when should one prefer Theano over Keras. Then Ive explained about advanced techniques like transfer learning and fine tuning, with a case study combining all the mentioned topics.Note:Before we proceed further, lets look at the definitions of Theano and Keras.Theano in a few lines:A programming language which runs on top of Python but has its own data structurewhich are tightly integrated with numpy, allowing faster implementation of mathematical expressionsKeras in a few lines:Keras is a high level library, used specially for building neural network models. Keras was specifically developed for fast execution of ideas.It is written in (and for) PythonTheano and Keras are built keeping specific things in mind and they excel in the fields they were built for. Theano is more of a matrix manipulation library with an optimized compiler on its backend. Whereas Keras is a deep learning library built over Theano to abstract most of the code to give an easier interface.Keras is a very useful deep learning library but it has its own pros and cons, which has been explained in my previos article onKeras. The main scenario in which you would prefer Theano is when you want to build a custom neural network model.Theano is flexible enough when it comes to building your own models. You can write you own functions and optimizations and easily integrate it with your model. This is especially useful in research environment, when it comes to innovating new ideas.Lets take an example:A recently published paper called Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or 1 uses an innovative strategy. As the name says, they change the typical values of weights & activations and replace them with binary values.To build this Binary Net, you would have to rewrite the code for value representations in a neural network. So using low-level libraries like Theano,can easily make this possible.Now, I will be taking you through an implementation of a neural network written in Theano.A typical implementation of Neural Network would be as follows:Here we solve our deep learning practice problem Identify the Digits. Lets for a moment take a look at our problem statement.Our problem is an image recognition, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model. So first, download the train and test files. The dataset contains a zipped file of all the images in the dataset and both the train.csv and test.csv have the name of thecorresponding train and test images. Any additional features are not provided in the datasets, just the raw images are provided in .png format.Now lets first get to know how to build a neural network model in Theano.The above image is represented as numpy array, as seen below:And were done! We just created our own trained neural network!As we have seen before, training a neural network from scratch is a pain. It can require extensive training times as the number of parameters increase. But now with techniques like transfer learning, you can essentially cut short a lot of this training time.But what is Transfer Learning? The technique can be interpreted by observing a teacher-student scenario. A teacher has years of experience in the particular topic he/she teaches. With all this accumulated information, the lectures that students get is a concise and brief overview of the topic. So it can be seen as a transfer of information from the learned to a novice.Keeping in mind this analogy, we compare this to neural network. A neural network is trained on a data. This network gains knowledge from this data, which is compiled as weights of the network. These weights can be extracted and then transferred to any other neural network. Instead of training the other neural network from scratch, we transfer the learned features.This transfer of learned features can be done across any domains. To make the models domain specific, you have to train it on the original domain. This is where fine tuning comes in. Fine tuning is retraining your model on the intended data but with decreased learning rates. You essentially try to change only those things which could increase your models overall effectiveness. Fine tuning can be considered the same as tuning strings of a guitar.Let us try understanding the topics with an example:Suppose you have images of dogs and you want to your model to recognize whether theres a dog in the picture? How would you solve this problem?We take a neural network mode, say VGG-16 (as displayed below) and train it on ImageNet data. For those who dont know what ImageNet is, its a dataset comprised of tens of thousands of images with their respective labels. Training on such a big dataset is no small feat. It takes a lot of time and resources to train on such a data.So what you would do is, download a pre-trained VGG model (which have been trained by other people) and use the weights of this model to initialize your own model. Then you retrain the model on your required problem. During retraining you can essentially freeze the first few layers, i.e. you can set the learning rates of these layers to 0, and continue training. You do this because you are fine-tuning your model, i.e. you adjust the parameters of a pre-trained network to better fit your data.Now lets jump on to a practical use case! We will be using the practice problem again, but this time well be combining everything weve learnt uptil now and implement it. A short overview of what will do, well first train a custom neural network built with Theano, extract pre-trained weights from this network and fine-tune a Keras model. Lets go!NOTE: Some steps well see now have been explained in above code. Were changing some parts of it to see a case study of transfer learning and fine tuningTechniques like Transfer Learning and Fine Tuning are a great way to increase the efficiency of neural networks both in terms of time and memory resources. As you saw in the case study, its pretty easy to implement ideas when you have an understanding of it. Tools like Theano and Keras give us the ability to go beyond what is already done, to innovate and build something useful. In reality, it is you who gets superpowers when you use them!There are many things still unexplored in Deep Learning / Neural Networks, each and every day, there are papers being published and projects being done to push the boundaries. Therefore knowing the tools and how to leverage them is an absolute must to everyone who wants to make a dent in this field.I hope you found this article helpful. Now, its time for you to practice and read as much as you can.Good Luck!Whats your Keras vs Theano story, and which superpower do you prefer? Tell me about it in the comments below and lets discuss further. And to gain expertise in working in neural network dont forget to try out our deep learning practice problem Identify the Digits.",https://www.analyticsvidhya.com/blog/2016/11/fine-tuning-a-keras-model-using-theano-trained-neural-network-introduction-to-transfer-learning/
Solutions for Skilltest Machine Learning : Revealed,Learn everything about Analytics|Introduction|Overall Results|Helpful Resources on Machine Learning|End Note,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Fine-tuning a Keras model using Theano trained Neural Network & Introduction to Transfer Learning|An Introduction to APIs (Application Programming Interfaces) & 5 APIs a Data Scientist must know!|
Ankit Gupta
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Automation and Intelligence has always been a driving force for technological advancements. Techniques like machine learning enable these advancements in every domain possible. With time, we will see machine learning everywhere  from mobile personal assistants to recommendation systems in e-commerce website. Even as a layman, you can not ignore the impact of machine learning on your life.This test was designed for individuals withabasic understanding of machine learning. For all those who took the test, they would have got a fair idea about their machine learning knowledge.A total of 1793 participants registered for the test. It is one of a kind challenge which was aimed to test your machine learning knowledge. And I am sure you must be eager to know the solutions. Read on to find out.And those who missed this test, you did miss out on a great opportunity.But anyways go on and find out how many questions you could have answered correctly. You will take away enough learning points from this article.Below are distribution of scores, this will help you evaluate your performance:You can access the final scores here. More than 600 people participated in the skill test and the highest score obtained was 28. Here are a few statistics about the distribution.Mean score  14.42Median score  16.0Mode score  17.0P.S. The scores on the leaderboard now mightbe different from the scores you got during the competition. We removed the questions with error and have re-scored everyone.Machine Learning basics for a newbie16 New Must Watch Tutorials, Courses on Machine LearningEssentials of Machine Learning AlgorithmsQ1. Which of the following method is best suited to detect outliers in a n-dimensional space, where n > 1:A. Normal Probability plotB. BoxplotC. Mahalonobis distanceD. Scatter PlotSolution: CMahalanobis distance is a statistical measure of the extent to which cases are multivariate outliers, based on a chi-squared distribution. For more detail refer this link.Q2. Logistics regression differs from multiple regression analysis in following ways ?A. It is specifically designed to predict the probability of an eventB. The goodness-of-fit indicesC. In the estimation of regression coefficientsD. All of the aboveSolution: DA: Logistic regression designed for classification problem where we can calculate what is the probabilities of an event occurring.B: A goodness-of-fit test, in general, refers to measuring how well do the observed data corresponds to the fitted (assumed) model. We use logistic regression as a way of checking the model fit.C: After fitting the logistic regression model, we can also observe the relationship (positive and negative relation) between the independent features to target using their coefficients.Q3. What does it mean to bootstrap data?A. To sample m features with replacement from the total M.B. To sample m features without replacement from the total M.C. To sample n examples with replacement from the total N.D. To sample n examples without replacement from the total N.Solution: CIf we dont have enough data to train our algorithm then we can increase the size of our training set by randomly selecting items and duplicating them (with replacement).Q4. Overfitting is a challenge with supervised learning, but not with unsupervised learning. Is the above statement True or False?A. TrueB. FalseSolution: BWe can evaluate an unsupervised machine learning algorithm with the help of unsupervised metrics. For example, we can evaluate the clustering model using adjusted rand score.Q5. Which of the following are true with regards to choosing k in k-fold cross validation?A. Higher value of k is not always better, choosing higher k may slow your process to evaluate your results.B. Choosing ahigher value of k leads to alower bias towards true expected error (as training folds will become similar to the total dataset).C. Select a value of k which always minimizes the variance in CV.D. All of the aboveSolution: D
Larger k value means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) and higher running time (as you are getting closer to the limit case: Leave-One-Out CV). We also need to consider the variance between the k folds accuracy while selecting the k.Q6. A regression model is suffering with multicollinearity. How will you deal with this situation without losing much information?1. Remove both collinear variables.
2. Instead of removing both variables, we can remove only one variable.
3. We can calculate VIF (variance inflation factor) to check the presence of multicollinearity and take action accordingly.
4. Removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalized regression models like ridge or lasso regression.Which of the above statements are true?A. 1B. 2C. 2 and 3D. 2,3 and 4Solution: DTo check multicollinearity, we can create a correlation matrix toidentify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity.VIF value <=4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalizedregressionmodels like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.Q7. After evaluation of model, it is identified that we have high bias in our model. What could be the possible ways to reduce it?A. Reduce the number of features in the model.B. Add more features to the model.C. Add more data points to the model.D. B and CE. All of the aboveSolution: BIf amodel is suffering from high bias, it means that model is less complex, to make themodel more robust, we can add more features in feature space. Adding data points will reduce the variance.Q8. While building a decision tree based model, we split a node on the attribute, which has highest information gain. In the image above, select the attribute which has the highest information gain?A. OutlookB. HumidityC. WindyD. TemperatureSolution: AInformation gain increases with theaverage purity of subsets. To understand the calculation of information gain, read here. You can also check out this slide.Q9. Which of the following is acorrect statement about Information Gain, while splitting a node in a decision tree?1. Less impure node requires more information to describe the population
2. Information gain can be derived using entropy as 1-Entropy
3. Information gain is biased towards choosing the attribute with large number of valuesA. 1B. 2C. 2 and 3D. All statements are correctSolution: CFor better understanding read this article and slide.Q10.An SVM model is suffering with under fitting. Which of the following steps will help to improve the model performance?A. Increase penalty parameter CB. Decrease penalty parameterC. Decrease kernel coefficient (gamma value)Solution: A
In the case of underfitting, we need to increase the complexity of a model. When we increase the value of C,it means that we are making decision boundary more complex. Hence A is the right answer.Q11. Suppose, we were plotting the visualization for different values of gamma (Kernel coefficient) in SVM algorithm. Due to some reason, we forgot to tag the gamma values with visualizations. In that case, which of the following option best explains the gamma values for the images below (1,2,3 left to right, so gamma values are g1 for image1, g2 for image2 and g3 for image3 ) in case of rbf kernel.A. g1 > g2 > g3B. g1 = g2 = g3C. g1 < g2 < g3D. g1 >= g2 >= g3E. g1 <= g2 <= g3Solution: C
Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem. Hence C is the right answer.Q12. We are solving a classification problem (Binary class prediction) and predicting the probabilities of classes instead of actual outcome (0,1). Now suppose, I have taken the model probabilities and applied a threshold of 0.5 to predict the actual class (0,1). Probabilities more than or equals to 0.5 will be consider as positive class (say 1) and below 0.5 will be considered negative class (say 0). Next, if I use a different threshold, which is higher than 0.5 for classification of positive and negative class, what is the most appropriate answer below you can think of?1. The classification will have lower or same recall after increasing threshold.
2. The classification will have higher recall after increasing threshold.
3. The classification will have higher or same precision after increasing threshold.
4. The classification will have lower precision after increasing threshold.A. 1B. 2C. 1 and 3D. 2 and 4E. None of the aboveSolution: CTo understand the impact on precision and recall rate after changing the probability threshold check out this article.Q13. Click through rate prediction is a problem with imbalanced classses (say 99% negative class and 1% positive class in our training data). Suppose, we were building a model on such imbalanced data and we found our training accuracy is 99%. What could be the conclusion?A. Model accuracy is very high we dont need to do anything further.B. Model accuracy is not good and we should try to build a better modelC. Cannot say anything about the modelD. None of theseSolution: BIn an imbalanced data set, accuracy should not be used as a measure of performance because 99% (as given) might only be predicting majority class correctly, but our class of interest is minority class (1%). Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we take the necessary steps. For more about to deal with imbalance class problem, you can refer this article.Q14. Lets say we are training a model using kNN where training data has less number of observations (below is a snapshot of training data with two attributes x, y and two labels as + and o). Now for k=1 in kNN, what would be the leave one out cross validation error?A. 0%B. 100%C. Between 0 to 100%D. None of the aboveSolution: B
In Leave-One-Out cross validation, we will select (n-1) observations for training and 1 observation of validation. Consider each point as a cross validation point and then find the nearest point to this point which will always give the opposite class. Hence for each observation will get misclassified which means that we will get the 100% error.Q15. We want to train a decision tree on a large dataset. What options could you consider for building a model which will take less time for training?1. Increase the depth of tree.
2. Increase learning rate.
3. Decrease the depth of tree.
4. Decrease the number of tree.A. 2B. 1 and 2C. 3D. 3 and 4E. 2 and 3F. 2, 3 and 4Solution: CIf remaining parameters are fixed for decision tree then we can conclude following options:Q16. Which of the following options are true regarding neural network?1. Increasing the number of layers may increase the classification error in test data.
2. Decreasing the number of layers always decreases the classification error in test data.
3. Increasing the number of layers always decreases the classification error in training data.A. 1B. 1 and 3C. 1 and 2D. 2Solution: A
It is generally observed that increasing the number of layers will make the model more generalized. Therefore it would perform better on both the train and test data. But this is not always true. In this paper, the authors have observed that deeper network (a neural network with more layers) has higher training error in comparison to a shallow network (a neural network with lesser layers).
Therefore options 2 and 3 are not true, because the hypotheses is not always correct, whereas option 1 may be true.Q17. Assume we are using the primal non-linearly separable version of the SVM optimization target function. What do we need to do to guarantee that the resulting model is linearly separable?A. C = 1B. C = 0C. C = InfiniteD. None of the aboveSolution: CIf we are using the primal non-linearly separable version of the SVM optimization target function, we need to set C = Infinite to guarantee that the resulting model is linearly separable. Hence option C is correct.Q18. After training an SVM, we can discard all examples which do not support vectors and can still classify new examples?A. TRUEB. FALSESolution: A
This is true because only support vectors affect the boundary.Q19. Which of the following algorithm(s) can be constructed with the help of a neural network?1. K-NN
2. Linear Regression
3. Logistic RegressionA. 1 and 2B. 2 and 3C. 1, 2 and 3D. None of the aboveSolution: B
1. K-NN is an instance-based learning algorithm and does not have any parameters to train. So it can not be constructed with the help of neural network.
2. The simplest neural network performs least squares regression.
3. Neural networks are somewhat related to logistic regression. Basically, we can think of logistic regression as one layer neural network.Q20. Please choose the datasets / problems, where we can apply Hidden Markov Model?A. Gene sequence datasetsB. Movie review datasetsC. Stock market price datasetsD. All of aboveSolution: D
All are the examples of time series dataset where Hidden Markov Model can be applied.Q21. We are building a ML model on a dataset with 5000 features and more than a million observations. Now, we want to train a model on this dataset but we are facing a challenge with this big size data. What are the steps will you consider to train model efficiently?A. We can take random samples from the dataset and will build the model on themB. We will try to use online machine learning algorithmsC. We will apply PCA algorithm to reduce number of featuresD. B and CE. A and BF. All of the aboveSolution: FProcessing a high dimensional data on a limited memory machine is a strenuous task, Following are the methods you can use to tackle such situation:Hence all are correct.Q22. We want to reduce the number of features in a data set. Which of the following steps can you take to reduce features (choose most appropriate answers)?1. Will use Forward Selection method
2. Will use Backward Elimination method
3. We train our model on all features once and get the accuracy of a model on test. Now, we will take 1 feature at a time and shuffle feature values for test data set and apply prediction on shuffled test data. After taking the prediction on this we will evaluate the model. If it increases the accuracy of a model, we will remove this feature.
4. Will look at the correlation table to remove features with high correlationA. 1 and 2B. 2, 3 and 4C. 1, 2 and 4D. AllSolution: DHence option D is correct.Q23. Please choose options which are correct in case of RandomForest and GradientBoosting Trees.A. 2B. 1 and 2C. 1, 3 and 4D. 2 and 4Solution: AHence answer A is true.Q24. For PCA (Principle Component Analysis) transformed features, the independence assumption of Naive Bayes would always be valid because all principal components are orthogonal and hence uncorrelated. Is this statement True or False?A TrueB. FalseSolution: BThis statement is false. Firstly, uncorrelation is not equivalent to independence. And secondly, transformed features are not necessarily uncorrelated.Q25. Which of the following statements is true about PCA?1. We must standardize the data before applying PCA
2. We should select the principal components which explain thehighest variance
3. We should select the principal components which explain thelowest variance
4. We can use PCA for visualizing the data in lower dimensionsA. 1, 2 and 4B. 2 and 4C. 3 and 4D. 1 and 3E. 1, 3 and 4Solution: AQ 26: What would be the optimum number of principle components in the figure givenA. 7B. 30C. 35D. Cant SaySolution: B
We can check in figure that the number of components = 30 is giving highest variance with lowest number of components. Hence option B is the right answer.Q27. Data scientists always use multiple algorithms for prediction and they combine output of multiple machine learning algorithms (known as Ensemble Learning) for getting more robust or generalized output which outperform all the individual models. In which of the following options you think this is true (Choose best possible answer)?A. Base models having thehigher correlation.B. Base models having thelower correlation.C. Use Weighted average instead of Voting methods of ensemble.D. Base models coming from the same algorithmSolution: BRefer below ensemble guides to understand it in detail:Basics of Ensemble Learning Explained in Simple English5 Easy questions on Ensemble Modeling everyone should knowQ28. How can we use clustering method in supervised machine learning challenge?1. We can first create the clusters and then apply supervised machine learning algorithm on different clusters separately.
2. We can take cluster_id as an extra feature in our features space before applying supervised machine learning algorithm.
3. We cant create the clusters before applying supervised machine learning.
4. We cant take cluster_id as an extra feature in our features space before applying supervised machine learning algorithm.A. 2 and 4B. 1 and 2C. 3 and 4D. 1 and 3Solution: BHence B is true.Q29. Which of the following statement(s) are correct?1. A machine learning model with higher accuracy will always indicate a better classifier.
2. When we increase the complexity of a model, it will always decrease the test error.
3. When we increase the complexity of a model, it will always decrease the train error.A. 1B. 2C. 3D. 1 and 3Solution: CQ30. Which options is / are true regarding the GradientBoosting tree algorithm.A. 2 and 4B. 2 and 3C. 1 and 3D. 1 and 4Solution: CQ31. Which of the following is a decision boundary of KNN?A) BB) AC) DD) CE) Cant SaySolution: B
The KNN algorithm classifies new observations by looking at the K nearest neighbors, looking at their labels, and assigning the majority (most popular) label to the new observation. So the decision boundaries may not be linear. Hence option B is correct.Q32. If a trained machine learning model achieves 100% accuracy on test set, does this mean that the model will perform similar on a newer test set, i.e. give 100%?A. Yes, because now the model is general enough to be applied on any dataB. No, because there still are things which the model cannot account, such as noiseSolution: B
The answer is No, Real world data will not always be noise free so in that case we wont get 100% accuracy.Q33. Below are the common Cross Validation Methods:i.Bootstrap with replacement.ii.Leave one out cross validation.iii. 5 Fold cross validation.iv. 2 repeats of 5 Fold cross validationArrange above four methods based on execution time required where sample size is 1000A. i > ii > iii > ivB. ii > iv > iii > iC. iv > i > ii > iiiD. ii > iii > iv > iSolution: BHence Options B is correct option.Q34. RemovedQ35. Variable selection is intended to select the best subset of predictors. In case of variable selection what are things we need to check with respect to model performance?1. Multiple variables trying to do the same job
2. Interpretability of the model
3. Feature information
4. Cross ValidationA. 1 and 4B. 1, 2 and 3C. 1,3 and 4D. All of the aboveSolution: CHence answer C is correct.Q36. Which of the following statement(s) may be true post including additional variables in a linear regression model?1. R-Squared and Adjusted R-squared both are increase
2. R-Squared is constant and Adjusted R-squared increase
3. R-Squared decreases and Adjusted R-squared also decreases
4. R-Squared decreases and Adjusted R-squared increasesA. 1 and 2B. 1 and 3C. 2 and 4D. None of the aboveSolution: D
R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why we must assess the residual plots. However, R-squared has additional problems that the adjusted R-squared and predicted R-squared are designed to address.
Every time you add a predictor to a model, the R-squared increases or remains same.The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.For more detail, you can refer this discussion.Q37. If we were evaluating the model performance with the help below visualizations which we have plotted for three different models for same regression problem on same training data. What do you conclude after seeing this visualization?1. The training error in first plot is maximum as compare to second and third plot.
2. The best model for this regression problem is the last (third) plot because it has minimum training error (zero).
3. The second model is more robust than first and third because it will perform best on unseen data.
4. The third model is overfitting more as compare to first and second.
5. All will perform same because we have not seen the testing data.A. 1 and 3B. 1 and 3C. 1, 3 and 4D. 5Solution: C
The trend in the graphs looks like a quadratic trend over independent variable X. A higher degree(Right graph) polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset. But if you see in left graph we will have training error maximum because it underfits the training data.Q38. What are the assumptions we need to follow while applying linear regression?1. It is important to check for outliers since linear regression is sensitive to outlier effects.
2. The linear regression analysis requires all variables must have normal distribution
3. Linear regression assumes that there is little or no multicollinearity in the data.A. 1 and 2B. 2 and 3C. 1,2 and 3D. None of theseSolution: DQ39. When we build linear models, we look at the correlation between variables. While searching for the correlation coefficient in the correlation matrices, if we found that correlation between 3 pairs of variables (Var1 and Var2 , Var2 and Var3 , Var3 and Var1) is -0.98, 0.45 and 1.23 respectively. What can we infer from this?1. Correlation between Var1 and Var2, it shows a high correlation
2. Since correlation between Var1 and Var2 is very high we can consider it a case of multicollinearity so we can remove either of Var1 and Var2 from our model
3. The correlation coefficient of 1.23 between Var3 and Var1 is not possible.A. 1 and 3B. 1 and 2C. 1,2 and 3D. 1Solution: CQ40. If there is a high non-linearity & complex relationship between dependent & independent variables, a tree model is likely to outperform a classical regression method. Is this statement correct?A. TRUEB. FALSESolution: AWhen data is non-linear, classical regression models fail to generalize on the data, whereas tree based models generally perform better.Q41.RemovedI hope you enjoyed taking the test and you found the solutions helpful. The test focussed on practical challenges one faces in machine learning on day to day basis.We tried to clear all your doubts through this article but if we have missed out on something then let me know in comments below. If you have any suggestions or improvements you think we should make in the next skilltest, let us know in the comments below.We will be launching more helpful skilltest in the coming months, so stay tuned for all the updates.",https://www.analyticsvidhya.com/blog/2016/11/solution-for-skilltest-machine-learning-revealed/
An Introduction to APIs (Application Programming Interfaces) & 5 APIs a Data Scientist must know!,Learn everything about Analytics|Introduction|Table of Contents|1. Overview|2. Categories of API|3. Difference between an API and a Library|4. Walk through an example|5. 5 APIs every Data Scientists should know|6. List of 5 cool data science projects using API|7. Welcome to the new Playground|End Notes,"Web-based system|Operating system|Database system|Hardware System|Facebook API|Google Map API|Twitter API|IBM Watson API|Quandl API|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Solutions for Skilltest Machine Learning : Revealed|Exclusive Interview with Data Scientist  Bishwarup Bhattacharjee (Analytics Vidhya Rank 8)|
Saurav Kaushik
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you are in tech domain, you will invariably bump in references to something called an API. You just cant skip it  if you do, you are boundto hear it again. APIsare being used almost everywhere. But, if you have ever wondered what exactly is an API? orWhy are they important? or How do they help? this article will help you out.In this article, I will explain what is an API in simple terms. I will tell various categories / types of API. Then I will be introducing you to the different APIs which you commonly encounter in your day to day life. To add more value, I have listed down 5 useful projects you can work on using an API. I bet you will be tempted to try your hands on at least one of them.Lets get started!In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software. Think of it like this,Graphical user interface (GUI) or command line interface (CLI) allows humans to Interact with code, where as anApplication programmable interface (API) allows one piece of code to interact with other code.One of the most common use case for APIs is on the web. If you have spent a few hours on internet, you have certainly used APIs. Sharing things on social media, making payments over the web, displaying list of tweets through a social handle  all of these services use API at the back.APIs are widely used by developers for implementing various features in their software. They simply use a simple API call within their software to implement complex features instead of having to code it by themselves.Lets try and understand it better with the help of an example:Pokemon Go has been one of the most popular smartphone games. But in order to build such a game taking in account the large ecosystem, one requires complete information of routes and roads across the globe. Im sure the developers of the Pokemon Go must have faced a dilemma if they should code the maps of the entire world or use the existing Google maps to build their application on top of it. They choose the latter, simply because its practically not possible to create something similar to Google maps in a short span of time.This is just one example. There are a lot of developers using various APIs to implement complex features into their applications instead of coding it themselves. Therefore, API provides a very convenient way of making code reusable.Basic elements of an API:An API hasthree primary elements:A web API is an interface to either a web server or a web browser. These APIs are used extensively for thedevelopment of web applications. These APIs work at either the server end or the client end. Companies like Google, Amazon, eBay all provide web-based API.Some popular examples of web based API are Twitter REST API, Facebook Graph API, Amazon S3 REST API, etc.There are multiple OS based API that offers the functionality of various OS features that can be incorporated in creating windows or mac applications.Some of the examples of OS based API are Cocoa, Carbon, WinAPI, etc.Interaction with most of the database is done using the API calls to the database. These APIs are defined in a manner to pass out the requested data in a predefined format that is understandable by the requesting client.This makes the process of interaction with databases generalised and thereby enhancing the compatibility of applications with the various database. They are very robust and provide a structured interface to database.Some popular examples are Drupal 7 Database API, Drupal 8 Database API, Django API.These APIs allows access to the various hardware components of a system. They are extremely crucial for establishing communication to the hardware. Due to which it makes possible for a range of functions from the collection of sensor data to even display on your screens.For example, the Google PowerMeter API will allow device manufacturers to build home energy monitoring devices that workwith Google PowerMeter.Some other examples of Hardware APIs are: QUANT Electronic, WareNet CheckWare,OpenVX Hardware Acceleration, CubeSensore, etc.At this point, I believe you might be scratching your head and confusing APIs with libraries. Let me simplify it for you, an application programming interface (API) is an interface that defines the way by which an application program may request service from the libraries.An API is a set of rules with which the interaction between various entities is defined. We are specifically talking about interaction between two software.Even a library also has an API which denotes the area of the library which is actually accessible to the user from outside.IBM Watson has made certain data science APIs public for people like us to build amazing projects with only a few lines of code. Here, well be looking at one such amazing API offered by IBM called Personality Insights.This API takes as input in JSON, HTML or simple text format. The input contains text related to the person whose personality interests you. It can be anything like tweets, daily experiences, applications, opinion, etc of that person.The output generated by the API is in the standard format of JSON or CSV file that contains the information on various social traits of that person. And the developer only needs to display this generated file to the user instead of coding the whole functionality yourself.There is also a demo on the IBM website that can be accessed here. You can choose either the tweets or replies of few famous personalities to analyze their personality traits. The text can also be customized based on what input you want to provide and analyze the personality traits of that person.You can integrate this API in your code as well and build an application on top of this API.Facebook API provides an interface to alarge amount of data generated everyday. The innumerable post, comments and shares in various groups & pages produces massive data. And this massive public data provides alarge number of opportunities for analyzing the crowd.It is also incredibly convenient to use Facebook Graph API with both R and python to extract data. To read more about the Facebook API, click here.Google Map API is one of the commonly used API. Its applications vary from integration in a cab service application to the popular Pokemon Go.You can retrieve all the information like location coordinates, distances between locations, routes etc. The fun part is that you can also use this API for creating the distance feature in your datasets as well.Read here to find out its complete implementation.Just like Facebook Graph API, Twitter data can be accessed using the Twitter API as well. You can access all the data like tweets made by any user, the tweets containing a particular term or even a combination of terms, tweets done on the topic in a particular date range, etc.Twitter data is a great resource for performing the tasks like opinion mining, sentiment analysis. For detailed usage of twitter API,read here.IBM Watson offers a set of APIs for performing a host of complex tasks such as Tone analyzer, document conversion, personality insights, visual recognition, text to speech, speech to text, etc by using just few lines of code.This set of APIs differ from the other APIs discussed so far, as they provide service for manipulating and deriving insights from the data.To know indepth details about this API, read here.Quandl lets you invoke the time series information of a large number of stocks for the specified date range. The setting up of Quandl API is very easy and provides a great resource for projects like Stock price prediction, stock profiling, etc. Click here, to read more details aboutQuandl API.I am sure that you are fascinated after reading about the above APIs, but wondering if you could a create project using these APIs which will be great value add to your CV? Well, heres the list of ideas you can start with. You can either use these APIs to retrieve data & manipulate it to extract insights from it or pass the data to these APIs & perform complex functions.Here are the list of the projects for you. Ill leave the execution of these ideas to you.Further reads:Take a step back  you have just got a glimpse of an entirely new world. Think of all the possibilities it enables  need face recognition on your mobile application  no worries! just invoke Google Face recognition API. Need to translate documents in Japanese to English  why not try Google Translate! The possibilities are limitless!A few things to keep in mind while you think of building and using APIs:After going through this article, I believe you would have acquired a better understanding of APIs and how helpful they can be. I have only mentioned some of the popular APIs but the list is endless. If you would like to add to the list of APIs, please share them in the comments below.Ill encourage you to pick up any of the suggested projects and work on it. I bet youll be shocked by the power and ease with which youll be able to perform a complex task that would otherwise have been difficult to implement yourself.Did you enjoyed reading this article? Do share your views in the comment section below.",https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-apis-application-programming-interfaces-5-apis-a-data-scientist-must-know/
Exclusive Interview with Data Scientist  Bishwarup Bhattacharjee (Analytics Vidhya Rank 8),Learn everything about Analytics|Introduction,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|An Introduction to APIs (Application Programming Interfaces) & 5 APIs a Data Scientist must know!|Symposium by IFIM Business School, Bangalore,19th Nov 2016|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Energy and Persistence conquers all things!                                      Benjamin FranklinBishwarup Bhattacharjee, seniordata scientist, Decision Mindsis an epitome of persistence and hard-work. The road to becoming a data scientists is tedious.It requires sheer perseverance and a lot of hard work. Bishwarups journey not only tells us how tomake a career in data science, but also how to become one of the best.Bishwarup completed his Bachelors in Statistics from University of Calcutta. His experience varies from being a data analyst to being an independent analytics consultant and finally joining a startup as a data scientist. He has won several competitions on Analytics Vidhya and is currently ranked 8th on our datahack platform.He is a huge inspiration for all of us and one of the best minds I have come across in analytics industry. We wanted to know more about his journey and what kept him going. So, we conducted an exclusive interview with him.Here are the excepts from my conversations with him!KJ: First of all, I would like to sincerely thank you for devoting time for this interview. Kindly tell us about yourself and how did you start your career in analytics?Bishwarup: I am glad to have this opportunity to present myself to such a fascinating group of professional and aspiring data scientists and I really thank you and the team AnalyticsVidhya at large for that. I hail from a Statistical background. I always liked the part where Statistical methods are used to solve real-life problems like driving the growth of a business or facilitating various workflows in a large organization and many more. On the other hand, Ive always had a knack for learning different programming languages and I am still learning. I think that helped me a lot to get me going and make my journey very interesting as a data scientist. I started as a Data Analyst in a product based start-up. However, not after long I started providing independent consulting services.KJ: When and why did you think that you will start your own consultancy?Bishwarup: While working as an independent consultant, I made quite a few contacts with offshore clients who wanted to work with me in a confidential manner and without the hassle of going through different freelancing web portals. In the beginning, I would take care of most of their requirements by myself, but going ahead I got involved with a number of long term projects where time management became an issue. I was almost working 16 hours a day at a point. So, I thought of reaching out to a few like-minded people I knew who could potentially help me in this regard. All of us together, thought that it would be better for us to work as a team rather than a number of people working on their own. So, I went ahead and registered a business. The business was good in its initial years. But today almost every analytics service is being automated and we too took a pretty bad hit. So very recently, I thought of moving out of what I was doing and at this moment, I am working with Decision Minds Pvt Ltd, a US based start-up, in the role of a Senior Data Scientist.KJ: Tell us 3 things life has taught you in your journey from a data analyst to founder of a company and going back to corporate again?Bishwarup:KJ: Tell us a bit more on what challenges did you face in your journey? How did you overcome them?Bishwarup: The most critical challenge, I think you would agree, to make up your mind when thinking something out of the box. Apart from that, I had a financial constraint at a point which got better with time. Also, I would like to mention that when I started my own consultancy services.KJ: You are currently ranked 74th on Kaggle among more than 50,000 people. Amazing feat. Please describe your journey.Bishwarup: I joined Kaggle almost a couple of years back. At that time, I was exploring potential ways of enhancing my skills in data science, not just by enrolling into some online course but something that will provide me with hands-on experience of what it takes to deal with large scale data. I found Kaggle very useful and I am really glad that I kept myself involved in there. My first competition in Kaggle was Springleaf Marketing and the data was quite large to fit into 4GB RAM laptop that I had at that point. I was confused by the advanced discussions going on the forums and I had little idea how to efficiently approach the problem. However, I went ahead and rented a AWS instance to implement whatever I could learn and we finally ended up at the 27th position in the private leaderboard among 2226 teams. I was pretty satisfied with my effort and since then Ive always believed I can do better in every competition in which I take part and thats what helped me to learn a lot of new things including stuff like how to preprocess large data files in a number of different ways, stacked generalization and many more. From my personal experience, I have seen people who think of platforms like Kaggle, AnalyticsVidhya, KDD etc. as just a fun competition organizers. However, if you ask me, I would rate these platforms even higher than attending a course in Coursera or Udacity. These platforms promote self-learning which, in my opinion, is the best way to master a subject.KJ: Recently, youve won various other data science competitions including AV Hackthons, CrowdAnalytix etc. I must say youve got the midas touch. Is there any structure / formula / framework you follow to build this winning streak?Bishwarup:KJ: How do you decide in which Kaggle competition should you participate?Bishwarup: Given the time, I would like to participate in all of the competitions as each of them helps one learn something new. However, there is a resource and time constraint on my side, so before entering a contest I like to think of the amount of time I would probably be able to invest behind it. There is no point taking part in a competition, copy some forum scripts and make a number of submissions.There are also a lot of competitions related to computer vision that are held in Kaggle, but I dont have much idea about that subject. I would really like to learn that in near future and participate in such competitions.KJ: Which mode do you prefer in a competition? Team or Self?Bishwarup: I personally prefer competing solo mainly because of the reason that after the end of the competition you get to know what possibly you could have done differently to make your model better. But participating in team is also advantageous for a number of reasons. For one, you get to learn different ideas and concepts from your teammates. At the same time, it offers a really good scope of ensembling different approaches and surging the leaderboard.KJ: According to you, what should be the ideal approach by people to solve problems in these competitions?Bishwarup: I dont think that there is a one size fits all solution here as the problems are quite varied and they come in their own flavours. However, certain things are common across them. For example,KJ: Which techniques / algorithms do you think are the most important to learn to give a tough fight in these competitions? Why?KJ: According to you, how different are these competitions to real life challenges which are solved in industry using data science and machine learning?Bishwarup:There is a fundamental difference indeed. In industry, businesses are looking for estimates which are backed by a certain confidence bands  the confidence interval is more crucial than just a point estimate whereas in online competitions we are crazy about optimizing the evaluation metric even up to the 5th or 6th decimal places. In real life use cases, people are more interested in a directional view of where the business is heading to and find the potential drivers of the change whereas in online contests we rarely bother about what insight we can gather from the data. Besides, the complicated stacked models that we develop to crack the online contests are hardly possible to implement in production environment due to their complexity/long training time/ stochastic nature of optimization. However, it does not mean that such contests offer no career value for people working in companies  I think most of the companies in their analytics wing, use algorithms like Random Forest or even xGboost. One just have to factor in the domain perspective rather than just applying the black-box machine learning to solve a problem in industry.KJ: As per your experience, which tutorials, online courses, MOOCs are must to undergo for aspiring data scientists? Which one helped you the most personally?Bishwarup: I havent taken any online courses, MOOCs or followed any online or offline contents. I learnt by debugging  whenever stuck with something I would go to stackoverflow or google forums to search for answers. I think its the best way to learn. Of course, the courses in websites like Coursera, Udacity or Udemy provide a lot of value  they will give you a jump start  but if you want to master a subject, better do it by practicing and being on your own. You will get hundreds of contents describing what deep learning is and where it is used, but not a single content in the web completely describing how to install [theano + gpu + cuda + cudnn] on a Windows machine. In my opinion, there is no use of going through an article if you cant practice the same side by side.KJ: If you had a chance to go back in time, what are the things you would have done differently?Bishwarup: I would learn Java to its core and also try to create some kind of routine in my life.KJ: What things a fresher must do to get his/her first break in analytics?Bishwarup: I would be honest, its a very competitive market down there. Going back 7-8 years from now, companies would hire people who can efficiently run a logistic regression but today the outlook is completely different. Corporate establishments are looking for people extremely well equipped with latest technologies  however, it is not to demotivate the folks who just came out of college or looking to move to data science domain  I would say:Also you should follow the AnalyticsVidhya blog posts which are full of helpful materials. Last time I remember, I read a blog post on D3.js and it was pretty well written.I would like to thank the AnalyticsVidhya team once more for providing me with this wonderful opportunity and I look forward to having a long-term relationship with you guys. Your blog posts are fantastic and you guys are doing a great job for the data science community at large. My best wishes for you.KJ:Thank you Bishwarup for your invaluable time and thoughts. I am sure a lot of people inanalytics & data science industry will benefit from this. All the best to you too!",https://www.analyticsvidhya.com/blog/2016/11/exclusive-interview-bishwarup-bhattacharjee-analytics-vidhya-rank-8/
"Symposium by IFIM Business School, Bangalore,19th Nov 2016",Learn everything about Analytics|Introduction,"Why should you attend this event?|Share this:|Like this:|Related Articles|Exclusive Interview with Data Scientist  Bishwarup Bhattacharjee (Analytics Vidhya Rank 8)|8 Interesting Data Science Games to break the ice & Monday Blues!|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Center of Excellence-Business Analytics, IFIM Business School in association with Scholars University, USA is organizing an exclusive half a day symposium on Digital Transformation: Impact on Corporate Strategy The event will witness two panel discussions having global experts and thought leaders as panelists.Venue: IFIM Business SchoolDate: 19th November 2016Time: 10 AMLocation: Bangalore RegisterHere
The panel discussions would focus on the following themes:THEME I: Role of Cloud, Social Media, Mobile &Big Data Analytics in driving Corporate Strategy by Reducing the Uncertaintiesin Value ChainTHEME II: Role of Technology like Cloud, Mobility, Big Data Analytics and Block Chain Technology in Logistics and Supply chain to drive Corporate Strategy.Following is the list of confirmed panelists for the event:For more details: VisitHere
You can also read this article on Analytics Vidhya's Android APP ",https://www.analyticsvidhya.com/blog/2016/11/symposium-by-ifim-business-school-bangalore19th-nov-2016/
8 Interesting Data Science Games to break the ice & Monday Blues!,Learn everything about Analytics|Introduction|List of Interesting Games,"Game 1: The Circle|Game 2: Data Science Dumb charade|Game 3: Who am I?|Game 4:Writing efficient code!|Game 5: Can you find me?|Game 6: What do I stand for ?|Game 7: Passing the Pass|Game 8: Quiz me up?|End Note|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Symposium by IFIM Business School, Bangalore,19th Nov 2016|Tryst with Deep Learning in International Data Science Game 2016|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"All of us have been there  coming to office after a hectic weekend trip or a late night binge on Sunday! It is difficult to drag yourself out of bed on Monday morning. To add to it, imagine having a team meeting on Monday morning where everyone in your team tells what happened in the weeks gone by and what is planned in the weeks to come!One of my managers used to hold these team meetings religiously. I can understand the reason behind these meetings but at times, you would see people walking in these meetings with sleep in their eyes. You could see people yawning when you are discussing the week gone by!To beat this routine, we decided to start these meetings with ice-breakers or some games which pull people out from the weekend slumber. Since this was a data science team, we decided to keep these games related to data science. Because of these games, the meetings transformed into high energy conversations and lot more fun than what they used to be.Since then, I have developed a repository of games and I use them before almost any long format team meeting. I am sharing some of them in this article today. I am sure these will appeal to the data nerd inside you!Go on  read and play along as you read!The team / group of people who forms a circle. One person comes inside the circle and has to ask one question on any package / technique / library related to data science which he / she thinks that the rest of the team wont be able to answer. If the rest of the team answers the question, the person gets out of the game. If the team is unable to answer, the person gets back into the game. The person entering the circle nominates someone to come inside the circle irrespective if he remains in the game or not. The last person remaining in the game wins. Rule 1: The questions cant be repeated. Rule 2: The person cannot take hints from the team members. Rule 3: Every individual should get a chance before a person gets a second chance. Rule 4: Every individual is given 1 min to ask a question, and the team members have to answer immediately. Tip : This game can change its character depending on how many people are playing it. In smaller team settings, it could be a lot of fun to ask some simple basic questions and you will be surprised to see how many people miss out on them! For example, ask whether a significant interaction term in regression means correlation in input values? and see the results for yourself.People are divided in 2 teams. One individual from each team comes forward and is asked to act out something related to data science. It could either names of a package, library or algorithm. The demonstrations can end up being hilarious, wacky and funny. The twist is that the teammates have to not only guess the name but also tell what it does. Each winning team is awarded one point every time. Rule 1: The person cannot speak or murmur or write anything.Rule 2: The team has to guess within 2 minutesRule 3: The names given should be validRule 4: Correct guess wins 1 point. No negative marking for incorrect guesses. Tip:If you are planning to play this, I would recommend deciding on a few signs before hand. Decide on signals whether the thing you are acting on is related to tools or techniques. If it is a tool, R vs. Pytohn vs. other language.There is a group of people and they all are given one post-it. They have to write a name of any package, library, tool or a language on the post-it secretively. Then each person sticks that post-it to another team members forehead without revealing what is written on the post it. Each person can have only one post-it on their forehead. Now, individually they have to guess what is written on their post-it by asking yes or no questions to other team members. The person who is unable to guess the name correctly at the end has to treat the team on a coffee. Rule 1: The person is not allowed to see his/her sticker. Rule 2: The person who has been asked a question is not allowed to speak words other than Yes or NoRule 3: No hints can be exchanged. Rule 4: No individual can write their own post-it. Rule 5:You can not ask more than 1 question to same person simultaneouslyTime to open up your laptops. People are sub-divided in 3  4 sub groupsand they are given a task to be applied on a data (like removing missing values, sorting the data, etc). They have to perform the task with minimum number of iterations. Most optimal code wins the challenge (based on time of execution).Rule 1: Each team has a maximum of 15 mins to write the code. Rule 2: The code evaluation should happen on the same machine for all the teams.P.S. Lazy operations are not allowed!There is a word cloud created with attributes of a machine learning algorithm. Based on the information provided, the teams have to guess which algorithm is described in the word cloud. There could be several rounds and difficulty level could vary from easy to hard. Each team winning a round will qualify for the next round. Rule 1: There may be two algorithms which can be described in the word cloud.Rule 2: Each team has limited amount of time(say 5 min). Rule 3: Teams can keep guessing until they find out the true algorithm(s). Rule 4: The level of difficulty for the word clouds increases with every round. Rule 5: The team winning the final round wins the game. Example:Here is a word cloud shown and you have to guess the name of algorithm associated with this word cloud:1. The terms Linear and Regression together shows that this is a linear algorithm which is used for regression problem.2. The terms Sumofsquare, Coefficient and Penalty: It can be seen as a term which adds a penalty to sum of squares of coefficients in the optimization objective.3. The term L2 has aspecial meaning which can be used toidentify ridge regression algorithm.4. The term Alpha can be seen as the parameter which balances the amount of emphasis given to minimizing RSS (Residual sum of square)So from above points, we can identify that the algorithm is a Ridge Regression.Acronym  A group is asked questions on the full forms of different data science terms. Rule 1: Time given for every attempt is 10 secondsRule 2: Points gained for a correct answer = +2. Incorrect answer = -1Rule 3:For every passed question answered correctly, a team wins = +2Example:Ask the full forms of the acronyms like RMSE, XG Boost, FTRL, SVM, etc.There is a box which has tickets in it with names of different libraries, packages, etc. People sit in a circle and music is played. People pass the box till the time music is played. When the music stops, the person who has a box will need to pull out a ticket and then say 3 sentences about the ticket he / she gets. Once done, the person moves out of the game.Rule 1: The person playing the music should not be able to see the rest of the teamRule 2: Music should be played for a fixed durationNormal quizzing round. In which the challenge consists of 20-30 questions on data science. By the end of the game, the person answering the maximum number of questions will be ranked number 1 and so on. Rule 1: The questions should have anequal distribution of easy, intermediate and hard questions.Rule 2: Questions can be passed to another candidate, and each pass questions wins 2 points. Rule 3: The challenge can have a mix of coding questions, visual questions and purely observational questionsRule 4: Each correct answer wins 1 point and no negative marking for wrong answers.Rule 5: the person buzzing first gets 15 secs to answer the question.Example:The questions can be on anything related to data science.For eg,Which of the following uses data on some object to predict values for other object ?a) Inferentialb) Exploratoryc) Predictived) None of the MentionedI hope you found these games interesting. Feel free to customize them as per your needs. Go and play these games with your teammates and tell me what was your experience. We will keep bringing you such interesting reads and will keep exciting you.In case you have any confusion about any of the games, please drop in your comments below. And if you have played any of these games I would love to know what was your experience. If you have some more additions to the list, feel free to tell us through comments.",https://www.analyticsvidhya.com/blog/2016/11/8-interesting-data-science-games-to-break-the-ice-monday-blues/
Tryst with Deep Learning in International Data Science Game 2016,Learn everything about Analytics|Introduction|About Data Science Game|About the hackathon|Detailed Approach,"1. Methodology|2. Implementation|End Note|About the Authors|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|8 Interesting Data Science Games to break the ice & Monday Blues!|Creating an artificial artist: Color your photos using Neural Networks|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Proof of the pudding lies in the eating. It takes working on the Deep network and witness it progressively produce good accuracy, to be truly amazed by the power of a Deep neural network. Using Deep Learning in a competitive environment requires all the more (as opposed to research) understanding of its strengths and costs. This is due to time boundedness and hence limited possibilities for experimentation.In this blog, we narrate our experience in using Deep Learning to achieve qualifying accuracy for apreliminary round of the International Data Science Game 2016. We are one of the 20 teams which qualified for the final round of the competition at Paris. We begin with a brief description of the problem statement, move through methodology and the implementation. We close the article, with few points from a panel discussion from the final event on the theme Does Deep Learning mark the end of conventional Feature Engineering?Most of the online resources on deep learning are focussed on helping the reader getting started. In this blog, we go a step ahead and discuss the use of Deep learning on a real life image Dataset. The analyses of this Dataset has potential applications in solar energy harness. The underlying idea is to bring out practical aspect of the much-discussed-in-theory-concepts like Data Augmentation and Semi-supervised learning.Data Science Game is an International Inter-University student team challenge, hosted by the Data Science Game Organization. The competition was conducted in 2 phases, an online preliminary round and a final face off in Paris. In the preliminary round, 143 teams coming from universities from 28 countries competed, with top 20 teams qualifying for the second round. The phase 2 of the competition involved a fresh data science problem over 2 days.The competition had two phases, the first phase was an online competition held in from June 17 to July 10.The participants were required to solve a real life business challenge.The participants were required to solve a predictive problem containing complex data, with the help of statistical algorithms. The second phase of the competition was held in Paris held on September 10th & 11th, where top 20 teams competed in a machine learning challenge.Phase 1Initial ParticipationThe initial participation included 143 teams participating more than 50 universities from 28 countries. The participants were tested on a real-life business challenge. It was an online round and all participants were testedon their data science skills.ChallengeDetailsThe participants were provided a computer vision problem. In France, OpenSolarMap provides satellite images of roof of 80,000 houses to find out the potential of solar energy production. Over 15,000 roof orientation have been collected by the company but they are facing an issue to automatically classify the orientation of roof. To tackle this problem, the participants were required to build an algorithm to test recognise the roof orientation from asatellite image.Phase 2The top 20 finalists from the Phase 1 were invited to Paris for the final round of the competition. All the participants were welcomed at Capgeminis Les Fontaines campus. There were around 80 students waiting to compete for the Data Science Champion title.Competition ChallengeThe problem set for the final round was based on an Automobile Insurance quotes received by Axa. In this, the participants had to predict if a person who requested a given quote bought the associated insurance policy.The participated were provided free access to Microsofts Azure computing clusters. The hackathon lasted for 30-hours and the performance measure used was log-loss.Now, we will be discussing the problem set from Phase 1 and what approach did we take. Read on, to know our complete approach.Problem set for the preliminary round comprised of classification of satellite images of rooftops into 4 classes as shown below. Training dataset comprised of 8000 images in the ratio of 4:2:1:2 for the classes North-South, East-West, Flat and Other. Evaluation was based on classification accuracy on 14000 images. In addition to the training and the test dataset there were 22000 unmarked images.North-SouthEast-WestFlatOtherThe use of Convolutional Neural Network helped us beat the accuracy of conventional methodologies. Case specific data augmentation strategies, semi-supervised learn ing and ensembling of numerous models were instrumental in achieving the qualification benchmark for accuracy. Many of these ideas evolved as a result of extensive experimentation with the data. In a data science competition you either go up or down. Hence, it is important to continuously experiment during the course of the competition.Before discussing our approach based on Deep Learning, we must pay respect to another school of thought, which employs supervised and unsupervised models with engineered features on available data as input. At the opening ceremony of the final round, the organizers announced that the problem of the prelim round could have been solved at 79% accuracy merely using multi-logit regression and feature engineering. It was a surprise to see such accuracy coming from the use of simple model. Hence we realized that it is less of Deep Learning versus Conventional Models, but more of Deep Learning versus the human ability to create distinguishing features in classification in cases of high level Data representation.Indeed, the problem given was dealing with images. A very generic representation of events and intuitive methods of generating granular features (for example, detecting presence of edges, finding contrast gradient, common shape identification) lead to an incoherent description of the image, which failed to capture all characteristic features. If these features are not good they can end up confusing the learner.Since a quantum leap was required to capture the higher level of abstraction present in the data like complex edge patterns, shapes, different color blobs etc., it was needed to choose a model which can bring in the complexity in feature generation. The feature generation process is eventually automated and tuned to reduce the training error as minimum as possible. We turned to Convolutional Neural Networks.Convolutional image networks are sparsely connected neural networks, with enhanced feature generation capabilities over the conventional image processing techniques. Deep Learning enables higher degree of abstraction. With Convolutional neural network, we could achieve a single model accuracy of approximately 80% and 82%. The team scoring highest achieved accuracy of 86%.Accuracy in a Deep Learning model is extremely sensitive to the volume of training data. With higher number of training examples, the generalization increases. We followed different data augmentation strategies. For example, adding noise to the image, images obtained by rotating original images, cropped portion of original images, 90 Degree rotation of the North-South and East-West Images. This process eventually generated another set of images those were almost similar to previous images but not completely. The added noise in those image data increased the information content and hence learner got new examples to learn. This way, the effective training data comprised of nearly 10 times the images provided in the original training data.Event organizers used crowdsourcing methods to mark the images in the training dataset. Since crowd sourced experimentation involves much time and cost, it is extremely likely to have large number of unlabeled data points. The whole data set consisted of 22, 000 unlabeled images, with potential of meaningful information for the Neural Network. Our strategy was to label the images, using the already trained network, with a hope that the volume advantage of having increased dataset for training is more than losses due to noise in the added dataset. Use of semi-supervised learning which allowed us to witness a clear break from our accuracy saturation and we left the plateau to climb up in the leaderboard. Till that point, our model was giving an accuracy of 78% on the validation set. We used an ensemble of models to predict the classes for the unlabeled images. The different models in the ensemble came from local minima of validation error. Finally, a majority vote over 10 models was a deciding call for a class to be attached to an unlabeled image. With lower variance due to bagging, we have certainly identified 80% of the unlabeled images correctly. The new information was again fed to the already trained network, resulting in accuracy improvement of 2-4%. Even the small increase in accuracy at this stage was critical since it was in the range where most of the top models at that time were. The overall experience shows the power of a Deep network acting as an oracle in deciding about the labels for unlabeled data and in turn using that imparted knowledge to make itself better learnt.To decrease the variance in prediction, it is advisable to use ensemble of learners to predict. As by the construct of the experiment, every time the network have been fed by a slightly different but bigger dataset, in different instances of training, it is possible to store different models which have potential to predict with almost similar accuracy. It has been thoroughly checked that these models indeed perform well on different segments of validation data. The difference in learning is caused by different updates of network weights across learning phases. The advantage of using Deep Network is that with slight variation in training data can tweak the optimization of minimizing training error and can lead to similar local optima with a different set of parameter (weights here) value. We also created ensemble of ensembles with proportional weights to their individual accuracy. This strategy boosted our performance by 2%, which again was quite critical at that stage.To implement the CNN model we used Keras, which is a Deep Learning library in Python. It was the natural choice for us since numerous tutorial and codes for Deep Learning on Keras were available online. Some of the benefits of using Keras based on our experience wereImages in the raw dataset were of non-uniform sizes. We decided to convert all the images to 128 * 128 Pixel as 128 was the approximate median of both length and breadth, since this would minimize information loss for most images. To fill empty portions within images left after resizing we used bilinear interpolation. For Image processing we used the OpenCV Package in Python. To verify our choice of the optimal size for images, we trained a simple network on 64*64 pixel images and 128*128 pixel images. We observed higher accuracy in the later, even though it came at the cost of higher training time.The ImageDataGenerator function simplifies the process of feeding data to the network. It automatically reads the training and validation images from the respective folders. The class should name the folders containing the images.In the limited time scenario of the competition, time taken to transfer train images within folders can be significant. The number of Train images increased even more after semi-supervised learning and further increased due to Data augmentation. To minimize the time going into transfer of large number of files, we used scripts in command line as well as the Shuttl module in python. The file transfer time was an important concern, since we had to do Image Preprocessing and Model Training on different computers, and hence first files had to be transferred to USB and from USB to other computer.Attached below is a snippet of the Network build on Keras.As the training dataset size increased via Augmentation strategies and Semi-Supervised learning, training time per epoch started to explode to an extent that it was difficult to decide the point of termination of the training. With more time per epoch, to be sure of the point of divergence of the validation error and training error, more time had to be spent per experimentation. To give an estimate of the epoch time, on a 8GB RAM machine with AMD M380 GPUs (96 GB/s) one epoch used to take half an hour on the 8000 images dataset. This time increased up to 3 hours for one epoch with augmented data. Typically the network would take 15-20 epochs to train. To some extent, this time can be minimized by generating images for augmentation on the fly in keras, which is primarily due to less time required for seeking files on the disk.The beginning of the final round of the competition was marked by a panel discussion on the lines of Deep Learning versus Conventional Algorithms. The speakers were Data Science heads of large organizations and academicians, and hence it was natural to expect thought provoking takeaways. Where at one end Deep learning has solved numerous challenges and opened doors to new possibilities, the importance of conventional methods, which focus on understanding data generation process, cannot be given less importance. Numerous applications of Data Science in industry strongly depend upon the interpretability of the underlying algorithm in addition to high accuracy. Very relevant here are the developments in programming, where despite massive leaps in higher level languages, low level languages like C# are indispensable in certain applications. Analogous to the scenario in programming, simple models combined with feature engineering will continue to have a very important position. Moreover, high accuracy in Deep learning is not without a skillful Data Scientist and hence Deep learning is not a replacement for skills of a Data Scientist. Summarizing, it is important to be aware of the power of Deep learning but a complete reliance is unadvisable.We hope you enjoyed reading this article.We are grateful to share our approach with you all to enhance learning and add more material on Deep Learning. If you have any questions or doubts, drop in your comments below.Robin SinghBodhisattwa MajumderThe authors are Robin Singhskilled in machine learning, complex networks & information retrieval andBodhisattwa Prasad Majumderskilled in machine learning, distributed computing & information retrieval. They both are students of Post Graduate Diploma in Business Analytics by ISI Kolkata, IIT Kharagpur and IIM Calcutta.Their teammates who participated in the Data Science Game challenge together with them are Ayan Sengupta andJayanta Mandi.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2016/11/tryst-with-deep-learning-in-international-data-science-game-2016/
Creating an artificial artist: Color your photos using Neural Networks,Learn everything about Analytics|Introduction|Table of Contents|What is Neural Art?|Getting into the brain of artificial artist:|Coding it up!|Where to go from here?|Additional Resources|References|Image Sources|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Tryst with Deep Learning in International Data Science Game 2016|An Introduction to Clustering and different methods of clustering|
Faizan Shaikh
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Art has always transcended eons of human existence. We can see its traces from pre-historic time as the Harappan art in the Indus Valley Civilization to the contemporary art in modern times. Mostly, art has been a means to express ones creativity, viewpoints of how we perceive the world. As legendary Leonardo Da Vinci has said,Painting is poetry that is seen rather than felt.What we sometimes forget that most of the art follows a pattern. A pattern that pleases us and makes sense in our brain. The next time you see a painting, try to notice the brush strokes in it. You will see a pattern arising out of the painting. We as humans are skilled in recognising these patterns. Our neural mechanisms has developed to be exceptionally great over the years recognising patterns in the wild.Now you may ask why I am ranting away about art and patterns? This is because I will show you how to create art with the help of artificial brains! In this article, we will build an artificial neural network which will extract style from one image and replicate it on the other. So are you ready?Lets try to understand this topic with an example.Source [1]
The above image is the famous The Starry Night by Vincent Van Gogh. Just look at the painting for a few minutes. What do you see? Do you notice the bush strokes? Do you see the curves and edges that define each and every object which makes it so easy for you to recognise them?Now lets do a quick assignment. Try to remember the patterns you see. Just cram your brain with every little detail. Done? Ok now take a look at the next image.Source [2]
This is a photograph taken of a town called Tubingen located in Germany. For the next step of the assignment, just close your eyes and try to replicate the style of astarry night with this image. Ask yourself, if you are Van Gogh (hypothetically of course!) and are asked to draw this photograph keeping in mind the styles you memorized before, how would you do it?Think....Did you do it? Great! You just made a neural art!...Want to see what an artificial neural network can do?
Source [2]
You may ask how did a machine accomplish such a task. Its simple once you get the gist of it!What neural network does is, it tries to extract the important points from the both the images, that is it tries to recognize which attributes define the picture and learns from it. These learned attributes are an internal representation of the neural network, which can be seen as below.Source [2]
So you got to know the theoretical concepts involved in neural art, now lets get to know the practical aspects of implementing it.Neural Art works in the following way:We will get to know some of the important points you ought to know before we jump in. While most of the fundamentals of Neural Networks are covered in this article, I will reiterate some of them and explain a few extra things.Now that weve understood what our flow will be to build a neural art, lets get down and start hacking stuff!This Diwali was an interesting one for me. I decided to do some research on neural art and how India illuminates during the Diwali day. I came across this image India on Diwali night. And I thought of creating something similar on the same lines. To do that, we will be combining the two images below with the help of neural art. Source [3]
So first we will first set the groundworks.Step 0: Install Keras and its dependencies . For this, we will be using a Theano backend. Change your backend by following the steps mentioned here. Also additionally you have to set the proper ordering for image. In the keras.json file, where you have changed the backend, replace image_dim_ordering with tr. So it should look like this,Step 1: Then go to your working directory and set your directory structure as belowStep 2: Start a jupyter notebook in your working directory by typing jupyter notebook and implement the following code. I will just provide you a step by step overview of what each block does.   NOTE: The code file can be viewed on github here.We have seen a small demo of a significant discovery in the art world. There have been many modifications done to this method to make it aesthetically pleasing. For example, I really like this implementation in which they have taken different styles and applied them to different regions.  The first two images are the masks, which help to set which part should be stylized. The next two images represent the styles to be used. The last image is the base image that has to be stylized.Below is the output that is generated by neural art.Looks awesome, doesnt it? I am sure like me, you are also fascinated to try your hands on neural art. To help you get started with it, I have covered the basics of neural art and how can you create your first image. I am sure you are eager to explore more and hence I am adding some additional resources only for you.These are some of the best resources I have come across on neural art. Go ahead and enter the fascinating world of neural art.I hope you found this article inspiring. Now, its time for you to go through it and make art yourself! If you create an art do share it with the community. If you have any doubts, Id love to interact with you in comments. And to gain expertise in working in neural network dont forget to try out our deep learning practice problem Identify the Digits.",https://www.analyticsvidhya.com/blog/2016/11/creating-an-artificial-artist-color-your-photos-using-neural-networks/
An Introduction to Clustering and different methods of clustering,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Overview|2. Types of Clustering|3. Types of clustering algorithms|4. K Means Clustering|5. Hierarchical Clustering|6. Difference between K Means and Hierarchical clustering|7. Applications of Clustering|8. Improving Supervised Learning Algorithms with Clustering|End Notes,"Got expertise in Machine Learning / Big Data / Data Science? Show case your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Related Articles|Creating an artificial artist: Color your photos using Neural Networks|Investigation on handling Structured & Imbalanced Datasets with Deep Learning|
Saurav Kaushik
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you come across a situation when a Chief Marketing Officer of a company tells you  Help me understand our customers better so that we can market our products to them in a better manner!I did and the analyst in me was completely clueless what to do! I was used to getting specific problems, where there is an outcome to be predicted for variousset of conditions. But I had no clue what to do in this case. If the person would have asked me to calculate Life Time Value (LTV) or propensity of Cross-sell, I wouldnt have blinked. But this question looked very broad to me.This is usually the first reaction when you come across an unsupervised learning problem for the first time! You are not looking for specific insights for a phenomena, but what you are looking for are structures with in data with out them being tied down to a specific outcome.The methodof identifying similar groups of data in a dataset is called clustering. It is one of the most popular techniques in data science. Entities in each group are comparatively more similar to entities of that group than those of the other groups. In this article, I will be taking you through the types of clustering, different clustering algorithms and a comparison between two of the most commonly used clustering methods.Lets get started.Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.Lets understand this with an example. Suppose, you are the head of a rental store and wish to understand preferences of your costumers to scale up your business. Is it possible for you to look at details of each costumer and devise a unique business strategy for each one of them? Definitely not. But, what you can do is to cluster all of your costumers into say 10 groups based on their purchasing habits and use a separate strategy for costumersin each of these 10 groups. And this is what we call clustering.Now, that we understand what is clustering. Lets take a look at the types of clustering.Broadly speaking, clustering can be divided into two subgroups :Since the task of clustering is subjective, the means that can be used for achieving this goal are plenty. Every methodology follows a different set of rules for defining the similarity among data points. In fact, there are more than 100 clustering algorithms known. But few of the algorithms are used popularly, lets look at them in detail:Now I will be taking you through two of the most popular clustering algorithms in detail  K Means clustering and Hierarchical clustering.Lets begin.K means is an iterative clustering algorithm that aims to find local maxima in each iteration. This algorithm works in these 5 steps :Here is a live coding window where you can try out K Means Algorithm using scikit-learn library.Hierarchical clustering, as the name suggests is an algorithm that builds hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left.The results of hierarchical clustering can be shown using dendrogram. The dendrogram can be interpreted as:At the bottom, we start with 25 data points, each assigned to separate clusters. Two closest clusters are then merged till we have just one cluster at the top. The height in the dendrogram at which two clusters are merged represents the distance between two clusters in the data space.The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.In the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB.Two important things that you should know about hierarchical clustering are:Clustering has a large no. of applications spread across various domains. Some of the most popular applications of clustering are:Clustering is an unsupervised machine learning approach, but can it be used to improve the accuracy of supervised machine learning algorithms as well by clustering the data points into similar groups and using these cluster labels as independent variables in the supervised machine learning algorithm? Lets find out.Lets check out the impact of clustering on the accuracy of our model for the classification problem using 3000 observations with 100 predictors of stock data to predicting whether the stock will go up or down using R. This dataset contains 100 independent variables from X1 to X100 representing profile of a stock and one outcome variable Y with two levels : 1 for rise in stock price and -1 for drop in stock price.The dataset is available here : DownloadLets first try applying randomforest without clustering.So, the accuracy we get is 0.45. Now lets create five clusters based on values of independent variables using k-means clustering and reapply randomforest.Whoo! In the above example, even though the final accuracy is poor but clustering has given our model a significant boost from accuracy of 0.45 to slightly above 0.53.This shows that clustering can indeed be helpful for supervised machine learning tasks.In this article, we have discussed what are the various ways of performing clustering. It find applications for unsupervised learning in a large no. of domains. You also saw how you can improve the accuracy of your supervised machine learning algorithm using clustering.Although clustering is easy to implement, you need to take care of some important aspects like treating outliers in your data and making sure each cluster has sufficient population. These aspects of clustering are dealt in great detail in this article.Did you enjoyed reading this article? Do share your views in the comment section below.",https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/
Investigation on handling Structured & Imbalanced Datasets with Deep Learning,Learn everything about Analytics|Introduction|Table of Contents|1. Overview|2. Methods|3. Results|4. Deep MLP Experiments|5. Discussion and Evaluation,"Dataset used|Introduction to KDD Cup 1999|Class Imbalance in KDD Cup 1999 Data Set|Results from the winning entry|Evaluation Metrics|Computational tools|Deep Learning library for the implementation|MNIST Experiment|KDD Cup 1999 dataset pre-processing|Details of MLP Experiments on KDD Cup 1999 data|Standard one hidden layer MLP for KDD Cup 1999 data|Experiments to deal with class imbalance|Cost-sensitive learning in MLP for KDD Cup 1999 data|Standard two hidden layers MLP for KDD Cup 1999 data|Standard three hidden layers MLP for KDD Cup 1999 data|Standard four hidden layers MLP for KDD Cup 1999 data|Cost-sensitive learning in four hidden layers MLP for KDD Cup 1999 data|Cost-sensitive learning with dropout in four hidden layers MLP for KDD Cup 1999|Endnotes|About the Author|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|An Introduction to Clustering and different methods of clustering|Complete Study of Factors Contributing to Air Pollution|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"While Deep Learning has shown remarkable success in the area of unstructured data like image classification, text analysis and speech recognition, there is very little literature on Deep Learning performed on structured / relational data. This investigation also focuses on applying Deep Learning on structured data because we are generally more comfortable with structured data than unstructured data.After extensive investigations, it does seem that Deep Learning has the potential to do well in the area of structured data. We investigate class imbalance as it is a challenging problem for anomaly detection. In this report, Deep Multilayer Perceptron (MLP) was implemented using Theano in Python and experiments were conducted to explore the effectiveness of hyper-parameters.It was seen that increasing the depth of the neural network helped in detecting minority classes. Cost-sensitive learning technique was also observed to work quite well to deal with the class imbalance. We conclude that adding dropout where feature complexity was relatively higher (KDD 1999 dataset that we have used) does not seem to give any improvement.1.Overview2. Methods3. Results4.Deep MLP Experiments5.Discussion and EvaluationA well-known and deeply studied dataset was chosen for this article to focus on understanding and implementing Deep Learning techniques rather than data pre-preparation. This data set does come with its fair share of warning though. It must not be used for IT security and intruder detection by IT security experts but, there is no harm in using it to show a concept like we do here by Deep Learning classification.Knowledge Discovery and Data Mining (KDD) is an international platform that organizes data mining competitions among academics researchers, and commercial entities. In 1999, there was a KDD cup competition related to intrusion detection. Since then, KDD cup 1999 has become the most widely used dataset for the evaluation of intrusion detection system that detects intruding attacks seen as anomalies. In this article, KDD Cup 1999 dataset is used to build a Deep Learning model that can distinguish between and classify good connections and bad connections. The attacks fall into four main classes:10% of the KDD Cup 1999 dataset consists of around 0.5 million samples in the training set and around 0.3 million samples in the test set. The distribution of the training set and test set is different. It is because there are some new attacks included in the test set that are not included in the training set making the problem challenging.There are hundreds of papers available, applying various machine learning algorithms on KDD Cup 1999 data. In a Deep Belief Net (DBN) pre-trained by three or more layers by Restricted Boltzmann Machine (RBM) proved to perform better than the Multilayer Perceptron (MLP) with one hidden layer and a support vector machine. In another paper twenty classifiers were tested on the KDD intrusion dataset achieving prediction performance in the range of 45.67% to 92.81% with random forest classifier achieving the best results. This shows that there is huge interest in this classification problem.In the machine learning literature, it has been pointed out that little work has been done in the area of classification by machine learning when there is a highly skewed distribution of the class labels in the data set. In many cases, a classifier tends to be biased towards the majority class resulting in poor classification rates on minority classes. As we can see in the training and test class distribution of the KDD cup 1999 data U2R and R2L attacks constitute 0.24% of the training dataset but these attacks take up 5.27% in the test data.Below is the class distribution table of KDD Cup 1999:
Below is the class distribution of the training set:Below is the class distribution of the test set.It is important to note that the test data is not from the same probability distribution as the training data, and it includes specific attack types not in the training data. This makes the task more realistic. Some intrusion experts believe that most novel attacks are variants of known attacks and the signature of known attacks can be sufficient to catch novel variants. The datasets contain a total of 24training attack types, with an additional 14 types in the test data only.The winning entry of the KDD Cup 1999 is set as the benchmark for the projects experimental results of KDD Cup 1999. The winning entry was submitted by Dr. Bernhard Pfahringer of the Austrian Research Institute for Artificial Intelligence using C5.0 decision tree classifier giving the benchmark for the comparison of our proposed machine learning algorithm. The winning entry achieved an average cost of 0.2331 per test example with the following confusion matrix:
The last row represents the recall rate and the last column represents the precision. The main issue is that the recall rate of 8.4% for the last class by the winning entry is quite low. The winning entrys classification technique is C5.0 Decision trees, a mixture of boosting and bagging, taking into account the minimization of the so-called conditional risk which is a similar approach as cost-sensitivity (introduced later). Bagging decision trees are random forests that represent an ensemble of decision trees averaged while training on different parts of the training set by sampling with replacement. Boosting is an iterative procedure used to adaptively vary the training sets distribution in order for the base classifiers to focus on examples that are hard to classify. In boosting, weights are assigned to the data in such a way that examples that are misclassified gain weight and examples that are classified correctly lose weight. Below is the cost matrix that will indicate of the cost of misclassifying between the various class labels.There are number of ways to evaluate the performance of a classifier:Recall is the fraction of relevant instances that are classified.Precision is the fraction of classified instances that are relevant.where tp : true positive which is the number of classes correctly predicted as belonging to the positive classfp: false positive which is the number of classes incorrectly predicted as belonging to the positive classfn: false negative which is the number of classes which were not predicted as belonging to the positive class but should have been.Confusion matrix also known as contingency table, is a table with rows and columns that reports the true positive, false positives, false negatives and true negatives as depicted in thefigure below.F-1 score is the weighted average of the precision and recall that lies between 0 and 1. If the equal weighting is given to the precision and recall then the following formula is used;where corresponds to the relative importance of precision over recall. Instead of F1-score the average cost per test example is considered as an evaluation metric for the overall performance of the various learning algorithm techniques throughout this report.When it comes to evaluating the performance of the classifier, it is better to rely on the precision, recall rates and F1-scores rather than accuracy levels. Lets say we have a data set with a (0.9, 0.1) class distribution, there is the possibility that a classifier predicts everything as amajor class and ignores the minor class. In that case, we get an accuracy level of 90% turning out to be apoor indicator of the performance of the classifier. There are better measures of performance of the techniques like the confusion matrix, recall (sensitivity), precision and F1-score.Python is the primary programming language used in this project. Python has a lot of libraries that can be used for data manipulation and analysis. Scikit-learn is the most popular machine learning Python library that offers a variety of algorithms along with utilities for calculating confusion matrices, accuracy levels, recall and precision tables to evaluate the performance of a learning algorithm. A Python dictionary function was used to store the results of the network and the Python pickle function was used to retrieve stored results.Python libraries like NumPy and Pandas were used extensively for data manipulation. A NumPy random seed is used to make sure that for every run the results are reproducible. Since the weights and biases are initialised randomly following a normal distribution, NumPy random seed is used so that the weights and biases initialised are the same in every run. Finally, the Matplotlib library was used for plotting. String formatting has been used to get separate graphs for different sets of parameters in the network.Theano is the Deep Learning Python library that has been used in this project. Introduced as a CPU and GPU compiler by Bergstra at the Lisa lab of the University of Montreal in 2010, it allows defining, optimizing, and evaluating mathematical expressions involving multi-dimensional arrays efficiently.Below are some of the appealing features of Theano:Import theano.tensor as TImport numpy as npT.dot(x,W_x) and np.dot(x,W_x) Both implement the same function, the difference between the two is that NumPy uses numeric variables while Theano uses symbolic variables.Theano is used for symbolic mathematical expressions that are compiled to a function that can operate on numerical data. For example,x = T.vector(""x"")y = T.vector(""y"")fn = x*yp = theano.function(inputs=[x, y], outputs=fn)We can assign variables x and y using NumPy arrays to the numerical data of these variables which can then be used by compiled function like above. To update a variable used in an equation (for example, while learning), Theano needs it to be in a special wrapper called a shared variable. Below are the model parameters for the first hidden layer in feedforward neural networks.W_x = theano.shared(W_x, name=""W_x"")b_h = theano.shared(b_h, name=""b_h"")[global]device=gpufloatX=float32Operations on data of type float32 are accelerated along with matrix multiplication and large element-wise operations especially when the arguments are large enough. The use of GPUs gives around 40x speedup over the use of CPUs particularly in larger networks. This feature of GPUs is one of the reasons for the revival and success of Deep Learning in the twenty-first century.Theano has a large community that has been very helpful in developing the codes and assisting in their use. Error messages produced by Theano are quite different from the error produced by the standard Python packages because Theano codes are compiled. Therefore at times it had been difficult to understand the error message and debug the error.In order to confirm that if our implementation of dropout is correct we decided to verify it on the well-known MNIST handwritten image dataset.The famous MNIST handwritten digits image containing dataset digits 0 to 9 has been used for the experiments with 50,000 training examples, 10,000 validation examples and 10,000 testing examples. An image is represented as a 1-dimensional array of 784 (28 x 28) float values between 0 and 1 (0 for black and 1 for white). There is no need for pre-processing and formatting the data. MNIST dataset is one of the most well studied datasets in the area of computer vision and machine learning. It is based on two data sets collected by NIST, the United States National Institute of Standards and Technology. The NIST data sets were stripped down and put into a more convenient format by Yann LeCun, Corinna Cortes, and Christopher J. C. Burges.An initial experiment was conducted to see the effect of dropout with a learning rate of 0.1 and no momentum. The figures below show the comparison.The experimental result of 2 hidden layers standard MLP of 800 hidden nodes with 3000 epochsThe experimental result of dropout on 2 hidden layers MLP of 800 hidden nodes and 8000 epochs as dropout made the learning of the network slower. Hence larger number of epochs is required.As can be seen in the figure above there is a fluctuation in the training cost value (obtained by the cost function), and accuracies created by the dropout noise. Moreover it turns out that problem of local minima in the error surface is encountered in the dropout network (figure abovedepicts error surface with local minima). That is cost value plateaus at the higher value than at the value without using dropout, making the slow learning process for the network. Training cost value without using dropout is 0.083 while using dropout is 0.225 suggesting problem of local minima at the end of epoch of 3000 and 8000 respectively. Varying learning rate and momentum rate helps to solve the problem of local minima. It is little wonder that Hinton used various techniques to make dropout work.Depicts an error surface with poor local minimaHinton experiment results on MNISTHinton starts off his experiment with learning rate of 10 and exponentially decays the rate at 0.998 for each training epoch and then stabilizes the learning rate at 500 epochs. In order to prevent the model from blowing up, due to the very high learning rate, he puts an upper bound constraint on the squared length that is L2 norm of the incoming weight vector for each individual hidden unit. After cross-validation it turns out that maximum squared length of l = 15 give the best results. If a weight-update goes beyond the upper bound, weights are renormalized accordingly. Similarly he initially set the momentum rate at 0.5 and linearly increases to 0.99 over the first 500 epochs speeding up the training, in the subsequent epoch momentum rate stays constant at 0.99. We had to use 8000 epochs because learning is significantly slower with dropout. As per Hinton, adding dropout in the input layer with 20% random noise improves the result by quite a large margin. Apart from these hyper parameters, in the network architecture he uses stochastic gradient descent as an optimizer with mini-batches of 100 and a cross-entropy objective function.In order to combat the problem of local minima we used some of the techniques of varying the learning rate and momentum over the epochs. The experiment started off at a learning rate of 0.1 and momentum of 0.2. The learning rate increased stepwise at 500, 1500, 3000 epochs decreasing to 0.08, 0.06 and 0.04 respectively at the same time momentum rate increased to 0.4, 0.6 and 0.8 at the same time. The testing accuracy level increased to 96.65% by varying learning rate and momentum as mentioned. The figurebelow shows the performance of the described networks.Table below is the comparison of our three experiments on the MNIST dataset. In the table test accuracy levels of three experiments can be seen. The number of epochs required to get the training costs were different for all three experiments.
It can be seen that at various training cost levels experiment B result is better than experiment As. It took a network without dropout only 10 epochs to reach training cost of 1.310, whereas around 110 epochs were required to get the same training costs and thus better accuracy level by a network with dropout. Interestingly if learning rate and momentum are varied as in experiment C, not only accuracy level increases but also it takes fewer epochs, around 95 epochs, boosting the overall performance.Later, we did the same experiment with 100-sized mini-batches along with the same architectures and parameters as before. It turns out that the mini-batches made the learning easier as it started off (in the initial epochs) with the higher training and testing accuracy levels and learning also converged quicker and at the higher level with testing accuracy level of 98.6%, getting closer to the benchmark.It is very important to get data ready before doing any analysis. The treatment of categorical and continuous variables is quite different. Categorical variables are converted by one-hot encoding. In this encoding, a feature is obtained for each categorical variable in which one neuron/input represents each category, for example the following is the subset of two features in KDD Cup 1999 data set.Sample with 2 features of KDD 99Table showing one-hot encoding of categorical samplesThe first column of figure gives the binary representation of (tcp / not tcp) encoded as (0, 1) respectively, where tcp is represented in the above diagram as 1 and whichever example is not tcp is represented as 0. Similarly, the second column of figure above gives the representation of (icmp / not icmp). When all the variables are encoded from the first column of the categorical feature, the feature variables from the second feature type will be encoded in the same manner. As a result of one-hot encoding the input size has expanded from 42 units to 120 units.Continuous features (attributes) have been transformed using the following rescaling;, where x is the value of the featureOne reason for scaling the data is to ensure that all variables are seen as equally important by smoothing out feature to feature variations. For example if feature A ranges between 1 and 100 while feature B ranges between 0.1 and 0.001, the network will assign smaller weights to feature A and larger weights to feature B. By rescaling all the features are brought to the same scale. Neural networks learn faster and give better result with pre-processed input variables as a result. All the samples in the dataset have been shuffled to ensure that if mini-batches are used then the training examples in the mini-batch will not necessarily have the same order of training sets in each mini-batch. Data shuffling randomizes the order of the data. However, instead of mini-batch training full gradient descent was carried out throughout this project.In the raw data there was various attack types that were categorize into four main attack classes by following to compare with the winning entrys method.One of the challenging tasks in neural networks is to find the optimal hyper-parameters to get the best performance. It is always difficult to tell which combination of parameters works well. In the end these are all still heuristics most of the time. In the neural networks the parameter space is huge and it is difficult to explore each hyper parameter extensively. The experiments started with one hidden layer along with the hyper-parameter tuning. Hidden layers were then stacked and evaluated. Four hidden layers were the maximum that we have stacked and reported the results of. Moreover some techniques mentioned in the literature review were incorporated in to the learning algorithm like SMOTE, dropout and cost-sensitive learning. The experiments were conducted in full batch mode of back-propagation learning that is weights updated after all training examples were presented to the learning algorithm in each epoch. The cost is computed and parameters are updated to minimize the cost value. At the time of prediction of unseen examples the final updated parameters are used and there are no parameter updates.Weights and bias are initialised using Python NumPy arrays. Weights are initialised using random standard normal distribution with mean zero and standard deviation of 0.05. Biases are initialised with values of zero.One hidden layer MLP implementation in Theano was taken from one of the study and from there we worked all the way and changed the code to incorporate various other techniques. In the implementation one more hidden layer is stacked by initialising weights and biases the same way as in one hidden layer MLP. Same goes on for three and four hidden layer MLPs given in the subsequent sections of the report.Theano codes for momentum, dropout and rectified linear units were fetched from the online Theano user community. The value of the momentum has to be multiplied by the parameter update of previous epoch to get the value of parameter updates of current epoch. To implement rectified linear units Theanos maximum function was used so that the value of the hidden layer stays non-negative resulting in the sparsity. Theano already had a module for tangent activation functions. In the implementation of dropout, Theano Bernoulli function is multiplied by each hidden layer function in order to drop the weights and biases with probability p at the time of training. For the test using unseen data separate hidden layers are written with the same value of initialised/ updated weights and biases being used but this time weights are scaled down by probability of .SMOTE has been implemented by using Costcla Python library which is built on top of Scikit- learn, Pandas and NumPy. Cost-sensitive learning classification has been implemented in detailed in the further article.The vanishing gradient problem was solved by using Rectified Linear Units as activation functions as these introduce sparsity effect on the networks. In order to evaluate the results, confusion matrix, recall rate, precision and cost per test sample have been considered because of the class imbalance problem. Accuracy levels of training and testing set have also been calculated to see when the network actually over-fits. Cross-validation does not seem to be an appropriate way to evaluate the performance and get the optimal results. It is because the distribution of the training set and test set is quite different. Cross-validation uses one subset of training data for learning and another subset of training data is used for validating. We used the subset of the training set for learning and the remaining subset of the training set for validating. It turned out that accuracy level of the validation set (subset of training data) was very high (higher than the accuracy level of the test set) confirming that the distribution of training and test set is quite different making the problem challenging.All the experiments were done on CPU (not GPU). Some of the experiments took days to finish. As the network was grown bigger, computation became time consuming task. The four hidden layer MLP network with dropout was the largest network experimented in this project and it took around four days to finish that experiment with no hyper-parameter tuning which is described in section 4.3.5. Tuning hyper-parameters of the networks was the most time consuming part of the exercise. Three and four hidden layers MLP usually took a day to finish running with each set of hyper-parameters.Initially a single hidden layer MLP was used in the experiments to get the optimal configurations and architecture. Description of the hyper-parameter search is given as follows.At the time of hyper-parameter tuning, a learning rate of 0.8 was chosen among the arbitrary set of 0.01, 0.1, 0.5, 0.8, 0.9, 1.0 and 1.2. Momentum of 0.8 was also chosen after hyper-parameter search. It is surprising to see a large learning rate along with large momentum to work well together because it is general practice to use a lower learning rate along with a large momentum to ensure that training cost oscillates smoothly down in the error surface while learning. However, since we are using full batch gradient descent a higher learning rate seems reasonable. Using an activation function of rectified linear units turns out to be better than the tangent units. Cost function of Negative Log-Likelihood (NLL) is chosen because of the softmax function in the output layer. It does not mean that NLL is the only cost function that can be used when softmax is in the output layer. Cost per test sample turns out to be 0.2472 by the standard single hidden layer MLP after hyper-parameter tuning. Cost per test sample is calculated by multiplying the confusion matrix given below and cost matrix given in section 1.3.3 and then divided by the total number of test examples which is around 0.3 million. Below are the graphical and tabular results of the network.Figure shows the cost, and training and test accuracies levels of single hidden layer MLP after all the hyper-parameter selectionsFigure shows the recall curves of all the five classes after all the hyper-parameter selectionsFigure shows the precision curves of all the five classes after all the hyper-parameter selectionsConfusion Matrix by the last 1000 epoch:Recall by the last 1000 epoch:Precision by the last 1000 epoch:In figure above,we can see that there is a gap between the training and test accuracy. This gap has been observed in all the experiments of the project. Training accuracy has been around 99% and test accuracy has been around 92%. This gap is due to the different distribution of training and test datasets. When portion of the training set was taken as the validation set, that same gap was not observed.The figures gives out the recall and precision results respectively. As we can see the performance of the minority class of 4 and 5 is quite low. Hence we tried several approaches to deal with the class imbalance problem. After getting the results of one hidden layer of MLP, the objective of this project is to get the better results.SMOTE-MLP for KDD Cup 1999 dataSMOTE is a technique to oversample the minority class by creating synthetic examples of minority class. SMOTE was used to increase the samples of minority class of U2R and Probe to 0.83% which is the proportion of R2L attack type from the proportion of 0.01% and 0.23% respectively. After the pre-processing step of oversampling the minority class along with data pre-processing step described in section 3.3 the processed KDD Cup 1999 data is tested on the single hidden layer MLP with the same architecture that is after hyper-parameter tuning.The overall performance of the network on the data pre-processed by SMOTE has declined as can be seen in figure precision and figure recall. The lower performance of the network can be explained by the fact that the synthetic examples created by SMOTE makes them very similar to the majority attacks in the feature space. As a result the network is confusing between the minority class with the synthetic examples and the majority classes.Figure shows the precision curves of all the classes by the one hidden layer MLP with SMOTEFigure shows the recall curves of all the classes by the one hidden layer MLP with SMOTE.Recall at the end of 1000th epoch:
Precision at the end of 1000th epoch:Confusion matrix at the end of 1000th epochIt can be seen from the above tabular results, the performance of the minority classes has decreased. One possible way to improve the results is to try tuning the parameter. If we refer to previous section on SMOTE, the difference between the data generated by K-nearest neighbour and the original point is multiplied by a random number in the range 0 to 1. It will be interesting to see if we vary the range of this random number. Lets say that this random number instead belong in the range of 0 to 0.5. If the minority classes with the synthetic samples stay indistinguishable from the majority attack classes, there might be an improvement.There are three methods for the cost-sensitive learning of neural networks. The first method, cost-sensitive classification, was implemented into the single hidden layer MLP with the same architecture that is after hyper-parameter tuning. The cost matrix is given in section 1.3.3 where the cost of misclassifying the R2L as normal is 4 and similarly, the cost of misclassifying the normal as R2L is 2. P(i) is an estimate of the prior probability that an example belongs to the i-th class. The following table is the proportion of the training data set considered as P(i)in the computation.While testing there is a function in the implementation Theano code that selects the class index with the highest index value and sets that class index value to 1 and assigns 0 to the rest of the class. Therefore, there is no need to normalize P'(i) with denominator of .The cost vector represents the expected cost of misclassifying an example that belongs to the ith class.Recall at the end of 1000th epoch:Precision at the end of 1000th epochAs a result of cost-sensitive learning in the neural networks the correct classification of minority class 5 has increased from 936 to 953 increasing the recall rate to 5.83% at the 1000th epoch. This shows that given the appropriate misclassification cost table, our learning algorithm can adjust so that expected cost of misclassification for various classes goes down.Below is the table of the comparison of test accuracy of one hidden layer MLP and one hidden layer MLP with cost-sensitive learning. As can be seen there is an overfitting by MLP with cost-sensitive learning where test accuracy declines after 750 epochs while training accuracy improves because it is supposed to stay intact in the cost-sensitive learning procedure. On the other hand, one hidden layer MLPs test accuracy remains the same. Trade-off for cost-sensitive classification is that since the modified network is biased towards classifying the minority class that carries higher misclassification costs, the classification performance for the majority class may go down as it is can be seen in the below table.One of the popular ways neural network models deal with over-fitting is to use an early stopping rule. Training of cost-sensitive learning neural network can be halted after around 750 epochs which is when model starts to over-fit.As more hidden layers are stacked it became apparent that the distribution of the number of hidden nodes needs to be changed as well. If more hidden layers are added then it is not necessary to have 175 hidden nodes in the first hidden layer. There is a distributed representation of the feature learning by the hidden nodes in the multiple hidden layers of the neural networks. Same configuration has been used as for the one hidden layer MLP apart from the hidden nodes. After hyper-parameter search from the following set of hidden nodes (175, 85), (175, 175), (100, 75), (75, 75), (75, 50), (100, 50), (75, 100), (100, 100) and (175, 150)175 and 150 hidden nodes have been chosen for first and second hidden layers respectively for two hidden layers MLP. Single hidden layer MLP with 175 hidden nodes gives a better result than two hidden layer MLP. Cost per test sample is 0.2507. Below is the confusion matrix, precision and recall of the test set:Precision at 1000th epoch:Recall at 1000th epoch:There seems to be no improvement in the performance of the two hidden layer MLP. Below is the performance of accuracy level over the 1000 epochs:Set of (100, 50, 75) were chosen for nodes of three hidden layers from the sets (100,50,35), (100,50,75), (100,75,35), (100,75,75), (175,150,50),(175,100,50) after hyper-parameter tuning. Architecture is same as multilayer perceptron with one or two hidden layers. It would have been interesting to see the effect of changing the remaining parameters like learning rate, momentum, etc on deeper network experimental results. However due to time-constraints it was not possible to carry that out.Interestingly this network performed quite well on the class 4 (minority). And cost per test data sample is 0.2466 which is better than the single hidden layer MLP. Below is the confusion matrix, precision recall and accuracy tables;Precision at the end 2000th epoch:Recall at the end of 2000th epoch:It can be seen that the precision and recall for minority class have improved. Number of epochs varies with the number of hidden layers. This is because it takes more time for the network to detect the minority classes and for the training cost to converge. Interestingly, training accuracy improves quicker as more hidden layers are stacked.One of the objectives of this article is to see the performance of dropout. In this experiment higher number of nodes is tested with the intention to use dropout (later with the same setup) to regularize and avoid over-fitting in the network. This was done deliberately to ensure that the network is expressive enough to benefit from the dropout. In Hinton said that if the network does not over-fit, make it bigger. In the three hidden layers MLP there seemed to be insignificant overfitting over 2000 epochs. Applying Hintons advice, one more hidden layer was stacked and this time added more hidden nodes across the hidden layers. Number of hidden nodes that is used was as follows 375, 200, 150, 75 for first, second, third and fourth hidden layers respectively. Table below is the performance of four hidden layers MLP.
Cost per test sample is 0.2466. It seems like as more hidden layers are stacked the cost per test sample seems to go down. Over-fitting is seen after 1500 epochs as test accuracy goes down while training accuracy stays around the same.Confusion matrix at the end of 3000th epoch:
Precision at the end of 3000th epochRecall at the end of 3000th epoch:Interestingly both training and test accuracy levels of four hidden layers MLP are higher than single hidden layer MLPs. The performance of the minority classes has also improved as depicted by above tables. Now cost-sensitive learning classification will be integrated to boost the performance of the minority classes.Cost-sensitive classification method was used here as well. This technique is incorporated into four hidden layer MLP. Table below is the performance of four hidden layers MLP with cost sensitive classification:Cost per test sample is 0.2424 and test accuracy of 92.64% was achieved at the end of 1500 epoch. These values have been the best that had been achieved throughout this project.Confusion matrix at the 1500th epoch:Precision at the 1500th epoch:Recall at the 1500th epoch:It is interesting that the neural networks for the class imbalance problem improves when more hidden layers are stacked, that is, if we build deeper networks, the performance on the minority classes gets better than with a shallower network. Throughout the iteration, the maximum correct number of classifications of minority class 5 has been 964, boosted to around 1000 correct when cost-sensitivity in the classification was incorporated. The performance of the minority class worsens after 1300 epoch, and after 1500 epochs the performance of the overall test accuracy starts to decline. Below is the graph of the recall for the classes.Dropout used in cost-sensitive learning in four hidden layer MLP for KDD Cup 1999 dataFinally, dropout regularisation is used with a cost-sensitive learning four hidden layer MLP to combat the over-fitting and investigate if the network performs. 2.5% of the noise level was added into the network by dropout. 2.5% is chosen because the use of dropout regularisation makes the learning a lot slower and we varied learning rate and momentum. In the beginning of this experiment, learning rate of 0.8 and momentum of 0.2 was used. Then learning rate changed to 0.6, 0.2 and 0.05 stepwise, momentum changed to 0.4, 0.8 and 0.85 stepwise at the epoch of 500, 1000 and 1500 respectively. Below are the performance graphs of the network. Spikes and fluctuations are seen in the training cost curve, precision and recall curves particularly for the minority classes in figures. This gives less confidence to use dropout in the network to make predictions.Figure shows the cost, and training and test accuracies levels using Deep MLP network with dropout of 2.5%Figure shows the precision curves of all the classes using Deep MLP network with dropout of 2.5%Figure shows the recall curves of all the classes using Deep MLP network with dropout of 2.5%Cost per test sample is 0.2477; it is calculated by the multiplying confusion matrix given below with cost matrix. Below are the confusion matrix, recall and precision tables;PrecisionRecallThere are other regularisers used in neural networks like weight decay L2 that deals with the overf-fitting. It will be interesting to see the comparison of dropout with other regularisers. Using dropout does not seem to give any advantage.This article addresses the problem of anomaly detection through proper classification. SMOTE, sampling approach to deal with the class imbalance problem can do well if features of the different classes are distinguishable. It will be interesting to see the performance of the data resampling by SMOTE on the binary classification to distinguish between the normal and intrusion connections in the KDD cup 1999 data.Cost-sensitive learning of neural networks seems to be effective in dealing with the class imbalance. The overall accuracy of 92.64% turns out to be the best result achieved by us by cost-sensitive classification of deep four hidden layer neural network. In another research by Kukar et al, Adaptive learning rate and minimization of the misclassification costs were concluded to be more effective than the cost-sensitive classification (the method that is actually implemented in this project is cost-sensitive classification). It will be interesting to see the performance of the network with those two other cost-sensitive learning methods of adaptive learning rate and minimization of the misclassification costs.Not only can cost-sensitive learning be adopted in network intrusion and classification problem but also in various other areas like in healthcare applications. For example the cost of misclassifying if a person is not likely to have a disease when they actually are likely to have one is more than the cost of misclassifying a person who is likely to have a disease but actually does not. We have applied cost sensitive learning here (like they do in healthcare) because here, like healthcare too, the cost of a false negative or misclassification can be huge for false positives and spark mass litigations in cyber breaches. Having to deal with a bit more false positives makes it worth it to have lower false negatives.Cost-sensitive learning seemingly has worked quite well in most of the research projects when dealing with the class imbalance problem. In the research by Dalyac the cost function called Bayesian cross entropy is introduced which tackles the problem of the image dataset using deep convolutional neural network. Bayesian cross entropy is simple modification of the equation of the cross entropy. In the Bayesian cross entropy, instead of assigning identical probability distribution to all the classes while trying to maximize the joint probability of occurrence, higher probability distribution to the minority class is assigned. This idea is very similar to the cost-sensitive learning. Below is the Bayesian cross entropy equation:, where is the number of classes.It turned out implementing any non-standard technique requires lot of work. If we had relied on a Deep Learning library like Pylearn2, the advantage would have been that we could have explored more techniques like Maxout, weight decay, etc. without worrying so much about the implementation, but the disadvantage would have been that we would not have had understood the implementation of techniques like Dropout extensively and may not have been able to implement highly customised changes.While Deep Learning has shown remarkable success in the area of unstructured data like image classification, text analysis and speech recognition, there is very little literature on Deep Learning performed on structured/relational data which makes this investigation intriguing. After the extensive investigation carried out in this project, it does seem that Deep Learning has the potential to do well in the area of structured data. In Deep Belief Networks have performed well (better than support vector machine and single hidden layer MLP) on the KDD Cup 1999 data with class imbalance. In this project it has been observed that if more hidden layers are stacked the performance on the minority class improves to some extent.Theano is particularly challenging as it is difficult to debug errors since Theano uses compiled functions. It would have been nicer, cleaner and avoided a lot of repetitive code if we had implemented the code in Python as an Object Oriented Programming (OOP) language. OOP can scale quite nicely as the program complexity grows. This is the approach of Pylearn2, Keras and other Deep Learning libraries.Aside from pylearn2, tensor flow and H20 are also good alternatives. H20 can be used for Deep Learning in both Python and R. H20 has scalable, fast Deep Learning using mostly on the feedforward architecture. When using momentum updates for momentum training, H20 recommends using the Nesterov accelerated gradient method, which uses the nesterov accelerated gradient parameter.It seems that if we are dealing with class imbalance problems then it is best to focus on the techniques that are specifically dealing with the class imbalance problem. On the other hand, Deep Learning shows the potential to optimise the results by that technique of stacking more hidden layers on the neural network.Hinton has explicitly stated that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology. All these tasks have unstructured data. This questions if dropout actually works on the structured data like KDD 1999 Cup. Further experiments on structured data are required to solidify this tentative doubt.An alternative to our method of using Multi-layer Perceptron can be utilizing Encoder-Decoder networks for structured data instead. Encoder-Decoder is a framework that aims to map highly structured input to highly structured output. This framework has been applied recently in machine translation where one language is translated by machine to another.An alternative for handling class imbalance problem is using Deep Coding networks as they change parameters as context of data changes on another dataset (preferably not KDD 1999). This is very important for fraud and intrusion as fraudsters are continuously scheming new contexts and ways to deceive.This project has shown that Deep Learning can be a powerful classification technique using categorical and continuous valued of the type typically collected by business processes. The expressive power of even relatively small neural networks has been seen in their ability to closely fit the large volume of training data. Deep Learning is clearly a powerful technique, and businesses may find many applications for it. However, it has also become clear through this project that considerable experience is required to get good results from Deep Learning. There are many architectures, techniques, and hyper-parameters that need to be carefully chosen before a model performs well on unseen data.Companies can gain much from applying Deep Learning techniques in many areas, such as proper classification and other methods mentioned in the previous sections. Many techniques are still recent and until the foundations of Deep Learning are better understood, or more general deep models are developed, businesses must be prepared to invest in gaining experience and a certain amount of trial and error should be carried out before usable results are obtained.We hope this article was a great value add for you. Tell us in the comments below if you found this study helpful or not. If you have any questions whatsoever, we are happy to answer them. Just post your questions in comments sections.For all the deep learning practinors, if you have worked on KDD dataset, share your experience with us and what approach you followed. Now time to explore more. Go start your search now.",https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/
Complete Study of Factors Contributing to Air Pollution,Learn everything about Analytics|Introduction|Table of Contents|1.Overview|2. Data Description and Preparation|3. Exploratory Data Analysis|4. Predictive Model Development|5. ODD-EVEN Campaign|End Notes,"Problem Statement|Objective and Scope of the Project||Data Source|Tools & Techniques|Limitations|Data Management|Data Quality|Data Preparation|Analyzing the impact of Vehicle Density & Vehicle Population|Insights|Identifying Patterns forAir Pollution in New Delhi|Histogram for Various Pollutants|Box Plot for Various Pollutants  All Locations|Summary of Data for KeyVariables for each Location
|Seasonality Analysis|Seasonality Analysis : Conclusion|Correlation Matrix & Analysis: Anand Vihar|Correlation Matrix: Punjabi Bagh|Correlation Matrix: R. K. Puram|Multiple Linear Regression Model (MLR) & Neural Network Model (NN)|Model Fit Graphs|Model Validation|Predictive Model Development Conclusions|Average Pollutant Level Analysis|PM 2.5 & PM 10 Levels during Phase 2|ODD-EVEN Impact on Traffic (Cars)|Impact of Bio Mass Residual Burning on ODD-EVEN Campaign:|Quantifying the Bio Mass Burning in India|Text Mining of Tweets for Odd-Even Phase-II (April 15th 2016  April30th 2016) for Sentiment Analysis|Analysis of tweets|Analysis Results|Conclusions: Odd-Even Campaign|Recommendations:|About the Authors|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Investigation on handling Structured & Imbalanced Datasets with Deep Learning|18 New Must Read Books for Data Scientists on R and Python|
Guest Blog
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1. Objective|   2. Scope|  3. Out of Scope:|Analytics approach|We plan to use the following Seven Step Analytical Approach for the Project.|Variables Transformation|Missing values and Outliers,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The air pollution is one of the main causes of death in the world. Several cities are on the radar of WHO, which are about to touch the dangerous level. Sadly, India is one of the countries with maximum number of most polluted cities in the world.Especially, on the onset of Diwali, the air qualityindex of DelhiNCR soars to new heights. This year the air quality index has already crossed last years post Diwali index.To know the intricacies of the problem, we decided to do an analytical study for the factors that contribute most to air pollution in New Delhi.In this article, we share a case study on Identifying Patterns in New Delhis Air Pollution, in which we closely studied the air quality data for New Delhi, identified patterns, factors that lead to rise in air pollution across three key locations in New Delhi. The article also includes, impact of the Delhi Governments initiative Odd-Even Pilot Project Phase II to tackle the problem of air pollution.On this occasion ofDiwali, we want to sensitize the readers towards celebratingenvironmentally safe Diwali this year.1. Overview2. Data Description and Preparation3. Exploratory Data Analysis4. Predictive Model Development5. Odd-Even CampaignThe rate at which urban air pollution has grown across India is alarming. A vast majority of cities are caught in the toxic web as air quality fails to meet health-based standards. Almost all cities are reeling under severe particulate pollution while newer pollutants like oxides of nitrogen and air toxics have begun to add to the public health challenge.According to WHO, India ranks among the worlds most polluted countries. Out of the 20 most polluted cities in the world, 13 are in India. In which, Delhi is the most polluted city in the world today.                        Figure: Chart showing the Air Quality Index for Beijing and New Delhi for a 4 Month periodExposure to particulate matter for a long time can lead to respiratory and cardiovascular diseases such as asthma, bronchitis, lung cancer and heart attack. Last year, the Global Burden of Disease study pinned outdoor air pollution as the fifth largest killer in India, after high blood pressure, indoor air pollution, tobacco smoking, and poor nutrition. In 2010, about 620,000 early deaths in India occurred from air pollution-related diseases. The Central Pollution Control Board (CPCB) sponsoredthe study that links the pollutants, pm 10 (particulate matter smaller than 10 microns), the cause of thesediseases. The central regulatory authority recently regulatedstricter norms for a number of air toxins and pollutants but omitted revision of the standard for pm 10.Figure: Chart showing Top 20 polluted cities in the G-20 Countries in terms of annual mean PM10Sunita Narain (Director General) Centre for Science and Environment (CSE) says, This data confirms our worst fears about how hazardous air pollution is in our region. In addition to this, Narain points out, 18 million years of healthy lives are lost due to illness burden that enhances the economic cost of pollution. Half of these deaths have been caused by ischemic heart disease triggered by exposure to air pollution and the rest due to stroke, chronic obstructive pulmonary disease, lower respiratory track infection and lung cancer.We feel, if we closely study the Air Quality Data, we should be able to identify patterns (spike in air pollution levels) and identify correlating factors on key levels of Air Pollution across New Delhi. Also as part of the exercise, we wanted to study the impact of Government sponsored Initiatives like Odd-Even Pilot Project Phase II. The Phase I of the Odd- Even experiment was a huge success in terms of people compliance and reduction of traffic congestion, it had very little impact on the Air Pollution levels during the Campaign period.It is also important to understand the behaviour of meteorological parameters in the planetary boundary layer because, atmosphere is the medium in which air pollutants are transported away from the source, which is governed by the meteorological parameters such as atmospheric wind speed, wind direction, and temperature.Air pollutants are being let out into the atmosphere from a variety of sources, and the concentration of pollutants in the ambient air depends not only on the quantities that are emitted but also the ability of the atmosphere, either to absorb or disperse these pollutants.There were conflicting reports in media on the actual cause of air pollution in New Delhi. Some sections claimed vehicles as the main source of pollution, while others held road dust & construction debris responsible. But the root cause of the problem is Industrial pollution.Through this study, we hope to develop some insights that can help organizations (State / Central Pollution Control Boards & NGOs) to advocate more stringent policies to control air pollution.The primary objectives of the study are:The data for the Project was obtained from the website of Central Pollution Control Board (CPCB). Currently, CPCB tracks the Air Pollution levels across 23 dimension (variables). Day wise, hour wise (for some variables). Data is available on-line across the following dimensions:Not all monitoring stations track Air Pollution on all the above mentioned parameters and for all days.Indias Central Pollution Control Board now routinely monitors four air pollutants namely Sulphur dioxide (SO2), oxides of nitrogen (NOx), suspended particulate matter (SPM) and respirable particulate matter (PM10) & (PM 2.5). These are target air pollutants for regular monitoring at 308 operating stations in 115 cities/towns in 25 states and 4 Union Territories of India.The monitoring of meteorological parameters such as wind speed and direction, relative humidity and temperature has also been integrated with the monitoring of air quality. The monitoring of these pollutants is carried out for 24 hours (4-hourly sampling for gaseous pollutants and 8-hourly sampling for particulate matter) with a frequency of twice a week, to yield 104 observations in a year.We have used the following Analytical techniques / methodology for analyzing the Data :The Analytical Approach will involve the following (not necessarily in the order) activities:Figure: High Level Process FlowThere are few limitations that this study has w.r.t data and the methodology that can be used.We have extracted data for a year across 23 variables. This was collected for about 4 centres in New Delhi, one centre in Bangalore and one in Chennai. Data was extracted from CPCBs real Time Air Quality data monitoring application that is available on-line. We have also extracted Data for Odd-Even Pilot project (Phase I & II). This data covers 4/5 major pollutant parameters like SO2, NO2, CO, PM2.5 & PM 10. The data covers 15 days prior to The Pilot project and the 15 days after the Pilot project.To derive a more accurate analysis of the pilot project,we have also collected data of social conversations that took place around the Odd-Even experiment (Phase II). We were able to collect nearly 1000 social mentions / conversation around this theme.Table 1: Table showing List of VariablesThe Exploratory Data Analysis is divided into three parts. They are:Analyzing three City Air Pollution Data and check whether the number of vehicles and vehicle density have any impact on air pollution levels:We used simple Graph to plot the Pollutant levels for PM2.5, SO2, NO2 & CO across New Delhi, Bangalore & Chennai. The Average Pollution levels of the Pollutants were mapped on X axis and the Vehicle Density and the number of vehicles were plotted on the Y-axis.Figure: Graph showing Pollution Levels of 3 cities Vs Vehicle Density & Vehicle PopulationVehicle density (measured as vehicles/km of road) does not have any impact on the air pollution. New Delhi has the least vehicle density amongst the three cities we have considered for the study, but the PM 2.5 levels are significantly higher in New Delhi as compared to Bangalore and Chennai. Though Chennai has the highest density of vehicles, but has a lower pollution levels for (PM 2.5)Our secondary research identified the three most polluted areas of New Delhi. They are Anand Vihar, R.K. Puram & Punjabi Bagh.Figure: Chart showing the three most polluted areas of New Delhi.Histogram showing pollutant levels for each of the three locations  Anand Vihar, R.K.Puram & Punjabi BaghThe histogram shows a few key attributes about the distribution of the different pollutants.Fig: Anand ViharFig: R.K.PuramFig: Punjabi BaghFig: Anand Vihar  Graph & Chart showing pollutant levels across seasonsFigure: R.K. Puram  Graph & Chart showing pollutant levels across seasonsFigure: Punjabi Bagh  Graph & Chart showing pollutant levels across seasonsFigure: Correlation Matrix for Anand ViharInsights:Figure: Correlation Matrix for Punjabi BaghInsights:Figure: Correlation Matrix for R.K. PuramInsights:The objective for the Predictive Model Development was to develop a model that can predict the next days level for key pollutants like PM 2.5, PM 10, SO2, CO etc.The Model Development was done at multiple levels to arrive at a most suitable model. At first level we developed two sets of Model using Multi Linear Regression (MLR). The first one with the actual available variables. The second Model (MLR) was developed using one additional variable i.e. Previous Days level for that particular Pollutant (Dependent Variable).Then, at the second level we developed the Model using Neural Network (NN). Once again this was further divided in two parts. First with using all the available variables as they are. The second NN Model was developed using one additional variable i.e. Previous Days level for that particular Pollutant (Dependent Variable).This Model building approach helped us with 4 sets of Model for each of the predictor variables, i.e,Key pollutantsThe data for the modeling was split into two parts train & Test data. The Split of the data is as follows:The following are the details for the Models:Since, the objective is to predict the next days value we have included the previous days level as Multiple Linear Regression which was run on Train data set using R package. Multi Linear Regression Model was used on Metrological variables like wind speed (WS), wind direction (WD), relative humidity (RH), solar radiation (SR) and temperature. The key pollutants like PM 2.5, PM 10, SO2, NO2, CO were kept as Dependent. Variables with low information value & high P-value were dropped. The resulting significant predictors, their p-values and the estimated signs for numeric predictors are shown in tables below.Table showing Anand Vihar Air Pollution Predictive Model ResultsMultiple Linear Regression Model Beta Coefficient TableNeural Network Model Results for w/o Previous Days and with PDsInference:Table showing Punjabi Bagh Air Pollution Predictive Model ResultsMultiple Linear Regression Model with Beta coefficientsNeural Network Model without Previous Days valueInference:Fig:Anand Vihar  Comparative Model Fit graph for PM 2.5Fig: Punjabi Bagh  Comparative Model Fit Graph for PM 2.5Fig: R.K. PURAM  Comparative Model Fit Graph for PM 2.5Relative Importance Variables for the Three LocationsInference:We used Jackknife Validation Method for validating the 4 Models and their relative performanceWe also used Root Mean Square Error (RMSE) Value method to validate and compare the relative performance of the 4 Models that we have developed.We also performed the relative error check to validity of the model. The results of the three validations are presented in thetable below.Inference:Next Steps:Analyzing the impact of the campaign on New Delhis air pollution levelsFor the Odd-Even Campaign Analysis, we have taken 4 locations for consideration. They are:The Key Air Pollutant levels were obtained for the 15 days prior to the Campaign and for the 15 days ofthe campaign period. For purpose of record, these days are:Pre Campaign Period: 1st April 2016 to 14th April 2016Campaign Period: 15th April 2016 to 30th April 2016Fig: Average Pollutant Levels across 4 locationsInsights:Fig: Pollution Level Trend Analysis Graph -All Locations combinedFigure (up) & (down): Graphs showing the PM 10 & PM 2.5 levels before and during Odd-Event Campaign (II)Insights:Fig: Impact on Number of Cars on the RoadInsights: Fig: Picture showing the Bio Mass Burning across North IndiaFig: Picture showing the impact of Bio-Mass burningBio Mass Residual Burning  2008-09  State wiseTable showing the amount of Pollutant generated due to Bio-Mass burning across various States of IndiaAs part of the study Identifying Patterns in New Delhis Air Pollution, text mining of tweets was undertaken to identify the sentiment of people towards Odd-Even Phase-II in New Delhi.Odd-Even rule was levied by the Delhi Government to reduce air pollution in New Delhi. According to the rule, only the cars with odd and even numbers were suppose to run on alternate days. The first trial period of this rule, i.e, Phase-I was applied from 1st January 2016 to 15th January 2016. The second trial period of this rule, i.e, Phase-II was applied from 15th April 2016 to 30th April 2016.During Phase-II the following vehicles were exempted from the rule:  2. ScopeThe document describes the approach to mining of tweets for Odd-Even Phase-II.The data pipeline built for mining of tweets is shown below:The tweets collected were analyzed using R through the following steps:Word Cloud for Tweets CollectedFig: Sentiment Polarity of Tweets over TimeStrick Norms with ALARM SYSTEM FOR Specific Decisive Interventions as illustrated here:Fig: Chart showing Trigger Alarm and corrective actionWe hope this article was an enriching experience for you and provided you enough insights about the factors that can lead torise in air pollution. so, watch out before we contribute to air pollution, knowingly or unknowingly. We thoroughly enjoyed working on this capstone project as part of our PGP-BABI program at Great Lakes.Our mentor guided us throughout and this project provided us immense learning.Here is what our mentor, Mr. Jatinder Bedi, had to say about our Capstone Project, Students wanted to do a typical legacy project in which they wanted to pick a dataset and run various Predictive models on top of it. I suggested them the idea of studying urban air pollution, a topic on which I was already working. I shared my thoughts and they picked up very smartly. The group had great energy to learn and was all willing to explore how concepts can be applied to real-world problems. It was an unsupervised study where we all learnt in every step. The project was a great showcase of how we can apply Analytics as a tool to understand problems around us and further take necessary steps to minimize the effects.Thanks to Dr. PKV for his consistent guidance and support. He has always been a great source of information for us.This article was contributed by Karthikeyan Gnanasekaran, Shrinivasabharathi Balasubramanian, Sankaranarayanan Mahadevan and Nagesh Shenoy M and the mentor Jatinder Bedi and was done as part of their capstone project. They were part of the GreatLakes PGPBABI program in Bangalore and finished their curriculum recently.",https://www.analyticsvidhya.com/blog/2016/10/complete-study-of-factors-contributing-to-air-pollution/
18 New Must Read Books for Data Scientists on R and Python,Learn everything about Analytics|Introduction|R for Data Science|Python for Data Science|End Notes,"Hands-on Programming with R|R for Everyone: Advanced Analytics and Graphics|R Cookbook|R Graphics Cookbook|Applied Predictive Modeling|Introduction to Statistical Learning|Elements of Statistical Learning|Machine Learning with R|Mastering Machine Learning with R|Machine Learning for Hackers|Practical Data Science with R|Mastering Python for Data Science|Python for Data Analysis|Introduction to Machine Learning with Python|Python Machine Learning|Building Machine Learning Systems with Python|Advanced Machine Learning with Python|Programming Collective Intelligence|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Complete Study of Factors Contributing to Air Pollution|Winners Approach & Codes from Knocktober : Its all about Feature Engineering!|
Analytics Vidhya Content Team
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Its called reading. Its how people install new software into their brainPersonally, I havent learnt as much from videos & online tutorials as much Ive learnt from books. Until this very moment, my tiny wooden shelf has enough books to keep me busy this winter.Understanding machine learning & data science is easy. There are numerous open courses which you can take up right now and get started.But, acquiring in-depth knowledge of a subject requires extra effort.For example: You might quickly understand how does a random forest work, but understanding the logicbehind its working would require extra efforts.The confidence of questioning the logic comes from reading books. Some people easily accept the status quo. On the other hand, some curious oneschallenge & say, Why cant it be done the other way? Thats where such people discover new ways of executing a task. Almost, every data scientist Ive come across in person, on AMAs, on published interviews, each one of them have emphasized the inevitable role of books in their lives.Here is a list of books on doing machine learning / data science in R and Python which Ive come across in last one year. Since reading is a good habit, with this post, I want pass this habit to you. For each book, Ive written a summary to help you judge its relevance. Happy reading!Disclosure:The amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission.This book is written by Garrett Grolemund. It is best suited for people new to R. Learning to write functions & loops empowers you to do much more in R, than just juggling with packages. People think, R packages can let them avoid writing functions & loops, but it isnt a sustainable approach. This bookintroduces you to details of R programming environment using interesting projects like weighted dice, playing cards, slot machine etc. The book language is simple to understand and examples can be reproduced easily.Available: Buy NowThis book is written by Jared P. Lander. Its a decent book covering all aspects of data science such as data visualization, data manipulation, predictive modeling, but not in as much depth. You can understand as, it covers a wide breath of topic and misses out on details of each.Precisely, it emphasizes on the usage criteria of algorithms and one example each showing its implementation in R. This books should be brought by people who are more inclined towards understand practical side of algorithms.Available: Buy NowThis book is written by Teetor Paul.It comprises of several tips, recipes to help people overcome daily struggles in data pre-processing and manipulation. Many a times, we are stuck in a situation where we know very well, what needs to be done. But, how it needs to be done becomes a mammoth challenge. This books solves the problem. It doesnt have theoretical explanation of concepts, but focuses on how to use them in R. It covers a wide range of topics such as probability, statistics, time series analysis, data pre-processing etc.Available: Buy NowThis book is written byWinston Chang. Data visualization enables a person to express & analyze their findings usingshapes & colors, not just in tables. Having a solidunderstanding of charts, when to use which chart, how to customize a chart and make it look good, is a key skill of a data scientist. This book doesnt bore you with theoretical knowledge, but focuses on building them in R using sample data sets. It focuses on ggplot2 package to undertake all visualization activities.Available: Buy NowThis book is written by Max Kuhn and Kjell Johnson. Max Kuhn is none other than creator of caret package too. Its one of the best book comprising a blend of theoretical and practical knowledge. It discusses several crucial machine learning topics such as over-fitting, feature selection, linear & non-linear models, trees methods etc. Needless to say, it demonstrates all these algorithms using caret package. Caret is one of the powerful ML package contributed in CRAN library.Available: Buy NowThis book is written by a team of authors including Trevor Hastie and Robert Tibshirani. It is one of the most detailed book on statistical modeling. Also, its available for free. It comprises of in-depth explanation of topics such as linear regression, logistic regression, trees, SVM, unsupervised learning etc. Since its the introduction, the explanations are quite easy and any newbie can easily follow it. Thus, I recommended this book to all people who are new to machine learning in R. In addition, several practice exercises in this book just adds cherry on top.Available: Buy NowThis book is written by Trevor Hastie, Robert Tibshirani and Jerome Friedman.This is the next part of Introduction to Statistical Learning. It comprises of moreadvanced topics, therefore I would suggest you not to directly jump to it. This book in best suited for people familiar with basics of machine learning. It talks about shrinkage methods, different linear methods for regression, classification, kernel smoothing, model selection etc. Its a must read book for people who want to understand ML in depth.Available: Buy NowThis book is written by Brett Lantz. I am impressed by the simplicity of this authors way of explaining concepts. Its a book on machine learning which is easy to understand, and would provide you a lot of knowledge about their practical aspects too. Algorithms such as Bagging, Boosting, SVM, Neural Network, Clustering etc are discussed by solving respective case studies. These case studies will help you understand the real world usage of these algorithms. In addition, knowledge of MLparameters is also discussed.Available: Buy NowThis book is written by Cory Lesmeister. It is best suited for everyone who want to master R for machine learning purposes. It comprises of all (almost) algorithms and their execution in R. Alongside, this book will introduce you to several R packages used for ML including the recently launched H2o package. Its a book which features latest advancements in ML forte, hence Id suggest it to be read by every R user. However, you cant expect to learn advanced ML techniques like Stacking from this book.Available: Buy NowThis book is written by Drew Conway and John Myles White. Its a relatively shorter book than others, but aptly brings out sheer importance of every topic discussed. After reading this book, I realized that the authors mindset is not to go deep in a topic, still making sure to cover important details. For enhanced understanding, the author also demonstrates several used cases, while solving which, explains the underlying methods too. Its a good read for everyone whod like to learn something new about ML.Available: Buy NowThis book is written by Nina Zumel & John Mount. As the name suggests, this book focuses on using data science methods in real world. Its different in itself. None of the books listed above, talks about real world challenges in model building, model deployment, but it does. The author doesnt move her focus from establishing a connect between theoretical world of ML and its impact on real world activities. Its a must read for freshers who are yet to enter analytics industry.Available: Buy NowThis book is written by Samir Madhavan. This book starts with an introduction to data structures inNumpy & Pandas and provides a useful description of importing data from various sources into these structures.You will learnto perform linear algebra in Python and make analysis by using inferential statistics. Later, the book takes onto the advanced concepts like building a recommendation engine, high-end visualization using Python, ensemble modeling etc.Available: Buy NowWant to get started with data analysis with Python? Get your hands on this data analysis guide by W Mckinney, the main author of Pandas library. There isnt any online course as comprehensive as this book.This bookcovers all aspects of data analysis from manipulating, processing, cleaning, visualization and crunching data in Python. If you are a new to data science python, its a must read for you. Its power-packed with case studies from variousdomains.Available: Buy NowThis book is written by Andreas Muller and Sarah Guido. Its meant to help beginners to get started with machine learning. It teaches to build ML models in python scikit-learn from scratch. It assumes no prior knowledge, hence its best suited for people with no prior python or ML knowledge. In addition, it also covers advanced methods for model evaluation and parameter tuning, methods for working with text-data, text -specific processing techniques etc.Available: Buy NowThis book is written by Sebastian Raschka. Its one of the most comprehensive books Ive found on ML in Python. The author explains every crucial detail we need to know about machine learning. He takes a stepwise approach in explaining the concepts supported by various examples. This book cover topics such as neural networks, clustering, regression, classification, ensemble etc. Its a must read book for everyone keen to master ML in python.Available: Buy NowThis book is written by Willi Richert, Luis Pedro Coelho. In this book the authors have chosen a path of, starting with basics, explaining concepts through projects and ending on a high note. Therefore, Id suggest this book to newbie python machine learning enthusiasts. It covers topics like image processing, recommendation engine, sentiment analysis etc. Its easy to understand and fast to implement text book.Available: Buy NowThis book is written by John Hearty. Its a definite read for every machine learning enthusiasts. It lets you rise above the basics of ML techniques and dive into unsupervised methods, deep belief networks, Auto encoders, feature engineering techniques, ensembles etc. Its definitely a book you would want to read to improve your ranks in machine learning competitions. The author lays equal emphasis on theoretical as well practical aspects of machine learning.Available: Buy NowThis book is written by Toby Segaran. With an interesting title, this book is meant to introduce you to several ML algorithms such as SVM, trees, clustering, optimization etc using interesting examples and used cases. This is book is best suited for people new to ML in python. Python, known for its incredible ML libraries & support should make it easy for you to learn these concepts faster. Also, the chapters include exercises for practice to help you develop better understanding.Available: Buy NowThe motive of this article is to introduce you to the huge reservoir of knowledge which you havent noticed yet. These books will not only provide you boundless knowledgebut also, enrich you with various perspectives on using MLalgorithms. You might feel puzzled atseeing so many books explaining similar concepts.What differentiates these books is the case studies & examples discussed.Trust me, sometimes theoretical explanations becomes quite difficult to decipher as compared to understanding practicalcases. Thats how I feel. Learning from these authors knowledge is the fastest way you can learn from so many people.Hope this article would help you selecting your next book on R or Python. Do keep me posted about your reading experience / suggestions or advises.",https://www.analyticsvidhya.com/blog/2016/10/18-new-must-read-books-for-data-scientists-on-r-and-python/
Winners Approach & Codes from Knocktober : Its all about Feature Engineering!,Learn everything about Analytics|Introduction|About the Competition|The Problem Set|Winners!|Winners Solutions|Rank 3: Sonny Laskar|Rank 2: Naveen Kumar Kaveti & Suprit Saha|Rank 1: Sudalai Rajkumar & Rohan Rao|Learnings from the Competition|EndNotes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|18 New Must Read Books for Data Scientists on R and Python|Complete Guide on DataFrame Operations in PySpark|
Kunal Jain
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Feature Engineering|Modelling|Key learnings,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you dont challenge yourself, you will never realize what you can becomeKnocktober the machine learning competition held last weekendsure made history. It was one of the most challenging and intimidating competitions on Analytics Vidhya. We saw top data scientists from across the world using best of their knowledge to secure top position on the leaderboard.We launched the competition on 21 Oct16 midnightwith over 2275 participants.At the time of launching this competition, we promised that you will be provoked to question your machine learning skills. And we bet you did. The competition ended on 23 oct16 leaving everyone clueless about private leaderboard ranks.The winners of the competition have generously shared their detailed approach and codes they used in the competition. XGBoost, Python & GBM were widely used inthe competition. But, it was the combination of best features and prudent validation technique which won the competition.For all those who didnt participate in this competition, youve missed out onone of the best opportunities. Nevertheless, theres always a next time. Stay tuned for upcoming hackathons.Medcamp, is a not for profit organisation which organizes health camps in cities with low work-life balance. It was started in 2006 and its core mission is to help people maintain a balanced work-life. For four years, they have conducted 65 health camps in various cities. But they are facing a big problem in their effective business operations. And they want data scientists to provide them a solution.Medcamp is incurring losses. Every year they see a huge drop inthe number of people registering for the camp and the ones actually taking test at the camp.The irregularity in the number of people showing at the camp creates a big problem for them. If the number of people taking the test is less than the total number of registration, then the company incurs loss of unnecessary inventories. And if, the number of people visiting the camp is more than the number of registration, then it can lead to shortage of inventory and thus, a bad experience for the people.To tackle this problem, Medcamp has provided the total number of registrationsfrom past 4 years andwant data scientists to provide them insights from the data.The evaluation metric used was AUC ROC.Knocktober ended on a pleasant note, leaving everyone inquisitive to know what the final results will reveal. And when we announcedthe Top 3 ranks, not just the participants but even we were surprised. Here are the Top 3 rankers and it was definitely not an easy win.Rank 1 : Sudalai Rajkumar & Rohan RaoRank 2 : Naveen Kumar Kaveti & Suprit SahaRank 3 : Sonny LaskarHere are the final rankings of all the participants at theleaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go ahead.Sonny Laskar is a data science expert currently heading the IT Operations Analytics wing at Microland. He often participates in machine learning challenges to test his expertise. Heres what Sonny says:Sonny LaskarEven though it was a binary classification problem, the data was not present in the standard labeled format. Hence many were confused as to what is the actual problem.Another interesting thing was the No IDs please clause. Recently, we have observed that some competitions in Analytics Vidhya and Kaggle turned out to a leak  explorationproblem which at the end of the day is of very less value to the client. Hence any leak exploration is strictly discouraged.As always, I started with a few visualizations. Below are few of the plots:Like others, I also had created few time-based features. The ratio of the days left for the event to end to the duration of the camp was an interesting feature that worked for me. Another interesting feature was how manydays have elapsed between the previous registrations by the same patient.Plots aboveshow that the 1s occurredmore if the patient registered for the event much closer to the event start date. May be as days pass, the patient loses interest.In my experience, tree-based models work better than linear models in such data sets. Hence I started with Xgboost, the universally accepted tree-based algo. Surprisingly, RandomForest didnt work as well , might be because I didnt tune that well. GBM performedslightly better than Xgboost.I created two bags for both Xgboost and GBM and did a final rank average ensemble of the scores.This competition reminded me of the thumb rule:Link to CodeNaveen Kumar Kaveti and Suprit Saha are both statistical analyst at Walmart Labs, India. Naveen and Suprit are both data science enthusiasts and often participates in competitions to test theirskills.Heres what they shared:Naveen KumarSuprit SahaInitially we generated numerous features but later realized that some of them are performing very well in the train set but they are adding noise in the test set. In this competition we mainly focused on feature engineering rather than algorithm selection.We built different models like logistic regression, random forest, xgboost and GBM (in h2o package) using our initial set of features and observed that, there is a significant difference between cross validation score and public leaderboard score. We realized that some of the features in train set have different distribution in test set then we started our hunt for finding out the variables, which are adding noise in the test set.To start with, we compared the distribution of variables across test and train and then we validated our results against public leaderboard score to find out noise variables.After eliminating noise variables, we tried aforementioned models again and among those GBM was outperforming others. Using only GBM we reached 8th position in the public leaderboard after that we tried different ensemble methods but none of them increased the score in public leaderboard.Key learning from this competition is that, feature engineering is one of the most important part of modeling, probably much more than the choice of algorithm. This competition reminds us the famous quote by George Box All models are wrong, but some are useful. I think here the useful ones are the one with good set of features.The major key for our success in this competition is elimination of noise variables and adding relevant features.Some of the features, which helped us in pushing leaderboard score:Tools used: R (Package: h2o)Link to CodeSudalai Rajkumar is a lead data scientist at Freshdesk and Rohan Rao is a lead data scientist at AdWyze. Their profound knowledge and deep understanding of machine learning conceptsis truly inspiring. Heres what they shared:Our approach was to explore the data independently and merge our models towards the end to generate a strong ensemble.SRKSudalaiRajkumar says:I started off with a camp based 5 fold-cross validation where camps were disjoint among the folds. This workeddecently for me. If there was a substantial increase in my CV score, there was some improvement in my LB score as well though not in the same magnitude. But one thing I realised was that I will not be able to create time based features in this methodology since this might cause a future leakage. From the data exploration, we can see that train and test are separated not only by camps but also by time. So, I created a validation set using the rows from the last few camps given in the training set and used the rest as development sample.Feature EngineeringOne thing I cherished a lot in this competition is feature engineering. Since I had some good amount of time during this hackathon unlike the recent ones, I spent time creating variables, checking the performance and then adding / discarding them.To start with, I ran an xgboost model using all the variables and checked the variable importance. I discarded those features with low variable importance (A trick learned from Rohan!). Removing those did not affect the cv scores and public LB score.Then I started creating additional variables. The ones that are included in my model and the reasoning for including the same are as follows:1. Duration of the camp in days  if the duration is long, then more people might attend2. Date difference between camp start date and registration date  If the person register close to the camp start date, then the chance of turning up for the camp is quite high.3. Date difference between camp end date and registration date  Some registration dates fall after camp start date as well. So to understand whether registration is done well ahead of the final day of the camp.4. Date difference between registration and first interaction  Frequent interactions might help gain more popularity for the camps.5. Date difference between camp start date and first interaction  Very similar to the reasoning of (2).6. Number of times the patient registered for a camp  To capture the individual characteristics.7. Number of patients registered for the given health camp  To capture the camp characteristics.8. Date difference between current registration date and the previous registration date  If the difference is a shorter time span, then there might be a higher chance to attend.9. Date difference between current registration date and the next registration date  Same reasoning as that of previous one.10. Mean outcome of the patient till the given date  To capture the historical patient outcome characteristics.11. Format of the previous camp attended  To capture if the patient attends any particular format of camps.12. Sum of donations given by the patient till the given date  If a patient donates more, then there might be a chance that the person is interested in attending the camps.13. Total number of stalls visited by the patient till the given date  If apatient visits more stalls, then the chance of attending the camps might be high.Variables that did not help me improve the score are:1. Different ratio variables on the date i.e. one date differences divided by another.2. Ratio variables using age, income and education.3. Mean health score values of the previously attended camps.4. Last known outcome value for the patient.5. Bayesian encoding on categorical variables.The reason could be that they were not really helpful or this information was already captured by some other variables present above.ModelMy final model was an xgboost which scored 0.8389 in public leaderboard and I bagged them thrice to get 0.8392.Rohans approach:Rohan RaoAs I was busy at the World Sudoku/Puzzle Championships, I had very less time in this hackathon. Hence, I decided to focus on building a tangential different model with features so as to blend well with SRKs.I spent 95% of my time in exploring the data and engineering features. I did not even have time to tune my parameters, so, stuck with the set I started with. I went with a minimalistic approach, carefully choosing the most effective features, which gave stable CV and LB scores.I used the following raw features:1. Category12. Category23. Age4. Education Score5. City Type6. IncomeThese mainly capture metadata on the camps and the patients.The following engineered features boosted my models performance:7. Start_Date_Diff: Difference between start date of camp and registration date of patient.8. End_Date_Diff: Difference between end date of camp and registration date of patient.9. Prev_Date_Diff: Difference between registration date of patient and previous registration date of the patient.10: Next_Date_Diff: Difference between registration date of patient and next registration date of the patient.11. Count_Patient: Number of camps a patient registered.12. Count_Patient_Date: Number of camps a patient registered on each date.13. Donation_Flag: Binary indicator if the patient made a donation in the past.Using the raw date features isnt a good idea since the test data is split on time, hence, converting the date features into date differences was very useful.I found that patients who donated had a higher response rate than other patients, and thats how donation_flag helped the model.The other features are similar to SRKs, which describe characteristics of the timing and patient.ModelMy final model was an XGBoost with these 13 variables. I also subsetted the data, excluding observations in 2003 and 2004, and ones where the registration date was missing.It scored 0.8375 on the public LB.We checked the correlation of our best performing models and to our surprise it was 0.93  low correlation for a second level input, given both of us used the same modelling algorithm  XGBoost. The varied set of features in our base models is the reason for this. Hence we averaged both our models together to get to 0.8432 in the public leaderboard, which proved to be stable and robust, as we finished 1st on the public as well as the final private leaderboard.On working together as a teamSudalai:Rohan was very busy during the weekend with his World Championships. Inspite of his busy schedule, he managed to squeeze some time and create an awesome model with very few features and varied from my model. Salutes and thanks to him! It is always a great learning experience working with Rohan and look forward to the same in the future as well.Rohan:It was fun and interesting to work with Sudalai, and our ensemble made a considerable difference at the end, which got us the win. Really happy with our team-work and performance. Thanks to SRK for all the help and support!Link to CodeBelow are the key learnings from this competition:As we come to the end of this article, I am sure you will agree with me that it was an incredible competition. We thoroughly enjoyed each & every bit of it. For all other participants who would like to share their codes with our fellow users, please post them in the comments below. And tell us more about your experience by dropping in your comments.For all those, who didnt participate in the competition, I am sure you are regretting it big time. To ensure that you dont miss out on such competitions, subscribe for email alerts and follow the link below for all the upcoming competitions. And until we meet again for another round of face-off with the machine learning champions, keep learning & improving your skills.",https://www.analyticsvidhya.com/blog/2016/10/winners-approach-codes-from-knocktober-xgboost-dominates/
Complete Guide on DataFrame Operations in PySpark,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. DataFrame in PySpark: Overview|2. Why DataFrames are Useful ?|3. Setup Apache Spark|4. How to create a DataFrame ?|5. DataFrame Manipulations|6. How to Apply SQL Queries on DataFrame?|7. Pandas vs PySpark DataFrame,"Creating DataFrame from RDD|Creating the DataFrame from CSV file|How to seedatatype of columns?|How to Show first n observation?|How to Count the number of rows in DataFrame?|How many columns do we have in train and test files along with their names?|How to get the summary statistics (mean, standard deviance, min ,max, count) of numerical columns in a DataFrame?|How to select column(s) from the DataFrame?|How to find the number of distinct product in train and test files?|What if I want to calculate pair wise frequency of categorical columns?|What If I want to get the DataFrame which wont have duplicate rows of given DataFrame?|What if I want to drop the all rows with null value?|What if I want to fill the null values in DataFrame with constant number?|If I want to filter the rows in train which has Purchase more than 15000?||How to find the mean of each age group in train?|How to create a sample DataFrame from the base DataFrame?|How to apply map operation on DataFrame columns?|How to sort the DataFrame based on column(s)?|How to add the new column in DataFrame?|How to drop a column in DataFrame?|What if I want to remove some categories of Product_ID column in test that are not present in Product_ID column in train?|Endnotes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Winners Approach & Codes from Knocktober : Its all about Feature Engineering!|Winning Strategies for ML Competitions from Past Winners|
Ankit Gupta
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In my first real world machine learning problem, I introduced you to basic concepts of Apache Spark like how does it work, different cluster modes in Spark and What are the different data representation in Apache Spark. To provide you with a hands-on-experience, I also used a real world machine learning problem and then I solved it using PySpark.In my second real world machine learning problem, I introduced you on how to create RDD from different sources ( External, Existing ) and briefed you on basic operations ( Transformation and Action) on RDD.In this article, I will be talking about DataFrame and its features in detail. Then, we will see how to create DataFrame from different sources and how to perform various operations in DataFrame.P.S.  If you have not read the previous 2 articles, I strongly recommend that you go through them before going further.In Apache Spark, a DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:My first exposure to DataFrames was when I learnt about Pandas. Today, it is difficult for me to run my data science workflow with out Pandas DataFrames. So, when I saw similar functionality in Apache Spark, I was excited about the possibilities it opens up!I am sure this question must be lingering in your mind. To make things simpler for you, Im listing down few advantages of DataFrames:In order to understand the operations of DataFrame, you need to first setup the Apache Spark in your machine. Follow the step by step approach mentioned in my previous article,which will guide you to setup Apache Spark in Ubuntu.DataFrame supports wide range of operations which are very useful while working with data. In this section, I will take you through some of the common operations on DataFrame.First step, in any Apache programming is to create a SparkContext. SparkContext is required when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. And the first step is to connect with Apache Cluster. If you are using Spark Shell, you will noticethat itis already created. Otherwise, we can create the SparkContext by importing, initializing and providing the configuration settings. For example,Again we need to do same with the SQLContext, if it is not loaded.sqlContext = SQLContext(sc)A DataFrame in Apache Spark can be created inmultiple ways:I am following these steps for creating a DataFrame from list of tuples:Lets check the type of schemaPeople.For reading a csv file in Apache Spark, we need to specify a new library in our python shell. To perform this action, first we need to download Spark-csv package (Latest version) and extract this package into the home directory of Spark. Then, we need to open a PySpark shell and include the package (I am using spark-csv_2.10:1.3.0).Lets read the data from csv file and create the DataFrame. To demonstrate this Im to using the train and test datasets from the Black Friday Practice Problem, which you can downloadhere.PATH is the location of folder, where your train and test csv files are located. Header is True, whichmeans that the csv files contains the header. We are using inferSchema = True option for telling sqlContext to automatically detect the data type of each column in data frame. If we do not set inferSchema to be true, all columns will be read as string.Now comes the fun part. You have loaded the dataset by now. Let us start playing with it now.To see the types of columns in DataFrame, we can use the printSchema, dtypes. Lets apply printSchema() on train which will Print the schema in a tree format.From above output, we can see that, we have perfectly captured the schema / data types of each columns while reading from csv.We can use headoperationto see first n observation (say, 5 observation). Head operation in PySpark is similar to head operationin Pandas.Above results are comprised of row like format. To see the result in more interactive manner(rows under the columns), we can use theshow operation. Lets apply show operation on train and take first 2 rows of it. We can pass the argument truncate = True to truncate the result.We can use countoperation to count the number of rows in DataFrame. Lets apply count operation on train & test files to count the number of rows.We have 550068, 233599 rows in train and test respectively.For getting the columns name we can use columnson DataFrame,similar to what we do for getting the columns in pandas DataFrame. Lets first print the number of columns and columns name in train file then in test file.Lets do same for the test.From the above output we can check that we have 13 columns in test file and 12 in train file. Purchase not present in test file where as Comb is only in testfile.We can also see that, we have one column () in test file which doesnt have a name.describeoperation is use to calculate the summary statistics of numerical column(s) in DataFrame. If we dont specify the name of columns it will calculate summary statistics for all numerical columns present in DataFrame.Lets check what happens when we specify the name of a categorical / String columns in describe operation.As we can see that, describe operation is working for String type column but the output for mean, stddev are null and min & max values are calculated based on ASCII value of categories.To subset the columns, we need to use select operation on DataFrame and we needto pass the columns names separated by commas insideselectOperation. Lets select first 5 rows of User_ID and Age from the train.The distinctoperation can be used here, to calculate the number of distinct rows in a DataFrame. Lets apply distinct operation to calculate the number of distinct product in train and test file each.We have 3631 & 3491 distinct product in train & test file respectively. After counting the number of distinct values for train and test files, we can see the train file has more categories than test file. Let us check what are the categories for Product_ID, which are in test file but not in train file by applying subtract operation.We can do the same for all categorical features.Above, you can see that 46 different categories are in test file but not in train. In this case, either we collect more data about them or skip the rows in test file for those categories (invalid category) which are not in train file.We can use crosstaboperation on DataFrame to calculate the pair wise frequency of columns. Lets apply crosstab operation on Age and Gender columns of train DataFrame.In the above output, the first column of each row will be the distinct values of Age and the column names will be the distinct values of Gender. The name of the first column will be Age_Gender. Pair with no occurrences will have zero count in contingency table.We can use dropDuplicates operation to drop the duplicate rows of a DataFrame and get the DataFrame which wont have duplicate rows. To demonstrate that I am performing this on two columns Age and Gender of train and get the all unique rows for these columns.The dropna operation can be use here. To drop row from the DataFrame it consider three options.Lett drop null rows in train with default parameters and count the rows in output DataFrame. Default options are any, None, None for how, thresh, subset respectively.Use fillna operation here. The fillna will take two parameters to fill the null values.Lets fill -1 inplace of null values in train DataFrame.We can apply the filteroperationon Purchase column in train DataFrame to filter out the rows with values more than 15000. We need to pass a condition. Lets apply filter on Purchase column in train DataFrame and print the number of rows which has more purchase than 15000.The groupbyoperation can be used here to find the mean of Purchase for each age group in train. Lets see how can we get the mean purchase for the Age column train.We can also apply sum, min, max, count with groupby when we want to get different summary insight each group. Lets take one more example of groupby to count the number of rows in each Age group.We can use sample operation to take sample of a DataFrame. The sample method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.Lets create the two DataFrame t1 and t2 from train, both will have 20% sample of train and count the number of rows in each.We can apply a function on each row of DataFrame using map operation. After applying this function, we get the result in the form of RDD. Lets apply a map operation on User_ID column of train and print the first 5 elements of mapped RDD(x,1) after applying the function (I am applying lambda function).In above code we have passed lambda function in the map operation which will take each row / element of User_ID one by one and return pair for them (User_ID,1).We can use orderByoperation on DataFrame to get sorted output based on some column. The orderBy operation take two arguments.Lets sort the train DataFrame based on Purchase.We can use withColumnoperationto add new column (we can also replace) in base DataFrame and return a new DataFrame. The withColumn operation will take 2 parameters.Lets see how withColumn works. I am calculating new column name Purchase_new in train which is calculated by dviding Purchase column by 2.To drop a column from the DataFrame we can use drop operation. Lets drop the column called Comb from the test and get the remaining columns in test.Here, we can use a user defined function ( udf ) to remove the categories of a column which are in test but not in train. Lets again calculate the categories in Product_ID column which are in test but not in train.We have got 46 different categories in test. For removing these categories from the test Product_ID column. I am applying these steps.Lets see how it works. First create not_found_catNow resister the udf, we need to import StringType from the pyspark.sql and udf from the pyspark.sql.functions. The udf function takes 2 parameters as arguments:In the above code function name is F1 and we are putting -1 for not found catagories in test Product_ID. Finally apply above F1 function on test Product_ID and take result in k1 for new column calles NEW_Product_ID.Now, lets see the results by again calculating the different categories in k and train subtract operation.The output 1 means we have now only 1 different category k and train.We have already discussed in the above section that DataFrame has additional information about datatypes and names of columns associated with it. Unlike RDD, this additional information allows Spark to run SQL queries on DataFrame. To apply SQL queries on DataFrame first we need to register DataFrame as table. Lets first register train DataFrame as table.In the above code, we have registered train as table(train_table) with the help of registerAsTable operation. Lets apply SQL queries on train_table to select Product_ID the result of SQL query will be a DataFrame. We need to apply a action to get the result.In the above code, I am using sqlContext.sql for specifying SQL query.Lets get maximum purchase of each Age group in train_table.Pandas and Spark DataFrame are designed for structural and semistructral data processing. Both share some similar properties (which I have discussed above). The few differences between Pandas and PySpark DataFrame are:In addition to above points, Pandas and Pyspark DataFrame have some basic differences like columns selection, filtering, adding the columns, etc. which I am not covering here.In this article, I have introduced you to some of the most common operationson DataFrame in Apache Spark. There are many more operationsdefined on DataFrame, but it is cumbersome (and unwanted) to cover all of them in one article. To learn more about operations on DataFrame, you can refer Pyspark.sql moduledocin Python.I hope you found this article helpful. Now, its time for you to practice and read as much as you can. Good luck! If you still have any difficulty in DataFrame operation,Id liketo interact with you in comments. If you have any more doubts or queries feel free to drop in your comments below.",https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/
Winning Strategies for ML Competitions from Past Winners,"Learn everything about Analytics|Introduction|1.Sudalai Rajkumar (SRK) , Senior Data Scientist, AV Rank 1|2. Rohan Rao, Lead Data Scientist, AV Rank 4|3. Steve Donoho, Top Data Scientist","EndNotes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Complete Guide on DataFrame Operations in PySpark|Knocktober  Machine learning Competition, 21st Oct, Analytics Vidhya|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We launched Knocktober last night and we were happy to see the excitement it has created among all the participants. This time we raised the bar for Analytics Vidhya hackathons. Im sure the faint hearted ones would have panicked after seeing the problem set. And the ones who didnt budge from their resolve of winning Knocktoberwill make history on Analytics Vidhya. Since the competition is unique, we thought of providing you the winning strategies from our past competition winners.Read on, to know the hackathon approach of three top data scientists. They have also shared useful tips & tricks, that will definitely help you to improve your leaderboard position.SRKHis approach in past competitions:Once youve executed these7 steps, a basic framework will be ready to do more experimentation. Further, you canconcentrate more on:Last but not the least,wemust perform a solid local validation. Else, we might end up over fitting on the public leader board.Tips from SRK:1.Understanding the problem It is really important to have a thorough understanding of the problem that we are trying to solve. Only after weve understood the problem clearly, we can derivesuitable insights from data to tackle the problem andobtain good results.2.Structured Thinking Its a unique way of thinking through the problems. Being a data scientist, one needs to be more structured in his/her thinking in order to obtain good results.3.Effective communication of results Effective communication of derived results is as important as performing thedataanalysis.Read detailed article hereRohan RaoHis approach in past competitions:Tips from Rohan:Read detailed articlehereSteve DonohoHis approach in past competitions:Tips from Steve:Read detailed article hereGo on & use these tips from the winners and grabyour first win in Knocktober.I am sure these approaches and tips will provide you an upper hand in the competition. Learn and improve from these tips. If you want to register for the ongoing competition, click here. I recommend that you atleast register for Knocktober and explore the problem statement. I assure you it will be a great learning experience for you.Did you find this article? Do you have any questions? Post them in the comments below and let me know how I can help you further. Also, I would like to hear, if you have any feedback for us.",https://www.analyticsvidhya.com/blog/2016/10/winning-strategies-for-ml-competitions-from-past-winners/
"Knocktober  Machine learning Competition, 21st Oct, Analytics Vidhya",Learn everything about Analytics|Introduction|Who should join this competition?|How much is the fee for this competition?|Why should you participate in this competition?,"Share this:|Like this:|Related Articles|Winning Strategies for ML Competitions from Past Winners|16 New Must Watch Tutorials, Courses on Machine Learning|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This friday it gets better than ever. Machine Learning war between Data Nerds only on Analytics Vidhya.Knocktober  a machine learning competition starting on21st Oct 2016. Gear up with all your tools and strategies because this time we bet you would plead ifyou had couple of more hours.Knocktober is a power-packed and mindboggling competition that will leave you grasping for breath.Come be part of this competition, that you will give you more reasons to love hackathons.Date: 21 Oct 2016 to 23 Oct 2016Mode: OnlineCash Prize: INR 90,000 RegisterHere
Anyone who is well skilled in machine learning techniques, and want to improve his or her knowledge in data science. Hackathons are the fastest way to improve your machine learning knowledge and excel in your career.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.We have very carefully designed this competition to provide you a competitive and enriching experience. Hackathons are becoming a popular way for individuals to showcase their machine learning expertise and knowledge. Hackathon mania is a new fever amongst data scientists and machine learning enthusiasts.For more details about the competition,  Visit Here
",https://www.analyticsvidhya.com/blog/2016/10/knocktober-machine-learning-competition-21st-oct-analytics-vidhya/
"16 New Must Watch Tutorials, Courses on Machine Learning",Learn everything about Analytics|Introduction|Are these tutorialsmeant for me?|Table of Contents|1. For Newbies in Machine Learning|2. Machine Learning Courses|3. Other Useful Talks|4. Machine Learning at Organizations|End Notes,"How to become a Data Scientist in 6 months|Essential Data Science Skills For every Programmer|Beginners Guide to Machine Learning Competitions|Machine Learning Recipes|Statistical Machine Learning|Machine Learning Course  University of Waterloo|Practical Machine Learning Tutorial with Python|Neural Networks for Machine Learning|Machine Learning With Imbalanced Data sets|Machine Learning Tutorial on Scikit-Learn|Advanced Techniques  Deep Learning|Pandas for Beginners|Predictive Modeling with Python|Machine Learning : Googles Vision|Machine Learning at Pinterest|Machine Learning Used by Grab Taxi|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Knocktober  Machine learning Competition, 21st Oct, Analytics Vidhya|Creating Interactive data visualization using Shiny App in R (with examples)|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most of us fail to acknowledgethat Youtube has a massive resource centerof machine learning tutorials which are free to access. You no longer need to wait for launch of new MOOCs to learn a new concept. Search it on YouTube and chance are high that youll find it.Last year, we published an article on top YouTube videosfeaturing best ever videos on neural networks, deep learning & machine learning. No doubt, videos are enriching. But, it happens that content (video, text) gets outdated overtime. Now is the time to update your knowledge.I wrote this article to help you discovernew tools, techniques, methods, practices undertaken in machine learning since last year. Always remember, the pursuit of knowledge is similar to the life of fresh water.Never stop the flow and get a perennial fresh stream ofperspectives.Depends! if you are curious and want to learn something newor enhance your skill sets.Ive categorized the videos in 4 principal sections, making sure that each one of you gets to learn something new. However, python users have more to learn. Youll find full lectures, practical workshops, short talks indicating the increased dominance of machine learning in real world. Alongside, youll also learn how machine learning is solving real world problems at Google, Pinterest & TaxiGrab.If you plan to watch them, make a schedule. Dont do it all in a day. Remember, the motive is not to just watch the videos, but to understand what is being taught. It takes time and discipline. Also to help you save time, Ive provided a short summary below every video to help you decide if thats what you should watch!Duration: 56:24 minsIn this video, Tetiana Ivanova shares her journey ofbecoming a data scientist in just 6 months. Participating in hackathons got her started with machine learning. If you have been wondering whether to go for analytics post graduate programor become self taught, you must watch this video. Tetiana shares her real life experience of making the career move, the hardships and truth behind the facade of a higher education.Duration: 3:23:19 hrsData Science has several tools used fordata exploration, visualization to modeling. Which are the must to have? Andy reveals the most important tools every person should use while working on Python. These tools arent just easy to learn, but also upgrade your style of coding outputs. It is a must watch for python beginners. Also, he demonstrates using these tools to produce different outputs. You should install these tools as instructed and practice alongside while watching it.Duration: 1:43:08 hrWhen will I win my first data science competition? Im sure everyone asks this question to themselves. Winning world level competitions but not impossible. Little bit guidance and practice does the trick. This tutorial trains you to solve Kaggle competition using an effective ML approach.The packages used in the tutorial are IPython notebook, scikit-learn, pandas and NLTK.You will learn about the process to follow incompetitions, and how to perform modeling, feature selection, optimization & validation.Duration:NANothing wouldintroduceyou to technical aspects of machine learning faster than these videos. Google released these 7 machine learning recipesthis year. These are short tutorial (~10 mins) which cover crucial aspects of machine learning such as feature extraction, decision tree visualization, classification model, tensor flow etc. The language used is Python. However, the conceptual knowledge is tool agnostic. I think, these videos can also be watchedduring lunch time!This course was being taught at Carnegie Mellon University (CMU)in Spring 2016 session. As itsname, the professor teaches topics such as regression, clustering, boosting, graphical models, minimax theory etc. This course is best suited for students having basic understanding of statistics & probability. Its a core mathematical course, therefore you should also be comfortable with understanding mathematical equations. Alongside, there are assignments & solution which would further improve your concepts.This detailed machine learning course from University of Waterloo, will guide you through basics & advanced chapters of machine learning. Its a conceptual course which will educate you on mathematical relations in ML algorithms.It has been taught by multiple professors including Shai Ben David, author of bookUnderstanding machine learning. It covers topics such as linear regression, bayesian, trees, clustering, neural networks, ensemble, hidden markov model and much more. Check out the other course material here.The first video in an introductory to the course so,feel free to skip the first 8 mins of the video.Python has quickly gained recognition in machine learning community. With its robust libraries and actively engaging communities, students are encouraged to learn python as their core language. If you code in python, this course will help you deepen your practical ML knowledge in python. If you are following the courses, after learning theoretical concepts from previous courses, here youll learn how to apply them. This playlist of 57 videos covers all important ML algorithms along with detailed version of each one.Deep Learning, a subfield of Artificial Intelligence has progressed bycontributions ofgreat minds like Geoffery Hinton.Learning from the master itself is a blessing in itself. Isnt it? This course on neural networks was taught by him at University of Toronto. The course is designed such that it progresses through basic topics and culminates at advanced side of neural networks. It include topics such as perceptrons, back propagation, CNN, RNN, gradient descent and a lot more in detail. Its a must watch for deep learning, neural network enthusiasts.Duration: 27:44 minutesClassification algorithm performs poorly when the data is skewed towards one class. This problem is prominentin real world while dealing withfraud detection, cancer detection or medical diagnosis. There are couple of methods like re-sampling, one-class learning, cost-sensitive learning to address this problem. This tutorialwill take you though different approaches to handle unbalanced data sets in fraud detection. Natalie also shares some practical advises which she learnt after working on numerous imbalanced problems.Duration: 3:03:54 hrsThis 3 hour tutorial touches upon the breadth of machine learning algorithms. The speaker, Sebastian Raschka, author of Python Machine learning, explains complex concepts using a beautiful mix of interactive images. Our brain is much more receptive to consume visual knowledge than textual or sound. This workshop was delivered at SciPy Conference 2016. He teaches supervised & unsupervised ML concepts supported with real life case studies. If you like this tutorial, heres Part 2 of this series.Duration: 1:36:32 hr
In past few years, the techniques of image classification, segmentation and object detection have evolved tremendously withDeep Learning. This tutorialwill take you through the advance concepts of Deep Learning focusing mainly on computer vision and image processing using Theano & Lasagne. Alongside, the speaker also discusses important tips & tricks such as dealing with less training data etc. To understand concepts, prior knowledge of algebra, calculus and machine learning is required.Duration: 1:47:48 hrOf all the python libraries, pandas becomes the first choice for data manipulation tasks. With its intelligent inbuilt functions, the painful task of summarizing & manipulating data becomes so much easy. This video is best suited for beginners who wants to learn python. In this tutorial, the speaker demonstrates tasks such as data selection, grouping, aggregation, plots etc. Make sure you do then side by side to develop better understanding.Duration: 58:28 minsOliver Grisel, one of the original contributor to Scikit learn library, talks about building high performance predictive models. How do you deal with large data sets in Python? Answers to such burning questions are given in this tutorial. Alongside, hell also introduce you to some interesting tools which can be used in conjunction with Python to fasten our predictive modeling process. Youll also learn about the backstage story of data and its chemistry with storage & distribution types.Duration: 44:44 minutesHow does google uses machine learning ? Everyone talks about it, but nobody can tell as accurately as this guy does. Learn more about googles take on machine learning and AI, how machine learning has streamlined googles end products. Also, ithas deployed practical A.I throughout its products and has brought an end user more closer to the technology.Hear from Googles machine learning leads on their breakthroughs in machine learning andtheir upcoming ML projects.Duration: 23:54In this video, Jure Leskovec, Chief Scientist at Pinterest explains how machine learning is used at Pinterest. Its motivating to see how ML is transforming ways of businesses on internet. Here,Jure explains different segmentsof Pinterest driven by machine learning which affectsnew user experience, interest recommendation, type of content, user actions prediction, pin ranking and visual features. Jure also shares insights about what worked for them and what lessons they learnt. I think its an interesting take on how machine learning is changing our day-to-day lives.Duration: 11:24Personally to me, it is surprising to seehow machine learning can solve business problems at different levels. One such example is how Grab Taxi uses machine learning to tackle the problem of taxi availability. To handle this problem, Grab started a unique initiative of bidding for a ride by the drivers and the fastest bidder wins and is assigned theride. Watch the full video to find out how theyused machine learning to build a predictive model on drivers bidding probability and used real time data to solve the problem.The tutorials listed above are meant to familiarize you with latest happeningsin machine learning. Most of the videos are > 1 hour, hence you areadvised to keep a schedule for watching them. Since there is an abundance of information on internet, it become crucial to find your right gems and stick to them.These tutorials are shortlisted on the basis of upload date, view count and relevance. We made sure that none of these tutorials have got featured in our previous article. Personally, I found these videos immensely useful in demonstrating python tasks. R users might be disappointed. I tried searching for new tutorials, but couldnt find anything helpful on youtube.Did you like this article ? Have you seen any video yet? What about the machine learning courses? Id love to hear your experience / suggestions in comments below.",https://www.analyticsvidhya.com/blog/2016/10/16-new-must-watch-tutorials-courses-on-machine-learning/
Creating Interactive data visualization using Shiny App in R (with examples),Learn everything about Analytics|Introduction|Table of Contents|1. Shiny: Overview|2. Setting up shiny|3. Writing ui.R|4. Writing SERVER.R|5.Deploying the Shiny app on the Web|6. Creating interactive visualization for data sets|7. Explore Shiny app with these add-on packages|8. Advantages and Disadvantages of Shiny|9. Additional Resources|End Notes,"Example 1: Drawing histograms for iris dataset in R using Shiny|Example 2: Drawing Scatterplots for iris dataset in R using Shiny|Example 3: Loan Prediction Practice problem|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|16 New Must Watch Tutorials, Courses on Machine Learning|Data Science Consultant Text Mining  Gurgaon / Pune ( 5-8 Years of Experience )|
Saurav Kaushik
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There is magic in graphs. The profile of a curve reveals a whole situation in a flash  history of an epidemic, a panic or an era of prosperity. The curve awakens the imagination. Henry D. HubbardData visualization plays a vital role in life of a Data Scientist.It is easier to visualize complex data and relationships than deciphering them fromspreadsheets / tables.There are severaltools for visualizing data such as Tableau, Qlik, Dygraphs, Kibana etc. If I talk specifically about R, itprovides three plotting systems:But, writing codes for plotting graphs in R time & again can get very tiring. Also, it is very difficult to create an interactive visualization for story narration using above packages.These problems can be resolved by dynamically creating interactive plots in R using Shiny with minimal effort.If you use R, chances are that you might have come across Shiny. It is an open package from RStudio, used to build interactive web pages with R. It provides a very powerful way to share your analysis in an interactive manner with the community. The best part about shiny is that you dont need any knowledge of HTML, CSS or JavaScript to get started.Today, I will walk you through all the steps involved in creating a shiny app as well as deploying it online to make it accessible to everyone. This article will provide you a good understanding n how shiny apps work and how they can be useful. To provide you a hands on experience on creating Shiny Apps on your own I will be using the Loan Prediction III Practice Problem. And am sure by the end of this article you will be able to create Shiny apps yourself.Note: This article requires basic knowledge of R language.Shiny is an open package from RStudio, which provides a web application framework to create interactive web applications (visualization) called Shiny apps. The ease of working with Shiny has what popularized it among R users. These web applications seamlessly display R objects (like plots, tables etc.) and can also be made live to allow access to anyone.Shiny provides automatic reactive binding between inputs and outputs which we will be discussing in the later parts of this article. It also provides extensive pre-built widgets which make it possible to build elegant and powerful applications with minimal effort.Any shiny app is built using two components:1.UI.R: This file creates the user interface in a shiny application. It provides interactivity to the shiny app by taking the input from the user and dynamically displaying the generated output on the screen.2. Server.R: This file contains the series of steps to convert the input given by user into the desired output to be displayed.Before we proceed further you need to set up Shiny in your system. Follow these steps to get started.1. Create a new project in R Studio2. Select type as Shiny web application.3. It creates two scripts in R Studio named ui.R and server.R.4. Each file needs to be coded separately and the flow of input and output between two is possible.If you are creatinga shiny application, the best way to ensure that the application interface runs smoothly on different devices with different screen resolutions is to create it using fluid page. This ensures that the page is laid outdynamically based on the resolution of each device.The user interface can be broadly divided into three categories:Lets understand UI.R and Server.R with an example:This acts as the brain of web application. The server.R is written in the form of a function which maps input(s) to the output(s) by some set of logical operations. The inputs taken in ui.R file are accessed using $ operator (input$InputName). The outputs are also referred using the $ operator (output$OutputName). We will be discussing a few examples of server.R in the coming sections of the article for better understanding.The shiny apps which you have created can be accessed and used by anyone only if, it is deployed on the web.You can host your shiny application on Shinyapps.io. It provides free of cost platform as a service [PaaS] for deployment of shiny apps, with some restrictions though like only 25 hours of usage in a month, limited memory space, etc. You can also use your own server for deploying shiny apps.Steps for using shiny cloud:Step 1: Sign up on shinyapps.ioStep 2: Go to Tools in R Studio.Step 3: Open global options.Step 4: Open publishing tabStep 5: Manage your account(s).Thats it! Using Shiny Cloud is that easy!Lets see a few examples:To provide you hands on experience of creating shiny app,we will be using the Loan Prediction practice problem.To brief you about the data set, the dataset we will be using is a Loan Prediction problem set in which Dream Housing Finance Company provides loans to customers based on their need. They want to automate the process of loan approval based on the personal details the customers provide likeGender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others.1. We will be creating an explanatory analysis of individual variables of the practice problem.2. Explanatory analysis of multiple variables of Loan Prediction Practice problem.Writing ui.R :Writing Server.R
To add some more functionality to your Shiny App, there are some kick-ass packages available at your disposal. Here are few from RStudio.There are plenty of other data visualization tools out there. So, to help you compare what differentiates Shiny and what you can and cannot do with Shiny, lets look at the advantages and disadvantages of using shiny.Advantages :Disadvantages :
In the article I havecovered the key areas of Shiny to help you get started with it. Personally to me Shiny is an interesting creation and to make the most of it, I think you should explore more. Here are few additional resources for you to become an adept in creating Shiny Apps.I hope you enjoyed reading the article. Now you should be able to create Shiny apps at your end, try on your ends on building web applications using Shiny with Loan Prediction Practice Problem.If you have created a Shiny App before, drop in your comments below and tell me more about it.Can you think of some other innovative ideas where Shiny can be useful? Which is the best Shiny app you have ever seen? Do let me know.",https://www.analyticsvidhya.com/blog/2016/10/creating-interactive-data-visualization-using-shiny-app-in-r-with-examples/
Data Science Consultant Text Mining  Gurgaon / Pune ( 5-8 Years of Experience ),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|Creating Interactive data visualization using Shiny App in R (with examples)|Data Science Consultant RWE  Gurgaon / Pune ( 5-8 Years of Experience )|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation :Data Science Consultant  Text MiningLocation : Gurgaon / PuneExperience Required : 5  8 YearsAbout employer:ConfidentialJob DescriptionA key enabler of theservices of the company is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that thecustomers are trying to answer continue to evolve. In theendeavor to stay ahead in providing solutions to these evolving complex problems, the company hasset up a Data Scientist track which has three major focus areas:The Data Science Consultant will play a critical role in designing, developing and implementing analytical techniques on large, complex, structured and unstructured data sets (including big data) to help client make better decisions in sales and marketing space, with guidance from Data Science senior leadership.Responsibilities:Client impactResearch & Firm Contribution:Qualifications:Key Competencies :Approach to work:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Data Science Consultant Text Mining- Gurgaon / Pune and following details:",https://www.analyticsvidhya.com/blog/2016/10/data-science-consultant-text-mining-gurgaon-pune-5-8-years-of-experience/
Data Science Consultant RWE  Gurgaon / Pune ( 5-8 Years of Experience ),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|Data Science Consultant Text Mining  Gurgaon / Pune ( 5-8 Years of Experience )|Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation: Data Science Consultant  RWELocation : Gurgaon / PuneExperience Required : 5-8 yearsAbout employer:ConfidentialJob Description :A key enabler of theservices of the company is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that thecustomers are trying to answer continue to evolve. In theendeavor to stay ahead in providing solutions to these evolving complex problems, the company hasset up a Data Scientist track which has three major focus areas:The Data Science Consultant will play a critical role in designing, developing and implementing analytical techniques on large, complex, structured and unstructured data sets (including big data) to help client make better decisions in sales and marketing space, with guidance from Data Science senior leadership.Responsibilities :Client impactResearch & firm contributionQualifications Required :Key Competencies :Approach to work :Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Data Science Consultant RWE  Gurgaon / Pune and following details:",https://www.analyticsvidhya.com/blog/2016/10/data-science-consultant-rwe-gurgaon-pune-5-8-years-of-experience/
Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study),"Learn everything about Analytics|Introduction|Table of Contents|1. Keras : Overview|2. Keras : Advantages|3. Keras : Limitations
|4. General way to solve problems with Neural Networks|5. Starting with a simple Keras implementation on Identify the Digits|6. Hyperparameters to look out for in Neural Networks|7. Getting your hands dirty|8. Where to go from here?|9. AdditionalResources|End Notes","STEP 0:Getting Ready|STEP 1: Data Loading and Preprocessing|STEP 2: Model Building|STEP 3: Model Evaluation|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Science Consultant RWE  Gurgaon / Pune ( 5-8 Years of Experience )|Exclusive Interview with Sr Data Scientist, Paytm  Rohan Rao (DataHack Summit  Workshop Speaker)|
Faizan Shaikh
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In myprevious article, I discussed the implementation of neural networks using TensorFlow. Continuing the series of articles on neural network libraries, I have decided to throw light on Keras  supposedly the best deep learning library so far.I have been working on deep learning for sometime now and according to me, the most difficult thing when dealing with Neural Networks is the never-ending range of parameters to tune. With increase in depthof a Neural Network, it becomes increasingly difficult to take care of all the parameters. Mostly, people rely on intuition and experience to tune it. In reality, research is still rampant on this topic.Thankfully we have Keras, which takes care of a lot of thishard work and provides an easier interface!In this article, I am going to share my experience of working in deep learning. We will begin with an overview of Keras, its features and differentiation over other libraries. We will then, look at a simple implementation of neural networks in Keras. And then, I will take you through a hands-on exercise on parameter tuning in neural networks.Keras is a high level library, used specially for building neural network models. It is written in Python and is compatible with both Python  2.7 & 3.5. Keras was specifically developed for fast execution of ideas.It has a simple and highly modular interface, which makes it easier to create even complex neural network models. This library abstracts low level libraries, namely Theano and TensorFlow so that, the user is free from implementation details of these libraries.The key features of Keras are:Being a high level library and its simpler interface, Keras certainly shines as one of the best deep learning library available.Few features of Keras, which stands out in comparison with other libraries are:Given the above reasons, it is no surprise that Keras is increasingly becoming popular as a deep learning library.Neural networks is a special type of machine learning (ML) algorithm. So, like every ML algorithm, it follows the usual ML workflow of data preprocessing, model building and model evaluation. For the sake of conciseness, I have listed out a To-D0 list of how to approach a Neural Network problem.Before starting this experiment, make sure you have Keras installed in your system. Refer the official installation guide. We will use tensorflow for backend, so make sure you have this done in your config file. If not, follow the steps given here.Here, we solve our deep learning practice problem Identify the Digits. Lets take a look at our problem statement:Our problem is an image recognition problem, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model. So first, download the train and test files. The dataset contains a zipped file of all the images and both the train.csv and test.csv have the name of corresponding train and test images. Any additional features are not provided in the datasets, just the raw images are provided in .png format.Lets start:a) Import all the necessary librariesb) Lets set a seed value, so that we can control our models randomnessc) The first step is to set directory paths, for safekeeping!a) Now let us read our datasets. These are in .csv formats, and have a filename along with the appropriate labelsb) Let us see what our data looks like! We read our image and display it.c) The above image is represented as numpy array, as seen belowd) For easier data manipulation, lets store all our images as numpy arrayse) As this is a typical ML problem, to test the proper functioning of our model we create a validation set. Lets take a split size of 70:30 for train set vs validation seta) Now comes the main part! Let us define our neural network architecture. We define a neural network with 3 layers input, hidden and output. The number of neurons in input and output are fixed, as the input is our 28 x 28 image and the output is a 10 x 1 vector representing the class. We take 50 neurons in the hidden layer. Here, we use Adam as our optimization algorithms, which is an efficient variant of Gradient Descent algorithm. There are a number of other optimizers available in keras (refer here). In case you dont understand any of these terminologies, check out the article on fundamentals of neural network to know more in depth of how it works.b) Its time to train our modela) To test our model with our own eyes, lets visualize its predictionsb) We see that our model performs well even on being very simple. Now we create a submission with our modelI feel that, hyperparameter tuning is the hardest in neural network in comparison to any other machine learning algorithm. You would be insane to apply Grid Search, as there are numerous parameters when it comes to tuning a neural network.Note: I have discussed a few more details, on when to apply neural networks in the following article An Introduction to Implementing Neural Networks using TensorFlowSome important parameters to look out for while optimizing neural networks are:Also, there may be many more hyperparameters depending on the type of architecture. For example, if you use a convolutional neural network, you would have to look at hyperparameters like convolutional filter size, pooling value, etc.The best way to pick good parameters is to understand your problem domain. Research the previously applied techniques on your data, and most importantly ask experienced people for insights to the problem. Its the only way you can try to ensure you get a good enough neural network model.
Here are some resources for tips and tricks for training neural networks. (Resource 1, Resource 2, Resource 3)Let us take our knowledge of hyperparameters and start tweaking our neural network model.This result blows your mind, doesnt it. Even with such small training time, the performance is way better! This proves that a better architecture can certainly boost your performance when dealing with neural networks.Its time to let go of the training wheels. Theres many things you can try, so many tweaks to do. Try this on your end and let us know how it goes!Now, you have a basic overview of Keras and a hands-on experience of implementing neural networks.There is still much more you can do. For example, I really like the implementation of keras to build image analogies. In this project, the authors train a neural network to understand an image, and recreate learnt attributes to another image. As seen below, the first two images are given as input, where the model trains on the first image and on giving input as second image, gives output as the third image.Neural network tuning is still considered as a dark art. So, dont expect that you would get the best model in your first try. Build, evaluate and reiterate, this is how you would be a better neural network practitioner.Another point you should know that there are other methods to ensure that you would get a good enough neural network model without training it from scratch. Techniques like pre-training and transfer learning, are essential to know when you are implementing neural network models to solve real life problems.I hope you found this article helpful. Now, its time for you to practice and read as much as you can. Good luck! If you have any recommendations / suggestions on neural networks,Id love to interact with you in comments. If you have any more doubts or queries feel to drop in your comments below. Try out the practice problem Identify the Digitsyourself and let me know what was your experience.",https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/
"Exclusive Interview with Sr Data Scientist, Paytm  Rohan Rao (DataHack Summit  Workshop Speaker)",Learn everything about Analytics|Introduction,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Tutorial: Optimizing Neural Networks using Keras (with Image recognition case study)|Winners Solution from the super competitive The Ultimate Student Hunt|
Kunal Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Rohan RaoThere are several aspects to learning a new technical skill. You obviously need to learn the technical stuff, theapplications, the hacks and obviously the science. But, in addition to these, you need mentors. You need people who have travelled the path before you, to make sure you learn from them.Interacting with industry experts is one way to do so. With this in mind, we asked one of our top data scientist,Mr. Rohan Rao, to do an interview with us and an AMA with participants of The Ultimate Student Hunt. Rohan not only agreed to give his time, but also made sure he answered all the questions by spending extra time with the participants of the hackathon.For those of you who dont know Rohan, he completed his Post-Graduation in Applied Statistics from IIT-Bombay and is deeply passionate about machine learning & numbers. He is currently ranked 1st on Analytics Vidhya and 131st on Kaggle (Updated in September 2017).Rohan is also a high achiever in the world of Sudoku and Puzzles. He is a 4-time winner of National Sudoku Championship, 4-time winner of National Puzzle Championship and 3-time winner of the prestigious Times National Sudoku Championship. He spends his free time reading and discussing about latest developments in the data science industry on hisblog.Rohan Rao and Phani Srikanth are taking 8-hours intense workshop during DataHack Summit 2017 on Machine Learning at Scale  Using SparkML for Big DataHere are some excerpts from our interview and AMA with Rohan.KJ : First of all, I would like to sincerely thank you for devoting time for this interview. To start with, please tell us how did your journey in analytics begin ?Rohan Rao :Ive always liked math and numbers. I pursued it through my studies, and analytics seemed a natural choice for me when I began working.With interest, passion and perseverance, I picked up and specialized in building end-to-end Machine Learning solutions, which I currently enjoy doing.KJ : No journey can reach this level with some challenges and sacrifices. What were the obstacles you faced and how did you overcome them ?Rohan Rao: The biggest one is opportunity cost. When you really want to pursue something, you have to give up on other things. I always found it hard to sacrifice things to pursue Sudoku or Machine Learning, but I tried doing it as best as I could. Now, when I look back, all those sacrifices were worth it. Also, there were times when I wasnt performing well and wasnt able to improve. I think everyone goes through good and bad phases, its best to just take some time off and come back recharged, harder and stronger. It worked for me.Always pursue your passion, whatever it may be.KJ : How did you start participating in the competitions and hackathons? When did the first win come your way ?Rohan Rao: I began competing in ML competitions in late-2013, as a means to improve my general know  how in the field. It quickly caught on to me and I started enjoying it with every new competition and challenge.I won my first contest in my first ever Kaggle competition (it was a hackathon!), but credit goes to my team-mates and colleagues, who did most of the work. But since then, Ive never looked back.KJ : How do you decide which competitions to participate ?Rohan Rao : Data consistency + Domain + Time + Personal value / growth from the competition.I think time is a crucial component. Competitions require a lot of investment of time and effort, and one needs to make a decision whether it would be worth it. When I compete, I go all in, never do it half-heartedly.KJ : According to you, how should people approach problems in data science competitions ?Rohan Rao: Let me put it down in 9 basic steps:Just like a Sudoku has 9 digits to work on, I would recommend these 9 steps to work on as much as possible in every DS competition.KJ : How do you gauge the complexity of the problem at the start of the competition ?Rohan Rao : Explore the data as much as possible. Plot features, summarize columns, build benchmark models, and during the process, get a sense of the problem, data, time, complexity, etc. And then slowly build a good solid concrete solution by working on one idea after another.KJ : Which programming language do you prefer to work in ?Rohan Rao : Tools, languages, models, algorithms are developing fast. The more you know, the better. Python and R are the best to start off and master, and I use them both.For quick summarization, plots and prototypes, I use R.For text mining, production models and scalable solutions at work, Ive generally used Python.KJ : Which is your favorite machine learning algorithms and why ?Rohan Rao : XGBoost will take the top spot. The main reason being, it is powerful, robust, fast and a clever algorithm.The beauty of ML solutions is in understanding the problem and data, and exploring it in detail for feature engineering. Traditionally, when I just started out, I was spending far too much time trying out different algorithms and tweaking parameters. Theres not much fun and skill in that. With XGBoost, getting a solid base algorithm became a matter of few minutes, and my focus naturally started shifting to feature engineering, which I believe is the most wonderful and challenging part of building ML solutions today.And of course, its been a part of my winning solution for most of the contests Ive done well in, so a big thanks to the community who are actively developing and improving it each day.I also like Collaborative Filtering techniques, which Ive implemented very often in my work. With a rich dataset, CF-based algorithms can give exceptionally good results for Recommendation Engines.KJ : Why is it important to know the exact functioning each algorithm we are working with ?Rohan Rao : A lot of people are building models as blackboxes. Its easy and it can take you to some level. If you really want to grow and get better, its useful to know how the algorithms work so that you can tie in feature engineering into the models and make better progress.KJ : What are the techniques you follow for feature engineering ?Rohan Rao : I use Excel a lot, very useful for really quick summaries and visuals. If data is huge, I work on a subset, and most of my feature ideas are via plotting or visualization. The exact feature to engineer becomes a bit dependent on the model, but once you spot a trend/pattern, converting it to a feature is a skill that gets developed over time.KJ : What are some good ways for feature selection ?Rohan Rao : My thumb rule of feature selection is based on CV or Val scores. If selecting a feature improves CV score, I use it, else discard. For large number of features, I usually build small quick models and check variable importance or information gain, and select the top-x from them.KJ : How do you deal with high cardinality categorical variables ?Rohan Rao : First try out Label Encoding and One Hot Encoding. In most datasets in my experience, one of these two is good enough. Also, in the meantime, things are developing. Earlier, Rs Random-Forest couldnt handle over 32 factors. Today, H2Os RF handles a categorical variable with 1000+ levels (if I remember right). Maybe someday, that number will be 1 lakh. Who knows!KJ : How to avoid over-fitting ?Rohan Rao : There are lots of methods like outliers-removal, regularization, bagging, etc. These days many of the latest models have parameters to control overfitting besides the fact that algorithms themselves are getting smarter by the day.KJ : What are some points to remember to prepare a robust cross-validation set ?Rohan Rao : Preparing a robust validation set is very important. Its best to replicate the system that is used on the Leaderboards or for the evaluation criteria. There are times when CV and test scores do not work in sync, and often these are uncontrollable due to the nature of the data.KJ : Who is your favorite Data Scientist or role model ?Rohan Rao : There are a few. Hard to choose one. I havent met many of them in person. But there is one who is really special to me. Shashishekhar Godbole (former Kaggle Top-20). He taught me a lot of ML during my initial years. One of the best Data Scientists I know.KJ : You just joined a high prize competition, Who are the 2 other data scientists you would want to see in the competition with you ?Rohan Rao : Wow! Ive never thought of that before. Lets see, Id pick Marios (a.k.a. KazAnova) and Owen.KJ : What is that one advice would you like to give to your younger self ?Rohan Rao : Interesting, Id tell myself to broaden my DS network sooner.KJ : What advice would you give to freshers to get their first break in data science / analytics industry ?Rohan Rao : Data Science is vast. Follow the divide-and-conquer philosophy. Pick small elements and problems, read/research about them in detail, work on ideas, build end-to-end solutions from scratch (including coding, analysing, presenting, etc.) and understand the entire scope and flow of the projects. Slowly and steadily, pick harder problems, pick more challenging competitions and start exploring/improving in areas you enjoy the most.So, LEARN and PRACTISE. Then PRACTISE and LEARN.We decided to add some fun in the AMA with a quick Rapid Fire round. Rohan was expected to answer the first thing that popped in his head after listening to the question. And here we go:KJ : Sudoku or Data Science ?Rohan Rao : BothKJ : 4 GB RAM Mac Book Air vs. 128 GB RAM instance on AWS ?Rohan Rao : 128 GB RAM on AWS/GCPKJ : In a Hackathon  team or individual ?Rohan Rao : IndividualKJ : Mumbai or Bangalore ?Rohan Rao : MumbaiKJ : Most memorable competition ?Rohan Rao:On AV : Seers Accuracy.In general : Telstra on Kaggle.KJ : One secret winning recipe you havent shared with anyone till now ?Rohan Rao : (If I share it, it wont be a secret anymore) Ok, but heres one. I love dropping features. Sometimes, less is better.KJ: Thanks Rohan for that awesome interview and AMA. I am sure the community will benefit tremendously by this. We wish you all the success in your upcoming endeavors and hope to see you in DataHack Summit 2017, 9  11 November in Bengaluru.For those of you, who want to continuously learn from top data scientists and learn by doing data science  check out our latest hackathons here.",https://www.analyticsvidhya.com/blog/2016/10/exclusive-interview-ama-with-data-scientist-rohan-rao-analytics-vidhya-rank-4/
Winners Solution from the super competitive The Ultimate Student Hunt,Learn everything about Analytics|Introduction|The Competition|The Problem Set|Winners|Winners Solutions,"Rank 5 : Akash Gupta ( Roorkee, India )|Rank 4 : Akhil Gupta ( Roorkee, India )|Rank 3 : Mikel Bober ( London, UK )|Rank 2 : Aakash Kerawat ( Roorkee, India ) & Akshay Karangle( Roorkee, India )|Rank 1:Benedek Rozemberczki ( Edinburg, UK )|End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Exclusive Interview with Sr Data Scientist, Paytm  Rohan Rao (DataHack Summit  Workshop Speaker)|Using PySpark to perform Transformations and Actions on RDD|
Kunal Jain
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The Ultimate Victory in a competition is derived from the inner satisfaction, of knowing that you have done your best and made most out of it.Last week, we conducted our first ever student only hackathon  The Ultimate Student Hunt. And it launched with a bang!Young machine learning champions across the world started battling it out to claim a spot at the leaderboard. During the competition, we had close to 1500 registrations and the participants made 15,000 submissions! The student communityexchanged more than 20,000 messages during the contest!Honestly speaking, we were both pleasantly surprised and amazed at the response we got from the participants. We had thought that we would need to do some hand holding for the student community. We couldnt have been more wrong! The quality of codes, the sophistication of their solutions and the sheer thought process and its application took us by surprise. We can confidently say that the future of Machine Learning is in the right hands.To enhance the experience for our participants,we conducted a live AMA with one of our Top Data Scientist  Rohan Rao,winner of last two AV hackathonsand current AVRank 4. Rohanwas a great sportand a huge inspiration to the students. He not only agreed on a short notice, but also spent more than the promised time on AMA and did a rapid fire round at the end!It was one-of-a-kind competition and if you didnt participate in it, trust me you have missed out on a great opportunity.To share the knowledge, the top 5 rankers from the competition have shared their approach and code. Read on to measure whatyou missed out and where you could have improved.The Competition was launched on 24 Sept16 with registrations from data science studentsfrom all over the world. The battle continued for 9 days and all participants were giving each other a cut-throat competition. We launched this competition with anaim tohelp students prove their mettle in machine learning hackathons. We were amazed by the interaction with these young machine learning enthusiasts.By witnessing the commotion created by the competition, students kept on participating even after it had already started. The competition ended on 02 Oct 2016 at 23:59 with a total of 1494 participants.The participants were required to help a country  Gardenia to understand the health habits of their citizens. The evaluation metric used was RMSE.The problem statement was designed foryoung data scienceenthusiasts to helpthem explore, innovate and use their machine learning skills at the fullest.Gardenia is a country which believes increating harmony between technology and natural resources. In the past few years, they have come up with ways to utilise natural resources effectively with technology advancements.The country takes pride in the way, it hasmaintained their natural resources and gardens.And the government of Gardenia wants to use data science to generate insights about the health habits of its people.The mayor of Gardenia wants to know how many people visit the parks in a particular day. They provided some environmental information about the country Gardenia and wanted young data scientists to help them in generating these insights.Although the competition was fierce and challenging, there are always few participants who use a slightly different approach, and come out as champions. Heartiest congratulations to all the winners, we know it wasnt any easy win.Below are the Top 5 winners from The Ultimate Student Hunt.Rank 1  Benedek RozemberczkiRank 2  Aakash Kerawat & Akshay Karangale ( Team  AK )Rank 3  Mikel Bober ( a.k.a. anokas )Rank 4  Akhil GuptaRank 5  Akash GuptaHere are the final rankings of all the participants at the leaderboard.All the five winners have shared their approach and codes they used inthe competition.Akash GuptaAkashGupta is an 4th year undergraduate students at IIT Roorkee, India. He is a data science enthusiast and participates in several competitions to test his knowledge and skills.Heres what he shared :I started with filling the missing values with the pad method ( i.e copying from the previous row ) since the attribute of a park should be similar to what it was the last day. A better approach could have been, to do this individually for each park. Then for the feature selection, I observed that the month of the year and the day of the month were highly influential in determining the footfall. I made a 0/1 feature for winters ( months 11,12,1,2,3 ). For the dates, I observed some pattern, in the variation of mean footfall with increasing dates. But there were some anomalies which I tried to treat by averaging across adjacent days. (I did this for all the parks together, a better approach could have been to do this for each park). I also binned the direction of wind to represent the 4 directions. For the modeling part, I started with gradient boosted trees and further tuned it to get the best result in cv (by testing on years 2000-2001) and then I tried xgboost and tuned it. Finally, I made a 1 hidden layer neural network with a wide hidden layer. I averaged the results of these 3 models to get the final output.In addition to all that, for the gbm and xgboost models,I trained the regressors for each park independently as I believed that each park will have an independent pattern and relationship with other variables. For the neural net model, I trained it for all parks together and giving in park id as a feature as it needed larger number of samples to be trained.Link to CodeAkhil GuptaAkhilGupta is a 4th year undergraduate student atIIT Roorkee. He is interested in exploring in-depths of data and wants to popularize data science amongst young minds.Heres what he shared:My solution approach was simple and straight forward. I focussed most on Data preprocessing and less on model implementation. It was a well balanced dataset with good number of categorical and continuous variables.I spent my initial time on imputing the missing values because some variables had 40% data missing. Pattern observed was that any feature for a park depends on that feature for other parks in the same location on that day.I submitted my first solution just by using Date, Month and Park ID which gave me a public LB score of 146. As I kept on imputing missing values and adding features, I got a huge boost to 113 just by missing values.
There was some noise which was cleaned.As the features varied a lot, I scaled them down to range between 0 to 10.Binning of categorical variables was important as when you observe the median (using Boxplot), you seem decent similarities between parks, months and dates.I didnt spend much time on my model owing to some commitments. Tried a GradientBoostingRegressor, but I am sure that XGB after parameter tuning could have given me a raise of 2-3 points more.Cross-validation was key and I used data for 2000-01 to check my model.Trust me, it was a really catch dataset and kept me busy for 4-5 days. My state of mind was relaxed and chilled out, but the LB was quite competitive which boosted me a lot regularly.Link to CodeMikel BoberMikelBober aka anokas is a young machine learning champion, who at such a young age has achieved high accolades on his name. A Kaggle Master and super genius, he is an inspiration even to the professionals. He kept discussions goingthroughout the competitions and shared his knowledge with all participants.Heres what he shared:In any competition my very first approach, is to submit a preliminary model to test data validations and set a benchmark score to compare it with future complex submissions. I began with XGBoost and I knew that a random split was not going to work. Because the competition is a time series prediction problem.I also split the training set into train and validation sets based on time, taking the last three years of training data as a validation set. My first model had 190 RMSE in validation, and scored 200 on the leaderboard. After this I decided to do some quick feature engineering. The first obvious thing that I had missing was the date variable, which I had excluded previously as it was not numeric. I made one-hot features from the Park ID, so XGBoost could better model each park individually.Then I added a feature importance function to my XGBoost model, and took a look at the importances, Strangely, Direction_Of_Wind was the most important feature, and contributed a lot to the score.After doing all the initial feature engineering, it usually takes some time for me to think of some new, super features. I find that forcefully staring at graphs etc. doesnt help me find new things in the data, but rather that taking a step back, and looking at the problem from a simpler viewpoint usually helps me find the things that others dont. The question you have to ask yourself is What information would affect whether people go to the park?Based on my conclusion it was, if it rained for continuously a week it would have a high impact and I made lead and lag features, which meant that the features for the last two days and next two days in the dataset were also included as features for the current day, allowing the model to learn these cases. The features were a success, improving my score to 109.To apply external weather conditions, I wrote a scraper to go through the weather channels website and download past data going back to the nineties about a bunch of major cities in India. However, none of the data seemed to match, and I hit a dead end.You mustnt be afraid of trying things that will most likely fail. The reason people win is because they had more failed features than you, but they keep going.The last step of any competition for me is to make an ensemble model or meta-model. For this model I went for a very simple ensemble. I used a bagged neural network in Keras. After every epoch, I saved the weights to disk, and after the network finished, I took all the epochs with less than a certain validation loss, and averaged them to make my final neural network predictions. At the end, I took a simple mean of my two models, and used this as the final submission.You can read his complete approach here.Link to CodeAakash KerawatAkshay KarangleAakash Kerawat&Akshay Karangleare 4th year undergrad students at IIT Roorkee, India. Aakash has participated in several machine learning competitions to test his skills in past.They both participated in the competition together.They shared :We started by exploring the data and visualising the target variable with date, we found a clear pattern. Owing to this we created features from date such as day of week, day of year, day of month, month and year. Using these features with the given raw features we tried few models from linear to XGBoost. Among these XGBoost proved to be the best and gave us a public LB score of 133.xx.After this we tried binning the continuous weather variables like Average_Breeze_Speed, Direction_Of_Wind and so on. Based on our intuition we also aggregated months into seasons since the footfall may directly be dependent on it. The visualisations also indicated a clear difference in the mean of footfall corresponding to different seasons. Adding to these we created range features for pressure, breeze speed etc. from given max and min values. These all features gave us an improved score of 123.xx. Tuning the model further improved the score to 118.xx. And this was the score on which we were stuck where nothing seemed to work further. Another set of features that we tried were grouped mean of footfall by Park_ID and binned direction of wind, Park_ID and min moisture etc., but they led to overfitting.As nothing was now working we thought of diving deep into the data with more visualisations. We plotted nearly all variables with date and sensed some possible noise in them. To remove the noise we thought of smoothing the variables with their moving averages / rolling means with windows of 3, 7 and 30 days and this was something which gave us a huge jump in our score from 118 to 107.xx. We didnt stop here and thought of another feature percentage change of weather variables from one day to next day since somehow we wanted to convey the algorithm the change in weather from a day to another. This worked too and our score further improved to 100.xx. We now played with different window for moving averages and checked how they reflected on CV while avoiding overfitting. Our final model had around 43 features which scored 96.xx and 86.xx on public and private LB respectively.What we learned is feature engineering is one of the key elements to improve your score if done in the right way.Link to CodeBenedekBenedek Rozemberczki is a data science research intern at The University of Edinburg. With his expertise and skills Benedek topped the competition.Heres what he shared:
1. The fact that I was able to take the lead early on was motivating. In addition, when other teams gained on me, I was able to come up with novel feature engineering methods.2. The main insight was that xgboost is able to deal with seasonality when the time series in the panel data is fairly stationary besides seasonality. Also it turned out that missing value imputation is useless if you are able to use across observational unit aggregates. Panel data gives neat opportunities for feature engineering.3. Coding in a mindless way and sitting next to your computer is not helpful when you hit a wall. You should try to get inspiration from somewhere else. Even sketching ideas and organizing them on a whiteboard might help.4. Understanding the missing values can help a lot. It also did in this case.5. Automatization of the data cleaning process  writing custom hot-one encoders and column normalizers is also helpful. I have my own function that I gradually use for data science projects. It is important to have a set of functions that You can use everywhere.Link to CodeIt was great interacting with all the winners and participants. We thoroughly enjoyed the competition. If you cant wait for the next hackathon, then stay tuned and check out all the upcoming competitions here.What are your opinions and suggestion about these approaches? Tell us in the comments below.",https://www.analyticsvidhya.com/blog/2016/10/winners-solution-from-the-super-competitive-the-ultimate-student-hunt/
Using PySpark to perform Transformations and Actions on RDD,Learn everything about Analytics|Introduction|Table of Contents|Recap|What is Transformation and Action?|ApplyingTransformation and Action|General transformations|Math / Statistical Transformation|Set Theory / Relational Transformation|Data Structure / I/O Transformation|General Actions|Mathematical / Statistical Actions,"Transformation: map and flatMap|Transformation:filter|Transformation:groupBy|Transformation:groupByKey / reduceByKey||Transformation:mapPartitions|Transformation:sample|Transformation:union|Transformation:join|Transformation:distinct|Transformation:coalesce|Action: getNumPartitions|Action: Reduce|Action:count|Action:max, min, sum, variance and stdev|End Note|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Winners Solution from the super competitive The Ultimate Student Hunt|Deep Learning Guide: Introduction to Implementing Neural Networks using TensorFlow in Python|
Ankit Gupta
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In my previous article, I introduced you to the basics of Apache Spark, different data representations (RDD / DataFrame / Dataset) and basics of operations (Transformation and Action). We even solved a machine learning problem from one of our past hackathons. In this article, I will continue from the place I left in my previous article. I will focus on manipulating RDD in PySpark by applying operations (Transformation and Actions).As you would remember, a RDD (Resilient Distributed Database) is a collection of elements, that can be divided across multiple nodes in a cluster to run parallel processing. It is also a fault tolerant collection of elements, which means it can automatically recover from failures. RDD is immutable, i.e. once created, we can not change a RDD. So, then how do I apply operations on a RDD? Well, we apply an operation and store results in another RDDFor this article, one must have some understanding about Apache Spark and hands on experience in python programming.Lets recall concepts about RDD from our previousarticle:Before applying transformations and actions on RDD, we need to first open the PySpark shell (please refer to my previous article to setup PySpark ).Spark has certain operations which can be performed on RDD. An operation is a method, which can be applied on a RDD to accomplishcertain task. RDD supports two types of operations, which are Action and Transformation. An operation canbe something as simple assorting, filtering and summarizing data.Lets take few examples to understand the concept of transformation and action better. Lets assume,we want to develop a machine learning model on a data set. Before applying a machine learning model, we will needto perform certain tasks:All the above mentioned tasks are examples of an operation. In Spark, operations are divided into 2 parts  one is transformation and second is action. Find below a briefdescriptions of these operations.Transformation: Transformation refers to the operation applied on a RDD to create new RDD. Filter, groupBy and map are the examples of transformations.Actions: Actions refer to an operation which also applies on RDD, that instructs Spark to perform computation and send the result back to driver. This is an example of action.The Transformations and Actions in Apache Spark are divided into 4 major categories:To understand the operations, I am going to use the text file from my previous article. Lets begin, I have already copied and pasted all text from my blog in a textfile called blogtexts. To download this file you can refer to thislink. Before applying operations on blogtexts, we need to first load this file with the help of SparkContext.In above code, PATH is the location of blogtexts. Letsseefirst 5 elements of RDD.Now lets see one by one how transformations and actions work on RDDs.For each transformation, I have first laid out the need of the transformation in the form of a question and then answered it in the subsequent section.Q1: Convert all words in a rdd to lowercase and split the lines of a document using space.To lowerthe case of each word of a document, we can use the map transformation. A map transformation is useful when we need to transform a RDD by applying a function to each element. So how can we use map transformation on rdd in our case?Solution: Lets see through the example, Apply a function called Func on each words of a document ( blogtexts ). Func will do two things:To do this first we need to writeFuncand then apply this function using map.After applying the function (Func) on rdd, we have transformed this rdd into rdd1, we can see the first 5 elements of rdd1 by applying take operation (which is an action).Output is too long so, I have just attached a snippet of it. We can also see that our output is not flat (its a nested list). So for getting the flat output, we need to apply a transformation which will flatten the output, The transformationflatMapwill help here:The flatMap transformationwill return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and map transformations. Lets apply a flatMap transformation on rdd , then take the result of this transformation in rdd2 and printthe result after applying this transformation.You can now observe that the new output is flattened out.Q2: Next, I want to remove the words, which are not necessary toanalyze this text. We call these words as stop words; Stop words do not add much value in a text. For example, is, am, are and the are few examples of stop words.Solution: To remove the stop words, we can use a filter transformation which will return a new RDD containing only the elements that satisfy given condition(s). Lets apply filter transformation on rdd2 and getwords which are not stop words and getthe result in rdd3. To do that:We can check first 10 elements of rdd3 by applying take action.After seeing the result of a filter transformation, we can check now we dont have specified stop words in rdd3 (there are no for and a).Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.Solution: The groupBy transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.After applying groupBy function, we storethe transformed result in rdd4 (RDDs are immutable  remember!). To view rdd4, we can print first (key, value) elements in rdd4.Q4: What if we want to calculate how many times each word is coming in corpus ?Solution: We can apply the groupByKey / reduceByKey transformations on (key,val) pair RDD. The groupByKey will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.To use groupbyKey / reduceByKey transformation to findthe frequencies of each words, you can follow the steps below:Lets see, how to convert rdd3 to new mapped (key,val) RDD. And then we can applygroupbyKey / reduceByKeytransformation on this RDD.In the above code I am first converting rdd3 into rdd3_mapped. The rdd3_mapped is nothing but a mapped (key,val) pair RDD. Then I am applying groupByKey transformation on rdd3_mapped to group the all elements based on the keys (words). Next, I amsaving the result into rdd3_grouped. Letssee the first 5 elements in rdd3_grouped.After seeing the result of the above code, I rechecked the corpus to know, how many times the word manager is there, so I found that manager is written more then once. I figure out that there are more words like manager. , manager, and manager:.Lets filter manager, in rdd3.We can see that in above output, we have multiple words with manager in our corpus. To overcome this situation we can do several things. We could apply a regular expression to remove unnecessary punctuation from the words. For the purpose of this article,I am skipping that part.Until now we have not calculated the frequencies / counts of each words. Letsproceed further :In the above code, I first applied mapValues transformation on rdd3_grouped. The mapValues (only applicable on pair RDD) transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of mapValues transformation, it will transform the values by applying some function and key will not be affected. So for example, in above code I applied sum, which will calculate the sum (counts) for the each word.After applying mapValues transformation I want to sort the words based on their frequencies so for doing that I am first converting a ( word, frequency ) pair to ( frequency,word ) so that our key and values will be interchanged then, I will apply a sorting based on key and then get a result in rdd3_freq_of_words. We can see that 10 most frequent words I used in my previous blog by applying take action.Above output shows that I used words spark 69 times and Apache 52 times in my previous blog.We can also use reduceByKey transformation for counting the frequencies of each word in (key,value) pair RDD. Lets see how will we do this.If we compare the result of both ( groupByKey and reduceByKey) transformations, we have got the same results. I am sure you must be wondering what is the difference in both transformations. The reduceByKey transformations first combined the values for each key in all partition, so each partition will have only one value for a key then after shuffling, in reduce phase executors will apply operation for example, in my case sum(lambda x: x+y).
Source:DatabricksBut in case of groupByKey transformation, it will not combine the values in each key in all partition it directly shuffle the data then merge the values for each key. Here ingroupByKey transformation lot of shuffling in the data is required to get the answer, so it is better to use reduceByKey in case of large shuffling of data.Source:DatabricksQ5: How do I perform a task (say count the words spark and apache in rdd3) separatly on each partition and get the output of the task performed in these partition ?
Soltion: We can do this by applying mapPartitions transformation. The mapPartitions is like a map transformation but runs separately on different partitions of a RDD. So, for counting the frequencies of words spark and apache in each partition of RDD, you can follow the steps:Lets apply above function called func on each partition of rdd3.I have used the glom function which is very useful when we want to see the data insights for each partition of a RDD. So above result shows that 49,39 are the counts of spark, apache in partition1 and 20,13 are the counts of spark, apache in partition2. If we wont use theglom function we wont we able to see the results of each partition.Q6: What if I want to work with samples instead of full data ?
Soltion:sample transformation helps us in taking samples instead of working on full data. The sample method will return a new RDD, containing a statistical sample of the original RDD.
We can pass the arguments insights as the sample operation:We can see the above output, we have total 4768,1895 words in rdd3 and rdd3_sampled.Q 7: What if I want to create a RDD which contains all the elements (a.k.a. union) of two RDDs ?
 Solution: To do so, we can use union transformation on two RDDs. In Spark union transformation will return a new RDD by taking the union of two RDDs. Please note that duplicate items will not be removed in the new RDD. To illustrate this:From the above output, we can see that the sample1, sample2 both have 914 elements each. And in the union_of_sample1_sample2, we have 1828 elements which shows that union operation didnt remove the duplicate elements.Q 8: If we want to join the two pair RDDs based on their key.
 Solution:The join transformation can help us join two pairs of RDDs based on their key. To showthat:Q 9: How to calculate distinct elements in a RDD ?
 Solution:We can apply distinct transformation on RDD to get the distinct elements. Lets see how many distinct words do we have in the rdd3.rdd3_distinct will contain all the unique words / elements present in rdd3. Wecan also check that we have 1485 unique words in the rdd3.Q 10: What if I want to reduce the number of partition of a RDD and get the result in a new RDD?
 Solution:We will use coalesce transformation here. To demonstrate that:2. And now apply coalesce transformation on rdd3 ,get the results in rdd3_coalesce and see the number of partitions.In some previous examples of transformation I already used some of the actions on different RDDs for printingthe result. For example,take to print the first n elements of a RDD ,getNumPartitions to know how many partition a RDD has and collect to print all elements of RDD.Now, I will take few more actions to demonstrate how we can get the results.Q 11: How do I find out number of parition in RDD ?Solution: WithgetNumPartitions, we can find out that how many partitions exist in our RDD. Lets see how many partition our initial RDD (""rdd3"") has.rdd3.getNumPartitions() Output: 2Q 12: If I want to find out the sum the all numbers in a RDD.Solution: To demonstrate this, I will:A reduce action is use for aggregating all the elements of RDD by applying pairwise user function.In the code above, I first created a RDD(num_rdd) from the list and then I applied a reduce action on it to sum all the numbers in num_rdd.Q 13: Count the number of elements in RDD.Solution:The count action will count the number of elements in RDD. To see that, lets apply count action on rdd3 to count the number of words in ""rdd3"".rdd3.count() Output: 4768To take the maximum, minimum, sum, variance and standard deviation of a RDD, we can apply max, min, sum, variance and stdev actions. Lets take the maximum, minimum, sum, variance and standard deviation of num_rdd.Taking a step back, we got introduced to the fascinating world of Apache Spark in the last article.In this article, I have introduced you to some of the most common transformations and actions on RDD. There are many more transformations and actions defined on RDDs, but it is cumbersome (and unwanted) to cover all of them in one article. To learn more about transformations and actions, you can refer RDD APIdocin Python.I suggest you to apply these operations at your end inRDD, and get hands on experience on what are the challenges you are facewhile applying these. Let me know your doubts & any challenges you face in the comments section and I would be happy to answer them.Also,if you have any questions or suggestionsabout other features of RDD that you would like to know about, please drop in your comments below. In the next article, Ill discuss about Dataframe operations in PySpark.",https://www.analyticsvidhya.com/blog/2016/10/using-pyspark-to-perform-transformations-and-actions-on-rdd/
Deep Learning Guide: Introduction to Implementing Neural Networks using TensorFlow in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|When to apply Neural Networks ?|General way to solve problems with Neural Networks|Understanding Image data and popular libraries to solve it|What is TensorFlow?|A typical flow of TensorFlow|Implementing Neural Network in TensorFlow|Limitations of TensorFlow|TensorFlow vs. Other Libraries|Where to go from here?|Useful Resources|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Using PySpark to perform Transformations and Actions on RDD|Most Active Data Scientists, Free Books, Notebooks & Tutorials on Github|
Faizan Shaikh
|81 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have been following Data Science / Machine Learning, you just cant miss the buzz around Deep Learning and Neural Networks.Organizations are looking for people with Deep Learning skills wherever they can. From running competitions to open sourcing projects and paying big bonuses, people are trying every possible thing to tap into this limited pool of talent.Self driving engineers are being hunted by the big guns in automobile industry, as the industry stands on the brink of biggest disruption it faced in last few decades!If you are excited bythe prospects deep learning has to offer, but have not started your journey yet  I am here to enable it.Starting with this article, I willwrite a series of articles on deep learning covering the popular Deep Learning libraries and their hands-on implementation.In this article, I will introduce TensorFlow to you. After reading this article you will be able to understandapplication of neural networks and use TensorFlow to solve a real life problem. This article will require you to know the basics of neural networks and have familiarity with programming. Although the code in this article is in python, I have focused on the concepts and stayed as language-agnostic as possible.Lets get started!An Introduction to Implementing Neural Networks using TensorFlowNeural Networks have been in the spotlight for quite some time now.For a more detailed explanation on neural network and deep learningread here.Its deeper versions are making tremendous breakthroughs in many fields such as image recognition, speech and natural language processing etc.The main question that arises is when to and when not to apply neural networks? This field is like a gold mine right now, with many discoveries uncovered everyday. And to be a part of this gold rush, you have to keep a few things in mind:Neural networks is a special type of machine learning (ML) algorithm. So as every ML algorithm, it follows the usual ML workflow of data preprocessing, model building and model evaluation. For the sake of conciseness, I have listed out a TO DO list of how to approach a Neural Network problem.For this article, I will be focusing on image data. So let us understand that first before we delve into TensorFlow.Images are mostly arranged as 3-D arrays, with the dimensions referring to height, width and color channel. For example, if you take a screenshot of your PC at this moment, it would be first convert into a 3-D array and then compress it .jpeg or .png file formats.While these images are pretty easy to understand to a human, a computer has a hard time to understand them. This phenomenon is called Semantic gap. Our brain can look at the image and understand the complete picture in a few seconds.On the other hand,computer sees image as just an array of numbers. So the problem is how to we explain this image to the machine?In early days, people tried to break down the image into understandable format for the machine like a template. For example, a face always has a specific structure which is somewhat preserved in every human, such as the position of eyes, nose or the shape of our face. But this method would be tedious, because when the number of objects to recognise would increase, the templates would not hold.Fast forward to 2012, a deep neural network architecture won the ImageNet challenge, a prestigious challenge to recognise objects from natural scenes. It continued to reign its sovereignty in all the upcoming ImageNet challenges, thus proving the usefulness to solve image problems.So which library / language do people normally use to solve image recognition problems? One recent survey I did that most of the popular deep learning libraries have interface for Python, followed by Lua, Java and Matlab. The most popular libraries, to name a few, are:Now, that you understand how an image is stored and which are the common libraries used, let us look at what TensorFlow has to offer.Lets start with the official definition,TensorFlow is an open source software library for numerical computation using dataflow graphs. Nodes in the graph represents mathematical operations, while graph edges represent multi-dimensional data arrays (aka tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.If that sounds a bit scary  dont worry. Here is my simple definition  look at TensorFlow as nothing but numpy with a twist. If you have worked on numpy before, understanding TensorFlow will be a piece of cake! A major difference between numpy and TensorFlow is that TensorFlow follows a lazy programming paradigm. It first builds a graph of all the operation to be done, and then when a session is called, it runs the graph. Its built to be scalable, by changing internal data representation to tensors (aka multi-dimensional arrays). Building a computational graph can be considered as the main ingredient of TensorFlow. To know more about mathematical constitution of a computational graph, read this article.Its easy to classify TensorFlow as a neural network library, but its not just that. Yes, it was designed to be a powerful neural network library. But it has the power to do much more than that. You can build other machine learning algorithms on it such as decision trees or k-Nearest Neighbors. You can literally do everything you normally would do in numpy! Its aptly called numpy on steroidsThe advantages of using TensorFlow are:Every library has its own implementation details, i.e. a way to write which follows its coding paradigm. For example, when implementing scikit-learn, you first create object of the desired algorithm, then build a model on train and get predictions on test set, something like this:As I said earlier, TensorFlow follows a lazy approach. The usual workflow of running a program in TensorFlow is as follows:Few terminologies used in TensoFlow;Lets write a small program to add two numbers!Note: We could have used a different neural network architecture to solve this problem, but for the sake of simplicity, we settle on feed forward multilayer perceptron with an in depth implementation.Let us remember what we learned about neural networks first.A typical implementation of Neural Network would be as follows:Here we solve our deep learning practice problem Identify the Digits. Lets for a moment take a look at our problem statement.Our problem is an image recognition, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model. So first, download the train and test files. The dataset contains a zipped file of all the images in the dataset and both the train.csv and test.csv have the name of corresponding train and test images. Any additional features are not provided in the datasets, just the raw images are provided in .png format.As you know we will use TensorFlow to make a neural network model. So you should first install TensorFlow in your system. Refer the official installation guide for installation, as per your system specifications.We will follow the template as described above. Create a Jupyter notebook with python 2.7 kernel and follow the steps below.The above image is represented as numpy array, as seen belowThis will be the output of the above codeAnd done! We just created our own trained neural network!Most of the above mentioned are in the sights of TensorFlow developers. They have made a roadmap for specifying how the library should be developed in the future.TensorFlow is built on similar principles as Theano and Torch of using mathematical computational graphs. But with the additional support of distributed computing, TensorFlow comes out to be better at solving complex problems. Also deployment of TensorFlow models is already supported which makes it easier to use for industrial purposes, giving a fight to commercial libraries such as Deeplearning4j, H2O and Turi. TensorFlow has APIs for Python, C++ and Matlab. Theres also a recent surge for support for other languages such as Ruby and R. So, TensorFlow is trying to have a universal language support.So you saw how to build a simple neural network with TensorFlow. This code is meant for people to understand how to get started implementing TensorFlow, so take it with a pinch of salt. Remember that to solve more complex real life problems, you have to tweak the code a little bit.Many of the above functions can be abstracted to give a seamless end-to-end workflow. If you have worked with scikit-learn, you might know how a high level library abstracts under the hood implementations to give end-users a more easier interface. Although TensorFlow has most of the implementations already abstracted, high level libraries are emerging such as TF-slim and TFlearn.I hope you found this article helpful. Now, its time for you to practice and read as much as you can. Good luck! If you follow a different approach / package / library to get started with Neural Networks, Id love to interact with you in comments. If you have any more suggestions,drop in your comments below. And to gain expertise in working in neural network dont forget to try out our deep learning practice problem Identify the Digits.",https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/
"Most Active Data Scientists, Free Books, Notebooks & Tutorials on Github",Learn everything about Analytics|Introduction|Table of Content|Githubs Story|Tutorials / Repositories|Free Books|Most Active Data Scientiststo Follow|End notes,"For Starters|ForR Users|For Python Users|More Useful Repositories|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Deep Learning Guide: Introduction to Implementing Neural Networks using TensorFlow in Python|A Beginners guide to Shelf Space Optimization using Linear Programming|
Analytics Vidhya Content Team
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whos your favorite data scientist? asked the recruiter. None of the candidates could give a satisfactory answer.May be, they thought becoming a data scientist has nothing to do with following them. Is it?Think back, when you were a kid and played sports, didnt you admire any sports player andaimedto be like him / her, when you grow up? I am sure you did. Actually, it helped you in two ways:The path to becoming a data scientist is exhausting, just like a marathon. To ensureyou dont fall out, it is important that you keep seeking motivation from what others are doing.In this article, Ive listed the most active data scientist on github, so that you can follow & see what are they upto (specially projects). Also, Iveenlisted the best github repositories, free books, notebooks to help you become better at machine learning & data science.Github is the de-facto social network for coders! You can connect, follow & learn from many successful coders and data scientists on the platform. Started in 2008, Github:Though, the most common languages on GitHub are Python, PHP, Javascript Y C++; R & Python (for data science) are steadily establishing their authority. Over the years, githubhas become an incredible source of usefulknowledge on machine learning.I was amazed to see the extent of knowledge freely available on github.Before moving forward, check out this ~ 2 minutes video on students using Github!Open Source Data Science  This repository encourages you to leverage open source education and become a self taught data scientist. Easier said than done. But, you need to be stay consistent in your efforts, follow the pedagogy as described. If you are a working professional, create a schedule and stick to it. If you are a student, invest as much time as you can.Awesome Data Science This repository familiarizes you with practical aspects of data science. It provides you data sets, ways to engage with communities, colleges etc. In addition, it has an interesting infographic section focused on job opportunities in data science industry.Machine Learning Packages  This repository comprises of an exhaustive list R packages for machine learning. Many a times, we find ourselves stuck at caret or e1071 packages. But, turns out there are many other ML packages which are equally powerful and canreduce our modeling time.Awesome R  Here youll find all the useful resources to learnR in a comprehensive manner. Not just predictive modeling, this repository contains tutorials on building web apps, visualization, programming, database management etc. R is a multi-purpose language. Most of us confine ourselves to predictive modeling, using this repository you can explore its various sides.Data Science in R  This repository takes you deeper into specifics of model building in R. It comprises several hot questions on topics like data exploration, data manipulation, time series analysis etc. Along side, youll also find additional tutorials missed in the above two repositories.Practice H2o  If H2o has helped you in reducing computational time, you might be interested inmastering this powerful package. This repository contains practical examples (airlines delay, bad loans, Citibike demand) using which you can explore various h2o features in model building.Awesome Machine Learning in Python As evident by its name, this repository enlists all the useful tutorials on doing machine learning, computer vision, natural language processing (NLP) in python. Considering the rapidly increasing usage of python in data science, its a good resource if you too are trying to enhance your python skills .IPython Notebooks What could be better than learning by doing? Yes, this repository contains ipython notebooks on ML algorithms (scikit learn) by solving various problems including titanic kaggle. Moreover, it also contains tensor flow notebooks to build scalable ML models in python.The focus of this repository is kept on exploring broad aspect of python in machine learning.Tutorials in Notebooks  More notebooks for you to practice and amplifyyour breadth of knowledge in machine learning.Interesting ipython notebooks  Even more notebooks.Data Science in Python  This repository consists of ML algorithms wise (neural network, decision trees, linear regression etc) list of tutorials to give you a clear view of how an algorithm works. Also, it introduces you to most common tasks in data manipulations and how to do them in python.TensorFlow Examples  TensorFlow ( library made for numerical computation) has rapidly gained popularity among machine learning practitioners in Python.This repository will help you get started with tensorflow and its features. This repository is best suited for beginners keen to learn tensorflow and looking for practical examples with concise explanation.useR 2016 Machine Learning  This repository consists of machine learning tutorials delivered at The R User Conference 2016. Mainly, it explains 6 popular supervised machine learning methods in R. Along with, several best practices which one should follow while model building.Machine Learning University Courses  This repository enlists all the ML programs undertaken at top universities around the world. Some of these universities also share course content online, which will also find here. It consists of the top courses undertaken at various universities. This repository should help you understand their course curriculum and depth of topics covered.Notebooks on Statistics & ML  This notebook demonstrates statistical concepts in python. The notebook shared above are focused on only machine learning methods. But, this repository contains notebooks which shows how statistical analysis can be done in python. For best results, you must have prior knowledge of statistics and related concepts.If you dont like reading books, you can skip to next section.Along with books,in this section, Ive listed the repositories which comprises of complete practical exercises done in some MLbooks. These notebooks would give you complete overview of implementation of ML methods. For theoretical understanding, you can read these book at your convenience.Free Data Science Books  This repository comprises of downloadable books on subjects like statistics, machine learning, data mining etc. If you like reading books, and prefer to gain knowledge from books than any other method, you have a lot to take home from this repository.Exercises from ML for Hackers  This book is written by John Myles White. If you have read this book, wonderful! In case you havent. nothing to worry. These exercises are simple and effective enough to make you understand the implementation of a particular method. Its good for people who learn better by doing than reading.Exercises from Probabilistic Programming  This book is written by Cam Davidson Pilon. This repository consists of exercises from described in his book Probabilistic Programming and Bayesian Method for Hackers. If you understand probability in depths, you must do these exercises and see how is it being used by machine learning.Machine Learning Books  This repository has 10 books on machine learning available for download.ML in Python  This repository consists of coding exercises from the book Introduction to Machine Learning in Python written by Andreas C Mueller and Sarah Guido. This is good for people who want to start with ML in python as the coding exercises are quite easy.Python Projects  Keen to do interesting python projects but dont know where to start ? Check out some interesting projects done in python, understand them and may be they could inspire you to start one on your own. In other words, these projects are nothing but recipes taken from the IPython Cookbook written by Dr. Cyrille Rossant.Below is the list (in no order) of most active data scientists on github. If you check their profiles, youd realize that they have avidly contributed knowledge in form of books, projects, tutorials for the welfare of worldwide ML community. Most of these people have accomplished something or the other to make our life easier. Some of the people also featured in our list released last year, because some people never lose their charm!The activeness attribute of a data scientist is measured by number of repositories (> 1000) added in last one year. However, few selection were made on the basis of their noted achievements also. The idea is to connect with them, keep an eye on their projects and this could provide you a career opportunity. Everyone seeks help. Right?The repositories, books, notebooks are selected on the basis of their usefulness to respective topics (R, Python, Machine Learning) along with their stars and forks. If you want to make the best use of these repositories, make a time table and define the dates according to which youll cover the chapters. Remember, discipline and consistency are the key to phenomenal success.Did you like reading this article? Did you find it useful? Please share your opinions / suggestions in the comments below.",https://www.analyticsvidhya.com/blog/2016/09/most-active-data-scientists-free-books-notebooks-tutorials-on-github/
A Beginners guide to Shelf Space Optimization using Linear Programming,Learn everything about Analytics|Introduction|Table of Contents|The basic concepts|Shelf Space Optimization||Defining the Problem & solving it|Other applications of optimization problems||End Notes,"Optimization|Linear Programming|Challenges|Lifts Table||Linear Optimization using Excel|Linear Optimization using Pulp library in Python|Challenges with Large Datasets|Greedy Algorithm||Conclusion|About the Author|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Most Active Data Scientists, Free Books, Notebooks & Tutorials on Github|AI startups are in the money: What are you doing?|
Guest Blog
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Decision Variables|Objective Function|Constraints|Set Objective,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever wondered why products in a Retail Store are placed in a certain manner? In the world of analytics, where retail giants like Walmart, Target etc. are collecting terabytes of data on a daily basis, every decision in the brick and mortar stores is carefully thought through and analyzed. Further, with an increasing number of smart shopping outlets, the data collection and the level of analysis have both become far more granular.Shelf space maximisation is one of key factors behind any marketing strategy for a brand. In this article, I will explain some challenges in shelf space optimization and then solve a toy example using excel, python and greedy algorithm. Read on to find detailed description along with the codes.Let me start by introducing the concepts we would be using later on:In this section, Ill introduce some terms Ill be using later in the article.Optimization is the science / process behind finding the best solution for a problem with given constraints. We come across optimization problems on a daily basis. These can be for finding the shortest path between your work place and office; maximizing revenues / customer happiness or minimizing costs / debts etc. We basically take a real world problem, model it mathematically and then solve it using mathematical techniqueswith in the constraints. Optimization is useful in Marketing, Manufacturing, Finance, Online advertising, Machine Learning and all fields you can imagine.Linear programming (LP) (also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programs can be expressed by:A linear programming algorithm finds a point in the feasible space where the Objective function has the smallest (or largest) value if such a point exists. Simplex Algorithm is the most commonly used algorithm to solve Linear Programming.Integer Programming is a special case of Linear Programming where the decision variables are restricted to be Integers. We will deal with an Integer Programming problem with only binary 0-1 outcomes.In a store, a products position in store can greatly affect its performance. Having the right space allocation for products and categories plays a critical role in its retail success. From retailers perspective, given the value of shelf space positions, it is very critical to ensure that retail space is working for valuemaximization for the store.The shelves near the POS offer maximum visibility to the customers and help the stores reap in those extra few dollars for items which were not even in the shoppers list. Marketing the right merchandise, at the right place, at the right time, in the right quantities is key to retail revenues and profitability. This has led to a war between brands to occupy the best possible space in a store. On the other hand, the stores also have to optimize their overall profitability considering the sales of all merchandise.The logic is comprehensible, but applying it can be difficult because the information needed for Space Optimization is most times unclear, complex or scattered throughout the business. Certain products may play a vital role (being essential to promotions program for instance); others may be duplicates / clones, but providehigher margins etc. Hence it may become difficult to measure them on a single parameter. Besides, an average retailer stocks around 30,000 SKUs (different products). Thousands of new items are introduced at retail every year. Optimizing a problem of that size becomes extremely difficult and often requires SMEs, Consultants and Statisticians to brainstorm a lot.This is a toy problem but the same concept can be expanded for a problem of bigger size. Let us understand the problem.Let us assume a retail store with 3 racks, Rack 1, Rack2 and Rack3 with 3, 3 and 4 shelves as shown in the below table. We have to stock products of 3 companies Unilever, Godrej and Dabur. Unilever, Godrej and Dabur has 3, 3 and 2 products respectively. The numbers that we see in the matrix are the lifts (increase in sales) that we achieve on placing a specific product on a specific rack / shelf (given by corresponding row).Now due to a difference in profit margin / inventory cost / demand / expiration date etc. of products, the store wants to optimize the placement of each product on the shelves and maximize the total sales (number of products) taking into account the constraints it has got.The decision variables will be in the form of a matrix of the same size as lift (10*8). The matrix will have binary values, 1 indicating a YES for the product/shelf pair and 0 indicating a NO. We will start with a matrix of all 0s and allow the solver to make changes to 1s where required.The objective function to be maximized is the Total Sales of all merchandize.The constraints used here are:This boils down to the conditions that Product 1 from Unilever cannot be marketed more than once. Similarly for the other products the constraints apply.There can be several more constraints applied as per the business understanding of a store and merchandizing best practices. However for this learning problem, this would suffice.Constraints can be taken care using the above two tables in excel.Let us go to the Solver in Excel. Go to DATA Solver. If its not visible you need to activate it by going to File  Options  Add-InsThis is how it looks like.The Objective function is given by the sumproduct of the lift and the decision variable matrix. Select the cell in spreadsheet which indicates this.We have to maximize the Profit.Decision variable is the matrix of same size as the lift. Select all cells representing it.For constraints select the cells that represent the Sum of rows and Sum of columns in the decision variable matrix. Assign them <= inequality. For all rows the sum <=1 and for columns it is given by the list of constraints as given in problem.Add another constraint to make the decision variables binary integers. (0s and 1s).Select Simplex LP and run.The objective function along with the constraints is solved and the maximum sales obtained is 4197. The decision variable matrix obtained is shown below:That was easy. But Excel has its limitations and cannot be used for a problems of large size. Also if there are too many constraints it will be a humongous task to take that in excel. Thats where Python comes to the rescue.Spreadsheet optimization is too cumbersome to use for day to day operation. Python can easily be used for large problem size and will only be limited by the computing limitations. Also once coded / automated it can be run for problems of varying sizes. Any new constraints can also be taken care later as and when they arise. I use Pulp library in python and its open source solver CBC to arrive at the best possible solution. There are other commercial solvers available like CPLEX, GUROBI etc. which are useful for very large problems as they provide speedier / better results.The python codes for as follows:#import all relevant librariesimport pandas as pdimport numpy as npimport mathfrom math import isnanfrom pulp import *from collections import Counterfrom more_itertools import unique_everseensales=pd.read_csv(""sales_lift.csv"",header=None) #input filelift=sales.iloc[2:,1:]lift=np.array(lift)lift = lift.astype(np.int) # read the lifts from csvbrands=sales.iloc[0:1,:]brands=np.array(brands)brands=np.delete(brands,0)brands=brands.tolist() # read the brands from csvff=Counter(brands)all_brands=ff.items()# the racks and the shelfs availablerack_shelf=[[1,1,2,3],[2,4,5,6],[3,7,8,9,10]]#define the optimization functionprob=LpProblem(""SO"",LpMaximize)#define decision variablesdec_var=LpVariable.matrix(""dec_var"",(range(len(lift)),range(len(lift[0]))),0,1,LpBinary)#Compute the sum product of decision variables and liftsprodt_matrix=[dec_var[i][j]*lift[i][j] for i in range(len(lift))for j in range(len(lift[0]))]#total lift which has to be maximized sum(prodt_matrix)#define the objective functionprob+=lpSum(prodt_matrix)order=list(unique_everseen(brands))order_map = {}for pos, item in enumerate(order): order_map[item] = pos#brands in order as in input filebrands_lift=sorted(all_brands, key=lambda x: order_map[x[0]])DEFINE CONSTRAINTS1) Each shelf can have only one product i.e. sum (each row)<=1for i in range(len(lift)): prob+=lpSum(dec_var[i])<=12) Each product can be displayed only on a limited number of shelves i.e. Column constraintsConstraints are given as col_con=[1,0,0,2,2,3,1,1]dec_var=np.array(dec_var)col_data=[]for j in range(len(brands)): col_data.append(list(zip(*dec_var)[j])) prob+=lpSum(col_data[j])<=col_con[j]#write the problemprob.writeLP(""SO.lp"")#solve the problemprob.solve()print(""The maximum Total lift obtained is:"",value(prob.objective)) # print the output#print the decision variable output matrixMatrix=[[0 for X in range(len(lift[0]))] for y in range(len(lift))]for v in prob.variables(): Matrix[int(v.name.split(""_"")[2])][int(v.name.split(""_"")[3])]=v.varValue matrix=np.int_(Matrix)print (""The decision variable matrix is:"")print(matrix)The results from python and Excel match exactly. This reinforces that the result obtained is the global maximum (lift), 4197 as the Total Lift.Let us understand what problems arise with large datasets. As in this example we understand that each decision variable can take values 0 or 1 that is 2^1 or 2 possible values. For 2 decision variables the total number of possible combinations can be 2^2 or 4 out of which one/more may give the optimized value of the Objective function. With 80 decision variables in our example, the total combinations is 2^80. This shows that the order of the problem is exponential and not linear. [In language of Computational Complexity Theory, exponential time O (2^n)]. Problems of exponential order are very intensive even for the best of computers. As in our example each of the 2^80 combinations will be evaluated to find the optimized solution.Thats where business understanding and domain knowledge comes into picture. A SME should be able to quickly reject some of the combinations by applying appropriate constraints to the problem and hence limiting the total # of possible solutions.Let us see how a greedy algorithm would perform under the same constraints. A greedy algorithm, as the name suggests tries to maximize the lift in each step irrespective of the total gain. This may or may not (in most cases) give the global optimum. Our greedy algorithm will attack the problem in the following way:I have coded the above greedy algorithm in python using a recursive function.Interestingly, the greedy algorithm gives the same results as the solver. However I tried changing the column constraints and the greedy algorithm gave slightly lesser total lift than that by the solvers.Now that you have seen a basic implementation of an optimization problem, let us understand applications in a few other domains:Google AdWords: Google uses LP for its online advertising. Google has different slotson its search page and based on the PPC (price per click), CTR (Click through Rate) and Budget (constraint) of the advertisers Google allots the slots and the number of times (decision variables) to display the add while maximizing its revenue (objective function). AdWords account for ~97% of Googles revenue.Airlines Revenue Management: Airlines use linear optimization to offer limited discounted tickets (decision variable) and also maximize their revenues (objective) for a forecasted demand (constraint) and plane type (limited seats, also constraints).Cancer treatment: LP is used to treat cancer patients by radiation. Tumorous cells are targeted with a desired radiation level (constraint) and at the same time the healthy tissue should be exposed to least radiation (minimize objective function).Promotion of ads on Television Channels: A TV channel has tens of Shows and thousands of promotions and commercials to market. Through linear optimization they decide which promotion to telecast in which slot and maintain a high audience viewership (Objective). Constraints come in the form of Budget of each promotions, max number of times a promotion can be shown etc.Dating Sites: Linear Optimization is also used by online dating sites like eHarmony. Based on a questionnaire which each user fills, eHarmony computes a compatibility score between two people and uses optimization algorithms like Linear Programming to determine their users best matches. Possible constraints here are Men are matched to only Women; One Man is matched to one Woman and vice versa.The toy problem can be expanded into a problem for the entire store where there would be thousands of Racks/Shelves etc. The constraints will also accordingly increase to thousands. This will allow the store to place an item at the right place and derive maximum total sales.I hope this will be a good reference material for beginners in Optimization. I am also in the process of exploring this further and doing more complex problems on it. LP has been an inherent part of Operations and Inventory management and many organizations have their own in-house tools for it. I hope you enjoyed reading this article and found it helpful. I would love to hear from you, if you have any questions / feedback / doubts feel free to drop in your comments below.Deepesh Singh is a Data Science enthusiast. He is a continuous learner and loves exploring diverse areas of Data Science. An engineer from NIT Silchar and armed with a one year certificate in Business Analytics from IIM-L and KSB (Indiana University), he currently solves business problems at Bangalore office of an Analytics organization. Outside of work, he loves Toast-mastering, working out at gym, practicing/teaching karate.",https://www.analyticsvidhya.com/blog/2016/09/a-beginners-guide-to-shelf-space-optimization-using-linear-programming/
AI startups are in the money: What are you doing?,Learn everything about Analytics|Introduction|Why are AI startups getting soldlike hot cakes?|Does this inspire you to start your own AI startup?|List of Startups which got acquired|AI Startups which can actuallymake BIG (For Investors)|End Notes,"Google|IBM|Apple|Intel|Yahoo|Microsoft|Amazon|Twitter|Salesforce|1. H2o.ai|2.DataRobot|3. Ayasdi|4. iCarbonX|5. Butterfly Network|6. Skytree|7.analyticsMD|8. Darktrace|9. 6Sense|10. X.ai|Looking for a job in analytics? Check out currently hiring jobsin machine learning and data science|Share this:|Like this:|Related Articles|A Beginners guide to Shelf Space Optimization using Linear Programming|Solutions for Skill test: Data Science in Python|
Kunal Jain
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You are either investing in AI or you are not. If you are, you are making a bet which might continue to pay off for next 10  20 years. If you are not, you are just signing your own Death Certificate.Let me explain why I make such a strong statement and like an analyst, I will start by throwing a few numbers. Here are a few predictions madeby Gartner recently. By 2018, more than 3 Mn workforce will be supervised by robo-boss or 20% of business content will be authored by machines! I am feeling my job is up for grabs by machine!Still not convinced? In last 5days, there has been news of 2 more AI startups have been acquired by Google & Amazon respectively.In fact, since 2011, more than 60 artificial intelligence companies have been acquired by tech giants including Google, Yahoo, IBM, Apple etc.There are many new ventureswhose acquisition contract might be in draft as I you are reading.Needless to say,companies are seeing huge potential in nurturing AI capabilities to become more powerful.But why?Whats so good about AI startups that they are being soldlike hotcakes? I thought about it and decided topresent my views in this article. Read on, you might get that breakthrough idea to take your career to next level!AI startups are getting soldlike hot cakes because they are building products, which are capable of solvingbusiness problems as efficiently (or even better) than human beings in various aspects. Large scale automation and Internet of Things and enabling a world of limitless possibilities.These startup ideas are innovative. These peopleare thinking ahead of time. With an awesome product, they may fail to create a large impact, but backingof a gianttech shark provides them accessto much neededresources.The recipe of building such products come from machine learning. Yes, using data and machine learning, even you can also build a program, which trains, learns and deliverresults better thanhumans. But, its not easy.Had it been easy, every other block in your neighborhood would be prospering with an AI startup. Lets look at some of theproblems being solved by AI startups:and, there are many others. Did you study a common pattern? All of these problems are the ones which prevailed over the decades, but havent found a solid solution. Thats why artificial intelligence has taken the initiative. Do humans really need to worry? I wonder!Trust me, timing couldnt have been better. If you are into Machine Learning, you already know the potential of what you are working on. If you are still not building products on top of your models, you are in a similar situation as that of a kid with a key. The child is fascinated by the looks of the key and theydoes not open the door in front of him.In order to inspire you further, I have created a list of startups which were bought in recent past. These startups have put their technical skills to practical use and are solving big real world problems. Have a look at them and you would know, what I am talking about.I hope you would be as inspired as I was while creating this list. This is your opportunity to make it big  identify your idea and start building a product around it.The list below comprises of topgiants from corporateindustry today. Beyond these companies, there are several other companies doing such acquisition in AI space but have goneunreported. Therefore, consider this list as just the tip of the iceberg.With no surprise, Google remains the largest shark in tech ocean. In last 5 years, it has acquired 9 AI startups. Lets look some of the startups acquired and what do they do:Other Acquisitions:Cleversense, DNNresearch, Emu, Jetpac, Dark Blue Labs, Vision Factory, Timeful, Granata Decision SystemsOther Acquisitions: ExplorysOther Acquisitions: VocalIQ, Emotient, PerceptioOther Acquisitions: SaffronOther Acquisitions: SkyphraseOther Acquisitions: TellApartThese are well funded startups but havent been acquired yet. Can make big? Chances are highbecause these big companiesare not ready to let go of even the smallest value provided by AI products.Among all sectors, it seems healthcare industry is completely soaked by path breaking AI startups.Below is the list of top 10 startups which I feel has an incredible product to offer and they can actually make BIG:Its the most popular startup in machine learning fraternity these days. In total, company has raised $34 Millions. It provides an open source platform enabled with fast scale machine learning for data scientists. Alongside, they offer multiple products such as Sparkling Water, which combines the sheer power of Apache Spark with H2o platform. The have realized its product APIs to make it accessiblethrough R, Python, Java as well.DataRobot is one of the fastest growing companies in US in analytics industry. Currently, this company is valued more than $60 million. It provides a cloud based machine learning software to generate predictive models. With intelligent algorithms and automation, they have made predictive modeling process faster than ever. The company is led by top kagglers around the world.Founded by mathematicians, this startup has raised $97.9 million till now. Ayasdi provides a machine intelligencesoftware which solves most complex and impact analytical challenges faced by Fortune 500 companies.Also, it is expert atdetecting, analyzing and exposing patterns from topological data that humans may miss. Last year, the company reported 400% growth in their bookings.This China based startup has brought revolution in the healthcare sector. This startup has raised $199 million till now. It provides an artificial intelligence platform which uses health data and make predictions about plausible disease strokes. Managing our lives digitally is the prime motto of this company. It is also aimed at creating a professional data collection platform of life long data from worlds largest chinese population.Noted bioscience entrepreneur Jonathan Rothberg along with a group of physicists , founded this startup which develops a new kind of medical imaging device that will see the human body in completelynew ways. Then, it uses artificial intelligence to extract clinical insights from it. This startup has raised $100 million till now. This startup also plans to develop non-invasive surgical technology.Skytree is a machine learning company which provides a predictive analytics software to help companies leverag the power to discover deep analytic insights,predict future trends, make recommendations and reveal untapped markets and customers. This startup has raised $18million in Series A funding in 2013. Its platform is designed to handle large amounts of data  structured or unstructured so that data scientist never have to work on samples.Emerged out of YCombinator, this startup is dedicated to tackle a critical but crucial problem i.e. devising ways to improve the operational efficiency of hospitals using AI. They provide a real time analytics platform which constantly monitors the demand fluctuations and inform hospital officialsin real time. This allows hospital make necessary changes in order to cater to the demand. This start has raised $840,ooo till now.This is a cyber security startup aimed atdeveloping technology to help companies respond to not only human-written cyberattacks but also the pending threat of machine-learning-based attacks that. Right now, they offer Enterprise Immune System that is installed in a companys network, which then learns, trains and makes sense of what the web traffic is upto. In case of malicious detection, it takes immediate actions to counter it. This start up has raised $104.5 million till now.This startup has emerged as an early leader in the fastest growing market for predictive B2B marketing and sales intelligence. They have built a Buyer Intent Network based on robust data science and machine learning techniques, capable to analyze &capture time-based, structured and unstructured behavioral data from thousands of sources. Currently, their platform processes billions of rows of buyer every month collected from online communities. This startup has raised $36 million till now.This start provides a smart AI enabled virtual assistant Amy whichhandlesall your meeting schedule, does the negotiations of time and place, and send out meeting invitation all with its inbuilt intelligence. Have you seen movie Her? You could relate better if youve seen it. This startup is based out of New York. Until now, it has raised $34.4 million in 3 rounds of funding.It wouldnt be incorrect to say that the world is progressing fast towards automation. Building products which work more efficiently than humans lies in the core of AIstartups. After traversing through these startups, you would have realized there is no problem being solved. All these problems have existed for long, its just some kids believe they have the technology which can solve the problem.If you were also thinking to solve challenging problem, think AI, who knows your idea might get noticed by YCombinator accelerator program. In this article, I discussed about the rising startup craze in AI domain and what can you learn from it.I hope you like reading this article. Do share your suggestions / experiences of AI startups in comments below.",https://www.analyticsvidhya.com/blog/2016/09/what-should-you-learn-from-the-incredible-success-of-ai-startups/
Solutions for Skill test: Data Science in Python,Learn everything about Analytics|Introduction|The Skill test|Skill test Questions and Answers||End Notes,"Overall Performance|Share this:|Like this:|Related Articles|AI startups are in the money: What are you doing?|18 Free Exploratory Data Analysis Tools For People who dont code so well|
Faizan Shaikh
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Python is gaining ground very quickly among the data science community. We are increasingly moving to an ecosystem, where data scientists are comfortable with multiple tools and use the right tool depending on the situation and the stack.Python offers ease of learning, large ecosystem (web development / automation etc.), an awesome community and of course multiple computing libraries. It is not only becoming the preferred tool for newbies wanting to learn data science, but also among the professional data scientists. Python offers the best eco-system, if you are looking to work on / learning deep learning.With this in mind, it was only a matter of time that we came out with a skill test for Python. So, we conducted our first Python Skilltest on 25th September and guess what the winner came out flying, karim.lulu scoring 44 out of 45 questions!If you use Python as your preferred tool for data science or are learning it, here is a chance to check your skills (in case you missed it). For those who took the test live, read on the to find the right questions.We got 1337 registrations for the Python skill test and more than 250 people actually made a submission.Topics covered:Versions of required libraries :We had 45 questions in the skill test. The winner got 44 answers right! Here is the distribution of the scores:Interesting distribution! Looks like our choice of questions intimidated a lot of people with a lot of them scoring 0. Overall, here is a brief summary of the performance:So, here are the questions along with there answers as were used in Python skill test:Q :1)
Above dataset has mix of categorical and continuous features. Every data scientist must know that handling categorical values is different from numerical values.So how would you calculate the number of columns having categorical values?A - (train.dtype == 'object').sum()
B - (train.dtypes == object).sum()
C - (train.dtypes == object).count()D  None of theseSolution: BCategorical variables are denoted with datatype as object.Q 2)
Now that you have found that there are some categorical columns present in the dataset. Each categorical column may contain more than two distinct values. For example, Married has two values, Yes and No.How will you find all the distinct values present in the column Education?A - train.Education.individuals()
B - train.Education.distinct()
C - train.Education.unique()D  None of theseSolution: CTo find all the distinct values of a particular column, the function unique can be used.Q 3)
Further, you observe that the column LoanAmount has some missing values.How can you find the number of missing values in the column LoanAmount?A - train.count().maximum() - train.LoanAmount.count()
B - (train.LoanAmount == NaN).sum()
C - (train.isnull().sum()).LoanAmountD  All of theseSolution: CThe function isnull() gives us individual boolean values of the missing values, i.e. is the value is missing or not. In python 2.7, boolean values are represented as 1 and 0 for True and False respectively. So taking their sum gives us the answer.Q 4)
Next, you also see that Credit_History has a few missing values. You want to first analyze people who have a Credit_History.You need to create a new DataFrame named new_dataframe, which contains rows which have a non-missing value for variableCredit_History in our DataFrame train. Which of the following commands would do this?A - new_dataframe = train[~train.Credit_History.isnull()]
B - new_dataframe = train[train.Credit_History.isna()]
C - new_dataframe = train[train.Credit_History.is_na()]D  None of theseSolution: AThe ~ operator works as a negation operator to boolean values. So in simple terms, option A is correct.Q 5)
In the dataset above, you can see row with Loan_id = LP001005 has very little information (i.e. most of the variables are missing). It is recommended to filter out these rows as they could create problems / noise in your model.If a row contains more than 5 missing values, you decide to drop them and store remaining data in DataFrame temp. Which of the following commands will achieve that?A - temp = train.dropna(axis=0, how='any', thresh=5)
B - temp = train.dropna(axis=0, how='all', thresh=5)
C - temp = train.dropna(axis=0, how='any', thresh=train.shape[1] - 5)D  None of theseSolution: CIn the thresh argument of dropna function, you have to specify the threshold after which to drop Nan values, i.e. if you want to drop rows with more than 5 missing values, you would have to subtract it with total number of columns.Q 6)
Now, it is time to slice and dice data. The first logical step is to make data ready for your machine learning algorithm. In the dataset, you notice that number of rows having Property_Area equal to Semiurban is very low. After thinking and talking to your business stakeholders, you decide to combine Semiurban and Urban in a new category City . You also decide to rename Rural to VillageWhich of the following commands will make these changes in the column Property_Area ?A - >>> turn_dict = ['Urban': 'City', 'Semiurban': 'City', 'Rural': 'Village']
 >>> train.loc[:, 'Property_Area'] = train.Property_Area.replace(turn_dict)
B - >>> turn_dict = {'Urban': 'City', 'Semiurban': 'City', 'Rural': 'Village'}
 >>> train.loc[:, 'Property_Area'] = train.Property_Area.replace(turn_dict)
C - >>> turn_dict = {'Urban, Semiurban': 'City', 'Rural': 'Village'}
 >>> train.iloc[:, 'Property_Area'] = train.Property_Area.update(turn_dict)D  None of theseSolution: BTo solve, first you create a dictionary with the specified conditions, than feed it in the replace functionQ 7) While you were progressing in direction of building your first machine learning model, you notice something interesting. On a quick overview of the first few rows, you see that percentage of people who are Male and are married (Married = Yes) seems high.To check this hypothesis, how will you find the percentage of married males in the data?A - (train.loc[(train.Gender == 'male') && (train.Married == 'yes')].shape[1] / float(train.shape[0]))*100
B - (train.loc[(train.Gender == 'Male') & (train.Married == 'Yes')].shape[1] / float(train.shape[0]))*100
C - (train.loc[(train.Gender == 'male') and (train.Married == 'yes')].shape[0] / float(train.shape[0]))*100D  None of theseSolution: DAlways remember, to take multiple boolean indexing, specify with & operator. Also shape[0] returns the total number of columns. And dont forget the case, as python is case sensitive!Q 8)
Take a brief look at train and test datasets mentioned above. You might have noticed that the columns in these datasets do not match, i.e. some columns in train are not present in test and vice versa.How to find which cols are present in test but not in train? Assume data has already been read in DataFrames train & test respectively.A - set(test.columns).difference(set(train.columns))
B - set(test.columns.tolist()) - set(train.columns.tolist())
C - set(train.columns.tolist()).difference(set(test.columns.tolist()))D  Both A and BSolution: DThis is a classic example of set theory.Q 9) As you might be aware, most of the machine learning libraries in Python and their corresponding algorithms require data to be in numeric array format.Hence, we need to convert categorical Gender values to numerical values (i.e. change M to 1 and F to 0). Which of the commands would do that?A - train.ix[:, 'Gender'] = train.Gender.applymap({'M':1,'F':0}).astype(int)
B - train.ix[:, 'Gender'] = train.Gender.map({'M':1,'F':0}).astype(int)
C - train.ix[:, 'Gender'] = train.Gender.apply({'M':1,'F':0}).astype(int)D  None of theseSolution: B(diff map, apply)Q 10)In the datasets above, Product_ID column contains a unique identification of the products being sold. There might be a situation when there are a few products present in the test data but not id train data. This could be troublesome for your model, as it has no historical knowledge for the new product.How would you check if all values of Product_ID in test DataFrame are available in train DataFrame dataset?A - train.Product_ID.unique().contains(test.Product_ID.unique())
B - set(test.Product_ID.unique()).issubset(set(train.Product_ID.unique()))
C - train.Product_ID.unique() = test.Product_ID.unique()D  None of theseSolution: BQ 11) If you look at the data above, Age is currently a categorical variable. Converting it to a numerical field might help us extract more meaningful insight.You decide to replace the Categorical column Age by a numeric column by replacing the range with its average (Example: 0-17 and 17-25 should be replaced by their averages 8.5 and 21 respectively)A - train['Age'] = train.Age.apply(lambda x: (np.array(x.split('-'), dtype=int).sum()) / x.shape)
B - train['Age'] = train.Age.apply(lambda x: np.array(x.split('-'), dtype=int).mean())C  Both of theseD  None of theseSolution: BA somewhat hacky approach, but it works. First you separate the string on - and then find its mean. (If you are wondering why option A doesnt work, check it out! )Q 12) The other scenario in which numerical value could be hiding in plain sight is when it is plagued with characters. We would have to clean these values before moving on to model building.For example, in Ticket, the values are represented as one or two blocks separated with spaces. Each block has numerical values in it, but only the first block has characters combined with numbers. (eg. ABC0 3000).Which of the following code return only the last block of numeric values? (You can assume that numeric values are always present in the last block of this column)A - train.Ticket.str.split(' ').str[0]
B - train.Ticket.str.split(' ').str[-1]
C - train.Ticket.str.split(' ')D  None of theseSolution: BTo index the last term of a python list, you can use -1Q 13)As you might have noticed (or if you havent, do it now!), the above dataset is the famous Titanic dataset. (PS: its a bit unclean than usual, but you get the gist, right? )Coming back to the point, the data has missing values present in it. It is time to tackle them! The simplest way is to fill them with known values.You decide to fill missing Age values by mean of all other passengers of the same gender. Which of the following code will fill missing values for all passengers by the above logic?A - train = train.groupby('Sex').transform(lambda x: x.fillna(x.sum()))
B - train['Age'] = train.groupby('Sex').transform(lambda x: x.fillna(x.mean())).Age
C - train['Age'] = train.groupby('Sex').replace(lambda x: x.fillna(x.mean())).AgeD  None of theseSolution: BTo solve, group the data on Sex, and then fill all the missing values with the appropriate mean. Remember that python lambda is a very useful construct. Do try to inculcate the habit of using it.Q 14) Lets get to know the data a bit more.We want to know how location affects the survival of people. My hypothesis is that people from location S (S=SouthHampton), particularly females, are more likely to survive because they had better survival instincts.The question is, how many females embarked from location S?A - train.loc[(train.Embarked == 'S') and (train.Sex == 'female')].shape[0]
B - train.loc[(train.Embarked == 'S') & (train.Sex == 'female')].shape[0]
C - train.loc[(train.Embarked == 'S') && (train.Sex == 'female')].shape[0]D  None of theseSolution: BQ 15)Look at the column Name  there is an important thing to notice. Looks like, every name has a title contained in it. For example, the name Braund, Mr. Owen Harris has Mr. in it.Which piece of code would help us calculate how many values in column Name have Mr. contained in them?A - (train.Name.str.find('Mr.')==False).sum()
B - (train.Name.str.find('Mr.')>0).sum()
C - (train.Name.str.find('Mr.')=0).sum()D  None of theseSolution: BAs highlighted previously, boolean value True is represented by 1. So option B would be the appropriate answer.Q 16) You can see that column Cabin has 3 missing values out 5 sample records.If a particular column has a high percentage of missing values, we may want to drop the column entirely. However, this might also lead to loss of information.Another method to deal with this type of variable, without losing all information, is to create a new column with flag of missing value as 1 otherwise 0.Which of the following code will create a new column Missing_Cabin and put the right values in it (i.e. if cabin_missing then 1 else 0)?A - train['Missing_Cabin'] = train.Cabin.apply(lambda x: x == '')
B - train['Missing_Cabin'] = train.Cabin.isnull() == False
C - train['Missing_Cabin'] = train.Cabin.isnull().astype(int)D  None of theseSolution: CTo convert boolean values to integer, you can use astype(int)Q 17)Let us take a look at another dataset. The data represents sales of an outlet along with product attributes.The problem is, the dataset does not contain headers. Inspite of this, you know what are the appropriate column names. How would you read the the dataframe by specifying the column names?A - pd.read_csv(""train.csv"", header=None, columns=['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility' ])
B - pd.read_csv(""train.csv"", header=None, usecols=['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility'])
C - pd.read_csv(""train.csv"", header=None, names=['Item_Identifier' ,'Item_Weight' ,'Item_Fat_Content', 'Item_Visibility'])D  None of theseSolution: CTo explicitly specify column names in pandas, you can use names argumentQ 18)Sometimes while reading the data in pandas, the datatypes of columns are not parsed correctly. To deal with this problem, you can either explicitly specify datatypes while reading the data, or change the datatypes in the dataframe itself.Which of the following code will change the datatype of Item_Fat_Content column from object to category?A - train['Item_Fat_Content'] = train['Item_Fat_Content'].asdtype('categorical')
B - train['Item_Fat_Content'] = train['Item_Fat_Content'].astype('category')
C - train['Item_Fat_Content'] = train['Item_Fat_Content'].asdtype('category')D  None of theseSolution: Bcategory datatype is a new feature added to pandas.Q 19)In above data, notice that the Item_Identifier column has some relation with the column Item_Type. As the first letter of Item_Identifier changes, the Item_Type changes too. For example, notice that if the value in Item_Identifier starts with F, then all the corresponding values in Item_Type are eatables, whereas those with D are drinks.To check this hypothesis, find all values in Item_Identifier that starts with F.A - train.Item_Identifier.str.starts_with('F')
B - train.Item_Identifier.str.startswith('F')
C - train.Item_Identifier.str.is_start('F')D  None of theseSolution: BUse str function in pandas to access string functions.D  None of theseSolution: BQ 21) I have another hypothesis that, if an item is more visible to new customers in a supermarket, then its more likely to be sold.So, find correlation between Item_Outlet_Sales and Item_Visibility (use correlation method pearson)A - train.Item_Visibility.corr(train.Item_Outlet_Sales, method='pearson')
B - train.Item_Visibility.corr(train.Item_Outlet_Sales)
C - train.Item_Visibility.corrwith(train.Item_Outlet_Sales, method='pearson')D  Both A and BSolution: DThe default argument for method in corr function is pearson.Q 22)We want to check the distribution of the column Hours.Per.Week with respect to Marital.Status and Occupation of the people. One thing we could do is to create a pivot table of Marital.Status vs Occupation and put the values.Create the pivot table as mentioned above, with the aggregating function as sumD  None of theseSolution: B(pivot_table vs pivot)D  None of theseSolution: CQ 24)Suppose the dataset is too big to be handled by your local machine, but you still want to load it into the memory. What you could do is, read only a specific number of rows, which could be easily read into the memory.Which of the command would read only the top 500 rows?D  None of theseSolution: AD  None of theseSolution: BQ 26) Above dataframe has Date_time_of_event column and it is currently read as an object. This will restrict us to perform any date time operation on it.Which command will help to convert the column Date_time_of_event to data type datetime?A - train['Date_time_of_event'] = pd.to_datetime(train.Date_time_of_event, date_format=""%d-%m-%Y"")
B - train['Date_time_of_event'] = pd.to_datetime(train.Date_time_of_event, format=""%d-%m-%Y %H:%M"")
C - train['Date_time_of_event'] = pd.to_datetime(train.Date_time_of_event, date_format=""%d-%m-%Y %h:%m"")D  None of theseSolution: BQ 27) As shown above, we want to create a new column Date from the given Date_time_of_event column.How would you you extract only the dates from the given Date_time_of_event column?A - train.Date_time_of_event.dt.days
B - train.Date_time_of_event.dt.day
C - train.Date_time_of_event.dt.DayD  None of theseSolution: BD  None of theseSolution: BQ 29)Sometimes the datetime column could be arranged in unix format. To extract useful information from the datetime column, you would have to convert it into usable format.How would you do it?A - pd.to_datetime(train['TIMESTAMP'],unit='s')
B - pd.to_datetime(train['TIMESTAMP'],unit='second')
C - pd.to_datetime(train['TIMESTAMP'],unit='unix')D  None of theseSolution: AQ 30) Find the difference between current time and column Date_time_of_event.A - pd.datetime.now - train.Date_time_of_event.dt
B - pd.datetime.now() - train.Date_time_of_event.dt
C - pd.datetime.now() - train.Date_time_of_eventD  None of theseSolution: CQ 31) Consider that you have to replace the Date_time_of_event column with the first day of the month.How would you do this in python?A - train['Date_time_of_event'] = train.Date_time_of_event.apply(lambda x: x.replace(day=1))
B - >>> train['month'] = train.Date_time_of_event.dt.month; train['year'] = train.Date_time_of_event.dt.year
>>> train['day'] = 1
>>> train['Date_time_of_event'] = train.apply(lambda x:pd.datetime.strptime(""{0} {1} {2}"".format(x['year'],x['month'], x['day']), ""%Y %m %d""),axis=1)
C  Both A and BD  None of theseSolution: CQ 32)The dataset above provides every day expenses on different necessities (days will be arranged in columns and expenses on necessities is in rows).Provide python code to compute cummulative cost for each day.A - a.sumcum(axis=0)
B - a.cumsum(axis=1)
C - a.sumcum(axis=1)D - a.cumsum(axis=0)
Solution: BQ 33)For three data sets given train,student and internship we need to merge these data sets in such a way that for train data every row must have the student details from student data and intern details from intern data. (Consider only the rows which are similar in both the respective datasets)Fill the blanks in the code:train=pd.merge(train,internship,on=_____,how=____)
train=pd.merge(train,student,on=_____,how=____)A - Student_ID, outer, Internship_ID, inner
B - Internship_ID, right, Student_ID, inner
C - Internship_ID, inner, Student_ID, underD - Internship_ID, inner, Student_ID, inner
Solution: DQ 34)In the data above, you might have noticed presence of duplicate rows. This might create problem during joins. To avoid this problem we need to remove the duplicates by keeping the first occurance only.Fill the blanks appropriately :student.______(subset=[Student_ID],keep=_____,inplace=____)A - drop_same, first, True
B - drop_duplicates, first, False
C - drop_same, last, TrueD - drop_duplicates, first, True
Solution: DD - match=re.compile(r""[\w._]@[\w.]"",string)
Solution: BQ 37) Complete the code for removing the Minimum_Duration and Preferred_location variables from the train data set.train=train.drop(['Preferred_location','Minimum_Duration'],___________)A - axis=0
B - axis=1
C - inplace=TrueD - inplace=False
Solution: BYou specify, axis=1 when you want to access columns. On the other hand, you specify axis=0 when accessing rows.D  none of the aboveSolution: AQ 40) A plot between temp and atemp was generated using the following code :plt.scatter(train.temp,train.atemp,alpha=1,c='b',s=20)How can we modify the code to generate a plot which will show Count with color intensity like the graph shown above?A - plt.scatter(train.temp,train.atemp,alpha=1,c=train.Count.value_counts,s=20)
B - plt.scatter(train.temp,train.atemp,alpha=1,c=train.Count,s=20)
C - plt.scatter(train.temp,train.atemp,alpha=1,s=20,color=train.Count)D - plt.scatter(train.temp,train.atemp,alpha=1,s=20,c=w) Solution: BQ 41)One of the hypothesis for the data above is that seasonal variation of temperature could affect our target variable Count.To visualize this, we could use a boxplot of based on the required columns.Which of the following code will create the boxplot as shown above?A - train.boxplot(column='season', by='temp')
B - train.boxplot(ax='temp', by='season')
C - train.boxplot(ax='temp', column='season')D - train.boxplot(column='temp', by='season')Solution: DQ 42)One way to visualize frequency of variables in a column is to plot a histogram. Histograms give a rough sense of the density of the underlying data.How will you plot a histogram of column temp with bin size as 50?A - train.hist(column='temp')
B - train.hist(column='temp', bin_size=50)
C - train.hist(column='temp', bins=50)D  None of theseSolution: CD  None of theseSolution: CQ 44)The plot is given for day wise distribution of the total no of rentals at every hour of the day first being Monday and last Sunday (transverse through rows).,Fill the blanks with correct instructions to create the daywise_rental plot :>>> fig=plt.figure()
>>> for i in range(0,7):
>>> fig.add_subplot(3,3,____)
>>> t1=train[train['______']==i]
>>> t1.________(['hour'])['count'].sum().plot(kind='bar')In options day is the new variable created by extracting day of the week from datetime variable.A - i+1, day, groupby
B - i, day, groupby
C - i, Count, groupbyD - i, day, value_countsSolution: AQ 45)
We want to output the total no of girls and boys data for the year 1880. Fill the blanks in the code for the following output :
>>> train.________(['Year','Gender']).size()._____[1880]A - groupby, idx
B - groupby, loc
C - groupby, iloc
D - value_counts, ilocSolution: BI hope you enjoyed this taking the skilltest and going through the detailed solution. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. And I would like to hear your feedback about the skilltest what you liked and what you think can be improved. Feel free to share them in comments below.",https://www.analyticsvidhya.com/blog/2016/09/solutions-data-science-in-python-skilltest/
18 Free Exploratory Data Analysis Tools For People who dont code so well,Learn everything about Analytics|Overview|Introduction|List of Non ProgrammingTools|End Notes,"1. Excel / Spreadsheet|2. Trifacta|3. Rapid Miner|4. Rattle GUI|5. Qlikview|6. Weka|7. KNIME|8. Orange|9. Tableau Public|10. Data Wrapper|11. Data Science Studio (DSS)|12. OpenRefine|13. Talend|14. Data Preparator|15. DataCracker |16. Data Applied|17. Tanagra Project|18. H2o|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Solutions for Skill test: Data Science in Python|Senior Database Administrator  Bengaluru ( 7-8 Years of Experience )|
Analytics Vidhya Content Team
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Some of these tools are even better than programming (R, Python, SAS) tools.All of us are born with special talents. Its just a matter of time until we discover it and start believing in ourselves.We all have limitations, but should we stop there? No.When I started coding in R, I struggled. Sometimes a lot more than one can ever think! Because I had never ever coded even <Hello World> in my entire life. My situation was similar to a guy who didnt know swimming but was manhandled into deep ocean, who somehow saved himself from drowning but ended up gulping lot of salty water.Now when I look back, I laugh at myself. Do you know why? Because, I could have chosen one of severalnon-coding tools available for data analysis, and couldve avoided the suffering.Data exploration is an inevitable part of predictive modeling. You cant make predictions unless you know what happened in the past. The most important skill to master data exploration is curiosity, which isfree of costyetisntowned by everyone.I have written this article to help you acknowledge various free tools available for exploratory data analysis. Now a days, ample of tools are available in the market which are free & quite interesting to work with. These tools doesnt require you to code explicitly but simple drag  drop clicks do the job.If you are transitioning into data science or have already survived for years, you would know, even after countless years,excel remainsan indispensable part of analytics industry. Even today, most of the problems faced in analytics projects are solved using this software. With larger than ever community support, tutorials, freeresources, learning this tool has become quite easier.It supportsall the important features like summarizing data, visualizing data, data wrangling etc. which are powerful enough to inspect data from all possible angles. No matter how many tools you know, excel must featurein your armory. Though, Microsoft excel is paid but you can still try various other spreadsheet tools like open office, google docs, which are certainly worth a try!Free Download: Click HereTrifactas Wrangler tool is challenging the traditional methods of data cleaning and manipulation. Since Excel possess limitations on data size, this tool has no such boundaries and you can securely work on big data sets. This tool has incredible features such as chart recommendations, inbuilt algorithms, analysis insights using which you can generate reports in no time. Its an intelligent tool focused on solving business problems faster, thereby allowing us to be more productive at data related exercises.Availability of such open source tools make us feel more confident and supportive, also that there are good people around the world who are working extremely hard to make our lives better.Free Download: Click HereThis tool emerged as a leader in 2016 Gartner Magic Quadrant for Advanced Analytics. Yes, its more than a data cleaning tool. It extends its expertise in building machine learning models. Yes, it comprises all the ML algorithms which we use frequently. Not just a GUI, it also extends support to people using Python & R for model building.It continues to fascinate people around the world with its remarkable capabilities. Above all, it claims to provide analytics experience at lightning fast level. Their product line has several products built for big data, visualizations, model deployment, some of which (enterprise) include a subscription fee. In short, we can say its a complete tool for any business which requires performing all tasks from data loading to model deployment.Free Download: Click HereIf you tried using R, but couldnt get a knack of whats going in, Rattle should be your first choice. This GUI is built on R and gets launched by typing install.packages(""rattle"") followed by library(rattle) thenrattle() in R. Therefore, to use Rattle you must install R. Its also more than just data mining tool. Rattle supports various ML algorithms such as Tree, SVM, Boosting, Neural Net, Survival, Linear models etc.Its being widely used these days. According to CRAN, Rattle is being installed 10000 times every month. It provides enough options to explore, transform and model data in just a few clicks. However, it has fewer options than SPSS for statistical analysis. Although, SPSS is a paid tool while Rattle is free of cost.Free Download: Click HereQlikview is one of the most popular tools in the business intelligence industry around the world. Deriving business insights and presenting it in an awesome manner, is what this tool does. With its state of the art visualization capabilities, youd be amazed by the amount of control you get while working on data. It has an inbuilt recommendation engine to update you from time to time about the best visualization methods while working on data sets.However, it is not a statistical software. Qlikview is incredible at exploring data, trends, insights but it cant prove anything statistically. In that case, you might want to look at other softwares.Free Download: Click HereAn advantage of using Weka is that it is easy to learn. Being a machine learning tool, its interface is intuitive enough for you to get the job done quickly. It provides options for data preprocessing, classification, regression, clustering, association rules and visualization. Most of the steps you think of while model building can be achieved using Weka. It is built on Java.Initially, it was designed for research purposes at University of Wakaito, but later it got accepted by more and more people around the world. However, over time I havent seen an enthusiastic Weka community like that of R and Python. The tutorial listed below should help you more.Free Tutorial: Click HereSimilar to RapidMiner, KNIME offers an open source analytics platform for analyzing data, which can later be deployed, scaled using other supportive KNIME products. This tool has an abundance of features on data blending and visualization, and advanced machine learning algorithms. Yes, using this tool you can build models as well. Although, there hasnt been enough talk about this tool, but considering its state of the art design, I think it will soon come under much needed limelight.Moreover, quick training lessons are available on their website to get you started with this tool right now.Free Download: Click HereAs cool as its sounds, this tool is designed to produce interactive data visualizations and data mining tasks. There are enough youtube tutorials to learn this tool. It has an extensive library of data mining tasks which includes all classification, regression, clustering methods. Along with this, the versatile visualizations which get formed during data analysis allow us to understand the data more closely.To build any model, youll be requiredto create a flowchart. This is interesting as it would help us further understand the exact procedure of data mining tasks.Free Download: Click HereTableau is a data visualization software. We can say, tableau and qlikview are the most powerful sharks in the business intelligence ocean. The comparison of superiority is never ending. Its a fast visualization software which lets you explore data, every observation using various possible charts. Its intelligent algorithms figure out by self about the type of data, best method available etc.If you want to understand data in real time, tableau can get the job done. In a way, tableau impartsa colorful life to data and lets us share our work with others.Free Download: Click HereIts a lightning fast visualization software. Next time, when someone in your team gets assigned BI work, and he/she has no clue what to do, this software is a considerable option. Its visualization bucket comprises of line chart, bar chart, column chart, pie chart, stacked bar chart and maps. So, its a basic software and cant be compared with giants like tableau and qlikview. This tool is browser enabled and doesnt require any software installation.It is a powerful tool designed to connect technology, business and data. It is available in two segments: Coding & Non-Coding. Its a complete package for any organization which aims to develop, build, deploy and scale models on network. DSS is also powerful enough to create smart data applications to solve real world problems. It comprises of features which facilitates team integration on projects. Among all features, the most interesting part is, you can reproduce your work in DSS as every action in the system is versioned through an integrated GIT repository.Free Download: Click HereIt started as Google Refine but looks like google plummeted this project due to reasons unclear. However, this tool is still available renamed as Open Refine. Among the generous list of open source tools, openrefine specializes in messy data; cleaning, transforming and shaping it for predictive modeling purposes. As an interesting fact, during model building, 80% time of an analyst is spent in data cleaning. Sounds unpleasant, but its a fact. Using openrefine, analysts can not only save their time, but put it to use for productive work.Free Download: Click HereDecision making these days is largely driven by data. Managers & professionals no longer take gut-based decisions. They require a tool which can help them quickly. Talend can help them to explore data and support their decision making. Precisely, its a data collaboration tool capable of clean, transform and visualize data.Moreover, it also offers an interesting automation feature where you can save and redo your previous task on a new data set. This feature is unique and havent been found in many tools. Also, it makes auto discovery, provides smart suggestion to the user for enhanced data analysis.Free Download: Click HereThis tool is built on Java to assist us in data exploration, cleaning and analysis. It includes various inbuilt packages fordiscretization,numeration,scaling,attribute selection,missing values,outliers, statistics, visualization, balancing, sampling, row selection, andseveral other tasks. Its GUI is intuitive and simple to understand. Once you start working on it, Im sure you wouldnt take lot of time to figure out how to work.A unique advantage of this tool is, the data set used for analysis doesnt get stored in computer memory. This means you can work on large data sets without having any speed or memory troubles.Free Download: Click HereIts a data analysis software which specializes on survey data. Many companies do survey but they struggle to analyze it statistically. Survey data are never clean. It comprises of multiple missing & inappropriate values. This tool reduces our agony and enhances our experience of working on messy data. This tool is designed such that it can load data from all major internet survey programs like surveymonkey, survey gizmo etc. There are several interactive features which helps to understand data better.Free Download: Click HereThis powerful interactive tool is designed to build, share, design data analysis reports. Creating visualization on large data sets can sometimes be troublesome. But this tool is robust in visualizing large amounts of data using tree maps. Like all other tools above, it has feature for data transformation, statistical analysis, detecting anomalies etc. All in all, its a multi usage data mining tool capable of of automatically extracting valuable knowledge (signal) from the raw data. Youd be amazed to see that such non-programming tools are no less than R or Python for data analysis.Free Download: Click HereYou might not like it because of its old fashioned UI, but this free data mining software is designed to build machine learning models. Tanagra project started as a free software for academic and research purposes. Being an open source project, it provides you enough space to devise your own algorithm and contribute.Along with supervised learning algorithms, it is enabled with paradigms such asclustering, factorial analysis, parametric and nonparametric statistics, association rule, feature selection and construction algorithms etc. Some of its limitations includeunavailability of wide set of data sources, direct access to datawarehouses and databases, data cleansing, interactive utilization etc.Free Download: Click HereH2o is one of the most popular software in analytics industry today. Infew years, this organization has succeeded in evangelizing the analytics community around the world. With this open source software, they bring lighting fast analytics experience, which is further extended using API for programming languages. Not just data analysis, but you can build advanced machine learning models in no time. The community support is great, hence learning this tool isnt a worry. If you live in US, chances are they would be organizing a meetup nearby you. Do drop by!Free Download: Click HereBonus Additions:In addition to the awesome tools above, I also found some more tools which I thought you might be interested to look at. However, these tools arent free but you can still avail them for trial:Once you start working on these tools (your choice), youd understand that knowing programming for predictive modeling isnt much advantageous. You can accomplish the same thing with these open source tools. Therefore, until now, if you were get disappointed at your lack of coding prowess, now is the time you channelize your enthusiasm on these tools. You may be interested to check 19 Data Science Toolsfor Non Coders.The only limitation I see with these tools (some of them) is, lack of community support. Except few tools, several of them dont have a community to seek help and suggestions. Still, its worth a try!Did you like reading this article? Have you worked on any of the tools listed above? Which one do you think is the most versatile? Drop your suggestions / opinions in the comments below.",https://www.analyticsvidhya.com/blog/2016/09/18-free-exploratory-data-analysis-tools-for-people-who-dont-code-so-well/
Senior Database Administrator  Bengaluru ( 7-8 Years of Experience ),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|18 Free Exploratory Data Analysis Tools For People who dont code so well|This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation: Senior Database AdministratorLocation: BengaluruExperience Required: 7-8 YearsAbout employer: ConfidentialInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Senior Database Administrator  Bengaluru and following details:,https://www.analyticsvidhya.com/blog/2016/09/senior-database-administrator-bengaluru-7-8-years-of-experience/
This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume,Learn everything about Analytics|Introduction|Howwillthis project add value to your resume?|Table of Contents|1. Problem Statement and Hypothesis Generation|2. Data Exploration|3. Data Cleaning|4. Data Manipulation|5. Machine Learning|End Notes,"The Problem Statement|Looking for a job in analytics? Check out currently hiring jobsin machine learning and data science|Share this:|Like this:|Related Articles|Senior Database Administrator  Bengaluru ( 7-8 Years of Experience )|Comprehensive Introduction to Apache Spark, RDDs & Dataframes (using PySpark)|
Analytics Vidhya Content Team
|69 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It takes sheer courage and hard work to become a successful self-taught data scientist or to make a mid career transition. But, with growingcommunity support, more and more people are now encouraged to make these bold career move. Do you dream to build a career in data science? Trust me, you are not alone!Like it or not, a bitter truth about data science industry is that self-learning / certification / coursework is not sufficient to get you a job. People with little or no experience in real data science projects usually get filtered out in early stages by recruiters. This leavesself taught learners or people transitioning in a difficult place  You need some experience on your CV to get your first data science job!  How do you solve for it?If you are facing this challenge, dont let it stop you. Dont lose hope. Here is the way out there are several open data set repositories.With themto our disposal, we canuse them tactically to build projects and make our resumes worthier. After all, recruiters are looking for proof of your knowledge.In this article, Ill get you started with this process. Ill tell you how to use open data sets to create meaningful projects and improve your knowledge in this process. Ive created this ML project (in R), which you can showcase on your resume. But,make sure that you work & develop theproject from where I leave. Having a resume which stands out by doing such projects, will increase your chances of getting hired magnificently.Note:This ML project will be most helpful to people (using R) who are actively looking for their first or next job in data science. You must have knowledge of ML algorithms. If you are already an experienced (>3 years) data scientist,you might have already worked on this / similar projects.The data used in this project is imbalanced. In real life, some extremelycritical situations result in imbalanced data sets. For example  fraud detection, cancer detection, manufacturing defects, online ads conversion etc. Thus, having prior experience of working on such data might rule the situation in your favor (worth a try!).Furthermore, some characteristics of this project which makes it a worthy project include:If not job, this project will give you enoughconfidence & knowledge that you can build more ML projects on your own. Once you have completed this project, put in on your GitHub repo and showcase it to the world. Leave the link in the comments below to showcase your motivation  you never know when a recruiter drops by!Note: If you are new to imbalanced classification problems. I recommend you to read this article.Given various features, the aim is to build a predictive model to determine the income level for people in US. The income levels are binned at below 50K and above 50K.From the problem statement, its evident thatthis is a binary classification problem.Generating hypothesis is the most crucial step in building models. Yet, most analysts tend to overlook this step. In simple words, this technique enlightens our way by indicating which direction (set of variables) to choose.This step should be practiced before looking at the data. This is done to think broadly and not be constrained by what is available. In this step, well create a laundry list of factors which we think could influence the prediction metrics. Read more about hypothesis generation here.Lets think of some hypothesis whichcan influence the outcome. Here is a set of hypothesis to get you started:H : There is no significant impact of the variables (below) on the dependent variable.Ha : There exists a significant impact of the variables (below) on the dependent variable.Remind you, this is not an exhaustive list. Id suggest you not to limit your thoughts with the ones above, your aim should be to make your project as comprehensive & presentable as possible.Also, every time you think of an hypothesis, try and think of what would be the relationship like and why would that hypothesis stand true. For example, when I say Education  what I really mean is this: I think that with higher education, people would have higher chances of better employment and hence their income would have higher chances of being more than 50K.Similarly, spend some time thinking about your set of hypothesis, how would they be impacting income and hence what is the best way to capture the mathematical relationship.P.S. Do this before you move forward in the articleFor this project, weve taken the data set from UCI Machine Learning Repository: Data set information.The first step is to look at the data and identify which of our hypothesis are available in the data.If youve used this data repository in past, you would know that downloading and modeling data isnt as easy as it might look. If you download the data from link given above, youd find that column headers are missing. Therefore, for your convenience, Ive provided the link for workable version of test and train data:Download Train Data
Download Test DataAs mentioned above, well use R for this project. Now, well load the data into R and look at it closely.#set working directory
> path <- ""C:/Users/manish/Desktop/Data/MLP""
> setwd(path)#load packages & data
> library(data.table)
> train <- fread(""train.csv"",na.strings = c("""","" "",""?"",""NA"",NA))
> test <- fread(""test.csv"",na.strings = c("""","" "",""?"",""NA"",NA))#look at data
> dim(train); str (train); View(train)
> dim(test); str (test); View(test)We see that train data has 199523 rows & 41 columns. Test data has 99762 rows and 41 columns. Generally, test data comes with one less column than train. It means that this data set has test prediction values also. This will help us in evaluating our model.#check first few rows of train & test
> train[1:5]
> test [1:5]#check target variables
>unique(train$income_level)
[1] ""-50000"" ""+50000""
> unique(test$income_level)
[1] ""-50000"" ""50000+.""The denominations of these target levels arent same. This disparity will cause trouble in model evaluation. Being a binary classification problem, we can encode these variables as 0 and 1.#encode target variables
> train[,income_level := ifelse(income_level == ""-50000"",0,1)]
> test[,income_level := ifelse(income_level == ""-50000"",0,1)]Lets look at the severity of imbalanced classes in our data:> round(prop.table(table(train$income_level))*100)
 0   1 
 94   6We see that the majority class has a proportion of94%. In other words, with a decent ML algorithm, our model would get 94% model accuracy. In absolute figures, it looks incredible. But, our performance would depend on, how good can we predict the minority classes. More on this in coming sections!As seen in str() above, the columns in the both data set arent as per column classes given on data set page. Lets update the column classes accordingly. data.table package offers fast and simple way to make changes in multiple columns at once.#set column classes
> factcols <- c(2:5,7,8:16,20:29,31:38,40,41)
> numcols <- setdiff(1:40,factcols)> train[,(factcols) := lapply(.SD, factor), .SDcols = factcols]
> train[,(numcols) := lapply(.SD, as.numeric), .SDcols = numcols]> test[,(factcols) := lapply(.SD, factor), .SDcols = factcols]
> test[,(numcols) := lapply(.SD, as.numeric), .SDcols = numcols]Now, lets separate categorical variables & numerical variables. This will help us in further analysis.#subset categorical variables
> cat_train <- train[,factcols, with=FALSE]
> cat_test <- test[,factcols,with=FALSE]#subset numerical variables
> num_train <- train[,numcols,with=FALSE]
> num_test <- test[,numcols,with=FALSE]
> rm(train,test) #to save memoryRemoving train and test files would allow us to use our memory for other computational purposes which was earlier held up by these data sets.Lets begin with numerical data now. The best way to understand these variables is using Histogram.#load libraries
> library(ggplot2)
> library(plotly)#write a plot function
> tr <- function(a){
      ggplot(data = num_train, aes(x= a, y=..density..)) + geom_histogram(fill=""blue"",color=""red"",alpha = 0.5,bins =100) + geom_density()
ggplotly()
}For ease of understanding, weve created a histogram overlapped with density curve. This curve will helps us decipher the distribution pattern more clearly. ggplotly() package will make our resultant plots interactive, thereby saving uslot of time. Lets look at some variables:#variable age
> tr(num_train$age)As we can see, the data set consists ofpeople aged from 0 to 90 with frequency of people declining with age. Now, if we think of the problem we are trying to solve, do you think population below age 20 could earn >50K under normal circumstances? I dont think so. Therefore, we can bin this variable into age groups.#variable capital_losses
> tr(num_train$capital_losses)This is a nasty right skewed graph. In skewed distribution, normalizing is always an option. But, we need to look into this variable deeper as this insight isnt significant enough for decision making. One option could be, to check for unique values. If they are less, we can tabulate the distribution (done in upcoming sections).Furthermore, in classification problems, we should also plot numerical variables with dependent variable. This would help us determine the clusters (if exists) of classes 0 and 1. For this, we need to add the target variable in num_train data:#add target variable
> num_train[,income_level := cat_train$income_level]
#create a scatter plot
> ggplot(data=num_train,aes(x = age, y=wage_per_hour))+geom_point(aes(colour=income_level))+scale_y_continuous(""wage per hour"", breaks = seq(0,10000,1000))As we can see, most of the people having income_level 1, seem to fall in the age of 25-65 earning wage of $1000 to $4000 per hour. This plot further strengthens our assumption that age < 20 would have income_level 0, hence we willbin this variable.Identifying hidden trends is easier said than done. We need to look at a variable(s) from different angles to spot the hidden trends.Dont stop here. I suggest you to plot all variables and understand their distribution. This would give us enough idea for doingfeature engineering.Similarly, we can visualize our categorical variables as well. For categories, rather than a bland bar chart, a dodged bar chart provides more information. In dodged bar chart, we plot the categorical variables & dependent variable adjacent to each other.#dodged bar chart
> all_bar <- function(i){
ggplot(cat_train,aes(x=i,fill=income_level))+geom_bar(position = ""dodge"", color=""black"")+scale_fill_brewer(palette = ""Pastel1"")+theme(axis.text.x =element_text(angle = 60,hjust = 1,size=10))
}#variable class_of_worker
> all_bar(cat_train$class_of_worker)Though, no specific information is provided about Not in universe category.Lets assume that, this response is given by people who got frustrated (due to any reason) while filling their census data. This variable looks imbalanced i.e. only two category levels seem to dominate. In such situation, a good practice is to combine levels having less than 5% frequency of the total category frequency.#variable education
> all_bar(cat_train$education)Evidently, all children have income_level0. Also, we can infer than Bachelors degreeholders have the largest proportion of people have income_level1.Similarly, you can plot other categorical variables also.Alternative way of checking categories is using 2 way tables. Yes, you can create proportionate tables to check the effect of dependent variable per categories as shown:> prop.table(table(cat_train$marital_status,cat_train$income_level),1)
> prop.table(table(cat_train$class_of_worker,cat_train$income_level),1)Lets check for missing values in numeric variables.#check missing values in numerical data
> table(is.na(num_train))
> table(is.na(num_test))We see that numeric variables has no missing values. Good for us! While working on numeric variables, a good practice is tocheck for correlation in numeric variables. caret package offers a convenient way to filter out variables with high correlation. Lets see:> library(caret)#set threshold as 0.7
> ax <-findCorrelation(x = cor(num_train), cutoff = 0.7)> num_train <- num_train[,-ax,with=FALSE]
> num_test[,weeks_worked_in_year := NULL]The variable weeks_worked_in_yeargets removed. For hygiene purpose, weve removed that variable from test data too. Its not necessary though!Now, lets check for missing values in categorical data. Well use base sapply() to find out percentage of missing values per column.#check missing values per columns
 > mvtr <- sapply(cat_train, function(x){sum(is.na(x))/length(x)})*100
 > mvte <- sapply(cat_test, function(x){sum(is.na(x)/length(x))}*100)
 > mvtr
 > mvteWe find thatsome of the variables have ~50% missing values. High proportion of missing value can be attributed to difficulty in data collection. For now, well remove these category levels. A simple subset() function does the trick.#select columns with missing value less than 5%
> cat_train <- subset(cat_train, select = mvtr < 5 )
> cat_test <- subset(cat_test, select = mvte < 5)For the rest of missing values, a nicer approach would be to label them as Unavailable. Imputing missing values on large data sets can be painstaking. data.tables set() function makes this computation insanely fast.#set NA as Unavailable - train data
#convert to characters
> cat_train <- cat_train[,names(cat_train) := lapply(.SD, as.character),.SDcols = names(cat_train)]
> for (i in seq_along(cat_train)) set(cat_train, i=which(is.na(cat_train[[i]])), j=i, value=""Unavailable"")
#convert back to factors
> cat_train <- cat_train[, names(cat_train) := lapply(.SD,factor), .SDcols = names(cat_train)]#set NA as Unavailable - test data
> cat_test <- cat_test[, (names(cat_test)) := lapply(.SD, as.character), .SDcols = names(cat_test)]
> for (i in seq_along(cat_test)) set(cat_test, i=which(is.na(cat_test[[i]])), j=i, value=""Unavailable"")
#convert back to factors
> cat_test <- cat_test[, (names(cat_test)) := lapply(.SD, factor), .SDcols = names(cat_test)]We are approaching towards machine learning stage. But, machine learning algorithms return better accuracywhen the data set has clear signals to offer. Specially, in case of imbalanced classification, we should try our best to shape the data such that we can derivemaximum informationabout minority class.In previous analysis, we saw that categorical variables have several levels with low frequencies. Such levels dont help as chances are they wouldnt be available in test set. Well do this hygiene check anyways, in coming steps. To combine levels, a simple for loop does the trick. After combining, the new category level will named as Other.#combine factor levels with less than 5% values
#train
> for(i in names(cat_train)){
         p <- 5/100
         ld <- names(which(prop.table(table(cat_train[[i]])) < p))
         levels(cat_train[[i]])[levels(cat_train[[i]]) %in% ld] <- ""Other""
}#test
> for(i in names(cat_test)){
         p <- 5/100
         ld <- names(which(prop.table(table(cat_test[[i]])) < p))
         levels(cat_test[[i]])[levels(cat_test[[i]]) %in% ld] <- ""Other""
}Time for hygiene check. Lets check if there exists a mismatch between categorical levels in train and test data. Either you can write a function for accomplish this. Well rather use a hack derived from mlr package.#check columns with unequal levels 
library(mlr)
> summarizeColumns(cat_train)[,""nlevs""]
> summarizeColumns(cat_test)[,""nlevs""]The parameter nlevs returns the unique number of level from the given set of variables.Before proceeding to the modeling stage, lets look at numeric variables and reflect on possible ways for binning. Since a histogram wasnt enough for us to make decision, lets create simple tables representing counts of unique values in these variables as shown:> num_train[,.N,age][order(age)]
>num_train[,.N,wage_per_hour][order(-N)]Similarly, you should check other variables also. After this activity, we are clear that more than 70-80% of the observations are 0 in these variables. Lets bin these variables accordingly. I used a decision tree to determine the range of resultant bins. However, it will be interested to see how 0-25, 26-65, 66-90 works (discerned from plots above). You should try it sometime later!#bin age variable 0-30 31-60 61 - 90
> num_train[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c(""young"",""adult"",""old""))]
> num_train[,age := factor(age)]> num_test[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c(""young"",""adult"",""old""))]
> num_test[,age := factor(age)]#Bin numeric variables with Zero and MoreThanZero
> num_train[,wage_per_hour := ifelse(wage_per_hour == 0,""Zero"",""MoreThanZero"")][,wage_per_hour := as.factor(wage_per_hour)]
> num_train[,capital_gains := ifelse(capital_gains == 0,""Zero"",""MoreThanZero"")][,capital_gains := as.factor(capital_gains)]
> num_train[,capital_losses := ifelse(capital_losses == 0,""Zero"",""MoreThanZero"")][,capital_losses := as.factor(capital_losses)]
> num_train[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0,""Zero"",""MoreThanZero"")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]> num_test[,wage_per_hour := ifelse(wage_per_hour == 0,""Zero"",""MoreThanZero"")][,wage_per_hour := as.factor(wage_per_hour)]
> num_test[,capital_gains := ifelse(capital_gains == 0,""Zero"",""MoreThanZero"")][,capital_gains := as.factor(capital_gains)]
> num_test[,capital_losses := ifelse(capital_losses == 0,""Zero"",""MoreThanZero"")][,capital_losses := as.factor(capital_losses)]
> num_test[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0,""Zero"",""MoreThanZero"")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]Now, we canremove the dependent variable from num_train, we added forvisualization purpose earlier.> num_train[,income_level := NULL]Making predictions on this data should atleast give us ~94% accuracy. However, while working on imbalanced problems, accuracy is considered to be a poor evaluation metrics because:In such situations, we should use elements of confusion matrix.Following are the metrics well use to evaluate our predictive accuracy:In quest of better accuracy, well use various techniques used on imbalanced classification. For modeling purpose, well use the fantastic mlr package, which I use(over) these days. I hope youve read the recommended articlementioned above because if you are unfamiliar with these techniques, chances are you would lose the way moving forward.#combine data and make test & train files
> d_train <- cbind(num_train,cat_train)
> d_test <- cbind(num_test,cat_test)#remove unwanted files
> rm(num_train,num_test,cat_train,cat_test) #save memory#load library for machine learning
> library(mlr)#create task
> train.task <- makeClassifTask(data = d_train,target = ""income_level"")
> test.task <- makeClassifTask(data=d_test,target = ""income_level"")#remove zero variance features
> train.task <- removeConstantFeatures(train.task)
> test.task <- removeConstantFeatures(test.task)#get variable importance chart
> var_imp <- generateFilterValuesData(train.task, method = c(""information.gain""))
> plotFilterValues(var_imp,feat.type.cols = TRUE)In simple words, you can understand that the variable major_occupation_code would provide highest information to the model followed by other variables in descending order. This chart is deduced using a tree algorithm, where at every split, the information is calculated using reduction in entropy (homogeneity). Lets keep this knowledge safe, we might use it in coming steps.Now, well try to make our data balanced using various techniques such as over sampling, undersampling and SMOTE. In SMOTE, the algorithm looks at n nearest neighbors, measures the distance between them and introduces a new observation at the center of n observations. While proceeding, we must keep in mind that these techniques have their own drawbacks such as:Being your first project(hopefully), we should try all techniques and experience how it affects.#undersampling
> train.under <- undersample(train.task,rate = 0.1) #keep only 10% of majority class
> table(getTaskTargets(train.under))#oversampling
> train.over <- oversample(train.task,rate=15) #make minority class 15 times
> table(getTaskTargets(train.over))#SMOTE
> train.smote <- smote(train.task,rate = 15,nn = 5)Looks like, my machine gave up at these SMOTE parameters. Its been over 50 minutes and this code hasnt executed. Look at the havoc this is creating in my poor machine:While working on such data sets, its important for you to learn the ways to hop such obstacles. Lets modify the parameters and run it again:> system.time(
  train.smote <- smote(train.task,rate = 10,nn = 3)
 )
Warning messages:
# 1: In is.factor(x) :
# Reached total allocation of 8084Mb: see help(memory.size)
# 2: In is.factor(x) :
# Reached total allocation of 8084Mb: see help(memory.size)
# user system elapsed 
# 81.95 21.86 184.56
> table(getTaskTargets(train.smote))It did run with some warning messages. We can ignore them for now. Lets now look at the available algorithms we can use to solve this problem.#lets see which algorithmsare available
> listLearners(""classif"",""twoclass"")[c(""class"",""package"")]Well start with naive Bayes, an algorithms based on bayes theorem.In case of high dimensional data like text-mining, naive Bayes tends to do wonders in accuracy. It works on categorical data. In case of numeric variables,a normal distribution is considered for these variables and a mean and standard deviationis calculated. Then, using some standard z-table calculations probabilities can be estimated for each of your continuous variables to make the naive Bayes classifier.Well use naive Bayes on all 4 data sets (imbalanced, oversample, undersample and SMOTE) and compare the prediction accuracy using cross validation.#naive Bayes
> naive_learner <- makeLearner(""classif.naiveBayes"",predict.type = ""response"")
> naive_learner$par.vals <- list(laplace = 1)#10fold CV - stratified
> folds <- makeResampleDesc(""CV"",iters=10,stratify = TRUE)#cross validation function
> fun_cv <- function(a){
  crv_val <- resample(naive_learner,a,folds,measures = list(acc,tpr,tnr,fpr,fp,fn))
  crv_val$aggr
}> fun_cv (train.task) 
# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean 
# 0.7337249    0.8954134   0.7230270  0.2769730> fun_cv(train.under) 
# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean 
# 0.7637315   0.9126978   0.6651696   0.3348304> fun_cv(train.over)
# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean 
#  0.7861459   0.9145749   0.6586852  0.3413148> fun_cv(train.smote)
# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean 
#  0.8562135   0.9168955  0.8160638   0.1839362This package names cross validated results are test.mean. After comparing, we see that train.smote gives the highest true positive rate and true negative rate. Hence, we learn that SMOTE technique outperforms the other two sampling methods.Now, lets build our model SMOTE data and check our final prediction accuracy.#train and predict
> nB_model <- train(naive_learner, train.smote)
> nB_predict <- predict(nB_model,test.task)#evaluate
> nB_prediction <- nB_predict$data$response
> dCM <- confusionMatrix(d_test$income_level,nB_prediction)
# Accuracy : 0.8174
# Sensitivity : 0.9862
# Specificity : 0.2299 #calculate F measure
> precision <- dCM$byClass['Pos Pred Value']
> recall <- dCM$byClass['Sensitivity']> f_measure <- 2*((precision*recall)/(precision+recall))
> f_measureThe function confusionMatrix is taken from library(caret). This naive Bayes model predicts 98% of the majority class correctly, but disappoints atminority class prediction (~23%). Let us not get hopelessand try more techniques to improve our accuracy. Remember, the more you hustle, better you get!Lets use xgboost algorithm and try to improve our model. Well do 5 fold cross validation and 5 round random search for parameter tuning. Finally, well build the model using the best tuned parameters.#xgboost
>set.seed(2002)
> xgb_learner <- makeLearner(""classif.xgboost"",predict.type = ""response"")
> xgb_learner$par.vals <- list(
           objective = ""binary:logistic"",
           eval_metric = ""error"",
           nrounds = 150,
           print.every.n = 50
)#define hyperparameters for tuning
> xg_ps <- makeParamSet(
        makeIntegerParam(""max_depth"",lower=3,upper=10),
        makeNumericParam(""lambda"",lower=0.05,upper=0.5),
        makeNumericParam(""eta"", lower = 0.01, upper = 0.5),
        makeNumericParam(""subsample"", lower = 0.50, upper = 1),
        makeNumericParam(""min_child_weight"",lower=2,upper=10),
        makeNumericParam(""colsample_bytree"",lower = 0.50,upper = 0.80)
)#define search function
> rancontrol <- makeTuneControlRandom(maxit = 5L) #do 5 iterations
#5 fold cross validation
> set_cv <- makeResampleDesc(""CV"",iters = 5L,stratify = TRUE)
#tune parameters
> xgb_tune <- tuneParams(learner = xgb_learner, task = train.task, resampling = set_cv, measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = xg_ps, control = rancontrol)
# Tune result:
# Op. pars: max_depth=3; lambda=0.221; eta=0.161; subsample=0.698; min_child_weight=7.67; colsample_bytree=0.642
# acc.test.mean=0.948,tpr.test.mean=0.989,tnr.test.mean=0.324,fpr.test.mean=0.676Now, we can use these parameter for modeling using xgb_tune$xwhich contains the best tuned parameters.#set optimal parameters
> xgb_new <- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)#train model
> xgmodel <- train(xgb_new, train.task)#test model
> predict.xg <- predict(xgmodel, test.task)#make prediction
> xg_prediction <- predict.xg$data$response#make confusion matrix
> xg_confused <- confusionMatrix(d_test$income_level,xg_prediction)
Accuracy : 0.948
Sensitivity : 0.9574
Specificity : 0.6585
> precision <- xg_confused$byClass['Pos Pred Value']
> recall <- xg_confused$byClass['Sensitivity']> f_measure <- 2*((precision*recall)/(precision+recall))
> f_measure
#0.9726374As we can see, xgboost has outperformed naive Bayes models accuracy (as expected!). Can we further improve ?Until now, weve used all the variables in the data. Shall we try using the important ones? Consider it your homework. Let me provide you hint to do this:#top 20 features
> filtered.data <- filterFeatures(train.task,method = ""information.gain"",abs = 20)
#train
> xgb_boost <- train(xgb_new,filtered.data)After this, follow the same steps as above for predictions and evaluation. Tell meyour understanding in comments below.Until now, our model has been making label predictions. The threshold used for making these predictions in 0.5 as seen by:> predict.xg$threshold
[[1]] 0.5Due to imbalanced nature of the data, the threshold of 0.5 will always favor the majority class since the probability of a class 1 is quite low. Now, well try a new technique:Well continue using xgboost for this stunt. To do this, we need to change the predict.type parameter while defining learner.#xgboost AUC 
> xgb_prob <- setPredictType(learner = xgb_new,predict.type = ""prob"")#train model
> xgmodel_prob <- train(xgb_prob,train.task)#predict
> predict.xgprob <- predict(xgmodel_prob,test.task)Now, lets look at the probability table thus created:#predicted probabilities
> predict.xgprob$data[1:10,]Since, we have obtained the class probabilities, lets create an AUC curve and determine the basis to modify prediction threshold.> df <- generateThreshVsPerfData(predict.xgprob,measures = list(fpr,tpr))
> plotROCCurves(df)AUC is a measure of true positive rate and false positive rate. We aim to reach as close to top left corner as possible. Therefore, we should aim to reduce the threshold so that the false positive rate can be reduced.#set threshold as 0.4
> pred2 <- setThreshold(predict.xgprob,0.4)
> confusionMatrix(d_test$income_level,pred2$data$response)
# Sensitivity : 0.9512 
# Specificity : 0.7228With 0.4 threshold, our model returned better predictions than our previous xgboost model at 0.5 threshold. Thus, you can see that setting threshold using AUC curve actually affect our model performance. Lets give one more try.> pred3 <- setThreshold(predict.xgprob,0.30)
> confusionMatrix(d_test$income_level,pred3$data$response)
#Accuracy : 0.944 
# Sensitivity : 0.9458 
# Specificity : 0.7771This model has outperformed all our models i.e. in other words, this is the best model because 77% of the minority classes have been predicted correctly.Similarly, you can try and test other threshold values to check if your model improves. In this xgboost model, there is a lot you can do such as:Apart from the methods listed above, you can also assign class weights such that the algorithm pays more attention while classifying the class with higher weight. I leave this part as homework to you. Run the code below and update me if you model surpassed our previous xgboost prediction. Use SVM in homework. An important tip: The code below might take longer than expected to run, therefore close all other applications.#use SVM
> getParamSet(""classif.svm"")
> svm_learner <- makeLearner(""classif.svm"",predict.type = ""response"")
> svm_learner$par.vals<- list(class.weights = c(""0""=1,""1""=10),kernel=""radial"")> svm_param <- makeParamSet(
      makeIntegerParam(""cost"",lower = 10^-1,upper = 10^2),
      makeIntegerParam(""gamma"",lower= 0.5,upper = 2)
)#random search
> set_search <- makeTuneControlRandom(maxit = 5L) #5 times#cross validation #10L seem to take forever
> set_cv <- makeResampleDesc(""CV"",iters=5L,stratify = TRUE)#tune Params
> svm_tune <- tuneParams(learner = svm_learner,task = train.task,measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = svm_param,control = set_search,resampling = set_cv)
)#set hyperparameters
>svm_new <- setHyperPars(learner = svm_learner, par.vals = svm_tune$x)#train model
>svm_model <- train(svm_new,train.task)#test model
>predict_svm <- predict(svm_model,test.task)> confusionMatrix(d_test$income_level,predict_svm$data$response)I hope this project will helped you understand the importance of data exploration, visualization, manipulation in machine learning. To showcase this project on your resume, create your github account, upload this project along with important findings. I have not deliberately provided my code so that you write it at your end.Make sure you understand this project and develop on it further. Recruiter can figure out whos honest and not. And trust me, such projects will different your profile from other candidates.If you want me to createand share more such machine learning projects, write your suggestion on problem types below. Id love work on something which you think is challenging and would like to conquerit.",https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/
"Comprehensive Introduction to Apache Spark, RDDs & Dataframes (using PySpark)",Learn everything about Analytics|Introduction|Table of Contents|Challenges while working with big data|What is Distributing Computing Framework?|What is Apache Spark?|Installation of Apache Spark with PySpark|Python vs Scala:|Apache Spark data representations:RDD / Dataframe / Dataset|Solving a machine learning problem:|End Note:,"History of Apache Spark|Key terms used in Apache Spark:|How Apache Spark is better than traditional big data framework?||RDD:|Share this:|Like this:|Related Articles|This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume|40 Interview Questions asked at Startups in Machine Learning / Data Science|
Ankit Gupta
|57 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Industry estimates that we are creating more than 2.5 Quintillion bytes of data every year. Think of it for a moment 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting, it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation.In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to bevery costly in terms of time and speed.On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.In this article, I will introduce you to one such framework, which has made querying and analysing data at a large scale much more efficient than previous systems / frameworks  Read on!P.S. This article is meant for complete beginners on the topic and presumes minimalprior knowledge in Big DataChallenges associated with big data can be classified in followingcategories:Now that I have spoken of Distributed computing, let us get a bit deeper into it!In simple terms, distributed computing is just a distributed system, where multiplemachines are doing certain work at the same time. While doing the work, machines will communicate with each other by passing messages between them. Distributed computing is useful, when there is requirement of fast processing (computation) on huge data.Let us take a simple analogy to explain the concept. Let us say, you had to count the number of books in various sections of a really large library. And you have to finish it in less than an hour. This number has to be exact and can not be approximated. What would you do?If I was in this position, I would call up as many friends as I can and divide areas / rooms among them. Ill divide the work in non-overlapping manner and ask them to report back to be in 55 minutes. Once they come back, Ill simply add up the numbers to come up with a solution. This is exactly how distributed computing works.Apache Hadoop and Apache Spark arewell-known examples of Big data processing systems. Hadoop and Spark are designed for distributed processing of large data sets across clusters of computers. Although, Hadoop is widely used for fast distributed computing, it has several disadvantages. For example, it does not use In-memory computation, which is nothing but keeping the data in RAM instead of Hard Disk for fast processing. In-memory computation enablesfaster processing ofBig data. When Apache Spark was developed, it overcame this problem by usingIn-memory computation for fast computing.MapReduce is also used widely, when the task is to process huge amounts of data, in parallel (more than one machines are doing a certain task at the same time), on large clusters. Youcan learn more aboutMapReduce from this link.Apache Spark is a fast cluster computing framework which is used for processing, querying and analyzing Big data. It is based on In-memory computation, which is a big advantage of Apache Spark over several other big data Frameworks. Apache Spark is open source and one of the most famous Big data framework. It can run tasks up to 100 times faster, when it utilizes the in-memory computations and 10 times faster when it uses disk than traditional map-reduce tasks.Please note that Apache Spark is not a replacement of Hadoop. It is actually designed to run on top of Hadoop.Apache Spark was originally created at University of California, Berkeleys AMPLab in 2009. The Spark code base was later donated to the Apache Software Foundation. Subsequently, it was open sourced in 2010. Spark is mostly written in Scala language. It has some code written in Java, Python and R. Apache Spark provides several APIs for programmers which include Java, Scala, R and Python.Image source: https://spark.apache.org/docs/1.1.1/img/cluster-overview.pngSpark Context: It holds a connection with Spark cluster manager. All Spark applications run as independent set of processes, coordinated by a SparkContext in a program.Driver and Worker: A driver is incharge of the process of running the main() function of an application and creating the SparkContext. Aworker, on the other hand, is any node that can run program in the cluster. If a process is launched for an application, then this application acquires executors at worker node.Cluster Manager: Cluster manager allocates resources to each application in driver program. There are three types of cluster managers supported by Apache Spark  Standalone, Mesos and YARN. Apache Spark is agnostic to the underlying cluster manager, so we can install any cluster manager, each has its own unique advantages depending upon the goal. They all are different in terms of scheduling, security and monitoring. Once SparkContext connects to the cluster manager, it acquires executors on a cluster node, these executors are worker nodes on cluster which work independently on each tasks and interact with each other.In-memory computation: The biggest advantage of Apache Spark comes from the fact that it saves and loads the data in and from the RAM rather than from the disk (Hard Drive). If we talk about memory hierarchy, RAM has much higher processing speed than Hard Drive (illustrated in figure below). Since the prices of memory has come down significantly in last few years, in-memory computations have gained a lot of momentum.Spark uses in-memory computations to speed up 100 times faster than Hadoop framework.Image Source:https://en.wikipedia.org/wiki/Memory_hierarchyIn Hadoop, tasks are distributed among the nodes of a cluster, which in turn save data on disk. When that data is required for processing, each node has to load the data from the disk and save the data into disk after performing operation. This process ends up adding cost in terms of speed and time, because disk operations are far slower than RAM operations. It also requires time to convert the data in a particular format when writing the data from RAM to disk. This conversion is known as Serialization and reverse is Deserialization.Lets look at the MapReduce process to understand the advantage of in-memory computation better. Suppose, there are several map-reduce tasks happening one after another. At the start of the computations, both technologies (Hadoop and Spark), read the data from disk for mapping. Hadoop performs the map operation and saves the results back to hard drive. However, in case of Apache Spark, the results are stored in RAM.In the next step (Reduce operation), Hadoop reads the saved data from the hard drive, where as Apache Spark reads it from RAM. This creates a difference in a single MapReduce operation. Now imagine, if there were multiple map-reduce operations, how much time difference would you see at the end of task completion.Language Support: Apache Spark has API support for populardata science languages like Python, R, Scala and Java.Supports Real time and Batch processing: Apache Spark supports Batch data processing where a group of transactions is collected over a period of time. It also supports real time data processing, where data is continuously flowing from the source. For example, weather information coming in from sensors can be processed by Apache Spark directly.Lazy operation:Lazy operations are used to optimize solutions in Apache Spark. I will discuss about lazy evaluation in later part of this article. For now, we can think that there are some operations which do not execute until we require results.Support for multiple transformations and actions: Another advantage of Apache Spark over Hadoop is that Hadoop supports only MapReduce but Apache Spark support many transformations and actions including MapReduce.There are further advantages of Apache Spark in comparison toHadoop. For example, Apache Spark is much faster while doing Map side shuffling and Reduce side shuffling. However, shuffling is a complex topic in itself and requires an entire article in itself. Hence, I am not talking about it in more details here.We can install Apache Spark in many different ways. Easiest way to install Apache Spark is to start with installation on a single machine. Again, we will have choices of different Operating Systems. For installing in a single machine, we need to have certain requirements fulfilled. I am sharing steps to install for Ubuntu (14.04) for Spark version 1.6.0. I am installing Apache Spark with Python which is known as PySpark (Spark Python API for programmer). If you are interested in the R API SparkR, have a look at this learning path.OS: Ubuntu 14.04, 64 bit . (If you are running on Windows or Mac and are new to this domain, I would strongly suggest to create a Virtual Ubuntu machine with 4 GB RAM and follow the rest of the process).Softwares Required: Java 7+, Python 2.6+, R 3.1+Installation Steps:Step 0: Open the terminal.Step 1: Install JavaIf you are asked to accept Java license terms, click onYes and proceed. Once finished, let us check whether Java has installed successfully or not. To check the Java version and installation, you can type:Step 2 :Once Java is installed, we need to install ScalaThis will show you the version of Scala installedStep 3: Install py4jPy4J is used on the driver for local communication between the Python and Java SparkContext objects; large data transfers are performed through a different mechanism.Step 4: Install Spark.By now, we have installed the dependencies which are required to install Apache Spark. Next, we need to download andextract Spark source tar. We can get the latest version Apache Spark using wget:Step 5: Compile the extracted sourcesbt is an open source build tool for Scala and Java projects which is similar to Javas Maven.This will take some time to install Spark. After installing, we can check whether Spark is running correctly or not by typing.this will produce the output:Pi is roughly 3.14042To see the above results we need to lower the verbosity level of the log4j logger inlog4j.properties.
After opening the file log4j.properties, we need to replace following line:

by
Step 6:Move the files in the right folders (to make it convenient to access them)Add this to your path by editing your bashrc file:Step 7: Create environment variables.To set the environment variables, open bashrc file in any editor.Set the SPARK_HOME and PYTHONPATH by adding following lines at the bottom of this fileNext, restart bashrc by typing in:Lets add thissetting for ipython by creating a new python script to automatically export settings, just in case above change did not work.Paste some lines in this file.Step 8:We are all set now. Let us start PySparkby typing command in root directory:We can also start ipython notebook in shell by typing:When we launch the shell in PySpark, it will automatically load spark Context as sc and SQLContext as sqlContext.One of the common question people ask is whether it is necessary to learn Scala to learn Spark? If you are some one who already knows Python to some extent or are just exploring Spark as of now, you can stick to Python to start with. However, if you want to process some serious data across several machines and clusters, it is strongly recommended that you learn Scala.Computation speedin Python is much slower than Scala in Apache Spark.Spark has three data representations viz RDD, Dataframe, Dataset. For each data representation, Spark has a different API. For example, later in this article I am going touse ml (a library), which currently supports only Dataframe API. Dataframeis muchfaster than RDD because ithas metadata (some informationabout data) associated with it, which allows Spark to optimizequery plan. Refer to thislink to know more about optimization. The Dataframe feature in Apache Spark was added in Spark 1.3. If you want to know more in depth about when to use RDD, Dataframe and Dataset you can refer this link.In this article, I will first spend some time on RDD, to get you started with Apache Spark. Later, I will spendsometimeon Dataframes. Dataframes share some commoncharacteristics withRDD (transformations and actions). In this article, I am not going to talk about Dataset asthis functionality is not included in PySpark.After installing and configuring PySpark, we can start programming using Spark in Python. But to use Spark functionality, we must use RDD. RDD (Resilient Distributed Database) is a collection of elements, that can be divided across multiple nodes in a cluster to run parallel processing. It is also fault tolerant collection of elements, which means it can automatically recover from failures. RDD is immutable, we can create RDD once but cant change it. We can apply any number of operation on it and can create another RDD by applying some transformations. Here are a few things to keep in mind about RDD:We can apply 2 types of operations on RDDs:Transformation: Transformation refers to the operation applied on a RDD to create new RDD.
Action: Actions refer to an operation which also apply on RDD that perform computation and send the result back to driver.Example:Map (Transformation) performs operation on each element of RDD and returns a new RDD. But, in case of Reduce (Action), it reduces / aggregates the output of a map by applying some functions (Reduce by key). There are many transformations and actions are defined in Apache Spark documentation, I will discuss these in a later article.RDDs use Shared Variable:
The parallel operations in Apache Spark use shared variable. It means that whenever a task is sent by a driver to executors program in a cluster, a copy of shared variable is sent to each node in a cluster, so that they can use this variable while performing task. Accumulator and Broadcast are the two types of shared variables supported by Apache Spark.
Broadcast: We can use the Broadcast variable to save the copy of data across all node.
Accumulator: In Accumulator variables are used for aggregating the information.How to Create RDD in Apache SparkExisting storage:When we want to create a RDD though existing storage in driver program (which we would like to be parallelized). For example, converting a list to RDD, which is already created in a driver program.External sources: When we want to create a RDD though external sources such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.Writing first program in Apache SparkI have already discussed that RDD supports two type of operations, which are transformation and action. Let us get down to writing our first program:Step1: Create SparkContextFirst step in any Apache programming is to create a SparkContext. SparkContext is needed when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. It is first step to connect with Apache Cluster. If you are using Spark Shell, we will find that this is already created. Otherwise, we can create the Spark Context by importing, initializing and providing the configuration settings. For example:Step2: Create a RDDI have already discussed that we can create RDD in two ways: Eitherfrom an existing storage orfrom an external storage. Lets create our first RDD. SparkContext has parallelize method, which is used for creating the Spark RDD from an iterable (like list, tuple..) alreadypresent in driver program.We can also provide the number of partitions as a parameter to parallelize method. If we donot give number of partition parameter, then Spark will automatically set the number of partition in a cluster. The number of partition can be set manually by passing second parameter to parallelize method. For example, sc.parallelize(data, 10)), where data is an existing data in driver program and 10 is the number of partitions.
Lets create the first Spark RDD called rdd.We have a collect method to see the content of RDD.To see the first n element of a RDD we have a method take:We have 2 parallel operations in RDD which are Transformation and Action. Transformation and Action were already discussed briefly earlier. So lets see how transformation works. Remember that RDDs are immutable  so we cant change our RDD, but we can apply transformation on it. Lets see an example of map transformation to demonstrate how transformation works.Step 3: Map transformation.Map transformation returns a Mapped RDD by applying function to each element of the base RDD. Lets repeat the first step of creating a RDD from existing source, For example,Now a RDD (name is Rdd) is created from the existing source, which is a list of string in a driver program. We will now apply lambda function to each element of Rdd and return the mapped (transformed) RDD (word,1) pair in the Rdd1.Lets see the out of this map operation.If you noticed, nothing happened after applying the lambda function on Rdd1 (we wont see any computation happening in a cluster). This is called the lazy operation. All transformation operations in Spark are lazy, which means that we will not see any computations on RDD, until we need them for further action.Spark remembers which transformation is applied to which RDD with the help of DAG (Directed a Cyclic Graph). The lazy evaluation helps Spark to optimize the solution because Spark will get time to see the DAG before actually executing the operations on RDD. This enables Spark to run operations more efficiently.In the code above, collect() and take() are the examples of an action.There are many number of transformation defined in Apache Spark. We will talk more about them in a future post.We have covered a lot of ground already. We started with understanding what Spark brings to the table, its data representations, installed Spark and have already played with our first RDD. Now, Ill demonstrate solution toPractice Problem: Black Friday using Apache Spark. Even if you dont understand these commands completely as of now, it is fine. Just follow along, we will take them up again in a future tutorial.Lets look at the steps:Reading a data file (csv)For reading the csv file in Apache Spark, we need to specify the library in python shell. Lets read the the data from a csv files to create the Dataframe and apply some data science skills on this Dataframe like we do in Pandas.For reading the csv file, first we need to download Spark-csv package (Latest) and extract this package into the home directory of Spark. Then, we need to open a PySpark shell and include the package (I am using spark-csv_2.10:1.3.0).In Apache Spark, we can read the csv file and create a Dataframe with the help of SQLContext. Dataframe is a distributed collection of observations (rows) with column name, just like a table.Lets see how can we do that.Please note that since I am using pyspark shell, there is already a sparkContext and sqlContext available for me to use. In case, you are not using pyspark shell, you might need to type in the following commands as well:First download the train and test file and load these with the help of SparkContextPATH is the location of folder, where your train and test csv files are located. Header is True, it means that the csv files contains the header. We are using inferSchema is True for telling sqlContext to automatically detect the data type of each column in data frame. If we do not set inferSchema to true, all columns will be read as string.Analyze the data typeTo see the types of columns in Dataframe, we can use the method printSchema(). Lets apply printSchema() on train which will Print the schema in a tree format.Previewing the data setTo see the first n rows of a Dataframe, we have head() method in PySpark, just like pandas in python. We need to provide an argument (number of rows) inside the head method. Lets see first 10 rows of train:To see the number of rows in a data frame we need to call a method count(). Lets check the number of rows in train. The count method in pandas and Spark are different.Impute Missing valuesWe can check number of not null observations in train and test by calling drop() method. By default, drop() method will drop a row if it contains any null value. We can also pass all to drop a row only if all its values are null.Here, I am imputing null values in train and test file with -1. Imputing the values with -1 is not an elegant solution. We have several algorithms / techniques to impute null values, but for the simplicity I am imputing null with constant value (-1). We can transform our base train, test Dataframes after applying this imputation. For imputing constant value, we have fillna method. Lets fill the -1 in-place of null in all columns.Analyzing numerical featuresWe can also see the various summary Statistics of a Dataframe columns using describe() method, which shows statistics for numerical variables. To show the results we need to call show() method.Sub-setting ColumnsLets select a column called User_ID from a train, we need to call a method select and pass the column name which we want to select. The select method will show result for selected column. We can also select more than one column from a data frame by providing columns name separated by comma.Analyzing categorical featuresTo start building a model, we need to see the distribution of categorical features in train and test. Here I am showing this for only Product_ID but we can also do the same for any categorical feature. Lets see the number of distinct categories of Product_ID in train and test. Which we can do by applying methods distinct() and count().After counting the number of distinct values for train and test we can see the train has more categories than test. Let us check what are the categories for Product_ID, which are in test but not in train by applying subtract method.We can also do the same for all categorical feature.Above you can see that 46 different categories are in test not in train. In this case, either we collect more data about them or skip the rows in test for those categories(invalid category) which are not in train.Transforming categorical variables to labelsWe also need to transform categorical columns to label by applying StringIndexer Transformation on Product_ID which will encode the Product_ID column of labels to a column of label indices. You can see more about this from the linkAbove, we build a labeller by applying fit() method on train Dataframe. Later we will use this labeller to transform our train and test. Let us transform our train and test Dataframe with the help of labeller. We need to call transform method for doing that. We will store the transformation result in Train1 and Test1.Lets check the resulting Train1 Dataframe.The show method on Train1 Dataframe will show that we successfully added one transformed column product_ID in our previous train Dataframe.Selecting Features to Build a Machine Learning ModelLets try to create a formula for Machine learning model like we do in R. First, we need to import RFormula from the pyspark.ml.feature. Then we need to specify the dependent and independent column inside this formula. We also have to specify the names for features column and label column.After creating the formula we need to fit this formula on our Train1 and transform Train1,Test1 through this formula. Lets see how to do this and after fitting transform train1,Test1 in train1,test1.We can see the transformed train1, test1.After applying the formula we can see that train1 and test1 have 2 extra columns called features and label those we have specified in the formula (featuresCol=features and labelCol=label). The intuition is that all categorical variables in the features column in train1 and test1 are transformed to the numerical and the numerical variables are same as before for applying ML. Purchase variable will transom to label column. We can also look at the column features and label in train1 and test1.Building a Machine Learning Model: Random ForestAfter applying the RFormula and transforming the Dataframe, we now need to develop the machine learning model on this data. I want to apply a random forest regressor for this task. Let us import a random forest regressor, which is defined in pyspark.ml.regression and then create a model called rf. I am going to use default parameters for randomforest algorithm.After creating a model rf we need to divide our train1 data to train_cv and test_cv for cross validation.Here we are dividing train1 Dataframe in 70% for train_cv and 30% test_cv.Now build the model on train_cv and predict on test_cv. The results will save in predictions.If you check the columns in predictions Dataframe, there is one column called prediction which has prediction result for test_cv.Lets evaluate our predictions on test_cv and see what is the mean squae error.To evaluate model we need to import RegressionEvaluator from the pyspark.ml.evaluation. Wehave to create an object for this. There is a method called evaluate for evaluator which will evaluate the model. We need to specify the metrics for that.After evaluation we can see that our root mean square error is 3773.1460883883865 which is a square root of mse.Now, we will implement the same process on full train1 dataset.After prediction, we need to select those columns which are required in Black Friday competition submission.Now we need to write the df in csv format for submission.After writing into the csv file(submission.csv). We can upload our first solution to see the score, I got the score 3822.121053 which is not very bad for first model out of Spark!In this article, I introduced you to the fascinating world of Apache Spark. This is only the start of things to come in this series. In the next few weeks, I will continue to share tutorials for you to master the use of Apache Spark. If this article feels like a lot of work, it is! So, take your time and digest this comprehensive guide.In the meanwhile, if you have any questions or you want to give any suggestions on what I should cover, feel free to drop them in the notes below.",https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/
40 Interview Questions asked at Startups in Machine Learning / Data Science,Learn everything about Analytics|Overview|Introduction|Interview Questions on Machine Learning|End Notes,"Looking for a job in analytics? Check out currently hiring jobsin machine learning and data science.|Share this:|Like this:|Related Articles|Comprehensive Introduction to Apache Spark, RDDs & Dataframes (using PySpark)|AWS / Cloud Engineer  Pune ( 4+ Years of Experience )|
Analytics Vidhya Content Team
|33 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Careful! These questions can make you think THRICE!Machine learning and data science are being looked as the drivers of the next industrial revolution happening in the world today. This also means that there are numerous exciting startups looking for data scientists. What could be a better start for your aspiring career!However, still, getting into these roles is not easy. You obviously need to get excited about the idea, team and the vision of the company. You might also find some real difficult techincal questions on your way.The set of questions asked depend on what does the startup do. Do they provide consulting? Do they build ML products ? You should always find this out prior to beginning your interview preparation.To help you preparefor your nextinterview, Ive prepared a list of 40 plausible & tricky questions whichare likely to come across your way in interviews. If you can answer and understand these question, rest assured, you willgive a tough fight in your job interview.Note:A key to answer these questions is to have concrete practical understanding on ML and related statistical concepts. You can get that know-how in our course Introduction to Data Science!Or how about learning how to crack data science interviews from someone who has conducted hundreds of them? Check out the Ace Data Science Interviews course taught by Kunal Jain and Pranav Dar.40 Interview Questions asked at Startups in Machine Learning / Data ScienceQ1. You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)Answer:Processing a high dimensional data on a limited memory machineis a strenuous task, your interviewer would be fully aware of that. Following are the methods you can use to tackle such situation:Note: For point 4 & 5, make sure you read about online learning algorithms & Stochastic Gradient Descent. These are advanced methods.Q2. Is rotation necessary in PCA? If yes, Why? What will happen if you dont rotate the components?Answer: Yes, rotation (orthogonal) is necessary because it maximizes the difference between variance captured by the component. This makes the components easier to interpret. Not to forget, thats the motive of doing PCA where, we aim to select fewer components (than features) which can explain the maximum variance in the data set. By doing rotation, the relative location of the components doesnt change, it only changes the actual coordinates of the points.If we dont rotate the components, the effect of PCA will diminish and well have to select more number of components to explain variance in the data set.Know more: PCAQ3. You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?Answer: This question has enough hints for you to start thinking!Since, the data is spread across median, lets assume its a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values.Q4. You are given a data set on cancer detection. Youve build a classification model and achieved an accuracy of 96%. Why shouldnt you be happy with your model performance? What can you do about it?Answer: If you have worked on enough data sets, you should deduce that cancer detection results in imbalanced data. In an imbalanced data set, accuracy should not be used as a measure of performance because 96% (as given) might only be predicting majority class correctly, but our class of interest is minority class (4%) which is the people who actually got diagnosed with cancer. Hence, in order to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine class wise performance of the classifier. If the minority class performance is found to to be poor, we can undertake the following steps:Know more: Imbalanced ClassificationQ5. Why is naive Bayes so naive ?Answer: naive Bayes is sonaive because it assumes that all of the features in a data set are equally important and independent. As we know, these assumption are rarelytrue in real world scenario.Q6. Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?Answer: Prior probability is nothing but, the proportion of dependent (binary) variable in the data set. It is the closest guess you can make about a class, without any further information. For example: In a data set, the dependent variableis binary (1 and 0). The proportion of 1 (spam) is 70% and 0 (not spam) is 30%. Hence, we can estimate that there are 70% chances that any new emailwould be classified as spam.Likelihood is the probability of classifying a given observation as 1 in presence of some other variable. For example: The probability that theword FREE is used in previousspam message is likelihood. Marginal likelihood is, the probability that the word FREE is used in any message.Q7. You are working on a time series data set. You manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time seriesregression model and got higher accuracy than decision tree model. Can this happen? Why?Answer: Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non  linear interactions. The reason why decision tree failed to provide robust predictions because it couldnt map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions.Q8. You are assigned a new project which involves helping a food delivery company save more money. The problem is, companys delivery team arent able to deliver food on time. As a result, their customersgetunhappy. And, to keep them happy, they end up delivering food for free. Which machine learning algorithm can save them?Answer: You might have started hopping through the list ofML algorithms in your mind. But, wait!Such questions are asked to testyourmachine learning fundamentals.This is not a machine learning problem. This is a route optimization problem. A machine learning problem consist of three things:Always look for these three factors to decide if machine learning is a tool to solve a particular problem.Q9. You came to know thatyour model is suffering from lowbias and high variance. Which algorithm should you use to tackle it? Why?Answer: Low bias occurs when the models predicted values are near to actual values. In other words, the model becomes flexible enough to mimic the training data distribution. While it soundslike great achievement, but not to forget, a flexible model has nogeneralization capabilities. It means, when this model is tested on an unseen data,it gives disappointing results.In such situations, we can use bagging algorithm (like random forest) to tackle high variance problem. Bagging algorithms dividesa data set into subsets made with repeated randomized sampling. Then, these samples are used to generate a set of models using a single learning algorithm. Later, the model predictions are combined using voting (classification) or averaging (regression).Also, to combathigh variance, we can:Q10. You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?Answer: Chances are, you might be tempted to say No, but that would be incorrect. Discarding correlated variables have a substantial effect onPCA because, in presence of correlated variables, the variance explained by a particular component gets inflated.For example: You have 3 variables in a data set, of which 2 are correlated.If you run PCA on this data set, the first principal component would exhibit twice the variance than it would exhibit with uncorrelated variables. Also, adding correlated variables lets PCA put more importanceon those variable, which is misleading.Q11. After spending several hours, you are nowanxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could performbetter than benchmark score. Finally, you decided to combinethose models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?Answer: As we know, ensemble learners are based on the idea of combining weak learners to create strong learners. But, these learners provide superior result when the combined models are uncorrelated. Since, we have used 5 GBM models and got no accuracy improvement, suggests that the models are correlated. The problem with correlated models is, all the models provide same information.For example: If model 1 has classified User1122 as 1, there are high chances model 2 and model 3 would have done the same, even if itsactual value is 0. Therefore, ensemble learners are built on the premise of combining weak uncorrelated models to obtain betterpredictions.Q12. How is kNN different from kmeans clustering?Answer:Dont get mislead by k in their names. You should know that the fundamental difference between both these algorithms is, kmeans is unsupervised in nature and kNN is supervised in nature. kmeans is a clustering algorithm. kNN is a classification (or regression) algorithm.kmeans algorithm partitions a data set into clusters such that a cluster formed is homogeneous and the points in each cluster are close to each other. The algorithm tries to maintain enough separability between these clusters. Due to unsupervised nature, the clusters have no labels.kNN algorithm tries to classify an unlabeled observation based on its k (can be any number ) surrounding neighbors. It is also known as lazy learner because it involves minimal training of model. Hence,it doesnt use training data to make generalization on unseen data set.Q13. How is True Positive Rate and Recall related? Write the equation.Answer: True Positive Rate = Recall. Yes, they are equal having the formula (TP/TP + FN).Know more: Evaluation MetricsQ14. You have built a multiple regression model. Your model R isnt as good as you wanted. For improvement, your remove the intercept term, your model R becomes 0.8 from 0.3. Is it possible? How?Answer: Yes, it is possible. We need to understand the significance of intercept term in a regression model.Theintercept term showsmodel prediction without any independent variable i.e. mean prediction. The formula of R = 1  (y  y)/(y  ymean) where y is predicted value. When intercept term is present, R value evaluates your model wrt. to the mean model. In absence of intercept term (ymean), the model can make no such evaluation, with large denominator,(y - y)/(y)equations value becomes smaller than actual, resulting in higher R.Q15. After analyzing the model, your manager has informed that your regression model is suffering from multicollinearity. How would you check if hes true? Without losing any information, can you still build a better model?Answer: To check multicollinearity, we can create a correlation matrix toidentify & remove variables having correlation above 75% (deciding a threshold is subjective). In addition, we can use calculate VIF (variance inflation factor) to check the presence of multicollinearity.VIF value <=4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Also, we can use tolerance as an indicator of multicollinearity.But, removing correlated variables might lead to loss of information. In order to retain those variables, we can use penalizedregressionmodels like ridge or lasso regression. Also, we can add some random noise in correlated variable so that the variables become different from each other. But, adding noise might affect the prediction accuracy, hence this approach should be carefully used.Know more: RegressionQ16. When is Ridge regression favorable over Lasso regression?Answer: You can quoteISLRs authors Hastie, Tibshirani who asserted that, in presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective.Know more: Ridge and Lasso RegressionQ17. Rise in global average temperature led to decrease in number of pirates around the world. Does that mean that decrease in number of pirates caused the climate change?Answer: After reading this question, you should have understoodthat this is a classic case of causation and correlation. No, we cant conclude thatdecrease in number of pirates caused the climate change because there might be other factors (lurking or confounding variables) influencing this phenomenon.Therefore, there might be a correlation between global average temperature and number of pirates, but based on this information we cant say that pirated died because of rise in global average temperature.Know more: Causation and CorrelationQ18. While working on a data set, how do you select important variables? Explain your methods.Answer: Following are the methods of variable selection you can use:Q19. What is the difference between covariance and correlation?Answer:Correlation is the standardized form of covariance.Covariances are difficult to compare. For example: ifwe calculate the covariances of salary ($) and age (years), well get different covariances whichcant be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale.Q20. Is it possiblecapture the correlation between continuous and categoricalvariable? If yes, how?Answer: Yes, we can useANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.Q21. Both being tree based algorithm, how is random forest different from Gradient boosting algorithm (GBM)?Answer:The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.In bagging technique, a data set is divided into n samples using randomized sampling. Then, using a single learning algorithm a model is build on all samples. Later, the resultant predictions are combined using voting or averaging. Bagging is done is parallel. In boosting, after the first round of predictions, the algorithm weighs misclassified predictions higher, such that they can be corrected in the succeeding round. This sequential process of giving higher weights to misclassified predictions continue until a stopping criterion is reached.Random forest improves model accuracy by reducing variance (mainly). The trees grown are uncorrelated to maximize the decrease in variance. On the other hand, GBM improves accuracy my reducing both bias and variance in a model.Know more: Tree based modelingQ22. Running a binary classification tree algorithm is theeasy part. Do you know how does a tree splitting takes place i.e. how does the tree decide which variable to split at the root node and succeeding nodes?Answer:A classification trees makes decision based on Gini Index and Node Entropy. In simple words, the tree algorithm find the best possible feature which can divide the data setinto purest possible children nodes.Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure. We can calculate Gini as following:Entropy is the measure of impurity as given by (for binary class):Here p and q is probability of success and failure respectively in that node. Entropy is zero when a node is homogeneous. It is maximum when a both the classes are present in a node at 50%  50%. Lower entropy is desirable.Q23. Youve built a random forestmodel with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on? Havent you trained your model perfectly?Answer:The model has overfitted. Training error 0.00 means the classifier has mimiced the training data patterns to an extent, that they are not available in the unseendata. Hence, when this classifier wasrun on unseen sample, it couldnt find those patterns and returned prediction with higher error. In random forest, it happens when we use larger number of trees than necessary. Hence, to avoid these situation, we should tune number of trees using cross validation.Q24. Youve got a data set to work havingp (no. of variable) > n (no. of observation). Why is OLS as bad option to work with? Which techniqueswould be best to use? Why?Answer: In such high dimensional data sets, we cant use classical regression techniques, since their assumptions tend to fail. When p > n, we canno longer calculate a unique least square coefficient estimate, the variances become infinite, so OLS cannot be used at all.To combat this situation, we can use penalized regression methods like lasso, LARS, ridge which can shrink the coefficients to reduce variance. Precisely, ridge regression works best in situations where the least square estimates have higher variance.Among other methods include subset regression, forward stepwise regression.Q25. What is convex hull ? (Hint: Think SVM)Answer:In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.Q26.We know that one hot encoding increasing the dimensionality of a data set. But, label encoding doesnt. How ?Answer: Dont get baffled at this question. Its a simple question asking the difference between the two.Using one hot encoding, the dimensionality (a.k.a features) in a data set get increased because it creates a new variable for each level present in categorical variables. For example: lets say we have a variable color. The variable has 3 levels namely Red, Blue and Green. One hot encoding color variable will generate three new variables as Color.Red, Color.Blue and Color.Green containing 0 and 1 value.In label encoding, the levels of a categorical variables gets encoded as 0 and 1, so no new variable is created. Label encoding is majorlyused for binary variables.Q27.What cross validation technique would you use on time series data set? Is it k-fold or LOOCV?Answer:Neither.In time series problem, k fold can be troublesome because there might be some pattern in year 4 or 5 which is not in year 3. Resampling the data set will separatethese trends, and we might end up validation on past years, which is incorrect. Instead, we can use forward chaining strategy with 5 fold as shown below:where 1,2,3,4,5,6 represents year.Q28. You are given a data set consisting of variables having more than 30% missing values? Lets say, out of 50 variables, 8 variables have missing values higher than 30%. How will you deal with them?Answer: We can deal with them in the following ways:29. People who bought this, also bought recommendations seen on amazon is a result of which algorithm?Answer:The basic idea for this kind of recommendation engine comes from collaborative filtering.Collaborative Filtering algorithm considers User Behavior for recommending items. They exploit behavior of other users and items in terms of transaction history, ratings, selection and purchase information. Other users behaviour and preferences over the items are used to recommend items to the new users. In this case, features of the items are not known.Know more: Recommender SystemQ30. What do you understand byType I vs Type II error ?Answer: Type I error is committed when the null hypothesis is true and we reject it, also known as a False Positive. Type II error is committed when the null hypothesis is false and we accept it, also known as False Negative.In the context of confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a value as negative (0) when it is actually positive(1).Q31. You are working on a classification problem. For validation purposes, youve randomly sampled the training data set into train and validation. You are confident that your model will work incredibly well on unseen data since your validation accuracy is high. However, you get shocked after getting poor test accuracy. What went wrong?Answer: In case of classification problem, we should always use stratified sampling instead of random sampling. A random sampling doesnt takes into consideration the proportion of target classes. On the contrary, stratified sampling helps to maintain the distribution of target variable in the resultant distributed samples also.Q32. You have been asked to evaluatea regression model based on R, adjustedR and tolerance. What will be your criteria?Answer: Tolerance (1 / VIF) is used as an indicator of multicollinearity. It is an indicator of percent of variance in a predictor which cannot be accounted by other predictors. Large values of tolerance is desirable.We will consider adjusted R as opposed to R to evaluate model fit because R increases irrespective of improvement in prediction accuracyas we add more variables. But, adjusted R would only increase if an additional variable improves the accuracy of model, otherwise stays same. It is difficult to commit a general threshold value for adjusted R because it variesbetween data sets. For example: a gene mutation data set might result in loweradjusted R and still provide fairly good predictions, as compared to a stock market data whereloweradjusted R implies that model is not good.Q33. In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance ?Answer: We dont use manhattan distance because itcalculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, euclidean metriccan be used in any space to calculate distance. Since, the data points canbe present in any dimension, euclidean distance is a more viable option.Example: Think of a chess board, the movement made by a bishop or a rook iscalculated by manhattan distance because of their respective vertical & horizontal movements.Q34. Explain machine learning to me like a 5 year old.Answer: Its simple. Its just like how babieslearn to walk. Every timethey fall down, they learn (unconsciously) & realize that their legs should be straight and not in a bend position. The next time they fall down, they feel pain. They cry. But, they learn not to stand like that again. In order to avoid that pain, theytry harder. To succeed, they even seek support from the door or wall or anything near them, which helps them stand firm.This is how a machine works &develops intuitionfrom its environment.Note: The interview is only trying to test if have the ability of explain complex concepts in simple terms.Q35. I know that a linear regression model is generally evaluated using Adjusted R or F value. How would you evaluate a logistic regression model?Answer:We can use the following methods:Know more: Logistic RegressionQ36. Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?Answer: You should say, the choice of machine learning algorithm solely depends of the type of data. If you are given a data set which is exhibits linearity, then linear regression would be the best algorithm to use. If you given to work on images, audios, then neural network would help you to build a robust model.If the data comprises of non linear interactions, then a boosting or bagging algorithm should be the choice. If the business requirement is to build a model which can be deployed, then well use regression or a decision tree model (easy to interpret and explain) instead of black box algorithms like SVM, GBM etc.In short, there is no one master algorithm for all situations.We mustbe scrupulous enough to understand which algorithm to use.Q37. Do you suggest that treatinga categorical variable as continuous variable would result in a better predictive model?Answer:For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.Q38.When does regularization becomes necessary in Machine Learning?Answer: Regularization becomes necessary when the model begins to ovefit / underfit. This techniqueintroduces a cost term for bringing in more features with the objective function. Hence, it tries to push the coefficients for many variables to zero and hence reduce cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing).Q39. What do you understand by Bias Variance trade off?Answer:The error emerging fromany model can be broken down into three components mathematically. Following are these component :Bias error is useful to quantify how much on an average are the predicted values different from the actual value. A high bias error means we have a under-performing model which keeps on missing important trends.Variance on the other side quantifies how are the prediction made on same observation different from each other. A high variance model will over-fit on your training population and perform badly on any observation beyond training.Q40. OLS is to linear regression. Maximum likelihood is to logistic regression. Explain the statement.Answer: OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words,Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting inminimum distance between actual and predicted values.Maximum Likelihood helpsin choosing the the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data.You might have been able to answer all the questions, but the real value is in understanding them and generalizing your knowledge on similar questions. If you have struggled at these questions, no worries, now is the time to learn and not perform. You should right now focus on learning these topics scrupulously.These questions are meant to give you a wide exposureon the types of questions asked at startups inmachine learning. Im sure these questions would leave you curious enough to do deeper topic research at your end. If you are planning for it, thats a good sign.Did you like reading this article? Have you appeared in any startup interview recently for data scientist profile? Do share your experience in comments below. Id love to know your experience.",https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/
AWS / Cloud Engineer  Pune ( 4+ Years of Experience ),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|40 Interview Questions asked at Startups in Machine Learning / Data Science|Skilltest Statistics II  Solutions|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation:AWS / Cloud EngineerLocation: PuneExperience Required:4+ YearsAbout the Employer: ConfidentialJob DescriptionIn this role as a AWS / Cloud Engineer, the candidate will be required to work on deploying and maintaining production code for a companys products. The candidate will also be in charge of implementing versions of the previously mentioned product tailored to the specific needs of the customers. As a back-end engineer the individual will be closely working with the R&D team and the product team. The required candidate is expected to have expertise in Java programming and be able to ship highly stable & reusable text. Leadership and ownership skills for projects with timely delivery is expected out of the candidate. The individual should becomfortable working in an agile environment. Ateam player who also knows how to work independently will be the best fit. The candidate should be eager to share knowledge and best practices among the peers and should be willing to mentor juniorcolleagues.RequirementsValuable plus (not required)Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as AWS / Cloud Engineer  Puneand following details:",https://www.analyticsvidhya.com/blog/2016/09/aws-cloud-engineer-pune-4-years-of-experience/
Skilltest Statistics II  Solutions,Learn everything about Analytics|Introduction|Overall Results|End Notes,"Skilltest Questions and Answers|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|AWS / Cloud Engineer  Pune ( 4+ Years of Experience )|How to prepare for your first data science hackathon in less than 2 weeks?|
Analytics Vidhya Content Team
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Statistics is one of the key ingredient any data scientist must know to havea long successful career in data science industry. After the overwhelming response from Skilltest Statistics we immediately launched Skilltest Statistics II. We launched this unique self-assessment challenge for individuals to test their knowledge and identify their strengths & weaknesses.As we had promised the Skilltest Statistics II will be more advanced, we delivered some nerve wrenching questions. The heat during the competition was intense and we kept wondering who will make it to the top this time.More than 1100 participants registered for the challenge and we received around 401submissions. A loud applaud to all the participants for making it a big success and heartiest congratulations to the winners.Who could have asked fora better way to analyze the results of a statistical skill test on this topic? Here is the distribution of the scores:Here are a few measures of the distribution:Mean =10.86Median =11.00Let us look at the variance:Standard Deviation = 8.3595% confidence interval  [0, 27.23)So, congratulations for the top 3 people (27and above) to set themselves above the rest of the population.If your score is more than 18, you are in the top 25 percentile  you deserve a pat!The skill test consisted of 40 questions based on hypothesis testing, t-test and correlation. We think any data scientist should know these concepts like back of their hand.Read on to find out detailed explaination of each question.1)Numerous studies have demonstrated that listening to music while studying can improve memory. To demonstrate this phenomenon a researcher obtains a sample of 36 students and gives them a standardised memory test while they listen to music. The sample mean of the students score in the test was observed to be 28(while they listen to music). Under normal circumstances(without music), the scores of the test form a normal distribution with a mean of 25 and S.D of 6. Use alpha = 0.05 (one tailed test). Which of the following best represents a null hypothesis?a) Listening to music while studying will not impact memoryb)Listening to music while studying may worsen memoryc) Listening to music while studying may improve memoryd) Listening to music while studying will not improve memory and mightmake it worse.Ans:d) Listening to music while studying will not improve memory and mightmake it worse.For more detailed explaination read here explaination 1 and explaination 2.
2)Numerous studies have demonstrated that listening to music while studying can improve memory. To demonstrate this phenomenon a researcher obtains a sample of 36 students and gives them a standardised memory test while they listen to music. The sample mean of the students score in the test was observed to be 28(while they listen to music). Under normal circumstances(without music), the scores of the test form a normal distribution with a mean of 25 and S.D of 6. Use alpha = 0.05 (one tailed test).Which of the following best represents an alternate hypothesis?a) Listening to music while studying will not impact memoryb) Listening to music while studying will worsen memoryc) Listening to music while studying will improve memoryd) Listening to music while studying will not improve memory and mightmake it worse.Ans: c) Listening to music while studying may improve memorySince the null is not improving memory and actually make it worse the alternate should be may improve memory.For more detailed explaination read here explaination 1 and explaination 2.3)Numerous studies have demonstrated that listening to music while studying can improve memory. To demonstrate this phenomenon a researcher obtains a sample of 36 students and gives them a standardised memory test while they listen to music. The sample mean of the students score in the test was observed to be 28(while they listen to music). Under normal circumstances(without music), the scores of the test form a normal distribution with a mean of 25 and S.D of 6. Use alpha = 0.05 (one tailed test).What is your statistical decision? (Hint: Use z-test)a) Reject the nullb) Accept the nullc) Cant sayd) None of theseAns:a) Reject the null. The standard error = 1 which gives the z-value (28  25)/1 = 3.Clearly z = 3 has a p-value less than 0.05 so the null is rejected.For detailed explaination read here. 4)Given the null hypothesis: that a process is producing no more than the maximum allowable rate of defective items. In this situation, a type II error would be:a)to conclude that the process is producing too many defectives when it actually is notb)to conclude that the process is not producing too many defectives when it actually isc)to conclude that the process is not producing too many defectives when it is notd)to conclude that the process is producing too many defectives when it isAns: b) to conclude that the process is not producing too many defectives when it actually isA type II error is a statistical term used within the context of hypothesis testing that describes the error that occurs when one accepts a null hypothesis that is actually false.For a detailed explainationread here.5) Which of these is known as Type 1 errora) Ab) Bc) Cd) DAns:a) AIn statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a false positive), while a type II error is incorrectly retaining a false null hypothesis (a false negative).For a detailed explainationread here.6) If the P-value of a distribution is less than alpha, we can say that :a)sample is not statistically significantb)failed to reject null hypothesisc)Sample is statistically significantd)Insufficient dataAns: c) Sample is statistically significantWhen p-value is less than alpha, we reject the null hypothesis. In other words we have evidence to believe that the sample statistic is significantly different from the population parameter.For further clarification read here.7) Given a sample, which of the following tests can be used to determine whether the sample is statistically significant when population standard deviation is unknowna)Z-testb)t-testc)F-testd)a & bAns: d) z-test or t-testGiven a large sample z-test is used, whereas a t-test is used for for a small sample even if population parameter is unknown.For detailed explaination read here.8) Given a sample, which of the following tests can be used to determine whether the sample is statistically significant when the sample size is small (n<30)?a)Z-testb)t-testc)F-testd)a & bAns: b) t-testFor a small sample(n < 30) t-test is used.For further clarification read here.9) A hypothesis test is conducted and values of the sample mean and sample standard deviation when n = 25 are such that they do not lead to the rejection of H0. You calculate a p-value of 0.0667. What will happen to the p-value if you observe the same sample mean and standard deviation for a sample > 25?a)Increaseb)Decreasec)Stays the samed)May increase or decreaseAns:b) decreaseIncreasing the sample size decreases the standard error, which in turn increases t-value/z-value thus decreasing the p-value.10)Population mean = 50, Sample mean = 37, Standard Deviation of the Population = 10, Sample Size = 4. Taking alpha=0.05, can we reject the null hypothesis(two-tailed test)?a)Yesb)Noc)Can be rejected for all alphad)None of theseAns: a)YesStandard error = 10/2 = 5. t-value = (50  37)/5 = 2.6, looking at the t-table with 4-1 = 3 degrees of freedom we reject the null hypothesis.11) Students of the Chinese population have a mean learning rate of 7.47 with a standard deviation of 2.41. All the students of the Chinese population are provided with tablets so as to improve their learning rate. The Chinese government draws a sample of 50 students and found their new learning rate(after providing tablets) to be 8.3. Which box best fits the scenario, if the population learning rate after providing tablets was found to be 7.8 (a few years later)?a) Ab) Bc) Cd) DAns:a) AStandard error = 2.41/sqrt(50). The z-value of the sample can be calculated to be 2.44=((8.3  7.47)/SE). Which rejects the null hypothesis. Similarly the actual learning rates z-value can be calculated to be 0.97 which accepts the null hypothesis. This is a typical case of Type 1 error.12) A two-tailed test is one where:a)results in only one direction can lead to rejection of the null hypothesisb)negative sample means lead to rejection of the null hypothesisc)results in either of two directions can lead to rejection of the null hypothesisd)no results lead to the rejection of the null hypothesisAns:c) results in either of two directions can lead to rejection of the null hypothesisFor a sample to be statistically significant(or synonymously rejecting the null hypothesis), the sample statistic can be either significantly lesser or greater that the population parameter. Hence c).13) There are two schools namely DAV and St.Mary. Population parameters of marks of two schools are: Mean (DAV)=70.79, Mean (St. Mary)= 44.45, STDEV (DAV)=17.34, STDEV (St. Mary)= 26.17. Can you conclusively say that students of DAV score more than students of St. Mary(with 95% confidence level)?a)Yesb) Noc) Insufficient Datad) Cant sayAns: a)YesMean (DAV)=70.79Mean (St. Mary)= 44.45STDEV (DAV)=17.34STDEV (St. Mary)= 26.17Step1: Find the Mean difference of both school and combined standard deviation using formulaMean difference = 70.79  44.45 = 26.34n1 for DAV = 107n2 for St Mary =140Combined Standard Deviation = sqrt( 17.45*17.45/107 + 26.17*26.17/140) = 2.78Step2: Calculate the Z valueZ= 26.34 / 2.78 = 9.47Here Z value is very high, which makes the p value ~0 (look at the z-table). So yes we can say that students of DAV score more than students of students of St. Mary(95% confidence).For more detailed explaination read here.14) There are two schools namely DAV and St.Mary. Population parameters of marks of two schools are: Mean (DAV)=70.79, Mean (St. Mary)= 44.45, STDEV (DAV)=17.34, STDEV (St. Mary)= 26.17. Find the p-value of the difference of marks between the schools(closest value).a)0b)0.1c)0.2d)0.3Ans: a) 0As calculated in the above question.15) What are the number of degrees of freedom for a N X N magic squarea)N^2b)N-1^2c)N * N-1d)(N-2)^2Ans: b) (N-1)^2In a magic square the sum of rows and columns should add up to a specified number. So in a n X n magic square we are free to fill n-1 cells in every row and n-1 cells in every column so that the last number can be fixed in such a way that the row sum or the column sum sums up to the specified number.16) For a t-distributiona)The curve is wider than the population distribution curveb)Curve becomes steeper on increasing the sample sizec)Curve becomes steeper on increasing the number of observationsd)All of theseAns: d) All of thesea) Curve is wider because of high standard error as compared to a z-distribution.b) Standard error reduces as sample size increases so curve becomes steeper.c) Synonymous to (b)So d) is the answer.17) Consider the following sample: 1,2,3,4,5,6. Alpha = 0.05 and the population mean is equal to 4.5. Find the standard deviation for the sample to be used for calculating t-value.a)1.87b)1.95c)2.03d)2.53Ans:a) 1.87Standard deviation = sqrt(sum((mean  observation value)^2)/number of observations)18) Consider the following sample: 1,2,3,4,5,6. Alpha = 0.05 and the population mean is equal to 4.5. Which of the following is true?a)Sample lies is the critical region of the t-distribution.b)sample does not lie in the critical region of the t-distribution.c)May or may not lied)None of theseAns:b)sample does not lie in the critical region of the t-distribution.Case 1: Two Tailed:
Critical Value of t = -2.571 or 2.571
Case 2: One Tailed:
Critical Value of t = -2.015 for left sided or 2.015 for right sidedt Statistic = -1.309Therefore, in either of the cases the t Statistic does not lie in the critical region.19) Consider the following sample: 1,2,3,4,5,6. Alpha = 0.05 and the population mean is equal to 4.5. The cohens d is given by:a)-0.65b)-0.53c)-0.44d)-0.78
Ans: b)-0.53Cohens d = (sample mean  population mean)/standard deviation, which can be calculated to be -0.53For further explaination read here.20) Consider the following sample: 1,2,3,4,5,6. Alpha = 0.05 and the population mean is equal to 4.5. Calculate the coefficient of determination for this data.a)0.36b)0.31c)0.25d)0.22Ans: c)0.25r-squared/coefficient of determination = 1  RSS/TSSWhere RSS(Residual sum of squares) = sum((mean of sample  observation)^2) andTSS(Total sum of squares) = sum((population mean  observation)^2)For further explaination read here.21) Consider the following sample: 1,2,3,4,5,6. Alpha = 0.05 and the population mean is equal to 4.5(two-tailed test). Calculate the margin of error for the sample.a)0.5b) 1c) 1.5d) 2
Ans: c) 1.5Margin of error = critical value * standard errorstandard deviation = 1.87, standard error = 1.87/sqrt(6) = 0.76Critical value = 1.96 (for alpha = 0.05 in two tailed test)Margin of error = critical value * standard error = 1.5 22) What is the degree of freedom for between group variability of k samples in a population of n elements?a)n-1b)nc)n-kd)n-k-1Ans: c) n-kSince the variability is k, the degree of freedom should be n-k.23) Calculate the r-square value for a sample with t-value of 1.5 and 35 sizea)0.04b)0.05c)0.06d)0.07Ans: c)0.06r-squared = t^2/(t^2 + df)= 2.25/(2.25 + 35) = 0.0624) F-test is a unidirectional test to compare samples of a population.a) Trueb) Falsec) Cannot be determinedAns: b)FalseF-test is not a uni-directional test.25) Which can be considered as an effect size indicator?a)r-squaredb) rc) eta-squaredd) all the threeAns: d)all threeAll three given quantities are measures of effect size.26) A statistical test used to compare two or more means is called ?a)one way analysis of varianceb) t-test for correlation coefficientsc)chi-square test for contingency tablesd)None of theseAns: a)ANOVAAmong the given tests only ANOVA can compare two samples, although t-test can also be used but it isnt mentioned here.27) Given the following plots of two variables, which among the plots suggest a non-correlation between the variables?a)b)c)d) None of theseAns:d) None of theseThe common misconception is that correlation is considered as linear correlation and c) could be thought to be the right answer. But those variables have a trigonometric correlation.28) GPA scores of two samples of students are given below. Sample A is the scores of students who sleep for around 8 hours a day and Sample B is the scores of students who slept around 6 hours a day. Sample A: 5,7,5,3,5,3,3,9. Sample B: 8,1,4,6,6,4,1,2. The School tries to study the impact of sleep on scores. Which tests would help us identify the impact of sleep?a)t-testb)ANOVAc)chi-squaredd)both a & bAns: d)Both t-test and ANOVA can be usedTwo samples can be compared by one way analysis of variance or two sample t-test.29) GPA scores of two samples of students are given below. Sample A is the scores of students who sleep for around 8 hours a day and Sample B is the scores of students who slept around 6 hours a day. Sample A: 5,7,5,3,5,3,3,9. Sample B: 8,1,4,6,6,4,1,2. The School tries to study the impact of sleep on scores. What are the degrees of freedom to test this hypothesis?a) 7b) 8c) 14d) 15Ans: c)14degrees of freedom = Size of sample A + Size of sample B  2.30) GPA scores of two samples of students are given below. Sample A is the scores of students who sleep for around 8 hours a day and Sample B is the scores of students who slept around 6 hours a day. Sample A: 5,7,5,3,5,3,3,9. Sample B: 8,1,4,6,6,4,1,2. What is the t-value(closest value)?a)0.847b)2.146c)6.614d) None of theseAns: a)0.847MeanA (Mean of Sample A) = 5S1 (standard deviation of Sample A) = 2.138MeanB (Mean of Sample B) = 4S1 (standard deviation of Sample B) = 2.563n1 =n2 = 8t-statistic for two samples is given by (MeanA  MeanB)/sqrt(S1^2/n1 +S2^2/n2) = 0.847For detailed explaination read here.31) GPA scores of two samples of students are given below. Sample A is the scores of students who sleep for around 8 hours a day and Sample B is the scores of students who slept around 6 hours a day. Sample A: 5,7,5,3,5,3,3,9. Sample B: 8,1,4,6,6,4,1,2. Does sleep affect GPA?
a) Yesb) Noc) Not enough datad) Cant conclusively sayAns: d) Cant conclusively sayWe can never conclusively say, we only have evidence to support our claim.32) The chi-square goodness-of-fit test can be used to test for:a)significance of sample statisticsb)normalityc)difference between population meansd)probabilityAns: b) NormalityChi-squared test is a test for normality.For further explaination read here.33) The ANOVA test is based on which assumptions?a)the sample are randomly selectedb)the population variances are all equal to some common variancec)the populations are normally distributedd) b and cAns:d) b and cThe ANOVA test being a frequentist test always assumes population parameters to be fixed and the population to be normally distributed.For further explaination read here.34) The chi-square test can be too sensitive if the sample is:a)very smallb)very largec)homogeneousd)non-homogeneousAns: b)very largeChi-square test has limitations, one such is sample size.For detailed explaination read here.35) One way ANOVA is used when:a) analysing the difference between more than two population meansb)analysing the difference between two population meansc)analysing the results of a two-tailed testd)analysing the results from a two tailed testAns: a)analysing the difference between more than two population meansFor further clarification go through this video.36) An investor obtains a random sample of 20 daily price changes for stock 1 and 20 daily price changes for stock 2. He wants to compare whether the stocks are similar in nature. These data are shown in the table below. Use alpha = 0.10. With have evidence to believe that the stocks are:a)Not similar with p-value < 0.025b)Similar with p-value > 0.2c)Not similar with a p-value between (0.02 to 0.1)d)Similar with p-value between 0.1 to 0.2Ans: a)not similar with a p-value < 0.25n1 = 20, s1 = 0.8487, n2 = 20, s2 = 0.5291Null hypothesis: (sigma1^2/sigma2^2) = 1Alternate hypothesis: (sigma1^2/sigma2^2) is not equal to 1F-statistic = sigma1^2/sigma2^2 = 2.573P-value=0.023Since the P-values is less than 0.10, we reject the null hypothesis of equal variances and conclude that the variances of the stocks are not equal at the 10% level.37) Given are yearly salaries of 18 recent graduates from 3 different departments. At the 0.05 level of significance, is there evidence of a significant difference in average salary among the various departments?(Select the most appropriate option).a)Yes, with a p-value < 0.05b)Yes, with a p-value < 0.005c)Yes, with a p-value < 0.0005d)No, with a p-value > 0.05Ans: c)Yes, with a p-value < 0.0005To test at the 0.05 level of significance whether the average salary is different across the three departments, we conduct an F test: H0: u1 = u2 = u3 H1: At least one mean is different.Since p-value = 0.0004 < alpha = 0.05, we reject H0. There is enough evidence to conclude that the average sales volumes in thousands of dollars are different across the three store aisle locations.38) Adding more variables to a linear model willa)Increase or not change the R-squared valueb)decrease the R-squared valuec)not change the R-squared valued)None of theseAns: a) Increase or not change the R-squared valueAdding more variables can increase or not change the R-squared value39) The following data lists eight different investment amounts (X) and the amount of interest they earned (Y). For the linear model Y ~ X the R-squared value will?1000300010000500020005005015050025010025a)positive and close to zerob)positive and close to onec)exactly oned)negativeAns: c) equal to oneIt is evident that the regression co-efficient is 1. Still formulas application will give the same result.For further explaination read here.40) In the above data, the sector of each investment is added. When compared to the R-squared value of the linear model Y ~ X, the multiple R-squared value of the model Y ~ X + Z will ?a) increaseb) decreasec) remain the samed) insufficient dataAns: c)remains the sameSince variables Y and X already correlated with R-squared value 1 (the maximum value), adding a new variable wont impact the model.How was your experience taking the second skill test in Statistics? This time the enthusiasm was double and the eagerness to take the second challenge in Statistics was immeasureable. In the above article we tried to answer all your queries but if you have any doubts / confusions in any question then bombard the comments with all your queries and we will be happy to answer them. Dont forget to share your feedback with us, because how will be improve if you dont tell us what to change.If you are a Python practitioner then you must register for the upcoming Skilltest  Python for Data Science.Do you know good probability skills are your sure shot way to develop strong data analysistechniques? Take our unique probability workshops AV Casino I and AV Casino II.",https://www.analyticsvidhya.com/blog/2016/09/skilltest-statistics-ii-solutions/
How to prepare for your first data science hackathon in less than 2 weeks?,Learn everything about Analytics|Table of Contents|How are Data Science / Machine Learning hackathons different?|Which tool should I choose  R / Python / SAS /Spark?|Where do you start? What is the roadmap?|A few other tips|Conclusion,"The Path for Beginners|Resources /Path for Intermediate practitioners|Resources / Path for advanced practitioners|Check out LiveCompetitionsand compete with the bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Skilltest Statistics II  Solutions|Manipal Global Academy of Data Science Launches Full Time & Part Time Data Science Program|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Hackathons are super fun! The thrill of finding a solution in a time bound, high pressure, competitive situation is addictive. However, if you are participating in a data science hackathon for the first time, the experience can be a bit intimidating.Which tool should you pick? Which is the best algorithm to apply on the problem statement? How do you even begin to contemplate the steps and the structure required to succeed in a hackathon?If you have been thinking about taking the plunge and participating in your first hackathon, this is the perfect guide for you! Even if you have taken part in a few hackathons, read on to get some tips on how you can potentially improve on your previous attempts.First things first, let us spend a few minutes understanding how data science / machine learning hackathons are different from other hackathons you might have attended in the past. For people participating in a data science hackathon for the first time, the experience can be a bit overwhelming. Why?There could be several reasons for this:I hope this has given you a fair idea about what goes on during a data science hackathon.If you are preparing for an upcoming hackathon, you dont have a lot of time to debate on this, and trust me it ends up helping more than restricting you in many ways. If you know any of these tools already, just use that tool / language and run with it. Focus on problem solving rather than learning a new tool.If you are completely new to data science and dont know any of the above tools  just pick Python and it will serve you well. I dont want to start a language war here  but my reasons for picking Python are that it is easier to learn and comes in handy with a larger ecosystem and production readiness. It is also clearly the most popular language currently being using in areas like deep learning.I assume you have either gone through the resources mentioned above, or have experience of solving a few practice problems in the past.So, you have been doing data science for some time now and you know the work flow well. You have mastered the art of handling different kind of variables and applied it to a few problems already. You would also have participated and got a high rank in a few hackathons already. Now is the time to put on your flying boots!At this point, you should have all the technical resources you need to make a killing in a hackathon. But, mastering the art of winning a hackathon actually takes much more than these technical skills. I have included a few (behavioural) tips from my experience. You can also read the tips from some of the past winners here.So, if I was a first time hackathon participant, I would make sure I understand the data science workflow well. I would focus on using one tool  likely Python for the ease of it (but this isnt set in stone) and make sure I focus on getting my fundamentals right. Believe me, this should be enough to make a splash! So what are you waiting for  go, make your mark!If you have any questions about gearing up for the hackathons, please feel free to ask them here. Alternately, if you have a suggestion which I have missed out highlighting  please add it below in the comments.",https://www.analyticsvidhya.com/blog/2016/09/how-to-prepare-for-your-first-data-science-hackathon-in-less-than-2-weeks/
Manipal Global Academy of Data Science Launches Full Time & Part Time Data Science Program,Learn everything about Analytics|Introduction,"||You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|How to prepare for your first data science hackathon in less than 2 weeks?|Project Manager  Pune / Kolkata ( 8+ Years of Experience)|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is a blessing to see an industry with your passion grow leaps and bounds. I was probably lucky to get into analytics and data science back in 2006, when these terms were not in fashion! But, things are different today and it is heartening to see so many aspiring data scientists and aspiring analysts around.With an aim to propel Indias growth in data science further, one of the Indias leading technology-enabled learning solutions provider, Manipal Global Education Service has forayed into the education of data science and analytics.I got a chance to interact withDr. S Ramesh Babu, Director & Head, Manipal Global Academy of Data Science.Dr. Babu has two decades of experience in capacity and capabilities building in the Technology space. He worked at Infosys for 19 years, and prior to that in Tata Research Design and Development Center at Tata Consulting Services. His last role has been Associate Vice President at Infosys.Here are few excerpts from my interaction with him.Dr S Ramesh BabuKJ: Tell us how and what made you think about starting a program in data science? What is the objective of this program?Dr. Babu: Hailed as the hottest job of the twenty-first century, data science and analytics is one of the fastest growing fields in the industry. Career opportunities are set to explode in data science, with India alone raking up a requirement of over 200,000 data analysts.As Indias leading technology-enabled learning solutions provider, Manipal Global Education Service has forayed into the education of data science and analytics through its newest initiative, the Academy of Data Science. The academy offers customized programs across various levels and is designed specifically for industries and domains where such talent is in demand.The Objective of the program is to build capacity and capabilities through full suite learning interventions, in the Data Science area for both individuals and corporates, who are looking at career shift into the Data Science field or those who are looking at career advancement in the Data Science field. This is done through a combination of best-in-class academic, IT and data science professionals, state-of-the-art infrastructure, etc that adheres to our core philosophy: Talent. Technology. Transformation.KJ: What is the course structure / content like?Dr. Babu:Ill let this figure do the talk!KJ:What tools and techniques are covered as part of the course?Dr. Babu: R, Databases (Sql an NO-sql) , Excel, Hadoop Ecosystem, Apache Spark and Storm, Data Visualization Tools, along with some of the proprietary products.KJ:Who is the target audience for the program?Dr. Babu: The program is targeted at those, who are looking at a career shift from other streams into Data Science career stream, and also those who are looking at deep diving into the Data Science career stream.Fresh Graduates who completed their B.E. or B.Tech. or Post graduation (M.E/M.Tech/MBA/MCA etc) and looking to build a career in data science can opt for the full time PG Diploma in Data Science.Working professionals who are looking to upgrade their skills or make a career shift in the Data science field can opt for the part time PG Diploma in Data Science.Corporate who are looking at learning interventions for their teams can opt for the modular and customized offerings.KJ: Since the program is also designed for freshers how do you make them industry ready?Dr. Babu: The industry partners bring the relevance through multiple touch points. The faculty dealing with these courses have been in the field and in the industry. They will bring the expertise and experience into the fold. In addition, Freshers will get Hands-on Experience on tools, they also get to work on real time case studies. Furthermore, there are behavioral training modules to help them transition to corporate. Then, there is an internship provided to students in 4th term with our corporate partners. All these will help them to be productive from the day one.KJ:Tell us about the faculty associated with the program and their experience?Dr. Babu:The faculty conducting the courses have significant years of experience working in the Industry and in the relevant areas. Forother faculty associated with the program clickhere.KJ:What are some of the unique aspects of the course, which makes this course stand out for a student?Dr S Ramesh Babu: The program has a layered approach to build competencies in Data Science  foundations, core and advanced. It has high level of Industry relevance through partnerships with large enterprises with prevalent data science practices. And finally a PG Diploma that is awarded by Manipal University.KJ:How about industry involvement?Dr. Babu: We are constantly seeking partnerships with Industry  presently Experian is our Knowledge Partner and Genpact is our Corporate Partner. We are in discussions with multiple other large enterprises across Technology, BFSI etc who will bring the relevance, industry exposure through learning interventions, share cases, and drive domain focus.KJ:What is the placement assistance provided?Dr. Babu: Yes, there is placement assistance provided. In addition to the current industry partners, we are working with multiple Banks, IT companies, and analytics companies for the placement towards the last term.KJ: What is the fee for the program?Dr. Babu: The Full-time Program Fee includes:   The Part-time Program Fee includes:Educational loans are facilitated too with our tie ups with Axis Bank, Avanse Financial Services etc.KJ:When does the batch begin and how can our community members enroll into the course?Dr. Babu: The batch begins in the 1st Week of October. The last date to apply for the program is 15th Sep. For more information log on to the course website.KJ: Thank you Dr. Babufor your time. I enjoyed talking to you and wish you success for this program. Thanks for providing these details about the course for our community members.",https://www.analyticsvidhya.com/blog/2016/09/manipal-global-academy-of-data-science-launches-full-time-part-time-data-science-program/
Project Manager  Pune / Kolkata ( 8+ Years of Experience),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|Manipal Global Academy of Data Science Launches Full Time & Part Time Data Science Program|Data Crawling Engineer  Pune / Kolkata ( 4-6 Years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation: Project ManagerLocation: Pune / KolkataExperience: 8+ YearsAbout employer: ConfidentialDescriptionThe Project Manager will take the lead role in overseeing the delivery of AI-based products for clients in retail and fashion industry. She/he will manage the local engineering team supporting the development and delivery of the AI products. An ideal candidate can simultaneously execute against multiple priorities, which include: project planning, analysis structuring/execution, team management, client management/communication, and oversight of the completion of time-sensitive deliverables. The position requires strong data pipeline and machine learning skills, intellectual curiosity, comfort with ambiguity, and an ability to leverage information to tell compelling storiesResponsibilitiesSkillsets RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Project Manager Pune/ Kolkata and following details:",https://www.analyticsvidhya.com/blog/2016/09/project-manager-pune-kolkata-8-years-of-experience/
Data Crawling Engineer  Pune / Kolkata ( 4-6 Years of Experience),Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|Project Manager  Pune / Kolkata ( 8+ Years of Experience)|18 Data Science & IoT Startups from Y Combinator School  Summer 2016|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation: Data Crawling EngineerLocation  Pune / KolkataExperience  4- 6YearsAbout Employer  ConfidentialResponsibilitiesSkillsets RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Data Crawling Pune/ Kolkata and following details:,https://www.analyticsvidhya.com/blog/2016/09/data-crawling-engineer-pune-kolkata-4-6-years-of-experience/
18 Data Science & IoT Startups from Y Combinator School  Summer 2016,Learn everything about Analytics|Introduction,"1.Iris Automation|2.CrowdAI|3.YesGraphs|4.GTrack Technologies|5.Utility Score|6.Raptor Maps|7. ZeroDB||8. Amberbox|9. Aptonomy|10. Vote.org|11. Wallarm|12. Mosaic|13. Emote|14. People.ai|15. Legalist|16. Robby.io|17. PatientBank|18. Neowize|Where would you put your money?|Endnotes|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Crawling Engineer  Pune / Kolkata ( 4-6 Years of Experience)|NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience )|
Kunal Jain
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Y Combinator recently conducted demo day for their Summersession of 2016. As usual, they had some awesome startups, which are bound to impact us in coming years. We compileda list of startups on data science and IoT, which we think deserve an applause.Let me add a bit of fun to itas well  assuming I had $1 Million, I will share my picks among these best startups. Do share yours with me in comments below.The recent regulations from Federal Aviation Administration have levied some strict norms on commercial drones. The drones have to remain in line of sight of pilot and has to be only flown during the daylight hours. The motive behind this regulation is to avoid any collision with buildings or people. However, this severely impacts the utility of drones and unmanned vehicles.Iris Automation has come up with a technology where they are building a collision avoidance system for drones. With the help of computer vision and deep learning, they planto instill the ability to detect moving objects or obstacles in the drones vicinity. Iris aims to makeautonomous, cheap and safe drones a reality. Check out this cool video about their product:CrowdAI provides scalable, high-quality image annotation. They claim to be able to annotate 95% of images using machine learning & computer vision.What do you use them for? Well, you are only limited by your imagination Self-driving cars, automated drones and satellites capturing the market are just a few use cases to start with.The company aims to decrease the interference of human annotators and automate the process of image recognition for all the industries.YesGraphs is bringing science in user referrals to mobile and web apps. They analyse social graphs of your users (obviously with their consent) and sends them a ranked list of referrals in their contact list.This gives companies more refined user outreach and will help them target the right audience and providing more accurate estimate for the results.By pumping nanoparticles into fracking wells and detecting what comes back, GTrack quantifies the amount of oil and gas available to suck up in a well.GTrack Technologies aims to save large amount of money and labour. Where oil and gas companies spend close to $12.5 billion GTrack aims to lower the cost to $20,000 per company.Owning a house is the biggest expense for any individual. The soaring electricity and utility bills makes it even more difficult. If there was a way in which you could predict the ulility costs before buying a house and compare between the best option to choose from. Sounds interesting isnt it?Utility Scoreis a tool which allows users to estimate the utility bills by recommending them house projects that provide a better ROI. The company has collated data from 7000 different sourcesabout national home information, water rates, local energy and uses algorithm to determine usage and costs at the individual home level.Once a user selects a property Utility Score makes recommendation based on his locality and lifestyle.Agriculture contributes one-third of revenue to GDP and is responsible for large employment. Farmers use variety of methods to increase the yield every year. But extensive cultivation on large scale of land requires much more than estimation and experience. Raptor maps has come up with a better technique for farmers to gauge at their crops and produce good quality crops.Raptor Maps can survey the entire farmland with high resolution images, provide crop analytics to pinpoint problem areas. It also detects insects, disease, nutrients deficiencies and quality of the harvested crop. This will let farmers yield a better quality produce and generate more profits.Raptor Maps are looking forward to changing the way farmers operate.The maturity of virtualization technology gave birth to cloud computing. With more businesses making the shift to avail on-demand benefits and cost reduction techniques, cloud computing has gained enough attention. But one factor is sure of worry, Data Breach. With numerousbenefits ofcloud computing, security remains the biggest risk.ZeroDB allows companies to outsource their data storage by securing it with end-to-end encryption. The database server receives only the encrypted data and the decryption key remains only with the company. Thus, lowering the risk of data breach. Now companies can enjoy cost and performance benefits without compromising on security.Gunshots kill more than 100,000 people every year in the world. Also, there is usually a delay of a few crucial minutes before intimation to the local authority. To combat this, Jake Poppers, the founder of Amberbox came up with an idea to install gunshot detectors in premises.Amberbox designed an integrated firm alarm and gunshot detector. It detects patented gunshots algorithms and immediately alerts the authorities.In case, multiple gunshot detectors are installed on the premise, they all form a network and lock down the premises to avoid the escape of shooter. The device alsoalerts the authorities providingthem specific details like where did the shooting began?Imagine your property being guarded by flying drones  a fleet of guards, who are autonomous, self charging, equipped with day and night cameras, obedient, loyal and work tirelessly for you. This is exactly what Aptonomy is creating. Check them out!Vote.org wants to apply data science inhyper-targeted manner toget people out for vote. The non-profit plans to reach 1.2 million voters in specific urban areas. This year, $10.2 billion will be spent on competitive races with anywhere between $12 and $315 spent per voter. If Vote.org can come up with enough funding, it will utilize SMS to to impact political engagement.A lot of attacks today get detected through a two step process  companies use a separate analytics tool to keep track of trends and then an analyst watches them to keep track of attacks. While there are a few rule based services, which help track attacks  they are not intelligent! This is what Wallarm is creating. They are applying machine learning to solve this exact problem.Wallarm profiles application structure and user behavior, then uses algorithms to identify anomalies and detect sophisticated attacks.Mosaic is a powerful voice controlled Artifical Intelligence (AI) platform, which aims to make your smart products even smarter. They aim to take hassle out of converting homes into smart homes.Emote wants to apply analytics on behaviours of children and help teachers become more effective in dealing with behavioural issues. Theywant to put powerful students behavioral information in the hands of teachers before students even walk into the classroom.Emote will be in 33 schools by the end of September and another 135 by January.People.ai is applying people analytics to sales. They capture activities of your sales team, attribute efforts to clients and opportunities automatically and then provides data backed insights for you to coach and mentor your sales team. In addition, they alsointegrate with calendars, phones, and emails and logs sales activity that leads to closing deals. Over 100 companies have partnered with people.ai over the last four monthsLegalist is bringing data science to litigation financing. Litigation financing, in its simplest form, is financing court cases and then taking a percentage of the outcome.Legalist gathers data from cases dating back to 1989 to figure out the risk and case duration for a specific case. Their model takes up to 58 variable correlated with case outcome as an input before making recommendations. Interesting approach for a relatively niche industry.If you think autonomous cars sounds exciting, you are already behind the curve. Meet Robby! Robby makes self-driving delivery robots, which can autonomously navigate sidewalks to your door. Robby can reduce costs of deliveries by as high as 80%. Its MIT PhD-built bots have already made 50 deliveries, and Robby about to start a pilot program with Instacart. Can you think of a better use of deep learning?I believe PatientBank can be a big data science startup of future. Currently, they are in phase 1 of their journey, where they are focusing on collecting medicalrecords of all patients on a single platform. If, they are able to execute this phase well, there is no stopping them. Why? Because, once they have access to this data, they can apply data science to create several value added services on top of basic data collection.Neowize uses deep learning to deliver personalization for e-commerce portal. They use click rate behaviour and past viewing history of clients to come up with relevant recommendations. Simple, powerful, effective!As said at the start, if I had a million dollars to invest in any of these business, I would put it in Robby! They are solving a big problem, have the right kind of founding team and have already started testing their product. Where would you put your money, if you had a Million dollars to invest? Let me know through comments below.I hope you enjoyed reading this article as much as I enjoyed writing it. If you were intrigued with all these new start-ups and have any favourites then drop in your comments below to tell me about it. Till then, stay tuned to more such interesting reads and well make sure we keep enriching your knowledge with more interesting finds.",https://www.analyticsvidhya.com/blog/2016/09/18-data-science-iot-startups-y-combinator-summer-2016/
"NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience )|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|18 Data Science & IoT Startups from Y Combinator School  Summer 2016|Our new section  Stories and Why I am super excited about them?|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Location: Pune/KolkataDesignation: NLP EngineerExperience Required: 3-8 YearsAbout employer: ConfidentialBrief Job DescriptionResponsibilities:
Lead cutting edge research in NLP and machine learning technology or algorithm.Produce benchmark results for existing NLP development platforms or offering fromVendors innovating with NLP, such as Microsoft, Google, Facebook, Amazon andStartups, by building and testing application.Write real-time prototypes of the newly designed algorithms.Develop solutions for real world, large scale problems, such as context sensitive.Languages understandingSkills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as NLP Engineer  Pune/ Kolkata and following details:",https://www.analyticsvidhya.com/blog/2016/09/nlp-engineer-pune-kolkata-3-8-years-of-experience/
Our new section  Stories and Why I am super excited about them?,Learn everything about Analytics|Launching AV Stories  uncut perspective from our community members|Have a story to tell? Share it with us,"Share this:|Like this:|Related Articles|NLP Engineer  Pune / Kolkata ( 3-8 Years of Experience )|MyStory: I became a Data Scientist after working for 10 years in IT Industry|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I started Analytics Vidhya with an itch to help out as many people as I can  people who needed technical help, people who needed career advice (related to analytics and data science), people who wanted to speak to other people from industry about their experience!Over the last 3 years, we have connected tens of thousands of data science professionals, written thousands on posts, conducted hundreds of competitions and helped a similar number of people find the right job! If that sounds a lot, it is a lot of work.To be honest, I had no clue, we would come this far. We were only looking up to the next big task and milestone at hand. My friends can vouch for those excited whatsapp messages every time we hit a new landmark.Initially, it was to get first 100 followers, then 100 comments and then 100 subscribers. Over time, these numbers moved to thousands and now millions.Over these years, we have added super useful tutorials and articles. One thing, which we havent done as much is share stories from our members and their perspective. And that is what we are launching today.While ourtechnical tutorials help you on a day to day basis, they dont replace the role of a mentor. They cant give you answers to those burning career related questions you might have. Over the last 3 years, we have got thousands of questions related to career in analytics, transitions and various challenges people face which they face.Inspite of our best efforts, we have not been able to answer all of them. AV Stories is our way of providing you with as many answers as we can. These stories are from real people  people who have faced similar challenges in past. These stories reflect their struggles, their motivations and their secrets to success. These are uncut perspective from our community members.We believethat these stories will provide you with the perspective required to enable success in your career. We believe that these story tellers will become mentors for our other community members. This is our way to celebrate the heroism of these story tellers  go on enjoy their uncut perspective, ask them difficult questions and make them heroes.If you have a story to share which will help other community members learn from your perspective, do share it with us. These could be your secret to make data science projects work, these could be related to your career transitions, these could be your challenges to break in to the top few in a competition and what you did to solve them.Just share those stories with us and our community. This is your uncut perspective for our community and you will find them in this special section here. You can share your stories with us on [emailprotected]",https://www.analyticsvidhya.com/blog/2016/09/our-new-section-stories-and-why-i-am-super-excited-about-them/
MyStory: I became a Data Scientist after working for 10 years in IT Industry,Learn everything about Analytics,"|Distant Memories (Prologue)|Business analytics as a new career Stream|A Rude(Self) awakening|Beginning of a thousand-mile|Advantages of structured transition|The First Date|The Swell is really big|The Meta Critics:|Dare to Dream|Foot notes|Share this:|Like this:|Related Articles|Our new section  Stories and Why I am super excited about them?|MyStory: How I transitioned to Data Science after 6 years in Data warehousing?|
karthe
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Getting into Analytics or Data science stream was never my dream. I got into this out of an accident. Prior to getting into data sciences, I was a mainframe programmer all through. The only aim that I had for a very long time was to get into a good MBA programme.It was 2013. I had 9+ years into software services industry by then. My career was almost stabilized and I could not see much growth out of it. Hence I was planning to turn up to my dreams of management education. But I was quite apprehensive as my long experience would bar me from getting into a decent MBA. It was when I came to know about the new upcoming stream called Business Analytics.My first step towards getting into analytics was scouting for an opportunity internally. It didnt worked out for me for two reasons. First, there were not much analytics initiatives happening within the companies at that time. Secondly, as the advanced analytics stream evolved as an extension to the business intelligence division in most of the companies (especially the Information technology services), they could not afford to take a non BI person like me and to train up for their analytics works. Also, there were not much courses online at that time. Or perhaps, I was not aware.That was the time I decided to reskill myself to suit to the analytics industry. I had applied for couple of long term analytics programmes available in India and I managed to convert one of them into admission. But I never know that the class of business analytics will be math (advanced math) and technology heavy, the areas which I was bit apprehensive, until I went to the first day of my class. Apprehensive was due to the fact that I was working on a very old technology and doesnt require you update constantly.The extreme main skill that any data scientist should have  more than the art of storytelling  is the art of questioning and analyzing the information in hand. The very first mistake that I did before getting into business analytics was failing to understand what Business analytics professional do in their work  atleast its literal meaning. My cluttered mind strongly correlated Business analytics with the term Business analysis. At the end of my first day class, one thing I really understood was  The decision to join the business analytics program was taken based on the gut feeling, rather than an informed decision.I had two stages of transition from being a mainframe programmer to getting into data sciences  First one was undergoing a one year programme and the other being the challenges that I face during the day to day work.The one year analytics programme was extremely challenging  I would call it as a toughest challenge (yet most rewarding at a later point of time), I had till now in life. Multifaceted responsibility is the first challenge that I had faced as soon as I enrolled to the programme. Being a dad to a 5 year old, a production support lead at the office, coping up with the course was extremely difficult for me. With the kind of rigorousness that came along with the course, be it in terms of week end online classes (apart from the regular classes that we attend at the campus), tests, home works and assignments I was completely bombarded every day.The one year spent at the programme was a roller coaster ride for all my student mates without any exception. The quality of problems that we were tested as part of assignments and their deadlines driven us to spend sleepless nights. The effort to put completing every mini project, the early morning hour classes, the late night discussions along with my family and office responsibilities  all together would have made my health worse, if had I not been supported by the awesome peer group. I still remember the days attending an hour of office call and taking up the exam in tandem. But I didnt realize God was preparing me for the challenges that would follow up later in my career in analytics.Job interviews are always like first date. The outcomes are seldom predictable.The next challenge that I faced was when I started looking for an opportunity to work in data science. Hiring managers are typically concerned about taking in someone who comes with considerable experience in non-data science stream. Luckily, the capstone project that I did as part of the course work with one of the prominent retail brands came in handy for me. The interviewer typically liked the process that was followed in building up the solution during this project. Thanks to awesome professors who mentored continuously.These days, I also see many people who apply and get selected in interviews just by participating actively in data science competitions. Infact, as far as I have seen, data science competitors outshined the one with real time work experience in the interviews.If the challenges pertaining during the academic stage of my transition was pertaining to the mathematical part, at work I was facing the challenges from the process and domain front. Here with I am briefing some of the lessons that I learned as part of my journey:Be good in domain  One of the foremost challenge that any data scientist would face is the short-come in domain knowledge. It is imperative for any data scientist to be good in the domain without which an analytics engagement can never be successful. Without the domain knowledge, a data scientist would never be able to answer any of the business question. He/She cannot build a good predictive model without the right set of variables. The insights that he generates will go in vain without the business knowledge.Setting the expectation right with the customer  Most important and vital to satisfy every stakeholder. Customers may not know about analytics. All they know is that if the data is given, data scientists do some magic and generate insights which can be used to improve the business.A data scientist should exhibit patience and teach them what can be possible with data science and what not. I remember the days I spent with the customer explaining why a statistical model is much better than predicting manually, what is mean by a model by itself, how to interpret confusion matrix etc.Knowledge of SQL and databases  I had an initial assumption that it is the duty of the data engineer to extract the relevant data required for building the use cases. It is false. Companies may not be interested in investing one more person for pulling the data. A data scientist is expected to know how to extract the data that he requires, and transform the data to the required format.Patience for analyzing the data Data analysis has to be done iteratively from various perspectives. We never know what kind of pattern exist inside the data. However, by following structured data analysis methods and little bit of patience, the pattern that the data follows can be identified.Presenting the solution In my view, this is more important than building a model. All the hard work put in by a data scientist can be showcased only in this area. Also it is important to know the context of to whom the presentation is made. We may not talk about that adjusted R square or an ROC curve to end user.Be comfortable when you fail  Most of the work that I did during the initial days failed to larger extent. Every time I presented the solution to the customer, there was a high probability that it gets rejected. I looked stupid every time I made silly and big mistakes. But I eventually learned that all these are part and parcel of the career in data science. Just try to have a mentor if you are new to the analytics projects or start small if you dont have a mentor.Here are some the questions that you should ask yourself before getting into a career transition:Why do you want to make a career shift?  I meet many peoples who come to me and say that they wanted to shift to analytics just because they have problem with existing job/manager/company etc. If this is the case with you, try to see if something can be made interesting in your current work. Changing the work stream may not solve your problem.Shifting career with high work-ex: If your overall experience is more than 7 years, please think thrice before changing stream. Please remember that with more than 7 years of experience, you may not be allowed to experiment in your work rather you will be expected to deliver.Why do you want to get into data science? If the answer is that data sciences present lucrative opportunity, then you might want to think again!! You will be getting into a field where you cant sail smooth atleast for few years down the line. You will be expected to keep yourself updated very frequently and it is very difficult for many people whom I had seen, including myself.What do you know about analytics to make a career in it?  Test this by doing some self-learning. Try some online courses in coursera or edx or udacity to check whether you will be comfortable with data science. Typically, you should not take data science as your career if you are averse to mathematics or if you show dislike to databases and querying tools like SQL. If you are new to all these, check these at khanacademy.com (for math courses) and w3schools.Life holds special magic for all those who are dare to dream beyond their abilities. Some tips for the people who would want to hold their dream and looking to get into data sciences.",https://www.analyticsvidhya.com/blog/2016/09/mystory-i-became-a-data-scientist-after-working-for-10-years-in-it-industry/
MyStory: How I transitioned to Data Science after 6 years in Data warehousing?,Learn everything about Analytics,"|Background|Initial Days|Great Lakes and Analytics Edge|Application of Whatever I learnt|How difficult/easy is to transition?|What works and what does not?|What kind of Challenges can come during the transition?|What would be your advice to the people?|How much time, effort and resources are required to make transition?|Summary|Share this:|Like this:|Related Articles|MyStory: I became a Data Scientist after working for 10 years in IT Industry|A Complete Guide on Getting Started with Deep Learning in Python|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Prior to getting initiated to Data Science, I was working in data intensive Data-warehousing for more than 6 years. In 2013, I had an opportunity to work in a problem wherein we were required to build a model to predict the probability of a customer buying a product as part of transformation initiative. This has widened my horizon.Until that time, I was under the impression that data can be used only for presenting or analyzing what had happened. I did some research on Analytics, Predictive Modelling and I realized that even though it is an extension of BI it requires different skill sets Ii.e. of Statistics, Machine Learning, SQL, and Business acumen in industry one is working.I had good knowledge of SQL and bit of business acumen. I was determined to learn Statistics and Machine Learning. In 2014, I enrolled in online course by Edvancer. Edvancer has provided me solid foundation in Analytics using SAS and R.I had plans to do MBA but could not pursue it full time as I could not leave my job. I did not want to do MBA through distance learning. Based on my search pattern google had recommended few Analytics program by reputed business schools. I had applied for IIM Bangalore and Great Lakes Institute of Management Chennai and was shortlisted from both of them.I had opted for Great Lakes as IIM Bangalore conducts most of the classes on weekends and was not in a position to attend classes on Saturday. Moreover, recording of classes were not available in IIMB. I was not in Bangalore at that point of time hence attending classes over the web would mean missing on peer learnings.I underwent Great Lakes PGPBABI program during period 2014-2015. As I am not a MBA graduate, I was able to fill certain gaps in my career and got good exposure and solid foundation on Analytics.After completion of PGPBABI program at Great Lakes, I did Analytics Edge program on edx (This tip was shared by Kunal. Thanks Kunal!). As part of Analytics Edge we were supposed to take part in Kaggle competition which is part of grading. I was able to finish in top 90among 2932 contestants (Top 3%) which gave me great boost to my confidence.Now the time has come to apply whatever I learned in my workplace. I have mentioned my interest to my boss who were happy to support me. Initial days were challenging as In theory there is no difference between theory and practice, but in practice there is.To overcome this challenge, Analytics Vidhya (AV) and its community were lot of help. I used to get my queries clarified over the discuss portal. What was even more helpful was the hackathon conducted by AV regularly. I made it a point to not to miss any hackathon even when I was travelling.Hackathon allows you to benchmark yourselves with others and get introduced to wider community and make friends. Like many others, I started at the bottom but slowly improved to finish within top 10 many times and within top 10% in a Kaggle competition.For every hackathon, I try something new. Sometimes it works and sometimes I get my fingers burned by overfitting the public leaderboard. Each of my experience in hackathon is worth weight in Gold. Thomas Edison once remarked I did not fail 1000 times but found out 1000 ways how not to make a bulb. My experience is similar to this.I am happy with my current ranking of being in top 10 in AV (at time of writing this article) but I am trying my best to improve my ranking. Still lot of distance to be traveled before reaching heights of @SRK or @Vopani or others in top 5.They will remain my inspiration to excel. By the time I reach that level I am sure they would be have doubled their level.These are some of FAQ for people in their mid careers and would like to make a transition to Data Science. I have answered to the extent I know. Please feel free to correct me if wrong.There is no single answer to this question. In fact, if one has enough data, one can run classification model to predict the probability of transition with the following variables (Some I can think of)Does one need to have all the above criteria to make a transition to Analytics? No, it depends on number of years of total experience (Relevant Analytics experience is assumed to be nil) as below (For MBA candidates strong background in Marketing, Risk, Supply Chain or Finance is assumed):0 -1 years or Freshers: Passion in Data Science, Curiosity and Comfortable with numbers. Good to have expertise in SQL and in-depth understanding of statistical concepts.1-2 years: Freshers + Expertise in either R or Python or SAS2-5 years: 1-2 years+ In depth understanding of statistical concepts. Chances of transition is more if either chance of internal transition or previous experience in BI is applicable. Good to have knowledge of both R and Python or experience in SAS (Non data science).5-10 years: 2-5 years + domain knowledge are applicable + knowledge of both R and Python or experience in SAS (Non data science). Chances of transition is more if either chance of internal transition or previous experience in BI is applicable. If one is Non MBA candidate it is good to have strong background in Marketing, Risk, Supply Chain or Finance.> 10 years: All are applicable. Should be able to formulate own use cases and derive ROI. Should not leave anything to chance.Internal transition is much better way of making transition. If one have more than 5 years of experience, having learn only data science course will not help. One need to substantiate his / her knowledge and learn to apply the knowledge acquired in his industry.For those people who are having more than 10 years of experience, they should be capable of providing end to end solution i.e. Starting from finding a use case to executing it and presenting the ROI to the stakeholders.Losing patience, aversion to coding, lack of domain knowledge in the industry the candidate has worked etcOne has to be patient and wait for opportunities and grab them when they present itself. I have seen persons who after few months, get demotivated and do not have the will to pursue data science careers. Obviously it depends on the passion component of the candidate also.There are few people who get trained in GUI driven softwares like SAS Enterprise Miner or SPSS and not willing to train themselves in R or Python as they are averse to writing codes. They should overcome this aversion to stand better chance.I have seen certain people especially people coming from software industry lack the domain knowledge required to provide end to end solution. They are comfortable with coding but do not have business acumen. This may be big handicap especially when your number of experience is more and relevant experience is less.It is true that as number of non relevant experience increases chances of transition to data science diminishes but probability never becomes zero. Remember!Being at the right place at the right time having right skills matter.Think like a CEO. Do not restrict oneself to just model building. Get involved in all phases of Data Science right from problem definition to ROI justification.During the first year or initial period, imagine yourselves training for Olympics. If one is working he has to put 2 hours of study in the morning and another 2 hours in the evening. Work with lot of data and be positive.Accept failures as stepping stones for success. Time taken to transition would range anywhere from 6 months to 2 years or sometime more than 2 years.How does industry view transitions?Industry view favorably transitions of candidates having less than 5 years of total non data science experience. For others with more than five years of total experience but less relevant experience in data science, they consider data science skills as only as an add on.Data Science is not about new tools or technology. Some of the algorithms used in Data Science were conceived 30-40 years back. It is all about Data to Decisions. To be a good data scientist you need to have mixture of Coding skills, data management skills, modeling skills, business skills to succeed.Do not confine yourselves to a tool or algorithm. There is nothing like good or bad tools or algorithm. Whatever works for the business to solve their problems and improve ROI are welcome. Transition for mid career people takes time and one should have patience and passion to make a successful transition.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2016/09/mystory-how-i-transitioned-to-data-science-after-6-years-in-data-warehousing/
A Complete Guide on Getting Started with Deep Learning in Python,Learn everything about Analytics|Introduction,"Step 0 : Pre-requisites|Step 1 : Setup your Machine|Step 2 : A Shallow Dive|Step 3 : Choose your own Adventure!|Step 4 : Deep Dive into Deep Learning|Noteworthy Resources|End Notes|You can test your skills and knowledge. Check out Live Competitions and compete with best Data Scientists from all over the world.|Share this:|Like this:|Related Articles|MyStory: How I transitioned to Data Science after 6 years in Data warehousing?|Full Solution  Skilltest on R for Data Science|
Faizan Shaikh
|27 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning, a prominent topic in Artificial Intelligence domain, has been in the spotlight for quite some time now. It is especially known for its breakthroughs in fields like Computer Vision and Game playing (Alpha GO), surpassing human ability. Since the last survey, there has been a drastic increase in the trends. (click here to check out the survey)Here is what Google trends shows us:If you are interested in the topic heres an excellent non-technical introduction. If you are interested to know the recent trends, heres a great compilation.Here our aim is to provide a learning path to all those who are new to deep learning and also the ones who want to explore it further. So are you ready to step onto the journey of conquering Deep Learning? Lets GO!It is recommended that before jumping on to Deep Learning, you should know the basics of Machine Learning. The Learning Path on Machine Learning is a complete resource to get you started in the field.If you want a shorter version, here it is:Timeline:Suggested: 2-6 monthsBefore going on to the next step, make sure you have the supported hardware. It is generally recommended that you should have atleastIf you are still unsure, go through this hardware guide.PS: If you are a hardcore gamer (not just candy crushers obviously!), you may already have the required hardware.If you dont have the required specifications, you could either buy it or lease an Amazon Web Service instance. Heres a good guide for using AWS for deep learning.Note: Do not install any deep learning libraries at this stage, do it on step 3.Now that you have a good enough knowledge of pre-requisites, you should go on further into understanding Deep Learning.As per your preference you could follow:Along with the pre-requisites, you should get to know the popular deep learning libraries and the languages for running them. Heres a (non-comprehensive) list (Check the wiki page for a more comprehensive list):Some other notable libraries include Mocha, neon, H2O, MXNet, Keras, Lasagne, Nolearn. Heres a list of Deep Learning libraries by Language.Check out Lecture 12 of Stanfords CS231n course for a brief overview of some of the popular libraries.Timeline: Suggested 1-3 weeksNow comes the interesting part! Deep Learning has been applied in various fields with state-of-the-art results. To get a taste of this side of the moon, you, the reader, gets to choose which path to take. This should be a hands-on experience, so that you get a proper foundation on what you have understood until now.Note: Each path contains a primer blog, a practical project, the required deep learning library for the project and an assisting course. First go through the primer, then install the required libraries and get on with the project. If you face any difficulties along the way, use the associated course to back you up.Timeline : Suggested 1-2 monthsNow you are (almost) ready to make a dent in Deep Learning Hall of Fame! The path ahead is long and deep (pun intended) and mostly unexplored. Now it is upto you to make use of this newly acquired skill as efficiently as you can. Here are some tips you should do to hone your skill.Timeline: Suggested  Infinity!I hope this learning path was helpful to you. I have tried to make it as comprehensive as possible. Now, its time for you to practice and read as much as you can.Togain expertise in working in neural network try out our deep learning practice problem Identify the Digits.Once you have an understanding of Deep Learning and its associated concepts, take the Deep Learning Skill test. The way Deep learning is gaining recognition it is important to be familiar with it.Good luck! Did you like reading this article? Do you follow a different approach / package / library to get started with Deep Learning? Id love to interact with you in comments.",https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/
Full Solution  Skilltest on R for Data Science,Learn everything about Analytics|Introduction|Overall Results|Skill Test Questions & Answers|End Notes,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|A Complete Guide on Getting Started with Deep Learning in Python|Solutions for Skilltest in Statistics Revealed|
Kunal Jain
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R is the most commonly used tool in analytics industry today. No doubt, python is catching up quickly. Many companieswhich were heavily reliant on SAS, have now started R in their day to day analysis. Since R is easy to learn, your proficiency in R canbe a massive advantage to your candidature.This test wasnt designed for freshers. But, for people having some knowledge of R. If youve taken this test thoroughly, you might be either disappointed orhappywith your performance and keen to know the solutions. As expected, weve complied the list of Q&A so that you can learn and improve.A best way to learn is to solve these questions at your end. Youll learn multiple ways to perform a task in R. In other words, youll be able to add more weapons to your R armory.If you dont understand anything, drop your question in comments!Below are the distribution of the scores. This will help you to evaluate your performance.Some of the interesting statistics from this competition:Mean  20.16Median  20Mode  0Range 49Standard Deviation 14.0995% Confidence Interval  [-7.45,47.77]
Heartiest Congratulations to participants who have scored 32 & above, they are in the top 25 percentile. And, people scoring more than 40 are in top 10 percentile, score 47 & above makes you in top 1 percentile.Due to wide range, the confidence interval doesnt seem so practical mathematically. Looks like many participants didnt take the complete test and left in between.Sincemajority of the questions were fairly easy, if you have scored less than 20, you are in an alarming situation. You need to spend more time practicing on R.Helpful Resources on R1). Two vectors X and Y are defined as follows  X <- c(3, 2, 4) and Y <- c(1, 2). What will be output of vector Z that is defined as Z <- X*YA  3,4,0B  3,4,4C  errorD  3,4,8Solution: BVector recycling takes place when 2 vectors of unequal lengths are multiplied.2).If you want to know all the values in c (1, 3, 5, 7, 10) that are not in c (1, 5, 10, 12, 14). Which code in R can be used to do this?A  setdiff(c(1,3,5,7),c(1,5,10,12,14))B  diff(c(1,3,5,7),c(1,5,10,12,14))C  unique(c(1,3,5,7),c(1,5,10,12,14))D  None of the Above.Solution: Asetdiff() function finds the values which are different in any given two vectors.3). What is the output of f(2) ?b <- 4
f <- function (a){
b <- 3
b^3 + g (a)
}g <- function (a){
a*b
}A  33B  35C  37D  31Solution: Bg(a) uses b <- 4 because it is globally available. Globally means to every variable in the environment. f(a) uses b <- 3 because it is locally available for the function. Therefore, for a function locally available information takes precedence over global information.4) The data shown below is from a csv file. Which of the following commands can read this csv file as a dataframe into R?Table1.csvA  read.csv(Table1.csv)B  read.csv(Table1.csv,header=FALSE)C  read.table(Table1.csv)D  read.csv2(Table1.csv,header=FALSE)Solution: BSince the table has no headers, it is imperative to specify it in the read.csvcommand.5). The missing values in the data shown from a csv file have been represented by ?. Which of the below code will read this csv file correctly into R?Table2.csvA  read.csv(Table2.csv)B  read.csv(Table2.csv,header=FALSE,strings.na=?)C  read.csv2(Table2.csv,header=FALSE,sep=,,na.strings=?)D  read.table(Table2.csv)Solution: CSince missing values comes in many forms and not just standard NA, it is essential to define by what character the NA values are represented. na.strings will tell read.csv to treat every question mark ? as a missing value.6). The table shown below from a csv file has row names as well as column names. This table will be used in the following questions:Which of the following code can read this csv file properly into R?Table3.csvA  read.delim(Train3.csv,header=T,sep=,,row.names=1)B  read.csv2(Train3.csv,header=TRUE,row.names=TRUE)C  read.table(Train3.csv,header=TRUE,sep=,)D  read.csv(Train3.csv,row.names=TRUE,header=TRUE,sep=,)Solution: ASince the first column has row names, it is important to specify it using row.names while loading data. row.names = 1 says that row names are available in the first column of the table.7). Which of the following code will fail to read the first two rows of the csv file?Table3.csvA  read.csv(Table3.csv,header=TRUE,row.names=1,sep=,,nrows=2)B  read.csv(Table3.csv,row.names=1,nrows=2)C  read.delim2(Table3.csv,header=T,row.names=1,sep=,,nrows=2)D  read.table(Table3.csv,header=TRUE,row.names=1,sep=,,skip.last=2)Solution- DExcept D, rest all the options will successfully read the first 2 lines of this table. nrows parameter helps to determine how many rows from a data set should be read.8). Which of the following code will read only the second and the third column into R?Table3.csvA  read.table(Table3.csv,header=T,row.names=1,sep=,,colClasses=c(NULL,NA,NA))B  read.csv(Table3.csv,header=TRUE,row.names=1,sep=,,colClasses=c(NULL,NA,NA))C  read.csv(Table3.csv,row.names=1,colClasses=c(Null,na,na))D  read.csv(Table3.csv,row.names=T, colClasses=TRUE)Solution: AYou can skip reading columns using NULL in colclasses parameter while reading data.9). Below is a data frame which has already been read into R and stored in a variable named dataframe1.Which of the below code will produce a summary (mean, mode, median etc if applicable) of the entire data set in a single line of code?Dataframe 1A  summary(dataframe1)B  stats(dataframe1)C  summarize(dataframe1)D  summarise(dataframe1)Solution:A10) dataframe2 has been read into R properly with missing values labelled as NA. This dataframe2 will be used for the following questions:Which of the following code will return the total number of missing values in the dataframe?dataframe2A  table(dataframe2==NA)B  table(is.na(dataframe2))C  table(hasNA(dataframe2))D  which(is.na(dataframe2)Solution: B11). Which of the following code will not return the number of missing values in each column?dataframe2A  colSums(is.na(dataframe2))B  apply(is.na(dataframe2),2,sum)C  sapply(dataframe2,function(x) sum(is.na(x))D  table(is.na(dataframe2))Solution: DRest of the options will traverse through every column to calculate and return the number of missing values per variable.12). The data shown below has been loaded into R in a variable named dataframe3. The first row of data represent column names. The powerful data manipulation package dplyr has been loaded. This data set will be used in following questions:Which of the following code can select only the rows for which Gender is Male?dataframe3A  subset(dataframe3, Gender=Male)B  subset(dataframe3, Gender==Male)C  filter(dataframe3,Gender==Male)D  option 2 and 3Solution: Dfilter function comes from dplyr package. subset is the base function. Both does the same job.13). Which of the following code can select the data with married females only?dataframe 3A  subset(dataframe3,Gender==Female & Marital Status==Married)B  filter(dataframe3, Gender==Female , Marital Status==Married)C  Only 1D  Both 1 and 2Solution: D14). Which of the following code can select all the rows from Age and Dependents?dataframe3A  subset(dataframe3, select=c(Age,Dependents))B  select(dataframe3, Age,Dependants)C  dataframe3[,c(Age,Dependants)]
D  All of the aboveSolution: DIf you got this wrong, refer to the basics of sub-setting a data frame.15). Which of the following codes will convert the class of the Dependents variable to a factor class?Dataframe 3A  dataframe3$Dependents=as.factor(dataframe3$Dependents)B  dataframe3[,Dependents]=as.factor(dataframe3[,Dependents])C  transform(dataframe3,Dependents=as.factor(Dependents))D  All of the AboveSolution: Das.factor() is used to coerce class type to factor.16). Which of the following code can calculate the mean age of Female?Dataframe3A  dataframe3%>%filter(Gender==Female)%>%summarise(mean(Age))B  mean(dataframe3$Age[which(dataframe3$Gender==Female)])C  mean(dataframe3$Age,dataframe3$Female)D  Both 1 and 2Solution: DOption A describes the method using dplyr package. Option B uses the base functions to accomplish this task.17). The data shown below has been read into R and stored in a dataframe named dataframe4. It is given that Has_Dependents column is read as a factor variable. We wish to convert this variable to numeric class. Which code will help us achieve this?dataframe4A  dataframe4$Has Dependents=as.numeric(dataframe4$Has_Dependents)B  dataframe4[,Has Dependents]=as.numeric(as.character(dataframe4$Has_ Dependents))C  transform(dataframe4,Has Dependents=as.numeric(Has_Dependents))D  All of the aboveSolution: B18). There are two dataframes stored in two respective variables named Dataframe1 and Dataframe2.Which of the following codes will produce the output as shown below?A  merge(dataframe1,dataframe2,all=TRUE)B  merge(dataframe1,dataframe2)C  merge(dataframe1,dataframe2,by=intersect(names(x),names(y))D  None of the aboveSolution: AThe parameter all=TRUE says to merge both the data sets, and even if there is no match found for a particular observation, return NA.19). Which of the following codes will create a new column named Size(MB) from the existing Size(KB) column? The dataframe is stored in a variable named dataframe5. Given 1MB = 1024KBdataframe5A  dataframe5$Size(MB)=dataframe$Size(KB)/1024B  dataframe5$Size(KB)=dataframe$Size(KB)/1024C  dataframe5%>%mutate(Size(MB)=Size(KB)/1024)D  Both 1 and 3Solution: D20). Following question will use the dataframe shown below:Dataframe6Certain Algorithms like XGBOOST work only with numerical data. In that case, categorical variables present in dataset are converted to DUMMY variables which represent the presence or absence of a level of a categorical variable in the dataset. From Dataframe6, after creating the dummy variable for Gender, the dataset looks like below.Which of the following commands would have helped us to achieve this?A  dummies:: dummy.data.frame(dataframe6,names=c(Gender))B dataframe6[,Gender] <- split(dataframe6$Gender, ifelse(dataframe6$Gender == Male,0,1))C  contrasts(dataframe6$Gender) <- contr.treatment(2)D  None of the aboveSolution: AFor Option A, install and load dummies package. With its fairly easy code syntax, one hot encoding in R was never easy before.21). We wish to calculate the correlation between column 2 and column 3. Which of the below codes will achieve the purpose?Dataframe 7A  cor(dataframe7$column2,dataframe7$column3)B  (cov(dataframe7$column2,dataframe7$column3))/(sd(dataframe7$column4)*sd(dataframe7$column3))C  (cov(dataframe7$column2,dataframe7$column3))/(var(dataframe7$column4)*var(dataframe7$column3))D  Allof the aboveSolution: A
cor is the base function used to calculate correlation between two numerical variables.22). Column 3 has 2 missing values represented as NA in the dataframe below stored in the variable named dataframe8. We wish to impute the missing values using the mean of the column 3. Which code will help us do that?Dataframe 8A  dataframe8$Column3[which(dataframe8$Column3==NA)]=mean(dataframe8$Column3)B  dataframe8$Column3[which(is.na(dataframe8$Column3))]=mean(dataframe8$Column3)C  dataframe8$Column3[which(is.na(dataframe8$Column3))]=mean(dataframe8$Column3,na.rm=TRUE)D  dataframe8$Column3[which(is.na(dataframe8$Column3))]=mean(dataframe8$Column3,rm.na=TRUE)Solution: COption na.rm=TRUE says that impute the missing values by calculating the mean of all available observations.23). Column7 contains some names with the salutations. In such cases, it is always advisable to extract salutations in a new column since they can provide more information to our predictive model. Your work is to choose the code that cannot extract the salutations out of names in Column7 and store the salutations in Column8.Dataframe 9A  dataframe9$Column8<-sapply(strsplit(as.character(dataframe9$Column7),split = [.]),function(x){x[1]})B  dataframe9$Column8<-sapply(strsplit(as.character(dataframe9$Column7),split = .),function(x){x[1]})C  dataframe9$Column8<-sapply(strsplit(as.character(dataframe9$Column7),split = .,fixed=TRUE),function(x){x[1]})D  dataframe9$Column8<-unlist(strsplit(as.character(dataframe9$Column7),split = .,fixed=TRUE))[seq(1,18,2)]
Solution: Bstrsplit is used to split a text variable based on some splitting criteria. Try running these codes at your end, youll understand the difference.24). Column 3 in the data frame shown below is supposed to contain dates in ddmmyyyy format but as you can see, there is some problem with its format. Which of the following code can convert the values present in Column 3 into date format?Dataframe 10A  as.Date(as.character(dataframe10$Column3),format=%d%m%Y)B  as.Date(dataframe10$Column3,format=%d%m%Y)C -as.Date(as.character(dataframe10$Column3),format=%d%m%y)D -as.Date(as.character(dataframe10$column3),format=%d%B%Y)Solution: A25). Some algorithms work very well with normalized data. Your task is to convert the Column2 in the dataframe shown below into a normalised one. Which of the following code would not achieve that? The normalised column should be stored in a column named column8.dataframe 11A  dataframe11$Column8<-(dataframe11$Column2-mean(dataframe11$column2))/sd(dataframe11$Column2)B  dataframe11$Column8<-scale(dataframe11$Column2)C  All of the aboveSolution: COption A describes simply the mathematical formula for standarization i.e x /26). dataframe12 is the output of a certain task. We wish to save this dataframe into a csv file named result.csv. Which of the following commands would help us accomplish this task?dataframe 12A  write.csv(result.csv, dataframe12)B  write.csv(dataframe12,result.csv, row.names = FALSE)C  write.csv(file=result.csv,x=dataframe12,row.names = FALSE)D  Both 2 and 3.Solution: C27) y=seq(1,1000,by=0.5)What is the length of vector y ?A  2000B  1000C  1999D  1998Solution: C28). The dataset has been stored in a variable named dataframe13. We wish to see the location of all those persons who have Ms in their names stored in Column7. Which of the following code will not help us achieve that?dataframe13A  grep(pattern=Ms,x=dataframe13$Column7)B  grep(pattern=ms,x=dataframe13$Column7, ignore.case=T)C  grep(pattern=Ms,x=dataframe13$Column7,fixed=T)D  grep(pattern=ms,x=dataframe13$Column7,ignore.case=T,fixed=T)Solution- DIn option D, we tell the function to find the match irrespective of lower or upper case i.e. it just matches the spelling the and return the output.29). The data below has been stored in a variable named dataframe14. We wish to find and replace all the instances of Male in Column1 with Man. Which of the following code will not help us do that?dataframe 14A  sub(Male,Man,dataframe14$Column1)B  gsub(Male,Man,dataframe14$Column1)C  dataframe14$Column1[which(dataframe14$Column1==Male)]=ManD  None of the above.Solution: DTry running these codes at your end. Every option will do this task gracefully.30) Which of the following command will display the classes of each column for the following dataframe ?A  lapply(dataframe,class)B  sapply(dataframe,class)C  Both 2 and 3D  None of the aboveSolution: CThe only difference in the answer of lapply and sapply is that lapply will return a list and sapply will return a vector/matrix.31).The questions below deal with the tidyr package which forms an important part of the data cleaning task in R.Which of the following command will combine Male and Female column into a single column named Sex and create another variable named Count as the count of male or female per Name.Initial dataframeFinal dataframeA  collect(dataframe,Male:Female,Sex,Count)B  gather(dataframe,Sex,Count,-Name)C  gather(dataframe,Sex,Count)D  collect(dataframe,Male:Female,Sex,Count,-Name)Solution: B32). The dataframe below contains one category of messy data where multiple columns are stacked into one column which is highly undesirable.Which of the following code will convert the above dataframe to the dataframe below ? The dataframe is stored in a variable named dataframe.A  separate(dataframe,Sex_Class,c(Sex,Class))B  split(dataframe,Sex_Class,c(Sex,Class))C  disjoint(dataframe,Sex_Class,c(Sex,Class))D  None of the aboveSolution: A33). The dataset below suffers from a problem where variables Term and Grade are stored in separate columns which can be displayed more effectively. We wish to convert the structure of these variables into each separate variable named Mid and Final.Which of the following code will convert the above dataset into the one showed below? The dataframe is stored in a variable named dataframe.A  melt(dataframe, Term, Mid,Final,Grade)B  transform(dataframe,unique(Term),Grade)C  spread(dataframe,Term,Grade)D  None of the aboveSolution: C34). The ________ function takes an arbitrary number of arguments and concatenates them one by one into character strings.A  copy()B  paste()C  bind()D  None of the above.Solution: B35). Point out the correct statement :A  Character strings are entered using either matching double () or single () quote.B  Character vectors may be concatenated into a vector by the c() function.C  Subsets of the elements of a vector may be selected by appending to the name of the vector an index vector in square brackets.D  All of the aboveSolution:D36) What will be the output of the following code ?> x <- 1:3
> y <- 10:12
> rbind(x, y)1- [,1] [,2] [,3]
x   1   2  3
y   10  11 122- [,1] [,2] [,3]
x    1   2   3
y   10 113- [,1] [,2] [,3]
x    1   2   3
y    4   5   64  All of the aboveSolution: A37). Which of the following method make vector of repeated values?A  rep()B  data()C  view()D  None of the aboveSolution: A38). Which of the following finds the position of quantile in a dataset ?A  quantile()B  barplot()C  barchart()D  None of the AboveSolution: A39) Which of the following function cross-tabulate tables using formulas ?A  table()B  stem()C  xtabs()D  All of the aboveSolution: D40) What is the output of the following function?A  Hello, world!14B  Hello, world!\n14C  Hello, world!13D Hello, world!\n13Solution: A41- Which is the missing value from running the quantile function on a numeric vector in comparison to running the summary function on the same vector ?A  MedianB  MeanC  MaximumD  MinimumSolution: B42- Which of the following command will plot a blue boxplot of a numeric vector named vec?A  boxplot(vec,col=blue)B  boxplot(vec,color=blue)C  boxplot(vec,color=BLUE)D  None of the aboveSolution: A43- Which of the following command will create a histogram with 100 buckets of data ?A  hist(vec,buckets=100)B  hist(vec,into=100)C  hist(vec,breaks=100)D  None of the aboveSolution: C44- What does the main parameter in barplot command does ?A  x axis labelB  Title of the graphC  I cant tellD  y axis labelSolution: B45- The below dataframe is stored in a variable named sam:We wish to create a boxplotin a single line of code per B i.e a total of two boxplots (one for East and one for West). Which of the following command will achieve the purpose ?A  boxplot(A~B,data=sam)B  boxplot(A,B,data=sam)C  boxplot(A|B,data=sam)D  None of the aboveSolution: A46- Which of the following command will split the plotting window into 3 X 4 windows and where the plots enter the window row wise.A  par(split=c(3,4))B  par(mfcol=c(3,4))C  par(mfrow=c(3,4))D  par(col=c(3,4))Solution  C47- A dataframe named frame contains two numerical columns named A and B. Which of the following commands will draw a scatter plot between the two columns of the dataframe?A  with(frame,plot(A,B))B  plot(frame$A,frame$B)D  All of the aboveSolution: D
48- The dataframe below is stored in a variable named frame.Which of the following command will draw a scatter plot between A and B differentiated by different color of C like the one below.A  plot(frame$A,frame$B,col=frame$C)B  with(frame,plot(A,B,col=C)C- 1 and 2D- None of the above.Solution: A49- Which of the following does not apply to Rs base plotting system ?A  Can easily go back once the plot has started.(eg: to change margins etc)B  It is convenient and mirrors how we think of building plots and analysing dataC  starts with plot(or similar) functionD  Use annotation functions to add/modify (text, lines etc)Solution: AThe following questions revolve around the ggplot2 package, which is the most widely used plotting package used in the R community and provides great customisation and flexibility over plotting.50- Which of the following function is used to create plots in ggplot2 ?A  qplotB  gplotC  plotD  xyplotSolution: A51- What is true regarding the relation between the number of plots drawn by facet_wrap and facet_grid ?A  facet_wrap > facet_gridB  facet_wrap < facet_gridC  facet_wrap <= facet_gridD  None of the aboveSolution: C52- Which function in ggplot2 allows the coordinates to be flipped? (i.e x bexomes y and vice-versa) ?A  coordinate_flipB  coord_flipC  coordinate_rotateD  coord_rotateSolution: B53- The below dataset is stored in a variable called frame.Which of the following commands will create a bar plot for the above dataset with the values in column B being the height of the bar?A  ggplot(frame,aes(A,B))+geom_bar(stat=identity)B  ggplot(frame,aes(A,B))+geom_bar(stat=bin)C  ggplot(frame,aes(A,B))+geom_bar()D  None of the aboveSolution: A54- The following dataframe is stored in a variable named frame and is a subset of a very popular dataset named mtcars.We wish to create a stacked bar chart for cyl variable with stacking criteria being vs variable .which of the following commands will help us do this ?A  qplot(factor(cyl),data=frame,geom=bar,fill=factor(vs))B  ggplot(mtcars,aes(factor(cyl),fill=factor(vs)))+geom_bar()C  All of the aboveD  None of the aboveSolution: C55  The question is same as above . The only difference is that you have to create a dodged bar chart instead of a stacked one. Which of the following command will help us do that ?A  qplot(factor(cyl),data=frame,geom=bar,fill=factor(vs),position=dodge)B  ggplot(mtcars,aes(factor(cyl),fill=factor(vs)))+geom_bar(position=dodge)C  All of the aboveD  None of the aboveSolution: BI hope you had fun participating in the assessment challenge and reading this article. We tried to answer all your queries but if we still havent cleared all your doubts , then feel free to post your questions in the comments below.And, since it was a new thing which we tried to enrich your experience we would like to know your thoughts / suggestions / feedback on the same. This will help us serve you better and help us understand where should we improve.Also, make sure you register in Statistics Skill Test  2.",https://www.analyticsvidhya.com/blog/2016/08/full-solution-skill-test-on-r-for-data-science/
Solutions for Skilltest in Statistics Revealed,Learn everything about Analytics|Introduction|Overall Results,"Useful resources to learnStatistics|Skill Test Questions and Answers|End Notes|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Full Solution  Skilltest on R for Data Science|10 Real World Applications of Internet of Things (IoT)  Explained in Videos|
Kunal Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Statistics is one of the founding pillars for a career in data science and business analytics. Unless a person understands the basics of statistics well, he will not be able to perform well in data science. We launched Statistics skill test to help our community with a tool to assess their skills in statistics. You can look at the leaderboard of the skill assessment platform hereMore than 1800 people registered on the hackathon and 533 people actually assessed themselves in 2 hours.For all those who could not attend the skill assessment, check out how many questions you can answer correctly. I am sure you will take away learning points form this article and improve your knowledge about statistics.For those who enjoyed the experience and would want to undergo this again on a more advanced topic, here is your chance to register in Statistics Skill Test  2. Also, check out our skill test on R.Who could have asked fora better way to analyze the results of a statistical skill test on this topic? Here is the distribution of the scores:Here are a few measures of the distribution:Mean = 14.99Median = 16Mode = 14Let us look at the variance:Standard Deviation = 8.1395% confidence interval  [0, 30.94)So, congratulations for the top 5 people (31 and above) to set themselves above the rest of the population.If your score is more than 21, you are in the top 25 percentile  you deserve a pat!On the other hand, people with score less than 9 probably need to spend more time on these concepts  believe me, it wasnt tough!The skill test consisted of 40 questions selected very carefully based on the concepts which we think any individual pursuing a career in analytics should have them on their tips.Read on to find out detailed solution of the all the questions.1) Which measure of central tendency describes the following right-skewed distribution in the best manner?a)Meanb)Medianc)Moded)All of theseAns: b) Median
In skewed distributions, the mean will be in one extreme(towards the skew) and mode on the other. Whereas the median lies in the centre.2) Which measure of central tendency describes the following nominal/categorical distribution in the best manner?a)Meanb)Medianc)Moded)All of theseAns: c) ModeMean and median dont make sense in categorical distributions. So mode describes central tendency at best.3) Which measure of central tendency describes the following left-skewed distribution in the best manner?a)Meanb)Medianc)Moded)All of theseAns: b) Median
In skewed distributions the mean will be in one extreme(towards the skew) and mode in the other. Whereas the median lies in the centre.4) Which measure of central tendency suits the best for this bi-modal distribution?a)Meanb)Medianc)Moded)Mean or MedianAns: b) MedianIn Bimodal distributions, if distribution is symmetric then mean or median could be the representative for Central tendency whereas in this case due to skewness which can be clearly seen in the image, the mode lies at the left bump and the mean lies close to the left bump too(due to the left skew). Whereas the median should lie fairly at the centre.5) Which measure of central tendency suits the best for a normal distribution?a)Meanb)Medianc)Moded)All of theseAns: d) All of theseMean = Median = Mode for a normal distribution, as evident in the image.6) Which of the following distribution satisfy the following relationship: Mode > Median > Mean?a)Positive skewedb)Negative skewedc)Normald)Bi-modalAns: b) Negatively skewed
In skewed distributions the mean will be in one extreme(towards the skew) and mode in the other. Whereas the median lies in the centre. In this case mean lies towards the left(the skew).Read more for a detailed explaination.7) Which of the following distribution satisfy the following relationship: Mode < Median < Mean?a)Positive skewedb)Negative skewedc)Normald)Bi-modalAns: a) Positively skewedIn skewed distributions the mean will be in one extreme(towards the skew) and mode in the other. Whereas the median lies in the centre. In this case mean lies towards the right(the skew).Read more for a detailed explaination.8) Which of the following distribution can satisfy the following relationship: Mode > Median > Mean?a)Normalb)Bi-modalc)Uniformd)None of theseAns: b) Bi-modal
Imagine a bi-modal distribution with the mode in the right bump. The relation satisfies in such a distribution.9) Which of the following operation reduces skewness in a Negatively skewed distribution in the best manner?a)logb)squarec)square rootd)skewness isnt reducible propertyAns: b) SquareAny reducible function(in this case log and sqrt) will increase the skew as the values will be pushed to the left. Hence square is the only possible option.Read more for detailed explanation.10) Which of the following operation reduces skewness in a Positively skewed distribution?a)logb)squarec)square rootd)Skewness isnt reducible propertyAns: a) logIn case of positive skew we need to scale the values towards the left to reduce skewness. So any reducible function would suffice(in this case log or sqrt). We cant conclusively say which of the functions work better without knowing the actual distribution.11) Which of these is not a measure of Variability?a)Inter Quartile Rangeb)Variancec)Ranged)MedianAns: d) MedianMedian is a measure of Central tendency whereas others measure Variability / spread.12) To quantify spread/variability a reasonable estimate of Variance can be calculated by averaging.a)squared errorb)absolute errorc)Errors^4d)a & bAns: d) A & BIt is only a matter of convenience on using either of the two. Sometimes people use absolute error and sometimes the square errors depending on their requirement.Read more for your better understanding.13) Why cant Errors^4 be averaged to calculate Variance?a)As per definitionb)because of heavy weightage to outliersc)Gives similar results like squared errorsd)Computationally expensiveAns: b) because of heavy weightage to outliersOur objective is to quantify spread, that is how far each point is from the mean. Sum of Errors^4 will increase the errors due to outliers substantially and overestimate Variance. Hence we avoid it.14) Why is error squared to calculate variance/S.D.?a)By definition of varianceb)So that positive  negative errors dont cancel outc)Empirical evidence shows that its the best estimated)None of TheseAns: b) So that positive  negative errors dont cancel outOur objective is to quantify spread, that is how far each point is from the mean. To compute how far we need to ensure that errors dont cancel out. Hence we square or take the absolute and then compute average.15) Which of these is not possible (Numerically)?a)Mean > Variance > Standard Deviationb)Variance > Standard deviation > Meanc)Mean > Standard Deviation > Varianced)None of theseAns: d) None of theseAll are possible. For Variance > 1, Variance is always greater than Standard Deviation. So a) and b) are possible(imagine a normal distribution with a large mean and negative mean respectively). c) is possible for Variance < 1 and a Mean > 1. So d) is the answer.16) Which of the following is the best point estimate for population mean?a)Sample meanb)Sample mean/root(n-1)c)Sample mediand)Sample median/root(n-1)Ans: a) Sample MeanExpected value(Sample Mean) = Population Mean17) Which of the following is the best point estimate for population standard deviation?a)Sample standard deviationb)sqrt(Sum of squared errors/n-1)c)sqrt(Sum of squared errors/n)d)None of TheseAns: b) sqrt((Sum of squared errors)/(n-1)))Expected value(sqrt((Sum of squared errors)/(n-1))) = Population Standard DeviationThis is called Bessels correction.Read more for better understanding.18) Population A has a normal distribution and Population B has an exponential distribution. The sampling distribution of sample means(large sample size) of both A and B area)Both Exponentialb)Normal for A and Exponential for Bc)Exponential for A and Normal for Bd)Both NormalAns: d) Both NormalCentral Limit theorem say that the sampling distribution of sample means for a large enough sample from any distribution follows a normal distribution.19) Since the population size is always greater than the sample size, which of the following is true ?a)the sample parameter can never be equal to the population parameterb)The sample parameter can never be greater than the population parameterc)The sample parameter can never be lesser than the population parameterd)None of theseAns: d) None of theseDepending on what sample has been drawn from the population, the statistic can be greater, lesser or equal to the population parameter.20) Population A has a normal distribution and population B has an exponential distribution. The z distributions of both A and B is ?a)the sample parameter can never be equal to the population parameterb)The sample parameter can never be greater than the population parameterc)The sample parameter can never be lesser than the population parameterd)None of theseAns: d) The same normal distributionThe z-distribution is one absolute normal distribution with mean 0 and standard deviation 1.21) Which diagram best represents u (point estimate),  (population mean),  (population standard deviation) for approximately 95 percent confidence interval ?Ans:a)b)c)d)
When we estimate population mean from sample mean we assume that sample mean lies within the 95% interval of the sampling distribution. Watch the below video to get a better understanding.22) Which is the best point estimate among the A, B, C & D (given are the frequency plots for each point estimate and  is the population parameter we are trying to estimate) ?a)b)c) d)Ans: b) The point estimate should have low bias and low variance. Option a) has zero bias and high variance. Option b) has low bias and low variance. Option c) has high variance and high bias.
Option d) has high bias and low variance. So we go with Option b).23) Suppose I say The population parameter lies in 80% confidence interval (100, 200). What is the confidence level?a)20%b)95%c)80%d)50%Ans: c) 80%Confidence level is how confident we are on the confidence interval, so 80%.24) A group of students were surveyed on whether they skip breakfast or not. The 95% confidence interval was found to be (0.20, 0.27). Which of the following is the correct interpretation of the 95% confidence interval ?a)There is a 95% probability that the proportion of young adults who skip breakfast is between 0.20 and 0.27b)If this study were to be repeated with a sample of the same size, there is a 95% probability that the sample proportion would be between 0.20 and 0.27.c)We can be 95% confident that the sample proportion of young adults who skip breakfast is between 0.20 and 0.27d)We can be 95% confident that the population proportion of young adults who skip breakfast is between 0.20 and 0.27.Ans: d) We can be 95% confident that the population proportion of young adults who skip breakfast is between 0.20 and 0.27By definition of Confidence interval d) suits the best.Read more for better understanding.25) What is the relationship between significance level and confidence level?a)Significance level = Confidence levelb)Significance level = 1  Confidence levelc)Significance level = 1/Confidence leveld)Significance level = sqrt(1-Confidence level)Ans: b) Significance level = 1  Confidence levelIf alpha equals 0.05, then your confidence level is 0.95. If you increase alpha, you both increase the probability of incorrectly rejecting the null hypothesis and also decrease your confidence level. alpha is synonymous with Significance level.Read more for better understanding.26) The distribution of number of travels per year is normal with a mean of 50 and a standard deviation of 8. Which option describes how to find the proportion of people that have a number of travels greater than 58?a)Find the area to the left of z = 1 under a standard normal curve.b)Find the area between z = -1 and z = 1 under a standard normal curve.c)Find the area to the right of z = 1 under a standard normal curve.d)Find the area to the right of z = -1 under a standard normal curve.Ans: c) Find the area to the right of z = 1 under a standard normal curve The z-value can be calculated to be 1. We are looking at proportions which have values greater than the given value as shown below.27) A sample of 400 students from a university were randomly selected. They were asked if the current duration of the university needed to be reduced. 46% of the students, answered yes. Which one of the following statements about the number 46% is correct?a)It is a sample statisticb)It is a population parameter.c)It is a margin of errord)it is a standard errorAns: a) It is a sample statistic400 is the sample size and 46% is the measure calculated on that sample otherwise known as sample statistic.28) A sample of 400 students from a university were randomly selected. They were asked if the current duration of the university needed to be reduced. 46% of the students, answered yes. What is the standard error of the sample proportion of students who answered yes to the question?a) 0.249b)0.0249c) 0.498d) 0.0498Ans: b) 0.0249SE of proportion = sqrt [ p(1  p) / n ], applying the above formula we can calculate the SE to be 0.0249.Read more for better understanding.29) A sample of 400 students from a university were randomly selected. They were asked if the current duration of the university needed to be reduced. 46% of the students, answered yes. If the sample proportion of students who answered yes to the question was 26% instead of 46%, the margin of error would be?a) smallerb) largerc) samed)Cant determineAns: a) smallerMargin of error = 2*SE of proportion = 2*sqrt [ p(1  p) / n ]. Calculating Margin of error for both 26% and 46% we find that Margin of error of 26% to be smaller than that of 46%.Read more for better understanding.30) A sample of 400 students from a university were randomly selected. They were asked if the current duration of the university needed to be reduced. 46% of the students, answered yes. If the sample consisted of 300 students instead of 400 students, but the sample proportion of students who answered yes to the question was still 46%, the margin of error would be ?a) smallerb) largerc) samed)Cant determineAns: b) largerMargin of error = 2*SE of proportion = 2*sqrt [ p(1  p) / n ]. Calculating Margin of error for both sample sizes (n = 400 and n = 300) find that Margin of error of n = 300 to be larger than that of n = 400.Read more for detailed explaination.31) The 95% Confidence interval of population mean is calculated from a sample. If a few outliers are added to the sample the new 95% Confidence interval would be ?a) widerb) thinnerc) samed)Insufficient dataAns: a) WiderConfidence interval = (sample mean  Margin of error, sample mean + Margin of error). The size of the interval is determined by Margin of error. Margin of error = 2 * Standard error, Standard error = Standard deviation/sqrt(n). Adding outliers will increase the standard deviation which will increase the standard error which again will in turn increase the margin of error, thus making the interval wider.32) For a population with standard deviation = 7, a sample of 9 elements was chosen arbitrarily. The sample mean was found out to be equal to 56. Calculate the margin of error assuming 95% confidence interval.a)6.79b)5.25c)4.57d)5.33Ans: c) 4.57Margin of error = 2 * Standard error, Standard error = Standard deviation/sqrt(n). Applying the formula gives 4.57 as the Margin of error.33) For a population with standard deviation = 7, a sample of 9 elements was chosen arbitrarily. The sample mean was found out to be equal to 56. Assuming that the sample mean lies in the margin of error, the 95% confidence interval in which population mean lies is given by ?a)(51.43, 60.57)b)(49.21, 62.79)c)(50.67, 61.33)d)(50.75, 61.25)Ans: a) (51.43, 60.57)Confidence interval = (sample mean  2*Standard error, sample mean + 2*Standard error)Read more for further explaination.34) Find the minimum confidence level for which population mean of 60 lies within the confidence interval of sample (sample mean = 54, standard deviation of the population = 10 and the size of sample = 25).a) 95%b)98.67%c)99.87%d)99.92%Refer to the z-table and t- table.Ans: c) 99.87%z = (x  u)/sigmaCalculating z-value and referring to the z-table we get c).Read more for further explaination.35) A 95% confidence interval was computed to be 0.20 to 0.27. From the information provided, we can determine that (where,  = sample mean, u = population mean) ?a) = 0.235 and margin of error = 0.035b) = 0.235 and margin of error = 0.07c)u = 0.235 and margin of error = 0.035d)u = 0.235 and margin of error = 0.07Ans: a)  = 0.235 and margin of error = 0.035Confidence interval = (sample mean  Margin of error, sample mean + Margin of error). From the above formula we obtain a).Read more for further explaination.36) From a sample the 95% confidence interval is already computed. What is the probability that the population parameter lies in the interval?a)0.95b)0.5c)0.05d)None of theseAns: b) 0.5This is a tricky question which requires a comprehensive explanation. The misunderstandings section in Wikipedia has a good explanation.Read more for detailed explaination.37) A random sample of 1000 people is taken from a population of over a billion, in order to compute a confidence interval for some proportion. If the researchers wanted to decrease the width of the confidence interval, they could ?a)decrease the size of populationb)decrease the size of samplec)increase the size of populationd)increase the size of sampleAns: d) increase the size of sampleConfidence interval = (sample mean  Margin of error, sample mean + Margin of error). The size of the interval is determined by Margin of error. Margin of error = 2 * Standard error, Standard error = Standard deviation/sqrt(n).38) Suppose that a 95% confidence interval for the proportion of students at a school who played cricket is 35% 5%. The confidence level is ?a)5%b)35%c)95%d)None of TheseAns: c) 95%Confidence level is the measure of confidence on the computed interval, which implies that confidence level is 95%39) Suppose that a 95% confidence interval for the proportion of students at a school who played cricket is 35% plus or minus 5%. The margin of error is ?a)10%b)5%c)35%d)95%Ans: b) 5%Confidence interval = (sample mean  Margin of error, sample mean + Margin of error). Hence b).40) Suppose that a 95% confidence interval for the proportion of students at a school who played cricket is 35% plus or minus 5%. The 95% confidence interval for the proportion of students playing cricket is ?a)10%b)5%c)35%d)95%Ans: d) 30% to 40%Confidence interval = (sample mean  Margin of error,sample mean + Margin of error). Hence d).I hope you had fun participating in the assessment challenge and reading this article. We tried to answer all your queries but if we still havent cleared all your doubts , then feel free to post your questions in the comments below. And since it was a new thing which we tried to enrich your experience we would like to know your thoughts / suggestions / feedback on the same. This will help us serve you better and help us understand where should we improve.Also, make sure you register in Statistics Skill Test  2and the upcomingskill test on Rtomorrow.",https://www.analyticsvidhya.com/blog/2016/08/solutions-for-skilltest-in-statistics-revealed/
10 Real World Applications of Internet of Things (IoT)  Explained in Videos,Learn everything about Analytics|Overview|Introduction|How Big is IoT?|10 Real World Applications of IoT|End Notes,"1. Smart Home|2. Wearables|3. Connected Cars||4. Industrial Internet|5. Smart Cities|6.IoT in agriculture|7.Smart Retail|8.Energy Engagement|9. IOT in Healthcare|10.IoT in Poultry and Farming|References|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Solutions for Skilltest in Statistics Revealed|Beginners Guide to Topic Modeling in Python|
Analytics Vidhya Content Team
|22 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you know what separates humans from other living beings?Curiosity. Humans are curious. We question a lot. We are the ones who challenge the status quo of existing rules and strive to build/produce something better. Such curiosity & efforts have promised us a life where electronic devices & machines will probably become our best friend.Yes, you read it correctly the vision to make machines smart enough to reduce human labour to almost nil. The idea of inter-connected devices where the devices are smart enough to share information with us, to cloud based applications and to each other (device to device).Smart devices or Connected devices  as they are commonly called, are designed in such a way that they capture and utilize every bit of data which you share or use in everyday life. And these devices will use this data to interact with you on daily basis and complete tasks.This new wave of connectivity is going beyond laptops and smartphones, its going towards connected cars, smart homes, connected wearables, smart cities and connected healthcare. Basically a connected life. According to Gartner report, by 2020 connected devices across all technologies will reach to 20.6 billion. Woah! thats a huge number.Source: HPHP did a small survey in which they estimated therise of connected devices over the years and the results are surprising. Are we moving towards a fully automated world?These devices will bridge the gap between physical and digital world to improve the quality and productivity of life, society and industries. With IoT catching up Smart homes is the most awaited feature, with brands already getting into the competition with smart applicances. Wearables are another feature trending second on the internet. With launch of Apple Watch and more devices to flow in, these connected devices are going to keep us hooked with the inter-connected world.A survey conducted by KRC Reserach in UK, US, Japan and Germany the early adopters of IOT has revealed which devices are the customers more likely to use in the coming years. Smart Appliances like thermostat, smart refrigerator to name a few are most liked by the customers and are seem to change the way we operate.Source:GSMA ReportIf you are wondering what impact will IoT have on the economy then for your information as per the Cisco report IoT will generate $14.4 trillion in value across all industries in the next decade. Yes, you are thinking correctly IoT will bring a wave, nobody can forsee.Now, to give you a glimpse of how applications of IoT will transform our lives I have listed down few areas where IoT is much awaited and companies are preparing to surprise you with smart devices.For better understanding, Ive added Youtube Videos for you, see what our future holds.Read on and tell us which smart devices are you eager to use.With IoT creating the buzz, Smart Home isthe most searched IoT associated feature on Google. But, what is a Smart Home?Wouldnt you love if you could switch on air conditioning before reaching home or switch off lights even after you have left home? Or unlock the doors to friends for temporary access even when you are not at home. Dont be surprised with IoT taking shape companies are building products to make your life simpler and convenient.Smart Home has become the revolutionary ladder of success in the residential spaces and it is predicted Smart homes will become as common as smartphones.The cost of owning a house is the biggest expense in a homeowners life. Smart Home products are promised to save time, energy and money. With Smart home companies like Nest, Ecobee, Ring and August, to name a few, will become household brands and are planning to deliver a never seen before experience.Heres a brief video which shows youa smart home from the future and how your life will be simplified.Read more to find out the best smart devices.Wearables have experienced a explosive demand in markets all over the world. Companies like Google, Samsung have invested heavily in building such devices. But, how do they work?Wearable devices are installed withsensors and softwares whichcollect data and information about the users. This data is later pre-processed to extract essential insights about user.These devices broadly cover fitness, health and entertainment requirements. The pre-requisite from internet of things technology for wearable applications is to be highly energy efficient or ultra-low power and small sized.Here are some top examples of wearable IoT devices that fulfill these requirements.Read more to find latestnews making headlines on smart wearables.The automotive digital technology has focused on optimizing vehicles internal functions. But now, this attention is growing towards enhancingthe in-car experience.A connected car is a vehicle which is able to optimize its own operation, maintenance as well as comfort of passengers using onboard sensors and internet connectivity.Most large auto makers as well as some brave startups are working on connected car solutions. Majorbrands like Tesla, BMW, Apple, Googleare working on bringing the next revolution in automobiles.Watch the video to experience the future of connected cars.Read more to know about the updates on connected cars.Industrial Internet is the new buzz in the industrial sector, also termed as Industrial Internet of Things ( IIoT ). It is empoweringindustrial engineering with sensors, software and big data analytics to create brilliant machines.According to Jeff Immelt, CEO, GE Electric, IIoT is a beautiful, desirable and investable asset. The driving philosophy behind IIoT is that, smart machines are more accurate and consistent than humans in communicating through data. And, this data can help companies pick inefficiencies and problems sooner.IIoT holds great potential for quality control and sustainability. Applications for tracking goods, real time information exchange about inventory among suppliers and retailers and automated delivery will increase the supply chain efficiency. According to GE the improvement industry productivity will generate $10 trillion to $15 trillion in GDP worldwide over next 15 years.The video explains emergence of IIoT in industries very accurately.Read moreto know the latest on IIoT.Smart city isanother powerful application of IoT generating curiosity among worlds population. Smart surveillance, automated transportation, smarter energy management systems, water distribution, urban security and environmental monitoring all are examples of internet of things applications for smart cities.IoT will solve major problems faced by the people living in cities like pollution, traffic congestion and shortage of energy supplies etc. Products like cellular communication enabled Smart Belly trash will send alerts to municipal services when a bin needs to be emptied.By installing sensors and using web applications, citizens can find free available parking slots across the city. Also, the sensors can detect meter tampering issues, general malfunctions and any installation issues in the electricity system.To understand better the functioning of Smart Cities check out this video.Read more to know more about Smart Cities.With the continous increase in worlds population, demand for food supply is extremely raised. Governments are helping farmers to useadvanced techniques and research to increase food production. Smart farming is one of the fastest growing field in IoT.Farmers are using meaningful insights from the data to yield better return on investment. Sensing for soil moisture and nutrients, controlling water usage for plant growth and determining custom fertilizer are some simple uses of IoT.If you are curious, the video below explains further about this concept.Read moreto know the latest about IoT in agriculture.The potential of IoT in the retail sector is enormous. IoT provides an opportunity to retailers to connect with the customers to enhance the in-store experience.Smartphones will be the way for retailers to remain connected with their consumers even out of store. Interacting through Smartphones and using Beacon technology can help retailers serve their consumers better. They can also track consumers path through a store and improve store layout and place premium products in high traffic areas.Watch this video to find out how connected retail will make your life easier.Read moreto know the latest technology changing the face of retail.Power grids of the future will not only be smart enough but also highly reliable. Smart grid concept is becoming very popular all over world.The basic idea behind the smart grids is to collect data in an automated fashion and analyze the behavior or electricity consumers and suppliers for improving efficiency as well as economics of electricity use.Smart Grids will also be able to detect sources of power outages more quickly and at individual household levels like near by solar panel, making possible distributed energy system.Heres a video to explain how smart grid operates.Read moreto know the power of IoT in energy saving.Connected healthcare yet remains the sleeping giant of the Internet of Things applications. The concept of connected healthcare system and smart medical devices bears enormous potential not just for companies, but also for the well-being of people in general.Research shows IoT in healthcare will be massive in coming years. IoT in healthcare is aimed at empowering people to live healthier lifeby wearing connected devices.The collected data will help in personalized analysis of an individuals health and provide tailor made strategies to combat illness. The video below explains how IoT can revolutionize treatment and medical help.Read more to know latest news about IoT in Healthcare.Livestock monitoring is about animal husbandry and cost saving. Using IoT applications to gather data about the health and well being of the cattle, ranchers knowing early about the sick animal can pull out and help prevent large number of sick cattle.With the help of the collected data and ranchers can increase the poultry production. Watch this interesting video.The future of IoT is more fascinating than this where billions of things will be talking to each other and human intervention will become least. IoT will bring macro shiftin the way we live and work.I hope you had fun reading about all thesepowerful and promisingapplications of Internet of things. There are many more areas where IoT is making an impact.Networked Toys is one application of IoT which will change the playing experience of your kids. IoT can also be used in thedetection ofenvironmental issues.Did you like reading this article? Now am sure you will be able to tell which smart device you are eagerly waiting for. Tell us in the comments below.And if you are currently related to an IoT related profile. Do share your experience and concerns in the comments sections.",https://www.analyticsvidhya.com/blog/2016/08/10-youtube-videos-explaining-the-real-world-applications-of-internet-of-things-iot/
Beginners Guide to Topic Modeling in Python,Learn everything about Analytics|Introduction|Table of Content|Latent Dirichlet Allocation for Topic Modeling|Running in python|Tips to improve results of topic modeling||Topic Modelling for Feature Selection|Endnotes|References,"Parameters of LDA|Preparing Documents|Cleaning and Preprocessing|Preparing Document-Term Matrix|Running LDA Model|Results|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|10 Real World Applications of Internet of Things (IoT)  Explained in Videos|Bringing Analytics into Indian Film Industry with Back Tracing Algorithm|
Shivam Bansal
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Industry is all about obtaining the Information from the data. With the growing amount of data in recent years, that too mostly unstructured, its difficult to obtain the relevant and desired information. But, technology has developed some powerful methods which can be used to mine through the data and fetch the information that we are looking for.One such technique in the field of text mining is Topic Modelling. As the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called topics) in large clusters of texts.Topics can be defined as a repeating pattern of co-occurring terms in a corpus. A good topic model should result in  health, doctor, patient, hospital for a topic  Healthcare, and farm, crops, wheat for a topic  Farming.Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example  New York Times are using topic models to boost their user  article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.So, if you arent sure about the complete process of topic modeling, this guide would introduce you to various concepts followed by its implementation in python.There are many approaches for obtaining topics from a text such as  Term Frequency and Inverse Document Frequency. NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation is the most popular topic modeling technique and in this article, we will discuss the same.LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. The following matrix shows a corpus of N documents D1, D2, D3  Dn and vocabulary size of M words W1,W2 .. Wn. The value of i,j cell gives the frequency count of word Wj in Document Di.LDA converts this Document-Term Matrix into two lower dimensional matrices  M1 and M2.
M1 is a document-topics matrix and M2 is a topic  terms matrix with dimensions (N, K) and (K, M) respectively, where N is the number of documents, K is the number of topics and M is the vocabulary size.Notice that these two matrices already provides topic word and document topic distributions, However, these distribution needs to be improved, which is the main aim of LDA. LDA makes use of sampling techniques in order to improve these matrices.It Iterates through each word w for each document d and tries to adjust the current topic  word assignment with a new assignment. A new topic k is assigned to word w with a probability P which is a product of two probabilities p1 and p2.For every topic, two probabilities p1 and p2 are calculated. P1  p(topic t / document d) = the proportion of words in document d that are currently assigned to topic t. P2  p(word w / topic t) = the proportion of assignments to topic t over all documents that come from this word w.The current topic  word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word  topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current words topic with new probability.After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.Alpha and Beta Hyperparameters  alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.Number of Topics  Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. I will not discuss this in detail, as it is too mathematical. For understanding, one can refer to this[1]original paper on the use of KL divergence.Number of Topic Terms  Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.Number of Iterations / passes  Maximum number of iterations allowed to LDA algorithm for convergence.You can learn topic modeling in depth here.Here are the sample documents combining together to form a corpus.doc1 = ""Sugar is bad to consume. My sister likes to have sugar, but not my father.""
 doc2 = ""My father spends a lot of time driving my sister around to dance practice.""
 doc3 = ""Doctors suggest that driving may cause increased stress and blood pressure.""
 doc4 = ""Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.""
 doc5 = ""Health experts say that Sugar is not good for your lifestyle."" # compile documents
 doc_complete = [doc1, doc2, doc3, doc4, doc5]Cleaning is an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the corpus. ```
 from nltk.corpus import stopwords 
 from nltk.stem.wordnet import WordNetLemmatizer
 import string
stop = set(stopwords.words('english'))
 exclude = set(string.punctuation) 
 lemma = WordNetLemmatizer()
def clean(doc):
  stop_free = "" "".join([i for i in doc.lower().split() if i not in stop])
  punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
  normalized = "" "".join(lemma.lemmatize(word) for word in punc_free.split())
  return normalizeddoc_clean = [clean(doc).split() for doc in doc_complete] 
 ```All the text documents combined is known as the corpus.To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix. Python provides many great libraries for text mining practices, gensim is one such clean and beautiful library to handle text data. It is scalable, robust and efficient. Following code shows how to convert a corpus into a document-term matrix.
```
# Importing Gensim
import gensim
from gensim import corpora

# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
 ```Next step is to create an object for LDA model and train it on Document-Term matrix. The training also requires few parameters as input which are explained in the above section. The gensim module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents.
```
# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel

# Running and Trainign LDA model on the document term matrix.
ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50) ``````
 print(ldamodel.print_topics(num_topics=3, num_words=3))
 ['0.168*health + 0.083*sugar + 0.072*bad,
'0.061*consume + 0.050*drive + 0.050*sister,
'0.049*pressur + 0.049*father + 0.049*sister]
 ```Each line is a topic with individual topic terms and weights. Topic1 can be termed as Bad Health, and Topic3 can be termed as Family.The results of topic models are completely dependent on the features (terms) present in the corpus. The corpus is represented as document term matrix, which in general is very sparse in nature. Reducing the dimensionality of the matrix can improve the results of topic modelling. Based on my practical experience, there are few approaches which do the trick.1. Frequency Filter  Arrange every term according to its frequency. Terms with higher frequencies are more likely to appear in the results as compared ones with low frequency. The low frequency terms are essentially weak features of the corpus, hence it is a good practice to get rid of all those weak features. An exploratory analysis of terms and their frequency can help to decide what frequency value should be considered as the threshold.2. Part of Speech Tag Filter  POS tag filter is more about the context of the features than frequencies of features. Topic Modelling tries to map out the recurring patterns of terms into topics. However, every term might not be equally important contextually. For example, POS tag IN contain terms such as  within, upon, except. CD contains  one,two, hundred etc. MD contains may, must etc. These terms are the supporting words of a language and can be removed by studying their post tags.

3. Batch Wise LDA In order to retrieve most important topic terms, a corpus can be divided into batches of fixed sizes. Running LDA multiple times on these batches will provide different results, however, the best topic terms will be the intersection of all batches.Note: We also havea video based course on NLP, covering Topic Modelling and its implementation in Python.Sometimes LDA can also be used as feature selection technique. Take an example of text classification problem where the training data contain category wise documents. If LDA is running on sets of category wise documents. Followed by removing common topic terms across the results of different categories will give the best features for a category.With this, we come to this end of tutorial on Topic Modeling. I hope this will help you to improve your knowledge to work on text data. To reap maximum benefits out of this tutorial, Id suggest you practice the codes side by side and check the results.Did you find the article useful? Share with us if you have done similar kind of analysis before. Do let us know your thoughts about this article in the box below.",https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/
Bringing Analytics into Indian Film Industry with Back Tracing Algorithm|Introduction,Learn everything about Analytics|Table of Contents|Assessing the contemporary landscape of Analytics|What is the reason for this disparity?|Challenges of Analytics Adoption in Films|Marketing Promotion inFilms|The Analysis|Advantages of the approach|End Notes,"But this is changing|Estimating the first week revenue from ticket sales|Effectively Allocating Spends|Finding Constraints and Optimizing|About the Authors|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Beginners Guide to Topic Modeling in Python|Senior Analyst  Ahmedabad (2 years of experience)|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With a turnover of 2.23 Billion USD and overall marketing spend of roughly 50% per film, the film business in India is one of the highest spenders in marketing promotion.A decade back most of this promotion was straightforward. It had limited number of channels to spread the marketing message to a less distracted target audience. With the dawn of the social media and ubiquity of smartphones, reaching an attention deficit audience is now the biggest challenge for production houses and their marketing teams.Which channels to focus on? How to customize the message for each channel? How to allocate channel wise spend? These are some of the questions that challenge todays film business.This whitepaper attempts to highlight the strategic and tactical challenges facing the current film business in India and how a predictive analytics framework can address them. Weve used back tracing algorithm and achieved 94% prediction accuracy.For over two decades analytics has been helping industries evolve their business models to address the myriad of challenges emerging in competitive markets.Industries such as CPG, Retail, Healthcare, Banking and Telecom are the pioneers of analytics directed business decision making globally. According to a study done by Everest research, the combined adoption of analytics by these sectors accounts to more than 80% of the overall analytics penetration in different sectors.Analytics adoption in these industries started with data reporting and has grown over the last couple of decades to provide a wide range of customized solutions. These solutions constantly assess the business impact and provide feedback on areas of improvement for business decision making. With the demand, these solutions are increasing at the volume and in sophistication as well.As the analytics adoption in these industries continues to fuel informed decision making, industry domains such as travel, logistics, film production, gaming, utilities and energy are still far away from adoption of analytics in their business.Even though the marketing spend in these industries have a similar revenue proportion to early adopters (CPG, Retail), these sectors see low adoption of analytics.Despite the lowest revenue amongst industries, Film production is the highest spender in marketing.To answer this, lets look into the enablers for early adopters:The emergence of social media, smartphones & digital advertising has resulted in an era of an always on consumer who is interested to directly engage with the brand and other consumers on their platforms. Such activities by a consumer lead a digital trail, commonly known as big data, which can now be mined for insights around brand usage, consumption patterns and ad effectiveness.The industry is also witnessing reforms in the traditional ways of data collection. For example, in in-cinema advertising, business models on contracts are shifting from lump sum deals to pay per view deals, which is a better way to measure the efficiency of the medium.The success of low budget Indian films like Manjhi, No one killed Jessica, Peepli Live, Kahaani etc. has given big budget films a run for their moneyWith an overall revenue of 138 billion INR and a projected revenue of 1660 billion INR by 2017*, with theatre revenues accounting for the major chunk of the total revenue, Bollywood can no longer be considered a fringe industry.Similar to banking or FMCG, marketing promotions for films account for a major spend in the industry and are no different from the planned marketing campaigns of product launches. It is imperative that the industry manages its spend and brings in data informed decision making to identify channels that can result in the highest ROMI (Return On Marketing Investment).Below are few trends that are disrupting the business as usual in Bollywood :                                      Source: KPMG India Analysis                                        Source: KPMG India AnalysisIt is now widely accepted that a film will not get a good opening weekend unless it is heavily promoted. These days, a films success heavily depends on its intelligent omni-channel marketing campaigns.A typical marketing mix for most film promotions looks like this:The marketing promotion for most films is becoming increasingly digital with promotion happening across TV, Radio, Social media websites such as Facebook, Instagram, Twitter to mobile gaming.Without proper marketing development, a film with fantastic plotlines, actors, sets, and special effects may fail to attract the audience. A successful campaign needs a thoroughly devised plan that focuses not only on the type of marketing channels but also on how they are to be connected (TV ads might drive people to check out the Wikipedia page or YouTube channel).As a surprising fact, a typical Indian film takes 6 months in planning and 18 months in execution, whereas a Hollywood film takes 36 months to plan and 12 months to execute.All these trends make this industry latent enough to gain enormously from analytical frameworks and applications. Analytics can jots down these trends in the form of data points to provide quantum of spend per marketing vehicle, coupled with the time gap between any two activities to account for audiences memory retention/decay factor.We initiated an 8-month analysis of the Indian film industry. Before diving into the solution, lets briefly understand how this industry works.Accounting for the rising trends while encompassing the above discussed process flow of distribution, weve come up with a solution to quantitatively plan and optimize a films marketing activities. The solution has three components:Weve used a back tracing model by initially estimating the first weeks box-office collection. First weeks collection will serves as a target set by film producers. Then, we find the contribution of each marketing activity and other factors associated with films success (such as genre, star cast etc).With DVD sales declining and mobile/online rights yet to gain momentum, studios strive to make back all of their money and more during theatrical runs.This estimate is heuristically computed to serve as a realistic target for the producer.First week box office collection= 9 territories * No. of screens * Average No. of seats * Average ticket price * 7 days of weekProducers have the information about the no. of screens in each territory. By classifying these screens into regular theatres, small multiplexes and large multiplexes, we can estimate the total no. of seats. For example- on an average, there are 200 seats in a regular theatre, 150 in a small multiplex and 225 in a large multiplex.Based on past records of the region and the films with similar banner, production budget, crew, genre, time of release etc., the average ticket price can be assumed to define a pragmatic box office target.Lets identify different factors that go into the success of film at box office in its first week. For each of the marketing activities, weve considered the factors listed below :We often hear people saying that its a Shahrukh Khan film or its a Yash Raj Banner, it will definitely make hundreds of crores at the box office. This suggests that audience does pay attention to the star cast and production house of the film, before deciding to watch it. The following factors are also important:Now comes the external factors that determine the footfall of audience to the theatres.While optimizing spends, we accounted for following constraintsWe gathered and used data for all the above factors and created a predictive algorithm to forecast first week collection at box office. The model had 94% accuracy. The algorithm (back tracing algorithm) was used to optimize and distribute spends across marketing activities in order to achieve the forecasted box office collection. The methodology incorporates time decay and position based algorithms to effectively spread marketing activities over time.  (Before)                                                  (After)                       (Before)                                                 (After)The above solution is customized and delivered in a user friendly interface that allows to simulate factors for upcoming films making it dynamic enough to empower the production house with effective decision making at any time in the future.The above comprehensive solution can address the major roadblocks in effectively planning a film promotion in todays digitally driven multi-channel world:The emergence of new technology and the era of big data has enabled such unconventional sectors as film industry to reap the benefits of analytics.Moreover, the latest trends witnessed by this industry have brought in a plethora of platforms to connect with the consumers, a scenario that calls for a more systematic approach to increase both the efficiency and efficacy of promotions through each platform in order to achieve maximum possible ROI from minimum possible investments.On top of this, the last decade has not only seen an increase in the diversity of devices on which entertainment is consumed but also in the profile of the audience that is now getting used to custom content. Profiling of customers for better targeting, personalization of content on select mediums etc. comprise endless opportunities to benefit from analytics.To conclude, we will not be wrong to say that such industries are like gold mines awaiting new frameworks to extract the most out of it. Hope you liked reading this article. Please share your experience / opinion on this interesting topic.Sanjeev Mishra is co-founder and CEO of Convergytics Solutions- an analytics consulting company. With 18 years of experience, he has been instrumental in institutionalising analytics in Fortune 50 companies across industries and domains in developed and developing economies.Mansha Sharma is an avid practitioner of analytics and in a very short time of 2 years, has become a sounding board for analytics in her organization. She enjoys providing solutions through transformation of business concerns into mathematical problems.",https://www.analyticsvidhya.com/blog/2016/08/bringing-analytics-into-indian-film-industry-with-back-tracing-algorithm/
"Senior Analyst  Ahmedabad (2 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Bringing Analytics into Indian Film Industry with Back Tracing Algorithm|Consultant  Ahmedabad (3-5 years of Experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Senior AnalystLocation  AhmedabadAbout employerConfidentialJob description:A Senior Analyst is in a role that requires thorough understanding of Marketing and Customer Data, Relational Database Models and working knowledge of SQL queries. It is implied that the individual has a strong analytical bent of mind and an urge to solve problems through the effective usage of Microsoft Excel, PowerPoint, Relational Database Models, SQL and SAS.
In-depth understanding and some working knowledge of Marketing and Customer data is necessary. The individual is expected to own and take initiative with a zeal to execute projects with minimum guidance from ConsultantsResponsibilitiesSpecifically, this person will:Competencies and Skills.Education and Experience:Interested people can apply for this job can mail their CV to[emailprotected]with subject asSenior Analyst  Ahmedabad",https://www.analyticsvidhya.com/blog/2016/08/senior-analyst-iqr-analytics-ahmedabad-0-3-years-experience/
"Consultant  Ahmedabad (3-5 years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  Ahmedabad (2 years of experience)|Industry Insight  Fighting Cyber Fraud with Analytics|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ConsultantLocation  AhmedabadAbout employer ConfidentialJob description:A Strategic Consultant will play the role of a Team Leader handling various projects/ teams simultaneously and must have had experience in handling Financial Services for Banks and other financial institutions, performing Marketing Analytics for Retail Banking and independently completing major projects in analytics for Credit Card portfolios that include Response modelling, BT modelling, Portfolio Diagnostics, DM campaign management etc.
The Consultant should be a thought leader donning multiple hats of Project Management, Team Management, being a champion of new initiatives and developing strategic analysis plans for specific programs or the entire banking portfolio. The individual must manifest a logical think tank and provide effective solutions to stake holders. The person is expected to have past client facing experience with knowledge on Statistical Modelling and familiarity with SQL. The Consultant should be a change agent for the organization and create positive results through increased ROI and projects for clients.Competencies and Skills. Experience:Interested people can apply for this job can mail their CV to[emailprotected]with subject asConsultant  Ahmedabad",https://www.analyticsvidhya.com/blog/2016/08/consultant-iqr-consulting-ahmedabad/
Industry Insight  Fighting Cyber Fraud with Analytics,Learn everything about Analytics|Introduction|Fraudsters  Enabled by Technology|Analytics Techniques for Detecting Fraud|Companies Fighting Cyber Fraud on the Field|What Does the Future Hold?,"Recent Stats on Cyber Fraud|Recent Frauds in the Banking & Healthcare Industries|Forensic Data Analytics Tools|End Notes|About the Author|||Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Consultant  Ahmedabad (3-5 years of Experience)|Launch of AV Casino  An Introduction to Probability|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dave Palmer, CTO of Darktrace, a global leader in cyber threat defence believes that technological progress has propelled society in to a golden age of criminality. This is not hard to believe considering news headlines about ransomware, stolen credit cards, identity thefts and DDos attacks have become a daily occurrence. Given that public knowledge of being compromised by a cybercrime can be severely damaging to an organizations brand, many breaches are swept under the rug or an agreement is reached with the perpetrators to avoid negative publicity. Although the majority of the media attention is devoted to attacks on the financial services sector, the truth is that energy, defense, technology, telecom and even the education sector are vulnerable.Ref: Statista-Avg annual cost of CyberCrime(1)According to a survey undertaken by KPMG, among 750 fraud cases worldwide, 190 cases had weak internal controls as the major contributing factor. However, there were cases where even when the controls were adequate and effective, fraudsters managed to trespass or override them. The fraudulent activity is normally detected by whistleblowers or suspicious vendors, however, a complex detection system is put in place to validate C-suite executives.Fraudsters mostly work in groups because overriding systems usually entails collusion to circumvent controls. This is compounded by the same survey, which revealed that a typical cyber fraudster is usually male (statistically speaking, 5 out of 6 fraudsters are male) in between the ages of 36 and 55, and is an executive member of the victim organization in disciplines such as operations, finance or generalmanagement.Ref: KPMG  Age of Fraudsters(2)Not surprisingly, the report by KPMG identified technology as an increasing driving force behind fraud so much so that technology was the conduit for 24% of the fraudsters. An IBM sponsored study(3) found that hackers and insiders were behind 47% of the data breaches experienced by participants of the study. The increase in the frequency of cyber-attacks was identified as one of the major contributors to a higher cost of a data breach. Despite the daily barrage of media headlines purporting cyber fraud as a real and imminent threat, the survey found that many companies were complacent and thus unprepared to respond to potential cyber threats.One of the largest banking organizations in the world, SWIFT, or Society for Worldwide Interbank Financial Telecommunication, recently admitted that it had become a victim of cybercrime for the second time in 2016, when fraudsters broke into a messaging system interconnecting 11000 banks and stole cash from one of the banks by maneuvering connections between the bank and SWIFT network. Another such attack was staged on a commercial bank in Bangladesh in which thieves stole $81 million using comparable tactics of credential theft through insider operations.Similarly, the healthcare space is in a worrisome state, having suffered from numerous cyber fraud incidents. This isnt surprising, given the presence of millions of healthcare records online  lucrative targets for data thieves. In 2015, Americas second largest insurer, Anthem(7), reported a data breach which compromised names, income data, social security numbers, medical ID numbers, and other confidential information belonging to 80 million subscribers. Another national healthcare provider, Community Health Systems, reported a highly sophisticated attack that accessed, copied and transferred identities of up to 4.5 million patients and employees between April and June 2014.As most hackers dont have direct use for the data they steal, they sell it to buyers on the deep web, usually selling the same database to multiple clients. This data is then used to file false patient claims with insurers and government health agencies, or purchase prescription drugs from a stolen account. The image below shows a buyer in a deep web forum looking to purchase healthcare and insurance data from hacker.Ref: Cybercrime in Healthcare industry(8)Detection and prevention are two ways to counter fraud. Fraud detection systems recognize attempts to fraud, while fraud prevention systems prevent it from occurring. While it is logical to use both in unison, prevention systems lead to hackers changing their strategies, which affects detection ability. Similarly, the existence of a detection system makes hackers devise novel ways to access confidential data, which weakens the systems own detection abilities. According to Bart Baesens, an expert in fraud analytics, and author of a book on fraud detection, techniques to detect or reduce fraudulent activities include descriptive, predictive and social network analytics.Descriptive analytics tracks behavior that is unusual or deviates from the norm, using techniques such as association rules, clustering, and peer group analysis. Predictive analytics uses historical data sets containing real fraudulent transactions to create fraud detection models that can be subsequently used to detect fraud in real-time data. Over time, the models have to continue to learn as new types of cyber-attacks are discovered. Techniques used to analyze fraudulent data sets include neural networks, random forests, and linear / logistic regression.Companies are increasingly turning to Social Network Analysis (SNA) to combat fraud. SNA helps detect fraud patterns across functions and products lines, beating earlier limitations caused by departments working in silos. Its advanced visual and analytics capabilities enables firms to detect and prevent fraud through online or traditional business channel.Since most companies nowadays deal with huge volumes of sensitive online data and are well aware of cyber fraud, you would expect to see a trend towards adoption of sophisticated anti-fraud technologies. Unfortunately, that is not the case as a study(9) by EY indicates that firms are still using traditional descriptive techniques for forensic data analytics and are slow to leverage more advanced technologies such as SAS, Hadoop and ACL. One reason why firms may find this challenging is due to the lack of employees with the required skills to operate more sophisticated technologies.Ref: EY-Global Forensic Data Analysis Survey(9)Forensic accountants dig deep into numbers to identify patterns: seeking out repeating figures, and discrepancies in totals and the reasons behind them  all this comes under data analytics. One common approach used by accounting firms to detect anomalies is the application of Benfords Law which states that 30.1% of numbers in a data set of financial transactions should begin with the number 1. Numbers starting with 2 will represent a smaller proportion of the data set and so on. This technique can be seen in action when applied to Enrons financial statement for the year 2000 where clear variations from Benfords Laws are observed.Ref: Data Analysis to Catch Cyber Fraud(10)Los Angeles County has been using SAS solutions to identify fraudulent social services claims since 2011. Predictive analytics along with social network analysis (SNA) and analytics were used to analyze social networks and calculate the likelihood of child care fraud in the Child Care Program. The Social Network Analysis component of the solution was highly valued as it allowed investigators to easily locate previously undetectable relationships based on matching telephone numbers and addresses.Memorial Health System is the second-largest public healthcare network in the US and produces massive documentation manually for vendors every year. In collaboration with IBM, the group developed VETTED, a vendor-credentialing system that gives Memorial staff greater visibility into their vendors activities and generates deep insights for better decision-making. The system successfully identified three vendors who were scheming to price fix a proposal. Other benefits included highlighting risky vendors and conflicts of interest.Despite these advances, according to a 2015 Deloitte poll(11), 1/5th of surveyed companies are not leveraging data science and analytics to counter fraud, despite expectations that fraud will increase and become tougher to detect in the next year. A PWC survey(12) revealed that a surprisingly high proportion of companies are ill-equipped to combat cybercrime attacks. Only 37% of firms surveyed had an effective response mechanism, most of them belonging to the heavily regulated financial sector. Nearly 30% had no definitive plan to counter an attack, out of which half believe they are secure enough to not require one. The survey suggests that the lack of attention is due to low interest by board members who lack the technological insight required to perceive cyber crimes as an imminent threat.Companies must start valuing the contributions of data analytics in detecting and countering fraud in a time when technology has made it easier to carry out complicated fraud schemes anonymously. What will it take for complacent organizations to take cyber fraud more seriously? Perhaps a face to face meeting with a ransomware note on a Monday morning telling the business that in order to be able to decrypt and use its own data, bitcoins must be purchased and deposited to an untraceable address.Ref: Nanolocker Ransomware Ananlysis (13)Research shows that not only are all industries affected by cyber-crimes but the damage done by technology fraud will grow to become a $2.1 trillion problem by 2019. With the never-ending headlines of cyber-crimes, you would expect organizations to take measures to protect their assets. That is not the case. Instead of using sophisticated technology tools and techniques to fight fraud, many organizations are still relying on primitive tools for forensic data analytics. A PWC survey found that only 37% of firms surveyed had an effective response mechanism. Unless companies replace complacency with a sense of urgency, we will continue to see a rise in cyber-fraud.References:Salman Khan has an undergraduate degree in Computer Science and is currently an MBA candidate at Ted Rogers School of Management with a focus on Technology and Innovation. He is interested in learning about big data analytics, machine learning and emerging technologies. Salman is currently working on a research project with author and Professor Murtaza Haider on understanding the ROI of big data analytics. You can follow Salmans blog to learn more big data analytics. Salman tweets at @SalmanK84s.",https://www.analyticsvidhya.com/blog/2016/08/industry-insight-fighting-cyber-fraud-with-analytics/
Launch of AV Casino  An Introduction to Probability,Learn everything about Analytics,"Why we designed the AV Casino?|Benefits of learning probability through AV Casino:|Agenda of AV Casino|Looking forward to your feedback:|Finally, thank you Mr. Rabbit!|Share this:|Like this:|Related Articles|Industry Insight  Fighting Cyber Fraud with Analytics|Winners Secrets Decoded from The Smart Recruits|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Over the last few months, you would have seen us experimenting with various formats to aid learning and knowledge exchange among our community members. These formats not only include the blog and the discussion portals, but also the ML competitions, Strategic thinking hackathons, workshops and interactive courses. The idea has been to try various formats and understand what helps our community learn analytics in the best possible manner. While we will continue to try some more interesting formats in the coming months, I wanted to talk specifically about our workshopsAV Casino  Introduction to Probabilityand AV Casino  Introduction to Prbability 2today.Here is an interesting conflict happening in the industry. If you talk to any experienced person in the industry with 5 or more years of experience  they will quote Probability and Statistics as one of the most critical skills for a career in Analytics and Data Science. Here is what Mr. Srikanth Velamakkani, CEO  Fractal Analytics had to say in an exclusive interview we did with them in past:I recommend analytics aspirants to develop a good understanding of probability and see if they enjoy it Mr. Srikanth Velamakkani, CEO, Fractal AnalyticsOn the other hand, I have met a lot of people early in their analytics career, who seem to think otherwise. A large chunk of people coming from software background tend to pick up machine learning as just an extension toa software code. They tend to skip the basic mathematical and statistical concepts and go on to simply write the code for machine learning algorithms without necessarily understanding the fundamentals. While it might help them in the short term, it definitely comes up as a gap in learning over long term.The idea behind creating AV Casino was to bring some fun in learning statistics and probability  so that it helps a larger audience. We could have easily written a few articles to explain people about basics of probability. While this would have given us good reach, it would not have been as effective and as much fun as we would have wanted it to be. Hence, we designed AV CasinoI and AV Casino II! RegisterHere  AV Casino I

Once you have taken the first workshop to go ahead to experience the next level. RegisterHere  AV Casino II
Probability is one of the most important concepts to learn, if you are serious about a career in Analytics. And learning it in a fun interactive manner would make it even better!Here are a few things we kept in mind while creating this workshop:And guess what, we think this is just the start of an interesting series. In future, we plan to do series of workshops as an extension to this workshop, which would include topics like Conditional Probability, Hypothesis Testing, Markov chains etc.Without stealing too much of thunder from the workshop, here is the agenda of the workshop:As you can see, the agenda starts from basics, builds up slowly and ends up with some of the most thought provoking problems in probability.I dont want to sound like I am boasting, but I have not seen such lucid content on this topic elsewhere!As usual, whenever we launch something, we look out to hear from you. That is the only way we learn how our content and tutorials are doing! So, I would request you to spend some time going through AV CasinoI & AV Casino II and provide us with your valuable feedback through the comments below. I would be looking out to hear from you.I would like to thank our intern Prasanna for giving shape to this idea. Prasanna is one of the most curious mind I have seen, when it comes to understanding of human psychology and logic behind it. He is an avid reader and an impeccable fielder on a cricket field. He can be found in deep thoughts on his seat in AV House any day!
You are free to question him about his pseudonym B.Rabbit ",https://www.analyticsvidhya.com/blog/2016/08/launch-of-av-casino-an-introduction-to-probability/
Winners Secrets Decoded from The Smart Recruits,Learn everything about Analytics|Introduction,"The Competition|The Problem Set|The problem  Who are the best agents?|Winners|Rank 3 : Yaasna Dua ( New Delhi, India) andKanishk Agarwal ( Bengaluru, India)  Qwerty Team|Rank 2 : Sudalai Rajkumar (Chennai, India) and Mark Landry (San Fransico, USA)|Rank 1:Rohan Rao ( Bengaluru, India )|End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Launch of AV Casino  An Introduction to Probability|Practicing Machine Learning Techniques in R with MLR Package|
Kunal Jain
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lao Tzu philosophy matches our thoughtsbehind AV hackathons. We believe knowledge can only be useful when it is applied andtested time and again. The motive behind AV hackathons is to challenge your self and realizeyour truepotential.Our recent hackathon, The Smart Recruits was a phenomenal success and we are deeply thankful to our community for theirparticipation. More than 2500 data aspirants made close to 10,000 submissions over a weekend to take the coveted spot.It was a 48 hour data science challenge and we challenged our community with a real life machine learning challenge. The competition was fierce and top data scientists competed against each other.As you would know, the best part of our community is that people share their knowledge / approach with fellow community members. Read on to find the secret recipe from the winners, which would help you to improve yourself further.The competition launched on midnight of 23rd July with more than 2500registration and 48 hours to go and as expected in no time our slack channel was buzzing with discussion. The individuals were pleasantly surprised with the data set and participants were convinced that its not a cake walk. Overnight, the heat rose and hackathon platform was bustling with ideas and execution strategies.The participants were required to help Fintro  a financial distribution company tohelp assessing potential agents which will be profitable for the company. The evaluation metric used was AUC- ROC.Fintro is an offline financial distribution company operating across India from past 10 years. They sell financial products to consumers with the help of agents. The managers at Fintro identify the right talents and recruit these agents. Once the candidate has been hired by the company they undergo a training for next 7 days and have to clear an assessment to become an agent with Fintro. The agents work as freelancers and get paid in commission for each product they sell.However, Fintro is facing a challenge of not being able to generate enough business from its agents.Fintro invests invaluable time and money in recruiting & training the agents. They expect the agent to have selling skills and generate as much business for the company as possible. But some agents dont perform, as expected.Fintro has shared the details of all the agents they have recruited from 2007 to 2009. The data contained demographics ofthe agents hired and the managers who hired them.They want the data scientists to provide insights frompast recruitment data and help them identify / hire potential agents.The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1  Rohan RaoRank 2  Sudalai Rajkumar and Mark LandryRank 3  Yaasna Dua and Kanishk Agarwal (Qwerty Team)Heres the final ranking of all the participants on leaderboard.For learning of the rest of the community, all the three winners shared their approach and code which they used in The Smart Recruits.Yaasna DuaKanishk AgarwalYaasna Dua is an Associate Data Scientist at Info Edge andKanishk Agarwal is an Associate Data Scientist at Sapient Global Networks. They both participated together ( Qwerty Team ) and were the first ones to find the time based insight on a given day.They said:This was our first hackathon on Analytics Vidhya. We followed CRISP-DM and concentrated on feature engineering. We ended up creating the following features-But feature engineering did not give us a good gain. Then we struck gold. Kanishk discovered that the target variable when grouped by application date gave a constant percentage! We incorporated this feature in our model and our score shot up. We tried random forest, xgboost and extra trees. Extra trees gave us the best result. Unfortunately, we did not have enough time to tune the model. Nevertheless, it was a good learning experience and we want to thank Analytics Vidhya for the same.Solution: Link to Code.Sudalai RajkumarMark LandrySudalai Rajkumar is Lead Data Scientist at FreshDesk andMark Landry is a Competitive Data Scientist and Product Manager at H2O.ai. They both worked together in a team to win The Smart Recruits competition. Their deep knowledge about machine learning and analytics have earned them the high rankings on Kaggle.They shared :The solution progression comes in two stages: before the application structure was identified, and after it.Solution : Link to CodeRohan RaoRohan Rao is a Lead Data Scientist at AdWyze. Rohan is an adept in machine learing and also won our last hackathon.(Rank 1 in Seers Accuracy ).He says:The CV-LB movement wasnt in sync. This turned tricky and a public-private LB shakeup was expected with scores being close. I dug back into the data trying to find a feature/pattern which can boost my model. And this proved to be a great decision.While plotting the target variable for a sample set of days, I found a glaring pattern. Within a day, a large proportion of positive samples appeared in the first half, and vice-versa. At first, it seemed too good to be true. I quickly created a feature based on ordering and saw a big jump in CV score, crossing the 0.8 mark. On exploring this pattern in detail, I was unsure whether it is leakage or whether it is just a pattern that applications received initially in the day are more likely to be accepted than ones later. Either ways, the data showed this and I used it in my model.
",https://www.analyticsvidhya.com/blog/2016/08/winners-approach-smart-recruits/
Practicing Machine Learning Techniques in R with MLR Package,Learn everything about Analytics|Introduction|Table of Content|Machine Learning with MLR Package|1. Getting Data|2. Exploring Data|3. Missing Value Imputation|4. Feature Engineering|5. Machine Learning|End Notes,"3. Decision Tree|4. Random Forest|5.SVM|7. Xgboost|What can you do next? Feature Selection ?|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Winners Secrets Decoded from The Smart Recruits|Innovation in Analytics Education: Great Lakes using mentored learning for Online Courses|
Analytics Vidhya Content Team
|48 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In R, we oftenuse multiple packages for doing various machine learning tasks. For example:we impute missing value using one package, then build a model with another and finally evaluate their performance using a third package.The problem is, every package has a set of specific parameters. While working with many packages, we end up spending a lot of time to figure out which parameters are important. Dont you think?To solve this problem, I researched and cameacross aR package named MLR, which is absolutely incredible at performing machine learning tasks.This package includes allof the ML algorithms which we use frequently.In this tutorial,Ive taken up a classification problem and tried improving its accuracy using machine learning.I havent explained the ML algorithms (theoretically) but focus is kept on their implementation. By the end of this article, you are expected to become proficient at implementing several ML algorithms in R. But, only if you practice alongside.Note: This article is meant only for beginners and early starters with Machine Learning in R. Basic statistic knowledge is required.Until now, R didnt have any package / library similar to Scikit-Learn from Python, wherein you could get all thefunctions required to domachine learning. But, since February 2016, R users have got mlr package using which they can perform most of their ML tasks.Lets now understand the basic concept of how this package works. If you get it right here, understanding the whole package would be a mere cakewalk.The entire structure of this package relies on this premise:Create a Task. Make a Learner. Train Them.Creating a task means loading data in the package. Making a learner means choosing an algorithm ( learner) which learns from task (or data). Finally, train them.MLR package has several algorithms in its bouquet. These algorithms have been categorized into regression, classification, clustering, survival, multiclassification and cost sensitive classification. Lets look at some of the available algorithms for classification problems:> listLearners(""classif"")[c(""class"",""package"")]
 class              package
1 classif.avNNet          nnet
2 classif.bartMachine      bartMachine
3 classif.binomial         stats
4 classif.boosting        adabag,rpart
5 classif.cforest         party
6 classif.ctree          party
7 classif.extraTrees       extraTrees
8 classif.knn           class
9 classif.lda           MASS
10 classif.logreg         stats
11 classif.lvq1          class
12 classif.multinom         nnet
13 classif.neuralnet       neuralnet
14 classif.nnet           nnet
15 classif.plsdaCaret       caret
16 classif.probit         stats
17 classif.qda           MASS
18 classif.randomForest     randomForest
19 classif.randomForestSRC   randomForestSRC
20 classif.randomForestSRCSyn  randomForestSRC
21 classif.rpart          rpart
22 classif.xgboost         xgboostAnd, there are many more.Lets start working now!For this tutorial, Ive taken upone of the popular ML problem fromDataHack (one time login will berequired to get data): Download Data.After youve downloaded the data, letsquickly get donewith initial commands such as setting the working directory and loading data.> path <- ""~/Data/Playground/MLR_Package""
> setwd(path)#load libraries and data
> install.packages(""mlr"")
> library(mlr)
> train <- read.csv(""train_loan.csv"", na.strings = c("""","" "",NA))
> test <- read.csv(""test_Y3wMUE5.csv"", na.strings = c("""","" "",NA))Once the data is loaded, you can access it using:>summarizeColumns(train) name       type  na  mean      disp   median   mad  min  max  nlevs
LoanAmount    integer 22  146.4121622 85.5873252  128.0  47.4432  9  700  0
Loan_Amount_Term integer 14  342.0000000 65.1204099  360.0  0.0000  12  480  0
Credit_History  integer 50  0.8421986  0.3648783   1.0   0.0000  0   1   0
Property_Area  factor  0   NA     0.6205212   NA    NA  179  233  3
Loan_Status   factor  0   NA     0.3127036   NA    NA  192  422  2This functions gives a much comprehensive view of the data set as compared to base str() function. Shown above are the last 5 rows of the result. Similarly you can do for test data also:> summarizeColumns(test)From these outputs, we can make the following inferences:Also, you can check the presence of skewness in variables mentioned above using a simple histogram.>hist(train$ApplicantIncome, breaks = 300, main = ""Applicant Income Chart"",xlab = ""ApplicantIncome"")>hist(train$CoapplicantIncome, breaks = 100,main = ""Coapplicant Income Chart"",xlab = ""CoapplicantIncome"")As you can see in charts above, skewness is nothing but concentration of majority of data on one side of the chart. What we see is a right skewed graph. To visualize outliers, we can use a boxplot:> boxplot(train$ApplicantIncome)Similarly, you can create a boxplot for CoapplicantIncome and LoanAmount as well.Lets change the class of Credit_History to factor. Remember, the class factor is always used for categorical variables.> train$Credit_History <- as.factor(train$Credit_History)
> test$Credit_History <- as.factor(test$Credit_History)To check the changes, you can do:> class(train$Credit_History)
[1] ""factor""You can further scrutinize the data using:> summary(train)
> summary(test)We find that the variableDependents has a level 3+ which shall be treated too. Its quite simple to modify the name levels in a factor variable. It can be done as:#rename level of Dependents
> levels(train$Dependents)[4] <- ""3""
> levels(test$Dependents)[4] <- ""3""Not just beginners, even good R analyst struggle with missing value imputation. MLR package offers a nice and convenient way to impute missing value using multiple methods.After we are done with much needed modifications in data, lets impute missing values.In our case, well use basic mean and mode imputation to impute data. You can also use any ML algorithm to impute these values, but that comes at the cost of computation.#impute missing values by mean and mode
> imp <- impute(train, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c(""integer"",""factor""), dummy.type = ""numeric"")
> imp1 <- impute(test, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c(""integer"",""factor""), dummy.type = ""numeric"")This function is convenient because you dont have to specify each variable name to impute. It selects variables on the basis of their classes. It also creates new dummy variables for missing values. Sometimes, these (dummy) features contain a trend which can be capturedusing this function. dummy.classes says for which classes should I create a dummy variable. dummy.type says what should be the class of new dummy variables.$data attribute of imp function contains the imputed data.> imp_train <- imp$data
> imp_test <- imp1$dataNow, we have the complete data. You can check the new variables using:>summarizeColumns(imp_train)
>summarizeColumns(imp_test)Did you notice a disparity among both data sets? No ? See again. The answer is Married.dummy variable exists only in imp_train and not in imp_test. Therefore, well have to remove it before modeling stage.Optional:You might be excited or curious to try out imputing missing valuesusing a ML algorithm.In fact, there are some algorithms which dont require you to impute missing values. You can simply supply them missing data. They take care of missing values on their own. Lets see which algorithms are they:>listLearners(""classif"", check.packages = TRUE, properties = ""missings"")[c(""class"",""package"")]
class              package
1 classif.bartMachine     bartMachine
2 classif.boosting      adabag,rpart
3 classif.cforest        party
4 classif.ctree         party
5 classif.gbm           gbm
6 classif.naiveBayes       e1071
7 classif.randomForestSRC  randomForestSRC
8 classif.rpart         rpartHowever, it is always advisable to treat missing values separately. Lets see how can you treat missing value using rpart:> rpart_imp <- impute(train, target = ""Loan_Status"",
 classes = list(numeric = imputeLearner(makeLearner(""regr.rpart"")),
 factor = imputeLearner(makeLearner(""classif.rpart""))),
 dummy.classes = c(""numeric"",""factor""),
 dummy.type = ""numeric"")Feature Engineering is the most interesting part of predictive modeling. So, feature engineering has two aspects: Feature Transformation and Feature Creation. Well try to work on both the aspects here.At first, lets remove outliers fromvariables like ApplicantIncome, CoapplicantIncome, LoanAmount. There are many techniquesto remove outliers. Here, well cap all the large values in these variables and set them to a threshold value as shown below:#for train data set
> cd <- capLargeValues(imp_train, target = ""Loan_Status"",cols = c(""ApplicantIncome""),threshold = 40000)
> cd <- capLargeValues(cd, target = ""Loan_Status"",cols = c(""CoapplicantIncome""),threshold = 21000)
> cd <- capLargeValues(cd, target = ""Loan_Status"",cols = c(""LoanAmount""),threshold = 520)#rename the train data as cd_train
> cd_train <- cd#add a dummy Loan_Status column in test data
> imp_test$Loan_Status <- sample(0:1,size = 367,replace = T)> cde <- capLargeValues(imp_test, target = ""Loan_Status"",cols = c(""ApplicantIncome""),threshold = 33000)
> cde <- capLargeValues(cde, target = ""Loan_Status"",cols = c(""CoapplicantIncome""),threshold = 16000)
> cde <- capLargeValues(cde, target = ""Loan_Status"",cols = c(""LoanAmount""),threshold = 470)#renaming test data
> cd_test <- cdeIve chosen the threshold value with my discretion, after analyzing the variable distribution. To check the effects, you can do summary(cd_train$ApplicantIncome)and see that themaximum value is capped at 33000.In both data sets, we see that all dummy variables are numeric in nature. Being binary in form, they should be categorical. Lets convert their classes to factor. This time, well use simplefor and if loops.#convert numeric to factor - train
> for (f in names(cd_train[, c(14:20)])) {
 if( class(cd_train[, c(14:20)] [[f]]) == ""numeric""){
 levels <- unique(cd_train[, c(14:20)][[f]])
 cd_train[, c(14:20)][[f]] <- as.factor(factor(cd_train[, c(14:20)][[f]], levels = levels))
 }
}#convert numeric to factor - test
> for (f in names(cd_test[, c(13:18)])) {
 if( class(cd_test[, c(13:18)] [[f]]) == ""numeric""){
 levels <- unique(cd_test[, c(13:18)][[f]])
 cd_test[, c(13:18)][[f]] <- as.factor(factor(cd_test[, c(13:18)][[f]], levels = levels))
 }
}These loops say  for every column name which falls column number 14 to 20of cd_train / cd_test data frame, if the class of those variables in numeric, take out the unique value fromthose columns as levels and convert theminto a factor (categorical) variables.Lets create some new features now.#Total_Income
> cd_train$Total_Income <- cd_train$ApplicantIncome + cd_train$CoapplicantIncome
> cd_test$Total_Income <- cd_test$ApplicantIncome + cd_test$CoapplicantIncome#Income by loan
> cd_train$Income_by_loan <- cd_train$Total_Income/cd_train$LoanAmount
> cd_test$Income_by_loan <- cd_test$Total_Income/cd_test$LoanAmount#change variable class
> cd_train$Loan_Amount_Term <- as.numeric(cd_train$Loan_Amount_Term)
> cd_test$Loan_Amount_Term <- as.numeric(cd_test$Loan_Amount_Term)#Loan amount by term
> cd_train$Loan_amount_by_term <- cd_train$LoanAmount/cd_train$Loan_Amount_Term
> cd_test$Loan_amount_by_term <- cd_test$LoanAmount/cd_test$Loan_Amount_TermWhile creating new features(if they are numeric), we must check their correlation with existing variables as there are high chances often. Lets see if our new variables too happens to be correlated:#splitting the data based on class
> az <- split(names(cd_train), sapply(cd_train, function(x){ class(x)}))#creating a data frame of numeric variables
> xs <- cd_train[az$numeric]#check correlation
> cor(xs)As we see, there exists a very high correlation of Total_Income with ApplicantIncome. It means that the new variable isnt providing any new information. Thus, this variable is not helpful for modeling data.Now we can remove the variable.> cd_train$Total_Income <- NULL
> cd_test$Total_Income <- NULLThere is still enough potential left to create new variables. Before proceeding, I want you to think deeper on this problem and try creating newer variables.After doing so much modifications in data, lets check the data again:> summarizeColumns(cd_train)
> summarizeColumns(cd_test)Until here, weve performed all the important transformation steps except normalizing the skewed variables. That will be done after wecreate the task.As explained in the beginning, for mlr, a task is nothing but the data set on which a learner learns. Since, its a classification problem, well create a classification task. So, the task type solely depends on type of problem at hand.#create a task
> trainTask <- makeClassifTask(data = cd_train,target = ""Loan_Status"")
> testTask <- makeClassifTask(data = cd_test, target = ""Loan_Status"")Lets check trainTask> trainTask
Supervised task: cd_train
Type: classif
Target: Loan_Status
Observations: 614
Features:
numerics factors ordered 
 13     8    0 
Missings: FALSE
Has weights: FALSE
Has blocking: FALSE
Classes: 2
N  Y 
192 422 
Positive class: NAs you can see, it provides a description of cd_train data. However, an evident problem is that it is considering positive class as N, whereas it should be Y. Lets modify it:>trainTask <- makeClassifTask(data = cd_train,target = ""Loan_Status"", positive = ""Y"")For a deeper view, you can check your task data using str(getTaskData(trainTask)).Now, we will normalize the data. For this step, well use normalizeFeatures function from mlr package. By default, this packages normalizes all the numeric features in the data. Thankfully, only 3 variables which we have to normalize are numeric, rest of the variables have classes other than numeric.#normalize the variables
> trainTask <- normalizeFeatures(trainTask,method = ""standardize"")
> testTask <- normalizeFeatures(testTask,method = ""standardize"")Before we start applying algorithms, we should remove the variables which are not required.> trainTask <- dropFeatures(task = trainTask,features = c(""Loan_ID"",""Married.dummy""))MLR package has an in built function which returns the important variables from data. Lets see which variables are important. Later, we can use this knowledge to subset out input predictors for model improvement. While running this code, R might prompt you to install FSelector package, which you should do.#Feature importance
> im_feat <- generateFilterValuesData(trainTask, method = c(""information.gain"",""chi.squared""))
> plotFilterValues(im_feat,n.show = 20)#to launch itsshiny application
>plotFilterValuesGGVIS(im_feat)If you are still wondering about information.gain, let me provide a simple explanation. Information gain is generally used in context with decision trees. Every node split in a decision tree is based oninformation gain. In general, ittries to find out variables which carries the maximum information using which the target class is easier to predict.Lets start modeling now. I wont explain these algorithms in detail but Ive provided links to helpful resources. Well take up simpler algorithms at first and end this tutorial with the complexed ones.With MLR, we can choose & set algorithms using makeLearner. This learner will train on trainTask and try to make predictions on testTask.1. Quadratic Discriminant Analysis (QDA).In general, qda is a parametric algorithm. Parametric means that it makes certain assumptions about data. If the data is actually found to follow the assumptions, such algorithms sometimeoutperform several non-parametric algorithms. Read More.#load qda
> qda.learner <- makeLearner(""classif.qda"", predict.type = ""response"")#train model
> qmodel <- train(qda.learner, trainTask)#predict on test data
> qpredict <- predict(qmodel, testTask)#create submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = qpredict$data$response)
> write.csv(submit, ""submit1.csv"",row.names = F)Upload this submission file and check your leaderboard rank (wouldnt be good). Our accuracy is ~ 71.5%. I understand,this submission might not put you among the top on leaderboard, but theres along way to go. So, lets proceed.2. Logistic RegressionThis time, lets also check cross validation accuracy. Higher CV accuracy determines that our model does not suffer from high variance and generalizes well on unseen data.#logistic regression
 > logistic.learner <- makeLearner(""classif.logreg"",predict.type = ""response"")#cross validation (cv) accuracy
> cv.logistic <- crossval(learner = logistic.learner,task = trainTask,iters = 3,stratify = TRUE,measures = acc,show.info = F)Similarly, you can perform CV for any learner. Isnt it incredibly easy? So, Ive used stratified sampling with 3 fold CV. Id always recommend you to use stratified sampling in classification problems since it maintains the proportion of target class in n folds. We can check CVaccuracy by:#cross validation accuracy
> cv.logistic$aggr
acc.test.mean
0.7947553
This is the average accuracy calculated on 5 folds. To see, respective accuracy each fold, we can do this:>cv.logistic$measures.test
 iter  acc
1 1  0.8439024
2 2  0.7707317
3 3  0.7598039Now, well train the model and check the prediction accuracy on test data.#train model
> fmodel <- train(logistic.learner,trainTask)
> getLearnerModel(fmodel)#predict on test data
> fpmodel <- predict(fmodel, testTask)#create submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = fpmodel$data$response)
> write.csv(submit, ""submit2.csv"",row.names = F)Woah! This algorithm gave us a significant boost in accuracy. Moreover, this is a stable model since our CV score and leaderboard score matches closely. This submission returns accuracy of 79.16%. Good, we are improving now. Lets get ahead to the next algorithm.A decision tree is said to capture non-linear relations better than a logistic regression model. Lets see if we can improve our model further. This time well hyper tune the tree parameters to achieve optimal results. To get the listof parameters for any algorithm, simply write (in this case rpart):>getParamSet(""classif.rpart"")This will return a long list of tunable and non-tunable parameters.Lets build a decision tree now. Make sure you have installed the rpart package before creating the tree learner:#make tree learner
> makeatree <- makeLearner(""classif.rpart"", predict.type = ""response"")#set 3 fold cross validation
> set_cv <- makeResampleDesc(""CV"",iters = 3L)Im doing a 3 fold CV because we have less data. Now, lets set tunable parameters:#Search forhyperparameters
> gs <- makeParamSet(
 makeIntegerParam(""minsplit"",lower = 10, upper = 50),
 makeIntegerParam(""minbucket"", lower = 5, upper = 50),
 makeNumericParam(""cp"", lower = 0.001, upper = 0.2)
)As you can see, Ive set 3 parameters. minsplit represents the minimum number of observation in a node for a split to take place. minbucket says the minimum number of observation I should keep in terminal nodes. cp is the complexity parameter. The lesser it is, the tree will learn more specific relations in the data which might result in overfitting.#do a grid search
> gscontrol <- makeTuneControlGrid()#hypertune the parameters
> stune <- tuneParams(learner = makeatree, resampling = set_cv, task = trainTask, par.set = gs, control = gscontrol, measures = acc)You may go and take a walk until the parameter tuning completes. May be, go catch some pokemons! It took 15 minutes to run at my machine. Ive 8GB intel i5 processor windows machine.#check best parameter
> stune$x
# $minsplit
# [1] 37
# 
# $minbucket
# [1] 15
# 
# $cp
# [1] 0.001Itreturns a list of best parameters. You can check the CV accuracy with:#cross validation result
> stune$y
0.8127132Using setHyperPars function, we can directly set the best parameters as modeling parameters in the algorithm.#using hyperparameters for modeling
> t.tree <- setHyperPars(makeatree, par.vals = stune$x)#train themodel
> t.rpart <- train(t.tree, trainTask)
getLearnerModel(t.rpart)#make predictions
> tpmodel <- predict(t.rpart, testTask)#create a submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = tpmodel$data$response)
> write.csv(submit, ""submit3.csv"",row.names = F)Decision Tree is doing no better than logistic regression. This algorithm has returned the same accuracy of 79.14% as of logistic regression. So, one tree isnt enough. Lets build a forest now.Random Forest is a powerful algorithm known to produce astonishing results. Actually, its prediction derive from an ensemble of trees. It averages the prediction given by each tree and produces a generalized result. From here, most of the steps would be similar to followed above, but this time Ive done random search instead of grid search for parameter tuning, because its faster.> getParamSet(""classif.randomForest"")#create a learner
> rf <- makeLearner(""classif.randomForest"", predict.type = ""response"", par.vals = list(ntree = 200, mtry = 3))
> rf$par.vals <- list(
 importance = TRUE
)#set tunable parameters
#grid search to find hyperparameters
> rf_param <- makeParamSet(
 makeIntegerParam(""ntree"",lower = 50, upper = 500),
 makeIntegerParam(""mtry"", lower = 3, upper = 10),
 makeIntegerParam(""nodesize"", lower = 10, upper = 50)
)#let's do random search for 50 iterations
> rancontrol <- makeTuneControlRandom(maxit = 50L)Though, random search is faster than grid search, but sometimes it turns out to be less efficient. In grid search, the algorithm tunes over every possible combination of parameters provided. In a random search, we specify the number of iterations and it randomly passes over the parameter combinations. In this process, it might miss out some important combination of parameters which could havereturned maximum accuracy, who knows.#set 3 fold cross validation
> set_cv <- makeResampleDesc(""CV"",iters = 3L)#hypertuning
> rf_tune <- tuneParams(learner = rf, resampling = set_cv, task = trainTask, par.set = rf_param, control = rancontrol, measures = acc)Now, we have the final parameters. Lets check the list of parameters and CV accuracy.#cv accuracy
> rf_tune$y
acc.test.mean 
 0.8192571#best parameters
> rf_tune$x
$ntree
[1] 168$mtry
[1] 6$nodesize
[1] 29Lets build the random forest model now and check its accuracy.#using hyperparameters for modeling
> rf.tree <- setHyperPars(rf, par.vals = rf_tune$x)#train a model
> rforest <- train(rf.tree, trainTask)
> getLearnerModel(t.rpart)#make predictions
> rfmodel <- predict(rforest, testTask)#submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = rfmodel$data$response)
> write.csv(submit, ""submit4.csv"",row.names = F)No new story to cheer about. This model too returned an accuracy of 79.14%. So, try using grid search instead of random search, and tell me in comments if your model improved.Support Vector Machines (SVM) is also a supervised learning algorithm used for regression and classification problems. In general, it creates a hyperplane in n dimensional space to classify the data based on target class. Lets step away from tree algorithms for a while and see if this algorithm can bring us some improvement.Since, most of the steps would be similar as performed above, I dont think understanding these codes for you would be a challenge anymore.#load svm
> getParamSet(""classif.ksvm"") #do install kernlab package
> ksvm <- makeLearner(""classif.ksvm"", predict.type = ""response"")#Set parameters
> pssvm <- makeParamSet(
 makeDiscreteParam(""C"", values = 2^c(-8,-4,-2,0)), #cost parameters
 makeDiscreteParam(""sigma"", values = 2^c(-8,-4,0,4)) #RBF Kernel Parameter
)#specify search function
> ctrl <- makeTuneControlGrid()#tune model
> res <- tuneParams(ksvm, task = trainTask, resampling = set_cv, par.set = pssvm, control = ctrl,measures = acc)#CV accuracy
> res$y
acc.test.mean 
 0.8062092#set the model with best params
> t.svm <- setHyperPars(ksvm, par.vals = res$x)#train
> par.svm <- train(ksvm, trainTask)#test
> predict.svm <- predict(par.svm, testTask)#submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = predict.svm$data$response)
> write.csv(submit, ""submit5.csv"",row.names = F)This model returns an accuracy of 77.08%. Not bad, but lesser than our highest score. Dont feel hopeless here. This is core machine learning. ML doesnt work unless it gets some good variables. May be, you should think longer on feature engineering aspect, and create more useful variables. Lets do boosting now.6. GBMNow you are enteringthe territory ofboosting algorithms. GBM performssequential modeling i.e after one round of prediction, it checks for incorrect predictions, assigns them relatively more weight and predict them again until they are predicted correctly.#load GBM
> getParamSet(""classif.gbm"")
> g.gbm <- makeLearner(""classif.gbm"", predict.type = ""response"")#specify tuning method
> rancontrol <- makeTuneControlRandom(maxit = 50L)#3 fold cross validation
> set_cv <- makeResampleDesc(""CV"",iters = 3L)#parameters
> gbm_par<- makeParamSet(
 makeDiscreteParam(""distribution"", values = ""bernoulli""),
 makeIntegerParam(""n.trees"", lower = 100, upper = 1000), #number of trees
 makeIntegerParam(""interaction.depth"", lower = 2, upper = 10), #depth of tree
 makeIntegerParam(""n.minobsinnode"", lower = 10, upper = 80),
 makeNumericParam(""shrinkage"",lower = 0.01, upper = 1)
)n.minobsinnode refers to the minimum number of observations in a tree node. shrinkage is the regulation parameter which dictates how fast / slow the algorithm should move.#tune parameters
> tune_gbm <- tuneParams(learner = g.gbm, task = trainTask,resampling = set_cv,measures = acc,par.set = gbm_par,control = rancontrol)#check CV accuracy
> tune_gbm$y#set parameters
> final_gbm <- setHyperPars(learner = g.gbm, par.vals = tune_gbm$x)#train
> to.gbm <- train(final_gbm, traintask)#test 
> pr.gbm <- predict(to.gbm, testTask)#submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = pr.gbm$data$response)
> write.csv(submit, ""submit6.csv"",row.names = F)The accuracy of this model is 78.47%. GBM performed better than SVM, but couldnt exceed random forests accuracy. Finally, lets test XGboost also.Xgboost is considered to be better than GBM because of its inbuilt properties including first and second order gradient, parallel processing and ability to prune trees. General implementation of xgboost requires you to convert the data into a matrix. With mlr, that is not required.As I said in the beginning, a benefit of using this (MLR) package is that you can follow same set of commands for implementing different algorithms.#load xgboost
> set.seed(1001)
> getParamSet(""classif.xgboost"")#make learner with inital parameters
> xg_set <- makeLearner(""classif.xgboost"", predict.type = ""response"")
> xg_set$par.vals <- list(
 objective = ""binary:logistic"",
 eval_metric = ""error"",
 nrounds = 250
)#define parameters for tuning
> xg_ps <- makeParamSet(
 makeIntegerParam(""nrounds"",lower=200,upper=600),
 makeIntegerParam(""max_depth"",lower=3,upper=20),
 makeNumericParam(""lambda"",lower=0.55,upper=0.60),
 makeNumericParam(""eta"", lower = 0.001, upper = 0.5),
 makeNumericParam(""subsample"", lower = 0.10, upper = 0.80),
 makeNumericParam(""min_child_weight"",lower=1,upper=5),
 makeNumericParam(""colsample_bytree"",lower = 0.2,upper = 0.8)
)#define search function
> rancontrol <- makeTuneControlRandom(maxit = 100L) #do 100 iterations#3 fold cross validation
> set_cv <- makeResampleDesc(""CV"",iters = 3L)#tune parameters
> xg_tune <- tuneParams(learner = xg_set, task = trainTask, resampling = set_cv,measures = acc,par.set = xg_ps, control = rancontrol)#set parameters
> xg_new <- setHyperPars(learner = xg_set, par.vals = xg_tune$x)#train model
> xgmodel <- train(xg_new, trainTask)#test model
> predict.xg <- predict(xgmodel, testTask)#submission file
> submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = predict.xg$data$response)
> write.csv(submit, ""submit7.csv"",row.names = F)Terrible XGBoost. This model returns an accuracy of 68.5%, even lower than qda. What could happen ? Overfitting. So, this model returned CV accuracy of ~ 80% but leaderboard score declined drastically, because the model couldnt predict correctly on unseen data.For improvement, lets do this. Until here, weve used trainTask for model building. Lets use the knowledge of important variables. Take first 6 important variables and train the models on them. You can expect some improvement. To create a task selecting important variables, do this:#selecting top 6 important features
> top_task <-filterFeatures(trainTask, method = ""rf.importance"", abs = 6)So, Ive asked this function to get me top 6 important features using the random forest importance feature. Now, replace top_task with trainTask in models above, and tell me in comments if you got any improvement.Also, try to create more features. The current leaderboard winner is at ~81% accuracy. If you have followed me till here, dont give up now.The motive of this article was to get you started with machine learning techniques. These techniques are commonly used in industry today. Hence, make sure you understand them well. Dont use these algorithms as black box approaches, understand them well. Ive provided link to resources.What happened above, happens a lot in real life. Youd try many algorithms but wouldnt get improvement in accuracy. But, you shouldnt give up. Being a beginner, you should try exploring other ways to achieve accuracy. Remember, no matter how many wrong attempts you make, you just have to be right once.You might have to install packages while loading these models, but thats one time only. If you followed this article completely, you are ready to build models. All you have to do is, learn the theory behind them.Did you find this article helpful ? Did you try the improvement methods I listed above ? Which algorithm gave you the max. accuracy? Share your observations / experience in the comments below.",https://www.analyticsvidhya.com/blog/2016/08/practicing-machine-learning-techniques-in-r-with-mlr-package/
Innovation in Analytics Education: Great Lakes using mentored learning for Online Courses,Learn everything about Analytics,"Background  the online course|Introduction of mentored learning|Benefits of mentored learning|Who are the mentors? / How are the mentors selected?|Early reactions from the mentors and the participants.||Who are the right fit for the course?|End Notes|Share this:|Like this:|Related Articles|Practicing Machine Learning Techniques in R with MLR Package|The Evolution and Core Concepts of Deep Learning & Neural Networks|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics education industry is increasingly becoming a competitive landscape. This is primarily fuelled by the fact that Analytics & Data Science industry is one of the fastest growing sector, has dearth of good talent and is poised to grow significantly in coming years.As the sector is still in its nascent stage and getting hold of good faculty is usually a challenge, most of the training programs have focused on delivering the training in a simple and traditional manner.It was only a matter of time before we started seeing institutes coming out with non-conventional ways to accelerate learning of their participants. And one of the first ones to do this is Great Lakes Institute of Management (GLIM), who combined the online course with mentored learning.As usual, when we heard about it, we decided to dig deeper into what is in store!The Business Analytics Certificate Program offered by GLIM in partnership with Great Learning is an interesting program. The program is designed for a duration of 6 months and is a combination of self-paced and live virtual online classes. It consist of 100 hours of self-learning resources, 40 hours of mentored learning led by analytics mentors and 20 hours of course assessments. The program covers essential tools like SAS, R and advanced excel which are being widely used in the industry today. The lectures are conducted on Learning Management System (LMS) including webinars and Q&A sessions. Students can also access the LMS app through their phones to continue their learning anywhere. The faculty teaching this course have more than 15 years of experience in analytics domain. The best part is that, the course material can be accessed for 3 years after the completion of the course.In the first year of the course it has generated enough buzz from the industry. The candidates enrolled for the course come from varied industry domains.Attrition rate is high in online courses. Debate or points of uncertainty can lead to confusion, even alienation. To tackle this problem GLIM combined their online BACP with mentored learning. A dedicated mentor coupled with online course ensures continuous motivation. The mentors provide complete support and takes away the uncertainty. Each mentor is assigned 4-5 students and interaction happen informally through Whatsapp groups.The mentor  mentee relationship is driven by mentee. The candidate is expected to take the initiative in communication and working with the mentor to establish expectations, goals, and objectives and benefit in achieving their career goals.Mentored learning provides an opportunity for candidates to understand the industry demands from an analytics professional. It helps the candidates carve their skills accordingly and get their dream job.The mentor/mentee partnership is bi-directional, an effective collaboration which is helpful for both individuals to upscale valuable skills and form relationships that can be utilized throughout their career.Lets understand how mentored learning can be beneficial and what can you expect from the course.Mentors are industry experts with in-depth knowledge about the industry who work closely with candidates to make them industry ready and prepare them for problem solving skills. They inspire candidates to think about the range of work options available and to actively support their career exploration, professional development, and networking in their field and/or industry.Industry mentors are selected based on their work and life experience in the private and public sector and willingness to share their time, skills, and knowledge with the BACP candidates.We spoke to two mentors associated with BACP to find out what is their take on this initiative. And what they told us was insightful.Jeetinder Bedi is an analytics expert with 11 years of experience in analytics industry and currently works with an Investment Bank. He is one of the mentors on the program and is very enthusiastic to share his knowledge with the mentees. Here, what he said I like to help analytics newbies to prepare them for challenges faced in the industry. It is a great opportunity for me to advise amateurs and teach them about latest industry trends. My main focus is to inculcate problem solving skills in the students, because execution is easy and one can learn about tools & techniques by reading online but challenge lies with the thought process and approach, & how to follow the right metrics. The mentored learning helps provide support system to the participants and help them identify right approach.Vinol Joy is another mentor associated with the program, with 4  years of experience working with big names like Musigma, Flipkart and is currently heading the Business Intelligence team at Qraved. Heres what he shared with us Its a well-structured program and mentored learning helps students get maximum out of the course knowing what to learn and how to implement what they have learned. Vinol also added Speaking to a mentor and understanding what the industry uses, top trending tools and processes of industry helps the person chalk out an accurate picture of the industry. It also creates a confide in zone where a student can ask a mentor any question from any topic and not have the same of asking a dumb question in front of an audience.Heres what the students had to say:Working Professionals with at least two years of full time post graduate experience are eligible to apply. The course is beneficial for those who want to improve their skills at a convenience of online learning. The professionals with educational background like engineering, mathematics, statistics and economics who want to shift their career in analytics will make most out of the course. The combination of online course with mentored learning is beneficial for all those who are fresher to the analytics industry.The majority of the batch has work experience between 2-5 years and 8-10 years. Candidates with exceptional background / achievements can also be selected for the course.I am personally excited to see this interesting approach to enhance the learning experience from someone like Great Lakes. I could see that the people undergoing the experience enjoyed it a lot and feel that this could add a new dimension to their learning.We wish Great Lakes continued success in their journey to create world class education in Analytics. If you have any queries regarding this program drop in your comments below.To know more about the course visit hereAre you planning to join this course? Do you have queries or confusions about the course ? Drop in your comments below and let us know how can we assist you.",https://www.analyticsvidhya.com/blog/2016/08/innovation-in-analytics-education-great-lakes-using-mentored-learning-for-online-courses/
The Evolution and Core Concepts of Deep Learning & Neural Networks,Learn everything about Analytics|Introduction|Table of Contents|History of Neural Networks|Single Layer Perceptron (SLP)|Multilayer Perceptron (MLP)|Overview of Deep Learning|End Notes,"Initializationof the parameters|Activation function|Backpropagation Algorithm|Gradient descent|Cost Function|Learning rate|Momentum|Softmax|Summary of Multilayer Perceptron (MLP)|Restricted Boltzmann Machine and Deep Belief Networks|Dropout|Techniques to deal with class imbalance|SMOTE: Synthetic Minority Over-sampling Technique|Cost-sensitive learning in neural networks|About The Authors|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Innovation in Analytics Education: Great Lakes using mentored learning for Online Courses|Tutorial  Data Science at Command Line with R & Python (Scikit Learn)|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1.Threshold function|2. Sigmoid Function|3. Hyperbolic Tangent Function|4. Rectified Linear Activation function (ReLU)|5. Maxout function|1. Mini-Batch Gradient Descent|2. Stochastic Gradient Descent|3. Full batch Gradient Descent|1. Mean Squared Error Function|2. Cross-Entropy Function|3. Negative Log-Likelihood Loss (NLL) function,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With the evolution of neural networks, various tasks which were considered unimaginablecan be done conveniently now. Tasks such as image recognition, speech recognition, finding deeper relations in a data set have become much easier. A sincere thanks to the eminent researchers in this field whosediscoveries and findings have helped us leverage the true power of neural networks.If you are truly interested in pursuing machine learning as a subject, a thorough understand of deep learning networks is crucial for you. Most ML algorithms tend of lose accuracy when given a data set with several variables, whereas a deep learning model does wonders in such situations.. Therefore, its important for us to understand how does it work!In this article, Ive explained the core concepts used in deep learning i.e. what sort of backend calculations result in enhanced model accuracy. Along side, Ive also shared various modeling tips and a sneak peek into the history of neural networks.Overview of Deep LearningAbout The AuthorsNeural networks are the building blocks of todays technological breakthrough in the fieldof Deep Learning. A neural network can be seen as simple processing unit that ismassively parallel, capable to store knowledge and apply this knowledge to make predictions.A neural network mimics the brain in a way the network acquires knowledge from its environment through a learning process. Then, intervention connection strengths known as synaptic weights are used to store the acquired knowledge. In the learning process, the synaptic weights of the network are modified in an orderly fashion to attain the desired objective. In 1950, the neuro-psychologist Karl Lashleys thesis was published in which he described the brain as a distributed system.Another reason that the neural network is compared with the human brain is that, they operate like non-linear parallel information-processing systems whichrapidly perform computations such as pattern recognition and perception.As a result, these networks perform very well in areas like speech, audio and image recognition where the inputs / signals are inherently nonlinear.McCulloch and Pitts were pioneers of neural networks who wrote a research article on the model with two inputs and single output in 1943. The following were the features of that model:A neuron would only be activated if:There is a certain threshold level computed by summing up the input values for which the output is either zero or one.In Hebbs 1949 book The Organization of Behaviour, the idea that the connectivity of brain is continuously changing in response to changes in tasks was proposed for the first time. This rule implies that the connection between two neurons is active at the same time. This soon became the source of inspiration for the development of computational models of learning and adaptive systems.Artificial neural networks have the ability to learn fromsupplied data, known as adaptive learning, while the ability of a neural network to create its own organization or representation of information is known as self-organisation.After 15 years, the perceptron developed by Rosenblatt in 1958 emerged as the next model of neuron. Perceptron is the simplest neural network that linearly separates the data into two classes. Later, he randomly interconnected the perceptron and used a trial and error method to change the weights for the learning.After 1969, the research came to a dead end in this area for the next 15 years after the mathematicians Marvin Minsky and Seymour Parpert published a mathematical analysis of the perceptron. Theyfound that the perceptron was not capable of representing many important problems, like the exclusive-or function (XOR). Secondly, there was an issue that the computers did not have enough processing power to effectively handle large neural networks.In 1986, the development of the back-propagation algorithm was reported by Rumelhart, Hinton, and Williams that can solve problems like XOR, beginning a second generation of neural networks. In that same year, the celebrated two-volume book, Parallel Distributed Processing: Explorations in the Microstructures of Cognition, edited by Rumelhart and McClelland, was published. That book has been a major influence in the use of back-propagation, which has emerged as the most popular learning algorithm for the training of multilayer perceptrons.The simplest type of perceptron has a single layer of weights connecting the inputs and output. In this way, it can be considered the simplest kind of feed-forward network. In a feed forward network, the information always moves in one direction; it never goes backwards.Figure 1Figure 1shows a single-layer perceptron for easier conceptual grounding and clarification into multilayer perceptron (explained ahead). Single layer perceptron represents the m weights that are seen as a set of synapses or connecting links between one layer and another layer within the network. This parameter indicates how important each feature is. Below is the adder function of features of the input multiplied by their respective synaptic connection:The bias , acts as an affine transformation to the output of the adder function  giving , the induced local field as:Moving onwards, multi-layer perceptron, also known as feed-forward neural networks, consists of a sequence of layers each fully connected to the next one.A multilayer perceptron (MLP) has one or more hidden layers along with the input and output layers, each layer contains several neurons that interconnect with each other by weight links. The number of neurons in the input layer will be the number of attributes in the dataset, neurons in the output layer will be the number of classes given in the dataset. Figure 2Figure 2 shows a multilayer perceptron where we have three layers at least and each layer is connected to the last one. To make the architecture deep, we need to introduce multiple hidden layers.Initialization of the parameters, weights and biases plays an important role in determining the final model. There is a lot of literature on initialization strategy.A good random initialization strategy can avoid getting stuck at local minima. Local minima problem is when the network gets stuck in the error surface and does not go down while training even whenthere is capacity left for learning.Doingexperiment by using various initialization strategies is out of the scope of this research work.The initialization strategy should be selected according to the activation function used. For tanh the initialization interval should be where is the number of units in the (i-1)-th layer, and is the number of units in the ith layer. Similarly for the sigmoid activation function the initialization interval should be . These initialization strategies ensure that information propagated upwards and backwards in the network at the early stage of training.The activation function defines the output of a neuron in terms of the induced local field v as:where (.)is the activation function. There are various types of activation functions, the following are the commonly used ones:Figure 2Figure 2indicates that either the neuron is fully active or not. However, this function is not differentiable which is quite vital when using the back-propagation algorithm (explained later).The sigmoid function is a logistic function bounded by 0 and 1, as with the threshold function, but this activation function is continuous and differentiable.where  is the slope parameter of the above function. Moreover, it is nonlinear in nature that helps to increase the performance making sure that small changes in the weights and bias causes small changes in the output of the neuron. (v) = tanh (v)This function enables activation functions to range from -1 to +1.ReLUs are the smooth approximation to the the sum of many logistic units and produce sparse activity vectors.Below is the equation of the function:Figure 3In figure 3,  is the smooth approximation to the rectifier).In 2013, Goodfellowfound out that the Maxout network using a new activation function is a natural companion to dropout.Maxout units facilitate optimization by dropout and improve the accuracy of dropouts fast approximate model averaging technique. A single maxout unit can be interpreted as making a piece wise linear approximation to an arbitrary convex function.Maxout networks learn not just the relationship between hidden units, but also the activation function of each hidden unit. Below is the graphical depiction of how this works:Figure 4Figure 4 shows the Maxout network with 5 visible units, 3 hidden units and 2 pieces for each hidden unit.               where is the mean vector of size of the input obtained by accessing the matrix W  at the second coordinate i and third coordinate j . The number of intermediate units (k )is called the number of pieces used by the Maxout nets.The back-propagation algorithm can be used to train feed forward neural networks or multilayer perceptrons. It is a method to minimize the cost function by changing weights and biases in the network. To learn and make better predictions, a number of epochs (training cycles) are executed where the error determined by the cost function is backward propagated by gradient descent until a sufficiently small error is achieved.Lets say in 100-sized mini-batch, 100 training examples are shown to the learning algorithm and weights are updated accordingly. After all mini-batches are presented sequentially,theaverage of accuracy levels and training cost levels are calculated for each epoch.Stochastic gradient descent is used in the real-time on-line processing, where the parameters are updated while presenting only one training example, and so average of accuracy levels and training costs are taken for the entire training dataset at each epoch.In this method all the training examples are shown to the learning algorithm and the weights are updated.There are various cost functions. Below are some examples: where is the predicted outputis the actual outputwhere the f function is the models predicted probability for the input label to be , Ware its parameters, and n is the training-batch size.NLL is the cost function used in all the experiments of the report.where is the value of the output is, is the value of the feature input, is the parameters and D is the training set.Learning rate controls the change in the weight from one iteration to another. As a general rule, smaller learning rates are considered as stable but cause slower learning. On the other hand higher learning rates can be unstable causing oscillations and numerical errors but speed up the learning.Momentum provides inertia to escape local minima; the idea is to simply add a certain fraction of the previous weight update to the current one, helping to avoid becoming stuck in local minima.where  is the momentum.Softmax is a neural transfer function that is generalized form of logistic function implemented in the output layer that turns the vectors into the probabilities that add up and constraint to 1.For classification, a softmax function may be incorporated in the output layer that will give the probability of each occurring class. Activation function is used to compute the predicted output of each neuron in each layer by using inputs, weights and bias.The back propagation method trains the multilayer neural network by modifying its synaptic connection weights between the layers to improve model performance based on the error correction learning function which needs to be continuous and differentiable. The following parameters have been evaluated in the experiments:Before 2006, various failed attempts at training deep supervised feed forward neural networks were made that resulted in over-fitting of the performance on the unseen datai.e.training error reduceswhile validation error increases.A deep network usually means an artificial neural network that has more than one hidden layer.Training the deep hidden layers required more computational power. Having a greater depth seemed to be better because intuitively neurons can make the use of the work done by the neuron in the layer below resulting in distributed representation of the data.Bengio suggests that the neurons in the hidden layers are seen as feature detectors learned by the neuron in the below layer. This result in better generalization that is a subset of neurons learns from data in a specific region of the input space.Moreover deeper architectures can be more efficient as fewer computational units are needed to represent the same functions, achieving greater efficiency. The core idea behind the distributed representation is the sharing of statistical strengths where different components of the architecture are re-used for different purposes.Deep neural architectures are composed of multiple layers utilizing non-linear operations, such as in neural nets with many hidden layers. There are often various factors of variation in the dataset, like aspects of the data separately and often independently may vary.Deep Learning algorithms can capture these factors that explain the statistical variations in the data, and how they interact to generate the kind of data we observe. Lower level abstractions are more directly tied to particular observations; on the other hand higher level ones are more abstract because their connection to perceived data is more remote.The focus of deep architecture learning is to automatically discover such abstractions, from low level features to the higher level concepts. It is desirable for the learning algorithms to enable this discovery without manually defining necessary abstractions.Training samples in the dataset must be at least as numerous as the variations in the test set otherwise the learning algorithm cannot generalize. Deep Learning methods aim to learn feature hierarchies, composing lower level features into higher level abstractions.Deep neural nets with a huge number of parameters are very powerful machine learning systems.However, over-fitting is a serious problem in deep networks. Over-fitting is when the validation error starts to go up while the training error declines. Dropoutis one of the regularization techniques for addressing this problem which is discussed later.Today one of the most important factors for the increased success of Deep Learning techniques is advancement in the computing power. Graphical Processing Units (GPU) and cloud computing are crucial for applying Deep Learning to many problems.Cloud computing allows clustering of computers and on demand processing that helps to reduce the computation time by paralleling the training of the neural network. GPUs, on the other hand, are special purpose chips for high performance mathematical calculations, speeding up the computation of matrices.In 2006-07, three papersrevolutionized the deep learning discipline. The key principles in their work were that each layer can be pre-trained by unsupervised learning, done one layer at a time. Finally supervised training by back-propagation of the error is used to fine-tune all the layers, effectively giving better initialization by unsupervised learning than by random initialization.One of the unsupervised algorithms is Restricted Boltzmann Machines (RBM) that is used to pre-train deep belief network.The RBM is a simplified version of the Boltzmann Machine, inspired by statistical mechanics, which models energy based probabilities for the underlying distributions of the given data sets from which conditional distributions can be derived.Boltzmann Machines are bidirectionally connected networks of stochastic processing units of visible units and hidden units. The raw data corresponds to the visible neurons and samples to observed states and the feature detectors correspond to hidden neurons. In a Boltzmann Machine, visible neurons provide the input to the network and the environment in which it operates. During training visible neurons are clamped (set to a defined value) determined by the training data.Hidden neurons on the other hand operate freely.However, Boltzmann Machine are difficult to train because of its connectivity. An RBM has restricted connectivity to make learning easier; there are no connections between hidden units in a single layer forming a bipartite graph, depicted in figure 2. The advantage of this is that the hidden units are updated independently and in parallel given the visible state.These networks are governed by an energy function that determines the probability of the hidden/visible states. Each possible joint configuration of the visible and hidden units has a Hopfield energy determined by the weights and biases. The energies of the joint configurations are optimized by Gibbs sampling that learns the parameters by minimizing the lowest energy function of the RBM.Figure 5In figure 5, left layer represents the visible layer and right layer represents the hidden layer.In Deep Belief Network (DBN), RBM is trained by input data with important features of the input data captured by stochastic neurons in the hidden layer. In the second layer the activations of the trained features are treated as input data. The learning process in the second RBM layer can be viewed as learning feature of features. Every time a new layer of features is added to the deep belief network, a variational lower bound on the log-probability of the original training data is improved.Figure 6Figure 6 shows RBM converts its data distribution into a posterior distribution over its hidden units.The weights of the RBM are randomly initialized causing the difference in the distribution of p(x) and q(x). During learning, weights are iteratively adjusted to minimize the error between p(x) and q(x). In figure 2 q(x) is the approximate of the original data and p(x) is the original data.The rule for adjusting the synaptic weight from neuron one and another is independent of whether both the neurons are visible or hidden or one of each.The updated parameters by the layers of RBM are used as initialization in DBNs that fine-tunes all the layers by supervised training of backpropagation.For the IDS data of KDD Cup 1999, it is appropriate to use multimodal (Bernoulli-Gaussian) RBM as KDD Cup 1999 consists of mixed data types, specifically continuous and categorical. In multimodal RBM there are two different channel input layers used in the RBM, one is Gaussian input unit used for continuous features and the other one is Bernoulli input unit layer where binary features are used. Using multimodal RBM is beyond the scope of this research work.Recent developments have introduced powerful regularizers to deep networks to reduce over-fitting. In machine learning, regularization is additional information usually introduced in the form of a penalty to penalize complexity of the model that leads to over-fitting.Dropout is a regularization technique for deep neural networks introduced by Hintonwhich consists of preventing co-adaptation of feature detectors by randomly turning o a portion of neurons at every training iteration but using the entire network (with weights scaled down) at test time.Dropout reduces over-tting by being equivalent to training an exponential number of models that share weights. There exists an exponential number of dierent dropout congurations for a given training iteration, so a dierent model is almost certainly trained every time. At test time, the average of all models is used, which acts as a powerful ensemble method.Figure 7In figure 7, dropout randomly drops the connections between the neural network layerFigure 8In figure 8, at training time the connections are dropped with probability, while at the test time weights are scaled to wAveraging many models usually has been the key for many winner of the machine learning competitions. Many different types of model are used and then combined to make predictions at test time.Random forest is a very powerful bagging algorithm which is created by averaging many decision trees giving them different training sample sets with replacement. It is well known that the decision trees are easy to fit to data and fast at test time so averaging different individual trees by giving them different training sets is affordable.However, using the same approach with deep neural networks will prove to be very computationally expensive. It is already costly to train individual deep neural networks and training multiple deep neural networks and then averaging seems to be impractical. Moreover a single network that is efficient at test time is needed rather than having lots of large neural nets.Dropout is an efficient way to average many large neural nets. Each time while training the model hidden units can be omitted with some probability as in figure 8, which is usually = 0.5, when the training example is presented. As a result a mean network model that has all the outgoing weights halved is used at test time as in figure 4. The mean network is equivalent to taking the geometric mean of the probability distributions over labels predicted by all possible networks with a single hidden layer of units and softmax output layer.As per the is mathematical proof of how dropout can be seen as an ensemble method.is the prediction of the ensemble using the geometric mean.is the prediction of a single sub model.d is the binary vector that tells which inputs to include into the softmax classifier.Suppose there are different units. There will be 2^N possible assignments to d, and so; where y is the single and is the vector of the classes index.The sum of the probabilities of the output by a single sub-model is used to normalize,as per the definition of softmaxSo, the predicted probability must be proportional to this. To re-normalize the above expression, it is divided by which means the predicted probability distribution isAnother way to view dropout is that, it is able to prevent co-adaption among the feature detectors. Co-adaption of the feature detector means that if a hidden unit knows which other hidden units are present, it can co-adapt with them on the training data. However, on the test dataset complex co-adaptions are likely to fail to generalize.Dropout can also be used in the input layer at a lower rate, typically 20% probability. The concept here is the same as de-noising auto encoders developed.In this method some of the inputs are omitted. This hurts the training accuracy but improves generalization acting in a similar way as adding noise to the dataset while training.In 2013 a variant of dropout is introduced called Drop connect.Instead of dropping hidden units with certain probability weights are randomly dropped with certain probability. It has been shown that a Drop connect network seemed to perform better than dropout on the MNIST data set.A class imbalance problem arises when one of the classes (minority class) is heavily under-represented in comparison to the other classes (majority class). This problem has real world significance where it is costly to misclassify minority classes such as detecting anomalous activities like fraud or intrusion. There are various techniques to deal with the class imbalance problem such as explained below:One widely used approach to address the class imbalance problem is resampling of the data set. The sampling method involves pre-processing and balances the training data set by adjusting the prior distribution for minority and majority classes. SMOTE is an over-sampling approach in which the minority class is over-sampled by creating synthetic examples rather than by over-sampling with replacement.It has been suggested that oversampling the minority class by replacement does not improve the results significantly rather it tends to over-fit the classification of the minority class. Instead the SMOTE algorithm operates in feature space rather than data space. It creates synthetic samples by oversampling the minority class which tends to generalize better.The idea is inspired by creating extra training data by operating on real data so that there is more data that helps to generalize prediction.In this algorithm firstly nearest neighbours are computed for the minority class. Then, synthetic samples of the minority class are computed in the following manner: a random number of nearest neighbours is chosen and distance between that neighbour and the original minority class data point is taken.This distance is multiplied by a random number between 0 and 1 and adds the result to the feature vector of the original minority class data as an additional sample, thus creating synthetic minority class samples.Cost sensitivity learning seems to be quite an effective way to address the class imbalance for classification problems. Three cost sensitive methods have been described that are specific to neural networks.Incorporate the prior probabilities of the class in the output layer of the neural network while testing unseen examplesAdjusted learning rates based on the costs. Higher learning rates should be assigned to examples with high misclassifications costs making a larger impact on the weight changes for those examplesModifying the mean square error function. As a result, the learning done by backpropagation will minimize misclassification costs. The new error function is:with the cost factor being K[i,j].This new error function results in a new delta rule used in the updating of the weights of the network:where the first equation represents the error function for output neurons and the second equation represents the error function for hidden neurons.If you are not comfortable with math, the mathematical functions explained above might seem intimidating to you. Therefore, you are advised to undergo online courses on algebra and integrals.In this article, we discussed the core concepts of deep learning such as gradient descent, backpropagation algorithm, cost function etc and their respective role in building arobust deep learning model. This article is a result of our research work done on deep learning. Hope you found this article helpful.And to gain expertise in working in neural network try out thedeep learning practice problem Identify the Digits.Have you done any research of a similar topics ? Let us know your suggestions / opinions onbuilding powerful deep learning models.",https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/
Tutorial  Data Science at Command Line with R & Python (Scikit Learn),Learn everything about Analytics|Introduction|Table of Contents|Why Data Science @ the Command Line?|Required Installations|Part 1: Obtain | Scrub | Explore Data|Part 2: Visualize  Using R on the Command Line|Part 3: Modeling with Scikit-Learn on the Command Line|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|The Evolution and Core Concepts of Deep Learning & Neural Networks|Making Predictions on Test Data after Principal Component Analysis in R|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",About Author,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The thought of doing Data Science at Command Line may possibly cause you to wonder, what new devilry is that?As if, it werent enough that, an aspiring data scientist has to keep up with learning, Python / R / Spark / Scala / Julia and what not just to stay abreast, that someones adding one more to that stack?Worry not, its not something new (a part of it is new though) but something thats already existed for a while and with a bit of fresh perspective might just become a quintessential part of your data-science-workflow.My inspiration to write this article derived from abook Data science at the command lineby Jeroen Janssens. Since then, Ive been reading up and applying things I learnt by using it in my workflow.I hope, by demonstrating some quick hands-on examples in this article, I can whet your appetite for more command line goodness and hopefully youll get brand new skill-set in your arsenal. Ive explained every bit of code to help you understand command line faster.Note:For modeling purpose, Ive used linear regression, random forest and gradient boosting algorithm.Given that data science is such a multi-disciplinary field, we have folks from wide variety of professional background and skills;one of the most common ones being some experience using the Unix command line, even if it be for basic actions like cp / grep / cat / wc / head / tail / awk */ sed* / bash*.The command line has existed for a long time and can be found on any Unix based OS. Even, Microsoft Windows has now come out with a bash shell. So, it becomes pretty appealing to have one system to carry out Data Science tasks.Taking the ideas from the UNIX philosophy, of text being the input /output most command produce and that a UNIX command performing one type of action and doing it well; piping these commands together, we can augment these in our data science workflows.UNIX pipes allow us to pass on the output of one command as input for the next one. Have a look at the GNU site to see the various free and robust tools developed by the OS community  quite interesting to have a wide-array of tools at your disposal to carry out tasks. Lastly, command line tasks can be automated and operated in parallel thereby making it quick and efficient.In the age of Big Data and IoT, the appeal for command line tools still persists. Imagine, instead of moving around large amounts of data, you could analyse it, visualise it, and get a feel for it directly from the command line!Does it mean, that we should do away with R / Python or others and do everything on the command line? Perhaps not, rather just use the right tool at the right time in the right context and use the command line to supplement R / Python / other tools.A popular taxonomy of data science describes it as OSEMN (pronounced: Awesome)(Dragon due of GoT Finale still fresh in my mind)In practice, OSEMN tends to be a cyclical process as opposed to a purely linear one. In this article, we shall only be covering the OSEM parts of the OSEMN acronym.Before we can get started, lets ensure we have everything requiredto work on command line:Packages & Commands To carry out the exercises, youll mostly need datamash, csvkit, and skll  which can be installed as follows,Not the most important right away,Windows: Youll need the same packages as in Linux / OSX section. Its just you might have to put more effort to install it.Once youve installed the tools, and need help, you can use the Unix command man command-nameor command-name --help on the command line to get further help or our good friend Google is always there!Welluse the data set from Black Friday Practice Problem. Feel free to work together on your command line as we go through the examples  just make sure, you have installed all the packages and refer to end notes if you get stuck.After youve downloaded the file from link above, unzip it in a folder and rename the extracted csv file as bfTrain.csv.Ensure that you change your present working directory to the folder where youve downloaded & extracted the csv file.Use the cd command to change directoryLets look at some of the most common tasks to be performed with a new data set. Well now check out the dimension of this data set using 2 methods (you can use any):> cat bfTrain.csv | awk -F, ' END {print "" #Rows = "" NR, "" # Columns = "" NF}
Output: #Rows = 550069 # Columns = 12#Method 1> cat bfTrain.csv | datamash t, check 
Output: 550069 lines, 12 fields #Method 2We know the file is comma separated and the first row is a header. In method 1, AWK is used to count the #Rows and #Columns and display it in a nice format.Just for fun, prefix the command, time to both the methods above to compare the 2 methods, like this:> time cat bfTrain.csv | datamash t, checkLets perform basic steps of data exploration now:1. Columns: Now, well check foravailable columnsin the data set:> head n 1 bfTrain.csv | tr , \n | nl
Output
1 User_ID
2 Product_ID
3 Gender
4 Age
5 Occupation
6 City_Category
7 Stay_In_Current_City_Years
8 Marital_Status
9 Product_Category_1
10 Product_Category_2
11 Product_Category_3
12 PurchaseIn the command above, we just took the first row of the file and used tr to transform the commas (remember, its a csv file) to a new-line separator. nl is used to print line numbers.2. Rows:Checkfirst few rows of the data> head n 5 bfTrain.csv | csvlookIn thiscommand, we used the regular head command to select top 5 rows and pipe it to command csvlook to present it in a nice tabular format.In case youre having difficulty viewing the output on your command line, try the following to view a section of the data at a time. Letsdisplay first 5 columns of the file:> head bfTrain.csv | csvcut c 1-5 | csvlookNow, displaying the remaining columns:> head bfTrain.csv | csvcut c 6- | csvlook3. Value counts: Lets dive deeper into the variables. Lets seehow many male / female respondents exist in the data.> cat bfTrain.csv | datamash -t, -s -H -g 3 count 3 | csvlookThe cat command just pips in the file. Then, usingdatamash, we ask to group by the 3rd column (Gender) and do a count on the same. The datamash documentation explains all the various options used.Tip: In case your data has too many columns, it might be a good idea to use csvcut to just select the columns you need and use it for further processing.Exercise: Try answering the following questions:-4. groupBy Stats: Is there a difference in the purchase amount by gender?There are many ways toanswerthis. We can use something everyone wouldbe very familiar withi.e.SQL  yes we can write sql queries directly on the command line to query a csv file.> cat bfTrain.csv | csvsql --query ""select Gender,avg(Purchase), min(Purchase), max(Purchase) from stdin group by 1"" | csvlook5. crosstabs: How are men / women distributed by age group?> cat bfTrain.csv | datamash -t, -s -H crosstab 4,3 | csvlook6. scrub data:Just for arguments sake, assume that we have to remove the + sign from the observations ofage column. We can do it as follows:> cat bfTrain.csv | sed s/+//g > newFile.csvHere, we use SED to remove the + sign in the Age column.7. NA / nulls:How do we find out if the data contains NAs?> csvstat bfTrain.csv --nullsHere, we call upon csvstat and pass the argument --nulls to get a True/False style output per column.Exercise: Pass the following arguments one-at-a-time to csvstat to get other useful info about the datamin, max , median, uniqueHopefullyby now, you might have realizedthat command line can be pretty handy and fast while working on a data set.However, until hereweve just scratched the surface by doing exploratory work so far. To drive the point home, just reflect on the point that we did all of the things above, without actually loading the data (in memory as in R or Python). Pretty cool eh?8.Different Data FormatsPerhaps youve noticed that until now, weve only used data in nicely formatted csv or txt files. But, surely in the age of Big Data and IoT thats not all what command line can handle, can it? Lets look at some available options:1. in2csvIts part of the csvkit command line utility. Its a really awesome command line tool that can handle csv, geojson, json, ndjson, xls, xlsx, and DBase DBF.2. Convert xlsx to csv> curl -L -O https://github.com/onyxfish/csvkit/raw/master/examples/realdata/ne_1033_data.xlsx > ne_1033_data.xlsx> in2csv ne_1033_data.xlsx > ne_1033_data.csvIn here, weve simplyused curl to download the excel file and save it. Then, we used in2csv to convert excel to a csv file of same name.3. Json to csvUtility tools such as in2csv and jq allow us to handle json and other data formats.4. DatabasesAgain csvkit comes to the rescue here, providing a tool sql2csv that allows us to query on several different databases (e.g. Oracle, MySQL, and Postgres etc.).How cool would it be if we could leverage our R knowledge, scripts directly on the command line?As you might have guessed by now, there exists a solution for this too. Jeroen (the author of Data Science at Command Line) has created many handy tools, and one of them is Rio. Its an acronym for R input/ output. Rio is essentially a wrapper function around R that allows us to use R commands on the command line.Lets get cracking!#Summary Stats
> cat bfTrain.csv | Rio e summary(df$Purchase)#calculating correlation between age and purchase amount
> cat bfTrain.csv | csvcut c Age,Purchase | cut d - f 2 | sed s/+//g | Rio f cor | csvlookLets understand how it works. Here, we select the columns Age, Purchase using csvcut.Since, Age is presented as a range of values like 0-17, 26-35 etc., we use cut to select the upper limit of the age range (think of it as using substring to select a specific portion of a text).Next, we remove the + sign from the Age column using sed.Finally, we make a call to Rio to check correlation between the 2 columns and present it nicely using csvlook.Data Visualisation:Lets use another data set now. Well refer to the UCI Irvine website and download Abalone dataset. This data set is about predicting the age of abalone (oyster like sea-creature) from physical measurements. The target column in the data is Rings.#Getting Actual Data
> curl http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data > abalone.csv#Getting Data Description
> curl http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names > abaloneNames.txt#View sample data
> head abalone.csv | csvlook#Create a scatterplot
> cat abalone.csv | csvcut c Rings,Diameter,Sex | Rio ge g+geom_point(aes(Diameter,Rings,color=factor(Sex))) | displayLets understand how the above code works!As usual, we take the input data file, select the columns we need. Then, we callRio and the R-code to create a scatter plot colour coded by Sex (M / F) and save it to a file. The code to create R-graphs is that of ggplot2.If you are unable to see the image , just change the command above as:> cat abalone.csv | csvcut c Rings,Diameter,Sex | Rio ge g+geom_point(aes(Diameter,Rings,color=factor(Sex))) > abaScatter.jpegThis will redirect the output graph as a jpeg file into your current working directory.This is how it would looklike:#Boxplot
> cat abalone.csv | csvcut c Rings,Sex | Rio ge g+geom_boxplot(aes(Sex, Rings)) > abaBoxP.jpegWe used ggplot to produce a boxplot and divert it as a jpeg file in your current working directory.Now we come to the final and perhaps the most interesting part of modeling data on the command line with Scikit-learn.Lets continue with the abalone data set used above.We can extract the column names from the abaloneNames.txt file as:> tail n +89 abaloneNames.txt | awk NR<10{print $1} | tr \n , | sed s/,//9
Output:Sex,Length,Diameter,Height,Whole,Shucked,Viscera,Shell,RingsLets see what we just did.We need to do some groundwork before we can start modelling. The steps are as follows,Letsdo it.1. Add the column names to the data file.Now, well extract the column names from the abaloneNames.txt and store it in a separate file abaloneCN.txt> tail n +89 abaloneNames.txt | awk NR<10{print $1} | tr \n , | sed s/,//9 > abaloneCN.txt> cat abalone.csv | header e cat abaloneCN.txt > abalone_2.csvIn the command above, I used sed to remove the 9th comma.Then, Ipiped in the abalone.csv file and called upon the command header which is allowed us to header to a standard output / file. The abaloneCN.txt is then passed on to it and the output is directed to a new file called abalone_2.csv.2. Add an id column at the start of the features.csv file> mkdir train | cat abalone_2.csv | nl s, -w1 v0 | sed 1s/0,/id, > ./train/features.csvLets see what we did:3. Creating a config fileWell now create a config file which contains the model configurations. You can create it using any text editor; just remember to save it as predict-rings.cfg. Its contents should be:[General]
 experiment_name = Abalone
 task = cross_validate
 [Input]
 train_location = train
 featuresets = [[""features.csv""]]
 learners = [""LinearRegression"",""GradientBoostingRegressor"",""RandomForestRegressor""]
 label_col = Rings
 [Tuning]
 grid_search = false
 feature_scaling = both
 objective = r2
 [Output]
 log = output
 results = output
 predictions = outputLets understand it as well. Here we call the experiment, Abalone. We used 10-fold cross-validation and 3 modeling techniques (Linear Regression, Gradient Boosting, Random Forest).The target column is specified under, label_col as Rings.More Info about creating cfg files for run_experiment can be found here.Now that the groundwork is done, we can call start modeling.> run_experiment l predict-rings.cfgIn this command, we call upon the run_experiment command to run the config file and start modeling. Be patient, this command might take time to run.Once modeling is done, the results can be found under the output directory (created in the directory folder, where the command run_experiment was run from).There are 4 types of files produced per modeling technique (for the 3 techniques we used here, we end up with 12 files). Files being (.log, .results, .results.json, .predictions) and a summary file (with suffix _summary) containing information about each fold. We can get the model summary as follows:> cat ./output/Abalone_summary.tsv | csvsql query select learner_name, pearson FROM stdin WHERE fold = 'average' ORDER BY pearson DESC"" | csvlookLets understand it now:GeneralAbout AWK / SEDseds/old/new where old is what you want to replace and new is what you will replace it with. Essentially old and new are regular expressions.This brings us to the end of this article. Weve covered quite some ground  going through the already familiar data science tasks (getting data directly from the source, summarizing it, understanding it, plotting it, finally modeling it) and using a different way to approach them.I hope youenjoyed working on data at command line (probably for the first time)and feel encouraged to explore it further. It can seem a bit daunting at first to remember the various options, tool names but its only a matter of little practice. In the world of fancy GUIs, a classical command line does still hold a certain allure for me.My personal goal is to get proficient using command line tools more as part of my workflow and combine it with Vowpal Wabbit (read more here), which is an open source fast out-of-core learning system library and program.Sandeep Karkhanis (@sskarkhanis) is passionate about numbers. He hasworked in few industries mainly Insurance, Banking and Telcos. He is currently working as a Data Scientist in London. Hestrives to learn new ways of doing things. His primarygoal is to use his data science skills for social causes and make human livesbetter. You can connect with Sandeep at[emailprotected]",https://www.analyticsvidhya.com/blog/2016/08/tutorial-data-science-command-line-scikit-learn/
Making Predictions on Test Data after Principal Component Analysis in R,Learn everything about Analytics|Introduction,"Read The Complete Article|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Tutorial  Data Science at Command Line with R & Python (Scikit Learn)|How to start applying for Analytics / Data Science Masters in the US Universities?|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This is an update of my previous article on Principal Component Analysis in R & Python. After having received several request on describing the process of model building with principal components, Ive added an exclusive section of model building in R.I came to know that R users often lost their way after doing PCA on train set. They become indecisive of next step i.e. how to use these components to make predictionson test data. I hope this article would help you understand PCA in detail and use it more frequently in your daily modeling process.Dont forget to drop in your suggestions / opinions on this topic. Even if you find any part of PCA difficult to understand, you can ask me below.",https://www.analyticsvidhya.com/blog/2016/07/making-predictions-test-data-principal-component-analysis/
How to start applying for Analytics / Data Science Masters in the US Universities?,Learn everything about Analytics|Introduction|Table of Contents|Phase1. Mental Preparation|Phase 2:Taking Pre-requisite Examinations|Phase 3: Preparing and Submitting Applications|Phase 4:Selecting from Admits & Flying Off to US|End Notes,"Start looking at Programs in US Universities|GRE (Graduate Record Examination)|TOEFL (Test Of English as a Foreign Language)|GRE and TOEFL Preparation|How much does the GRE and TOEFL score matter in the application?|Finalizing Universities|Statement of Purpose (SOP)|A Good Strategy to Follow|Some More Advice|Resume/Curriculum Vitae|Letter of Recommendation (LOR)|Transcripts and Other Formalities|Share this:|Like this:|Related Articles|Making Predictions on Test Data after Principal Component Analysis in R|Senior Software Engineer/ Senior NLP Engineer  Pune / Kolkata|
Aarshay Jain
|45 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Planning a masters program in data science in US? But, not completely aware of the application process? Or afraid of the application process? Dont worry. Im here to help. I will take you through the complete processrequired to apply for analytics / data science programs in the US.Also, Illshare some useful tipsto help you get over this process faster.You must be thinking, how do I know this process ?Because I have gone through the entire process myself. I am going to pursueMS in Data Science at Columbia University in Fall 2016. Though, I also had admits for MS in Analytics at Georgia Institute of Technology and MS in Analytics at Northwestern University. Not heard of their names before? Well, you must check out my previous article.I have divided the complete application process into 4 phases, precisely as follows:A couple of things tonote before we proceed:Statue of Liberty and New York CityThisis one of the most important step. The entire process is quitecumbersome and requires great endurance & patience which is difficult to finduntil you are determined enoughto get into a masters program. You have to give it a good thought. Not only you, but your family also has to agree to this decision (if it matters).In particular, you and your family should be mentally preparedfor the following:Once you are prepared, you should get an idea of what exactly those programs offer. You should not go ahead with any wrongexpectations. You should carefully analyze your profile, look at prospective colleges, the kind of people who get in there etc. In fact, you can also talk to people who have been through the process and ask them where would your profile fit. You can read my previous article.This will give you an idea of what youre getting into. Youll also get to know a benchmark of the GRE score required for your desired institution. All this will help you plan your approach.For instance in my case, I am a graduate from NSIT, Delhi University in Manufacturing and Automation Engg. So, I understood early that its difficult for me to get in right after college. Therefore,I worked in analytics and data science for almost 1.5 yrs before applying again.I had targeted 320 for GRE and got 323 which is decent given my profile. I also had a chat sessions with many seniors, friends already in US and some professional consultants as well. Remember, that some things come only with experience and there isa limit to which Google can help in some situations.There might be other constraints specific to your case.You should consider all of them and proceed only if you are mentally prepared.Geta feel of the programsandthe future prospects ofplacesyou are more likely to fit in. Once you set yourself to this path, there should be no turning back!Now that you have taken a decision, the next step is to take the GRE and TOEFL exam and get a good score. It is important to get this inyour head before you beginyour application because:Letstry to understand each of these exams separately.The GRE exam isdesigned to test your aptitude. It assess your logical reasoning and grasp over english language withquantitativeability and verbal ability sections.The exam contains 6sections  3 verbal and 3 quantitative out of which 1 is not graded, but we dont know which one that is. Also, it is a section-wise adaptiveexam, i.e. the difficulty of the next section depends on the score of the previous. So, if you ace your first quantitative section, the next one will be difficult and vice-versa.The score of a GRE exam is valid for 5 years. In terms of preparation time, it would largely depend on your graspover English. If youre really good, you can do it in 3-4 months. I was average and it took me ~6 months. If youre not even average, expect 6-8 months. Moreover, I believe that there is a level to which you can increase your score. After that the effort required is exponential and you might not get that much returns. So youve to identify that level for you and achieve it. Like I figured I was a 320 guy. So I settled for a 323.TOEFL is designed to test your grasp over English. It has 4 sections  Reading, Listening, Speaking and Writing, each with 30 marks totaling to 120.Each section has questions in a specific format which you can find in any of the preparation books. I will not go into the details here. It is a relatively easier exam as compared to a GRE. Most of the questions will be straight forward and the scoring is also relaxed. There is another exam called IELTS which works more in the UK but some of the American universities also accept its score.In terms of a good score, 100/120 is considered a benchmark. I would say that its fairly easy to score a 100. With some good practise, you can easily cross 110. I attemptedthe exam twice because it is valid for only 2 years. I scored 114 in first attempt and 117 in second and I studied literally for 3 days in my second time. So, you can imagine how easy the exam is.Now comes the important question, how to prepare?For GRE, I would recommend taking some sort of classes, either online or in-person. I personally took in-person classes at Princeton Review for GRE and I got a package with TOEFL exam included for around4K. I really liked Princeton Review and I would definitely not have been able to get a 323 without there help.I think most of us would need external help unless off-course someone is really good in English. Itll really help if you like reading novels and newspaper. You should read some reviews about the center before joining because there are bad ones as well which will waste your time and money.If youve decided that you are serious about it, I recommend you should start by learning new words. You can pick up the famous book Word Power Made Easy, Norman Lewis.This is a really good book to start enriching your vocabulary. It uses etymology, which is a technique of learning words through there roots or origins. It has some interesting exercises to help you seta strong vocabulary base. Along with this you should start looking at some coaching centers in your area and once you feel comfortable with this, you should start the coaching as well. Readingthis book as a primer will be really helpful.Another word of caution  DONT IGNORE MATHS!I know that quantitative is really easy and scoring even 165 is not very difficult. But in order to get a full score, you mustpractice sufficiently. Ive seen many people who underestimatequant and end up scoring in the early 160s. Just take thispart as an exam and it should be fine.For TOEFL preparation, you can either buy a book and do it yourself, take an online course or just a package with your GRE coaching. Its not that difficult and should work out. Focus on getting the GRE done first and then go for TOEFL.This is a crucial question, one that many people wonder and generally over-estimate.In my opinion, GRE would matter 2-10% depending on the university, generally being on the lower side. Trust me many people haveasked me How much GRE score is needed to get into XYZ university? Honestly, GRE score is not the deciding factor. If you consider that different applicants are a part of a race, then GRE score would decide where you start. An approximate analysis can be done as follow (for universities for which 320 is considered a par score):I hope this analogy makes sense. The idea is to tell you that GRE score is important but your profile matters more. So you should decide the time you spend on getting a good GRE score wisely because that time can also be used to improve your profile.This isthe most critical phase of the application process. It involves the following:Lets consider each of these individually.This is the first step after you have scored GRE and TOEFL exams. You should select somewhere between 5-15 universities to apply to. Though, there are not many good universities offering programs in analytics/data science, buy you can still have a big list. My previous articlewill help you to refine your search.The hard part is to decide which universities are safe for you and which ones are hard to get into. Honestly, its a very hard question to get ananswer. Some of the tips you can use:Finalizing universities is important because it will help you plan your applications based on the deadlines. Each university will have some common and some individual aspects. Thus, prioritization as per the deadlineis really important for you to finish everything on time.SOP is the most dynamic and open ended part of your application where you can speak your heart out.Its importance cannot be over-emphasized and thus its pivotalto understand what a SOP actually mean. In my opinion, it is simply a reflection of your life. As the name suggests, it should endorse your fitness for the program.The reader should feel that this program is tailor made for you and you were born to take the course.Its like a marriage, the higher the compatibility between the two (you and program), higher the chances of success. Some important points to take care while writingthe SOP:Now that you know what all to take care, I would like to tell you a fewideas which you can use to structure your SOP and give it a good shape. A wise man oncesaid If Im given 45 minutes to cut a tree, Ill spend the first 30 minutes sharpening my ax! Well, planning and thinking are as important in an SOP as actual writing. Now that we have writing tips, lets get some planning tips. This might be a good strategy:I know Ive already spoken a lot about SOPs but these arethe last set of precautions/things to take care.Moving on to another important part of the application  the Resume! You would have written a resume/CV during your undergrad placement season or even later if you have worked in the industry. Its not very different from that but you have to take care of a few things.LOR are letters from your professor or professional supervisors who have supervised/guided you in academic or professional capacities. Its generally recommended to take LOR from people who know you for atleast 1 year. But if it really makes the profile stronger, you can take it from internship supervisors with 2-3 months of experience. But it should just bein one case not all.Most universities would require 3 LORs. They might/might-not specify the split between academic and professional LOR. Some universities may specify 2 academic and 1 professional LOR while others might not.You should take LORs from people who were with you from experiences more related to analytics/data science.For instance, I am from a manufacturing engineering background so I chose 1 academic and 2 professional. Even if the university says it needs 2 academic, you can talk to them and they are mostly flexible about it.In India, its a sad thing but mostly students have to write a draft of the LOR and the professors make minor adjustments. But you should take care of the following if you or your professor write it:A transcript is nothing but an official document from your universities containing attested copies of your marksheets and final degree, if you have it already. The documents are sealed in an envelop with registrars signature on the seal. Your undergrad college will do the needful once you informthem that you need a transcript.Some universities can have other formalities as well. For example:Now that the applications are made, its time to wait for the results.This can be very daunting at times because some universities can delay the result by a long time. I have even seen people getting admits in June. But most of the replies should come around April-May. If things get further delayed, most of the universities will tell you that you are on a waiting list.Once you have the admits and rejects, its time to take the final decision.Now its time to explore thoseprograms in a lot more detail. Some of the following can help:Universities generally give around 2-3 weeks to make a decision. You might have to take a decision when other results are still awaited. There might even be an offer acceptance fee ($1000-$20000) which is non-refundable if you reject the offer later. But that risk has to be taken and there is little you can do about it.After you select thecollege, there are many more formalities which youve to be prepared for. Some of these are:There are many small things which are requiredbefore you can finally start packing your bags and board your flight.I hope this article helped in you in deciding your career move to US. You can say, it is an honest review of the complete process from my side. I faced enough troubles in the whole process and I expect this article will help you overcome many of them.In this article, I took you through the entire journey from thinking about going for an MS program in US to taking the final decision and completing all formalities.If you have any other thoughts or comments, please feel free to drop me a note below. Ill be happy to discuss further.",https://www.analyticsvidhya.com/blog/2016/07/apply-analytics-data-science-masters-us-universities/
"Senior Software Engineer/ Senior NLP Engineer  Pune / Kolkata|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|How to start applying for Analytics / Data Science Masters in the US Universities?|12 Winning Tips to Clinch Your First Win in Data Science Competitions|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Job Description:|About the Company:|Responsibilities:|Requirements:|Desired Skills and Experience,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Location: Pune / KolkataAbout the employer: ConfidentialResponsible for developing engineering solutions and services, the companys core text analytics engine in collaboration with the team. Implements professional concepts in accordance with company objectives to solve complex problems in the field of NLP and machine learning in creative, innovative and effective ways.Investigates, creates and develops new methods and technologies for project advancement. Regularly exercises technical discretion in design, execution and interpretation of experiments that contribute to project goals. Contributes to project process within scientific discipline through innovative research. Prepares technical reports, summaries, protocols and quantitative analyses.Should be able to customize solutions built on top of Xpresso for different clients from different domains. Should lead the effort in domain modelling, data extraction and preparation. Prepares and installs solutions by determining and designing system specifications, standards, and programming. Improves operations by conducting systems analysis; recommending changes in policies and procedures.Develops NLP and other text analytics components and services across the company. These components will be used for developing sentiment analysis, emotion analysis, content filtering, topic extraction and auto-tagging. You will work on sentiment analysis, information retrieval and other related areas. Develops software solutions by studying information needs; conferring with users; studying systems flow, data usage, and work processes; investigating problem areas; following the software development life cycle.Experience with part of speech taggers, NER, resolving semantic disambiguity, syntactic parsers is very essential. Collaborate closely with fellow architects, engineers and product management to ensure functionality is designed properly so that its stable, scalable, usable, of high quality and meets customer needs. Should collaborate with the testers and ensure quality and quantity of testing, dataset preparation and annotation etc. He will also lead the effort in fixes, improvements and enhancements based on the testing outcomes.Research, experiment, and build prototypes using new technologies such as machine learning and natural language processing applicable to the proposed solutions. Partner with external teams to understand frameworks or tools being developed elsewhere to identify areas where the team can leverage, co-develop, or share technologies.Participate in scientific conferences and make contributions to publications, research journal write up and patents. Take part in companys Intellectual Property submissions, lucid problem conceptualizations, institutional innovations and subsequent bench-marking.We are a well funded, revenue generating company looking to augment our highly talented text analytics R&D team. You will be working on our core XPRESSO text analytics engine that takes in unstructured text from various sources and finds key hidden insights. We are on our way to building the best analytics engine there is. We are able to look at a statement such as This washer uses a lot of power and understand that this represents a negative sentiment in an unsupervised way. If you are able to see why this is hard to do (sentiment lexicons fail on this), well like to talk with you.You should be totally comfortable producing production level code in addition to evaluating the relative merits of various academic papers. We come up with and implement bleeding edge NLP algorithms on a daily basis and you should be excited at the prospect of doing the same.As part of the company, youll have the perfect opportunity to grow and develop your skills in a highly relevant industry vertical (NLP, machine learning).You will also get the opportunity to directly interact with some of the most renowned academics in the field of NLP.Research at here is centered around the domain of data sciences on structured and unstructured data, using big data analytics, social media analytics, natural language processing, machine learning algorithms, statistical modeling, predictive analytics, information retrieval, artificial intelligence, text analytics and processing, knowledge-base buildup, pattern recognition, and semantic parsing.Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Senior Software Engineer / Senior NLP Engineer  Pune / Kolkata and the following details:",https://www.analyticsvidhya.com/blog/2016/07/senior-software-engineer-senior-etl-engineer-pune-kolkata/
12 Winning Tips to Clinch Your First Win in Data Science Competitions,Learn everything about Analytics|Introduction|Tips by 4 DataHack Winners,"Nalin Pasricha, DataHack Rank 1, Mumbai|Sudalai Rajkumar (SRK), DataHack Rank 2, Chennai|Rohan Rao, DataHack Rank 5, Bengaluru|Shantanu Dutta, DataHack Rank 6, Kolkata|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Senior Software Engineer/ Senior NLP Engineer  Pune / Kolkata|20 Challenging Job Interview Puzzles whichevery analyst should solve atleast once|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"So, what are you doing this weekend ? We have an amazing opportunityyou wouldnt want to miss (if you are crazy about machine learning). Brace yourself up, the action is about to get started.Lord of the Machines is round the corner. To help you make last minute strategic plans, we thought of sharing thesewinning tips with you all. These tips will provide you a unique perspective to help you build better ML models.Competition conducted on Datahack are fast paced (live for only 48  72 hours)so that you startto think, act and respond faster to solve business problems. We want you to be fast and efficient. So, if you are determined to walk with ourpace, very soon youll reach another milestone in your life. Stay with us!Follow the below tips shared by four datahack champions.Nalin is an investment banker turned data scientist who currently works as an independent consultant.He has participated in 17 hackathons at DataHack. He wonData Hackathon 3.xand emerged as the 1st Runner Up in Black Friday DataHack.Check out hiscomplete profile here.Heres what Nalin has to say:SRK is a Senior Data Scientist at Tiger Analytics. He is currently positioned at Rank 23 on Kaggle and has been bestowed with the Grandmaster Title on Kaggle. He is an inspiration for most of the aspiring data scientists in our community.He has participated in 17 hackathons on DataHack. Hes a two time winner of Mini DataHack and 2nd runner up for theBlack FridayDatahack. Check out his complete profile here.Heres what SRK has to say:Rohan is theLead Data Scientist at AdWyze. He is currently positioned at Rank 70 on Kaggle and holds the prestigious Kaggle Master title.He has represented and brought laurels to India in World Sudoku championships.He has participated in 11 hackathons on DataHack. Hes the winner of The Seers Accuracy DataHack and stood as 1strunner up in theLast Man Standing. Check out his complete profile here.Heres what Rohan has to say:Shan is a Senior Associate at ACME. He is a self learned data scientist and specializes in BFSI and marketing. So, all this way, if you ever doubted that self learning cant make you a data scientist, you were wrong.Shan has participated in 37 hackathons on DataHack. He won Date Your Dataand Re-date Your Datacompetition. Check out his complete profile here.Heres what Shan has to say.Now, you have the winning potion. Its time to test your winning habit. Use these tips in our upcoming competition Lord of the Machines and shine out as a champion.This competition is going to be intense and mind-boggling, you will have to fight and survive to reach the end. Be a Winner, and challenge all your limits this time. Register Now
To know more about the competition  Visit Here
",https://www.analyticsvidhya.com/blog/2016/07/12-winning-tips-clinch-win-data-science-competitions/
20 Challenging Job Interview Puzzles whichevery analyst should solve atleast once,Learn everything about Analytics|Introduction|20 Job Interview Puzzles|End Notes,"#1 Bag of Coins|#2 Prisoners and hats|#3 Blind games|#4 Sand timers|#5 Chaos in the bus|#6 Mad men in a circle|#7 Lazy people need to be smart|#8 These kids deserve medals|#9 More prisoners and more hats|#10 All men must die|#11 Lumos|#12 4 points in a sphere|#13 Misogynist country|#14 The Red wedding|#15 Life and luck|#16 Weighing balls|#17 Bias and unbias|#18 Chameleons go on a date|#19 The Einstein puzzle|#20 Circles whirl my mind|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|12 Winning Tips to Clinch Your First Win in Data Science Competitions|Practical Guide on Data Preprocessing in Python using Scikit Learn|
Analytics Vidhya Content Team
|36 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In the current scenario, getting your first break in data science can be difficult. Around 30% of analytics companies (specially the top ones) evaluate candidateson their prowess at solving puzzles. It implies thatyou are logical, creative and good with numbers.The ability to bring unique perspective into solving business problems can provide you a huge advantage over other candidates. Such abilities can only be develop with regular practice and consistent efforts.For me, solving puzzles is like mental exercise. I do it everyday and have fairly improved over period of time. To help you achieve this skill, I am sharing some of the trickiest & head scratching questions Ive come across in my journey. These questions have been asked at companies like Goldman Sachs, Amazon, Google, JP Morgan etc.P.S  I want you to try solving them before checking the solution.Doshare your logic & solutions in the comments. Id love to see how uniquely can someone think!Looking to crack your first data science interview? Puzzles are just one part of the entire process. Learn the various levels that go into cracking data science interviews with the Ace Data Science Interviews course, curated by experts who have conduced hundreds of these interviews!You have 10 bags full of coins. In each bag are infinite coins.But one bag is full of forgeries, and you cant remember which one.But you do know that a genuine coins weigh 1 gram, but forgeries weigh 1.1 grams. You have to identify that bag in minimum readings. You are provided with a digital weighing machine.Answer: 1 reading.Take 1 coin from the first bag, 2 coins from the second bag, 3 coins from the third bag and so on. Eventually, well get 55 (1+2+3+9+10) coins. Now,weigh all the 55 coins together. Depending on the resulting weighing machine reading, you can find which bag has the forged coinssuch thatif the reading ends with 0.4 then it is the 4th bag, if it ends with 0.7 then it is the 7th bag and so on.There are 100 prisoners all sentenced to death. One night before the execution, the warden gives them a chance to live if they all work on a strategy together. The execution scenario is as follows On the day of execution, all the prisoners will be made to stand in a straight line such that one prisoner stands just behind another and so on. All prisoners will be wearing a hat either of Blue colour or Red. The prisoners dont know what colour of hat they are wearing. The prisoner who is standing at the last can see all the prisoners in front of him (and what colour of hat they are wearing). A prisoner can see all the hats in front of him. The prisoner who is standing in the front of the line cannot see anything.The executioner will ask each prisoner what colour of hat they are wearing one by one, starting from the last in the line. The prisoner can only speak Red or Blue. He cannot say anything else. If he gets it right, he lives otherwise he is shot instantly. All the prisoners standing in front of him can hear the answers and gunshots.Assuming that the prisoners are intelligent and would stick to the plan, what strategy would the prisoners make over the night to minimize the number of deaths?Answer:The strategy is that the last person will say red if the number of red hats in front of him are odd and blue if the number of red hats in front of him are even. Now, the 99th guy will see the if the red hats in front of him are odd or even. If it is odd then obviously the hat above him is blue, else it is red. From now on, its pretty intuitive.You are in a dark room where a table is kept. There are 50 coins placed on the table, out of which 10 coins are showing tails and 40 coins are showing heads. The task is to divide this set of 50 coins into 2 groups (not necessarily same size) such that both groups have same number of coins showing the tails.Answer:Divide the group into two groups of 40 coins and 10 coins. Flip all coins of thegroup with 10 coins.You have two sand timers, which can show 4 minutes and 7 minutes respectively. Use both the sand timers(at a time or one after other or any other combination) and measure a time of 9 minutes.Answer:So effectively 8 + 1 = 9.There is a bus with 100 labeled seats (labeled from 1 to 100). There are 100 persons standing in a queue. Persons are also labelled from 1 to 100.People board on the bus in sequence from 1 to n. The rule is, if person i boards the bus, he checks if seat i is empty. If it is empty, he sits there, else he randomly picks an empty seat and sit there. Given that 1st person picks seat randomly, find the probability that 100th person sits on his place i.e. 100th seat.Answer:The final answer is the probability that the last person ends in up in his proper seat is exactly 1/2The reasoning goes as follows:First, observe that the fate of the last person is determined the moment either the first or the last seat is selected! This is because the last person will either get the first seat or the last seat. Any other seat will necessarily be taken by the time the last guy gets to choose.Since at each choice step, the first or last is equally probable to be taken, the last person will get either the first or last with equal probability: 1/2.N persons are standing in a circle. They are labelled from 1 to N in clockwise order. Every one of them is holding a gun and can shoot a person on his left. Starting from person 1, they starts shooting in ordere.g for N=100,person 1 shoots person 2,then person 3 shoots person 4,then person 5 shoots person 6..then person 99 shoots person 100,then person 1 shoots person 3,then person 5 shoots person 7and it continues till all are dead except one. Whats the index of that last person ?Answer:Write 100 in binary, which is 1100100 and take the complement which is 11011 and it is 27. Subtract the complement from the original number. So 100  27 = 73.Try it out for 50 people.50 = 110010 in binary.Complement is 1101 = 13.Therefore, 50  13 = 37.For the number in form 2^n, it will be the first person. Lets take an example:64 = 1000000Complement = 111111 = 63.64-63 = 1.You can apply this for any n.Four glasses are placed on the corners of a square Lazy Susan (a square plate which can rotate about its center). Some of the glasses are upright (up) and some upside-down (down).A blindfolded person is seated next to the Lazy Susan and is required to re-arrange the glasses so that they are all up or all down, either arrangement being acceptable (which will be signalled by say ringing of a bell).The glasses may be rearranged in turns with subject to the following rules: Any two glasses may be inspected in one turn and after feeling their orientation the person may reverse the orientation of either, neither or both glasses. After each turn the Lazy Susan is rotated through a random angle.The puzzle is to devise an algorithm which allows the blindfolded person to ensure that all glasses have the same orientation (either up or down) in a finite number of turns.(The algorithm must be deterministic, i.e. non-probabilistic )Answer:This algorithm guarantees that the bell will ring in at most five turns:There are 10 incredibly smart boys at school: A, B, C, D, E, F, G, H, I and Sam. They run into class laughing at 8:58 am, just two minutes before the playtime ends and are stopped by a stern looking teacher: Mr Rabbit.Mr Rabbitsees that A, B, C and D have mud on their faces. He, being a teacher who thinks that his viewpoint is always correct and acts only to enforce rules rather than thinking about the world that should be, lashes out at the poor kids.Silence!, he shouts. Nobody will talk. All of you who have mud on your faces, get out of the class!.The kids look at each other. Each kid could see whether the other kids had mud on their faces, but could not see his own face. Nobody goes out of the class.I said, all of you who have mud on your faces, get out of the class!Still nobody leaves. After trying 5 more times, the bell rings at 9 and Mr Rabbitexasperatedly yells: I can clearly see that at least one of you kids has mud on his face!.The kids grin, knowing that their ordeal will be over soon.Sure enough, after a few more times bawlingof All of you who have mud on your faces, get out of the class!, A, B, C and D walk out of the class.Explain how A, B, C and D knew that they had mud on their faces.What made the kids grin? Everybody knew that there was at least one kid with mud on his face. Supportwith a logicalstatement that a kid did not know before Mr Rabbits exasperated yell at 9, but that the kid knew right after it.Answer:After Mr Rabbits first shout, they understoodthat at least one boy has mud on his face. So, if it was exactly one boy, then the boy would know that he had mud on his face and go out after one shouting.Since nobody went out after one shouting, they understoodthat at least two boys have mud on their faces. If it wereexactly two boys, those boys would know (they would see only one others muddy faceandtheyd understand their face is muddy too) and go out after the next shouting.Since nobody went out after the second shouting,it meansthere are atleast three muddy faces And so on, after the fourth shouting, A, B, C and D would go out of the class.This explanation does leave some questions open. Everybody knew at least three others had mud on their faces, why did they have to wait for Mr.Rabbits shoutat the first place? Why did they have to go through the allfour shoutings after that as well?In multi-agent reasoning, an important concept arises of common knowledge. Everybody knows that there are at least three muddy faces but they cannot act together on that information without knowing that everybody else knows that too. And that everybody knows that everybody knows that and so on. This is what well be analyzing. It requires some imagination, so be prepared.A knows that B, C and D have mud on their faces. A does not know if B knows that three people have mud on their faces. A knows that B knows that two people have mud on their faces. But A cant expect people to act on that information because A does not know if B knows that C knows that there are two people with mud on their faces. If you think this is all uselessly complicated, consider this:A can imagine a world in which he does not have mud on his face. (Call this world A) In As world, A can imagine B having a world where both A and B do not have mud on their faces. (Call this world AB)A can imagine a world where B imagines that C imagines that D imagines that nobody has mud on their faces. (Call this world ABCD).So when Mr Rabbitshouted initially, it could have been that nobody was going out because a world ABCD was possible in which nobody should be going out anyway.So heres a statement that changes after Mr. Rabbits yell. World ABCD is not possible i.e. A cannot imagine a world where B imagines that C imagines that D imagines that nobody has mud on their faces.So now in world ABC, D knows he has mud on his face. And in world ABD, C knows he has mud on his face and so on.There are 7 prisoners sitting in a circle. The warden has caps of 7 different colours (an infinite supply of each colour). The warden places a cap on each prisoners head  he can chose to place any cap on any others head. Each prisoner can see all caps but her/his own. The warden orders everybody to shout out the colour of their respective caps simultaneously. If any one is able to guess her/his colour correctly, he sets themfree. Otherwise, he send themin a dungeon to rot and die. Is it possible to devise a scheme to guarantee that nobody dies?Answer:Assign to each of the 7 colours a unique number from 0-6. Henceforth, we will only be doing modular arithmetic (modulo 7).Assign to each of the 7 prisoners a unique number from 0-6. If the number assigned to prisoner P is N, then P always guesses that the sum of the colours assigned to all prisoners is M (modulo 7). Thus, hecalculates his own colour under this assumption (= (M - sum(colours of the 6 prisoners he can see))%7).There will always be a prisoner who guesses the correct sum (as the sum lies in 0-6), and this prisoner therefore correctly guesses his own colour.If there is a solution, then exactly one prisoner is correct (no more). This is because there are 7^7 scenarios.Each prisoners response is a function of the colours of the other 6, so if you fix their colours and vary his colour, you can see that he will be correct in exactly one-seventh of the cases (=7^6). The sum (across all scenarios) of the number of prisoners who are correct is 7*(7^6)=7^7.If each scenario is to have at least one person right, this implies that each scenario cannot have more than one person who is right.Being right about ones colour is equivalent to being right about the sum of colours of all prisoners (modulo 7). (The colours of the other 6 are known.) So guessing ones colour is the same as guessing the sum. How do we make sure that at least one person guesses the correct sum? By making sure that everybody guesses a different sum.One day, analien comes to Earth. Every day, eachalien does one of four things, each with equal probability to:(i) Kill himself
(ii) Do nothing
(iii) Split himself into two aliens (while killing himself)
(iv) split himself into three aliens (while killing himself)What is the probability that the alien species eventually dies out entirely?Answer:The answer is2  1.Suppose that the probability of aliens eventually dying out is x.Then for n aliens, the probability of eventually dying out is xnbecause we consider everyalien as a separate colony. Now, if we comparealiensbefore and after the firstday, we get:x = (1 /4) * 1 + (1 /4) * x + (1 /4) * x + (1 /4) * xx + x  3x + 1 = 0 (x  1)(x 2 + 2x  1) = 0 We get, x = 1, 1   2, or  1 +  2 We claim that x cannot be 1, which would mean that all alienseventually die out. The number of aliensin the colony is, on average, multiplied by 0+1+2+3 4 = 1.5 every minute, which means in general the aliensdo not die out. (A more rigorous line of reasoning is included below.) Because x is not negative, the only valid solution is x =  2  1.To show that x cannot be 1, we show that it is at most  21.Let xnbe the probability that a colony of one bacteria will die out after at most n minutes. Then, we getthe relation:xn+ 1 = 1/4 (1 + xn+ xn+xn)We claim that xn   2  1 for all n, which we will prove using induction.It is clear that x1= 1 /4   2  1. Now, assume xk   2  1 for some k. We have:xk+1  1/4 (1 + xk + xk + xk )  1/4 ( 1 + ( 2  1) + ( 2  1) + ( 2  1) )=  2  1which completes the proof that xn   2  1 for all n. Now, we note that as n becomes large, xn approaches x. Using formal notation, this is: x = lim (n ) xn   2  1, so x cannot be 1.A photon starts moving in random direction from the center of square of size 3. Lets say it first colloids to the glass wall AB. What is the expected distance traveled by photon before hitting the wall AB again?Answer:Above is a pictorial representation of the photon. We can calculate its distance as shown below:d1= x cosec ()d2= (3 - x) cosec ()d3= (3 - y) cosec ()d4= y cosec ()Total distance = d1+d2+d3+d4
= 6cosec ()We know, varies between/4 and 3/4Therefore, E(distance) =6 E (cosec )
= 6 x (2 /)cosec()d  (limits /4 to 3/4)
= 12/ ln (2 + 1/2 + 1)Consider a unit sphere. 4 points are randomly chosen on it, what is the probability that the centre (of sphere) lies within the tetrahedron (/ polygon) formed by those 4 points?Answer:Let A, B and C be random points on the sphere with Aa, Bb and Cc being diameters.The spherical (minor) triangle abc is common to the hemispheres abc, bca and cab (where the notation abc represents the hemisphere cut off by the great circle through a and b and containing the point c, etc), therefore the probability that a further random point, D, lies on this triangle is:1/2 x 1/2 x 1/2 = 1/8(For centre to lie in the tetrahedron D should lie in the triangle i.e the opposite hemisphere of ABC)In a country in which people only want boys, every family continues to have children until they have a boy. If they have a girl, they have another child. If they have a boy, they stop. What is the proportion of boys to girls in the country?Answer:Following is the required calculation:Expected Number of boys for 1 family = 1*(Probability of 1 boy) + 1*(Probability of 1 girl and a boy) + 1*(Probability of 2 girls and a boy) + For C couples = 1*(C*1/2) + 1*(C*1/2*1/2) + 1*(C*1/2*1/2*1/2) + Expected Number of boys = C/2 + C/4 + C/8 + C/16 + Expected Number of boys = CExpected Number of girls for 1 family = 0*(Probability of 0 girls) + 1*(Probability of 1 girl and a boy) + 2*(Probability of 2 girls and a boy) + For C couples = 0*(C*1/2) + 1*(C*1/2*1/2) + 2*(C*1/2*1/2*1/2) + Expected Number of girls = 0 + C/4 + 2*C/8 + 3*C/16 + Expected Number of girls = CTherefore, the proportion is C/C = 1:1A bad king has a cellar of 1000 bottles of delightful and very expensive wine. A neighbour queen plots to kill the bad king and sends a servant to poison the wine.Fortunately (or say unfortunately) the bad kings guards catch the servant after he couldpoisononly one bottle. Alas, the guards dont know which bottle, but know that the poison is so strong that even if diluted 100,000 times it would still kill the king.Furthermore, it takes one month to have an effect. The bad king decides he will get some of the prisoners in his vast dungeons to drink the wine. Being a clever bad king, he knows that he needs to murder no more than 10 prisoners  believing he can fob off such a low death rate  and will still be able to drink the rest of the wine (999 bottles) at his wedding party in 5 weeks time.Explain what is in mind of the king, how will he be able to do so ? (he has only 10 prisoners in his prisons)Answer:The number the bottles are 1 to 1000. Now, write the number in binary format. We can write it as:bottle 1 = 0000000001 (10 digit binary)bottle 2 = 0000000010...bottle 500 = 0111110100bottle 1000 = 1111101000Now, take 10 prisoners and number them 1 to 10. Let prisoner 1 take a sip from every bottle that has a 1 in its least significant bit. And, this process will continue for every prisoner until the last prisoner is reached. For example:Prisoner = 10 9 8 7 6 5 4 3 2 1Bottle 924 = 1 1 1 0 0 1 1 1 0 0For instance, bottle no. 924 would be sipped by 10,9,8,5,4 and 3. That way if bottle no. 924 was the poisoned one, only those prisoners would die.After four weeks, line the prisoners up in their bit order and read each living prisoner as a 0 bit and each dead prisoner as a 1 bit. The number that you get is the bottle of wine that was poisoned.We know, 1000 is less than 1024 (2^10). Therefore, if there were 1024 or more bottles of wine it would take more than 10 prisoners.You and your friend are caught by gangsters and made to play a game to determine if you should live or die.The game is simple.There is a deck of cards and you both have to choose a card. You can look at each others cards but not at the card you have chosen. You both will survive if both are correct in guessing the card they have chosen. Otherwise both die.What is the probability of you surviving if you and your friend play the game optimally?Answer:We know, A and B have picked a card at random from a deck. A can see Bs card and vice versa. So, A knows (s)he has not picked Bs card, but apart from that, (s)he knows that the card is equally probable to be any of the other 51 cards. So, if A guesses Bs card, they lose. But if A guesses any other card, theres a 1/51 chance that A is right. This also implies that total probability of success <= 1/51.As aim now is to tell any card apart from Bs card that gives B the most information about Bs own card. So they can plan beforehand as follows:Consider the sequence of cards Clubs 1-13, Diamonds 1-13, Hearts 1-13, Spades 1-13. A will tell the card after Bs card in this sequence. (If A says 4 of Hearts, it means B has 3 of Hearts. If A says Ace of Clubs, it means B has King of Spades)With As guess, which is always different from Bs card, B gets to know exactly which card (s)he has and can always guess correctly. So the probability of success is 1/51, which is the maximum achievable.You have 12 balls that all weigh the same except one, which is eitherslightly lighter or slightly heavier. The only tool you have is abalance scalethat can only tell you which side is heavier.Using only three weightings, how can you deduce, without a shadow of adoubt, which is the odd one out, and if it is heavier or lighter than theothers?Answer:First we weigh {1,2,3,4} on the left and {5,6,7,8} on the right. There are three scenarios which can arise from this:If they balance, then we know 9, 10, 11 or 12 is fake. Weigh {8, 9} and {10, 11} (Note: 8 is surely not fake).If they balance, we know 12 is the fake one. Just weigh it with any other ball and figure out if it is lighter or heavier.If {8, 9} is heavier, then either 9 is heavy or 10 is light or 11 is light. Weigh {10} and {11}. If they balance, 9 is fake (heavier). If they dont balance then whichever one is lighter is fake (lighter).If {8, 9} is lighter, then either 9 is light or 10 is heavy or 11 is heavy. Weigh {10} and {11}. If they balance, 9 is fake (lighter). If they dont balance then whichever one is heavier is fake (heavier).If {1,2,3,4} is heavier, we know either one of {1,2,3,4} heavier or one of {5,6,7,8} is lighter but it is guarantees that {9,10,11,12} are not fake. This is where it gets really tricky, watch carefully. Weigh {1,2,5} and {3,6,9} (Note: 9 is surely not fake).If they balance, then either 4 is heavy or 7 is light or 8 is light. Following the last step from the previous case, we weigh {7} and {8}. If they balance, 4 is fake(heavier). If they dont balance then whichever one is lighter is fake (lighter).If {1,2,5} is heavier, then either 1 is heavy or 2 is heavy or 6 is light. Weigh {1} and {2}. If they balance, 6 is fake (lighter). If they dont balance then whichever one is heavier is fake (heavier).If {3,6,9} is heavier, then either 3 is heavy or 5 is light. Weigh {5} and {9}. They wont balance. If {5} is lighter, 5 is fake (lighter). If they balance, 3 is fake (heavier).If {5,6,7,8} is heavier, it is the same situation as if {1,2,3,4} was heavier. Just perform the same steps using 5,6,7 and 8. Unless maybe you are too lazy to try and reprocess the steps, then you continue reading the solution. Weigh {5,6,1} and {7,2,9} (Note: 9 is surely not fake).If they balance, then either 8 is heavy or 3 is light or 4 is light. Following the last step from the previous case, we weigh {3} and {4}. If they balance, 8 is fake(heavier). If they dont balance then whichever one is lighter is fake (lighter).If {5,6,1} is heavier, then either 5 is heavy or 6 is heavy or 2 is light. Weigh {5} and {6}. If they balance, 2 is fake (lighter). If they dont balance then whichever one is heavier is fake (heavier).If {7,2,9} is heavier, then either 7 is heavy or 1 is light. Weigh {1} and {9}. If they balance, 7 is fake (heavier). If they dont balance then 1 is fake (lighter).Robin and Williams are playing a game. An unbiased coin is tossed repeatedly. Robin wins as soon as the sequence of tosses HHT appears. Williams wins as soon as the sequence of tosses HTH appears. The game ends when one of them wins. What are the probabilities of winning for each player?Answer:(Robin) HHT  2/3 (Williams) HTH 1/3Let the probability of Robin winning be p. The probability of Williams winning is (1-p). If the first toss is tails, it is as good as the game has not started, hence the probability of Robin winning is p after the first tail.p = (1/2)*p + .Let the first toss be heads. If the second toss is heads, then Robin definitely wins. Since HH has occurred, and at some point, tails will occur, so HHT will occur. Hence Robin wins with probability 1 for HH.p = (1/2 )*p + (1/2)*((1/2)*1 + .....) Let the second toss be tails. If the third toss is heads, Robin loses as HTH occurs. If the third toss is tails (HTT)  since two tails have occurred in a row, now it is as good as the game has started from the beginning, so the chances of Robin winning are back to p.    T HH HTH HTTp = (1/2)*p + 1/2 ((1/2)*1 + 1/2 ((1/2)*0 + (1/2) * p))p = (1/2)*p + (1/4)*1+ (1/8)*0+ (1/8)*pFinally, solving this equation gives us p= 2/3.On an island live 13 purple, 15 yellow and 17 maroon chameleons. When two chameleons of different colors meet, they both change into the third color. Is there a sequence of pairwise meetings after which all chameleons have the same color?Answer:Let <p, y, m> denote a population of p purple, y yellow and m maroon chameleons. Canpopulation <13, 15, 17> be transformed into <45, 0, 0> or <0, 45, 0> or <0, 0, 45> through a series of pairwise meetings?We can define function:X(p, y, m) = (0p + 1y + 2m) mod 3An interesting property of X is that its value does not change after any pairwise meeting becauseX(p, y, m) = X(p-1, y-1, m+2) = X(p-1, y+2, m-1) = X(p+2, y-1, m-1)Now X(13, 15, 17) equals 1. However,X(45, 0, 0) = X(0, 45, 0) = X(0, 0, 45) = 0**This means that there is no sequence of pairwise meetings after which all chameleons will have identical colour.Answer the question using the given information and hints.The Question: Who owns the fish ?Hints:FYI  This question is famously knownas Einstein Puzzle.Answer:Read MoreConsider natural numbers form 1 to 21 that is 1,2,3,4.21. Find number of distinct circular permutation of these numbers such that any number must have different neighbours.Answer  Still thinking.! Post solution if you get.I hope these questions would have daredyou enough to get your brain rolling. I understand these question might appear as challenging to a lot of you, but believe me, they arent difficult. If you find any trouble in understanding any solution or question, feel free to drop me a message below.Ive obtained the solution from various sources. Also, Ive answered some on my own. Try tounderstand these questions well. Once you do it, youd find it easier to solve similar questions during interviews.Did you like solving these puzzles? Do let me know your experience, suggestions & solutions in the comments below. I am excited to see if you can think any different!I also encourage you to check out the Ace Data Science Interviews course to learn how to solve such puzzles and many other important aspects of the interview rounds!",https://www.analyticsvidhya.com/blog/2016/07/20-challenging-job-interview-puzzles-which-every-analyst-solve-atleast/
Practical Guide on Data Preprocessing in Python using Scikit Learn,Learn everything about Analytics|Introduction|Available Data set|Feature Scaling|Feature Standardization|Label Encoding|One-Hot Encoding|End Notes,"Exercise 1|Exercise 2|Exercise 3|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|20 Challenging Job Interview Puzzles whichevery analyst should solve atleast once|Going Deeper into Regression Analysis with Assumptions, Plots & Solutions|
syed danish
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article primarily focuses on data pre-processing techniques in python. Learning algorithms have affinity towards certain data types onwhich they perform incredibly well. They are also known togive recklesspredictions with unscaled or unstandardized features.Algorithm like XGBoost, specifically requires dummy encoded data while algorithm like decision tree doesnt seem to care at all (sometimes)!In simple words, pre-processing refers to thetransformations applied to your data before feeding it to the algorithm. In python, scikit-learn library has a pre-built functionality under sklearn.preprocessing. There are many more options for pre-processing which well explore.After finishing this article, you will be equipped with the basic techniques of data pre-processing and their in-depth understanding. For your convenience, Ive attached some resources for in-depth learning of machine learning algorithms and designed few exercises to get a good grip of the concepts.For this article, I have used a subset of the Loan Prediction(missing value observations are dropped) data set from You can download the final training and testing data set from here: Download DataNote :Testing data that you are provided is the subset of the training data from Loan Prediction problem.Now, lets get started by importing important packages and the data set.Lets take a closer look at our data set.Feature scaling is the method tolimit the range of variables so that they can be compared on common grounds.It is performed on continuous variables.Lets plot the distribution of all the continuous variables in the data set.After understanding theseplots, we inferthat ApplicantIncomeand CoapplicantIncomeare in similar range (0-50000$) where as LoanAmountis in thousands and it ranges from 0 to 600$. The story for Loan_Amount_Term is completely different from other variables because its unit is months as opposed to other variables where the unit is dollars.Ifwe try to apply distance based methods such as kNN on these features, feature with the largest range will dominate the outcome results and well obtain less accurate predictions. We can overcome this trouble using feature scaling. Lets do it practically.Resources :Check outthis article on kNNfor better understanding.Lets try out kNN on our data set to see how well it will perform. Here is a live coding window to get you started. You can run the codes and get the output in this window itself:Wow !! we got an accuracy of 63% just by guessing, What is the meaning of this, getting better accuracy than our prediction model ?This might be happening because of some insignificant variable with larger range will be dominating the objective function. We can remove this problem by scaling down all the features to a same range. sklearn provides a toolMinMaxScaler that will scale down all the features between 0 and 1. Mathematical formula for MinMaxScaleris.Lets try this tool on our problem.Now, that we are done with scaling, lets apply kNN on our scaled data and check its accuracy.Great !! Our accuracy hasincreased from 61% to 75%. This means that some of the features with larger range weredominating the prediction outcome in the domain of distance based methods(kNN).It should be kept in mind while performing distance based methods we must attempt toscale the data, so that the feature with lesser significance might not end up dominating the objective function due to its larger range. In addition,features having different unit should also be scaled thus providing each feature equal initial weightage and at the end we will have a better prediction model.Try to do the same exercise with a logistic regression model(parameters : penalty=l2,C=0.01) and provide your accuracy before and after scaling in the comment section.Before jumping to this section I suggest you to complete Exercise 1.In the previous section, we worked on the Loan_Prediction data set and fitted a kNN learner on the data set. Afterscaling down the data, we have got an accuracy of 75% which is very considerably good. I tried the same exercise on Logistic Regression and I got the following result :Before Scaling :61%After Scaling :63%The accuracy we got after scaling is close to the prediction which we made by guessing, which is not a very impressive achievement. So, what is happening here? Why hasnt the accuracy increased by a satisfactory amount as it increased in kNN?Resources :Go through this article on Logistic Regression for better understanding.Here is the answer:In logistic regression, each feature is assigned a weight or coefficient (Wi). If there is a feature with relatively large range and it is insignificant in the objective function then logistic regression will itself assign a very low value to its co-efficient, thus neutralizing the dominant effect of that particular feature, whereas distance based method such as kNN does not have this inbuilt strategy, thus it requires scaling.Arent we forgetting something ? Our logistic model is still predicting with an accuracy almost closer to a guess.Now, Ill beintroducing a new concept here called standardization. Many machine learning algorithms in sklearn requires standardized data which means having zero mean and unit variance.Standardization (or Z-score normalization) is the process where the features arerescaled so that theyll have the properties of a standard normal distribution with=0and =1,where  is the mean (average) and  is the standard deviation from the mean. Standard scores (also called z scores) of the samples are calculated as follows :Elements such as l1 ,l2 regularizer in linear models (logistic comes under this category) and RBF kernel in SVM in objective function of learners assumes that all the features are centered around zero and have variance in the same order.Features having larger order of variance woulddominate on the objective function as it happened in the previous section with the feature having large range. As we saw in the Exercise 1 that without any preprocessing on the data the accuracy was 61%, lets standardize our data apply logistic regression on that. Sklearn provides scaleto standardize the data.We again reached to our maximum score that was attained usingkNN after scaling. Thismeans standardizing the data when using a estimator having l1 or l2 regularization helps us to increase the accuracy of the prediction model. Other learners like kNN with euclidean distance measure, k-means, SVM, perceptron, neural networks, linear discriminant analysis, principal component analysis mayperform better with standardized data.Though,I suggest you to understand your data and what kind of algorithm you are going to apply on it; over the time you will be able to judge weather to standardize your data or not.Note :Choosing between scaling and standardizing is a confusing choice, you have to dive deeper in your data and learner that you are going to use to reach the decision. For starters, you can try both the methods and check cross validation score for making a choice.Resources :Go through this article on cross validation for better understanding.Try to do the same exercise with SVM model and provide your accuracy before and after standardization in the comment section.Resources :Go through this article onsupport vector machinesfor better understanding.In previous sections, we didthe pre-processing for continuous numeric features. But, our data set has other features too such as Gender, Married, Dependents, Self_EmployedandEducation. All these categorical features have string values. For example,Genderhas two levels either Male or Female. Lets feed the features in our logistic regression model.We got an error saying that it cannot convert string to float. So, whats actually happening here is learners like logistic regression, distance based methods such as kNN, support vector machines, tree based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners.Sklearn provides a very efficient tool for encoding the levels of a categorical features into numeric values.LabelEncoderencode labels with value between 0 and n_classes-1.Lets encode all the categorical features.All our categorical features are encoded. You can look at your updated data set usingX_train.head(). We are going to take a look at Genderfrequency distribution before and after the encoding.Now that we are done with label encoding, lets now run a logistic regression model on the data set with both categorical and continuous features.Its working now. But, the accuracy is still the same as we gotwith logistic regression after standardization from numeric features. Thismeans categorical features we added are not very significant in our objective function.Try out decision tree classifier with all the features as independent variables and comment your accuracy.Resources : Go through this article on decision trees for better understanding.One-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active.Most of the ML algorithms either learn a single weight for each feature or it computes distance between the samples. Algorithms like linear models (such as logistic regression) belongs to the first category.Lets take a look at an example from loan_prediction data set. Feature Dependents have 4 possible values 0,1,2 and 3+ which are then encoded without loss of generality to 0,1,2 and 3.We, then have a weight W assigned for this feature in a linear classifier,which will make a decision based on the constraints W*Dependents + K > 0 or eqivalently W*Dependents < K.Let f(w)= W*DependentsPossible values that can be attained by the equation are 0, W, 2W and 3W. A problem with this equation is that the weight W cannot make decision based on four choices. It can reach to a decision in following ways:Here we can see that we are loosing many different possible decisions such as the case where 0 and 2W should be given same label and 3W and W are odd one out.This problem can be solved by One-Hot-Encoding as it effectively changes the dimensionality of the feature Dependents from one to four, thus every value in the feature Dependents will have their own weights. Updated equation for the decison would be f'(w) < K.where, f'(w) = W1*D_0 + W2*D_1 + W3*D_2 + W4*D_3
All four new variable has boolean values (0 or 1).The same thing happenswith distance based methods such as kNN. Without encoding, distance between 0 and 1 values of Dependents is 1 whereas distance between 0 and 3+ will be 3, which is not desirable as both the distances should be similar. After encoding, the values will be new features (sequence of columns is 0,1,2,3+) : [1,0,0,0] and [0,0,0,1] (initially we were finding distance between 0 and 3+), now the distance would be 2.For tree based methods, same situation (more than two values in a feature) might effect the outcome to extent but if methods like random forests are deep enough, it can handle the categorical variables without one-hot encoding.Now, lets take look at the implementation of one-hot encoding with various algorithms.Lets create a logistic regression model for classification without one-hot encoding.Now we are going to encode the data.Now, lets apply logistic regression model on one-hot encoded data.Here, again we got the maximum accuracy as 0.75 that we have gotten so far. In this case,logistic regression regularization(C) parameter 1 where as earlier we used C=0.01.The aim of this article is to familiarize you with the basic data pre-processing techniques and have a deeper understanding of the situations of where to apply those techniques.These methods work because of the underlying assumptions of the algorithms. This is by no means an exhaustive list of the methods. Idencourageyouto experiment with these methods since they can be heavily modified according to the problem at hand.I plan to provide more advance techniques of data pre-processing such as pipelineand noise reduction in my next post, so stay tuned to dive deeper into the data pre-processing.Did you like reading this article ? Do you follow a different approach / package / library to perform these talks. Id love to interact with you in comments.",https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/
"Going Deeper into Regression Analysis with Assumptions, Plots & Solutions",Learn everything about Analytics|Introduction|Assumptions in Regression|What if these assumptions getviolated ?|Interpretation of Regression Plots|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Practical Guide on Data Preprocessing in Python using Scikit Learn|Manager  Underwriting  Gurgaon (4  5 Years of Experience)|
Analytics Vidhya Content Team
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"All models are wrong, but some are useful  George BoxRegression analysis marksthe first step inpredictive modeling. No doubt, its fairlyeasy to implement. Neither its syntax nor its parameters create any kind of confusion.But, merely running just one line of code, doesnt solve the purpose. Neither just looking at R or MSE values. Regression tells much more than that!In R, regression analysis return 4 plots using plot(model_name)function. Each of the plot provides significant information or rather an interesting story about the data. Sadly, many of the beginners either fail to decipher the information or dont care about what these plots say. Once you understand these plots, youd be able to bring significant improvement in your regression model.Formodel improvement, you also need to understand regression assumptions and ways to fix them when they get violated.In this article, Ive explained the important regression assumptions and plots (with fixes and solutions) to help you understand the regression concept in further detail. As said above, with this knowledge you can bring drastic improvements in your models.Note:To understand these plots, you must know basics of regression analysis. If you are completely new to it, you can start here. Then, proceed with this article.Regression is a parametric approach. Parametric means it makes assumptions about data for the purpose of analysis. Due to its parametric side, regression is restrictivein nature. Itfails to deliver good results withdata sets which doesnt fulfillits assumptions. Therefore, for a successful regression analysis, its essential to validate these assumptions.So, how would you check (validate) if a data set follows all regression assumptions? You check it using the regression plots (explained below) along with some statistical test.Lets look at the important assumptions in regression analysis:Lets dive into specific assumptions and learn about their outcomes (if violated):1. Linear and Additive: If you fit a linear model to a non-linear, non-additive data set, the regression algorithm would failto capture the trend mathematically, thus resulting in an inefficient model. Also, this will result in erroneous predictions on an unseen data set.How to check: Look for residual vs fitted value plots (explained below). Also,you can includepolynomial terms (X, X, X) in your model to capture the non-linear effect.2. Autocorrelation: The presence of correlation in error terms drastically reduces models accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error.If this happens, itcausesconfidence intervals and prediction intervals to be narrower. Narrower confidence interval meansthat a 95% confidence interval would have lesser probability than 0.95 that it would contain the actual value of coefficients. Lets understand narrow prediction intervals with an example:For example, the least square coefficient of X is 15.02 and its standard error is 2.08 (without autocorrelation). But in presence of autocorrelation, the standard error reduces to 1.20. As a result, the prediction interval narrows down to (13.82, 16.22) from (12.94, 17.10).Also, lower standard errors would cause the associatedp-values to be lower than actual. This will make us incorrectly conclude a parameter to be statistically significant.How to check: Look for Durbin  Watson (DW) statistic. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0< DW < 2 implies positive autocorrelation while 2 < DW < 4indicates negative autocorrelation.Also, you can seeresidual vs time plot and look for the seasonal or correlatedpattern in residual values.3. Multicollinearity: This phenomenon exists when the independent variables are found to be moderately or highly correlated. In a model with correlated variables, it becomes a tough task to figure out the true relationship of a predictors with response variable. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable.Another point, with presence of correlated predictors, the standard errors tend to increase. And, with large standard errors, the confidence interval becomes wider leading to less precise estimates of slope parameters.Also, when predictors are correlated, the estimated regression coefficient of a correlated variable depends on which other predictors are available in the model.If this happens, youll end up with an incorrect conclusion that a variable strongly / weakly affects target variable. Since,even if you drop one correlated variable from themodel, itsestimated regression coefficients would change. Thats not good!How to check: You can use scatter plot to visualize correlation effect among variables. Also, you can also use VIF factor. VIF value <=4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. Above all, a correlation table should also solve the purpose.4. Heteroskedasticity:The presenceof non-constant variance in the error terms results inheteroskedasticity. Generally, non-constant variance arises in presence of outliers or extreme leverage values. Look like, these values get too much weight, thereby disproportionately influences the models performance.When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.How to check: You can look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern (shown in next section). Also, you can use Breusch-Pagan / Cook  Weisberg test or White general test to detect this phenomenon.5. Normal Distribution of error terms:If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficientsbased on minimization of least squares.Presence of non  normal distribution suggests that there are a few unusual data points which must be studied closely to make a better model.How to check:You can look at QQ plot (shown below). You can also perform statistical tests of normality such as Kolmogorov-Smirnov test, Shapiro-Wilk test.Until here, weve learnt about the important regression assumptions and the methods to undertake, if those assumptions get violated.But thats not the end. Now, you shouldknow the solutions also to tackle the violation of these assumptions. In this section, Ive explained the 4 regression plots along with the methods to overcome limitations on assumptions.1. Residual vs Fitted ValuesThis scatter plot shows the distribution of residuals (errors) vs fitted values (predicted values). It is one of the most important plot which everyone must learn. It reveals various useful insights including outliers. The outliers in this plot arelabeled by their observation number which make themeasy to detect.There are two major things which you should learn:Solution:To overcome the issue of non-linearity, you can do a non linear transformation of predictors such as log (X),X or X transform the dependent variable. To overcome heteroskedasticity, a possible way is to transform the response variable such as log(Y) orY. Also, you can use weighted least square method to tackle heteroskedasticity.2. Normal Q-Q PlotThis q-q or quantile-quantile is a scatter plot which helps us validatethe assumptionof normal distribution in a data set. Using this plot we can infer if the data comes from a normal distribution. If yes, the plot would show fairly straight line. Absence of normality in the errors can be seen with deviation in the straight line.If you are wondering what is a quantile, heres a simple definition: Think of quantiles as points in your data below which a certain proportion of data falls. Quantile is often referred to as percentiles. For example: when we say the value of 50th percentile is 120, it means half of the data lies below 120.Solution: If the errorsare not normally distributed, non  linear transformation of the variables (response or predictors)canbring improvement in the model.3. Scale Location PlotThis plot isalso used to detect homoskedasticity (assumption of equal variance). It shows how the residual are spread along the range of predictors. Its similar to residual vs fitted value plot except it uses standardized residual values. Ideally, there should be no discernible pattern in the plot. This would imply that errors are normally distributed. But, in case, if the plot shows anydiscernible pattern (probably a funnel shape), it would implynon-normal distribution of errors.Solution:Follow the solution for heteroskedasticity given in plot 1.4. Residuals vs Leverage PlotIt is also known as Cooks Distance plot. Cooks distance attempts to identify the points which have more influence than other points. Such influential points tends to have a sizable impact of the regression line. In other words, adding or removing such points from the model can completely change the model statistics.But, can these influential observations be treated as outliers? This question can only be answered after looking at the data. Therefore, in this plot, the large values marked by cooks distance might require further investigation.Solution:For influential observations which are nothing but outliers, if not many, you can remove those rows. Alternatively, you can scale down the outlier observation with maximum value in data or else treat those values as missing values.Case Study: How I improved myregression model using log transformationYou can leverage the true power of regression analysis by applying the solutions described above.Implementing these fixes in R is fairly easy. If you want to know about any specific fix in R, you can drop a comment, Id be happy to help you with answers.My motive of this article was to help you gain the underlying knowledge and insights of regression assumptions and plots. This way, you would have more control on your analysis and would be able to modify the analysisas per your requirement.Did you find this article useful ? Have you used these fixes in improving models performance? Share your experience / suggestions in the comments.",https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/
"Manager  Underwriting  Gurgaon (4  5 Years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Going Deeper into Regression Analysis with Assumptions, Plots & Solutions|3 Must Know Analytical Concepts For EveryProfessional / Fresherin Analytics|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Location : GurgaonDesignation: Manager  UnderwritingExperience : 4  5 yearsAbout employer : ConfidentialResponsibilities:An exciting opportunity has become available to join a vibrant team. The role of an Underwriting Manager is critical in supporting the decision that are made across the business. You are expectedto meet the customers requirement to evolve & deeper the underwriting criteria.Qualification and Skills Required:Interested people can apply for this job by sending their updated CV to [emailprotected]with subject as Manager in Underwriting Gurgaon and following details:,https://www.analyticsvidhya.com/blog/2016/07/manager-underwriting-gurgaon-4-5-years-experience/
3 Must Know Analytical Concepts For EveryProfessional / Fresherin Analytics,Learn everything about Analytics|Introduction|End Notes,"1. Descriptive Analytics|2. Predictive Analytics|3. Prescriptive Analytics|About the Author|Share this:|Like this:|Related Articles|Manager  Underwriting  Gurgaon (4  5 Years of Experience)|10 Analytics / Data Science Masters Program by Top Universities in the US|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The use of analytical methods have gained immediate importance in the last few years.The practice of gaining useful insights from datahave helped several companies improve their business performance.Analytics allowcompanies to obtaina clear picture of events in the past and the future of their performance. A peek into the future helps companies to get prepared for the misfortune (if any) about to arrive.Using analytics, companies can find answers to three main questions: What has happened, What is happening and What will happen. It wouldnt be incorrect to say that the rise in data has fueled this outrageous penetration of analytics use.Analyticsis not just limited to deriving insights from the past, but also enables predicting future outcomes and optimizing business resources.As a result, the more advanced forms of analytics namely, predictive and prescriptive have assumed greater importance in supporting organizations decision making needs.In this article, Ive explained the 3 major forms of analytics which categorizes all forms of analytical models applied across countries.According to a study, organizations that focus on basic automation to expand their reporting capabilities can improve their ROI by 188 percent. But, adding advanced analytic deployments that enhance organization strategy can extend their ROI by as much as 1209 percent.So in principal, what are the different types of analytics?Lets start with the most basic type of analytics i.e. descriptive analytics. The objective of descriptive models is to analyze historical trends and figure out relevant patterns to gain insights on the population behavior. Descriptive analytics involve finding answers to what has happened?. It is the most commonly used form of analytics by organizations for their day to day functioning and is generally the least complex.Descriptive models use basic statistical and mathematical techniques to derive key performance indicators that highlight historical trends. The primary purpose of the model is not to estimate a value, but gain insight on the underlying behavior. Common tools used for running descriptive analysis include MS Excel, SPSS, and STATA.A typical example in the Banking industry would be customer segmentation. Historical data is mined to analyze customers spending patterns and wallet share to enable a targeted approach to marketing and sales. Such models are powerful tools to profile the population, but are limited in their predictive ability with respect to the individual members behavior from the same population.Helpful resources:Predictive analytics use statistical modeling to determine the probability of a future outcome or a situation occurring. It involves finding answers to What could happen?.Predictive models build up on descriptive models as they move beyond using historical data as principal basis for decision making, often using structured and unstructured data from various sources. They enable decision makers to make informed decisions by providing a compressive account of an events likelihood to occur in the future. They encompass various advanced statistical models and sophisticated math concepts like random forests, GBM, SVM, GLM, game theory, etc.A predictive model builds on a descriptive model to predict the future behavior. However, unlike a descriptive model that only profiles the population, a predictive model focuses on predicting a single customer behavior.Tools used to run predictive models vary by the nature of the models complexity, however some of the commonly used tools are RapidMiner, R, Python, SAS, Matlab, Dataiku DSS, amongst many others. Online resources on using these tools can be found on Coursera.A typical example in the banking industry would be advanced campaign analytics. It can help predict the likelihood that a customer would respond to a given marketing offer to improve the cross-selling and up-selling of products. Another example would be predicting the probability of fraud on credit cards.Helpful resourcesPrescriptive analytics is the most sophisticated type of analytics that uses stochastic optimization and simulation to explore a set of possible options and recommend the best possible action for a given situation. It involves finding answers to What should be done?.Prescriptive models go beyond descriptive models that only address what is going on, and beyond predictive models that can only tell what will happen, as they go on to advise what actually should be done in the predicted future. They quantify the effect of future actions on key business metrics and suggest the most optimal action.Prescriptive models synthesize big data and business rules using complex algorithms to compare the likely outcomes of a number of actions, and choose the most optimum action to drive business objectives. Most advanced prescriptive models follow a simulation process where the model continuously and automatically learns from the current data to improve its intelligence.These models are typically the most complex in nature and hence are being used by some progressive and large companies, as they are difficult to manage. However, when implemented correctly, they can have a strong impact on a companys decision making effectiveness and hence, its bottom line.That said, technical advancements such as super computers, cloud computing, Hadoop HDFS, Spark, in-database processing, MPP architecture, etc. have made deployment of complex prescriptive models using structured and unstructured data much easier. Tools used to run prescriptive modelsare mostly the same as predictive models however, require advanced data infrastructure capabilities.A common example of prescriptive models in the retail banking industry is the optimum allocation of sales staff across various branches of the bank to maximize new customer acquisitions. Marrying geo-location information with every branchs performance and potential, the model can prescribe the most optimum allocation of sales staff across all the branches.A more sophisticated prescriptive modeling approach is used in the airline ticket pricing systems to optimize the price of air tickets based off travel factors, demand levels, purchasing timing, etc. to maximize profit margins, but also at the same time not deter sales.According to a research, about 10% organizations use some form of prescriptive analytics at present, this figure has increased from 3% in 2014 and is expected to rise to 35% by 2020. Factors like massive investments in predictive analytics, expansion of IoT capabilities that compliment prescriptive analytics are driving this growth and expanding the scope of prescriptive models.Helpful resources (in addition to the ones on Predictive analytics):In this article, Ive discussed 3 different versions of analytics used across industriestoday. These are the building blocks of the analytics industry across the world. It is fairto say that all models, developments, and discoveries made using data can beclassified under any of these three categories.This article is meant to help people who are new to analytics or plan to switch over toanalytics to get a clear view of the domain. I hope the mentioned resources help you getstarted with learning.Sajal Jain is an analytics professional with 6+ years of experience in banking and workforce analytics. He completed his MSc in Statistics from the London School of Economics (on scholarship) and is currently working with a research based consulting and technology firm in Gurgaon.Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.",https://www.analyticsvidhya.com/blog/2016/07/3-analytical-concepts-every-professional-fresher-in-analytics/
10 Analytics / Data Science Masters Program by Top Universities in the US,Learn everything about Analytics|Introduction|Why study in the US?|How to decide if a Program is Good ?|Analytics / Data Science Programs by Top Universities in the US|SummaryTable|Some More Programs|End Notes,"1. MS in Data Science, ColumbiaUniversity|2. MS in Data Science, New York University|3. MS in Computational Data Science, Carnegie Mellon University|4. MS in Machine Learning,Carnegie Mellon University|5. MS in Analytics, Northwestern University|6. MS in Analytics, Georgia Institute of Technology|7. MS in Analytics, North Carolina State University|8. MS in Analytics, Texas A&M University|9. MS in Business Analytics, Michigan State University|10. MS in Business Analytics, University of Cincinnati|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|3 Must Know Analytical Concepts For EveryProfessional / Fresherin Analytics|AV LearnUp, DelhiNCR Chapter, India, 17th July 2016|
Aarshay Jain
|70 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Doing Post-graduation in theUnited States of America (USA) is a dreamof countlessstudents across the world.Every year, million of students worldwideappear in examinations like GREs, SATs, TOEFL with a hope of studyingin the top US universities. Only a small percentage of these applicants get through!Qualifying for studying Analytics / Data Science as a post graduate course in US is not easy. But its not impossible either. I recently got selected in2016-2018 batch forMS in Data Science at Columbia University.So, I thought I will share my learnings and research with our community.Before starting the research, you should be asking this. If you follow this field closely, the answer should be obvious.US is the largest analytics / data science market in the entire world. The major benefit of pursuing Masters in US is to gain access to the large pool of upcoming job opportunities in US. It is also one of the most mature market in analytics / data science evolution.If youve ever dreamed of working as a data scientist in US, this guide with take you a step closer.In this article, Ive provided a detailed analysis of 10 goodMS Programs in Analytics /Data Science in US. Ive seen that people become clueless inchoosing the best college / university for themselves. Therefore, Ive also provided a detailed explanation of selection parameters which can be used to evaluate goodness of any university program.Note:This is not an exhaustive list. Ive only listed the best universities which one should consider which applying for analytics programs in US.Before jumping into the programs, I would like to discuss someaspects which you should consider while judging a program. There is no absolute ranking among programs because each program is stronger / better than othersin some aspects. So, choosing a university completelydepends on your preference and choiceof parameters as described below:Having understood the key parameterswe should keepin mind while evaluating a masters program, lets consider some of the good programs that I came across whileapplying for my masters.For a better view,Ive provided ranking to these universities on 4 parameters namely Mathematics, Statistics, Computer Science and Businessbased on US News.Therefore, for you to decide which is better, you would have to weight these parameters accordingly. For example, if you think that you are good at mathematics but not computer science, choose theprograms with a higher concentration on mathematics in the curriculum.Columbia University is located in the heart of New York city. Being an Ivy League institution, there are no questions about its reputation. The MS programis being run by the Data Science Institute at Columbia. The students have access to courses from all the top programs at the institute. The general course duration is 16 months,i.e. 3 semesters of study and an internship semester.Conclusion:The program provides a good foundation inmachine learning and programming along with practical experience. Moreover Columbiais ranked in top 20 in all the domains related to data science making ita good choice. One drawback of the program could be that the curriculum is a bit inclined towards programming and more technical in nature than few other programs, which are more business oriented.NYUis located in New York city and is fairlyreputed. The MS programis being run by the Center for Data Science at NYU. The students have access to courses from a wide range of departments including statistics, AI, bio-statistics, business, economics, psychology etc. Thecourse can be completed in 3 or 4 semesters, depending on the choice of students.Conclusion: This program will provided a strong foundation inmachine learning and ample experience in a particular domain through the 6 electives courses. NYU might lack slightly in terms of the departmental rankings but the program structure and location of NYCwill definitelyCarnegie Mellon University (CMU) is one of the topmost universities for research in computer science. Its CS department also run fewspecialized masters programs.These programs focus on one core domain, have higher tuition fee and offers no assistance. They treat them as cash cow programs butstudents benefit from the high quality pedagogy. MSCDS is one suchprogram. It spans over 16 months with 3 semesters of study and an internship semester.Conclusion:This is a CS oriented program and idealfor people with some coding experience who want to get into machine learning. The drawback being that the business side of the program is weak and you should not expect getting some domain experience like finance/healthcare. Itis better suited for software engineering roles rather than data scientist roles.This is another program like #3 offered by the Machine Learning Department in the dept of CS at CMU. The core idea is the same except for a couple of changes:This would also prepare you for software engineering or research roles. You should choose this if youhave a theoretical bent of mind and would like to pursue a doctorate (Ph.D) after masters.This interdisciplinary masters program is being run by McCormick (engineering), Kellogg (management) and Medill (journalism) schools at NWU along with industry professionals in the Chicago area. Its a 15 month program with a 10 week internship.Conclusion:This program is designed for people working in a particular domain who want to understand analytics and its applications in different industries. It is not designed for techies who want to incorporatemachine learning algorithms in their software. The program makes heavy use of the industry connections coming from Kellogg School of Management, which is one of the most reputed management institution of the world.This interdisciplinary program is run jointly by the Collegeof Engineering, Business and Computing at GaTech. Its a 1 year program and covers the fall, spring and summer semesters.Conclusion:This is a typical coursework based program. One drawback could bethe choice between a capstone and internship. Also, the short duration of the program will put additional academic burden and restricts thenetworking opportunities. The positives are in terms of GaTechs brand name and the involvement of operations research courses in which GaTech is one of the best institutes.This program is managed by the Institute of Advanced Analytics at NCSU and is the first analytics program started way back in 2007.Most of the other programs are 2-4 years old and thus lack recognition. But NCSU is a highly reputed program in the analytics industry, even though NCSU as a whole is considered a tier 2 institution. This is a 10-month intensive program, with 3 semesters starting in the summer and ending in spring. Moreover, GRE score is not required for application, only TOEFL is required.Conclusion:NCSU is a well reputed program with good future prospects. It prepare candidates well for data scientist roles as it exposes them to a wide spectrum of analytics techniques. Strong mathematics and statistics fundamentals are required to get into this program and you should apply only if you are confident aboutthe same.The masters program at TAMU is offered by the department of statistics and its a part-time program for working professionals. The program website is not much informative but TAMU as an institution has a decent reputation in the industry. Being a part time program, it is spread over 5 semesters.Overall, it is a decent program anddesigned specifically for working professionals.This is a 1 year program which commencesin the spring semester and continues in summer and fall with graduation in December. The course prepares students for data scientist roles in industries such asconsulting, automotive, consumer products, retail, and financial services.Conclusion:This is a goodprogram and if you like the fixed curriculum, it might work out. Also, since Michigan State University is not as reputed as some other universities mentioned here, it might be easier to get in.This is another 1 year program commencingin fall, with a more or less fixed curriculum. It prepares the candidates for business analyst and data scientist positions.Conclusion: This is a slightly less reputed university with a decent program which should be comparatively easier to get through. But you should be comfortable with the curriculum before you think about taking it up.Im adding a list of other programs which you can consider and evaluate using the ideas shared above. Please feel free to drop a comment if you feel other programs should be added and Ill be happy to make a mention here.MS in other departmentswith Data Science/Machine Learning specialization:Other related programs:List of online data science programs  click here.List of onlinebusiness analytics programs  click here.In this article, Ivediscussed the various factors which you should consider while selecting a masters program in data science in USA. I have also evaluated10 programs on some of these factorsbased on available information.This should be sufficient for making an initial selection of which courses to apply to. But while making the final selection of which institution to attend, you should consider additional factors which might require your extraeffort and research.Please note that these days there are many traditional courses which offer a specialization in data science like an MS in Computer Science with machine learning track or an MS in Statistics with data science track. I haventconsidered such courses here because they have a focus on the core subject with subtle emphasis on data science. If you have a core domain, you can check out such courses as well.I would like to restate that this is by no means a ranking of the institutions. Actually, rankings will be very relative because each program has some pros and cons making the suitability vary from one individual to other. However, I have provided rankings from a trusted source for different subjects here which can be used as a metric to judge the university in area of your interest.Hope this helps you in making a good career choice. If you have any questions, please feel free to discuss in comments.",https://www.analyticsvidhya.com/blog/2016/07/10-analytics-data-science-top-universities-masters-usa/
"AV LearnUp, DelhiNCR Chapter, India, 17th July 2016",Learn everything about Analytics|Introduction|Who should attend this meetup?|How much is the fee for this meetup?|Why should you attend this meetup?,"Share this:|Like this:|Related Articles|10 Analytics / Data Science Masters Program by Top Universities in the US|Strategic Thinking Competition by Analytics Vidhya, 16th July 2016|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Science is seen as a hope for a better future, the impending shortage for analytics professionals has sparked off a great demand.The analytics industry is at a nascent stage right now and the bundle of opportunities are huge.If you are seeking to shift to analytics industry or you are a novice whos eager to build a career in data science then this could be your ticket to the hottest job in the industry.Our Analytics for Beginners meet-up in collaboration for Bridge School of Management is just the place for you.I invite your participation to one of the most informative workshops and take your first step towards Data Science/ Analytics profession.Date: 17th July 2016Time: 10:30 am to 5:30 pmVenue:Bridge School of Management NoidaNear sector  15 metro station, Noida RegisterHere
Anyone who is keen to learn analytics or wants to shift his/ her career in analytics is welcomed to come and join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.Food & Fun will be on the house. Please make sure you bring your laptop along. Learn ways to start your career in analyticsLearn about the best practices (tips & tricks) to become successful in data science. Get the chance to learn from real life experiences of best data scientists in Delhi NCR. Gain hands on experience on predictive modeling by working on a live practice problemGet working knowledge of R / Python / SAS / Excel from scratchFor more details of this meetup,  VisitHere
For more information & details on these meetups, feel free to drop us a mail at[emailprotected].",https://www.analyticsvidhya.com/blog/2016/07/av-learnup-learn-data-science-scratch/
"Strategic Thinking Competition by Analytics Vidhya, 16th July 2016",Learn everything about Analytics|Introduction|Who should join this competition?|How much is the fee for this competition?|Why should you participate in this competition?,"|Share this:|Like this:|Related Articles|AV LearnUp, DelhiNCR Chapter, India, 17th July 2016|Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have been eagerly waiting for the our next hackathon, your wait is over. This July is packed with excitement and lot of challenges. We bring you a minihack Mind Your Strategy.Mind Your Strategy-is a strategic thinking competition where you have to analyze a business case and answer the given questions.You will be challenged on your structured thinking and problem solving abilities. So put on your strategic thinking caps and participate in this competition.Date : 16th July 2016Duration : 3 hoursMode: OnlineCash Prize : INR 55k (~$810) RegisterHere
Anyone who is keen to test their logical thinking and problem solving or wants to compete with analytics professionals across the globe is welcome to join us.There is NO FEESfor participating in this event. We believe that learning should not be obstructed due to financial commitments. Hence, you are invited.For more details about the competition,  Visit Here
You can also read this article on Analytics Vidhya's Android APP ",https://www.analyticsvidhya.com/blog/2016/07/data-hackathons-analytics-vidhya/
Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R,Learn everything about Analytics|Introduction|What is LogLoss Metric ?|Why do we need calibration (adjustment) of probabilities ?|2 Calibration Methods to Minimize LogLoss Error|End Notes,"1. Platt Scaling|2.Isotonic Regression|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Strategic Thinking Competition by Analytics Vidhya, 16th July 2016|Tapping Twitter Sentiments: A CompleteCase-Study on 2015 Chennai Floods|
NSS
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article is best suited for people who actively (or are aspiring to) participate in data science / machine learning competitions and try hard to improve their models accuracy.Many a times, we faceproblems (data sets) whose evaluation metric is LogLoss. Its mostly seen in problems where probabilities needs to bepredicted. Recently, I came across a similar problem.You know, its being said, when you know no boundaries, you do things which were seen improbable. So, in the quest of my accuracy improvement, I discovered 2 powerful methods to improve accuracy of predicted probabilities.In this article, youll learn about these methods and how to implement them in R to improve your rankings and score.Also, Ill show you how I achieved a uplift of 87 ranks on competition leaderboard using one of these methods.Note: Ive participated inPredict Blood Donationscompetition which is currently active and uses Log Loss as its evaluation metric.This Evaluation Metric is employed in the classification tasks where rather than predicting the actual classes, one needs to predict the probabilities corresponding to that classes. So, in a way this metric measures the quality of predictions rather than the accuracy. The formula for Log Loss goes as-Whereis the actual class value (i.e 0 or 1 in case of binary classification) of a particular observation(row) andis the predicted probability of that class.Now, lets take an example of a single row where the actual class is 0 and you predict the probability with absolute certainty i.e 1. Log Loss in this case turns out to be infinity. So, it is easy to see that Log Loss metric penalizes absurdmisclassification with a high certainty such that the rest of predictions become irrelevant.So in a way, a better model is one which gives average probability to the observation about which it is unsure of. The closer the predicted probability to the actual class value, the better it is.Here I provide you the relevant code in R which can be used to calculate the Log Loss during the cross-validation stage while model building:# Log Loss as defined on the kaggle forumLogLoss<-function(act, pred)
 {
 eps = 1e-15;
 nr = length(pred)
 pred = matrix(sapply( pred, function(x) max(eps,x)), nrow = nr) 
 pred = matrix(sapply( pred, function(x) min(1-eps,x)), nrow = nr)
 ll = sum(act*log(pred) + (1-act)*log(1-pred))
 ll = ll * -1/(length(act)) 
 return(ll);
 }

 LogLoss(actual,predicted)Im sure many of you would have this question, why and under what circumstances will I need to calibrate the probabilities?.Over time and due to the efforts of a scholarly researchers anddata scientists, experiments have shown that maximum margin methods such as SVM, boosted trees etc push the real posterior probability away from 0 and 1 while methods such as Naive Bayes tend to push the probabilities towards 0 and 1. And in cases where predicting the accurate probabilities is more important, this poses a serious problem.It has also been noticed empirically that boosted trees, random forests and SVMs performs best after calibration. Lets see what works best!Here, we discuss two methods of calibrating the posterior probabilities  Platt Scaling and Isotonic Regression with the help of a realdata set. Ill show youhow I achieved a boosting of 87 ranks just by applying Platt Scaling on my model.First of all, I would like to introduce reliability plots which can be used to visualize our calibration. As described in Niculescu-Mizil et al:On real problems where the true conditional probabilities are not known, model calibration can be visualized with reliability diagrams (DeGroot & Fienberg, 1982). First, the prediction space is discretized into ten bins. Cases with predicted value between 0 and 0.1 fall in the first bin, between 0.1 and 0.2 in the second bin, etc.
 For each bin, the mean predicted value is plotted against the true fraction of positive cases. If the model is well calibrated the points will fall near the diagonal line.Now, lets learn to create reliability diagrams in R. The number of bins have been converted into an user-entered parameter.I would like you to pay attention to the very first line of this code. This line creates a simple plot with a grey line inclined at an angle of 45 . This line will form our benchmark solution. Any line closer to this diagonal line in comparison to the other lines will be a better solution on Log Loss metric.You should probably save this function somewhere and keep it handy since we will be using it to graphically view the performance of our model on Log Loss metric.Practice time! It is better if we understand all this using a real dataset. The dataset that I will be using is the Predict Blood Donationsdataset.Since our motive is to understand the usage and impact of Platt Scaling, we wont be dealing with pre-processing and feature engineering. I have also removed the IDand the Total Volume Donatedvariable since ID is of no use for the prediction purpose and Total Volume Donated is perfectly correlated to Number of Donations.Platt scaling is a way of transforming classification output into probability distribution. For example: If youve got the dependent variable as 0 & 1 in train data set, using this method you can convert it into probability.Lets now understandhow Platt Scaling is applied in real Predictive Modeling problems (in order):I hope that was easy to visualize and understand. Now lets start with the actual coding process for a practicalunderstanding.# reading the train dataset
train <-read.csv(""train.csv"")# reading the test dataset
test <-read.csv(""test.csv"")#converting the dependent variable into factor for classification purpose.
train$Made.Donation.in.March.2007<-as.factor(train$Made.Donation.in.March.2007) # removing the X column since it is irrelevant for our training and total colume column since it is perfectly correlated to number of donations
train<-train[-c(1,4)]# splitting the train set into training and cross validation set using random sampling
set.seed(221)
 sub <- sample(nrow(train), floor(nrow(train) * 0.85))
training<-train[sub,]
 cv<-train[-sub,]# training a random forest model without any feature engineering or pre-processing
library(randomForest)
 model_rf<-randomForest(Made.Donation.in.March.2007~.,data = training,keep.forest=TRUE,importance=TRUE)#predicting on the cross validation dataset
result_cv<-as.data.frame(predict(model_rf,cv,type=""prob"")) #calculating Log Loss without Platt Scaling
LogLoss(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv$`1`) # performing platt scaling on the dataset
dataframe<-data.frame(result_cv$`1`,cv$Made.Donation.in.March.2007)
 colnames(dataframe)<-c(""x"",""y"") # training a logistic regression model on the cross validation dataset
model_log<-glm(y~x,data = dataframe,family = binomial) #predicting on the cross validation after platt scaling
result_cv_platt<-predict(model_log,dataframe[-2],type = ""response"")
LogLoss(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv_platt)# plotting reliability plots# The line below computes the reliability plot data for cross validation dataset without platt scaling
 k <-reliability.plot(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv$`1`,bins = 5)
 lines(k$V2, k$V1, xlim=c(0,1), ylim=c(0,1), xlab=""Mean Prediction"", ylab=""Observed Fraction"", col=""red"", type=""o"", main=""Reliability Plot"")#This line below computes the reliability plot data for cross validation dataset with platt scaling
 k <-reliability.plot(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv_platt,bins = 5)
 lines(k$V2, k$V1, xlim=c(0,1), ylim=c(0,1), xlab=""Mean Prediction"", ylab=""Observed Fraction"", col=""blue"", type=""o"", main=""Reliability Plot"")legend(""topright"",lty=c(1,1),lwd=c(2.5,2.5),col=c(""blue"",""red""),legend = c(""platt scaling"",""without plat scaling""))As you can see the blue line is more closer to the grey line indicating that Platt Scaling actually improved (or reduced) our Log Loss error metric. The most important point to be noted here is that other metrics like accuracy, AUC etc are not influenced to an appreciable extent using the Platt Scaling.Now, we will use the above method to make predictions on the data set. Lets find out how much improvement did we achieve:# The below line makes the test data set similar to train data set by removing the X and the Total Volume Donated column.
test<-test[-c(1,4)]# Predicting on the test dataset without Platt Scaling
result_test<-as.data.frame(predict(model_rf,newdata = test,type = ""prob""))# Predicting on the test dataset using Platt Scaling
dataframe1<-data.frame(result_test$`1`)
colnames(dataframe1)<-c(""x"")
result_test_platt<-predict(model_log,dataframe1,type=""response"")The result_test scored 1.6932on the Public Leaderboard without any feature engineering and Platt Scaling,whereas result_test_platt scored 0.4895without feature engineering but platt scaling.So, the method we used is simple, but its results are astonishing. You can see, I got a massive improvement in my error score. This musthave givenyou enoughidea about the prowess of Platt Scaling.Next, lets discuss another interesting method which can be used to improve the performance on a Log Loss metric- Isotonic Regression.Isotonic Regression is similar to Platt Scaling. Its a non-parametric regression technique. Non-parametric means that it doesnt make any assumptions such as of linearity among variables, constant error variance etc.The only difference lies in the function being fit. The function wefit in isotonic regression continuously increases/decreases. The below code builds upon the Platt Scaling code mentioned above, to buildthe reliability plot before followed byisotonic regression. The method employs the use of isoreg function in the stats package:fit.isoreg <- function(iso, x0) 
{
 o = iso$o
 if (is.null(o)) 
 o = 1:length(x)
 x = iso$x[o]
 y = iso$yf
 ind = cut(x0, breaks = x, labels = FALSE, include.lowest = TRUE)
 min.x <- min(x)
 max.x <- max(x)
 adjusted.knots <- iso$iKnots[c(1, which(iso$yf[iso$iKnots] > 0))]
 fits = sapply(seq(along = x0), function(i) {
 j = ind[i] # Handles the case where unseen data is outside range of the training data
 if (is.na(j)) {
 if (x0[i] > max.x) j <- length(x)
 else if (x0[i] < min.x) j <- 1
 } # Find the upper and lower parts of the step
 upper.step.n <- min(which(adjusted.knots > j))
 upper.step <- adjusted.knots[upper.step.n]
 lower.step <- ifelse(upper.step.n==1, 1, adjusted.knots[upper.step.n -1] ) # Perform a liner interpolation between the start and end of the step
 denom <- x[upper.step] - x[lower.step] 
 denom <- ifelse(denom == 0, 1, denom)
 val <- y[lower.step] + (y[upper.step] - y[lower.step]) * (x0[i] - x[lower.step]) / (denom) # Ensure we bound the probabilities to [0, 1]
 val <- ifelse(val > 1, max.x, val)
 val <- ifelse(val < 0, min.x, val)
 val <- ifelse(is.na(val), max.x, val) # Bit of a hack, NA when at right extreme of distribution
 val
 })
 fits
}fit.isoreg function fits a linear line to the function since the isoreg function fits a step wise function to the data.Lets visualise the iso.model function fitted through the isoreg function.#This part of the code removes the duplicate predicted values of the cross validation set
idx <- duplicated(result_cv$`1`)
result_cv_unique <- result_cv$`1`[!idx]
cv$Made.Donation.in.March.2007<-as.numeric(as.character(cv$Made.Donation.in.March.2007))
cv_actual_unique<- cv$Made.Donation.in.March.2007[!idx]# This line of code creates an isotonic regression model on the cross validation dataset
iso.model <- isoreg(result_cv_unique, cv_actual_unique)plot(iso.model,plot.type = ""row"")Here x0 is the predicted value on the cross validation data set using the model_rf (random forest) and x$yconsist of actual dependent values on the cross validation data set. The red line in the first diagram shows a isotonic function fitted to the cross validation data set. Figure 2 is the cumulative version of the the Figure 1.The fit.isoreg function mentioned above smooths (makes more flexible) the stepping function for better prediction.Now, lets actually compute the reliability plot data for the isotonic regression and then calculate the LogLoss.# Predicting the cross validation dataset after the isotonic regression
result_cv_isotonic <- fit.isoreg(iso.model, result_cv$`1`)# plotting isotonic reliabililty plot
plot(c(0,1),c(0,1), col=""grey"",type=""l"",xlab = ""Mean Prediction"",ylab=""Observed Fraction"")
k<-reliability.plot(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv$`1`,bins = 5)
lines(k$V2, k$V1, xlim=c(0,1), ylim=c(0,1), xlab=""Mean Prediction"", ylab=""Observed Fraction"", col=""red"", type=""o"", main=""Reliability Plot"")
k<-reliability.plot(as.numeric(as.character(cv$Made.Donation.in.March.2007)),result_cv_isotonic,bins = 5)
lines(k$V2, k$V1, xlim=c(0,1), ylim=c(0,1), xlab=""Mean Prediction"", ylab=""Observed Fraction"", col=""blue"", type=""o"", main=""Reliability Plot"")legend(""topright"",lty=c(1,1),lwd=c(2.5,2.5),col=c(""blue"",""red""),legend = c(""isotonic scaling"",""without isotonic scaling""))It might seem like isotonic function didnt work because of the restriction of number of bins. But, you can change the number of bins to have a better view. Lets see if isotonic function actually worked by making the calibrated prediction on test data set and uploading the results.result_test_isotonic<-as.data.frame(fit.isoreg(iso.model,dataframe1$x))The Log Loss for the result_test on the Public Leaderboard was 1.6932 without any feature engineering and isotonic regressionand after isotonic regression on the Public Leaderboard was 0.5050.This article iscode intensive and focused mainly in providing the readers about the technique that kagglers often employ to improve their scores on Log Loss metric.These methods work because of the underlying assumptions of the algorithms. This is by no means an exhaustive list of the methods. Idencourageyouto experiment with these methods since they can be heavily modified according to the problem at hand.Also, please feel free to suggest any more methods that are currently being employed or you can think of. For yourconvenience, I have explained the above two examples with a particular seed value so that youcan reproduce exact results. I have also uploaded the code file in R on Github.Did you like reading this article ? Do share your experience / suggestions in the comments section below. Id love to know your intuitive solutions to learn more ways of solving LogLoss problems.",https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/
Tapping Twitter Sentiments: A CompleteCase-Study on 2015 Chennai Floods,Learn everything about Analytics|Introduction|Table of Contents|1. ChennaiFloods  Quick Story|2. Exploratory Data Analysis|3. Clustering and Topic Modeling|4. Conclusion|Our learning & journey,"2.1 The typical tweet|2.2  Data Extraction|2.3 Data Preparation & Exploration|2.4 Text Preparation|2.5 Word Frequencies and Associations|3.1 Hierarchical Clustering|3.1.1 Interpretation|3.2 K-Means Clustering|3.2.1 Choosing k|3.3 Topic Modeling|3.3.1 Latent Dirichlet Allocation|4.1 Limitations of this study|4.2  Challenges to Real-Time Analysis of Tweets|4.3  Applications & Scope for Further Work|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R|Solving Case study : Optimize the Products Price for an Online Vendor (Level : Hard)|
Guest Blog
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We did this case study as a part of our capstone project at Great Lakes Institute of Management, Chennai. After wepresented this study, we got an overwhelming response from our professors & mentors. Later, they encouraged us to share our work to help others learn something new.Weve been following Analytics Vidhya for a while now. Everyone knows, its probably the largest engine to share analytics knowledge. We tried and got lucky in connecting with their content team.So, using thiscase study weve analyzed the crisis communication on Twitter happened during Chennai Floods. Also, wevediscovered patterns & themes of communication,the way in whichthe platform was used to share information and how it shaped response to the crisis. After successfully finishing our study, the following objectives were achieved:This study isdone on a set of social interactions limited to the first two days of December 2015, as these were the worst days of the crisis. The analysis wasrestricted to a set of 6000 tweets, for want of computing power. The tweets wereextracted by looking for the hashtag #ChennaiFloods.Amonganalytical approach & tools used, Topic Analysis of tweets is done usingLatent Dirichlet Allocation.K-Means & Hierarchical Clustering isemployed on the themes of tweets. Tableau isused to create word clouds. Microsoft Excel and Word isused to perform spellchecks on extracted words.The findings of this study explores the viability of analyzing Twitter communication real-time with an aim of providing timely assistance to the affected areas.Chennai Floods  Quick StoryExploratory Data Analysis2.1  The Typical Tweet2.2  Data Extraction2.3  Data Preparation & Exploration2.4  Text Preparation2.5  Word Frequencies and AssociationsClustering and Topic Modeling3.1  Hierarchical Clustering3.1.1  Interpretation3.2  k-Means Clustering3.2.1  Choosing k3.3  Topic Modeling3.3.1  Latent Dirichlet AssociationConclusion4.1  Limitations of this study4.2  Challenges in Real Time Analysis of Tweets4.3  Applications and Scope of Further WorkOur learning & JourneyIn NovemberDecember 2015, the annual northeast monsoon generated heavy rainfall in the Coromandel Coastregion of the South Indian States of Tamil Nadu and Andhra Pradesh, the union territory of Pondicherry with the city of Chennai particularly hard-hit.Termed 2015 South Indian floods asthe black swan caused an enormous destruction.It is estimated that around 500 people lost their lives and over 18 lakh (1.8 million) people were displaced.With approximations of damages and losses ranging from 50000 crore (US$7 billion) to 100000 crore (US$15 billion), the floods were the costliest to have occurred in 2015, and were among the costliest natural disasters of the year. The flooding has been attributed to the El Nio phenomenon.In the city of Chennai, the impact of the heavy rainfall was compounded by the overfull water reservoirs, which soon started to spill over into the city. Whole neighborhoods were inundated, with loss of power, communication and access to essentials. The National Disaster Response Force (NDRF), the Indian Army and disaster management units were pressed into service. Rescue and rehabilitation efforts spanned well over 2 weeks.What emerged conspicuously during this unprecedented crisis was the coordination of relief efforts over social media platforms such as Twitter, Facebook and WhatsApp. Volunteers, NGOs and other rescue parties were observed to be sending out alerts, requests and sharing useful information related to the flooding on social media. Others used social media to check up on their loved ones, share information, express opinions and sending out requests for help.A tweet is a social media message posted on Twitter.com. It is restricted to 140 characters. Though most tweets contain mostly text, it is possible to embed URLs, pictures, videos, vines and GIFs.Tweets contain components called hashtags, which are words that capture the subject of the tweet. They are prefixed by the # character. They can also convey emotion (#sarcasm) or an event (#IndiaVsPakistan) or popular catchphrase in pop culture (#ICantBreathe).Usernames or handles of those who post are recognized by the @ symbol. A user can direct a message to another user by adding the handle, with the @ symbol.A retweet (rt for short) is a tweet by a user X that has been shared by user Y to all of Ys followers. Thus, retweets are a way of measuring how popular the tweet is.A user can favorite a tweet; this is analogous to a Like on Facebook.Twitter has an official API called OAuth, a token-based authentication system that indexes tweets that match a given search string and writes the output to a file. While this service is free and convenient to perform a quick and efficient extraction of tweets, it has a crucial limitation: it can retrieve tweets only from the previous week.Hence, a repository on the code sharing portal GitHubwas utilized to search for and extract tweets. The search term was the hashtag #ChennaiFloods. The python language was used (in an Ubuntu Linux environment) to perform the extraction; the extracted tweets were written to a comma-separated value file.The information gathered contained hashtags and mentions of the users who tweeted about this event. These were separated in R using regular expressions.The following wordcloud shows the hashtags used in the tweets.From the hashtags used, the following themes are evident, other than the disaster itself:Sympathy (#PrayForChennai)Requests for help (#savechennai, #ChennaiRainsHelp)Information on further weather forecasts (#chennaiweather)Information on specific areas in Chennai (#airport, #Chromepet)Cautionary messages (#ExerciseCaution)Various hashtags for the same topic are observable. This would make it challenging to separate all tweets on the subject.The below chart shows the top 10 active Twitter users that participated in relief efforts. Some handles belong to prominent celebrities.(It must be noted that these facts are true only with respect to the tweets collected; in reality, if all the tweets on the subject were analyzed, it is likely the no. of retweets, tweets & favorites per user may change.)Retweets are also likely to be heavily favorited.The tweets were parsed into a corpus for text analysis. The following steps were executed to clean the corpus and prepare it for further analysis. Only the text portion of the tweet (the actual message) was considered.Removing numbers: TweetIDs are number generated by Twitter to identify each tweet. Numbers as such dont serve any purpose for text analysis and hence they are discarded.Removing URLs & links: Many tweets contained links to webpages and videos elsewhere on the Internet. These were removed with regular expressions.Removing stopwords: Stopwords are words in English that are commonly used in every sentence, but have no analytical significance. Examples are is, but, shall, by etc. These words were removed by matching the corpus with the stopwords list in the tm package of R. Expletives were also removed.Removing non-English words: The corpus generated after performing the last 3 steps were broken into their constituent words and all English words and words less than 4 characters long were removed. What remained was the list of non-English words, words that mentioned areas in Chennai, words that are actually Tamil words written in English and misspellings of normal English words.The non-English words that denoted the names of localities were used to form a wordcloud.Stemming words: In text analysis, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formgenerally a written word form. Stemming is done to reduce inflectional forms and sometimes derivationally related-forms of a word to a common base form. Many methods exist to stem words in a corpus.Suffix-dropping algorithms: The last parts of all the words get truncated. For example, words like programming, programmer, programmed, programmable can all be stemmed to the root program. On the other hand, rescuing, rescue, rescued are stemmed to form rescu, which is not a word or a root.This method was chosen for this study for simplicity.Lemmatisation algorithms: Each word is the determination of the lemma for a word in the corpus. This is done with the understanding of the context, part of speech and the lexicon for the language. For example, better is related to good, running is related to walk and so on.n-gram analysis: Each word is broken into a part of its whole by n characters, and the one that makes most sense is retained. For example, for n=1 (unigram), the letters f, l, o, o, d are individually parsed from flood. For a higher n (say n=5), flood is retained from flooding, although at n=4, ding can also be construed as a word.Removing punctuation: Punctuation marks make no impact to the analysis of text and are hence removed.Stripping whitespace: Words that have extra whitespaces at the beginning, middle or end are subjected to a regular expression that removes the whitespace and retains only the words themselves.Checking for impure characters: A check on the corpus after the modifications made thus far revealed that some URLs were left behind, due to the removal of whitespaces, numbers and punctuations. Regular expressions were used to remove them.After the necessary cleaning, another wordcloud was plotted to understand the most frequently used terms. The following observations are made:A few simple words repeat more often than others: people, stay, safe, food etc. These are immediate reactions and responses to the crisis.Some infrequent terms are street, nagar, mudichur, road etc. that give information about the situation in a few areas.Words like pray, hope and proud are also used, in messages that mostly convey sympathy or hope.There were tweets about news reports covering the crisis, with words like channel, media etc.Words like help, please, food etc. were used in tweets that requested for volunteers to participate in rescue and rehabilitation efforts.A few words are associated strongly with the most frequent words than others. The below table describes the most common word associations.It was observed that the names of many localities feature in the associations list. These localities were mentioned with a lot less frequency than those in Figure 4. Considering these associations are from a list of 6303 tweets, more associations and localities are likely to emerge if the entire set of tweets on the crisis is considered.When a crisis like the Chennai floods strikes, a large number of similar tweets get generated that it becomes challenging to make meaningful interpretations from the huge volumes of data that need to be processed.One solution is to cluster similar tweets together after performing the necessary EDA operations that it becomes easier to manage the flow of information.Hierarchical clustering attempts to build different levels of clusters. Strategies for hierarchical clustering fall into two types (Wikipedia, n.d.):Agglomerative: where we start out with each document in its own cluster. The algorithm iteratively merges documents or clusters that are closest to each other until the entire corpus forms a single cluster. Each merge happens at a different (increasing) distance.Divisive: where we start out with the entire set of documents in a single cluster. At each step the algorithm splits the cluster recursively until each document is in its own cluster. This is basically the inverse of an agglomerative strategy.The results of hierarchical clustering are usually presented ina dendrogram.The R function, hclust() was used to perform hierarchical clustering. It uses the agglomerative method. The following steps explain hierarchical clustering in simple terms:Assign each document to its own (single member) clusterFind the pair of clusters that are closest to each other and merge them, leaving us with one less clusterCompute distances between the new cluster and each of the old clustersRepeat steps 2 and 3 until you have a single cluster containing all documentsTo perform this operation, the corpus was converted into a matrix with each tweet (or document) given an ID. Extremely sparse rows, i.e. rows with elements that are part of less than 2% of the entire corpus were removed. Wards method for hierarchical clustering was used.The dendrogram output is to be interpreted as follows:Farther the nodes, greater is the dissimilarity and more robust is thatThe closer the node, the weaker is theThe height of each node in the plot is proportional to the value of the intergroup dissimilarity between its twoThe following distinct clusters of tweets are observable from the dendrogram:Tweets that talk about general information about affected individuals, areas and news about the crisis.Tweets that talk about food, supplies and rescue efforts.Tweets that describe the weather, forecasts of rain and further developments.Tweets that caution people on risky areas and share information on relief efforts.It is also seen that there is a significant similarity between clusters of tweets; this is expected as the terms used across tweets are more or less similar.No locality or specific names are mentioned as the clustering was performed on a matrix that did not contain such rarely-occurring terms.As opposed hierarchical clustering, where one does not arrive at the number of clusters until after the dendrogram, in K-means, the number of clusters is decided beforehand. The algorithm then generates k document clusters in a way that ensures the within-cluster distances from each cluster member to the centroid (or geometric mean) of the cluster is minimized.A simplified description of the algorithm is as follows:Assign the documents randomly to k binsCompute the location of the centroid of eachCompute the distance between each document and each centroidAssign each document to the bin corresponding to the centroid closest toStop if no document is moved to a new bin, else go to stepThe most significant factor of employing k-means clustering is choosing the no. of clusters, k. The elbow method, wherein the SUM of Squared Error (SSE, the sum of the squared distance between each member of the cluster and its centroid) decreases abruptly at that value that is theoretically the optimal value of k, is widely applied to arrive at k.When k is plotted against the SSE, it will be seen that the error decreases as k gets larger; this is because when the number of clusters increases, they become smaller, and hence the distortion is also smaller.Here, the optimal value for k is shown to be 3, as that is where the SSE decreases abruptly. With k=3, the matrix of tweets was clustered using k-means.The plot clearly shows that there is only marginal dissimilarity with a corpus at 98% sparsity. This is evident from the top 10 words in each of the three clusters.cluster 1: rain need status flood helplin number contact stay safe atus
 cluster 2: food need contact avail peopl near water area call status 
cluster 3: peopl safe stay flood rain need near road contact mediaWith respect to clustering, subject matter and corpus knowledge is the best way to understand cluster themes. With the insights gleaned thus far, it is reasonable to assume the following:Cluster 1 contains news updates and cautionaryCluster 2 contains messages about requests for help and volunteersCluster 3 contains messages about area-specific updates and some cautionaryAnother technique that is employed to deduce the themes of text is topic modeling.A topic model is a type of statistical model for discovering the abstract topics that occur in a collection of documents (tweets in this case). Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: in this case, help is quite common to almost every tweet.A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about subject A and 90% about subject B, there would probably be about 9 times more words about B than words about A (Wikipedia, n.d.).Topic modeling has implementations in various algorithms, but the most common algorithm in use is Latent Dirichlet Allocation (LDA).Latent Dirichlet Allocation (LDA) is a statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each words creation is attributable to one of the documents topics.LDA allows the possibility of a document to arise from a combination of topics. For example, the following tweet may be classified as (say) 90% information & 10% sympathy/hope.LDA was performed with an objective to discover 6 topics. The output gives us following set of words for each topic.Topics 1 & 3 are quite similar; this is in agreement with the results of the K-means exercise.This plot graphs the probability associated with the top 100 words for each topic, sorted from most to least likely. Here too, the lines almost overlap, indicating the content similarity in the tweets.It is clear that the general sentiment across the tweets render the tweets quite similar. It has been demonstrated that crucial information like the worst-hit areas can be identified by analyzing tweets and performing basic text analytics.It is clear that the power of social media can be harnessed to great effect in times of crisis. This has not escaped Twitters notice; they have initiated the practice of creating hashtags specific to individual crises to index tweets easily. Facebook launches Mark Safe feature to those who have listed a crisis-hit location as their place of residence.The government agencies, NDRF and other relief agencies would do well to develop analytics capabilities focused on mining Twitter for real-time, tangible updates to take meaningful action.The study considers only 6000 tweets of the whole set of tweets that would have been sent on the subject.The study also did not consider captions of pictures, news reports, and other social media reports which could have generated additional insights.There exist other topic models and black box techniques for similar analysis that have a record of better performance. These have not been performed as they are beyond the scope of this exercise.The following points highlight a few challenges that will be faced by any researcher trying to solve the same problem.Retweets contain information that many users find relevant. The subjectivity of this relevance to the crisis at hand is difficult, if not impossible to measure.This problem is compounded if the tweets contain no hashtags. In the dataset generated for this analysis, 1399 tweets (22%) had no hashtags. These tweets may also be highly relevant to the crisis but may be missed due to the lack of hashtags.Twitter has a 140 character-limit on all tweets (not including pictures and videos). This leads users to truncate or shorten words to forms that is easily interpretable to humans, but is challenging for a machine. Eg: afcted ppl is easily understandable to mean affected people for a human, but not for a program. One way to solve this problem is to maintain a dictionary of such terms and match them in real-time.As mentioned in the introductory chapter, this is an active field. The power of social media will continue to be researched and newer applications will continue to be built to harness its power.One area is quashing rumors. During the Chennai floods, quite a number of false news reports and alerts circulated on Facebook, Twitter and the mobile messaging application WhatsApp. Machine learning can be employed to check the veracity of social media by comparing contents from actual news reports andDuring the 2011 Australia floods, the civic authorities were among the most prolific users of Twitter in disseminating safety tips, general information and coordinating volunteer relief work. Every civic authority would do well to develop a framework and system to manage crises also through social media. This covers all disasters, both natural (earthquakes, floods, hurricanes) and man-made (terror strikes, shootouts).Media outlets and government agencies can work together in planning for incidents that are expected by creating distinct identifiers and hashtags for each scenario and making the public aware of theDisasters may strike at any time. While it may not be possible to prevent them, it is prudent to be prepared for unfortunate eventualities. Having a dedicated social network analysis platform to analyze information in real-time will definitely aid in this endeavor.We came to Great Lakes PGPBABI with very little knowledge about analytics a year ago. While this capstone project is just the starting of our career, we feel proud of our first project and the amount of learning it brought to us.Here is what our mentor, Mr. Jatinder Bedi, had to say about our Capstone Project success, Great Lakes group approached me with a conventional topic in hand for which they already had the data. It was a movie data, which they wanted to analyze to build a movie recommendation engine. We had a choice of doing this conventional project or picking some challenging topic which would help us apply the new concepts learnt in a Great Lakes Business Analytics program. As it was a Chennai group, so I suggested them a topic Tapping Social Media Exchanges on Twitter: A Case-Study of the 2015 Chennai Floods with which they can relate well as it was more aligned to recent floods in Chennai. As I shared the topic, students got excited & everybody said yes. The idea was to build something from scratch and this was our chance to showcase our skills. Dr PKV, also liked the idea and gave us a go-ahead. I would say this project was a great success. Lastly, I feel very proud to be a mentor of this great project. Special thanks to Dr. PKV for his support and guidance.Also, Dr. P.K Viswanathan, Program Director  PGPBABI (Chennai) and Professor, remarked, The Capstone Project titledTapping Social Media Exchanges on Twitter: A Case-Study of the 2015 Chennai Floodsundertaken by the BABI participants of Batch 3 is a great revelation to me in terms of innovation in approach to the entire concept of an Analytic Study. Typically, students think that analysis and modeling using quantitative data is the key to success in any Business Analytics Project. This study completely deviates from the orthodox path, yet rich in a very substantive manner what analytics could do in the ambit of Natural Language Processing(NLP) of social media messages in the form of tweets leading to actionable insights on a real time basis.Cohesive Themes, Cogent Presentation, Mining using Advanced Analytic Modeling, and Excellent Insights that can be leveraged by policy makers in Government are the hallmarks of this project. It was rated by the evaluation committee headed by me as the top ranking project among the many studies that were presented.I congratulate the team comprising Anandhi Venkat, Ashvin Kumar Elangovan, Vignesh Kumar and Vijay Somanath and the mentor Jatinder Bedi for this outstanding study. I also take this opportunity to wish them all for many such original works.We hope this article would provide learners in analytics with a few ideas to do their own Capstone Projects and would have provided a glimpse of the program to those interested. You can read more details about the program here.This article was contributed by Anandhi Venkat, Ashvin Kumar Elangovan, Vignesh Kumar and Vijay Somanath and the mentor Jatinder Bedi and was done as part of their capstone project. They were part of the GreatLakes PGPBABI program in Chennai and finished their curriculum recently.",https://www.analyticsvidhya.com/blog/2016/07/capstone-project/
Solving Case study : Optimize the Products Price for an Online Vendor (Level : Hard),Learn everything about Analytics|Introduction|Case Study|Business approach to solve the problem|Analytical approach to solve the problem|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Tapping Twitter Sentiments: A CompleteCase-Study on 2015 Chennai Floods|Learning Path : Step by Step Guide for Beginners to Learn SparkR|
Tavish Srivastava
|43 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Solving case studies is a great way to keep your grey cells active. You get to use math, logic and business understanding in order to solvequestions. Do you find it exciting too ?Having received an overwhelming response on my last weeks case study, I thought the show must go on.Do check out the last weeks case study before solving this one.This case study is one of my favorite because of its real life implementation. The calculations which youll do in solving this case are the ones which often take place in real life. Therefore, its not just mathematical but practical also. For experienced job roles, similarcase studiesoftenappears in job interviews also. So, give your best attempt!The objective of this case study is to optimize the price level of products for an online vendor. Ihave over-simplified the case study to make it a similar platform for all, including people who have worked in similar industry. Ive solved this case study in two ways, using business approach & analytical approach (using R). The data set is available for download below.Are you preparing for your next data science interview? We have put together several such case studies as part of our comprehensive Ace Data Science Interviews course. Make sure you check it out!Suppose, you are the owner of an online grocery store. You sell 250 products online. A conventional methodology has been applied to determine the price of each product. And, the methodology is very simple  price the product at par with the market price of the product.You plan to leverage analytics to determine pricing to maximize the revenue earned.Out of 100000 people who visit your website, only 5000 end up purchasing your products. Now, all those who made a purchase, you have obtained their buying patterns, including their average unit purchased etc.To understand the impact of price variations, you tried testing different price points for each product. You gotastonished by the results. The impact can be broken down into two aspects:For instance, Product 1 might be a frequently used product. If you decrease the price point of product 1, then the customer response rate which was initially 5% goes up to 5.2% over and above the fact that the customer will purchase more of product 1.On the other hand, decrease in product price obviously decreases the margin of the product.Now, you want to find the optimum price points for each of the product to maximize the total profit earned. In this case study, you are provided with a table with all 250 items : Download DatasetFollowing are the variables available in the data set:Note: The maximum price hike permitted is 20%. So, basically the price of a product can be varied between -10% to +20% around the average price/unit.Snapshot of data setIf you make the calculation of the profit earned per customer who comes to your portal :Total Profit Earned : $165We will try to solve this case study both by a business approach and ananalytical approach. Lets do it!To solve the problem without applying any technique, lets analyze the frequency distributions.Profit Margin : Here is a distribution of profit margin for each of 250 products.Lets divide the products based on profit margin bands.Low Profit Margin: Less than 10% ProfitMedium Profit Margin: 10%  25% ProfitHigh Profit Margin: 25% + ProfitIncremental volume: Here is a distribution of incremental volume for each of 250 products.Lets categorize incremental volume bands:Low Incremental Volume: Less than 2%Medium Incremental Volume: 2%  6%High Incremental Volume: 6% + ProfitIncremental Acquisition: Here is a distribution of incremental acquisition for each of 250 products.Finally, we should split incremental acquisition bands also:Low Incremental Acquisition: Less than 0.1% ProfitMedium Incremental Acquisition: 0.1%  0.4% ProfitHigh Incremental Acquisition: 0.4% + ProfitLets discuss the pricing strategy now:Until here,we have got three decision attributes for each product. The extreme of the pricing is -10% and 20%. We need a -10% for products which can give a big acquisition increment or high volume increment or both. For rest we need to increase the profit margin by increasing pricing by +20%.Obviously, this is a super simplistic way to solve this problem, but we are on a track of getting low hanging fruits benefit.Because the incremental acquisition has a mean at 0.4%,we know that if we decrease the cost of products with high acquisition incremental rate, we will have significant incremental overall sales, but quite less impact due to less profit margins.For medium incremental acquisitions, we need to take decision on profit margins and incremental volumes. All the cells shaded in green are the ones that will be tagged for -10% and rest at 20% price increase. The decision here is purely intuitive taking into account the basic understanding of revenue drivers for which we have seen the distributions above.Now if we calculate the total profit earned, we see significant gains over and above the initial profit.Total Profit earned : $267 (Increase of 62%)Lets take a more analytical/numerical way to solve the same problem. Here is what we try in this algorithm :Here is the R code:Total Profit earned : $310 (88% incremental benefit)2 minutes silence for all those who questioned the power of data analytics! The data used in this analysis is derived from a simulation of prices of a grocery store. The best approach is generally a combination of both business and numerical method. I encourage you to try such algorithms on this data and share with us your approach and total profit incremental benefit.Did you like reading this article ? Do share your experience / suggestions in the comments section below. Id love to know your intuitive solutions to learn more ways of solving this case.",https://www.analyticsvidhya.com/blog/2016/07/solving-case-study-optimize-products-price-online-vendor-level-hard/
Learning Path : Step by Step Guide for Beginners to Learn SparkR,Learn everything about Analytics|Introduction|Step 1: What is Spark? Why do we need it?|Step 2: What is Spark R?|Step 3 : Setting up your Machine|Step 4 : Getting the Basics Right|Step 5 : Data Exploration with SparkR and SQL|Step 6 : BuildingPredictiveModels (Linear) on SparkR|Step 7 : Integrating SparkR with Hive forFaster Computation|End Notes,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathons to compete with many Data Scientists from all over the world.|Share this:|Like this:|Related Articles|Solving Case study : Optimize the Products Price for an Online Vendor (Level : Hard)|12 Free Mind Mapping Tools For a Data Scientist To Enhance Structured Thinking|
Shashwat Srivastava
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Lately, Ive been reading the bookData Scientist at Work to draw some inspiration from successful data scientists. Among otherthings, I found that most of the data scientists have emphasized upon the evolution of Spark and its incredible extent of computational power.This piquedmy interest to know moreabout Spark. Since then, Ive done an extensive research on this topic to come across every possible bit of information I could find.Fortunately, Spark has extensive packages for different programming languages. I think, being an R user, my inherent inclination to SparkR is justified.After I finished with theresearch, I realized there is no structured learning path available on SparkR. I even connected with folks who are keen to learn SparkR, but none came across such structured learning path. Have you faced the same difficulty ? If yes,heres your answer.This inspired me to create this step by step learning path. Ive listed the best resources available on SparkR. If you manage to complete the 7 steps thoroughly, you are expected to acquire intermediate level of adeptness on Spark. However, your journey from intermediate to expert level would require hours of practice. You knew that, right ? Lets begin!Spark is an Apache project promotedas lightning fast cluster computing. Its astonishing computing speed makes it 100x faster than hadoop and 10x faster than Mapreduce in memory. For large data processing, Spark has becomefirst choice of every data scientist or engineer today.You seeAmazon, eBay, Yahoo, Facebook, everyone is using Spark for data processing on insanely large data sets.Apache Spark hasone of the fastest growing big data community with more than 750 contributors from 200+ companies worldwide. According to the 2015 Data Science Salary Survey by OReilly, presence of Apache Spark skills added $11,000 extra to the median salary.To explore the amazing world of Spark in detail, you can refer this article.You can also watch this video to learn more about the value that Spark has added to the business world:However, if you more of a person who readstuffs, you can skip the video andcheck this recommendedblog.Interesting Read: Apache officially sets a new record in large scale sortingBeing an R user, lets channelize our focus on SparkR.R is one of the most widelyused programming languages in data science. With its simple syntax and ability to run complex algorithms, it is probably the first choice of language for beginners. But, R suffers from a problem. That is, its data processing capacity is limited to memory on a single node. This limits the amount of data you can process with R. Now, you know why does R runs out of memory when you attempt to work on large data sets. To overcome this memory problem, we can use SparkR.Along with R, Apache Spark provides APIs for various languages such as Python, Scala, Java, SQL and many more. These APIs act as a bridge in connecting these tools with Spark.For a detailed view of SparkR, this is a must watchvideo:Note:SparkR has a limitation. Currently, it only support linear predictive models. Therefore, if you were excited to run boosting algorithm on SparkR, you might have to wait until the next version is rolled out.If you are still reading, I presume that this new technology has sparked a curiosity in you and that you would be determinedto complete this journey. So, lets move on with setting up the machine:To installSparkR, firstly, we need to install Spark in our systems, since it runs at the backend.Following resourceswill help you in installation on your respective OS:After youve successfully installed,it justtakesfew extra steps to initiate SparkR , once you are done with Spark installation.Following resources will help you to initiate SparkR locally:Start with R: Though I assume that you would be knowing R if you are interested to work with Big Data. However, if R is not your domain, this course by data camp will help you to get started with R.Exercise: Install a package swirl in R and do the complete set of exercises.Database handling with SQL: SQL is widely used in SparkR in order to implement functions easily usingsimple commands. This helps in reducing the code linesyou have to write. Also, increases the speed of operations. If you are not familiar with SQL, you should do thiscourseby codecademy.Exercise:Practice 1 and Practice 2Onceyour basics are at place, its time to learn to work with SparkR & SQL.SparkR enables us to use a number of data exploration operationsusing a combination of R and SQL simultaneously.The most common ones being select, collect, group_By, summarize, subset and arrange.You can learn these operationswiththisarticle.Exercise: Do this exercise by AmpBerkleyDataset used in aboveexercise:DownloadAs mentioned above, SparkR only supports linear modeling algorithms such as Regression. However, its just a matter of time until we are facing this constraint. I am expecting them to soon roll out an updated version which would support non-linear models as well.SparkR implements linear modeling using the function glm. On the other hand, at present, Spark has a machine learning library known asMLlib (for more info on MLlib, click here), which supports non-linear modeling. Learn and Practice: To build your first linear regression model on SparkR, follow thislink.To build a logistic regression model, follow this link.SparkR works even faster with Apache Hive for database management.Apache Hive is a data warehouse infrastructure built on top of Hadoop for providing data summarization, query, and analysis. Integrating Hive with SparkR would help running queries even faster and more efficiently.If you want to step into bigdata, the use of hive would really be a great advantage for efficient data processing. You can install Hive by following the links given for respective OS:After youve installed R successfully, you can start integrating Hive with SparkR using the stepsdemonstrated in this video. Alternatively, if you are more comfortable in reading, this video is also available in text format on thisblog.For a quick overview on SparkR, you can also follow its official documentation.I hope that I have made the learning path clear enough to accelerateyour journey into data science using SparkR. SparkR is often being seen as an intermediate step to switch into Big Datausing R. I learned SparkR because I used to find immense difficulty in working on large data sets in R. SparkR providedme a convenient and cost free way to continue with my learning.In addition, for a R user, SparkR can also provide headstart to someone who wishes to transition into big data industry. Its is much powerful than I have explored yet.Did you find this article helpful ? Have you worked on SparkR ? Do share your suggestions / experience in the comments section below.",https://www.analyticsvidhya.com/blog/2016/06/learning-path-step-step-guide-beginners-learn-sparkr/
12 Free Mind Mapping Tools For a Data Scientist To Enhance Structured Thinking,Learn everything about Analytics|Introduction|What are Mind Mapping Tools ?|Whats in it for adata scientist ?|How tocreate a Mind Map ?  Practice Time!|List of 12 Mind Mapping Tools|End Notes,"The art of Structured thinking|XMind|Coggle|Freemind|Wisemapping|Mind42|LucidChart|MindManager|SpiderScribe|Bubbl.us|Freeplane|MindApp|Text2MindMap|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Learning Path : Step by Step Guide for Beginners to Learn SparkR|Operations analytics case study (level : hard)|
Analytics Vidhya Content Team
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let us start this with a simple exercise, the kind of which every data scientist faces regularly:You have been appointed as a store manager for our worst performing store. What are the possible factors / changes you would make in your store?Take a few minutes to think over this. Once you have written at least a few factors, lets move ahead.So, how was the exercise? Was it easy or difficult? How sure are you that you have written all possible factors as part of this thought capture?If you are not sure that you have captured most of the factors or your list looks like a simple to do list without any framework or structure, this article should help you tremendously.Everyonehasthe ability to think simultaneously in all directions. But, the ability to think and ideate in a structured manner is what makes a data scientist special.Structured thinking is a process of putting a framework to an unstructured problem. Having a structure not only helps an analyst understand the problem at a macro level, it also helps by identifying areas which require deeper understanding.Structured Thinking allows us to map our ideas in structuredfashion, thereby enabling us to identify which areas need the most attention.Sadly, people dont realize the importance of structured thinking or go back to their unstructured instincts as soon as they face an unstructured problem. When I give the problem mentioned above to people, more than 70% of them dont put a structure to it.In this article, Ive listed down someamazing mind mapping tools useful for a data scientists. These tools provide an excellent way ofgenerating ideasin a creative way. These tools will enhance your breadth of thinking and your ability to comprehend and retain more information. Once you have gone through this article, you can bring these tools in your work flow process.Note: Majority of the tools listed below can be availed for FREE. Links for access / download are provided. This article is not meantto promote any tool commercially.I too had this question when I first heard about these tools.So, the existence of these tools derive from the concept of mind-mapping.Mind Mapping isnt a new concept. Its been used for centuries now. Basically, its a form of creative thinking.Mind mapping is a process of creating mind maps usedto convey ideas and concepts in a visual form. Mind maps are nothing but a visual representation of your thinking in a structured manner. Sometimes, you might findyourself in a situation where you find it immensely difficult to convince people with your idea. Be it your manger, boss or an investor. Has it happened with you?Such situations occur when you idea fails to establish a visual impact in the mind of people at other end. Thats where mind mapping tools comes to your rescue. These tools use different colors, tree structures, pictures, animations to ensure that an idea is presented in the most comprehensible form.Good question!Among all the analysts, the idea of using mind mapping tool would appeal the most to people who are an ardent user of tree based algorithms such as decision trees, random forest, boosting etc.Actually, mind maps follows a tree based structured, visual representation of ideas. When I first created one, I felt like as if I am creatingtreenodes and terminal leaves of my ideas.These tools canhelp youin following ways:In short, a data scientist will be accomplish more in less time using this creativelyfastapproach.Creating mindmaps is easy. You can draw it even using a pen and paper. The general methodology is as follows:Write the main idea in the center. Draw branches from the center such they are connected with one another with final outputs shown towards the end.Lets create a mind map based on adata scienceproblem.Ive taken the problem from Big Mart Sales PredictionIII. The task is to predict the sales of products.Well create a mind map for hypothesis generation. Hypothesis generation helps us to understand the problem in detail by brainstorming possible factors which can impact the outcome.It is done byunderstanding the problem statement thoroughly and before looking at the data.You can read the problem statement after login at the competition page. Below is a simple mind map Ive created representing the possible factors which can affect product sales. Ive used coggletool (listed below).Lets understand it.So, Ive thought of possible factors on fourlevels: Store Level,Product Level, Customer Level and Macro Level. Let me quickly explain each factor:Store Level Hypotheses :Product Level Hypotheses:Customer Level HypothesesMacro Level HypothesesMind you, this is not an exhaustive list. These are just some basic 21 hypothesis I have made, but you can think further and create some of your own. After this step, Ill download the data and proceed with data analysis and predictive modeling stages.Im sure, after this activity you would have understood that the importance of a mind map lies in representing ones thoughts in a better manner (as shown above).Homework: Create a mind map of hypothesis on Loan Prediction Problem III. Share your mind maps on our twitter or facebook page. Best entry will receive a special gift hamper from AV.If you are feeling inspired, its time to get heads on with these tools.Most of the tools listed below are free. Ofcourse, youll get more features in the paid tools but these ones are good enough to get you started with this activity. You can choose any of them. Some of them are more simpler than other. To help you reach them, Ive listed their advantage in a short description.One of the most powerful open source mind mapping tools. It is used for brainstorming, manage complex information, and organize thoughts and ideas in different designs. You can choose different flow charts and add images if you want. It works great with multimedia and links. Works best for teams leaders where you can manage and organize it as you want.The best part is its absolutely free and available openly.Available for windows/Mac/Linux.Price: FREEEasiest and simplest mind mapping tool, just login with google and get started. You can drag and drop images on the diagrams, and it automatically creates mind maps. You can customize the colors and edit the flow charts as you desire. Also, keep a check on your change history and track your every move.You can export the files and share it on social media or mails.Its a freeware web application.Price: FREEFree mind mapping tool built in Java, its flexibility and performance differentiates it from other tools. It allows a user to create hierarchical set of ideas around the central concept. It offers complex diagrams and branches, graphics and icons to differentiate notes and connect with them. It has a wide variety of features, including location based mind mapping, collaboration tools, restore session support and more. Its available forwindows/mac/ linux.Price: FREEThis is a browser enabled tool used to create mind maps. Hence, you are not required to setup / install anything, just go to its website and get started. It includes various standard features such as sharing maps with friends, easy to embed in web pages, import the maps in other applications and its easy to do interface.Using the mind maps generated from wisemapping, you can create a connected network of ideas, words, thought to solve a bigger problem. The elements in this tool are arranged intuitively such that a user doesnt have to spend time in figuring out its controls and options.Price: FREEMind42 is supported with free, fast and easy ways to create mind maps and share with collaborators. It offers a setup which can be run in browser itself. Its a multitasking tool used for brainstorming, organizing events, todo lists, mind maps (obviously!) and a simple interface which can be understood in minutes.Using its simple interface, you can quickly create, modify, redesign all form of charts and structures required. The mind maps created with this tool can be easily imported by other applications such as Freemind and Mindmanager.Price: FREELucidChart provides an entirely new perspective to the process of creating diagram. No longer, you need to feel annoyed about inconsistent lines and shapes. This application provides asimple and sleek editor with built in collaboration features to create flowcharts, process maps, network diagrams using simple drag and drop moves.It can be easily integrated with applications like powerpoint, google apps, chrome, word and many more. This tool has received massive appreciation from top companies such as Mashable, Inc, Entrepreneur Magazine, PC World and many more.Price: Basic version is $4.95 /moMindManager from Mindjet is a powerful mind mapping tool, which enables users to visualize information in mind maps and flowcharts. Its a complete suite for project management, strategic planning, research accelerator and lets a user stay organized.Its not just a mind mapping software but lets a user to do much more than that. Its designed in a such a way that you can assign different arms of your project to different people,flesh out all of the individual to-dos and jobs required to make the whole project a success, and it works just as well if youre working with a hundred people, a dozen people, or just organizing your own to-dos.Price:$349 for full version and $179 for upgrade from 2014/2015Like others, using this tool also you can organize ideas, plan projects and connect different pieces of information in a structured manner. Needless to say, its interface is easy to use and understand. With the increased penetration of internet, the maps created are stored on cloud, thus you can access them from any corner of the world. Moreover, you can also create private maps, share your maps with friends and professionals to accomplish various goals.Price: FREE (Personal Plan)Brainstorming could never besimple without Bubbl.usMost of the brainstorming sessions end up with no conclusion. Why is that? Because, it has been seen that sudden stream of many times overwhelms the moderator of the session. The entire discussion time is spent in deciding the validity of ideas. It happens a lot. Hence, this tool is a must use for you if youve faced a similar situation.This tool will help you to connect and create a powerful story out of plethora of individual scattered thoughts. If anyone missed the session, no problem. You can share it with your colleagues as well. The work experience is amazing. Youll see that the tree automatically reposition itself to new ideas.Price: FreeFreeplane is an open source application available for download. Though, it doesnt have a fascinating UI, but to an extent it is unique in its own way. The ease and speed of creating mind maps, flow charts, notes, tree building supported by several automatic features makes it one of a kind. Features like easy integration of add-ons, approximate search, integration with literature suit, availability of tasking & reminders and many more, makes it one of the most used tool by people.For further support, you can also reach out to its community and forum.Price: FreeIf you are in a hurry and want to create a quick mind map or flow chart, mindapp would be your first choice. Though, the interface is quite ordinary, but the ease of accessibility makes it faster. This app is available for download for windows users. However, you can also access it directly in your browser.There are not enough features available like task manager, reminder, automatic node aligner etc. But, you can always use to deliver quick projects. It comprise of just enough options and features to create a decent flow of information.Price: FREEIts one of the besttool Ive used to create instant mind maps & designs. There is no need to login and create account. You can right away start in the browser without any hassle. Just add some text, use tab to align and with one click you can obtain your mind map. Once the mind maps are created, you can easily download them in PDF or image format. Moreover, availability ofkeyboard shortcuts to perform common tasks makes it faster than others. Sharing options are available. Other than FREE option, this tool can also be availed in mini, standard and pro version with higher features.Price: FREEThe idea behind this article was to introduce you to a vast ocean of resources which you might not have explored to enhance your structured thinking. If you are new to this concept, Id suggest you to try the FREE tools first. It would help you gain confidence in creating mind maps.The ability to channelize thoughts in a structured manner isbiggest blessing for any data scientist. Many a times, Ive seen that analyst lose their way while thinking onintuitive projects. Using these tools, youll be able to enhance your thinking ability and do more in less time.Did you like reading this article ? How was your experience working on these tools ? Did it help ? Do share your experience / suggestions in the comments below.",https://www.analyticsvidhya.com/blog/2016/06/12-free-mind-mapping-tools-data-scientist-enhance-structured-thinking/
Operations analytics case study (level : hard),Learn everything about Analytics,"Case study|Kick Start the process|Introduction to transportation problem|North west corner rule|Minimum cost method|Penalty cost method|Thinkpot|End Notes|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|12 Free Mind Mapping Tools For a Data Scientist To Enhance Structured Thinking|Bayesian Statistics explained to Beginners in Simple English|
Tavish Srivastava
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In previous few articles (beginner, intermediate and queuing theory), we have completed a variety of case studies used in operation analytics. One of which was based on assignment algorithm in call centers. Todays case study will be similar but we will now look at a more broader level instead of individual caller assignment. We will formulate this problem in a similar fashion as we try a transportation algorithm. While solving this problem, we will try multiple well known solutions to the transportation problem.You own a call center with 1000 resources. Each of these resources have gone through a different set of trainings. These training combinations are A,B,C,D,E and F. For instance set A is the population who have gone through training 1,2,3,4,5 and 6. Similarly, set B has gone through some other set of trainings. The distribution of these callers areA : 200B : 200C : 100D : 100E : 250F : 150Also, the calls you get on a daily basis have different types of queries. This can again be classified into 4 categories. Lets call them X,Y,Z and W. Each of these category can be identified through an IVR and you can try to allot any caller to these customers. On a daily basis you get following distribution of calls :X : 20%Y : 30%Z : 15%W : 35%The number of calls might vary with days but remains almost constant at this ratio. With every set of trainings, you get a different type of skill set which enables a caller to resolve varied type of calls in different time. For instance, a set A agent can resolve a call X in 10 minutes. Following is a grid, which you can refer to check the resolution time for each combination :Here is the catch! Your objective is to complete the work of all the callers in the least aggregate time. This will allow you to train them on different skills and therefore decrease the resolution time dynamically. Ultimately, you want a dynamic system which can do such an allocation on real time. However, for now you need to make the assignment so that you minimize the total time taken by all callers combined.As of now, the problem is open ended till you assume the actual inflow of calls to a certain number. To simplify things, lets assume that we have a balanced transportation problem in hand. Following is the distribution of calls :X : 200 callsY : 300 callsZ : 150 callsW : 350 callsTotal calls : 1000 callsHence, the number of agents and calls are exactly equal. Even if the total number of calls increase, our current solution will still be valid as the ratio holds true.This is a classic transportation problem where the time is just a substitute of cost. Here, we try to minimize the total cost by making the right set of assignments. We can either solve it through a simplex or a tabular solution. Transportation problems are generally solved in two steps :For this article, my focus will be to complete the first step. I will leave the second step for the reader and based on the response on this article, will publish the second part as well if required.Identifying a basic feasible solution can be done through 3 approaches :The process is very simple. We just assign the maximum possible values on the north west corner till we exhaust all the callers. We start as follows :Here we were able to make this assignment because both supply and demand is 200. However, we make an assignment of a value which is the minimum of two, which coincidentally is the same here. Now we have exhausted both the first row and first column. Hence we move to the cell (B,Y) and make the next assignment. Sequentially we make all the possible assignments.Now we calculate the total time which in this case comes out as 7550 minutes. To simply evaluate the results , we assign all As for X type of calls. We will divert around 33% of Y type calls to C type agents and 66% to B type agents & so on. This is generally the least optimal solution and we can get better results by the rest two methods.This method is slightly evolved version of the last one. Here we simply try finding out the smallest time which for a combination of caller-customer that are available. For instance, if we look at the entire cost matrix, we find the minimum time is for X customer being attended for caller D, hence we make the maximum assignment at this combination.Finally we arrive at the following assignments :The total time for this assignment now reduces to 3900 minutes which is almost a 50% reduction on the overall time.This is an even more evolved version of initial assignment procedure. Here we start with calculating the difference between the minimum and the 2nd minimum time for each row and column. This is basically the cost of not making an assignment in the current iteration. Following is our penalty cost table :
As we see the penalty cost is maximum for the caller types D. And we need to immediately assign the maximum value to caller D. Once done, we again recalculate all the penalty costs and move on with the assignment. Finally, following are the assignmentsThe total cost here is 4400, which is not better the last method. However, there is no guarantee of which algorithm wins in any iteration. It completely depends on the chances you break the tie in favor of lowering the cost or in opposite direction.Beyond this step, we now start our journey of optimization. This can again be done by multiple algorithms, namely  Stepping stone, MODI etc. However, you can progress on the simplex optimization to get the final solution. Once you have the final solution to this problem, mention the methodology and the final answer in the comment box below.Transportation problem is used widely in operation research but very rarely used in analytics industry. However, I have seen enormous number of opportunities which can be converted into such transportation problems. My motive of this article was to make you realize that how powerful these operation research tools can be, if we convert our problem in hand into such objective functions. Also do mention in the comment box below, if you would like me to further kill the problem of optimizing such transportation problem with the algorithms I have mentioned above.Did you like reading this article ? Do share your experience / suggestions in the comments section below. Id love to know your",https://www.analyticsvidhya.com/blog/2016/06/operations-analytics-case-study-level-hard/
Bayesian Statistics explained to Beginners in Simple English,Learn everything about Analytics|Overview||Introduction|Table of Contents|1. Frequentist Statistics|2. The Inherent Flaws in Frequentist Statistics|3. Bayesian Statistics|4. Bayesian Inference|5. Test for Significance  Frequentist vs Bayesian|End Notes,"3.1 Conditional Probability|3.2 Bayes Theorem|4.1. Bernoullilikelihood function|4.2. Prior Belief Distribution|4.3. Posterior Belief Distribution|5.1. p-value|5.2. Confidence Intervals|5.3. Bayes Factor|5.4.High Density Interval (HDI)|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Operations analytics case study (level : hard)|Web Analytics  Bangalore (5 -7 years of experience)|
NSS
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Bayesian Statistics continues to remain incomprehensible in the ignited minds of many analysts. Being amazed by the incredible power of machine learning, a lot of us have become unfaithful to statistics. Our focus has narrowed down to exploring machine learning. Isnt it true?We fail to understand that machine learning is not the only way to solve real world problems. In several situations, it does not help us solve business problems, even though there is data involved in these problems. To say the least,knowledge of statistics will allow you to work oncomplex analytical problems, irrespective of the size of data.In 1770s, Thomas Bayes introduced Bayes Theorem. Even after centuries later, the importance of Bayesian Statistics hasnt faded away. In fact, today this topic is being taught in great depths in some of the worlds leading universities.With this idea, Ive created this beginners guide on Bayesian Statistics.Ive tried to explain the concepts in a simplistic manner with examples. Prior knowledge of basic probability & statisticsis desirable. You should check out this course to get a comprehensive low down on statistics and probability.By the end of this article, you will have a concrete understanding of Bayesian Statistics and its associated concepts.Before we actually delve in Bayesian Statistics, let us spend a few minutes understanding Frequentist Statistics, the more popular version of statistics most of us come across and the inherent problems in that.The debate between frequentist and bayesian have haunted beginners for centuries. Therefore, it is important to understand the difference between the two and howdoes there exists a thin line of demarcation!Itis the most widely used inferential technique in the statistical world. Infact, generally it is the first school of thought that a person entering into the statistics world comes across.Frequentist Statisticstests whether an event (hypothesis) occurs or not. It calculatesthe probability of an event in the long run of theexperiment (i.e the experiment is repeated under the same conditions to obtain the outcome).Here, thesampling distributions of fixed size are taken. Then,the experiment is theoretically repeated infinite number of times but practically done with a stopping intention. For example, I perform an experiment with a stopping intention in mind thatI will stop the experiment when it is repeated 1000 times or I see minimum 300 heads in a coin toss.Lets go deeper now.Now, wellunderstand frequentist statisticsusing an example of coin toss. The objective is to estimate the fairness of the coin. Below is a table representing the frequency of heads:We know that probability of getting a head on tossing a fair coin is 0.5. No. of heads represents the actual number of heads obtained. Difference is the difference between 0.5*(No. of tosses) - no. of heads.An important thing is to note that, though the difference between the actual number of heads and expected number of heads( 50% of number of tosses) increases as the number of tosses are increased, the proportion of number of heads to total number of tosses approaches 0.5 (for a fair coin).This experiment presents us with a very common flaw found in frequentist approachi.e.Dependence of the result of an experiment on the number of times the experiment is repeated.To know more aboutfrequentist statistical methods, you can head to this excellent courseon inferential statistics.Till here, weve seen just one flaw in frequentist statistics. Well, its just the beginning.20th century saw a massive upsurge in the frequentist statistics being applied to numerical models to check whether one sample is different from the other, a parameter is important enough to be kept in the model and variousother manifestations of hypothesis testing. But frequentist statistics suffered some great flaws in its design and interpretation which posed a serious concern in all real life problems. For example:1. p-values measured against a sample (fixed size) statistic with some stopping intention changes with change in intention and sample size. i.e If two persons work on the same data and have different stopping intention, they may get two different p- valuesfor the same data, which is undesirable.For example: Person A may choose to stop tossing a coin when the total count reaches 100 while B stops at 1000. For different sample sizes, we getdifferent t-scores and different p-values. Similarly, intention to stop may change from fixed number of flips to total duration of flipping. In this case too, we are bound to get different p-values.2- Confidence Interval (C.I) like p-value depends heavily on the sample size. This makes thestopping potential absolutely absurd since no matter how many persons perform the tests on the same data, the results should be consistent.3- Confidence Intervals (C.I) are not probability distributions therefore they do not providethe most probable value for a parameter and the most probable values.These three reasons are enough to get you going into thinking about the drawbacks of the frequentist approach and why is there a need for bayesian approach. Lets find it out.From here, well first understand the basics of Bayesian Statistics.Bayesian statistics is a mathematical procedure thatapplies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data.You got that? Let me explain it with an example:Suppose, out of all the 4 championship races (F1) between Niki Lauda and James hunt, Niki won 3 times while James managed only 1.So, if you were to bet on the winner of next race, who would he be ?I bet you would say Niki Lauda.Heres the twist. What if you are told that it rained once when James won and once when Niki won and it is definite that it will rain on the next date. So, who would you bet your money on now ?By intuition, it is easy to see that chances of winning for James have increased drastically. But the question is: how much ?To understand the problem at hand, we need to become familiar with some concepts, first of which is conditional probability (explained below).In addition, there are certain pre-requisites:Pre-Requisites:It is defined as the: Probability of an event A given B equals the probability of B and A happening together divided by the probability of B.For example: Assume two partially intersecting sets A and B as shown below.Set A represents one set of events and Set B represents another. We wish to calculate the probability of A given B has already happened. Lets represent the happening of event B by shading it with red.Now since B has happened, the part whichnow matters for A is the part shaded in blue which is interestingly . So, the probability of A given B turns out to be:Therefore, we can write the formula for event B given A has already occurred by:or
Now, the second equation can be rewritten as :This is known asConditional Probability.Lets try to answer abetting problem with this technique.Suppose, B be the event of winning of James Hunt. A bethe event of raining. Therefore,Substituting the values in the conditional probability formula, we get the probability to be around 50%, which is almost the double of 25% when rain was not taken into account (Solve it at your end).This further strengthened ourbelief of James winning in the light of new evidence i.e rain.You must be wondering that this formula bears close resemblance to something you might have heard a lot about. Think!Probably, you guessed it right. It looks like Bayes Theorem.Bayes theorem is built on top of conditional probability and lies in the heartof Bayesian Inference. Lets understand it in detail now.Bayes Theorem comes into effect when multiple events form an exhaustive set with another event B. This could be understood with the help of the below diagram.Now, B can be written asSo, probability of B can be written as,ButSo, replacing P(B) in the equation of conditional probability we getThis is the equation ofBayes Theorem.There is no point in diving into the theoretical aspect of it. So, well learn how it works! Lets take an exampleof coin tossing to understand the idea behind bayesian inference.An important partofbayesian inference is the establishment of parameters and models. Models are the mathematical formulation of the observed events. Parameters are the factors in the models affecting the observed data. For example, in tossing acoin, fairness of coinmay be defined as the parameter of coin denoted by. The outcome of the events may be denoted by D.Answer this now. What is the probability of 4 heads out of 9 tosses(D) given the fairness of coin (). i.e P(D|)Wait, did I ask the right question? No.Weshould bemore interested in knowing : Given an outcome (D) what is the probbaility of coin being fair (=0.5)Lets represent it using Bayes Theorem:P(|D)=(P(D|) X P())/P(D)Here, P()is the priori.e the strength of our belief in the fairness of coin before the toss. It is perfectly okay to believe that coin can have any degree of fairness between 0 and 1.P(D|)is the likelihood of observing our result given our distribution for. If we knew that coin was fair, this gives the probability of observing the number of heads in a particular number of flips.P(D) is the evidence. This is the probability of data as determined by summing (or integrating) across all possible values of , weighted by how strongly we believe in those particular values of .If we had multiple views of what the fairness of the coin is (but didnt know for sure), then this tells us the probability of seeing a certain sequence of flips for all possibilities of our belief in the coins fairness.P(|D) is the posterior belief of our parameters after observing the evidence i.e the number of heads .From here, well dive deeper into mathematical implications of this concept. Dont worry. Once you understand them, getting to its mathematicsis pretty easy.To define our model correctly , we need two mathematical models before hand. One to represent the likelihood function P(D|) and the other for representing the distribution of prior beliefs .The product of these two gives the posterior belief P(|D) distribution.Since prior and posterior are both beliefs about the distribution of fairness of coin, intuition tells us that both should have the same mathematical form. Keep this in mind. We will come back to it again.So, there are several functions which support the existence of bayes theorem. Knowing them is important, hence I have explained them in detail.Lets recap what we learned about the likelihood function. So, we learned that: It is the probability of observing a particular number of heads in a particular number of flips for a given fairness of coin. This means our probability of observing heads/tails depends upon the fairness of coin ().P(y=1|)=  [If coin is fair =0.5, probability of observing heads (y=1) is 0.5]
P(y=0|)=[If coin is fair =0.5, probability of observing tails(y=0) is 0.5]
It is worth noticing that representing 1 as heads and 0 as tails is just a mathematical notation to formulate a model.We can combine the above mathematical definitions into a single definition to represent the probability of both the outcomes.P(y|)=This is called the Bernoulli Likelihood Function and the task of coin flipping is called Bernoullis trials.y={0,1},=(0,1)And, when we want to see a series of heads or flips, its probability is given by:Furthermore, if we are interested in the probability of number of heads z turning up in N number of flips then the probability is given by:This distribution is used to represent our strengths on beliefs about the parameters based on the previous experience.But,what if one has no previous experience?Dont worry. Mathematicians have devised methods to mitigate this problem too. It is known as uninformative priors. I would like to inform you beforehand that it is just a misnomer. Every uninformative prior always provides some information event the constant distribution prior.Well, the mathematical function used to represent the prior beliefs is known asbeta distribution.It has some very nice mathematical properties which enable us to model our beliefs about a binomial distribution.Probability density function of beta distribution is of the form :where, our focus stayson numerator. The denominator is there just to ensure that the total probability density function upon integration evaluates to 1. and are called the shape deciding parameters of the density function. Here is analogous to number of heads in the trials and corresponds to the number of tails. The diagrams below will help you visualize the beta distributions for different values of andYou too can draw the beta distribution for yourself using the following code in R:> library(stats)
> par(mfrow=c(3,2))
> x=seq(0,1,by=o.1)
> alpha=c(0,2,10,20,50,500)
> beta=c(0,2,8,11,27,232)
> for(i in 1:length(alpha)){
   y<-dbeta(x,shape1=alpha[i],shape2=beta[i])
   plot(x,y,type=""l"")
}Note: and are intuitive to understand since they can be calculated by knowing the mean () and standard deviation () of the distribution. In fact, they are related as :If mean and standard deviation of a distribution are known , then there shape parameters can be easily calculated.Inference drawn from graphs above:The reason that we chose prior belief is to obtain a beta distribution. This is because when we multiply it with a likelihood function, posterior distribution yields a form similar to the prior distribution which is much easier to relate to and understand. If this much information whets your appetite, Im sure you are ready to walk an extra mile.Lets calculate posterior belief using bayes theorem.Calculating posterior belief using Bayes TheoremNow,our posterior belief becomes,This is interesting.Just knowing the mean and standard distribution of our belief about the parameter and by observing the number of heads in N flips, we can update our belief about the model parameter().Lets understand this with the help of a simple example:Suppose, you think that a coin is biased. It has a mean () bias of around 0.6 with standard deviation of0.1.Then ,= 13.8 ,=9.2i.e our distribution will be biased on the right side. Suppose, you observed 80 heads (z=80) in 100 flips(N=100). Lets see howour prior and posterior beliefs are going to look:prior = P(|,)=P(|13.8,9.2)Posterior = P(|z+,N-z+)=P(|93.8,29.2)Lets visualize both the beliefs on a graph:The R code for the above graph is as:> library(stats)
> x=seq(0,1,by=0.1)
> alpha=c(13.8,93.8)
> beta=c(9.2,29.2)
>for(i in 1:length(alpha)){
   y<-dbeta(x,shape1=alpha[i],shape2=beta[i])
   plot(x,y,type=""l"",xlab = ""theta"",ylab = ""density"")}As more and more flips are made and new data is observed, our beliefs get updated. This is the real power of Bayesian Inference.Without going into the rigorous mathematical structures, this section will provide you a quickoverviewofdifferent approaches of frequentist and bayesian methods to test for significance and difference between groups and which method is most reliable.In this, the t-score for a particular sample from a sampling distribution of fixed sizeis calculated. Then, p-values are predicted. We can interpret p values as (taking an example of p-value as 0.02 for a distribution of mean 100) : There is 2% probability that the sample will have mean equal to 100.This interpretation suffers from the flaw that for sampling distributions of different sizes, one is bound to get different t-score and hence different p-value. It is completely absurd. A p-value less than 5% does not guarantee that null hypothesis is wrong nor a p-value greater than 5% ensures that null hypothesis is right.Confidence Intervals also suffer from the same defect. Moreover since C.I is not a probability distribution , there is no way to know which values are most probable.Bayes factor is the equivalent of p-value in the bayesian framework. Lets understand itin an comprehensive manner.The null hypothesis in bayesian framework assumes  probability distribution only at a particular value of a parameter (say =0.5) and a zero probability else where. (M1)The alternative hypothesis is that all values of are possible, hence a flat curve representing the distribution. (M2)Now, posterior distribution ofthe new data looks like below.Bayesian statistics adjusted credibility (probability) of various values of. It can be easily seen that the probability distribution has shifted towards M2 with a value higher than M1 i.e M2 is more likely to happen.Bayes factor does not depend upon the actual distribution values of but the magnitude of shift in values of M1 and M2.In panel A (shown above): left bar (M1) is the prior probability of the null hypothesis.In panel B (shown), the left bar is the posterior probability of the null hypothesis.Bayes factor is defined as the ratio of the posterior odds to the prior odds,To reject a null hypothesis, a BF <1/10 is preferred.We can see the immediate benefits of using Bayes Factor instead of p-values since they are independent of intentions and sample size.HDI is formed from the posterior distribution after observing the new data. Since HDI is a probability, the 95% HDI gives the 95% most credible values. It is also guaranteed that 95 % values will lie in this interval unlike C.I.Notice, how the 95% HDI in prior distribution is wider than the 95% posterior distribution. This is because our belief in HDI increases upon observation of new data.The aim of this article was to get you thinking about the different type of statistical philosophies out there and how any single of them cannot be used in every situation.Its a high time that both the philosophies are merged to mitigate the real world problems by addressing the flaws of the other. Part II of this series will focus on the Dimensionality Reduction techniques using MCMC (Markov Chain Monte Carlo) algorithms. Part III will be based on creating a Bayesian regression model from scratch and interpreting its results in R.So, before I start with Part II, I would like to have your suggestions / feedback on this article.Did you like reading this article ? As a beginner, were you able to understand the concepts? Let me know in comments.",https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
"Web Analytics  Bangalore (5 -7 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Bayesian Statistics explained to Beginners in Simple English|Winners of Mini DataHack (Time Series)  Approach, Codes and Solutions|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Web AnalyticsLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asWeb Analytics  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/06/web-analytics-bangalore-5-7-years-experience/
"Winners of Mini DataHack (Time Series)  Approach, Codes and Solutions",Learn everything about Analytics|Introduction|Winning Approach and Solutions|ImportantLearnings|End Notes,"Rank 3 :Sudalai Rajkumar, Chennai, India|Rank 2:Sailesh Mohanty, Kolkata, India|Rank 1: Surya Parameswaran, Chennai, India|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Web Analytics  Bangalore (5 -7 years of experience)|11 Must Read Books This Summer on Internet of Things (IoT)|
Analytics Vidhya Content Team
|36 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It takes sheer commitment and knowledge to build a predictive model in 3 hours.The motive of this competition was to make people think, decide and implement multitude of ideas quickly. Its the aha! factor whichmost companies seek in a data scientist. The ability of make justifiable and quick decisions can makeany candidate stand out for a job.More than 1200 participants from all over the world registered for this competition. Winnerswere chosen on the basis of RMSE score.Since time was limited, we decided to provide a relatively simple data set. A time series problem was given. The data set hadfewer variables. Therefore, participants got more time to focuson modeling techniques rather than data exploration.This battle of intense coding and machine learning algorithms continued for 3 hours.Winners took the smartest way of all. Below are the top 3 winners:Here is the final rankings of all participants: LeaderboardFor your learning purpose, below is the complete approach, solution and codes used by top 3 winners.Note: I would like to sincerely thank our winners for their immense cooperation and patience shown in sharing their competition experience.SRK says:In this mini-hack, I followed asimilar approach, Iusedin previous edition ofmini hack (explained here. This time also, my best modelwas a weighted average of XGBoost model and Linear Regression. Yes, linear regression is an under-dog but powerful team player.It was evident from the date variable that it contained a lot of information. So, I created new time based features:I used these features as input variables tothe model. These two models (XGBoost and Regression) did a fairly good job in capturing the variation in sales and the increasing trend.Then, I did data exploration, just to ensure that I dont miss out on visible patterns in the data. Interestingly, when I createda scatterplot of sales, I saw that there were two points way higher than the rest of the sales (about 70x of the median value). When probed deeper, I found out the dateswere 25th Dec 2007 and 24th Dec 2008. Also, beingthe last working day on both these years, I thought this higher sales could be due to:After this information, when I re-checked my model output, I found that this pattern (higher sales during Christmas) didnt get captured. So, I thought ofcreating a separate variable (a binary variable like Christmas flag) to capture this trend.But, due to time constraint I couldnt do it satisfactorily.So, I just did a manual correction for Christmas Eve sales and made the final submission. I am fairly sure that because of this last step, I ended up at 3rd position. Had I got few more minutes, I might have done better.In the end, it was an amazing learning experience. I learned that it is essential to do some data exploration even if we use powerful algorithms as sometimes they might fail to capture the obvious patterns.Solution: Link to CodeSailesh says:Unlike full fledged long hours hackathons, the key to crack a 3 hour mini hack is just being smartat handling the data.In 3 hour, you dont getthe luxury of trying out manydifferent approaches ( because youve limited time), so adopting asmart approach will giveyou a definitecompetitive advantage.But, Itried several methodsto deal with given time series data. After progressing through failed attempts, I finally found the model which helped me secure 2nd position. So, heres a quick review of my approach used:Method 1  As a no-brainer , I startedwith times series (decomposition and forecasting)conceptstocheck for trend and seasonality. Then, I eliminated the unsual trends to avoid biasness and built an ARIMA model. With this method, I became aware of thehidden patterns in the data. Though, the forecast values were pretty off target, but starting here did give me a base to improve upon.Method 2  After scrutinizing the data, I found out that the data had abnormally high sales on Christmas Eve and September (which I believe is due to festive season). Beyond these abnormal observations, the random fluctuations in the time series seem to be roughly constant in size over time. Therefore, it wouldnt be incorrect to describe the data using an additive model. Thus,I madeforecasts using simple exponential smoothing i.e. Holt Winters model. But again the results were unsatisfactory. Still, I kept trying.Method 3  The forecast package in R contains functions to make forecasts using Neural Networks with nnetar function. I tried it and gotslightly better results. Yet,I was still way down the leaderboard.Method 4  This time I thought of doing something drastically different.I eliminated the outliers, gave higher weight to recent data, generated a feature month and categorized itas high sale and low sale. Then, I generated a week day feature (weekends generally had more sales) and finallyused a simple XGBOOST model with random hyper parameter tuning using MLR package in R. This did the trick.It was great fun participating in this competition.As someone who has studied and learnt statistical and analytical concepts from IIT, IIM and ISI, I want to state that AVs blog, tutorials and competitions have beenof great help to understandstatistical conceptsbetter and tokeep up with the latest developments in the field. AVs competitions also draws significant interest from my batchmates here. Thank you for making them so interesting.Solution: Link to CodeSurya says:This was my first hack @ AV. I joined the hack pretty late and didnt have much time left. When I exploredthe data, I foundevidence of year on year trend and some seasonality (specially year end sales). Sales were also erratic at places.So, with limited time in hand, I decided to build a model usingexponential smoothing time series method.This helped me fetch the winning model.Had I started earlier, I would have ideally captured seasonal elements separately like weekly seasonal indices, holiday seasonal indices (using some generic holiday calendar) and the trend part. With the de-seasonalized data, I would have predicted daily forecast using any of the time series model and multiplied the seasonal and trend components to it.Overall I enjoyed the experience and look forward to participating in many more hacks to come.Solution: Link to CodeThe motive of this article is to make you familiar with simple & advanced techniques used in a time series problem. Here are the key takeaways from this article:It was a wonderful experience interacting with these winners and knowing about their secretive coding styles. Hopefully, you would be able to evaluate your hits and misses in this competition.Did you find this helpful ? Do share your competition experience and feedback in comments below.",https://www.analyticsvidhya.com/blog/2016/06/winners-mini-datahack-time-series-approach-codes-solutions/
11 Must Read Books This Summer on Internet of Things (IoT),Learn everything about Analytics|Introduction|List of Books on Internet of Things|End Notes,"The Second Machine Age: Work, Progress and Prosperity in a Time of Brilliant Technologies|Getting started with Internet of Things|The Silent Intelligence|IoT Disruptions: The Internet of Things  Innovation & Jobs|Meta Products: Building the Internet of Things|Everyware: The dawning age of ubiquitous computing|Trillions|Designing Connected Products|Learning Internet of Things|Big Data and The Internet of Things|The Design of Everyday Things|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Winners of Mini DataHack (Time Series)  Approach, Codes and Solutions|Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience)|
Analytics Vidhya Content Team
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine a world where your car texts you saying, You didnt close the back door properly. Please come and do it before its too late :DNot just your car, what if your refrigerator, AC, microwaveand all other digital devices become your every day companion?Such is the world, we humans have promised ourselves in near future. Undoubtedly, there is a long road to achieve this end state, but the work has already begun.The massive upsurge in the availability of data in last few years have fueled this passion for breakthrough innovation. The idea of making human life faster and comfortable is the essence of discoveringInternet of Things.This concept is fairly easy to understand. Let me explain it in one line:Internet of things means connecting things (digital devices) with internet. IoT is majorly driven by data, sensors and actuators. With basic programming skills, anyone can build variety of useful products.If you are curious to know more about it (Im sure you would), reading books is the best way to nourish ones curiosity. Therefore, Ive listed some of the best books to make you familiar with complete picture of Internet of Things (IOT).Note: This article is not intended to promote any book. These books have been selected on the basis of user reviews and extent of topics coverage. For your convenience, Ive provided amazon links to buy books.This book is written by Erik Brynjolfsson and Andrew McAfee. This bookdefines the sheer power of digital forces in near future. The world is yet to see the revolution brought by machines. It admits that the future will be dominated by connected intelligent machines. It talks about the influence of technological advancements on businesses around the world. The focus is kept on envisaging ways which will instigate progression in future economies. To make it interesting, the author discusses concepts like Moores law, artificial intelligence, digitization and many more.Available:Buy on AmazonWhat if you want to try to hand at building IOT products, but dont know programming ? This book will help you sail through ocean ofIOT concepts and provide amazing projects to work on. This book is written by Cuno Pfister. It demonstrate the methods used to connect sensors, processors and actuators over internet. Precisely, it teaches programming using .NET micro framework and Netduino Plus Board. For better understanding, Cuno has adapted a step wise approach to explain concepts. Also, the practice examples areprovided to facilitate practical understanding.Available: Buy on AmazonIts a must read book for anyone who feels curious about internet of things and think of making investments in IOT products. Not much has been said and revealed about this quantum leap of technology. But, this book is written to make every entrepreneur, executive, investor familiar with growth, possibilities and roadblocks in living with connected products. This book is written by Daniel Kellmereit and Daniel Obodovski.Reading this book will provide you a comprehensive outlook of the chances of living with connected devices in near future.Available: Buy on AmazonAmidst the excitement of living with connected devices, we fail to notice that IoT becomes a completely untapped domain to locate jobs and opportunities. Do you know from where should you start ? This book, written by Sudha Jamthe, empowers you with the knowledge require to get your first job in IoT domain. There is no doubt that companies across all industries are investing heavily in this concept, this book provides enough inspiration to seek IoT as your next career option.Available: Buy on AmazonThis book is best suited for people whose interest lies in consumer products and technology. Meta products are nothing but, our daily use physical products connected with internet. This book is written by Wimer Hazenberg, Menno Huisman and Sara Cordoba Rubino. It introduces you with the inherent conceptsofdesigning meta products , provides several ideas and shows a crystal clear picture of yet to come future. For practical understanding, authors have shared real life case studies at the end of every chapter.Available: Buy on AmazonThis book is written by Adam Greenfield.If youve imagined yourself in a world where the number of connected devices will be more than human population, this book will allow you to delve into the plausible future of human world. Everyware is nothing but we being surrounded by connected devices everywhere.Itmakes you understand the ubiquitous influence of computational devices on human lives.The author has aptly sharedhis invaluable thoughts on a world, driven by connected devices and highlights the issues and possibilities which can arise when the two world meets.Available: Buy on AmazonTrillions is a complete package to learn about past, present & future ofconnected devices. It is written by Peter Lucas and Joe Ballay. Trillions refers to the number of connected devices in near future. It is an insightful guide to prepare companies for future technology innovations and help them understand the real value ofoperating in information age. This thought provoking book is a must read for designers, entrepreneurs, professionals, executives and anyone who likes to walk ahead of time. It encompasses the disciplinary ideas from biology, economic, ecology to provide a deep enough overview of IoT world.Available: Buy on AmazonThis book is written by Claire Rowland, Elizabeth Goodman, Martin Charlier and Alfred Lui. We are preparing well to enter in the world of connected devices.But, designing those devices has its own multitude of challenges. If you ever want to contribute to growth of IOT by designing products, this is a good place to start. This book is best suited for people having experience in UX designing. It explains wide range of critical designs, challenges, user touch points and roadblocks in building great products. In short, its a hand book for designing connected products.Available: Buy on AmazonThis book is written by Peter Waher. Experiencing a change brought by someone is easy, but bringing that change is difficult.This book is a must read for people who want to create IOT products and change the world. Books listed above are good enough to make you familiar with the impact and applications of IOT devices. But, this book comprises of step wise tutorial based on Raspberry Pi. In this book, you will learn about popular protocols, network topology, scalability, communication patterns and much more.Available:Buy on AmazonThe staple food of IoT is data. In order to design IoT products, it is immensely necessary to learn the ways of handling, storing, building architectures to accommodate massively large amounts of data. This book is must read of enterprise managers, big data professionals to learn about the most effective ways of storing data and using that data for building products. In addition, it provides practical advises, list of challenges and risk factors which must be kept in mind while integrating big data withIoT products.Available: Buy on AmazonWhile this book is not about data or Internet of Things specifically, I believe this is must for people designing or creating new products (which should interest most of the people reading booksin IoT)This book is a must read for people who find product designing as a fascinating subject. To design successful products, its important that we learn about the tiny nuances which makes a product great or fail. This book is written by Don Norman. It consists of various insightful instances where youll be compelled to think twice before turning the page. It focuses on designing everyday things in the most efficient manner. You dont require programming skills to understand this book, its just a reservoir of useful ideas which can be helpful for anyone.Available: Buy on AmazonDisclosure:The amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission.After reading these books, youll realize that the real impact of data revolution is yet to be seen. Using data for building predictive models is just the tip of the iceberg. The real usage of personal data will be seen once connected devices get the life some of these books describe. That will be a world where humans will be busy engaged with personal devices and would have no time enjoy the natural scenery. Well, everything has a cost. Right ?What do you think about Internet of Things? Have you read any of these books? Do you think IoTis just a hype and will calm down in years to come? Do share your interesting viewpoints in comments.",https://www.analyticsvidhya.com/blog/2016/06/11-read-books-summer-internet-iot/
"Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|11 Must Read Books This Summer on Internet of Things (IoT)|9 Challenges on Data Merging and Subsetting in R & Python (for beginners)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Team Manager / Group Manager  BILocation GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asTeam Manager / Group Manager  BI  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/06/team-manager-group-manager-bi-gurgaon-8-12-years-experience/
9 Challenges on Data Merging and Subsetting in R & Python (for beginners),"Learn everything about Analytics|Introduction|Table of Contents|Challenge 1 : Adding more observations|Challenge 2 : Dropping Observations
|Challenge 3 : Adding Column(s) Horizontally|Challenge 4: Adding Column(s) based on common attribute
|Challenge 5 : Adding Column(s) based on observation serial (index)|Challenge 6:Removing Duplicate Observations|Challenge 7: Dropping Columns|Challenge 8: Modifying Value(s) of a DataFrame|Challenge 9: Renaming Column Name(s)|End Notes
","i) In Structured Data Set|ii) In Unstructured Data Set|Important Points||Question : Which of the candidate has the largest army ?|Question : List all the houses along with their military strength and the rightful heir|
Question : List the houses that have atleast one heir?
|Question : List all the available information about houses and heirs?
|Caution:|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Team Manager / Group Manager  BI  Gurgaon (8-12 years of experience)|Consultant  Amman,Jordan (6+ years of experience)|
syed danish
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Dropping rows based onconditions,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Juggling with multiple data sets is a common task fora data scientist. And, its immensely important fora beginner or intermediate to learn this skill.I got the idea of writing this article from the past data science competitions. Many a times, people end up getting undesirable NA values while combining or subsetting data sets. If it still happens with you, dont freak out. You just have to practice these challenges well and you wont get any NAsagain.After you finish these challenges successfully, it is expected that you will become proficient atmanipulating data frames, merging multiple data sets to perform several basic operations on data frame(s). For your convenience, Ive used R and Python to demonstrate the operations. Also, Ive given 4 practice exercises.P.S  For fun, Ive useda dummy data set from apopular TV Series named Game of Thrones. If you are crazy about it, great! If not, you will still find it easy to understand.Data set for every challenge is available for download here.Before starting with the challenges, make sure youve downloaded the data.We have adummy data setfrom Game of Thrones named house. It contains information aboutvarious clans (houses). Think of houses as families. Lets say, two new houses have emergedwhoseinformation is contained in the data set house_extra.Task: To include the houses in house_extra in the data set house (i.e merging the two data sets)You can make a row wise addition ofhouse_extra to house data set by using the following code :Python CodeR Code
#using base function merge
> house <- merge(house,house_extra,all=TRUE,sort=FALSE)The output will be :Now there might be a case where you have an unstructured data. Think of unstructured data as a data without any matrix or data frame. Still, could we add new observations to it?Suppose you have a new housein the house data frame. The new house is Redwyne which is present in The Reach region. Now, we want to add this new observation in our existing house data. Lets see how to do it.Python Code> house.loc[len(house)]=['Redwyne','The Reach']R Code> house <- rbind(house,data.frame(House=c(""Redwyne""),Region=c(""The Reach"")))The output will be :As you can see, a new house (Redwyne) has beenadded to the house data frame with its region.1. At times there are situations when we are required to add observations from a data set which has a new column than the existing one.Confused ?Lets do some changes in data set and understand this point:Now, the house data set contains two columns (House, Region). Anotherdata set,house_newcontains columns (House, Region and Religion). Keep in mind, Religion column is not available in other data set. We are asked to combine these data sets. How can we do?Lets find the solution.Python Code> house=house.append(house_new)
or
> house=pd.concat([house,house_new],axis=0)R Code> house <- merge(house,house_new,all=TRUE,sort=FALSE)We can see that it assigned NaN to the elements of house data frame because Religion variable was not present in the houseset.2.This point is only applicable for python users.Continuing frompoint1, after adding the new houses to the olddata set, we end up getting repeated index values. This is definitely a problem.Now, if we try to access the first element of the data frame, what will we get?Take a minute to think about it.> house.ix[0]The output will be:We seetwo elements in the output. Why is that ?This is because, aftermergingthe two data frames, the index of new observations havent changed according to the new data set. So, if we try to access the first element of the data frame, the result will besame as above. To handle this problem, we cantreat index also. It can be done by :#ignore_index does not takes the old index in consideration
>house_data=pd.concat([house,house_new],axis=0,ignore_index=True)Now the output after vertically adding the data frames will be :Exercise 1 : What will be the output of the append or concat operations on house and house_new if there is an extra variable present in house data frame?
Exercise 2 : Write the code for adding house data frame to house_new(house_new observations will be on top)?
Provide your answers in comments below.Dropping RowsSuppose we have a data set candidates which contains information about heirs (successors) of each house (family). They are sorted on the basis of age in descending order within the same house. (There is no order between the houses).Now we want to remove the top two rows of the candidates data frame :Python Code# 0 and 1 are the index of the rows we want to remove
> candidates=candidates.drop([0,1])R Code# Note: In R, index starts from 1 and not 0.
> candidates=candidates[-c(1:2),]Output will be :In the TV Series, Robb Stark was killed at a wedding. Since hes no more alive, he cant be an heir to the Stark House. We should remove him from the data set.Task: Prepare the new guest list with Robb Starks entry removed. This is how we do it :Python Code# We have taken all candidates except ""Rob Stark""
> candidates[candidates.Name!= ""Robb Stark""]R Code> candidates[which(candidates$Name!=""Robb Stark""),]Output will be :At times, a data set is provided in different files. Eachfilecontainssome unique information. We are requiredto merge them in such a way that we can extract maximuminformation.In such cases, how can wedecide what kind of merging technique we shouldapply?The answer is, it depends on the requirement of problem statement. Below are the different types of merge operations and insights on how to decide which merging technique to apply in varioussituations.Sometimes the problem statement is straight forward.Lets take this case. The data frames required tocombine areshown below. Military strength of each house is given in the same sequence as the sequence of houses in house data set. In such situation, we simply need to map the indexes with one another.To get more information about each houses military strength, well simply addmilitary data set to the house data set:Python Code> house=pd.concat([house,military],axis=1)R Code> house=cbind(house,military)The output will be:Wasnt that easy ? Actually, since the index alignment was similar thats why we were able to merge these two data sets but this is always not the case. In fact, this would rarely happen in any data science competition.Challenge 4 shows the realtrouble.Now, how to merge the data sets if their indexes are not aligned?In such situations, there is always a common attribute (key or keys) that aids us in combining data sets. But, you needto find the common attribute(s) present in the data frames. They could be column(s) or index(s).
Hint : Most of the times, common attribute will be some sort of ID. Keep an eye for it.There can be different ways to merge the data depending upon the type of question asked. Here we have the house data set and candidates data set. To show you different variations in applying these operations, I am going to solvedifferent questions and situations around the data sets.Think about the question for a while before diving into the solution.Just by looking at the heirs name or his/her house name from the candidates data set will not answer this question.To answer this question, we have to extract the information about military strength from their corresponding houses.How can we do that? Their index are not aligned, is there any common attribute between them?Yes, both the data frames have House column. We will now see how can we merge the above data set on the basis of a common column.Python Code> house = pd.merge(candidates,house,on=""House"",how=""left"")
#or
> house = pd.merge(candidates,house,left_on=""House"",right_on=""House"",how=""left"")R Code>house <- merge(candidates,house,by='House',all.x=TRUE,sort=FALSE)The output will be:By looking at the above data frame we can say that Daenerys Targaryen has the largest army. But, why did Jorah Mormont get NaN?There are a few things to notice here (refer above for codes) :Left merge is used here because we want information about the candidates. So, in the code above,candidates was the left data set and it uses House key from left frame only.Left MergeNote : I have used merge in headings and explanations. It is the general term that I will be using for combining or joining data set. Where as merge in the code is the syntax that is being used for merging, joining or combining.Now, we have to deliver information about each house. So, military strength is already present in house data set. All we have to do is to find the heir of each house who is present in candidates data set.In simple words, we want to extract candidates name from candidates data frame and place it with corresponding house. Lets see.Python Code> house = pd.merge(house,candidates,on=""House"",how=""left"")This will provide all the candidates from each house along with their military strength with older candidates at the top. In candidates data frame, an older member of a particular house is placed above the others. We can also perform a right merge operation to do the same thing.> house=pd.merge(candidates,house,on=""House"",how=""right"")R Code> house <- merge(candidates,house,by=""House"",all.y=TRUE,sort=FALSE)The output will be:Here how=""right""/all.y=True uses the key from right frame only.Right MergeSometimes we just need the part of the combined data where information from both data setis present . In simple words, we can say that we want the intersection between the available information or elements of house & candidates data frame.Rather than removing the observations having missing values we can directly merge the data set by using the following code :Python Code> merge_inner = pd.merge(candidates,house,on=""House"",how=""inner"")R Code> merge_inner <- merge(candidates,house,by=""House"",all=FALSE,sort=FALSE)The output will be:Here how=""inner""/all=FALSE uses intersection of keys from both frames.Intersection mergeWe want all data about the candidates and houses together, regardless of availability of information. In simple words, we can say that we want the union of all the information of house & candidates data frames.We can do that withfollowing codes :Python Code> merge_outer = pd.merge(candidates,house,on=""House"",how=""outer"")R Code> merge_outer <- merge(candidates,house,all=TRUE,sort=FALSE)Here how=""outer""/all=True uses union of key from both frames.Union MergeSometimes the common attribute or the key is index in both the data frames or index in one and a column in the other. We are going to handle a similarproblem where both the keys are index. Later, you have to find the solution for the other case in the exercise given. Suppose the data sets are :We can merge these two data frames by using the following codes :Python Code> house = candidates.join(house,how='left')
#or
#right_index and left_index enables the merging on the index.
#Here also how will have four options 'inner','outer','right' and 'left'
> house = pd.merge(candidates, house, right_index=True, left_index=True,how ='left')Join function is a convenient method for combining two data frames on the basis of index (by default). But, we can also merge if one of the keys is a column by using on parameter.R Code> house<-merge(candidates,house,sort=FALSE,all.x=TRUE)What if the data frames you are merging have a same column name other than the common attribute? What will happen?You can handle that very easily. Lets seeSuppose you have the following data frames :If you run the merge code, it will add some suffix by default (_x,_y) but you can add your own suffix by using the following code :Python Code> house = pd.merge(candidates,house, on='House', how='left', suffixes=('_left', '_right'))R Code> house <-merge(candidates,house,by=""House"",all.x=TRUE,
sort=FALSE,suffixes=c(""_left"",""_right"")The output will be :Exercise 3 : Comment the code that will left merge the following data set on the basis of House :Let me start this section with a question.Again, you have two data sets house & candidates as shown below. You are asked about the details of the next heir for each house with his/her military strength (in this case next heir will be the older child).Now, lets apply merge operation using the house key from candidates frame :Python Code#using left join
> heir = pd.merge(candidates,house,on=""House"",how='left')R Code> heir = merge(candidates,house,by=""House"",all.x=TRUE,sort=FALSE)The output will be :As you can see, we have repetition here which is not needed for the question I asked. You have to understand the structure of data sets and the method being applied for merging. Otherwise you can end up with a data set that you think is ready for analysis, but it is not the required one and can impact the output.Just take a minute and think about the possible solutions for this problem.To tackle this problem we can apply several methods. Here we are going to use the following method :1. Removing the duplicates : We can remove the redundant entries from the candidates data frame by keeping the first entry of the candidate from the top. We are using the first entry because candidates are sorted age wise in descending order. So, the older child will be at higher position in the data frame. Below are the codes to implement it :
#keep = 'first' will keep the first occurrence
#keep = 'last' will keep the last occurrence
#keep = False will drop all the duplicates
# make sure to use inplace=True to save the modified data framePython Code> candidates.drop_duplicates(subset=['House'],keep='first',inplace=True)R Code> candidates <- candidates[!duplicated(candidates[,1]),]Now we can merge the data frames :#In python
> pd.merge(candidates,house,on=""House"",how='left')#In R
> merge(candidates,house,by=""House"",all.x=TRUE,sort=FALSE)The output will be :As we can see Arya Stark from house Stark and Cersi Lannister from house Lannister are removed in the resultant data set thus providing the rightful heirs from each house.2. Aggregating the duplicates :Let understandthis usingan example.Suppose if there are two members from a house then each member will have the military strength shown separately. As given in housedata set, Starks have 20,000 soldiers. Then, both Arya Stark and Robb Stark gets20,000 soldiers individually.But, while forming the list of the rightful heirs from each house, the oldest heir getsthe first right to claim the throne / title. Hence, we dont need just one heir for a house.How can we remove this redundancy? By simply removing the repetition as done in previous method will end up getting us wrong data, thus we have to add up the military strength for the members of same house. For different situation this technique will differ.After merging the data sets, weended up with a data set consisting a large number of columns. Some of the columns are trivial or consist of information whichis already present in other variables (co-related).Suppose we have the data set :Thedata frame shown above has two similar kind of variables Region_left and Region_right. Out of these two Region_right is insignificant we want to remove this variable :Python Code# axis=1 will remove the mentioned columns
> merged_candidates = merged_candidates.drop('Region_right',axis=1)R Code> merged_candidates <- subset(merged_candidates,select=-c(Region_Right))The output will be :Modifying a particular elementOur work is not finished yet. There aresome irregularities that we still have to take care of. A data set might contain incorrectinformation. In our merged data set from last section candidate Arya Stark is in North region but this datashowsWestros as her region.We have to change the region of Arya Stark to North :Python Code# Arya Stark's region will be replaced from ""Westros"" to ""North""
>merged[merged['Name']==""Arya Stark""]=merged[merged['Name']==""Arya Stark""].replace(""Westros"",""North"")R Code> merged$Region_left[which(merged$Name==""Arya Stark"")] <- ""North""The output will look like :After this operation region of Arya Stark is changed to North.Modifying elements on the basis of a conditionIn our data set, region Westros is a more general term consisting of many kingdoms so we have to change all the entries Westros to Kings Landing. Lets see how to do itPython Code# All the Westros are replaced by Kings Landing
> merged.replace(""Westros"",""Kings Landing"",inplace=True)R Code> merged$Region_left[which(merged$Region_left==""Westros"")]=""Kings Landing""The output will be:We are just finishing up with all the challenges, lastly there might be a scenario where you want to change a column name. Lets say in our data set, we want to change the column name from Region_left to Region .Before changing the column name our data set was :Changing the column name using the following codePython Code> merged.columns = merged.columns.str.replace(""Region_left"",""Region"")R Code> colnames(merged)[which(names(merged)==""Region_left"")] <- ""Region""The output will be:If you have reached this line, I would like toapplaud you for the patience and persistence youve shown in traversing over these challenges. Ive considered all types of situations which could arise while merging, joining and subsetting data set. Hence, working on these challenges will make your knowledge comprehensive enough to deal with any situation.For best results, make sure you do these 9 challenges and 4 exercises given. If there is anything else, you think could be made better, feel free to drop your suggestions.Did you like reading this article ? Do you follow a different approach / package / library to perform these talks. Id love to interact with you in comments.",https://www.analyticsvidhya.com/blog/2016/06/9-challenges-data-merging-subsetting-r-python-beginner/
"Consultant  Amman,Jordan (6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|9 Challenges on Data Merging and Subsetting in R & Python (for beginners)|Exclusive Python Tutorials & Talks from PyCon 2016 Portland, Oregon|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  ConsultantLocation  JordanAbout employer  ConfidentialDescriptionResponsibilitiesQualifications and Skills requiredSalaryThe salary will be calculated based on the number of years of experience of the candidates and in accordance with UNHCR policy on Individual Consultants. For reference, a selected candidate with 15 years of experience will get a daily rate of 500 USD.Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Consultant  Amman,Jordan and the following details:",https://www.analyticsvidhya.com/blog/2016/06/consultant-jordan-6-years-experience/
"Exclusive Python Tutorials & Talks from PyCon 2016 Portland, Oregon",Learn everything about Analytics|Introduction|List ofWorkshops|List of Talks|End Notes,"1. Keynote by Guido van Rossum|2. Essential Data Science Skills For Every Programmer|3. The Fellowship of the Data|4. Computational Statistics|5. Cleaning and Prepping Data|6. Introduction to Python for Data Analysis and Visualization|7. Regular Expressions|8. Practical Network Analysis Made Simple|9. Diving into Machine Learning with Tensor Flow|10. Machine Learning with Text in Scikit Learn||11. Large Scale Data Analysis Tools in Python|12. Making an impact with Python NLP Tools|13. Faster Python Programs|14. Computational Geometry in Python|1. Beginners Guide to Deep Learning|2. Web Scraping and Data Analysis of NHL Penalties|3. IPython Notebook in Data Intensive Communities|4.Statistics for Hackers|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Consultant  Amman,Jordan (6+ years of experience)|Getting Started with Big Data Integration using HDFS and DMX-h|
Kunal Jain
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Workingwith Python has always been a good experience for me. Not just because of its easy code syntax, but due to itsphenomenalcommunity support. I must admit that Python has always surprised me with its extended industry use.PyCon Conference 2016was held between May 28th  June 5th at Portland, Oregon. It witnessedan amazingseries of python tutorial and talks. The panel of speakers were second to none. Also, the father of Python Guido Van Rossum delivered akeynotefocusing on stateof Python.For people like you and me, who couldnt attend this conferenceIve created this post so that you cantake part in this knowledge fest too.Be it a novice, intermediate or an expert python user, PyCon had something for everyones happiness. Be it in web development or data science. Topics like Bayesian Statistics, Deep Learning, Data Cleaning, Text Mining, Machine Learning gotdiscussed.From data science perspective,Ive listed the most useful tutorials from PyCon 2016. For your convenience, Ive also added a short summary of each video. Youll find tutorial in two forms: Workshops and Talks. Workshops are of longer duration involving detailed explanation of concepts. Talks are of shorter durations.Note: I would like to sincerely thank PyCon for generously sharing such enriching content from PyCon Conference 2016 @ Youtube.Duration  42:13 minsIt would be unfair if we dont devote some time and be all ears to thecreator of Python. As an entrepreneur, Ive always found more excitement in building products than just using them. Having createdsuch a magnificent product, Van Rossum shares his experience on driving the evolution of python programming language in near future. Also, he talks about Python 2.7, 3.5, 3.6 and various other developments which you as a python user must know.Duration  3:23:19 hrsThis is a must watch tutorial for anyone aspiring to learn python for data science. Its a beginner level tutorial. In this tutorial, Andy Terrel, will take you through hands on practice on data sets using Pandas, Scikit-learn and Pydata tools. Youll become familiar with data munging, modeling and methods to makepredictions. Towards the end of tutorial, youll also learn about building interactive data visualizations which can be deployed over web.Duration  1:52:12 hrsSo, we always talk about building predictive models in python on available data sets. Have you ever wondered about collecting data using Python ? Thats why Im in love with Python. It can do much more than you can imagine.This tutorial teachesvarious methods of collecting, storing and organizing data using Python. Good thing is, youd learn by doing. By the end of this tutorial, youd be able to collect, store and merge data in one pipeline using python.Duration  2:29:18 minsStatistics and Math are the two things which a data scientist must be good at. Hence, if you are one of the aspiring data scientist, you must watch this video.This tutorial will introduce you to the traditional yet powerful methods of statistics such as estimations, hypothesis testing, monte carlo simulations etc. More than theoretical explanations, the focus has been kept on learning withpractical exercises.Duration  1:55:07 hrs90% of the times, chances are you would get messy data sets for model building i.e. comprising of invalid values, missing values, outliers etc. As a data scientist, it is important to learn the skills of data cleaning and preparing a informative data set for model building. This tutorial is a must watch for beginners. In this tutorial, Renee adapts a step wise approach to demonstrate data cleaning in python. Be ready with your code editor, its a practical workshop.Duration  2:54:16 hrsData Analysis helps to discover underlying hidden trends in the data.Its an absolutely must watch tutorial for every novice in data science. Here, youll learn about the stepsinvolved in data analysis and ways to perform these stepsinPython. By the end of this video, youll get enough expertise to study and analyze small data sets.Duration: 1:42:00If you still struggle with using regular expression(as most of us do) in modeling, this beginners tutorial has to be your next halt. Trey Hunner (the speaker) will guide you through the basics of regular expression. By the end of this video, youd be able to build regular expressions on your own by working withpractice problems and discussions. Good thing is, there is no theory involved. The focus is kept on enhancing its practical understanding.Duration  2:40:13 minsIf youve always preferred to learn from applications rather than digging theories, this tutorial is must for you! Eric J. Ma demonstrates the use of network analysis in python. But, what is network analysis ? Network Analysis is simply a useful modeling tool widely used to map complex relationship. This concept is being extensively applied by Facebook, Google, Amazon in their recommender systems. Its an intermediate level talk. Make sure you are good with basics of python.Duration  2:55:05 hrsTensorFlow is an open source software library from Google which provides access to numerical computations using data flow graphs. This tutorial will guide you through the basics of machine learning to building a text classification model using TensorFlow. Along the way, youll understand about tensorflows working and how you can build and train model with its help!Duration  2:44:32 hrsSo, you are given a data set. You have numeric variables on which you can easily work. Along with, you have variables which comprises of text such as house address, product description etc. The knowledge of dealing with such variables can provide immense boost to your predictive model. In this tutorial, Kevin (Founder of dataschool.io) shares this knowledge using multiple practice examples. Its a practical workshop, hence be ready to reproduce codes at your end.Duration  2:54:41 hrsWith the growing demand of large scale data analysis, would you think python would stay back ? Broadly, this tutorial teaches you to handle big data in Python. Here youll learnthe basics ofusing Hadoop / MapReduce and Spark in Python. In the end, Sarah & Sean provide a hands on exercise for practical understanding of data analysis on large data sets.Duration  2:54:35 hrsThis tutorial is best suited for people having prior experience in string manipulation.In this tutorial, youll get familiar with a toolkit specially designed to work with text data in Python. Brands are pursuing the power of NLP to identify their customer sentiments on social media platforms, feedback forms for enhancing their brand perception. Hence, this concept is widely used in industry and must to know fora data scientist.Duration  3:03:58 hrsThis would interest you if you are comfortable coding in python and would like to reduce your computational time. Mike Mueller introduces some handy tips and tricks to optimize your python programs. For optimization, one would require knowledge of algorithms and data structures which also has been explained. The ability to write faster programs for creating quick visualization and models is driven by an optimization strategy. If you arent understanding here, may be youll do more while doing practical exercises with Mike.Duration  2:34:50 hrsIf you are interested in pursuing field like Robotics, Geo mapping, Astrophysics and more, this tutorial should give you a good headstart. In simple words, computational geometry is nothing a wayto solve a problem which are influenced by dimensions such asgeographical information, network building etc. Understanding this video requires deep knowledge of mathematics and related concepts.Duration  28:51 minsDeep Learning techniques have brought disruptive advancements in the field of data science. Be it learning from robots, images, speech or detecting anomalies, deep learning algorithms are widely known to solve complex data problems. This talk introduces you to concepts like convolutional nets, backpropagation, image recognition and restricted boltzman constant. Irene has used interesting examples from real life to set up deep learnings connection with human lives.Duration  30:19 minsThis is a good video to watch andunderstand the use of python, data analysis and web scrapingin real life. Wendy, used python to do web scraping and data analysis in order to compute results on NHL Penalties. In this video, Wendy follows astep wise approach, right from collecting data, analyzing it and generating useful insights. A lot has been said in previous videos though, a complete end to end overview is still missing above.Duration  30:01 minsThis talk encourages using ipython notebook for data science work. I myself prefer working on IPython Notebook rather than working in any other text editor. There are several benefits. I could state many. But, thats what these guys have interactively explained in the video. So, if you are a beginner or intermediate in data science and use python, this video willprovide you a fresh perspective about this tool.Duration  40:32 minsAnother must watch for novices. You can say, its a crash course on statistics using Python. In this talk, Jake will solve all confusions about the jargons like distribution, confidence interval, p-value, t-test by using computational methods like sampling, shuffling, simulation and cross-validation. And, he shares hisstrategies and approach to build a powerful statistical model.Just watching these videos wouldnt make you a better analyst. You need to practice too. For best results, you can take notes from the video. This will help you to quickly refer the topic at a later point in time.While watching these video, there were several moments when I felt, there are lot many things in Python which I am yet to explore. Once again I would like to thank python community for being so generous, helpful and always being helpful in time of need. If you would like to see more such videos from PyCon 2016, you can check out their Youtube channel.Did you find this list of tutorials and talks helpful ? Which tutorial or talk you like the most ? Share your experience/ suggestion in the comments below.",https://www.analyticsvidhya.com/blog/2016/06/exclusive-python-tutorials-talks-pycon-2016-portland-oregon/
Getting Started with Big Data Integration using HDFS and DMX-h,Learn everything about Analytics|Introduction|What is Hadoop Distributed File System (HDFS)?|What are the components of HDFSArchitecture?|What is MapReduce?|Using DMX-h for Data Integration|End Notes,"Advantages of MapReduce|MapReduce Process Flow|Task Editor Explained|JobEditor Explained|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community by posting your blog.|Share this:|Like this:|Related Articles|Exclusive Python Tutorials & Talks from PyCon 2016 Portland, Oregon|Quick Guide to Build a Recommendation Engine in Python & R|
Guest Blog
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The data researchersno longer depend only on interviews, surveys, observational studies to collect data. Instead, they have switched to thefaster ways of data collection which includes leveraging internet, cameras, smartphones, drones, bots and many more.Later, the collected data is used by organization/ governments to makebusiness decisions. But, before that, they requirea device or system which can store and secure such big data sets. One such system is Hadoop File Distribution System, commonly known as HDFS.If you are new to HDFS architecture, let me provide a simple picture of what it does:In simple terms, HDFS stores raw data is such a manner that the data iseasily extracted at a later stage. Data is stored in clusters to enable parallel mode of extraction. After the data is stored in HDFS, we use query languages (such as SQL) to extract the data and use it for analysis / modeling purposes.In this tutorial,we will look into basic concepts of Hadoop Distributed File System and the various components that make up HDFS. In the end, well learn about a data integration tool DMX-hfor practical understanding.The D in HDFS refers to breakingthe files into various subsets and storing it in different clusters. This provides much needed paceand scalabilityfor big data processing.HDFS is highly fault-tolerant andis designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applicationswhichhave large data sets.It has a well structured architecture and incorporates MapReduce technique for processing and distributing large data sets. Lets start with the architecture.There are 3 major components which supports HDFS architecture namelyClient machines, Master nodes, and Slave nodes. Lets understand each of them:Master Node:It oversees the two key functional pieces that make up Hadoop: storing lots of data (HDFS), and running parallel computations on all that data (Map Reduce).Slave Node: Itmakes up the vast majority of machines and do all the dirty work of storing the data and running the computations, hence the name Slave.Client Machines: Client machines are those which have hadoop installed with defaultclusters settings andassistin loading data and steers the complete cycle of data processing.Lets dive deep into nodes.Nodes in HDFS are made up of a two components: Master Node and Multiple Worker Nodes. The master node consists of a Job Tracker, Task Tracker, NameNodeand DataNode.A name node manages the file system metadata and data node store the actual data. A data node stores data in the [Hadoop File System]. A functional filesystem has more than one DataNode, with data replicated across them. A slave or worker node acts as both a DataNode and TaskTracker, though it is possible to have data-only worker nodes and compute-only worker nodes.Below are some important points which every big data / database analyst must know about HDFS:MapReduce is a processing technique and a program model for distributed computing based on java. The mapreduce algorithm contains two important tasks, namely Map and Reduce.Map takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs).Reducetakes the output from a map as an input and combines those data tuples into a smaller set of tuples.As the sequence of the name MapReduce implies, the reduce task is always performed after the map job.The major advantage of using MapReduce is that it is easy to scale data processing over multiple computing nodes.MapReduce can be used to perform intensive operations such as change data capture. Change Data Capture is a processing intensive methodology used to make current data available to users. Because, it is so processing intensive, it often makes sense to perform the processing on Hadoop as opposed to Teradata or other platforms.Below is the complete process flow of MapReduce technique:MapReduce  It steers the completeprocessing of data.Until here, we discussed about theoretical aspects of data integration, lets now practice it using DMX-hin order to develop even better understanding.One of the key functions that is involved working with Hadoop/Big data is ETL (Extract, Transform, Load). In addition, we still moving datafrom HDFS/Map Reduce outputs to, lets say, other traditional data warehouses, basically perform data integration.One of the tools that is available in the market today is called DMX-h from Syncsort .Its not an open source tool. Hence, for installation you need to create a one time account for installation here.There are 2 major components of DMX-h. They are:The DMX-h Task Editor provides the featureto create tasks that deal with movement of data. If you are an experienced professional, the interface incertain ways mightremind you of SQL Server Integration Services (SSIS).The following demo shotsexplain how data can be copied from source to target.1. Right Click on Source and Target to add the source and destination location of the files.2. Now move to target file. It simply means assigning a file to the target location.3. Once the source and target file locations have been assigned, the task is saved in the DMX-h Task Editor.Now, we move to the DMX-h Job editor where we can load all the tasks created in the DMX-h Task Editor. Then, we connect them according to the data transformation requirements.4. In the screen shot below, we load the Sample_File_Copy Task created in the Task Editor to the Job editor.5. Now, well run the job in the editorwith the results of execution of the job (for privacy purpose, username is masked).Once the job has been completed, we shouldverify if the source file has been successfully copied to the target location.Here I have demonstrated a simple example, this can be expanded to perform CDC operations and MapReduce Functionality. 6. The screenshot below showshow we can have different tasks (sort, split) in a job editor and also how we use the MapReduce component in the Job Editor.I hope this article provided you with the basic construct of HDFS and how tools like DMX-h can be used to perform different file operations on a Hadoop Cluster. In the end, I would like to thank Debra Aaron (from bank of America) for providing crucialinputs to the article.In this article, we discussed about the basics of HDFS & Mapreduce and its practical implementation using DMX-h tool. While writing this article, I was keen to understand the role of open source tools in Big Data. If anyone of you have any experience, I would love to interact in comments.About AuthorRamdas Narayanan is currently working as a Data Architect at Bank of America with focus on Business Intelligence and Data integration for Mortgage Technology. He has more than22 years of experience in Information Technology, ERP and Relational Database Systems.Did you like reading this article ? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/06/started-big-data-integration-hdfs-dmexpress/
Quick Guide to Build a Recommendation Engine in Python & R,Learn everything about Analytics|Overview|Introduction|Project to Build your Recommendation Engine|Problem Statement|Topics Covered|1. Type of Recommendation Engines|2. The MovieLens DataSet|3. A Simple Popularity Model|4. A Collaborative FilteringModel|5. Evaluating RecommendationEngines|Implementation in R|Projects||End Notes,"Case 1: Recommend the most popular items|Case 2: Using a classifier to make recommendation|Case 3: Recommendation Algorithms|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Getting Started with Big Data Integration using HDFS and DMX-h|Director / VP (Analytics)  Gurgaon (8  14 Years of Experience)|
Aarshay Jain
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Users|Ratings|Items,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This could help you in building your first project!Be it a fresher or an experienced professional in data science, doing voluntary projects always adds to ones candidature. My sole reason behind writing this article is to get your started with recommendation systems so that you can build one. If you struggle to get open data, write to me in comments.Recommendation engines are nothing but an automated form of a shop counter guy. You ask him for the product. Not only he shows that product, but also the related ones which you could buy. They are well trained in cross selling and up selling. So, does our recommendation engines.The ability of these engines to recommend personalized content, based on past behavior is incredible. It brings customer delight and gives them a reason to keep returning to the website.In this post, I will cover the fundamentals of creating a recommendation system using GraphLabin Python. We will get some intuition into how recommendation work and create basic popularity model and a collaborative filtering model.Quick Guide to Build a Recommendation Engine in PythonMany online businesses rely on customer reviews and ratings. Explicit feedback is especially important in the entertainment and ecommerce industry where all customer engagements are impacted by these ratings. Netflix relies on such rating data to power its recommendation engine to provide the best movie and TV series recommendations that are personalized and most relevant to the user.This practice problem challenges the participants to predict the ratings for jokes given by the users provided the ratings provided by the same users for another set of jokes. This dataset is taken from the famous jester online Joke Recommender system dataset.Practice NowBefore moving forward, I would like to extend my sincere gratitude to the Courseras Machine Learning Specialization by University of Washington. This course has been instrumental in myunderstanding of the concepts and this post is an illustration of my learnings from the same.Before taking a look at the different types of recommendation engines, lets take a step back and see if we can make some intuitive recommendations. Consider the following cases:A simple approach could be to recommend the items which are liked by most number of users. This is a blazing fastand dirty approach and thus has a majordrawback. The things is, there is no personalizationinvolved with this approach.Basically the most popular items would besame for each user since popularity is defined on the entire user pool. So everybody will see the same results. It sounds like, a website recommends you to buy microwave just because its been liked by other usersand doesnt care if you are even interested in buying or not.Surprisingly, such approach still works inplaces like news portals. Whenever you login to say bbcnews, youll see a column of Popular News which is subdivided into sections and the most read articles of each sections are displayed. This approach can work in this case because:We already know lots of classification algorithms.Lets see how we can use the same technique to make recommendations. Classifiers are parametric solutions so we just need to define some parameters (features) of the user and the item. The outcome can be 1 if the user likes it or 0 otherwise. This might work out in somecases because of following advantages:But has some major drawbacks as well because of which it is not used much in practice:Now lets come to the special classof algorithms which are tailor-made for solving the recommendation problem. There are typically two types of algorithms  Content Based and Collaborative Filtering. Youshould refer to our previous article to get a complete sense of how they work. Ill give a short recap here.We will be using the MovieLens dataset for this purpose. It has beencollected by the GroupLens Research Projectat the University of Minnesota. MovieLens 100K dataset can bedownloaded fromhere. Itconsists of:Lets load this data into Python. There are many files in the ml-100k.zip file which we can use. Lets load the three most importance files to get a sense of the data. I also recommend you to read the readme document which gives a lot of information about the difference files.Now lets take a peak into the content of each file to understand them better.This reconfirms that there are 943 users and we have 5 features for each namely their unique ID, age, gender, occupation and the zip code they are living in.This confirms that there are 100K ratings for different user and movie combinations. Also notice that each rating has a timestamp associated with it.This dataset contains attributes of the 1682 movies. There are 24 columns out of which 19 specify the genre of a particular movie. The last 19 columns are for each genre and a value of 1 denotes movie belongs to that genre and 0 otherwise.Now we have to divide the ratings data set into test and train data for making models. LuckilyGroupLens providespre-divided data wherein the test data has 10 ratings for each user, i.e. 9430 rows in total. Lets load that:Since well be using GraphLab, lets convert these in SFrames.We can use this data for training and testing.Now that we have gathered all the data available. Note that here we have user behaviour as well as attributes of the users and movies. So we can make content based as well as collaborative filtering algorithms.Lets start with making a popularity based model, i.e. the one where all the users have same recommendationbased on the most popular choices. Well usethegraphlab recommender functions popularity_recommender for this.We can train a recommendation as:Arguments:Lets use this model to make top 5 recommendations for first 5 users and see what comes out:Did you notice something? The recommendations forall users are same  1500,1201,1189,1122,814 in the same order. Thiscan be verified by checking the movies with highest mean recommendations in our ratings_base data set:This confirms that all the recommended movies have an average rating of 5, i.e. all the users who watched the movie gave a top rating. Thus we can see that ourpopularity system works as expected. But it is good enough?Well analyze it in detail later.Lets start by understanding the basics of a collaborative filtering algorithm.The core idea works in 2 steps:To give you a high level overview, this is done by making an item-item matrix in which we keep a record of the pair ofitems which were rated together.In this case, an item is a movie.Once we have the matrix, weuse it to determine the best recommendations for a user based on the movies he has already rated. Note that there a few more things to take care in actual implementation which would require deeper mathematical introspection, which Ill skip for now.I would just like to mention that there are 3 types of item similarity metrics supported by graphlab. These are:Lets create a model based on item similarity as follow:Here we can see that therecommendations are different for each user. So, personalization exists. But how good is this model? We need some means of evaluating a recommendation engine. Lets focus on that in the next section.For evaluating recommendation engines, we can use the concept of precision-recall. You must be familiar with this in terms of classification and the idea is very similar. Let me define them in terms of recommendations.Nowif we think about recall, how can we maximize it? If we simply recommend all the items, they will definitely cover the items which the user likes. So we have 100% recall! But think about precision for a second. If we recommend say 1000 items and user like only say 10 of them then precision is 0.1%. This is really low. Our aim is to maximize both precision and recall.An idea recommender system is the one which only recommends the items which user likes. So in this case precision=recall=1. This is an optimal recommender and we should try and get as close as possible.Lets compare both the models we have built till now based on precision-recall characteristics:Here wecan make 2 very quick observations:Now let us learn to build a recommendation engine in RStep 1: Importing the data filesStep 2: Validating the imported data filesOutputStep 3: Loading the train and test datasetStep 4: Validating the test and train datasetOutputStep 5 Building a simple Popularity ModelThe movies with the highest mean recommendations in our data_train data set:OutputAll the recommended movies have an average rating of 5, i.e. all the users who watched the movie gave a top rating. Thus we can see that our popularity system works as expected.Step 6 Building a collaborating filtering modelLets create a model based on item similarity as follow:Here we can see that the recommendations are different for each user. So, personalization exists. But how good is this model? We need some means of evaluating a recommendation engine. Lets focus on that in the next section.Step 7  Evaluating RecommendationEnginesLets compare both the models we have built till now based on precision-recall characteristics:ObservationsThere is a big scope of improvement here. But I leave it up to you to figure out how to improve thisfurther. I would like to give a couple of tips:In the end,I would like to mention that along with GraphLab, you can also use some other open source python packages like the following:Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your journey and use recommendation engines to solve these Practice Problems:In this article, wetraversed through the process of makinga basic recommendation engine in Python using GrpahLab. We started by understanding the fundamentals of recommendations. Then wewent on to load the MovieLens 100K data set for the purpose of experimentation.Subsequently we made a first model as a simple popularity model in which the most popular movies were recommended for each user. Since this lacked personalization, we made another model based on collaborative filtering and observed the impact of personalization.Finally, we discussed precision-recall as evaluation metrics for recommendation systems andon comparison found the collaborative filtering model to be more than 10x better than the popularity model.Did you like reading this article ? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/06/quick-guide-build-recommendation-engine-python/
"Director / VP (Analytics)  Gurgaon (8  14 Years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Quick Guide to Build a Recommendation Engine in Python & R|8 Reasons Why Analytics / Machine Learning Models Fail To Get Deployed|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Location: GurgaonDesignation:Director / VP (Analytics)Experience: 8-14 yearsWe are looking to hire a person who can lead our analytics strategy. This is a key role as the person would be defining the analytics road map for the organization. This would include:Ideally someone with 8  14 years of experience in banking analytics, with some time in risk analyticsInterested people can apply for this job by sending their updated CV to [emailprotected] with subject as Director / VP (Analytics)  Gurgaon and following details:",https://www.analyticsvidhya.com/blog/2016/05/director-vp-analytics-gurgaon-8-14-years-experience/
8 Reasons Why Analytics / Machine Learning Models Fail To Get Deployed,Learn everything about Analytics|Introduction|8 Reasons ForModel Deployments Failure|End Notes,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Director / VP (Analytics)  Gurgaon (8  14 Years of Experience)|Learning Path for Developers & IT Professionals to become a Data Scientist|
Tavish Srivastava
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dont be a data scientist whose models fail to get deployed!An epic exampleof model deployment failure is from Netflix Prize Competition. In a short story, it was an open competition. Participants had to build a collaborative filtering algorithm to predict user rating for films. The winners received grand prize of US$1,000,000. In the end, the complete model never got deployed.Not just Netflix, such dramatic events occursinmost of the companies.Recently, I have been talking with C-Suite professionals of many leadinganalytics companies. The biggest concern I hear about is 50% of the predictive models dontget implemented.Would you want to build a model which doesnt gets used in real world ? Its like baking a cake which youve tasted and found wonderful but would never be eaten by anyone.In this article, I have listed down all the possible reasons which you should keep in mind while building models. In my career, Ive faced such situations many a times. Hence, Ithink my experience could help you in overcoming such situations.1. High amount of false positive : This might seem a bit technical. Here, its important to understand what is false positive?In a classification model, assume that we want to predictwhether the customer is a responder (one who give answers) or a non-responder (one who doesnt). Imagine that you predict that a person X will be a responder but in realityhe does not respond. Person X in this case is called a false positive. So how does this effect in real world ? I knew you would ask this question.Lets take an example. For instance, you have given the responsibilityto build aretention campaign for 1000 customers. Out of these 1000, 100 customers will actuallyattrite (leave). You create an amazing model which has a 4X lift on top decile (10 equal large subsections). This means, out of your top 100 predicted customers, 40 customers will actually attrite. So, you recommend the business to target all these 100 customers with a fascinating shopping offer which canstop them from attriting. But, heres the challenge. The challenge is that for every dollar you spend on each of these customers, only $0.4 get used to stop attrition. Rest $0.6 just go to false positivecustomers who were really not in a mood of attrition. This calculation will some times make these models less likely to be implemented as a result of negative P&L (Profit & Loss).2. Low understanding of underlying models with business : Lately, there has been a rising requirement of using machine learning algorithms and more complex techniques for model building. In other words, companies are drifting away from using traditionalmodels techniques.Undoubtedly, using ML techniqueslead to an incremental power of prediction, but the businesses are still not very receptive to such black box techniques. In my experience, this leads to a lot longer lead time for a predictive strategy to get implemented. And as most of the applications in business are highly dynamic, the model become more and more redundantwith higher lead time.   3. Not enough understanding of thebusiness problem: Predictive models are good for resumes of both analyst and the business counterparts. However, that is not the purpose of the model you would build. In some cases, analyst run into creating model phase and try to cut down the time that should have been allottedto understanding the business problem.4. Too complex models for implementation : Predictive power of models is the soul of these exercises. But in general, predictive power comes at a cost of complexity of models. We start bringing in bivariates and tri-variates to make models stronger, even when these variables make no sense as per business. Such models might be amazing in books, and hence, they just stay in these books and never see the actual light of real-world.5. Not addressing the root cause just trying to improve the effect of a process : Why do we make models? The most important reason is to find the drivers of a particular response. What are these drivers? Drivers are always the root cause of response rate. What will happen if you bring in all the effects as the input variable and these variables also come out as significant? It will hardly be of any use as you are not changing things that can really bring changes.Source: ThinkReliability6. Training population significantly different from Scoring Population : In many cases, we end up creating models on a population which is significantly different from the actual population. For instance, if you are creating a campaign targeting population and you have no previous similar campaign. In such cases we start with the basic assumption that a population with high response rate might also have a high incremental response rate. But this assumption is rarely true and hence the model would behardly used.7. Unstable models : High performing models are often highly unstable and do not perform at par with time. In such cases, business might demand high frequency model revision. With higher lead time in model creation, business might start going back to intuition based strategy.8. Models dependent on highly dynamic variables : Dynamic variable are those variables which bringin the real prediction power to the model. However, you might have a culprit variables which might bring in such values which have never been seen in training window.For instance, you might getnumber of working days as a significant variable to predict monthly sales of a branch. Lets say this variable is highly predictive. But for our scoring window, we have months which have just 10-15 working days. If your training data does not have any such month, your model might not be capable of making this prediction accurately.I believe if we do understand these challenges, we can think of better ways of not getting entangled with such catches. Also, knowing them makes us see what business is really looking out for. I welcome you to append this list which will make our analysis more comprehensive and take the team a step ahead.Did you like reading this article ? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/05/8-reasons-analytics-machine-learning-models-fail-deployed/
Learning Path for Developers & IT Professionals to become a Data Scientist,"Learn everything about Analytics|Introduction|Self-assessment: Asummary of where you stand|Where should you start ?|Mistakes You Should Avoid While Learning|Finally, a peek into the Life of Data Scientists at Companies|End Notes","Step 1: Getting your machine ready|Step 2: Getting used to solving ambiguous problems (i.e. DevelopingStructured Thinking)|Step 3: Basics of Math & Statistics|Step 4: Basics of new tools (R & Python)|Step 5: Get your hands dirty  Your First Project|Step 6: Follow the steps in learning path|Step 7: Start participating in the competitions|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|8 Reasons Why Analytics / Machine Learning Models Fail To Get Deployed|Infographic: 16 Genius Minds Whose Inventions Made Data Science Easier For Us|
Kunal Jain
|39 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This guide to meant to help web developers, software engineersand other IT industry people totransitioninto analytics / data science industry.Last week, I was taking a guest lecture with one of the well known institutes in India. Rather (un)surprisingly, more than 60% of the studentscomprised of experienced IT Professionals. Most of them are facing a common problem, I have been in IT / software / web development for more than a few years and want to up-skill myself in analytics. I have taken a fewMOOCs and have tried using a few books / platforms. Still, I dont get it what should I do next?This scenario is not very different from several learnups / meetups we have conducted over the last year. In order to help these people as much as I can,Ive created this comprehensive career guide to get you quickly started with data science.Onceyou finish reading this post, you would know thenext steps in making a transition.People working in IT industry are generally comfortable with coding, working with databases and using frameworks. After spending a few years as a developer, you would know at least some languages like Java, ASP.net, Javascript, C++, C, HTML, Python, PHP, and you would have worked with several databases including SQL, MongoDb, Oracle etc. With these skills, you are envied bypeople trying to transition from non-programming background. Ask them, how they feel about it!Let me explain the set of skills (eventually advantages) which I expect every good IT professional tohave:On the other hand, you can be fooled to believe that data science is about learning a few more tools. Just like in programming, knowing a few languages or frameworks would not make you a good software engineer.What differentiates a good analyst from a bad one is the problem solving and structured thinking skills. Tools are just a way to implement this thinking. Hence, I recommend people to just pick one tool depending on their convenience and then focus on getting hands on experience.Here are the areas you need to focus on going forward:The problem in todays world is the problem of plenty. I am sure you couldnt agree more. Try searching for resources on Internet for R / Python / Data Science and you end up with a long list of resources. Talk to a few people who have made the transition and they would add a few more resources, which worked for them. If you are an avid reader, you can add a few books and blogs over and above this. Check out the platforms offering MOOCs and you can see a few good courses.The sad part is that while you have access to plenty of resources today, you find it difficult to find your way through these resources. Hence, I have created this learning path:The Hardware:Data Science is computationally intense (this should not be a news to you!). So, the first thing you should do is to set up a machine which helps you in your learning. Ideally, I would say that any machine used for serious data science work should have at least 8 GB RAM and an equivalent of i5 / i7 intel chips. Of course, the higher capacity you buy, the better it is! If you have some more money to spend at this stage, you can even get a SSD upgrade.The Operating System:Ironically, there is no single OS which works perfectly for Data Science. You would likely need a mix of Unix and Windows machine. Unix is better in resource management and for performing the data science work. On the other hand, you would need Windows for Powerpoint and Excel, both of which are used very heavily in data science work flows.Also, there will be a few visualization tools, which work better in Windows environment. Hence, I would recommend to use Linux as the core OS with a virtual machine running Windows or vice versa. If you are used to Mac and can work comfortably on Excel on Mac, you might be good too.The Softwares:You would need to choose the language / tool of your preference here. If you have experience in coding with Object oriented languages, I would recommend Python. It is easy of learn and has a vibrant community on internet. If you arent used to object oriented programming, you can give R a try as well. If you need more details before making a decision, read this comparison  SAS vs. R vs. PythonHere is a list of softwares, I would recommends at the minimum:If you have chosen Python as your preferred tool:If you have chosen R as your preferred tool:Your machine is ready to crunch some numbers now!The art of structure thinking is a tacit requirement but profoundlybeing sought in every data scientist. Otherwise, below are some good resources to enhanceyour skill.Assignment:Solve this case study on operational analytics: Call Center Optimization.Its a beginner level assignment. Once you complete it successfully, you can move to medium and advanced level.Think you are ready for the next level, try out our practice problem on Strategic thinkingMathematics plays an important role in defining data science. Thankfully, you dont need to learn all of math, just a few topics would do. You can start from the basic topics (marked mandatory) and pick up the rest of the topics as you progress:Assignment: Do a statistical analysis on Big Mart Practice Problem. After you have finished with this assignment, you can showcase it on your LinkedIn profile as project work.After step 3, you should do programming. Coding in data science is laconic in nature. Best practitioners avoid redundant lines of code and adopt ways to make it faster. Your prior knowledge of programming basics, should give you a nice head start in solving practical problems using R or Python.Get used to the basics of R / Python from any of the following introductory courses:Apart from these 2 tools, you can also use Julia, Go, Java to build predictive models. However, a possible drawback with other programming languages is the lack of community support. Till now, Python and R have the best community support on web. Thus, would help you to debug issues and learn faster.Assignment: Already given the practice links above.Time to get your hands on your first project. Like programming, the best way to learn data science is to do data science. Hence, let us start by taking up a problem to work on. You can choose any of our Practice problemsor any of the projects mentioned here to start with. Perform an exploratory analysis on the data to get you started.Here are a few good places to look at:Assignment: Perform a similar analysis on the project of your choice.Now that you have tasted blood, go for the kill! Check out our learning paths on R & Python  follow them step by step. Skip the steps you are comfortable with. Do as many exercises as you can!Here are some additional machine learning resources you can look at. Remember that the best way to do data science is to learn data science thoroughly:Assignments: Build a machine learning model on Loan Prediction Problemusing the following algorithms:For Decision Tree and Random Forest, you can seek help from: Complete Guide on Tree Based Modeling.Make sure you understand how each of these algorithms works. Just implementing them and obtaining predictions wouldnt be a success for you. The real success lies in gaining knowledge of how they work!Time to step in battle ground. A benefit of being a part of analytics community is that you get to access so many thrilling ways of learning concepts. You no longer need to stick to traditional ways of learning.Competitions:Several data science competitions get organized across the globe where you can participate and win prizes too. After youve completed above steps you must participate in these competitions to assess your learning level.Assignment: In 6 months  1 years, try to rank in top 100 in the competition rankings on both websites. This would give a massive boost to your profile.We are prone to make mistakes (unknowingly)in pursuit of learning conceptsquickly. Nothing to worry, we all are susceptible to such things. But, we need to be prudent enough to analyze our learning pace and proceed accordingly. Below are the list of mistakes ( bad practices) you should avoid while completing this learning path:Just for somemotivationI hope this guide will help people from IT / software development background to take up data science / machine learning as a career option. In summary, rely on your strengths, focus on developing structured thinking & problem solving, practice a lot and get your hands dirty on as many real life problems as possible. In the process, if you get stuck, leverage the communities and people in your network to help you out.As usual, if you have any questions or suggestions I might have missed out on, feel free to reach out to us through the comments below.",https://www.analyticsvidhya.com/blog/2016/05/learning-path-turning-web-developer-professional-data-scientist/
Infographic: 16 Genius Minds Whose Inventions Made Data Science Easier For Us,Learn everything about Analytics|Introduction,"You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Learning Path for Developers & IT Professionals to become a Data Scientist|Data Scientist  Delhi NCR (4+ years of experience)|
Analytics Vidhya Content Team
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you know that the concept of Regression wasinventedalmost 2 centuries ago ?Neither did I, until I decided to step intothe glorious history of analytics. Yes! some of the concepts which we use today were invented centuries ago. In my journey of looking at the past, I came across people who were true genius.Their innate desire to share knowledge and challenge concepts has shaped the future for rest of the world.After reading their life stories, there is one thing I found common inall these souls. That is, all these people weredriven by the power of questioning. i.e. they never settled with that they had.Instead, they showedinterested in knowing why does something happen as it happen? Probably, this intense curiosity made them invent things.Ive listed these 16 genius minds whose inventions hold immenseimportance in the 21st century. Be it Python, R, SQL or Hadoop, their inventions has given livelihood / jobs to million of people around the world. If you are inspired by their work, may be you can send them a tweet if they exist.Note: This isnt an exhaustive list. If youve been inspired by the great work of other people, please share about them in comments.",https://www.analyticsvidhya.com/blog/2016/05/infographic-16-genius-minds-inventions-data-science-easier/
"Data Scientist  Delhi NCR (4+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Infographic: 16 Genius Minds Whose Inventions Made Data Science Easier For Us|A comprehensive beginners guide to start ML with Amazon Web Services (AWS)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation Delhi NCRAbout employer  ConfidentialDescriptionWe are a multinational business process and information technology services company. We are a global leader in designing, transforming and running business process operations, including those that are complex and industry-specific.Now, we are in process of re-defining the use of big data and machine learning for managing human resources in a company. Hence, we are looking for a curious data scientist, who can find his / her way through big data.The person would be responsible for setting up an HR analytics practice, proving the value to the organization and scaling it up. The person should be a thought leader, open to try out new approaches and ready to work on problems, not many have solved.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Delhi NCR and the following details:",https://www.analyticsvidhya.com/blog/2016/05/data-scientist-delhi-ncr-4-years-experience/
A comprehensive beginners guide to start ML with Amazon Web Services (AWS),Learn everything about Analytics|Introduction|Table of Contents|1. What are AWS Instances?|2. How to Configure and Launch an Instance?|3. How to make a first model on anAWS instance?|End Notes,"1.1 Instances and AMIs|1.2 Regions and Availability Zones|1.3 InstanceTypes|Step0: Login to Your AWS Account|Step 1:Choose an Amazon Machine Image (AMI)|Step 2: Choose an Instance Type|Step 3: Configure Instance Details|Step 4: Add Storage|Step 5: Final Launch|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Scientist  Delhi NCR (4+ years of experience)|Solve Interview Case Studies 10x Faster Using Dynamic Programming|
Aarshay Jain
|27 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",1.3.1 ECU (EC2 Compute Unit)|1.3.2 Burstable Performance Instances|1.3.3 Amazon EBS (Elastic Block Store)|1.3.4 Dedicated Instances,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Learn to connectAWS instance with your laptop / desktop for faster computation!Do you struggle with working on big data (large data sets) on your laptop ? I recently tried working on a10GBimage recognition data set. But, due to the limited computational power of my laptop, I couldnt proceed further. I was determinedto solve this problem, andthankfully, in few hours, I managed to set up a 24GB machine on AWS for FREE and got improved results.I got it for FREE because I used the trial version with limited features, just to see how fast could it work. To my surprise, it was blazing fast.I am sure there would be many more like me, who just dont know how to deal with big data sets on their laptop. In fact, mostof us think of buying a new powerful laptop when faced with this hurdle. But, computing on AWS platform is quite cheaper. Trust me!Just to help you deal with this problem, Ive shared the step by step process I used to set up connect 24GB AWS instance with my laptop. To show how it works, Ive also demonstratedsome computations on iris data set in Python. For R users, everything remains same except on line of code which Ive highlighted as well.Note: This is not a promotional post. I do not intend to publicize AWS in analytics community. This post is meant for people who face trouble working on large data sets and wish to work faster.Amazon EC2 (Elastic Compute Cloud) instances provide access to scalable computing resources onthe cloud. This alloworganizations and individuals to stayaway from setting up and maintaining hardware on site. You can use it just like your own laptop but get your dream configuration for a cheaper bargain with the option of varying it as you need in a pay-per-use setting. Amazon EC2 allows you to manage your security, networking and storage as desired.Before moving forward, lets try and understand some of the terminologies used by AWS. Thiswill make it easier forto use the interface and the platform.As described by AWS, An AMI (Amazon Machine Image) is a template that contains a software configuration (for example, an operating system, an application server, and applications). You can understand it as a high level definition of your system. Different AMIs can be understood as different categories of systems.For instance, Windows OS vs Linux OS. But this is high level definition and not sufficient to completely define the hardware. So, we go to the next level and define an instance which is just a copy of an AMI but with granular details specified which are required for a system, such asthe RAM, memory storage, number of cores, etc.You can run many instances of same or different types of AMIs. The limit typically depends on your price-plan which can be configured on individual basis. You can find more details here.Also, some AMIs are predefined by AWS but you can also define your custom AMI. For instance, if you have a website then you can define your own AMI containing your website codeandstatic content so that it is easy to fasterworking on a new instance. Some AWS developers have also developed some custom AMIs which can be purchased from AWS stores.Amazon data centers are location globally at multiple locations. Aregion refers to a geographicallyseparated location while availability zones are isolated locations within a region. You can choose to place your resourcesacross multiple availability zones as well as regions. If a particular locationis affected by a failure, all other locations will still continue to run normally.The infrastructure of AWS can be observedon the following map given by AWS:Some points to note:The instance type determines the hardware of the host computer used for the instance. Each instance type offers different compute, memory, and storage capabilities and are grouped in instance families based on these capabilities.Typically, some part of the host computers resources are dedicated for an instance but in case of under-utilized resources, the same can be given to other instances. So the resources vary between a range for a given type of instance.Instances are either current generation instances or previous generation instances. The new users should always use current generation as the previous generation instances are for those who already have their applications optimized on them and shifting to a new generation instance is time consuming.AWS provides a plethora of instances for different use cases. You can find a full list here. I will not go into each one of them. But here I will discuss some concepts/AWS terminologieswhich areimportant to understand some of the features of those instances.Since AWS servers are based on a variety of hardware, its very difficult to provide a performance comparison in absolute terms. To overcome this challenge, AWS has coined an ECU as a hardware benchmark. 1 ECU is the equivalent CPU capacity of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor. So now we can easily compare the performance of a 10ECU instance vs say an 88ECU instance.In such an instance, a minimum level of computing power is predefined but it can have a boost depending upon the idle time and usage. These instances earn credits when they are idle and use them when they go active. The number of credits earned depends on the size of instance.These are designed for applications such as web servers, developer environments and small databases which dont need consistently high levels of CPU, but benefit significantly from having full access to very fast CPUs when they need them.In AWS, memory is available in two forms. First is the instance store, which is an ephemeral storage, i.e. it is temporary and all data in it gets deleted as the instance is stopped. But this storage is right on the instance and data transfer is very fast.Another form is Amazon EBS which is a like an external hard-disk attached to the system. Any data available on this remains there even when the instance is not under operation. But since this is like an external device, the data transfer rate will be slow as multiple devices will be using space on the same hardware.The speed will this vary depending on the load. To overcome this challenge, AWS offers an option called as EBS Optimized. This allows a dedicated bandwidth between AWS server and the data storage device. It might come complimentary with the instance or can be purchased for an additional fee.These are instances which are dedicated at the server hardware level. Typically, a server will be used to run multiple instances belonging to various customers. But if a customer demands a dedicated server which will only run instances for that particular account, dedicated instances are an option.Now, lets setup and launch an instance using the knowledge weve gained till now. For the purpose of this demo, I will use theinstance which is available for free so that you can also try it out with no cost involved. However, for specific applications, you will have to use an instance depending on the computing requirements.In order to get your first instance ready, please follow the given steps:Obviously, first we need an AWS account and we need to log into that. AWS offers some free instances for the first which we are going to leverage in this tutorial. After logging into your account, your management console should look like this:On this page, you should select the EC2 option which Ive highlighted in orange on top-left side. This will take you to the EC2 Dashboard which looks like:This is the console where you can manage your instances. This gives a snapshot of your account. Since were making our first instance, lets directly click the big blue button in the center  Launch Instance. This will start the process of launching a new instance.Now we have to select our AMI. The Launch Instance button will take us to the following screen:You can see various options here:Right now, well just use one of the Quick Start AMIs. Lets take an Ubuntu Server 14.04AMI as selected by me in the image above. You should explore other options as well whenever you get a chance.Note that this page shows only 4 AMIs but there are many more as you scroll down. You should be cautious of the Free Tier Eligible tag. Only AMIs having this tag will fall under the free resources available in the first year.Lets select the Ubuntu Server and move ahead.In this step, we have to choose an instance which will be based on the AMI selected above. The page looks like:Here you can see 2 filter options on the top:Here well simply select t2.micro instance because it is the only one with Free tier eligible.The interface looks like:This page allows various settings like:Now we can configure the memory settings of the instance. The page looks like:As you can see, by default an 8GB root memory is available. As the page specifies, we can add upto 30GB of general purpose EBS as part of the free tier. So Ive added another 16GB memory here.Note that we will skip the next 2 steps in this basic tutorial. You can review and fill some values as per your application. Lets click on Review and Launch togo to the last step.This step shows you a summary of the selections madebefore you launch. For us, the page would look like:You can check the selections and launch the instance. While launching, AWS will pop up for a key-value pair. This is nothing but a security measure. Instead of remembering the AMI password for launching the instance each time, we can create a key-paid which we can download on our local as a .pem file. This can be used in place of the password always. It can be created as:Just select Create a new key pair, then type a name and download the .pem file and store in a known location. This will be used to access the instance.Now that the instance is launched, it will redirect us to the EC2 Dashboard where we can check the instances running.For me, this looks like:Here you can see 2 instances but the top one is the one weve created just now. Note thatinstance takes some time to get configured. The Status Checkscolumns shows Initializing.. as a comment while the instance is being launched. Now that we can see all checks passed, lets clickConnectwhich will allow us to start talking (I mean in code language :P) to our instance.Youll get the following pop-up:This shows 2 options to connect to the instance. First is to use a standalone SSH client like Putty. Second is to use the browser directly. Since I have a Mac OS X system, I can simple use my terminal to connect using ssh. If you are using windows, you can install Putty or use the second option.Now our instance is up and running.Now letsrun some Python code on that instance.First, Ill take you through the steps I followed to connect to the instance using my Mac terminal.Now well go into the terminal of the ubuntu instance. A snapshot of my terminal with all the above steps is as following:You can see that in the end, we get into the AWS instance and can access its terminal. Now we can easily run some python codes in it.Lets start by getting into python using the command  pythonas shown:Remember that this is like a brand newsystem so we have to install everything from scratch. Lets start by installing pip which can be used to install other packages. This can be done using the command:These codes will successfully install pip. Now we caninstall numpy, scipy and sklearn using the following commands:These commands will install the respective libraries. If you get permission denied issues, you can add sudo(super used do) in front of these which should solve the issue. Note that here Im just doing this for demonstration so this works. But in practice, you might want to create some virtual environment where your code goes so that it doesnt affect the other code.Lets enter into python now and import these libraries to be sure that they are installed.Now lets load the iris dataset as present in sklearn and make a decision tree classifier on it.Now we can make a quick decision tree on it and analyze thecross validation score.So we have successfully created our first model on an AWS instance and that too for free! I will give this tutorial a halt at this point. Remember that you should stop your instance when not in use else AWS will keep on charging you. Stay tuned for another article where I will take a more detailed practical application with lots of data and train some big models using AWS instances.In this article, we began by getting acquainted to the concepts and terminologiesrequiredfor understanding the AWS system. Then we moved on to setting up our own AWS instance for free! The journey wasclearly laid out with screenshots of the entire process.Finally, we installed the basic python libraries includingnumpy and sklearn and made our first model on the IRIS dataset. This was a very high level introduction to AWS but Ive tried to stick to the fundamentals.If you follow this article, Im sure youll get a feel of the system which will help you in hardcore practical applications in future.Did you like reading this article ? Do share your experience / suggestions in the comments section below. Id love to know your experience working on AWS.",https://www.analyticsvidhya.com/blog/2016/05/comprehensive-guide-ml-amazon-web-services-aws/
Solve Interview Case Studies 10x Faster Using Dynamic Programming,Learn everything about Analytics|Introduction|Case Study  Taxi Replacement|What is Dynamic Programming?|Solution|End Notes,"Problem at Hand|Few more questions|Solution|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|A comprehensive beginners guide to start ML with Amazon Web Services (AWS)|Business Consultant  Bangalore (2+ years of experience)|
Tavish Srivastava
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The ability to solve case studies comes withregularpractice. Manya times, if you find yourself failing at thinking like a pro, perhaps, its just because you havent practiced enough.To help you become confident, Ive written multiple case studies in last one month. You can check therecent ones here. If you havent solvedany of them, Id suggest you tocheck them out first.Dynamic Programming a.k.a Dynamic Optimization isnt any trick or amathematical formula whichdelivers correct answer just by providing the inputs. Rather, its a combination of structured thinking & analytical mindset which does the job. The concept is an old one, yet used by just few of us.In this article, I have explained the art of dynamic programming using a Taxi-Replacement case study. The case study is meant for advanced users. Also, Ive used R to implement dynamic programming so that, you get some real coding experience as well.For freshers, Id recommend checking out the previous case study before trying their hand at this one. And if you truly want to land your first data science role, you cant go wrong with the Ace Data Science Interviews course. Start your data science journey today!Assume, youve decided to start a Taxi Operator firm. In fact, youve a meeting scheduled with top investors next week. For which, you need to present your business plan. The focus of your plan should be theamount of revenue expected year on year for next 25 years and your growth strategy.A critical decision which can set you for success is to consider / estimatethe time to change the cars. You know, you cant operate on a taxi forever. Precisely, acar (taxi) cannot be used for more than 10 years. Also, you are not allowed to buy a used car.You plan to run 100 cars in total, however doing calculations for a single car will suffice. Here are the assumptions you need to make in this case study :Cost (INR/Km)The salvage value is the price of the used car (second hand) at the end of respective year. As you see, the demand of the car decreases as the car grows old. Also, the mileage of the car decreases over the years.The initial car cost (as of now) is 100,000 INR. As you can see, the value of car depreciatesto 0 INR in the 11th year. This is because a caris not allowed to be drivenmore than 10 years (national rules). In addition, you need to take into account the inflation of values as below :Lets get started!Dynamic programming/Optimization is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions  ideally, using a memory-based data structure.It is a widely knownalgorithm in Operation Research. But today, we will try to use this technique to solve the problem at hand.Lets define a simple optimization function for the current problem. We are trying to optimize the profit in the current problem and profit is given as:Profit(t) = Demand(t) * (Cab Fare(t)-Cost(t)) + Sell of flag * (NewCarCost - SalvageValue(t))Total Profit = Profit(t) + Profit(t+) +Profit(t-)Sell of flag is1 if we decide to sell the car and 0 otherwise. You should notice that all the values in this function aredependent on time. So, from where do we start solving this problem?Fromthe very last time point. The reason being, at the last time point, we do not have any dependency of future time points. For instance at the last time point (l), we know thatProfit(l) =Demand(l)*(Cab Fare(l)-Cost(l)) + Sell off flag * (NewCarCost - SalvageValue(l))and,Profit(l-1,l) = Demand(l-1)*(CabFare(l-1) - Cost(l-1)) + Sell off flag *
(NewCarCost - SalvageValue(l-1)) + Max(Profit(l))Max(Profit(l)) is the maximum value of Profit(l) for the decision variable Sell off flagSo on till..Total Profit = Max(Profit(l)) + Max(Profit(l-1)) + ......+Max(Profit(1))We can solve the problem at hand with the following R code :#Basic Assumptions> Demand <- c(50000,49000,48000,47000,46000,45000,44000,43000,42000,41000,40000)
> cost <- c(3.1,3.3,3.6,4,4.5,5,5.6,6.3,7,8,9)
> Salvage<-c(1000000,600000,500000,400000,350000,300000,250000,200000,150000,130000,100000,0)#Inflation rates
> Salvagegrowth <- 1.01
> InitialCostgrowth <- 1.05
> Demandgrowth <- 1.03
> cabfaregrowth <- 1.04
> costinggrowth <- 1.02#Build the function to do dynamic programming> findpath <- function(time){
 profit_sell <- matrix(0,time + 1,12)
 profit_keep <- matrix(0,time + 1,12)
 profit_sell[,12] <- -100000000000
 profit_keep[,12] <- -100000000000
 sell_keep_grid <- matrix(""Keep"",time + 1,11)
 Demand <- Demand*(Demandgrowth)^time
 Salvage[1] <- Salvage[1]* (InitialCostgrowth)^time
 Salvage[2:11] <- Salvage[2:11]*(Salvagegrowth)^time
 cabfare <- 12*(cabfaregrowth)^time
 cost <- cost*(costinggrowth)^time
 for (i in time:1){
 Demand <- Demand/Demandgrowth
 Salvage[1] <- Salvage[1]/InitialCostgrowth
 Salvage[2:11] <- Salvage[2:11]/Salvagegrowth
 cabfare <- cabfare/cabfaregrowth
 cost <- cost/costinggrowth
 Profit <- Demand*(cabfare - cost) 
 for (j in 1:11){
 profit_sell[i,j] <- max(profit_sell[i+1,2],profit_keep[i+1,2])+ Profit[1] + Salvage[j] - Salvage[1] 
 profit_keep[i,j] <-max(profit_sell[i+1,j+1],profit_keep[i+1,j+1])+Profit[j]
 sell_keep_grid[i,j] <- ifelse(profit_sell[i,j] > profit_keep[i,j],""Sell"",""Keep"")
 sell_keep_grid[i,j] <- ifelse(i < j,""N/A"",sell_keep_grid[i,j])
 }
 } Path <- rep(""Keep"",time)
 num_years <- rep(0,time) lookat <- 1
 for(i in 1:time) {
 Path[i] <- sell_keep_grid[i,lookat]
 num_years[i] <- lookat 
 lookat <- ifelse(sell_keep_grid[i,lookat] == ""Sell"",1,lookat+1)
 }
 setwd(""C:\\Users\\ts93856\\Desktop\\Insurance"")
 write.csv(profit_sell,""Sell.csv"")
 write.csv(profit_keep,""keep.csv"")
 write.csv(sell_keep_grid,""sellkeepgrid.csv"")
 write.csv(Path,""Path.csv"")
 print(num_years)
 print(Path)
 return(Path)
}findpath(time = 25)Here is the final output you get for Keep/Sell decision :Further, lets look at the Optimum revenue matrix :As you can see, the total revenue in 1st cell is INR 22.4 Million which is the maximum profit expected by a single car in 25 years. Hence, the total expected profit by 100 cars will be 100*22.4 Million = INR 2.24 Billion based on our current assumptions.Lets try to answer a few more question using the same analysis.Solved Questions:There are 4 cycles evidentfrom the first table. So, we need 1 original and 3 replacements for each car requirement. The first car stays for 7 years, 2nd stays for 7 years, 3rd for 6 years and last for 5 years. So, the maximum age of the car here is 7 years.To find the year wise profit in present value, following is the table:Hence, the maximum profit in present value happens at 21st year.I have left one of the question for youto practice the concept. This is one of the most powerful tools given by operation research domain to optimize such complex problems. You just saw, it isnt any difficult to learn. You just need to put a framework around the give problems.In future articles I will illustrate with more examples of how Dynamic Programming/Optimization is used in different industries.Id recommend you to practice this technique so that you do well in your next job interview. All the best.Did you like reading this article ? Do share your experience / suggestions in the comments section below. Id love to know your",https://www.analyticsvidhya.com/blog/2016/05/ase-studies-10x-faster-using-dynamic-programming/
"Business Consultant  Bangalore (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Solve Interview Case Studies 10x Faster Using Dynamic Programming|Use H2O and data.table to build models on large data sets in R|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business ConsultantLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredAdditional SkillsInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Consultant  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/05/business-consultant-bangalore-2-years-experience/
Use H2O and data.table to build models on large data sets in R,Learn everything about Analytics|Introduction|Table of Contents|What is H2O ?|What makes it faster ?|Solving a Problem|End Notes,"1. Getting Started|2. Data Exploration using data.table & ggplot|3. Data Manipulation using data.table|4. Model Building using H2O|Multiple Regression in H2O|Random Forest in H2O|GBM in H2O|Deep Learning in H2O|What could you do to further improve this model ?|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Business Consultant  Bangalore (2+ years of experience)|Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience)|
Analytics Vidhya Content Team
|45 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Last week, I wrote an introductory articleon the package data.table. It was intended to provide you a head start and become familiar with its unique and short syntax. The next obvious step is to focus on modeling, which we will do in this post today.With data.table, you no longer need to worry about your machine cores (or RAM to some extent). Atleast, I used to think of myself as a crippled R user when faced with large data sets. But not anymore! I would like to thank Matt Dowle again for this accomplishment.Last week, I received an email saying:Okay, I get it. data.table empowers usto do data exploration & manipulation. But, what about model building ? I work with8GB RAM. Algorithms like random forest (ntrees = 1000) takes forever to run on my data set with 800,000 rows.Im sure there are many R users who are trapped in a similar situation. To overcome this painstaking hurdle, I decided to write this post which demonstrates usingthetwo most powerful packages i.e. H2O and data.table.For practical understanding, Ive taken the data set from a practice problemand tried to improve the scoreusing 4 different machine learning algorithms (with H2O) & feature engineering (with data.table). So, getready for a journey from rank 154th to 25th on the leaderboard.Note: Consider this article as a startersguide formodel buildingusingdata.table and H2O. I havent explained these algorithms in details. Rather, the focus is kept on implementing these algorithms using H2O. Dont worry, links to resources are provided.H2O is an open source machine learning platform where companies can build models on large datasets (no sampling needed) and achieve accurate predictions. It is incredibly fast, scalable and easy to implement at any level.In simple words, they provide a GUI driven platform to companies for doing faster data computations. Currently, their platform supports advanced & basic level algorithms such as deep learning, boosting, bagging, naive bayes, principal component analysis, time series, k-means, generalized linear models.In addition, H2O has released APIs for R, Python, Spark, Hadoop users so that people like us can use it to build models at individual level. Needless to say, its free to use and instigatesfaster computation.H2O has a clean and clear feature of directly connecting the tool (R or Python) with your machines CPU. This way we get to channelize more memory, processing power to the tool for making faster computations. This will allow computations to take place at 100% CPU capacity (shown below). It can also be connected with clusters at cloud platforms for doing computations.Along with, it uses in-memory compression to handle large data sets even with a small cluster.It also include provisions to implement parallel distributed network training.Tip: In order to channelize all your CPUs processing power formodel computation, avoid using any application or software which consumes too much memory. Specially, avoid opening too many tabs on google chrome or any other web browser.Lets get down to use these package and build some nice models.Data Set: Ive taken the data set from Black Friday Practice Problem. The data set hastwo parts: Train and Test. Train data set contains 550068 observations. Test data set contains 233599 observations.To download the data and read the problem statement: Click Here. One time login will berequired.Lets get started!Ideally, the first step in model building is hypothesis generation. This step is carried out after you have read the problem statement but not seen the data.Since, this guide isnt designed to demonstrate all predictive modeling steps, I leave that upto you. Heres a good resource to freshen up yourbasics: Guide to Hypothesis Generation. If you do this step, may be you could end up creating a better model than mine. Do give your best shot.Starting with loading data in R.> path <- ""C:/Users/manish/desktop/Data/H2O""
> setwd(path)#install and load the package
> install.packages(""data.table"")
> library(data.table)#load data using fread
> train <- fread(""train.csv"", stringsAsFactors = T)
> test <- fread(""test.csv"", stringsAsFactors = T)Within seconds, fread loads the data in R. Its that fast. The parameter stringsAsFactors ensures that character vectors are converted into factors. Lets quickly check the data set now.#No. of rows and columns in Train
> dim(train)
[1] 550068   12#No. of rows and columns in Test
> dim(test)
[1] 233599   11> str(train)
Classes data.table and 'data.frame': 550068 obs. of 12 variables:
 $ User_ID : int 1000001 1000001 1000001 1000001 1000002 1000003 1000004 1000004 1000004...
 $ Product_ID : Factor w/ 3631 levels ""P00000142"",""P00000242"",..: 673 2377 853 829 2735 2632
 $ Gender : Factor w/ 2 levels ""F"",""M"": 1 1 1 1 2 2 2 2 2 2 ...
 $ Age : Factor w/ 7 levels ""0-17"",""18-25"",..: 1 1 1 1 7 3 5 5 5 3 ...
 $ Occupation : int 10 10 10 10 16 15 7 7 7 20 ...
 $ City_Category : Factor w/ 3 levels ""A"",""B"",""C"": 1 1 1 1 3 1 2 2 2 1 ...
 $ Stay_In_Current_City_Years: Factor w/ 5 levels ""0"",""1"",""2"",""3"",..: 3 3 3 3 5 4 3 3 3 2 ...
 $ Marital_Status : int 0 0 0 0 0 0 1 1 1 1 ...
 $ Product_Category_1 : int 3 1 12 12 8 1 1 1 1 8 ...
 $ Product_Category_2 : int NA 6 NA 14 NA 2 8 15 16 NA ...
 $ Product_Category_3 : int NA 14 NA NA NA NA 17 NA NA NA ...
 $ Purchase : int 8370 15200 1422 1057 7969 15227 19215 15854 15686 7871 ...
 - attr(*, "".internal.selfref"")=<externalptr>What do we see ? I see 12 variables, 2 of which seems to have so many NAs. If you have read the problem description and data information, we see Purchase is the dependent variable, rest 11 are independent variables.Looking at the nature of Purchase variable (continuous), we can infer that this is a regression problem. Even though, the competition is closed but we can still check our score and evaluate how good we could have done. Lets make our first submission.With all the data points weve got, we can make our first set of prediction using mean. This is because, mean prediction will give us a good approximation of prediction error. Taking this as baseline prediction, our model wont do worse than this.#first prediction using mean
> sub_mean <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = mean(train$Purchase))
> write.csv(sub_mean, file = ""first_sub.csv"", row.names = F)It was this simple. Now, Ill upload the resultant file and check myscore and rank. Dont forget to convert .csv to .zip format before you upload. You can upload and check your solution at the competition page.Our mean prediction gives us a mean squared error of 4982.3199. But, how good is it? Lets check my ranking on leaderboard.Thankfully, I am not last. So, mean prediction got me 154 / 162 rank.Lets improve this score and attempt to rise up the leader board.Before starting with univariate analysis, lets quick summarize both the files (train and test) and decipher, if there exist any disparity.> summary (train)> summary (test)Look carefully (check at your end) , do you see any difference in their outputs? Actually, I found one. If you carefully compareProduct_Category_1, Product_Category_2 & Product_Category_3 in test and train data, there exist a disparity in max value.max value of Product_Category_1 is 20 whereas for others is 18. These extra category levels appears to be noise. Make a note this this. Well need to remove them.Lets combine the data set. Ive used rbindlist function from data.table, since its faster than rbind.#combine data set
> test[,Purchase := mean(train$Purchase)]
> c <- list(train, test)
> combin <- rbindlist(c)In the code above, weve first added the Purchase variable in the test set so that both data sets have equal number of columns. Now, well do some data exploration.In this section, well do some univariate and bivariate analysis, and try to understand the relationship among given variables. Lets start with univariate.#analyzing gender variable
> combin[,prop.table(table(Gender))]
Gender
F     M
0.2470896 0.7529104#Age Variable
> combin[,prop.table(table(Age))]
Age
0-17    18-25    26-35    36-45    46-50    51-55    55+
0.02722330 0.18113944  0.39942348  0.19998801  0.08329814 0.06990724 0.03902040#City Category Variable
> combin[,prop.table(table(City_Category))]
City_Category
 A     B     C 
0.2682823 0.4207642 0.3109535#Stay in Current Years Variable
> combin[,prop.table(table(Stay_In_Current_City_Years))]
Stay_In_Current_City_Years
 0     1     2     3     4+ 
0.1348991 0.3527327 0.1855724 0.1728132 0.1539825#unique values in ID variables
> length(unique(combin$Product_ID))
[1] 3677>length(unique(combin$User_ID))
[1] 5891#missing values
> colSums(is.na(combin)) User_ID   Product_ID 
 0       0 
 Gender    Age 
 0       0 
 Occupation  City_Category 
 0       0 
Stay_In_Current_City_Years  Marital_Status 
 0               0 
 Product_Category_1      Product_Category_2 
 0               245982 
 Product_Category_3      Purchase 
 545809            0 Following are the inferences we can generate from univariate analysis:Weve got enough hints from univariate analysis. Lets tap out bivariate analysis quickly. You can always make these graphs look beautiful by adding more parameters. Heres a quick guide to learn making ggplots.> library(ggplot2)#Age vs Gender
> ggplot(combin, aes(Age, fill = Gender)) + geom_bar()#Age vs City_Category
ggplot(combin, aes(Age, fill = City_Category)) + geom_bar()We can also create cross tables for analyzing categorical variables. To make cross tables, well use the package gmodels which creates comprehensive cross tables.> library(gmodels)
> CrossTable(combin$Occupation, combin$City_Category)With this, youll obtaina long comprehensive cross table of these two variables. Similarly, you can analyze other variables at your end. Our bivariate analysis havent provided us much actionable insights. Anyways, we get to data manipulation now.In this part, well create new variables, revalue existing variable and treat missing values. In simple words, well getour data ready for modeling stage.Lets start with missing values. We saw Product_Category_2 and Product_Category_3 had a lot of missing values. To me, this suggests a hidden trend which can be mapped by creating a new variable. So, well create a new variable which will captureNAs as 1 and non-NAs as 0 in the variables Product_Category_2 and Product_Category_3.#create a new variable for missing values
> combin[,Product_Category_2_NA := ifelse(sapply(combin$Product_Category_2, is.na) ==  TRUE,1,0)]
> combin[,Product_Category_3_NA := ifelse(sapply(combin$Product_Category_3, is.na) == TRUE,1,0)]Lets now impute the missing values with any arbitrary number. Lets take -999#impute missing values
> combin[,Product_Category_2 := ifelse(is.na(Product_Category_2) == TRUE, ""-999"", Product_Category_2)]
> combin[,Product_Category_3 := ifelse(is.na(Product_Category_3) == TRUE, ""-999"", Product_Category_3)]Before proceeding to feature engineering, lastly, wellrevalue variable levels as inferred from our univariate analysis.#set column level
> levels(combin$Stay_In_Current_City_Years)[levels(combin$Stay_In_Current_City_Years) == ""4+""] <- ""4""#recoding age groups
> levels(combin$Age)[levels(combin$Age) == ""0-17""] <- 0
> levels(combin$Age)[levels(combin$Age) == ""18-25""] <- 1
> levels(combin$Age)[levels(combin$Age) == ""26-35""] <- 2
> levels(combin$Age)[levels(combin$Age) == ""36-45""] <- 3
> levels(combin$Age)[levels(combin$Age) == ""46-50""] <- 4
> levels(combin$Age)[levels(combin$Age) == ""51-55""] <- 5
> levels(combin$Age)[levels(combin$Age) == ""55+""] <- 6#convert age to numeric
> combin$Age <- as.numeric(combin$Age)#convert Gender into numeric
> combin[, Gender := as.numeric(as.factor(Gender)) - 1]It is advisable to convert factor variables into numeric or integer for modeling purpose.Lets now move one step ahead, and create more new variables a.k.a feature engineering. To know more about feature engineering, you can read more.During univariate analysis, we discovered that ID variables have lesser unique values as compared to total observations in the data set. It means there are User_IDs or Product_IDs must have appeared repeatedly in this data set.Letscreate a new variable which captures the count of these ID variables. Higher user count suggests that a particular user haspurchased products multiple times. High product count suggests that a product has been purchased many a times, which shows its popularity.#User Count
> combin[, User_Count := .N, by = User_ID]#Product Count
> combin[, Product_Count := .N, by = Product_ID]Also, we can calculate the mean purchase price of a product. Because, lower the purchase price, higher will be the chances of that product being bought or vice versa. Similarly, we can create another variable which maps the average purchase price by user i.e. how much purchase (on an average) is made by a user. Lets do it.#Mean Purchase of Product
> combin[, Mean_Purchase_Product := mean(Purchase), by = Product_ID]#Mean Purchase of User
> combin[, Mean_Purchase_User := mean(Purchase), by = User_ID]Now, we are only left with one hot encoding of City_Category variable. This can be done in one line using library dummies.> library(dummies)
> combin <- dummy.data.frame(combin, names = c(""City_Category""), sep = ""_"")Before, proceeding to modeling stage, lets check data types of variables once, and make the required changes, if necessary.#check classes of all variables
> sapply(combin, class)#converting Product Category 2 & 3
> combin$Product_Category_2 <- as.integer(combin$Product_Category_2)
> combin$Product_Category_3 <- as.integer(combin$Product_Category_3)In this section, well explore the power of different machine learning algorithms in H2O. Well build models with Regression, Random Forest, GBM and Deep Learning.Make sure you dont use these algorithms like a black box. It is advisable to know how do they work. This will help you to understand the parameters used in building these models. Here are some useful resources to learn about these algorithms:But, first things first. Lets divide the data set into test and train.#Divide into train and test
> c.train <- combin[1:nrow(train),]
> c.test <- combin[-(1:nrow(train)),]As discovered in beginning that the variable Product_Category_1 in train has some noise. Lets remove it as well by selecting all rows in Product_Category_1 upto 18, thereby dropping rows which has category level 19 & 20.>c.train <- c.train[c.train$Product_Category_1 <= 18,]Now, our data set is ready for modeling. Time to install H2O package in R. The procedure to load package remains same. For faster computation make sure, youve closed all other applications. So, how does H2O in R work ? Its simple actually!R uses REST API as a reference object to send functions, data to H2O. The data set is then assigned a key for future reference. H2O doesnt uses .csv data, instead it converts .csv to its own H2O instance data. Youd be surprised to know that H2O has its own functions for data manipulation too. But, data.table is no bad either.> install.packages(""h2o"")
> library(h2o)To launch the H2O cluster, write > localH2O <- h2o.init(nthreads = -1)This commands tell H2O to use all the CPUs on the machine, which is recommended. For larger data sets (say > 1,000,000 rows), h2o recommends running cluster on a server with high memory for optimal performance. Once the instance starts successfully, you can also check its status using:> h2o.init()Connection successful!R is connected to the H2O cluster: 
 H2O cluster uptime: 1 days 9 hours 
 H2O cluster version: 3.8.1.3 
 H2O cluster name: H2O_started_from_R_manish_vkt788 
 H2O cluster total nodes: 1 
 H2O cluster total memory: 1.50 GB 
 H2O cluster total cores: 4 
 H2O cluster allowed cores: 4 
 H2O cluster healthy: TRUE 
 H2O Connection ip: localhost 
 H2O Connection port: 54321 
 H2O Connection proxy: NA 
 R Version: R version 3.2.2 (2015-08-14)Lets now transfer the data from R to h2o instance. It can be accomplished using as.h2o command.#data to h2o cluster
> train.h2o <- as.h2o(c.train)
> test.h2o <- as.h2o(c.test)Using column index, we need to identify variables to be used in modeling as follows:#check column index number
> colnames(train.h2o)
[1] ""User_ID""           ""Product_ID"" 
 [3] ""Gender""           ""Age"" 
 [5] ""Occupation""         ""City_Category_A"" 
 [7] ""City_Category_B""       ""City_Category_C"" 
 [9] ""Stay_In_Current_City_Years"" ""Marital_Status"" 
[11] ""Product_Category_1""     ""Product_Category_2"" 
[13] ""Product_Category_3""     ""Purchase"" 
[15] ""Product_Category_2_NA""   ""Product_Category_3_NA"" 
[17] ""User_Count""         ""Product_Count"" 
[19] ""Mean_Purchase_Product""   ""Mean_Purchase_User""#dependent variable (Purchase)
> y.dep <- 14#independent variables (dropping ID variables)
> x.indep <- c(3:13,15:20)Lets start with Multiple Regression model.> regression.model <- h2o.glm( y = y.dep, x = x.indep, training_frame = train.h2o, family = ""gaussian"")> h2o.performance(regression.model)
H2ORegressionMetrics: glm
** Reported on training data. **MSE: 16710563
R2 : 0.3261543
Mean Residual Deviance : 16710563
Null Deviance :1.353804e+13
Null D.o.F. :545914
Residual Deviance :9.122547e+12
Residual D.o.F. :545898
AIC :10628689GLMalgorithm in H2O can beused for all types of regression such as lasso, ridge, logistic, linear etc. A user onlyneeds to modify the family parameter accordingly. For example: To do logistic regression, you can writefamily = binomial.So, after we print the model results, we see that regression gives a poor R value i.e. 0.326. It means that only 32.6% of the variance in the dependent variable is explained by independent variable and rest is unexplained. This shows that regression model is unable to capture non linear relationships.Out of curiosity, lets check the predictions of this model. Will it be worse than mean predictions ? Let see.#make predictions
> predict.reg <- as.data.frame(h2o.predict(regression.model, test.h2o))
> sub_reg <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.reg$predict)> write.csv(sub_reg, file = ""sub_reg.csv"", row.names = F)Lets upload the solution file (in .zip format) and check if we have got some improvement.Wow! Our prediction score has improved. We started from 4982.31 and with regression weve got an improvement over previous score. On leaderboard, this submission takes me to 129th position.It seems, we can do well if we choose an algorithm which maps non-linear relationships well. Random Forest is our next bet. Lets do it.#Random Forest
> system.time(
 rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 1122)
)# |====================================================================| 100%
# user system elapsed 
# 21.85 1.61 2260.33With 1000 trees, random forest model took approx ~38 minutes to run. It operated at 100% CPU capacity which can be seen in Task Manager (shown below).Your model might not take same time because of difference in our machine specifications. Also, I had to open web browsers which consumed a lot of memory. Actually, your model might take lesser time. Youcan check the performance of this model using the same command as used previously:> h2o.performance(rforest.model)#check variable importance
 > h2o.varimp(rforest.model)Lets check the leaderboard performance of this model by making predictions. Do you think our score will improve ? Im a little hopeful, though!#making predictions on unseen data
> system.time(predict.rforest <- as.data.frame(h2o.predict(rforest.model, test.h2o)))
# |====================================================================| 100%
# user system elapsed 
# 0.44 0.08 21.68#writing submission file
> sub_rf <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.rforest$predict)
> write.csv(sub_rf, file = ""sub_rf.csv"", row.names = F)Making predictions took ~ 22 seconds. Now is the time to upload the submission file and check the results.Random Forest was able to map non-linear relations way better than regression ( as expected). With this score, my ranking on leaderboard moves to 122:This gave a slight improvement on leaderboard, but not as significant as expected. May be GBM, a boosting algorithm can help us.If you are new to GBM, Id suggest you to check the resources given in the start of this section. We can implement GBM in H2O using a simple line of code:#GBM
system.time(
 gbm.model <- h2o.gbm(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, max_depth = 4, learn_rate = 0.01, seed = 1122)
)
# |====================================================================| 100%
# user system elapsed 
# 7.94 0.47 739.66With the same number of trees, GBM took less time than random forest. It took only 12 minutes. You can check the performance of this model using:> h2o.performance (gbm.model)
H2ORegressionMetrics: gbm
** Reported on training data. **
MSE: 6319672
R2 : 0.7451622
Mean Residual Deviance : 6319672As you can see, our R has drastically improved as compared to previous two models. This shows signs of a powerful model. Lets make predictions and check if this model brings us some improvement.#making prediction and writing submission file
> predict.gbm <- as.data.frame(h2o.predict(gbm.model, test.h2o))
> sub_gbm <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.gbm$predict)
> write.csv(sub_gbm, file = ""sub_gbm.csv"", row.names = F)We have created the submission file. Lets upload it and check if weve got any improvement.I never doubted GBM once. If done well, boosting algorithms usually pays off well. Now, will be interesting to see my leaderboard position:This is a massive leaderboard jump! Its like a freefall but safe landing from 122nd to 25th rank. Can we do better ? May be, we can. Lets now use Deep Learning algorithm in H2O and try to improve this score.Let me give you a quickoverview of deep learning. In deep learning algorithm, there exist 3 layers namely input layer, hidden layer and output layer. It works as follows:Lets implement this algorithm now.#deep learning models
> system.time(
      dlearning.model <- h2o.deeplearning(y = y.dep,
      x = x.indep,
      training_frame = train.h2o,
      epoch = 60,
      hidden = c(100,100),
      activation = ""Rectifier"",
      seed = 1122
      )
)
# |===================================| 100%
# user system elapsed 
# 0.83 0.05 129.69It got executed even faster than GBM model. GBM took ~739 seconds.The parameter hidden instructs the algorithms to create 2 hidden layers of 100 neurons each. epoch is responsible for the number of passes on the train data to be carried out.Activation refers to the activation function to be used throughout the network.Anyways, lets check its performance.> h2o.performance(dlearning.model)
H2ORegressionMetrics: deeplearning
** Reported on training data. **
MSE: 6215346
R2 : 0.7515775
Mean Residual Deviance : 6215346We see further improvement in the R metric as compared to GBM model. This suggests that deep learning model has successfully captured large chunk of unexplained variances in the model. Lets make the predictions and check the final score.#making predictions
> predict.dl2 <- as.data.frame(h2o.predict(dlearning.model, test.h2o))#create a data frame and writing submission file
> sub_dlearning <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.dl2$predict)
> write.csv(sub_dlearning, file = ""sub_dlearning_new.csv"", row.names = F)Lets upload our final submission and check the score.Though, my score improved but rank didnt. So, finally we ended up at 25th rank by using little bit of feature engineering and lot of machine learning algorithms. I hope you enjoyed this journey from rank 154th to rank 25th. If you have followed me till here, I assume youd be ready to go one step further.Actually, there are multiple things you can do. Here, I list them down:Try these steps at your end, and let me know in comments how did it turn out for you!I hope you enjoyed this journey withdata.table and H2O. Once you become proficient at using these two packages, youd be able to avoid a lot of obstacles which arises due to memory issues. In this article, I discussed the steps (with R codes) to implement model building using data.table and H2O. Even though, H2O itself can undertake data munging tasks, but I believe data.table is a much easy to use (syntax wise) option.With this article, my intent was to get you started with data.table and H2O to build models. I am sure after this modeling practice you will becomecurious enough to take a step further and know more about these packages.Did this article made you learn something new? Do write in the comments about your suggestions, experience or any feedback which could allow me to help you in a better way.",https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/
"Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Use H2O and data.table to build models on large data sets in R|Winners Talk: Top 3 Solutions ofThe Seers Accuracy Competition|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Scientist (Machine Learning)Location  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist (Machine Learning)  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/05/data-scientist-machine-learning-gurgaon-2-4-years-experience/
Winners Talk: Top 3 Solutions ofThe Seers Accuracy Competition,Learn everything about Analytics|Introduction|The Competition|Winners!|Key Takeaways from this Competition|End Notes,"Problem Statement|Rank 3: Bishwarup Bhattacharjee, Kolkata, India|Rank 2:Oleksii Renov (Dnipro, Ukraine) andThakur Raj Anand (Hyderabad, India)|Rank 1: Rohan Rao, Mumbai, India|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Scientist (Machine Learning)  Gurgaon (2-4 years of experience)|data.table() vs data.frame()  Learn to work on large data sets in R|
Analytics Vidhya Content Team
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",About ElecMart|The problem  Where are the recurringbuyers?|Data provided,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Surprises arrivewhen you expect them least toarrive.The Seers Accuracy turned out to be a challenging surprise for data scientists. So, what changed this time ? Actually, there was notest or train file given. Participants were given just one file to download. Would you believe?Everyone was puzzled. The weak ones gave up at the beginning; but the determined ones stayed till the end and learned something new!Did you miss out on this experience ? If you didnt, great! But if you did, unfortunately you just missed out a wonderful opportunity to learn something new. Though, I cant bring back the thrilling experience, but I can give you one more chance to learn (data set is live again).The Seers Accuracy held from 29th April to 1st May 2016. This competition enticed more than ~2200 participants across the world.In this 72 hour battle, the first thing which participants were requiredto do is to create a train and test file themselves. After which, the race to seer would start.Once again, XGBoost and Ensemble Modeling helped winners to discover the highly accuratesolutions. Below are the winning solutions of top 3 winners. Here is quick short interview of winners highlighting their approach and thought process which made them got in top 3.If you participated in this competition, its time to analyze your hits and misses andbecome better for next.Note: R was extensively used by winning team members. Special thanks to these winners for their immense cooperation in sharing their experience and knowledge.The participants were required to help ElecMart, a chain of electronic superstores looking to increase its sales from existing customers. The evaluation metric used was AUC  ROC.ElecMart, as the name suggests is a supermarket for Electronics. They serve the needs of both, retail clients and various corporate clients.Customers not only get to see and feel a wide range of products, they also receive exciting discounts and excellent customer service. ElecMart started in 1999 and launcheda customer loyalty program in 2003.ElecMart aims to be largest Electronic superstore across the nation, but they have a big hurdle ahead!The loyalty program is meant for customers who want to take benefit from repeat purchases and register at the time of purchase. They need to present the loyalty card at Point of Sale at time of purchase and the benefits are non-transferrable. Also corporate sales automatically get the benefits of the loyalty program.In a recent benchmarking activity and market survey which ElecMartsponsored, it wasfound that the Repeat purchase rate i.e. customer who come again for purchases from these customers is very low compared to other competitors. Increasing sales to these customers is the only way to run a successful loyalty program.ElecMart has shared all the transactions it had with their loyalty program customers since the loyalty program has started. They want to do focused campaigns with these customers highlighting the benefits of continued shopping with ElecMart. You are expected to identify the probabilty of the each customer (in the loyalty program) making a purchase in next 12 months.You are expected to upload the solution in the format of sample_submission.csv.Thepublic-private split is 20:80Note: For practice, the data set is currently available for download on Link. Please note that the data set is available for your practice purpose and will be accessible until 12th May 2016.Bishwarup is an entrepreneur and is currently the CEO of Alphinite Analytics. He is a Kaggle Master and is currently ranked 13 on Data Hack. He won INR 20,000 ($300).He said:The data for this particular competition was a bit different from the conventional ML problems. It had no target column and no explicit separation between the training and test set. So I discovered, there were more than one potential ways to tackle such problems.However, since the evaluation metric for the competition was the area under the ROC curve (AUC), I preferred to first formulate the problem as a case of supervised learning which I think majority of the participants did as well.I used the data from 2003-2005 as my training set and matched the customers who repeated in 2006 to derive the labels for my data. That was pretty straightforward. Just formulating the problem in this way and using a very simple xgboost model, I could get > 0.83 on the public leaderboard.Then, feature engineering playeda huge role to play in my success. Since, we were ultimately supposed to predict the probability of a repeat on per user basis, I summarized multiple user records in the training data to one single training instance. The features which helped me are as follows:There were more features which I derived, but they did not help my models accuracy.In the end, I trained two xgboost models on the above features selecting a part of it in each of them and the rank average of them got me to end at 3rd position in this competition with 0.874409 accuracy.My Solution: LinkThakur Raj AnandThis was the first time, a team (Team Or) managed to secure a position in Top 3. This team won INR 35,000 ($500).Thakur Raj Anand(DataGeek)isa data science analyst with Masters in Quantitative Finance based out of Hyderabad. Hemostly uses R and Python for data science competitions. Oleksii Renov (orenov) isa data scientist based out of Dnipro, Ukraine. He loves to do programming in Python, R and Scala.They said:We spent 40% of the time exploring data and converted the problem into a Supervised problem.Oleksii RenovWe generated negative sample by assigning 0 to those IDs which had no transaction in the year 2006 but had a history before 2006. We constructed 4 different representation of data to make models with the idea of capturing different signals from different representations.For modeling, we mainly used XGBoost but we did try Random Forest and ExtraTrees which unfortunatelydidnt help to improve ourfinal predictions accuracy.Oleksii has an usual habit of looking for unusual patterns in data. He foundthat predictions from tree model and linear model were very different and averaging them was giving a significant boost in CV as well as on LB.We kept exploring different styles and finallywe made 4 tree models and 1 linear model using XGBoost.We only made a linear model on the final representation of data on which XGBOOST was giving best CV. We finished at Rank 2 with0.876660 accuracy.In this competition, we learned a lot about sparse matrices. We decided to learn simple things like aggregating, transformation etc. on sparse matrices which is very helpful in exploring large data sets in an efficient way.In the end, we would like to tell young aspiring data science folks to never give up. Every time you feel like giving up, try to make a different representation of data and try different models on them.Our Solution: LinkRohan Rao is currently working as a Data Scientist at AdWyze. He is a Kaggle Master and currently ranked 6 on Data Hack. He is a three time National Sudoku Champion and currently ranked 14th in the world. He won INR 70,000 ($1000).He said:Hackathons might be meant for quick and smart modelling, but this one restored my faith in focusing on smart.Ive been regularly participating in competitions at Data Hack. More than anything, Ive learned many new things. I am glad I finally got my maiden win!The road to achieve a seers accuracy turned out to be interesting. Unlike a majority of predictive modelling competitions, this hackathon did not have the standard train/test data format.I started off with understanding how best to build a machine-learning based solution with the data, along with setting up a stable validation framework.Based on my CV-LB scores from an XGBoost model, that were quite well in sync, I explored each variable and started working on feature engineering. I could see, that there is subtle but good scope of creating new variables.My final model was an ensemble of 3 XGBoost models, each having a different set of data points, features and parameters. The ensemble was mainly to ensure more stability in the predictions.I explored few other ML-based models, but none were as good as XGBoost. Even their ensembling with XGBoost did not help. This way I won this competition withtheaccuracy of 0.88002.I feel, it is always wonderful to work with clean datasets that are designed over a good problem statement. And, this hackathon was very well organized. The CV-LB stable correlation was a huge plus because it enabled me to focus on feature engineering, which is the most exciting part of building machine learning models.It was nice to see and compete with many of the top data scientists in India, and at the end, Im glad I finished 1st to win my maiden competition on AnalyticsVidhya.The biggest learning for me from this competition was the importance of drilling down into understanding the problem statement inside out and building a robust and solid solution step-by-step. And, then practice more so that one can do these as quickly as possible. It might sound cliche but it actually works!Finally, some of the tips I would like to give to aspiring data scientists:Always trust your Cross-Validation (CV) score. And to trust your CV, you need to build the right validation method depending on the problem, data and evaluation. During the competition, explore and try out as many ideas as possible. Youd be surprised to know that sometimes, the simplest algorithmor the least obvious ones could also work out. In the end, always be ready to learn from others and never hesitate in asking for help. Theres always something to learn for everyone.My Solution: LinkThis competition gave a clean well structured data set. Hence, no efforts were required in data cleaning. But, problem framing (which most of us overlook) paved the way towards success. Moving away from a conventional ML competition, turned out to be challenging event for participants but eventually gave them something new to learn. Below are the key takeaways from our top 3 participants:Some of your might have sought motivation & some of you take away knowledge from this article. If you have thoroughly read the winners talk, you would have realized that winning this competition didnt require anything extra ordinary technique. It wasnt about knowing advanced machine learning algorithms, but required a simple approach of understanding the problem.Therefore, next time when you come for challenge, make sure youve understood what has been asked for, and then start working on predictive modeling. This way youll have more confidence while working. Last but not the least, learn about cross validation, xgboost and feature engineering.Did you find this article helpful ? Were you able to analyze your hits and misses ? Dont worry, there is always a next time. Winning is a good habit. Coming up soon is Mini Data Hack.",https://www.analyticsvidhya.com/blog/2016/05/winners-approach-solution-seers-accuracy/
data.table() vs data.frame()  Learn to work on large data sets in R,Learn everything about Analytics|Introduction|Why does your machine fail to work with large data sets?|What is data.table ?|Why should you usedata.table instead ofdata.frame?|Important Data Manipulation Commands|End Notes,"1. How to subsetrows & columns?|2. How to order variables in ascending or descending order?|3. How to add / update / delete a column or values in a data set?|4. How to compute functions on variables basedon grouping a column?|5. How to use keys for subsetting data ?|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Winners Talk: Top 3 Solutions ofThe Seers Accuracy Competition|Big Data Architect Mumbai (5+ years of experience)|
Analytics Vidhya Content Team
|39 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R users (mostly beginners) struggle helplessly while dealing with large data sets. They get haunted by repetitive warnings, error messages of insufficient memory usage. Most of them come to an immediate conclusion, that their machine specification isnt powerful enough. Its time to upgrade the RAM or work on a new machine. Have you ever thought this way?If you have seriously worked on data sets, Im sure you would have. Even, I did too when I participated in The Black Friday. The data set contained more than 400,000 rows. I was totally clueless. Honestly, it was frustrating to see RStudio taking hours to execute one line of code. As we say, necessity is the mother of all inventions. I was in need of a solution.After 2 hours of internet research, I came across an interesting set of R packages and APIs, specially made to work with large data sets without compromising with execution speed. One such package is data.table.In this article, Ive shared a smart approach which you should use when you work on large data sets. As you scroll down, you will come across the type of changes you can make to improve your R coding. Its time to write codes which are fast and short. Consider it as a quick tutorial on data.table package.Note: This article is best suited to beginners in data science using R who mainly work on data sets usingdata.frame() .If you are already a proficient user of data.table, this might not interest you.Its important to understand the factors which deters your R code performance.Many a times, theincompetencyof your machine is directly correlated withthe type of work you do while running R code. Below are some practiceswhich impedes Rs performance on large data sets:Note: My system specification is Intel(R) Core (TM) i5-3230M CPU @ 2.60GHz, 2 Core(s), 4 Logical Processors with 8GB RAM.The package data.table is written by Matt Dowle in year 2008.Think of data.table as an advanced version of data.frame. It inherits from data.frame and works perfectly even when data.frame syntax is applied on data.table. This package is good to use with any other package which accepts data.frame.The syntax of data.table is quite similar to SQL. Therefore, if youve worked on SQL you would quickly understand it. The general form of syntax is:DT[i, j, by]where:For example:#creating a dummy data table
DT <- data.table( ID = 1:50,
        Capacity = sample(100:1000, size = 50, replace = F),
        Code = sample(LETTERS[1:4], 50, replace = T),
        State = rep(c(""Alabama"",""Indiana"",""Texas"",""Nevada""), 50))#simple data.table command
> DT[Code == ""C"", mean(Capacity), State]Lets see how does this command work. After the data table is created, I asked data table to filter the rows whose code is C. Then I asked it to calculate the mean capacity of the rows which have code C for every state separately. Its not necessary that you always mention all the three parts of the syntax.Try doing the following commands at your end :Write your answers in the comments! Lets see how quickly you are getting this concept.After I delved deeper into data.table, I found several aspects at which data.table package outperforms data.frame. Therefore, I would recommend every R beginner to use data.table as much as they can. There is a lot to explore. The earlier you start, the better youll become. You should use data.table because:1. It provides blazing fast speed when it comes to loading data. With the fread function in data.table package, loading large data sets need justfew seconds. For example: I checked the loading time using a data set which contains439,541 rows. Lets see how fast is fread > system.time(dt <- read.csv(""data.csv""))
 user system elapsed 
 11.46 0.21  11.69> system.time(dt <- fread(""data.csv""))
 user system elapsed 
 0.66 0.00  0.66> dim(dt)
[1] 439541 18As you saw, loading data with fread is 16x faster than the base function read.csv. fread() is faster than read.csv() because, read.csv() tries to first read rows into memory as character and then tries to convert them into integer and factor as data types. On the other hand, fread() simply reads everything as character.2. It is even faster than the popular dplyr, plyr packages used for data manipulation. data.table provides enough room for tasks such as aggregating, filtering, merging, grouping and other related tasks. For example:> system.time(dt %>% group_by(Store_ID) %>% filter(Gender == ""F"") %>%                    summarise(sum(Transaction_Amount), mean(Var2))) #with dplyr
 user system elapsed 
 0.13 0.02  0.21 > system.time(dt[Gender == ""F"", .(sum(Transaction_Amount), mean(Var2)), by = Store_ID])
 user system elapsed 
 0.02 0.00  0.01data.table has processed this task 20x faster than dplyr. It happened because it avoids allocating memory to the intermediate steps such as filtering. Also, dplyr createsdeep copiesof the entire data frame where as data.table does a shallow copy of the data frame.Shallow copy means that the data is not physically copied in systems memory. Its just a copy of column pointers (names). Deep copy copies the entire data to another location in the memory. Hence, with memory efficiency, the speed of computation is enhanced.3. Not just reading files, writing the files using data.table is much faster than write.csv(). This packages provides fwrite() function enabled with parallelised fast writing ability. So, next time you get to write 1 million rows, try this function.4. In built features such as automatic indexing, rolling joins, overlapping range joins further enhances the user experience while working on large data sets.Therefore, you see there is nothing wrong with data.frame, it just lacks the wide range of features and operations that data.table is enabled with.The idea of this tutorial is to provide you handy commands which can speed up your modeling process. Actually, there is so much to explore in this packages, chances are you might get puzzled from where to start, which command to stick with and when to use a particular command. Here, I provide answer to some of the most common questions which you come across while doing data exploration / manipulation.The data set used below can be download from here: download. The data set contains 1714258 rows of 12 columns. It will be interesting to see, how long does the data.table takes in loading this data. Time for action!Note: The data set contains uneven distribution of observations i.e. blank columns and NA values. The reason of taking this data is to check the performance of data.table on large data sets.#set working directory
> setwd("".../AV/desktop/Data/"")#load data
> DT <- fread(""GB_full.csv"")
Read 1714258 rows and 12 (of 12) columns from 0.189 GB file in 00:00:07It took only 7 seconds to read this file. Do try at your end.#subsetting rows
> sub_rows <- DT[V4 == ""England"" & V3== ""Beswick""]#subsetting columns
> sub_columns <- DT[,.(V2,V3,V4)]In a data table, columns are referred to as variables. Therefore, we dont need to refer to variables as DT$column name, column name alone works just fine. If you do DT[,c(V2,V3,V4)], it would return a vector of values. Using .() symbol, wraps the variables within list() and returns data table. In fact, every data table or data frame is a compilation of list of equal length and different data types. Isnt it?Subsetting data can be done even faster settingkeys in data table. Keys are nothing but supercharged rownames. A part of it has been demonstrated below.#ordering columns
> dt_order <- DT[order(V4, -V8)]Order function is data table is much faster than base function order(). Reason being, order in data table uses radix order sort which impart additional boost. - sign results in descending order.#add a new column
> DT[, V_New := V10 + V11]We did not assign the results back to DT. This is because, := operator modifies the input object by reference. It results in shallow copies in R which leads to better performance with less memory usage. The result is return invisibly.#update row values
> DT[V8 == ""Aberdeen City"", V8 := ""Abr City""]With this line of code, weve updated Aberdeen City to Abr City in column V8.#delete a column
> DT [,c(""V6"",""V7"") := NULL ]Check View(DT).We see that the data containsblank columns in the data set.It can be removed using the code above. In fact, all the three steps can be done in command as well. This is known as chaining of commands.> DT[V8 == ""Aberdeen City"", V8 := ""Abr City""][, V_New := V10 + V11][,c(""V6"",""V7"") := NULL]Lets calculate mean of V10 variable on the bases of V4 (showing country).#compute the average
> DT[, .(average = mean(V1o)), by = V4]#compute the count
> DT[, .N, by = V4].N is a special variable in data.table used to calculate the count of values in a variable. If you wish to obtain the order of the variable specified with by option, you can replace by with keyby. keyby automatically orders the grouping variable in ascending order.keys in data table delivers incredibly fast results. We usually set keys on column names which can be of any type i.e. numeric, factor, integer, character. Once a key is set of a variable, it reorders the column observations in increasing order. Setting a key is helpful, specially when you know that you need to make multiple computations on one variable.#setting a key
> setkey(DT, V4)Once, the key is set, we can subset any value from the key. For example:#subsetting England from V4
> DT[.(""England"")]Once the key is set, we no longer need to provide the column name again and again. If we were to look for multiple values in a column, we can write it as:>DT[.(c(""England"", ""Scotland""))]Similarly, we can set multiple keys as well. This can be done using:> setkey(DT, V3, V4)We can again, subset value from these two columns simultaneously using:> DT[.(""Shetland South Ward"",""Scotland"")]There are several other modifications which can be done in the 5 steps demonstrated above. These 5 steps illustrated above will help you to perform the basic data manipulation tasks using data.table. To learn more, I would suggest you to start using this package in your every day R work. Youd face various hurdles and thats where your learning curve would accelerate. You can also check the official data.table guide here.This article is written to provide you a path using which you can easily deal with large data sets. No longer, you need to spend money on upgrading your machines, instead its time to upgrade your knowledge of dealing with such situations. Apart from data.table, there are several other packages for parallel computing available. But, I dont see any need to any other package for data manipulation, once you become proficient with data.table.In this article, I discussed about some important aspects which every beginner in R must know while working on large data sets. After data manipulation, the very next hurdle which comes is model building. With large data sets, packages like caret, random forest, xgboost takes a lot of time in computation. Has it occurred to you?I plan to provide an interesting solution in my post next week! Do let me know your pain points in dealing with large data stets.Did you like reading this article? Which other package do you use when dealing with large data sets? Drop your suggestions / opinions in the comments.",https://www.analyticsvidhya.com/blog/2016/05/data-table-data-frame-work-large-data-sets/
"Big Data Architect Mumbai (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|data.table() vs data.frame()  Learn to work on large data sets in R|How to predict waiting time using Queuing Theory ?|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Big Data ArchitectLocation  MumbaiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBig Data Architect Mumbai and the following details:,https://www.analyticsvidhya.com/blog/2016/04/big-data-architect-mumbai-5-years-experience/
How to predict waiting time using Queuing Theory ?,Learn everything about Analytics|Introduction|Table of Contents|What is Queuing Theory ?|Kendalls notation|Parameters of Interest|LittleTheorem|Case Study 1|Case Study 2|End Notes,"Solution|Solution|You cantest your skills and knowledge.Check outLiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Big Data Architect Mumbai (5+ years of experience)|15 Must Read Books for Entrepreneurs in Data Science|
Tavish Srivastava
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Queuing Theory, as the name suggests, is a study of long waiting lines done to predict queue lengths and waiting time. Its a popular theoryused largelyin the field of operational, retail analytics. In my previous articles, Ive already discussed the basic intuition behind this concept with beginnerand intermediate levelcase studies.This is the last articleof this series. Hence, make sure youve gone through the previous levels (beginnerand intermediate).Until now, we solved cases where volume of incoming calls and duration of call was known before hand. In real world, this is not the case. In real world, we need to assume a distribution for arrival rate and service rate and act accordingly. I wish things were less complicated! Lets understand these terms:Arrival rate is simply a resultof customer demand and companies donthave control on these. Service rate, on the other hand, largely depends on how many caller representative are available to service, what is their performance and how optimized is their schedule.In this article, I will bring you closer to actual operations analytics usingQueuing theory. We will also address few questions which we answered in a simplistic manner in previous articles.As discussed above, queuing theory is a study oflong waiting lines done to estimate queue lengths and waiting time. It uses probabilistic methods to make predictions used in the field of operational research, computer science, telecommunications, traffic engineering etc.Queuing theory was first implemented in the beginning of 20th century to solve telephone calls congestion problems. Hence, it isnt any newly discovered concept. Today,this conceptis being heavily used bycompanies such asVodafone, Airtel, Walmart, AT&T, Verizon and many more to prepare themselves for future traffic before hand.Lets dig into this theory now. Well now understandan important concept of queuing theory known as Kendalls notation & Little Theorem.This notation canbe easily applied to cover a large number of simple queuing scenarios.This is a shorthand notation of the typeA/B/C/D/E/FwhereA, B, C, D, E,Fdescribe the queue. Every letter has a meaning here. The various standard meanings associated with each of these letters are summarized below.A is the Inter-arrival Time distribution . Here are the possible values it can take :B is the Service Time distribution. Here are the possible values it can take:C gives the Number of Servers in the queue. It has to be a positive integer.D gives the Maximum Number of jobs which areavailable in the system counting both those who are waiting and the ones in service. In case, if the number of jobs arenotavailable, then the default value of infinity () is assumed implying that the queue has an infinite number of waiting positionsE gives the number of arrival components. In general, we take this to beinfinity () as our system accepts any customer who comes in. However, in case of machine maintenance where we have fixed number of machines which requires maintenance, this is also a fixed positive integer.F represents the Queuing Discipline that is followed. The typical ones are First Come First Served (FCFS), Last Come First Served (LCFS), Service in Random Order (SIRO) etc.. If this is not given, then the default queuing discipline of FCFS is assumed. Possible values are :The simplest member of queue model is M/M/1///FCFS.This means that we have a single server; the service rate distribution is exponential; arrival rate distribution is poisson process; with infinite queue length allowed and anyone allowed in the system; finally its a first come first served model.A queuing model works with multiple parameters. These parameters help us analyze the performance of our queuing model. Think of what all factors can we be interested in? Here are a few parameters which we would beinterested for any queuing model:Its an interesting theorem. Lets understand it using an example.Consider a queue that has a process with mean arrival rate ofactually entering the system. LetNbe the mean number of jobs (customers) in the system (waiting and in service) andWbe the mean time spent by a job in the system (waiting and in service).Littles Resultthen states that these quantities will be related to each other as:N=WThis theorem comes in very handy to derive the waiting time given the queue length of the system.Parameters for 4 simplest series:1. M/M/1//Here, N and Nq arethe number of people in the system and in the queue respectively. Also W and Wq are the waiting time in the system and in the queue respectively. Rho is the ratio of arrival rate to service rate. Also the probabilities can be given as :where, p0 is the probability of zero people in the system and pk is the probability of k people in the system.2. M/M/1//Queuewith Discouraged Arrivals :This is one of the common distribution because the arrival rate goes down if the queue length increases. Imagine you went to Pizza hut for a pizza party in a food court. But the queue is too long. You would probably eat something else just because you expect high waiting time.As you can see the arrival rate decreases with increasing k.3. M/M/c//With c servers the equations become a lot more complex. Here are the expressions for such Markov distribution in arrival and service.Imagine, you work for a multi national bank. You have the responsibility of setting up the entire call center process. You are expected to tie up with a call centre and tell them the number of servers you require.You are setting up this call centre for a specific feature queries of customers which has an influx of around 20 queries in an hour. Each query take approximately 15 minutes to be resolved. Find out the number of servers/representatives you need to bring down the average waiting time to less than 30 seconds.The given problem is a M/M/c type query with following parameters.Lambda = 20
Mue = 4Here is an R code that can find out the waiting time for each value of number of servers/reps.Here are the values we get for waiting time:A negative value of waiting time means the value of the parameters is not feasible and we have an unstable system. Clearly with 9 Reps, our average waiting time comes down to 0.3 minutes.Lets take a more complex example.Imagine, you are the Operations officer of a Bank branch. Your branch can accommodate a maximum of 50 customers. How many tellers do you need if the number of customer coming in with a rate of 100 customer/hour and a teller resolves a query in 3 minutes ?You need to make sure that you are able to accommodate more than 99.999% customers. This means only less than 0.001 % customer should go back without entering the branch because the brach already had 50 customers. Also make sure that the wait time is less than 30 seconds.This is a M/M/c/N = 50/ kind of queue system. Dont worry about the queue length formulae for such complex system (directly use the one given in this code). Just focus on how we are able to find the probability of customer who leave without resolution in such finite queue length system.Here is the R-codeClearly you need more 7 reps to satisfy both the constraints given in the problem where customers leavingWith this article, we have now come close to how to look at an operational analytics in real life. The application of queuing theory is not limited to just call centre or banks or food joint queues. It expands to optimizing assembly lines in manufacturing units or IT software development process etc.Did you like reading this article ? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/04/predict-waiting-time-queuing-theory/
15 Must Read Books for Entrepreneurs in Data Science,Learn everything about Analytics|Introduction|Why read books ?|List of Books|End Notes,"Data Science For Business|Big Data at Work|Lean Analytics|Moneyball|Elon Musk|Keeping up with the Quants|The Signal and the Noise|When Genius Failed|Lean Startup|Web Analytics 2.0|Predictive Analytics|Freakonomics|Founders at Work|Bootstrapping a Business|Analytics at Work|Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|How to predict waiting time using Queuing Theory ?|Its our 3rd Birthday  Come & Celebrate|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The roots of entrepreneurship are old. But, the fruits were never so lucrative as they have been recently. Until 2010, not many of us had heardof the term start-up. And now, not a day goes by when business newspapers dont quote them. There is sudden gush in the level of courage which people possess.Today, I see1 out of 5 person talking about a new business idea. Some of them even succeed too in establishing their dream company. But, only the determined ones sustain. In data science, the story is bitdifferent.The success in data science is mainlydriven by knowledge of the subject. Entrepreneurs are notrequired to work at ground level, but must have sound knowledge of how it is being done. What algorithms, tools, techniques are being used to create products & services.In order to gain this knowledge, you have two ways:I would opt forsecond option.Think of our brain as a library. And, its a HUGElibrary.How would an empty library look like? If I close my eyes and imagine, I see dust, spider webs, brownian movement of dust particles and darkness. If this imagination horrifies you, then start reading books.The books listed below gives immense knowledge and motivation in technology arena. Reading these books will give you the chance to live many different entrepreneurial lives. Take them one by one. Dont get overwhelmed. Ive displayed a mix of technical and motivational books for entrepreneurs in data science. Happy Reading!Note: I do not intend to promote any book. Neither I have any affiliation with amazon. However, for your perusal, I have shared amazon links in case you are interested to buy any.This book is written byFoster Provost & Tom Fawcett. It gives a great head start to anyone, who isserious about doing business with big data analytics. It makes you believe, data is now business. No business in the world, can now sustain without leveraging the power of data. This books introduces you to real side of data analysis principles and algorithms without technical stuff. It gives you enough intuition and confidence to lead a team of data scientists and recommend whats required. More importantly, it teaches you the winning approach to become a master at business problem solving.Get the book: Buy NowThis book is written by Thomas H. Davenport. It reveals the increasing importance of big data in organizations. It talks with interesting numbers, researches and statistics. So until 2009, companies worked on data samples. But with advent of powerful devices and data storage capabilities, companies now work on wholedata. They dont want to miss even a single bit of information.This book unveils the real side of big data, its influence on our daily lives, on companies and our jobs. As an entrepreneur, it is extremely important for you understand big data and its related terminologies.Get the book: Buy NowThis book is written by Alistair Croll and Benjamin Yoskovitz. Its one of the most appreciated books on data startups. It consist of practical & detailed researches, advice, guidance which can help you to build your startup faster. It gives enough intuition to build data driven products and market them. The language is simple to understand. There are enough real world examples to make you believe, a business needs data analytics like a human needs air. To an entrepreneur, this will introduce the practical side of product development and what it takes to succeed in a red ocean market.Get the book: Buy NowThis book is written by Michael Lewis. Its a brilliant tale which sprinkles someserious inspiration. A guy named billy bean does what most of the world failed to imagine, just by using data and statistics. He paved the path to victory when situations werent favorable. Running a business needs continuous motivation. This can be a good place to start with. However, this book involves technical aspects of baseball. Hence, if you dont know baseball, chances are you might struggle in initial chapters. A movie also has been made on this book. Do watch it!Get the book: Buy NowThis book is written by Ashlee Vance. Im sure none of us are fortunate tolive the life of Elon Musk, but this book lets us dive in his life and experience rise of fantastic future. Elon is the face behind Paypal, Tesla and SpaceX. He has dreamed of making space travel easy and cheap. Recently, he was applauded by Barack Obama for the successful landing of his spaceship in an ocean. People admire him. They want to know his secrets and this is where you can look for. As on entrepreneur, you will learn about must have ingredients which you need to a become successful in technology space.Get the book: Buy NowThis book is written by Thomas H Davenport and Jinho Kim. As we all know, data science is driven by numbers & maths (quants). Inspired from moneyball, this book teaches you the methods of using quantitative analysis for decision making. An entrepreneur is a terminal of decision making. One must learn to make decisions using numbers & analysis, rather than intuition. The language of this book is easy to understand and suited for non-maths background people too. Also, this book will make you comfortable with basics statistics and quantitative calculations in the world of business.Get the book: Buy NowThe author of this book is Nate Silver, the famous statisticianwho correctly predicted US Presidential elections in 2012. This books shows the real art and science of making predictions from data. This art involves developing the ability to filter out noise and make correct predictions. It includes interesting examples which conveys the ultimate reason behind success and failure of predictions. With more and more data, predictions have become prone to noise errors. Hence, it is increasingly important to understand the science behind making predictions using big data science. The chapters of this book are interesting and intuitive.Get the book: Buy NowThis book is written by Roger Lowenstein. Itis an epic story of rise and failure of a hedge fund.For an entrepreneur, this book has ample lessons on investing, market conditions and capital management. Its a story of a small bank, which used quantitative techniques for bond pricing throughout the world and ensured every invested made gives a profitable results. However, they didnt sustain for long. Their quick rise was succeeded by failure. And, the impact of their failure was so devastating that US Federal bank stepped in to rescue the bank, because the funda bankruptcy would have large negative influence on worlds economy.Get the book: Buy NowThis book is written by Eric Ries. In one line, itteaches how to not to fail at the start of your business. It reveals proven strategies which are followed by startups around the world. It has abundance of stories to make you walk on the right path. An entrepreneur should read it when he/she feel like draining out of motivation. It teaches to you to learn quickly, implement new methods and act quickly if something doesnt work out. This book applies to all industries and is not specific to data science.Get the book: Buy NowThis book is written by Avinash Kaushik. It is one of the best book to learn about web analytics. Internet is the fastest mode of collecting data. And, every entrepreneur must learn the art of internetaccounting. Most of the businesses today face the challenge of weak presence on social media and internet platforms. Using various proven strategies and actionable insights, this book helps you to solve various challenges which could hamper your way. It also provides a winning template which can be applied in most of the situations. It focuses on choosing the right metric and ways to keep them in control.Get the book: Buy NowThis book is written by Eric Seigel. It is a good follow up book after web analytics 2.0. So, once youve understood the underlying concept of internet data, metrics and key strategies. This book teaches you the methods of using that knowledge to make predictions. Its simple to understand and covers many interesting case studies displaying how companies predict our behavior and sell us products. It doesnt cover technical aspects, but explains the general working on predictive analytics and its applications. You can also check out this funny rap video by Dr. Eric Seigel:Get the book: Buy NowThis book is written by Steven D Levitt and Stephen J Dubner. It shows the importance of numbers, data, quantitative analysis using various interesting stories. It says, there is a logic is everything which happens around us. Reading this book will make you aware ofthe unexplored depth at which data affects our real lives. It draws interesting analogy between school teachers and sumo wrestlers. Also, the bizarre stories featuring cases of criminal acts, real-estate, drug dealers will certainly add up to your exciting moments.Get the book: Buy NowThis book is written by Jessica Livingston. Again, this isnt data science specific but a source of motivation to get you moving forward. Its a collection of interviews with the founders of various startups across the world. The focus has been kept on early days i.e. how did they act when they started. This book will give you enough proven ideas, strategies and lessons to anticipate and avoid pitfalls in your initial days of business. It consist of stories by Steve Wozniak (Apple), Max Levchin (Paypal), Caterina Fake (Flikr) and many more. In total, there are 32 interviews listed which means you have the chance to learn from 32 mentors in one single book. Must read for entrepreneurs.Get the book: Buy NowThis book is written by Greg Gianforte and Marcus Gibson. It teaches about the things to do when you are running short of money and still dont want to stop. This is a must read book for every entrepreneur. Considering the amount of investment required in data science startups, this book should have a special space in an entrepreneurs heart. It reveals various eye opening truths and strategies which can help you build a great company. Greg and Marcus proves that money is not always the reason for startup failure, its all about founders perspective. This book has stories of success and failures, again a great chance for you to live many lives by reading this book.Get the book: Buy NowThis book is written by Thomas H Davenport, Jeanne G Harris and Robert Morrison. This books reveals the increased use of analytical tools & concepts by managers to make informed business decisions. The decision making process has accelerated. For a greater impact, it also consists of examples from popular companies like hotels.com, best buy and many more. It talks about recruiting, coordination with people and the use of data and analytics at an enterprise level. Many of us are aware of data and analytics. But, only a few know how to use them together. This quick book has it all !Get the book: Buy NowDisclosure:The amazon links in this article are affiliate links. If you buy a book through this link, we would get paid through Amazon. This is one of the ways for us to cover our costs while we continue to create these awesome articles. Further, the list reflects our recommendation based on content of book and is no way influenced by the commission.This marks the end of this list. While compiling this list, I realized most of these books are about sharing experience and learning from the mistake of others. Also, it is immensely important to posses quantitative ability to become good in data science. I would suggest you to make a reading list and stick to it throughout the year. You can take up any book to start. Id suggest to start with a motivational book.Have you read any other book ? What were your key takeaways? Did you like reading this article? Do share your knowledge & experiences in the comments below.",https://www.analyticsvidhya.com/blog/2016/04/15-read-books-entrepreneurs-data-science/
Its our 3rd Birthday  Come & Celebrate,Learn everything about Analytics|A big Thank You to our community. We just turned 3!|My Experience My Way||Whats the celebration plan?|For Beginners:|For Intermediates:|For Experts:,"Share this:|Like this:|Related Articles|15 Must Read Books for Entrepreneurs in Data Science|Practical Guide to implementing Neural Networks in Python (using Theano)|
Kunal Jain
|52 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Experiments with Data (Analytics Workshop for Beginners)|The Creative Analyst (Data Visualization competition)||The Strategist (Strategic Thinking Competition)|The Seers Accuracy (Data Science Competition)|The Seers Accuracy (Data Science Competition)|McKinsey Hiring Hack,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"On this day, 3 years back  I started Analytics Vidhya. I knew I had to address the glaring knowledge gap in the industry, but I did not know How to address the knowledge gap?. So I started Analytics Vidhya as a blog. Over these 3 years, we first blossomed as a blog and have now transitioned the blog into a community. Going by a popular statistic, less than 10% of start-ups see this fate. We have been lucky to do so.God has been kind and so has been the awesome support of our community members. I have called up many of you at odd times with seemingly weird requests, but never has a request been turned down  no matter what it is.I feel proud and blessed to know you and be a part of shaping this community.Back in 2014 I left my well paying job to raise 2 babies together. Jenika (my daughter) was barely 6 months then and Analytics Vidhya was starting to get a few thousand visitors a month. I knew I wanted to create a knowledge engine for the world, but didnt know how I would do it.Thankfully, I didnt dwell muchon that question. We just focused on adding quality articles and resources to our portal. With each article and each question this community wrote, we served many more people with high quality knowledge. Obviously, there were those odd days, when I felt things are not moving as they should. But thankfully, they were small in number and I had the support of my friends, family, our awesome team at AV and the larger community.I truly believe that the best is yet to come  now more than ever. All our new initiatives in last year have seen tremendous response from the community and we have no plan to change that any time soon.Today is our 3rd Birthday!Analytics Vidhya has completed 3 years of its dedicated supportto global analytics community. And, weve made sure there is a lot on table for you to feast. Lets begin the party!Yeah! I knew many of you wanted to know this.To make this occasion special, we have set the table with many exciting options. Also, this occasion marks the beginning of AV Data Fest 2016.We didnt want geographical boundaries to be stop you from joining the party. Thus, this is an online festevent where several activities such as competitions, workshopshave been organizedKeeping in mind the various levels of expertise people possess, we have something for every category of data practitioners:We have arranged a workshop whichis meant for anyone interested in learning more about data analytics.You can attend this even if you dont have any prior experience in analytics. Some background in statistics will help, but is not mandatory. All your concepts will get clear from the video tutorials and content we are providing. You will also get to work on a practice problem at the end for which you will be getting a score to evaluate yourself.Participate HereAre you looking to develop additional skills that will make your demand higher in the job market? Then you need to practice and exploit your skills. Here we are! with a series of hackathons for you. Test your competence level and get to understand where you need to work more.Do you possess the blend of creativity and analytics?Participate HereCan you take an unstructured business problem and solve it using your structured thinking? Take this challenge and find out.Participate HereIf this does not fulfill your appetite, then you can try something more challenging.Participate HereSo now you possess the tag of being into the sexiest profession and know how to deal with large amount of data. This is the chance for you to get the limelight by showing off your acquired skills to the community of data scientists out there. And what if we say that you also stand a chance to grab the best in industry job profile.A 3 day contest where you will get to play with your machine learning or data mining abilities.Participate HereThe branditself suggests the value of this hiring hackathon. You mayget to work with one of the most famous multinational company.Participate HereConsidering the response we have got from our community members I feel a deep responsibility to make most of my time here and serve you the best I can.In the end, I would personally like to know your feedback so that we can improve, your experience (good or bad) or a success story.Please feel free to drop a personal note at [emailprotected] or leave a comment here. Ill be waiting to hear from you.Cheers!",https://www.analyticsvidhya.com/blog/2016/04/celebrating-3-years-analytics-vidhyas-success/
Practical Guide to implementing Neural Networks in Python (using Theano),Learn everything about Analytics|Introduction|Table of Contents|1. Theano Overview|2. Implementing Simple Expressions|3. TheanoVariable Types|4. Theano Functions|5. Modeling a Single Neuron|6. Modeling a Two-Layer Neural Network|End Notes,"Feed Forward Pass|Backward Propagation|Step 1: Define variables|Step 2: Define mathematical expression|Step 3: Define gradient and update rule|Step 4: Train the model|Check out LiveCompetitionsand compete with bestData Scientists aroundthe world.|Share this:|Like this:|Related Articles|Its our 3rd Birthday  Come & Celebrate|Case Study For Freshers (Level : Medium)  Call Center Optimization|
Aarshay Jain
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Step 0: Import libraries|Step 1: Define variables|Step 2: Define expression|Step 3: Evaluate Expression|Return Multiple Values|Computing Gradients,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In my last article, I discussed the fundamentals of deep learning, where I explained the basic working of a artificial neural network. If youve been following this series, today well become familiar with practical process of implementing neural network in Python (using Theano package).I found various other packages also such as Caffe, Torch, TensorFlow etc to do this job. But, Theano is no less than and satisfactorily execute all the tasks. Also, it has multiple benefits which further enhances the coding experiencein Python.In this article, Illprovide a comprehensive practical guide to implement Neural Networks using Theano. If you are here for just python codes, feel free to skip the sections and learn at your pace. And, if you are new to Theano, I suggest you to follow the article sequentially to gain complete knowledge.Note:In short, we can define Theano as:As popularly known, Theano was developed at the University of Montreal in 2008. It isused for defining and evaluating mathematical expressions in general.Theano has several features whichoptimize the processing time of expressions. For instanceitmodifies the symbolic expressions we define before converting them to C codes. Examples:Below are some powerfuladvantages of using Theano:Lets now focus on Theano (with example) and try to understand it as a programming language.Lets start by implementing a simple mathematical expression, say a multiplication in Theano and see how the system works. In later sections, we will take a deep dive into individual components.The general structure of a Theano code works in 3 steps:Lets look at the following code for simply multiplying 2 numbers:Here, we have simply imported 2 key functions of theano  tensor and function.Here 2 variables are defined. Note that we have used Theano tensor object type here. Also, the arguments passed to dscalar function are just name of tensors which are useful while debugging. Theycode will work even without them.Here we have defined a function f which has 2 arguments:Now we are simply calling the function with the 2 inputs and we get the output as a multiple of the two. In short, we saw how we can define mathematical expressions in Theano and evaluate them. Before we go into complex functions, lets understand some inherent properties of Theano which will be useful in building neural networks.Variables are key building blocks of any programming language. In Theano, the objects are defined as tensors. A tensor can beunderstood as a generalized form of a vector with dimension t. Different dimensions are analogousto different types:Watch thisinteresting video to geta deeper level of intuition into vectors and tensors.These variables can be defined similar to our definition of dscalar in the above code. The various keywords for defining variables are:Now you understand thatwe can define variables with different memory allocations and dimensions. But this is not an exhaustive list. We can define dimensions higher than 4 using a generic TensorType class. Youll find more details here.Please note that variables of these types are just symbols. They dont have a fixed value and are passed into functions as symbols. They only take values when a function is called. But, we often need variables which are constants and which we need not pass in all the functions. For this Theano provides shared variables. These have a fixed value and are not of the types discussed above. They can be defined asnumpydata types or simple constants.Lets take an example. Suppose, we initializea shared variable as 0 and use a function which:This can be done as:Note that here function has an additional argument called updates. It has to be a list of lists or tuples, each containing 2 elements of form (shared_variable, updated_value).The output for 3 subsequent runs is:You can see that for each run, it returns the square of the present value, i.e. the value before updating. After the run, the value of shared variable gets updated. Also, note that shared variables have 2 functions get_value() and set_value() which are used to read and modify the value of shared variables.Till now we saw the basic structure of a function and how it handles shared variables.Lets move forward and discuss couple more things we can do with functions:We can return multiple values from a function. This can be easily done as shown in following example:We can see that the output is an array with the square and cube of the number passed into the function.Gradient computation is one of the most important part of training a deep learning model. This can be done easily in Theano. Lets define a function as the cube of a variable and determine its gradient.This returns 48 which is 3x2 for x=4. Lets see how Theano has implemented this derivative using the pretty-print feature as following:In short, it can be explained as: fill(x3,1)*3*x3-1 You can see that this is exactly the derivative of x3. Note that fill(x3,1) simply means to make a matrix of same shape as x3 and fill it with 1. This is used to handle high dimensionality input and can be ignored in this case.We can use Theano to compute Jacobian and Hessian matrices as well which you can find here.There are various other aspects of Theano like conditional and looping constructs.You can go into further detail using following resources:Lets start by modeling a single neuron.Note that I will take examples from my previous article on neuron networks here. If you wish to go in the detail of how these work, please read this article. For modeling a neuron, lets adopt a 2 stage process:Lets implement an AND gatefor this purpose.An AND gate can be implemented as:Now we will define a feed forward network which takes inputs and uses the shown weights to determine the output. First we will define a neuron which computes the output a.I have simply used the steps we saw above. If you are not surehow this expression works, please refer to the neural networks article I have referred above. Now lets test out all values in the truth table and see if the AND function has been implemented as desired.Note that, in this case we had to provide weights while calling the function. However, we will be required to update them while training. So, its better that we define them as a shared variable. The following code implements w as a shared variable. Try this out and youll get the same output.Now the feedforward step is complete.Now we have to modify the above code and perform following additional steps:Lets initialize the network as follow:Note that, you will notice a change here as compared to above program. I have defined x as a matrix here and not a vector. This is more of a vectorized approach where we will determine all the outputs together and find the total cost which is required for determining the gradients.You should also keep in mindthat I am using the full-batch gradient descent here, i.e. we will use all training observations to update the weights.Lets determine the cost as follows:In this code, we have defined a_hat as the actual observations. Then we determine the cost using a simple logistic cost function since this is a classification problem. Now lets compute the gradients and define a means to update the weights.In here, we are first computing gradient of the cost w.r.t. the weights for inputs and bias unit. Then, the train function here does the weight update job. This is an elegant but tricky approach where the weights have been defined as shared variables and the updates argument of the function is used to update them every time a set of values are passed through the model.Here we have simply defined the inputs, outputs and trained the model. While training, we have also recorded the cost and its plot shows that our cost reduced towards zero and then finally saturated at a low value. The output of the network also matched the desired output closely. Hence, we have successfully implemented and trained a single neuron.I hope you have understood the last section. If not, please do read it multiple times and proceed to this section. Along with learning Theano, this will enhance your understanding of neural networks on the whole.Lets consolidate our understandingby taking a 2-layer example. To keep things simple, Ill take the XNOR example like in my previous article. If you wish to explore the nitty-gritty of how it works, I recommend reading the previousarticle.The XNOR function can be implemented as:As a reminder, the truth table of XNOR function is:Now we will directly implement both feed forward and backward at one go.In this step we have defined all the required variables as in the previous case. Note that now we have 3 weight vectors corresponding to each neuron and 2 bias units corresponding to 2 layers.Here we have simply defined mathematical expressions for each neuron in sequence. Note that here an additional step was required where x2 is determined. This is required because we want the outputs of a1 and a2 to be combined into a matrix whose dot product can be taken with the weights vector.Lets explore this a bit further. Both a1 and a2 would return a vector with 4 units. So if we simply take an array [a1, a2] then well obtain something like [ [a11,a12,a13,a14], [a21,a22,a23,a24] ]. However, we want this to be [ [a11,a21], [a12,a22], [a13,a23], [a14,a24] ]. The stacking function of Theano does this job for us.This is very similar to the previous case. The key difference being that now we have to determine the gradients of 3 weight vectors and 2 bias units and update them accordingly.We can see that our network has successfully learned the XNOR function. Also, the cost of the model has reduced to reasonable limit. With this, we have successfully implemented a 2-layer network.In this article, we understood the basics of Theano package in Python and how it acts as a programming language. We also implemented some basic neural networks using Theano. I am sure that implementing Neural Networks on Theano will enhance your understanding of NN on the whole.If hope you have been able to follow till this point, you really deserve a pat on your back. I can understand that Theano is not a traditional plug and play system like most of sklearns ML models. But the beauty of neural networks lies in their flexibility and an approach like this will allow you a high degree ofcustomization in models. Some high-level wrappers of Theano do exist like Keras and Lasagne which you can check out. But I believe knowing the core of Theano will help you in using them.Did you find this article useful ? Please feel free to share your feedback and questions below. Eagerly waiting to interact with you!",https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/
Case Study For Freshers (Level : Medium)  Call Center Optimization,Learn everything about Analytics|Introduction|Business Case(Level : Medium)|Data you need to deal with|Lets find the Solution|Final Results|End Notes,"Explore Data|Solution|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Related Articles|Practical Guide to implementing Neural Networks in Python (using Theano)|A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)|
Tavish Srivastava
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Last week, I introduced you to a classic problem of operational analytics. If you didnt get a chance to check it, you can do it right here.I had drafted it mainly for freshers who lack confidence in solving case studies. And, this becomes one of the key reason for their rejection in job interviews. Now, if you are still reading this, I take it that you ready to walk the next level with me!I made the first levelsimple to understand to get you wanting to go to the next. It just required a logical understanding of how things happen in a call center.However, that was an over simplified version of what actually happens in a call center. In this article, I will take a step forward and talk about a more real life case in a call center optimization problem. I believe it to be more helpful for R users as Ive demonstrated the codes in R. However, even if you dont know R, you can still work your way out in Excel.Make sure you check out the Ace Data Science Interviews course for multiple such case studies. We have put together a very comprehensive course to help you successfully land your first data science job  dont miss it!We assumed multiple things in the previouscase study. Some of which were:Lets ease outthe first assumption to make the case study more realistic.Assume, you are setting up a call center for a mid-sized E-Commerce firm. You have been asked to find the total strength of callers required for this requirement. This requirement will be outsourced to a call center which guarantees availability of caller for 24 hours with the exact same efficiency.Using this efficiency, you have also estimated the time of each call from the customer and the duration of these calls. This estimation is based on your market research and prediction through customer behavior in past. You can assume that this prediction is accurate. Now, you need to estimate the following:The data provided to you is of 10k calls which are made in a day. You can download the data here. The data looks something like this :Here are few things you should consider:As always I say, its essential to explore and analyze data distribution at first. So, here is the distribution of call duration in the data:As you can observe most of the calls end up (call duration)between 3-7 minutes with a peak at 5 minutes. Lets get to the next variable.Here is the distribution of Call Timing :To me, it also looks normally distributed i.e. it follows a similar shape like previous graph. We see maximum calls arrivebetween 9am to 4 pm with a peak at 12 noon.We are done with exploring data. Now, well get to the solution.Let us start with a very simple solution. If we ignore the time when the calls were made, the sum of all call duration comes out to be 50635 minutes.Available time for a caller(24*60)= 1440 minutesNumber of callers required = (50635/ 1440) = 35.14So we need approx. 36 callers if we had the choice to call back the customer whenever our caller is free. So, during interviews, when you dont get much time but need an intuitive solution, such kind of assumptions work well!But the real life is not that simple. Here we need to account for the time at which customer called the call center.Therefore, for the actual solution, you will need to simulate for every customer  caller combination. I am doing it in R, you can use any tool such as excel, python to accomplish this. Here is a simple R code:#set working directory
> setwd(""C:\\Tavs\\CC"") #Read data
> data <- read.csv(""Case_Level2.csv"")
> summary(data)#Create a matrix where we will store the maximum waiting time for each value of the number of callers
> caller_opt <- matrix(0,100,2)#Run loop for every number of callers possible. Here we have taken the range from 1 to 100
> for (number_of_callers in (1:100)){
       #Initialize the available time for each caller
       caller <- rep(0,number_of_callers)       #Index will be used to refer a caller
       index <- 1:number_of_callers       #Here we store the difference of each callers availability from the time when        the call was made
       caller_diff <- rep(0,number_of_callers)       #We add two columns to the table : Caller assigned to the customer & Wait time        for the customer
       data$assigned <- 1
       data$waittime <- 0
       for (i in 1:length(data$Call))
       {
          caller_diff <- data$Time[i] - caller
          best_caller_diff <- max(caller_diff)
          index1 <- index[min(index[caller_diff == best_caller_diff])]
          data$assigned[i] <- index1
          data$waittime[i] <- max(-best_caller_diff,0)
          caller[index1] <- caller[index1] + data$Duration.of.calls[i] 
        }
        caller_opt[number_of_callers,1] = number_of_callers
        caller_opt[number_of_callers,2] = max(data$waittime)
        print(caller_opt[number_of_callers,])
}Here is what we get as theresult:As you can observe from the graph that deciding the right number of callers is immenselyimportant. Missing the number by just 10% can also increase the wait time for a customer significantly. In our case if we keep 4 call center reps less (~44), the maximum wait time for a customer becomes 87 minutes, which is something no company will ever want.Therefore,We have still managed to keep the case study simple enough by even varying the time of calling. However, two big assumptions still there are:Beyond these two assumption, we havent touched how to make predictions for call duration and call time. But this case study will give you a good feel of how to simulate an entire environment in such an operation intensive function. In futurecase studies, we will start relaxing these assumptions as well, making to simulation even more closer to reality.Did you like reading this article ? Can you think of other checks to make this case study mimicking the actual call center in a better way? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/04/case-study-level-medium-call-center-optimization/
A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. What is a Decision Tree ? How does it work ?|2. Regression Trees vs Classification Trees|3. How does a tree decide whereto split?|4. What are the key parameters of tree modelingand how can we avoid over-fitting in decision trees?|5. Are tree based models better than linear models?|6. Working with Decision Trees in R and Python|7. What are ensemble methods in tree based modeling ?|8. What is Bagging? How does it work?|9. What is Random Forest ? How does it work?|10. What is Boosting? How does it work?|11. Which is more powerful:GBM or Xgboost?|12. Working with GBM in R and Python|13. Working with XGBoost in R and Python|14. Where to practice ?||End Notes,"Types of Decision Trees|Important Terminology related toDecision Trees|Advantages|Disadvantages|Gini
|Chi-Square|Information Gain:|Reduction in Variance|Setting Constraints on Tree Size|Tree Pruning|How does it work?|Advantages of Random Forest|Disadvantages of Random Forest|Python & R implementation|How does it work?|GBM in R (with cross validation)|GBM in Python|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Case Study For Freshers (Level : Medium)  Call Center Optimization|Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|
Analytics Vidhya Content Team
|64 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Tree based learning algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based methods empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).Methods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. Hence, for every analyst (fresher also), its important to learn these algorithms and use them for modeling.This tutorial is meant to help beginners learn tree based modeling from scratch. After the successful completion of this tutorial, one is expected to become proficient at using tree based algorithms andbuild predictive models.Note: This tutorial requires no prior knowledge of machine learning. However, elementary knowledge of R or Python will be helpful. To get started you can follow full tutorial in R and full tutorial in Python. You can also check out the Introduction to Data Science course covering Python, Statistics and Predictive Modeling.Decision treeis a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used inclassification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator ininput variables.Tree based modeling in R and PythonExample:-Lets say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket inleisure time. Now, I want to create a model topredict who will play cricket during leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.This is where decision tree helps, it will segregate the students based on all values of three variable andidentify the variable, which creates thebest homogeneous sets of students (which are heterogeneous to each other). In the snapshot below, you can see that variable Gender is able to identify best homogeneous sets compared to the other two variables.As mentioned above, decision tree identifies the most significant variable and its value that gives best homogeneous sets of population. Now the question which arises is, how does it identify the variable and the split? To dothis, decision tree uses various algorithms, which we will discuss in the following section.Types of decision tree is based on the type of target variable we have. It can be of two types:Example:-Lets say we have a problem to predict whether a customer will pay his renewal premium with an insurance company(yes/ no). Here we know that income of customer is asignificant variable but insurance company does not have income details for all customers. Now, as we know thisis an important variable, then we can build a decision tree to predict customer income based on occupation, product and various other variables. In this case, we are predicting values for continuous variable.Lets look at the basic terminology used with Decision trees:These are the terms commonly used for decision trees. As we know that every algorithm has advantages and disadvantages, below are the important factors which one should know.We all know that the terminal nodes (or leaves) lies at the bottom of the decision tree. This means that decision trees are typically drawn upside down such that leavesare the the bottom & roots are the tops (shown below).Both the trees work almost similar to each other, lets look at the primary differences & similaritybetween classification and regression trees:The decision of making strategic splits heavily affects a trees accuracy. The decision criteria is different for classification and regression trees.Decision trees use multiplealgorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.The algorithm selection is also based on type of target variables. Lets look at the four most commonlyused algorithms in decision tree:Gini says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.Steps toCalculate Gini for a splitExample:  Referring to example used above, where we want to segregate the students based on target variable ( playing cricket or not ). In thesnapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini .Split on Gender:Similar for Split on Class:Above, you can see that Gini score for Split on Gender is higher than Split on Class,hence, the node split will take place on Gender.You might often come across the term Gini Impurity which is determined by subtracting the gini value from 1. So mathematically we can say,Gini Impurity = 1-GiniIt is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it bysum of squares of standardizeddifferences between observed and expected frequenciesof target variable.Steps toCalculate Chi-square for a split:Example: Lets work with above example that we have used to calculate Gini.Split on Gender:Split on Class:Perform similar steps of calculation for split on Class and you will come up with below table.Above, you can see that Chi-squarealso identify the Gender split is more significant compare to Class.Look at the image below and think which node can be described easily. I am sure, your answer isC because it requires lessinformation as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say thatC is a Pure node, B is less Impure and A is more impure.Now, we can build aconclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory isa measure to define this degree of disorganization in a systemknown as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50%  50%), it has entropy of one.Entropy can be calculated using formula:-Here p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.Steps to calculate entropy for a split:Example:Lets use this method to identify best split for student example.Above, you can see that entropy forSplit on Gender is the lowest among all,so the tree will split onGender. We can derive information gain from entropy as 1- Entropy.Till now, we have discussed the algorithms for categorical target variable. Reduction in variance is an algorithm used forcontinuoustarget variables (regression problems). This algorithm uses the standard formulaof variance to choose the bestsplit. The split with lower variance is selected as thecriteria to split the population:Above X-bar is mean of the values, X is actual and n is number of values.Steps to calculate Variance:Example:- Lets assign numerical value 1 for play cricket and 0 for not playing cricket. Now follow the steps to identify the right split:Above, you can see that Gender split has lower variance compare to parent node, so the split would take place on Gender variable.Until here, we learnt about the basics of decision trees and the decision making process involved to choose the best splits in building a tree model. As I said, decision tree can be applied both on regression and classification problems. Lets understand these aspects in detail.Overfitting is one of the key challenges faced while modeling decision trees. Ifthere is no limit set of a decision tree, it will give you 100% accuracy on training set becausein the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting ispivotal while modeling a decision tree and it can be done in 2 ways:Lets discuss both of these briefly.This can be done by using various parameters which are used to define a tree.First, lets look at the general structure of a decision tree:The parameters used for defining a tree are further explained below. The parameters described below are irrespective of tool. It isimportant to understandthe roleof parameters used in tree modeling. These parameters are available in R & Python.As discussed earlier, the technique of setting constraint is agreedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Lets consider the followingcase when youre driving:There are 2 lanes:At this instant, you are the yellow car and you have 2 choices:Lets analyze these choice. In the former choice, youll immediatelyovertake the car ahead and reachbehind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you!This is exactly the difference between normal decision tree & pruning. A decision tree with constraints wont see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice.So we know pruning is better. But howto implement it in decision tree? The idea is simple.Note that sklearns decision tree classifier does not currentlysupportpruning.Advanced packages like xgboost have adoptedtree pruning in their implementation.But the library rpart in R, provides a function to prune. Good for R users!If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees? Many of us have this question. And, this is a valid one too.Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Lets look at some key factors which will help you to decide which algorithm to use:For R users and Python users, decision tree is quite easy to implement. Lets quickly look at the set of codes that can get you started with this algorithm. For ease of use, Ive shared standard codes where youll need to replace your data set name and variables to get started.In fact, you can build the decision tree in Python right here! Heres a live coding window for you to play around the code and generate results:For R users, there are multiple packages available to implement decision tree such as ctree, rpart, tree etc.In the code above:For Python users, below is the code:The literary meaning of word ensemble is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree basedmodels.Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, how much on an average are the predicted values different from the actual value. Variance means, how different will the predictions of the model be at the same point if different samples aretaken from the same population.You build a small tree and you will get a model with low variance and high bias. How do you manage to balance the trade off between bias and variance ?Normally, as you increase the complexity of your model, you will see a reduction in prediction error due to lower bias in the model. As you continue to make your model more complex, you end up over-fitting your model and your model will start suffering from high variance.A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors.Ensemble learning is one way to execute this trade off analysis.Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking. In this tutorial, well focus on Bagging and Boosting in detail.Baggingis a technique used to reduce the variance of our predictionsby combiningthe resultof multipleclassifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:
The steps followed in bagging are:Note that, herethe number of models built is not a hyper-parameters.Higher number of models are always better or may give similarperformance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.There are various implementations of bagging models. Random forest is one of them and well discuss it next.Random Forest is considered to be a panaceaof all data science problems. On a funny note, when you cant think of any algorithm (irrespective of situation), use random forest!Random Forestis a versatile machine learning method capable of performing both regression and classification tasks.It also undertakesdimensional reduction methods, treats missing values, outlier valuesand other essential steps of data exploration,and does a fairly good job.It isa type of ensemble learning method, where a group of weak models combineto form a powerful model.In Random Forest, we growmultipletrees as opposedto a single tree in CART model (see comparison between CART and Random Forest here, part1 and part2).To classify a new object based on attributes, each tree gives a classification and we say the tree votes for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.It works in the following manner.Each tree is planted & grown as follows:Tounderstand more in detail about this algorithm using a case study, please read thisarticle Introduction to Random forest  Simplified.Random forests have commonly known implementations in R packages and Python scikit-learn. Lets look at the codeof loading random forest model in R and Python below:RCodeDefinition: The term Boosting refers to a family of algorithms whichconverts weak learner to strong learners.Lets understand this definition in detail by solving a problem of spam email identification:How would you classifyan email as SPAM or not? Like everyone else, our initial approach would beto identify spam and not spam emails using following criteria. If:Above, weve defined multiple rules to classifyan email into spam or not spam.But, do you think these rules individually are strong enough to successfully classifyan email? No.Individually, these rules arenot powerful enough to classify an email into spam or not spam.Therefore, these rules are called as weak learner.To convert weak learner to strong learner, well combine the prediction of each weak learner using methods like:For example: Above,we have defined 5 weak learners. Out of these 5, 3 arevoted asSPAM and 2 are voted as Not a SPAM. In this case, by default, well consider an email as SPAM because wehave higher(3) vote for SPAM.Now we know that, boosting combines weak learner a.k.a. base learner to form a strong rule. An immediate question which should pop in your mind is, How boosting identify weak rules?To find weak rule, we applybase learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.Heres another question which might haunt you, How do we choose different distribution for each round?For choosing the right distribution, here are the following steps:Step 1: Thebase learner takes all the distributions and assign equal weight or attention to each observation.Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, weapply the next base learning algorithm.Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.Finally, it combines the outputs fromweak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting payshigherfocus on examples which are mis-classied or have higher errors by preceding weak rules.
There are many boosting algorithms which impart additional boost to models accuracy. In this tutorial, well learn about the two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost.Ive always admired the boosting capabilities that xgboost algorithm. At times, Ive found that it providesbetter result compared to GBM implementation, but at times you might find that the gains are just marginal. When I explored more about its performance and science behind its high accuracy, I discovered many advantages of Xgboost over GBM:Before we start working, lets quickly understand the important parameters and the working of thisalgorithm. This will be helpful for both R and Python users. Below is the overall pseudo-code of GBM algorithm for 2 classes:This is an extremely simplified (probably naive) explanation of GBMs working. But, it will help every beginners to understand this algorithm.Lets considerthe important GBMparameters used to improve model performance in Python:Apart from these, there are certain miscellaneous parameters which affect overall functionality:I know its a long list of parameters but I have simplified it for you inan excel file which you can download from thisGitHub repository.For R users, using caret package, there are 3 main tuning parameters:Ive shared the standard codes in R and Python. At your end, youll be required to change the value of dependent variable and data set name used in the codes below. Considering the ease of implementing GBM in R, one can easily perform tasks like cross validation and grid search with this package.XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Its feature to implement parallel computing makes itat least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.R Tutorial: For R users, this is a complete tutorial on XGboost which explains the parameters along with codes in R. Check Tutorial.Python Tutorial: For Python users, this is a comprehensive tutorial on XGBoost, good to get you started. Check Tutorial.Practice is the one and true method of mastering any concept. Hence, you need to start practicingif you wish to master these algorithms.Till here, youve got gained significant knowledge on tree based models along with these practical implementation. Its time that you start working on them. Here are open practice problems where you can participate and check your live rankings on leaderboard:Tree based algorithm are important for every data scientist to learn. In fact, tree models are known to provide the best model performance in the family of whole machine learning algorithms. In this tutorial, we learnt until GBM and XGBoost. And with this, we come to the end of this tutorial.We discussed about tree based modeling from scratch. We learnt the important of decision tree and how that simplistic concept is being used in boosting algorithms. For better understanding, I would suggest you to continue practicing these algorithms practically. Also, do keep note of the parameters associated with boosting algorithms. Im hoping that this tutorial would enrich you with complete knowledge on tree based modeling.Did you find this tutorial useful ? If you have experienced, whats the best trick youve used while using tree based models ? Feel free to share your tricks, suggestions and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
"Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|A Complete Tutorial on Tree Based Modeling from Scratch (in R & Python)|Operational Analytics Case study For Freshers: Call Center optimization|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Hadoop DeveloperLocation Delhi NCR/ BangaloreAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asHadoop Developer  Delhi NCR/Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/04/hadoop-developer-delhi-ncrbangalore-2-years-experience/
Operational Analytics Case study For Freshers: Call Center optimization,Learn everything about Analytics|Introduction|Case Study : Call Center Optimization|Type of channels used in customer service|Customers : What might influence the time a customer takes on Call?|Callers : What might take a caller higher time than others?|How do start modeling ?|Sample Problem Statement|End Notes,"You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Senior Hadoop Developer  Delhi NCR/Bangalore (6  8 years of experience)|Gartner Business Intelligence, Analytics and Information Management Summit 2016, 7-8 June, Mumbai, India|
Tavish Srivastava
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ive seen freshers strugglingto solvecase studies during interview. Do you also find it difficult? Yes?Thats okay. But, since you now have an abundance of resources to enhance your skills, you have no reason to not become a master in it. I myself had struggled a lot during initial days, but with practice I was able to overcome this difficulty. You also CAN!There is noone shot winning formula in case studies. Let me tell you what helped me to solve case studiesduring interviews:Look, my process of solving is simple. If you manage to follow even this much, youwould be just fine. You just need practice. And, practice with different types of case studies.And if youre looking for more case studies like this, check out the comprehensive Ace Data Science Interviews course! We have curated it with tons of videos, hundreds of questions and plenty of resources, just for you.This article is about a case study on Call Center Optimization. Im sure this is a new case study type for you. Hence, do it thoroughly and focus on the underlying aspects of how call center works!What is the biggest turn-off when you call the customer service center for seeking support on any issue?For me, it is the waiting time. The line : Your call is important to us really does not do any consolation to the customer who is waiting on line.Lets say, I am looking at comparable Internet Service provider : A, B and C. What will be my main considerations to choose one of them? First, will be the internet speed and second will be the customer support. Customer support is supremelyimportant for any company whether it is a Telecom company, Internet Service Provider, Bank, Insurance company or an E-Commerce Firm. It is the assurance from the company that anything going bad will be resolved as soon as possible.So, we all understand that Customer Service Center is probably the second most important consideration just after the actual product. Also, customer service is one of the biggest contributors to the cost component for any firm. So, within the same budget can we make the customer service better by using analytics. Lets try looking at it with a case study. Note that all the numbers are simulated and are used to bring out a concept and does not come from a real case.All companies provide multiple channels through which customers can reach out / connect with them. Here are a few of these channels :Other channels might include brick and mortar branches / outlets, 1-on-1 customer relationship managers etc. The most important of all is the call center (calling process) which every company needs to maintain but at minimum cost possible.So, how can we optimize the expense for a call center. To optimize this problem, you first need to understand that here we are dealing with two entities : Customers and the caller. And to optimize combination of customer-caller pair, you need to understand how are customer different from each other and how are callers different from each other.Because customers might not call the call centersfrequently, more important attributes will be his /her demographic and relationship with the company. Lets consider a bank. So, for a bank, these attributes can be :Beyond these, there can be past call data which might be useful :Even though the time taken by callers might not be very different from each other, some level of segmentation can definitely be found. Here are a few variables which can influence the amount of time caller takes :What is the objective function when we are trying to optimize the call center efficiency? Here are a bunch of objective function I can think of:So, how do we solve for two objective function? In general, to make it simpler, we take one of the objective function as a constraint and other as the main objective function. We will try to create a formulation which can be solved using an assignment problem. For simplicity we will not take customer satisfaction in this analysis.You have 7 types of callers and 7 types of customers. Assume that all seven customers call at the same time, how do you assign callers to each of these customers so that the total time of the call center is least.All numbers shown here represent time in minutes. Imagine this problem getting to an extent where1000s of callers respond and 100000s of customers call. So, we probably need a more scientific way to do this problem.Lets first see, what is the time if 1st caller gets assigned to 1st customer and 2nd to 2nd and so on. The total time becomes 23 + 84 + 91 + 82 + 67 + 63 + 6 = 416 which is 59.4 minutes/customer. Now, lets try to optimize this problem using something called assignment problem solution using Hungarian method. The steps are :Doing the calculation, the total time now comes out as 165 which is just 23 minutes/ customer. This is a far better assignment than the random allotment.A real life call center optimization is far more complex than this scenario. For a real case we will need to consider packing algorithms which can incorporate the fact that different calls come at different time and what is the best packaging. Other level of complexity can be brought in by considering customer satisfaction rate. We will try taking up these pieces in coming articles.Did you like reading this article ? Can you think of other checks to make this case study mimicking the actual call center in a better way? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/04/operational-analytics-case-study-freshers-call-center-optimization/
"Gartner Business Intelligence, Analytics and Information Management Summit 2016, 7-8 June, Mumbai, India",Learn everything about Analytics|Information & Analytics Leadership: Empowering People with Trusted Data,"Share this:|Like this:|Related Articles|Operational Analytics Case study For Freshers: Call Center optimization|Deep Learning for Computer Vision  Introduction to Convolution Neural Networks|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In our increasingly digital world, organizations that develop a robustly information-centric culture with the ability to recognize, manage and exploit their information assets will ultimately outpace the competition. To realize this you need to empower people to build the analytics they need to improve their line of business. Capitalize on these opportunities, whilst managing the risks, by ensuring you have the right team, with the right skills, the right roles and the right leader in every part of the business. This is more important than ever. Will you need a Chief Data Officer or a Chief Analytics Officer, if you dont have one already? Should you be the person most likely to take on this role?The Gartner Business Intelligence, Analytics & Information Management Summit 2016, between 7-8 June in Mumbai, is the place to discover the latest research and transformative insight designed to help you drive maximum business value from your own programs.From positioning Master Data Management for success to advancing leading-edge analytics youll find the strategies and best practices to maximize the impact and business value of your own programs at the Gartner BI, Analytics & IM Summit 2016. Join us this year to get a practical update as well as a strategic view across the breadth of todays IM and analytics priorities. Discover new governance practices you can use to ensure order and consensus without inhibiting innovation. Identify next steps to prepare for the Internet of Things (IoT) and understand the latest fast-moving trends such as advanced Multi-domain MDM, analytic marketplaces, data lakes, smart machines and Hadoop. Harness the full value of your information assets. Seize the analytic opportunities.View Conference BrochureView Agenda at a GlanceView Top recommended sessionsView Analyst GuideView Analyst Q&A with Bhavish SoodView Smarter with Gartner",https://www.analyticsvidhya.com/blog/2016/04/gartner-business-intelligence/
Deep Learning for Computer Vision  Introduction to Convolution Neural Networks,Learn everything about Analytics|Introduction|Table of Contents|1. Challenges in Computer Vision (CV)|2. Overview of Traditional Approaches|3. Review ofNeural Networks Fundamentals|4. Introduction to Convolution Neural Networks|5. Case Study: AlexNet|6. Implementing CNNsusing GraphLab|Projects||End Notes,"Activation Functions|Data Preprocessing|Weight Initialization|Convolution Layer|Pooling Layer|Fully Connected Layer|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Gartner Business Intelligence, Analytics and Information Management Summit 2016, 7-8 June, Mumbai, India|New Case Study for Analytics Interviews: Dawn of Taxi Aggregators|
Aarshay Jain
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The power of artificial intelligence is beyond our imagination. We all know robots have already reached a testing phase in some of the powerful countries of the world. Governments, large companies are spending billions in developing this ultra-intelligence creature.The recent existence of robots have gained attention of many research houses across the world.Does it excite you as well ? Personally for me, learning about robots & developments in AI started witha deep curiosity and excitement in me! Lets learn about computer vision today.The earliest research in computer vision started way back in 1950s. Since then, we havecomea long way but still find ourselves far from the ultimate objective. But with neural networks and deep learning, we have become empowered like never before.Applications of deep learning in vision have taken this technology to a different level and madesophisticated thingslike self-driven cars possible in near future.In this article, I will also introduce you to Convolution Neural Networks which form the crux of deep learning applications in computer vision.Note: This article is inspired byStanfords Class on Visual Recognition. Understanding this article requires prior knowledge of Neural Networks. If you are new to neural networks, you canstart here. Another useful resource on basics of deep learning can be foundhere.As the name suggests, the aim ofcomputer vision (CV) is to imitate the functionality of human eye and brain components responsible for your sense of sight.Doing actionssuch as recognizing an animal, describing a view, differentiating among visible objects are really a cake-walk for humans. Youd be surprised to know thatit took decades of research to discover andimpart the ability ofdetecting an object toa computer with reasonable accuracy.The field of computer vision has witnessed continualadvancements in the past 5 years. One of the most stated advancement is Convolution Neural Networks (CNNs).Today, deep CNNs form the crux of mostsophisticated fancy computer vision application, such as self-driving cars, auto-tagging of friends in our facebook pictures, facial security features, gesture recognition, automatic number plate recognition, etc.Lets get familiar with it a bit more:Object detection is considered to be the most basic application of computer vision. Rest of the other developments in computer vision are achieved by making small enhancements on top of this. In real life, every time we(humans) open our eyes, we unconsciously detect objects.Since it is super-intuitive for us, we fail to appreciate the key challenges involved when we try to design systems similar to our eye. Lets start by looking at some of the key roadblocks:These are just some of the challenges which I brought up so that you can appreciate the complexity of the tasks which your eye and brain duo does with such utter ease. Breaking up all these challenges and solving individually is still possible today in computer vision. But were still decades away from a system which can get anywhere close to our human eye (which can do everything!).This brilliance of ourhuman body is the reason why researchers have been trying to break the enigma of computer vision by analyzing the visual mechanics of humans or other animals. Some of the earliest work in this direction was done by Hubel and Weisel with their famous cat experiment in 1959. Read more about it here.This was the first study which emphasized the importance of edge detection for solving the computer vision problem. They were rewarded thenobel prize for their work.Before diving into convolutional neural networks, lets take a quick overview of the traditional or rather elementarytechniques used in computer visionbefore deep learning became popular.Various techniques, other than deep learning are available enhancing computer vision. Though, they work well for simpler problems, but as the data become huge and the task becomes complex, they are no substitute for deep CNNs. Lets briefly discuss two simple approaches.I hope this gives some intuition into the challenges faced by approaches other than deep learning. Please note that more sophisticated techniques can be used than the ones discussed above but they would rarely beat a deep learning model.Lets discuss some properties ofa neural networks. I will skip the basics of neural networks here as I have already covered that in my previous article Fundamentals of Deep Learning  Starting with Neural Networks.Once your fundamentals are sorted, lets learn in detail some important concepts such asactivation functions, data preprocessing, initializing weights and dropouts.There are various activation functions which can be used and this is an active area of research. Lets discuss some of the popular options:To summarize, ReLU ismostly the activation function of choice.If the caveats are kept in mind, these can be used very efficiently.For images, generally the following preprocessing steps are done:Note that normalization is generally not done in images.There can be various techniques for initializing weights. Lets consider a few of them:One more thing must be remembered while using ReLU as activation function. It is that the weights initialization might be such that some of the neurons might not get activated because of negative input. This is something that should be checked. You might be surprised to know that 10-20% of the ReLUs might be dead at a particular time while training and even in the end.These were just some of the concepts I discussed here. Some more concepts can be of importance like batch normalization, stochastic gradient descent, dropouts which I encourage you to read on your own.Before going into the details, lets first try to get some intuition into why deep networks work better.As we learnedfrom the drawbacks of earlier approaches, they are unable to cater to the vast amount of variations in images. DeepCNNs work by consecutively modeling small pieces of information and combining them deeper in network.One way to understand them is that the first layer will try to detect edges and form templates for edge detection. Then subsequent layers will try to combine them into simpler shapes and eventually into templates of different object positions, illumination, scales, etc. The final layers will match an input image with all the templates and the final prediction is like a weighted sum of all of them. So, deep CNNs are able to model complex variations and behaviour giving highly accurate predictions.There is an interesting paper on visualization of deep features in CNNs which you can go through to get more intuition Understanding Neural Networks Through Deep Visualization.For the purpose of explaining CNNs and finally showing an example,I will be using the CIFAR-10 dataset for explanation here and you can download the data set from here. This dataset has 60,000 images with 10 labels and 6,000 images of each type. Each image is colored and 3232 in size.A CNN typically consists of 3 types of layers:You might find some batch normalization layers in some old CNNs but they are not used these days. Well consider these one by one.Since convolution layers form the crux of the network, Ill consider them first. Each layer can be visualized in the form of ablock or a cuboid. For instance in the case of CIFAR-10 data, the input layer would have the following form:Here you can see, this is the original image which is 3232 in height and width. The depth here is 3 which corresponds to the Red, Green and Blue colors, which form the basis of colored images. Now a convolution layer is formed by running a filter over it. A filter is another block or cuboid of smaller height and width but same depth which is swept over this base block. Lets consider a filter of size 5x5x3.We start this filter from the top left corner and sweep it till the bottom left corner. This filter is nothing but a set of eights, i.e. 5x5x3=75 + 1 bias = 76 weights. At each position, the weighted sum of the pixels is calculated as WTX + b and a new value is obtained. A single filter will result in a volume of size 28x28x1 as shown above.Note that multiple filters are generally run at each step. Therefore, if 10 filters are used, the output would look like:Here the filter weights are parameters which are learned during the back-propagation step. You might have noticed that we got a 2828 block as output when the input was 3232. Why so? Lets look at a simpler case.Suppose the initial image had size 6x6xd and the filter has size 3x3xd. Here Ive kept the depth as d because it can be anything and its immaterial as it remains the same in both.Since depth is same, we can have a look at the front view of how filter would work:Here we can see that the result would be 4x4x1 volume block. Notice there is a single output for entire depth of the each location of filter. But you need not do this visualization all the time. Lets define a generic case where image has dimension NxNxd and filter has FxFxd. Also, lets define another term stride (S) here which is the number ofcells (in above matrix) to move in each step. In the above case, we had a stride of 1 but it can be a higher value as well. So the size of the output will be:output size = (N  F)/S + 1You can validate the first case where N=32, F=5,S=1. The output had 28 pixels which is what we get from this formula as well. Please note that some S values might result in non-integer result and we generally dont use such values.Lets consider an example to consolidate our understanding. Starting with the same image as before of size 3232, we need to apply 2 filters consecutively, first 10 filters of size 7, stride 1 and next 6 filters of size 5, stride 2. Before looking at the solution below, just think about 2 things:Here is the answer:Notice here that the size of the images is getting shrunk consecutively. This willbe undesirable in case of deep networks where the size wouldbecome very small too early. Also, it would restrict the use of large size filters as they would result in faster size reduction.To prevent this, we generally use a stride of 1 along withzero-padding of size (F-1)/2. Zero-padding is nothing but adding additional zero-value pixels towards the border of the image.Consider the example we saw above with 66 image and 33 filter. The required padding is (3-1)/2=1. We can visualize the padding as:Here you can see that the image now becomes 88 because of padding of 1 on each side. So now the output will be of size 66 same as the original image.Now lets summarize a convolution layer as following:Some additional points to be taken into consideration:Having understood the convolution layer, lets move on to pooling layer.When we use padding in convolution layer, the image size remains same. So, pooling layers are used to reduce the size of image. They work by sampling in each layer using filters. Consider the following 44 layer. So if we use a 22 filter with stride 2 and max-pooling, we get the following response:Here you can see that 4 22 matrix are combined into 1 and their maximum value is taken. Generally, max-pooling is used but other options like average pooling can be considered.At the end of convolution and pooling layers, networks generally use fully-connected layers in which each pixel is considered as a separate neuron just like a regular neural network. The last fully-connected layer will contain as many neurons as the number of classes to be predicted. For instance, in CIFAR-10 case, the last fully-connected layer will have 10 neurons.I recommend reading the prior section multiple times and getting a hang of the concepts before moving forward.In this section, I will discuss the AlexNet architecture in detail. To give you some background, AlexNet is the winning solution of IMAGENET Challenge 2012. This is one of the most reputed computer vision challenge and 2012 was the first time that a deep learning network was used for solving this problem.Also, this resulted in a significantly better result as compared to previous solutions. I will share the network architecture here and review all the concepts learned above.The detailed solution has been explained in this paper. I will explain the overall architecture of the network here. The AlexNet consists of a 11 layer CNN with the following architecture:Here you can see 11 layers between input and output. Lets discuss each one of them individually. Note that the output of each layer will be the input of next layer. So you should keep that in mind.I understand this is a complicated structure but once you understand thelayers, itll give you a much better understanding of the architecture. Note that youfill find a different representation of the structure if you look at the AlexNet paper. This is because at that GPUs were not very powerful and they used 2 GPUs for training the network. So the work processing was divided between the two.I highly encourage you to go through the other advanced solutions of ImageNet challenges after 2012 to get more ideas of how people design these networks. Some of interesting solutions are:This video gives a brief overview and comparison of these solutions towards the end.Having understood the theoretical concepts, lets move on to the fun part (practical) and make a basic CNN on the CIFAR-10 dataset which weve downloaded before.Ill be using GraphLab for the purpose of running algorithms. Instead of GraphLab, you are free to usealternatives tools such asTorch, Theano, Keras, Caffe, TensorFlow, etc. But GraphLab allows a quick and dirty implementation as it takes care of the weights initializations and network architectureon its own.Well work on the CIFAR-10 dataset which you can downloadfromhere. The first step is to load the data. This data is packed in a specific format which can be loaded using the following code:We can verify this data by looking at the head and shape of data as follow:Since well be using graphlab, the next step is to convert this into a graphlab SFrame and run neural network. Lets convert the data first:GraphLab has a functionality of automatically creating a neural network based on the data. Lets run that as a baseline model before going into an advanced model.Here it used a simple fully connected network with 2 hidden layers and 10 neurons each. Lets evaluate this model on test data.As you can see that we have a pretty low accuracy of ~15%. This is because it is a very fundamental network. Lets try to make a CNN now. But if wego about training a deep CNN from scratch, we will face the following challenges:To overcome these challenges, we can use pre-trained networks. These are nothing but networks like AlexNet which are pre-trained on many images and the weights for deep layers have been determined. The only challenge is to find a pre-trianed network which has been trained on images similar to the one we want to train. If the pre-trained network is not made on images of similar domain, then the features will not exactly make sense and classifier will not be of higher accuracy.Before proceeding further, we need to convert these images into the size used in ImageNet which were using for classification. The GraphLab model is based on 256256 size images. So we need to convert our images to that size. Lets do it using the following code:Here we can see that a new column of type graphlab image has been created but the images are in 3232 size. So we convert them to 256256 using following code:Now we can see that the image has been converted into the desired size. Next, we will load the ImageNet pre-trained model in graphlab and usethe features created in its last layer into a simple classifier and make predictions.Lets start by loading the pre-trained model.Now we have to use this model and extract features which will be passed into a classifier. Note that the following operations may take a lot of computing time. I use a Macbook Pro 15 and I had to leave it for whole night!Lets have a look at the data to make sure we have the features:Though, we have the features with us, notice here that lot of them are zeros. You can understand this as a result of smaller data set. ImageNet was created on 1.2Mn images. So there would be many features in those images that dont make sense for this data, thus resulting in zero outcome.Now lets create a classifier using graphlab. The advantage with classifierfunction is that it will automatically create various classifiers and chose the best model.The various outputs are:The final model selection is based on a validation set with 5% of the data. The results are:So we can see thatBoosted Trees Classifier has been chosen as the final model. Lets look at the results on test data:So we can see that the test accuracy is now ~50%. Its a decent jump from 15% to 50% but there is still huge potential to do better. The idea here was to get you started and I will skip the next steps. Here are some things which you can try:You can find many open-sourcesolutions for this dataset which give >95% accuracy. You should check those out. Please feel free to try them and post your solutions in comments below.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:In this article, we covered the basics of computer vision using deep Convolution Neural Networks (CNNs). We started by appreciating the challenges involved in designing artificial systems which mimic the eye. Then, we looked at some of the traditional techniques, prior to deep learning, andgot some intuition into their drawbacks.We moved on to understanding the some aspects of tuning aneural networks such as activation functions, weights initializationand data-preprocessing. Next, we got some intuition into why deep CNNs should work better than traditional approaches and we understood thedifferent elements present in a general deep CNN.Subsequently, we consolidated our understanding by analyzing the architecture of AlexNet, the winning solution of ImageNet 2012 challenge. Finally, we took the CIFAR-10 data and implemented a CNN on it using a pre-trained AlexNet deep network.I hope you liked this article. Did you find this article useful ? Please feel free to share your feedback throughcomments below.And to gain expertise in working in neural network try out the deep learning practice problem Identify the Digits.",https://www.analyticsvidhya.com/blog/2016/04/deep-learning-computer-vision-introduction-convolution-neural-networks/
New Case Study for Analytics Interviews: Dawn of Taxi Aggregators,Learn everything about Analytics|Introduction|Why is structured thinkingkey to success in anycase study?|Resources to improve your Structured Thinking|Basic parameters of Taxi Aggregators|How to approach this case study?|Answers|End Notes,"Aggregator 1|Aggregator 2|Aggregator 3|Questions|Assumptions|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Related Articles|Deep Learning for Computer Vision  Introduction to Convolution Neural Networks|Sr Eng Manager or Director of Engineering  Mumbai (10+ years of experience)|
Tavish Srivastava
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Cab aggregator is a new business concept in India. Today we have Ola, Uber, Taxi for Sure etc. which not only compete to provide better service to customers but also design more consumer friendly price structures. Gone are the days, when your trip costonly depended on the number of kilometers. Today it depends on a number of factors making it confusing enough to puzzle customers and the drivers. Here are a few charges you might see on your hired taxis invoice :You see ? Undoubtedly, travelling in India has become a lot convenient. But, the companies have found subtle ways to get compensated well. Now, imagine comparing these taxi services with all these components together. Can we make this puzzle easier for Bengaluru, India population ? Lets crack this puzzle for once and for all.In this article, well solve a case study of taxi aggregators. Along side, Ill also focus on the essentials required for solving a case like a pro. Specially, consultant firms like Bain, BCG, McKinsey prefer candidates who think like a pro in solving any case study. Lets make you one.Disclaimer:This case is a hypothetical scenario associated with the popular brand names. The intention is to letpeople relate the importance of this case study in real life.We do not intend to promote / besmirchany brand.You should also check out the Ace Data Science Interviews course if youre struggling to land your first data science job! The course has been curated by experts who has conducted hundreds of interviews and includes multiple resources including the most comprehensive interview guide!Always remember, playing with structured thinking in any case study will putyouat the front foot (dominant
position). The technique of structured thinking is beneficial in many ways such as:Now, you have the weapon. The questions are below.Lets get ready!If youve trembled at the thought of structured thinking, dont worry! There are numerous ways using which you can build and improve this skill. Its easy to build but difficult to master. With time and practice, that can also be achieved. I was terrible at solving case studies until few years back. But, I patiently worked towards itand became better with time.Here are few useful resources to build structured thinking:In this case study, the taxi aggregator companies are selected from Indian markets. The currency used to denote travel fare isINR (Indian National Rupee). For people outside India, if you face any difficulty in understanding the severity of the travel fare, simply divide the fare by your currencys conversion rate to get travel fare in your currency.There arefour categories of taxis:Here are the taxiservice providers you need to consider :Ola: They have three category for Taxis: 1. Micro , 2. Mini , 3. PrimeIn addition to this, minimum fare of Micro is fixed at Rs. 50.Taxi For Sure: They also have 3 categories of Taxis as shown:Before you rush to answer section, think!Try to find out your way of solving this case. This phase is known as approaching the problem. It requires complete understanding of the problem. Considering the fact that this case has too many numbers, it will be a good practice to check their trend. This will help you to analyze taxis fare much faster.Using structured thinking, break the problem into 3 sections (one for each taxi class). Now, look for trends within each taxi type. Always keep the questions asked in mind. This gives you an aim to target. Once you start digging for answers, it becomes much easier to know where to stop!Which of the MICRO vehicles will be cheapest if your distance lies between 1 to 8 kms?1 . Only Ola and Taxi for sure have the option of taxi type Micro. So, if we compare the two in the given distance range, TFS (Taxi For Sure) starts lower and crosses Ola at 4 km. Beyond 4 km Ola comes out to be cheaper. The answer will be cannot be determined in the distance range.Which MINI vehicles arethe cheapest if your distance is between 1 to 10 kms?2. All the three taxi aggregators have option for Mini taxi. We clearly see that Uber is the cheapest car for the distance range and also seem to be the cheapest option for distances beyond 10 km looking at the trend.If you get a free upgrade from Ola Micro to Mini, will it be cheaper than Uber Mini for distance 2-6 kms?3. Free upgrade is a common scenario in Ola from Micro to Mini. So, this is a very relevant question to ask. The trend looks to be very interesting. Uber Go starts at a slightly lower rate than Ola Micro but Ola takes over after 5 km distance. So, again our answer will be cannot be determined in the distance range. However, it is interesting to notice that a Ola Microtaxi comes out to be more expensive than Uber Mini car for shorter than 5 km distance.Uber is charging a muliplier of 2.1 and Ola is charging a multiplier of 1.4 on their Sedan Vehicles (Ola Prime vs. UBER X). Which one will cost less ?4. Multipliers are often added in peak traffic hours. It becomes very difficult to compare rates in such cases. Heres one of those scenarios. Ola is generally more expensive compared to Uber, but in such extreme multiplier cases, we see that Ola comes out to be cheaper option throughout.You have already booked UberGo for a multiplier of 1.5 and now you are getting a Ola Mini vehicle without peak charges?The challenge is that if you cancel an Uber you will incur a cancellation charge penalty. However, if you choose to cancel, you stand a chance to save on peak charges for Uber. At what distance will you break even on the cancellation charges on Uber, in case you choose to go ahead with Ola?5. What you need to keep in mind is the cancellation charges of UBER. Here is the table for OLA vs. UBER :As can be seen from both table and the graph, the break even only happens between 13-14 kms. Hence, you should make a switch only if you want to travel more than 13 kms.This was a fun exercise and hopefully useful for Indian population, who are confused with which cab service to take. You can expect such realistic case study in your interviews which might break many myths you had about the subject. For instance, before doing this analysis, I always assumed OLA Micro is the cheapest available option for all distances. However, looks like UBER Mini can be cheaper than OLA micro for smaller distance.Did you like reading this article ? Have you encountered any such case studies in your interview? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/04/case-study-analytics-interviews-dawn-taxi-aggregators/
"Sr Eng Manager or Director of Engineering  Mumbai (10+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|New Case Study for Analytics Interviews: Dawn of Taxi Aggregators|Data Scientist (Product)  Mumbai (1+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Sr Eng Manager or Director of EngineeringLocation  MumbaiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asSr Eng Manager or Director of Engineering  Mumbai and the following details:,https://www.analyticsvidhya.com/blog/2016/03/sr-eng-manager-director-engineering-mumbai-10-years-experience/
"Data Scientist (Product)  Mumbai (1+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr Eng Manager or Director of Engineering  Mumbai (10+ years of experience)|News: Praxis Business School launches PGP Business Analytics Program in Bangalore|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Scientist (Product)Location  MumbaiAbout employer  ConfidentialDescriptionThe Data Scientist (Product) will be responsible for developing and maintaining healthy product analytics environment, predictive systems, creating efficient algorithms and improving data quality. This individual will work closely with the Head of Analytics to identify, evaluate, design and implement statistical analyses of gathered open source, proprietary, and customer data to create analytic metrics and tools suitable for use in Digital Sandbox applications. This individual will have the opportunity to contribute directly to the features and capabilities deployed in our applications.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist (Product)  Mumbai and the following details:",https://www.analyticsvidhya.com/blog/2016/03/data-scientist-product-mumbai-1-years-experience/
News: Praxis Business School launches PGP Business Analytics Program in Bangalore,Learn everything about Analytics,"Introduction|KJ: How do you look at the 5 years of journey of teaching analytics in India?|KJ: Take us through the thought behind the Bangalore launch.|KJ: The three critical strengths of the Praxis program have been its curriculum, the faculty and the placement program. Do you see a risk of dilution in these attributes with the Bangalore launch?|KJ: Is there any change in the kind of applicants you are targeting in the two geographies? Can you briefly explain the selection criteria?|KJ: Who should apply to the Praxis programs  and why?|KJ: There are several players in the hybrid and on-line space  which gives a much wider reach. You continue to stick to the full-time, high-touch program. Any specific reason?|KJ: Praxis is one of the few programs, which takes ownership of the placements of the students. What is the thought process behind it and how has this changed over years?|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Scientist (Product)  Mumbai (1+ years of experience)|13 Machine Learning & Data Science Startups from Y Combinator Winter 2016|
Kunal Jain
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","To know more about the program, you canvisit here.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Praxis Business School launched its one year full time Business Program in 2011. The Praxis brand was new to this domain. When I heard about people mentioning Praxis on our blog initially, I thought it was one of the many programs which were started by Management Schools in the country. I dismissed it without opening their course page.A few months after this incident, a couple of our community members had great things to say about the program. I usually consider this as one of the strongest early indicator about the quality of program and long term success. So, this time I talked to these community members for a long time over phone and also spent time understanding about their offering. I also went through the course details and looked at the profile of faculty.There were quite a few things this program was doing differently:All of this was being offered at a price point comparable to part time programs in the country. The only concern I could think was that the program was based only in Kolkata. While Kolkata has been a city of scholars, I personally thought a program like this would benefit a lot if there was a larger industry ecosystem in the city.Irrespective, I reached out to Praxis and asked them to visit the campus and interact with the students in the program. They happily obliged! We reviewed the program and included it in our rankings after that. Over one of the discussions with the team at Praxis, I mentioned that while Praxis was not suffering because of its Kolkata location, a program like this should also have a center in Bangalore.A few years later (i.e. last week), I got a call from the Praxis team informing me that they are now launching the same program in Bangalore. I congratulated the team and had a long discussion with Prof. Charanpreet Singh about the vision and addition of Bangalore as a program location. Here are a few excerpts from the discussion:Prof. Charanpreet SinghIt has been a wonderful ride. We started the one-year full-time program in 2011 on the basis of suggestions made by our industry associates. As we did our own research, three things became abundantly clear:We started with a batch of just 8 students in 2011  and today we are an established brand in analytics in the country with perhaps the only 360 degree solution for the student  concept strengthening, experiential learning with labs and projects, and placement into the corporate world of analytics.Our alumni have performed with distinction and we get repeat recruiters at our campus  two pieces of unambiguous evidence that our efforts are bearing fruit. And yes, in these five years the world of analytics has evolved with speed  and so has our curriculum!As the awareness of analytics and data science gains momentum, the demand for good quality programs is bound to accelerate. Since we have chosen the high touch, in-class, full-time option, one way for us to grow and meet this increasing demand is to make the same program available in more geographies.We started in Kolkata and focussed on getting our course content, faculty team and industry engagement right. We are now confident of a phased expansion, and Bangalore is quite obviously the best candidate for the first phase. It is the analytics capital of the country  with the highest number of people working with Tech companies and the largest number of potential recruiters.As someone warned us a few months back  if you are in analytics you just have to be in Bangalore. We are excited about engaging with students in Bangalore and believe that this move will help enhance the Praxis position of being a top analytics institution.We gave a lot of serious thought to this and came to the conclusion that this extension will actually enhance our strengths. We are preserving the core of the curriculum  the subjects will remain the same across Kolkata and Bangalore, but each faculty member will, of course, add his/ her own flavour.We have engaged with some very accomplished people in Bangalore with a mix of academic and industry experience to deliver the course.This will strengthen our faculty team and create a rich resource-pool that we can draw on for research as well as teaching. Finally, a larger group of trained candidates amplifies our value proposition to the recruiters, as the demand for data scientists keeps galloping.The selection process and criteria remain same irrespective of the geography.We have two simple criteria for establishing suitability for this program  the extent of commitment to the domain of analytics, and the degree of problem solving passion and ability. Our program in Kolkata has had the full spectrum of students  from freshly graduated engineers and eco/ stats majors to professionals with 10 years of experience in IT and ITES.The median would be someone with about 3-4 years of experience who is seeking a transition to a career in analytics. We have discovered that there are ready recruiters for the right talent and skill-set at every level.Let me sketch a picture of a Praxis aspirant  serious about a career in Analytics  willing to deep-dive and devote a year to learning  loves numbers and loves solving problems  is not daunted by technology or complexity.If one is seeking a comprehensive, rigorous and cutting-edge curriculum supported by a strong placement program that creates compelling career opportunities, Praxis is a great option. I would also recommend that aspirants research the quality of the programs they are applying to, and engage with the institute alumni before they decide.Agree on-line does give you reach. However, I look at two distinct types of audiences  one, those who are looking at analytics as an add-on to their existing set of skills with an objective of strengthening their capability in their current assignments; and two, those who are looking to transition from their existing careers into analytics.Our programs address the specific requirements of the latter  analytics is a complex domain and a budding data scientist needs to devote his/ her full energy and time to the understanding of concepts and their applications.A part-time program, in my opinion, may land the candidate a job, but may not be adequate for the person to perform with distinction. Even in a full-time program students sometimes struggle to grapple with advanced statistics and econometrics, data mining algorithms, tools like R, SAS, Python, new technologies like Hadoop and its eco-system  all packed into a tight 500 hours capsule.Moving forward, we do have plans to address the needs of the on-line audience as well.We are clear that our target students are seeking a transition into a career in analytics  thats the reason they join our program. So its our responsibility to make sure of both  that they get the required learning that prepares them for this career and that they get institutional support in their quest for the all-important (and most tricky) first job in this area.And I guess the two go hand-in-hand  as we understand the corporate expectations better with every interaction, we make appropriate improvements in our academic content and delivery.Thus, by taking up the responsibility of placements with a lot of seriousness, we ensure that the quality of the program is continually enhanced. It also ensures that we keep adding new recruiters every year, in addition to retaining the present ones.This approach introduces a lot of clarity in our selection process as well  if we feel certain candidates will encounter seemingly insurmountable barriers in entering this domain for any reason we make that abundantly clear to them and generally do not admit them into the program (unless they insist  and a few actually do  that they are happy to just learn and work something out for themselves).From an outcome perspective, our curriculum has evolved substantially in the last 5 years and continues to be acknowledged as the most comprehensive and current program; and we have found the first analytics job for over 90% of our students  consistently, batch after batch.KJ: Thanks Prof. Singh for your time. As usual, I enjoyed talking to you. I wish you all the success for this program and hope that you continue to build the momentum and brand Praxis in future.Did you like reading this article ? Are you planning join business analytics program? Do share your experience /suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/praxis-business-school-launches-pgp-business-analytics-program-bangalore/
13 Machine Learning & Data Science Startups from Y Combinator Winter 2016,Learn everything about Analytics|Introduction|What will you learn and Whyshouldyou care?|Machine Learning / Data Science Startups|End Notes,"Netomi.com (Formerly Msg.ai)|Protonet|Interstate|Elucify|GitPrime|Nova|Enflux|Hykso|Skymind|Wakie|Zenysis Technologies|PaveIQ
|DeepGram|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|News: Praxis Business School launches PGP Business Analytics Program in Bangalore|Practical Guide to deal with Imbalanced Classification Problems in R|
Analytics Vidhya Content Team
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Entrepreneurs inspiration lies in a businessidea!If youve been planning to builda product, Id suggest you to check these startups first. May be, you can find a new angle to your product and make it more powerful usingmachine learning & predictive analytics.These startups got featured at Y Combinator Winter 2016. Y Combinator is a startup accelarator which invests ~ $120k in startups twice a year. Successfulcompanies like Reddit, Quora, Airbnb, Dropbox are known to emerge from Y Combinator.The essence of helping businesses lies in the heart of these startups. These people are smart at reading trends, harnessing technologyto making business easier and faster. If you were taking 2 days to finish a job, some of thestartupsmight help you finish it in hours time.Youd be amazed to see the kind of idea running through peoples brain around the world. Some of the startups listed below are simply incredible and looks to have a promising future. Youll see how people have designed their products using deep learning and artificial intelligence.Note: This is an inspirational post for people who see themselves as data science entrepreneurs in fewyears from now and aspireto work on a business idea.Many of us, worry about the future of machine learning & artificial intelligence. A popular forum Quora is filled with questions like How far will machine learning reach in 2020?. If you too have similar question pertaining to future of data science in mind, hereyoull find a good reason to trust.The acceptance of machine learning and predictive analytics has seen an overwhelming surge. Almost every industry (Im not sure about oil and metals), has invested millions of dollars in implementing data driven business methods.These startups will define the future of data science industry. Such startups will be the main source of increased jobs and business projects. And, these are just the cherry picked ones. There are many more which couldnt become a part of this list. Hence, be assured about the future of this industry.With this post youll learn about upcoming business ideas in machine learning and data science industry. Who knows, you might end up presenting your business idea at Y Combinator next year!Most of us love doing messaging. As a result, apps like Whatsapp, Facebook, Snapchat have gathered millions of users worldwide. Considering the rising popularity of messaging medium, Netomi.com (formerly Msg.ai) provides a messaging software which uses machine learning & deep learning to intelligently interact with customers. This software allows companies to establish their presence in all popular messaging platforms and accessing every app from this software. Also, this product is capable of performing sentimental analysis, trend analysis and provide detailed reports.This start up has all the reasons as to why shouldnt a company should not use public cloud servers. As a result, they have introduced a more secure way of sharing data using Private Cloud Servers. Protonet help companies to fight against data security using a project management and collaboration software on secure private cloud servers. The product is built to deliver high performance, faster data access and uploads. Its multitude of features include task management, file sharing, business communication, mobile collaboration and much more.The methods of marketing have become much more intelligent. Companies no longer spend recklessly but have understood the importance of marketing analytics. Interstate is a free marketing analytics and attribution platform. Itapplies predictive modeling to help companies make better budget allocation, target the right set of customers and help save money. In addition, it also provides dynamic marketing expenditure data, allows cross device tracking and integration with worlds popular apps.Generating lead data is time consuming. Majority of the time gets wasted in data wrangling. But, with AI powered solutions Elucify aims to speed up and strength the complete sales conversion process. Elucify helps companies to reap the most benefits out of lead data using artificial intelligence and machine learning. Their product allows companies to identify & clean old leads, extracts the targeted leads for fast conversion, dynamically search and update lead data and do all this in couple of hours. After all, speed matters!GitPrime get its name from the popular community GitHub. GitPrime aims to make software engineering less painful and more scale. It makes use of statistical analysis and identify patterns to let enginners improve the method of building software. It is known for providing personalized recommendations. Also, it allows the software engineering team to communicate, share, tracks team progress with data and much more. It endeavours to make software engineers more productive by providing data based timely feedback and analyzing weekly performance.Nova aims to enhance sales team effectiveness using a data science approach. This is accomplished using a messaging platform. This platform enables a representative to send customized emails according to the best suited reply. This software uses text mining, analyzes the sentiments and decides for a best suited email for a particular personnel. Using this software, company claims to have improve deliverability (less spam), 3X speed and increases shoots up representative response rate.People nowadays are more health-conscious than they used to be. They understand that good health is above wealth, and hence try to adopt every best possible thing or habit available. This startup provides machine washable clothes which is embedded with sensors to capture a persons complete workout motion. In simple words, this workout dress is enabled to track body movements and generate data. Once the data is captured, it delivers specific body insights.Who wondered even the clothes we wear will some day track data ?Can boxers use analytics ? Even if they couldnt, Hysko has made this possible with a large impact. This startup has come up with a product which tracks every move in terms of speed, intensity, count and delivers analytical insights. I find it similar to fitbit app but for boxers. It provides real time monitoring, reviews performance and helps a player to overcome hurdles quickly. Needless to say, it is fitted with sensors which helps to collect data and convert them to numbers.Growing volumes and varieties of available data is increasing the demand of Machine learning. Technology is growing fast and among them Skymind is an emerging but powerful player. Skymind aims to provide advanced analytical solution to businesses using scalable deep learning. Their software is well equipped with Hadoop, Spark, Neural nets and other important integrals. Looking at their current setup, their focus lies in Finance & marketing industry.Ever thought of asking a stranger to wake you up in the morning ? Wakie app has made this possible. Now, you can talk to friendly strangers about any topic, advice, suggestions without having exchanged your contact details. At a deeper level, they use machine learning to analyze the content with knowledge and skills of users. Finally, selects the best user to answer a question. Also, they usevoice recognition technique to evaluate the quality of answers provided.Putting data together to draw a conclusion is not an easy task. Zenysis Technologies hasbuild a data integration software which will compile and analyse all the data at one place. A data analysis software one can say!With this, they plan to help government and developing countries to improve their administration using their huge sets of generated data. They provide actionable insights which will help countries to make informed decision making and remove any ambiguity from decision making process.Not sure if you are getting the best return on investments? Are your marketing strategies giving you the requiredoutput? Startups likePave IQ have come to yourrescue. Pave IQ is a marketing analytics startup which help companies to make their google analytics insights more actionable. It analysesa companys goals and uses machine intelligence to extract the results of marketing through allchannels. It also provides customized data driven reports on how to increase ROI.How about extracting insights, sentiments from an audio ? Yes, this is now possible. DeepGram has created an AI enabled tool which builds AI models to automatically analyze and classify the audio/ video streams. It is known to locate the required keyword without needing to stop/pause the audio repeatedly. Also, it uses deep learning algorithms to extract speech to text insights relieving humans of manual process. It is highly useful in call centers, search from audio data sets and media centers.The startups listed above have a unique value to deliver. Even if the product exists, the industry size is huge that these people can bring a positive change to the society. Out of 120 startups, 13 startups are found to have built product empowered with machine learning and artificial intelligence. This suggests, people around the world are not only concerned about human efficiency but the pace of completing work!In this post, Ive listed the 13 machine learning & data science startups from Y Combinator Winter 2016 batch. You should check these out, mainly because of the idea. This should help you understand about the diverse applications and usage of these techniques. Dont think machine learning or analytics is limited to a particular industry. Instead, people have already started finding ways of using it across all industries.Did you like reading this article ? Have you been a part of any Y Combinator startup ? Do share your experience /suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/13-machine-learning-data-science-startups-combinator-winter-2016/
Practical Guide to deal with Imbalanced Classification Problems in R,Learn everything about Analytics|Introduction|What is Imbalanced Classification ?|Why do standard ML algorithms struggle with accuracyon imbalanced data?|What are the methods to deal with imbalanced data sets ?|Which performance metrics to use to evaluate accuracy ?|Imbalanced Classification in R|End Notes,"1. Undersampling|2. Oversampling|3. Synthetic Data Generation|4. Cost Sensitive Learning (CSL)|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|13 Machine Learning & Data Science Startups from Y Combinator Winter 2016|Exploring Recommendation System (with an implementation model in R)|
Analytics Vidhya Content Team
|34 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have several machine learning algorithms at our disposal for model building. Doing data based prediction is now easier like never before. Whether it is a regression or classification problem, one can effortlessly achieve a reasonably high accuracy usinga suitable algorithm. But, this is not the case everytime.Classification problems can sometimes get a bit tricky.ML algorithms tend to tremble when faced with imbalanced classification data sets. Moreover, they result in biased predictions and misleading accuracies. But, why does it happen ? What factors deteriorate their performance ?The answer is simple. Withimbalanced data sets, an algorithm doesnt get the necessary information about the minority class to make an accurateprediction. Hence, it is desirable to use ML algorithms with balanced data sets. Then, how should we deal with imbalanced data sets ? The methods are simple but tricky as described in this article.In this article, Ive shared the important thingsyou need to know to tackle imbalanced classification problems. In particular, Ive kept my focus on imbalance in binary classification problems. As usual, Ive kept the explanation simple and informative. Towards the end, Ive provided a practical view of dealing with such data sets in R withROSE package.Imbalanced classification is a supervised learning problem where one class outnumbers other class by a large proportion.This problem is faced morefrequently in binary classification problems than multi-level classification problems.The term imbalanced refer to the disparity encountered in the dependent (response) variable. Therefore, an imbalanced classification problem is one in which the dependent variable has imbalanced proportion of classes. In other words, a data set that exhibits an unequal distribution between its classes is considered to be imbalanced.For example: Consider a data set with 100,000 observations. This data set consist of candidates who applied for Internship in Harvard. Apparently, harvard is well-known for its extremely low acceptance rate. The dependent variable represents if a candidate has been shortlisted (1) or not shortlisted (0). After analyzing the data, it was found ~ 98% did not getshortlisted and only ~ 2% got lucky. This is a perfect case of imbalanced classification.In real life, does such situations arise more ? Yes! For better understanding, here are some real life examples. Please note that the degree of imbalance varies per situations:There are many more real life situations which result in imbalanced data set. Now you see, the chances of obtaining an imbalanced data is quite high. Hence, its important to learn to deal with such problems for every analyst.This is an interesting experiment to do. Try it! This way you willunderstand the importance of learning the ways to restructure imbalanced data. Ive shown this in the practical section below.Below are the reasons which leads to reduction in accuracy of ML algorithms on imbalanced data sets:The methods are widely known as Sampling Methods. Generally, these methods aim to modify an imbalanced data into balanced distribution using some mechanism. The modification occurs by altering the size of original data set and provide the same proportion of balance.These methods have acquired higher importance after many researches have proved that balanced data results in improved overall classificationperformancecompared to an imbalanced data set. Hence, its important to learn them.Below arethe methods used to treat imbalanced datasets:Lets understand them one by one.This method works with majority class. It reduces the number of observations from majority class to make the data set balanced. This method is best to use when the data set is huge and reducing the number of training samples helps to improve run time and storage troubles.Undersampling methods are of 2 types: Random and Informative.Random undersampling method randomly chooses observations from majority classwhich areeliminated until the data set gets balanced. Informative undersampling follows a pre-specified selection criterion to remove the observations from majority class.Within informative undersampling, EasyEnsemble and BalanceCascade algorithms are known to produce good results. These algorithms are easy to understand and straightforward too.EasyEnsemble: At first, it extracts several subsets of independent sample (with replacement)from majority class. Then, it develops multiple classifiers based on combination of each subset with minority class. As you see, it works just like a unsupervised learning algorithm.BalanceCascade: It takes a supervised learning approach where it develops an ensemble of classifier and systematically selects which majority class to ensemble.Do you see any problem with undersampling methods?Apparently, removing observations may cause the training data to lose important information pertaining to majority class.This method works with minority class. It replicates the observations from minority class to balance the data. It is also known as upsampling. Similar to undersampling, this method also can be divided into two types: Random Oversampling and Informative Oversampling.Random oversampling balances the data by randomly oversampling the minority class. Informative oversampling uses a pre-specified criterion and synthetically generates minority class observations.An advantage of using this method is that it leads to no information loss. The disadvantage of using this method is that, since oversampling simply adds replicated observations in original data set, it ends up adding multiple observations of several types, thus leading to overfitting. Although, the training accuracy of such data set will be high, but the accuracy on unseen data will be worse.In simple words, instead of replicating and adding the observations from the minority class, it overcome imbalances by generates artificialdata.It is also a type of oversampling technique.In regards to synthetic data generation, synthetic minority oversampling technique (SMOTE) is a powerful and widely used method. SMOTE algorithm creates artificial data based on feature space (rather than data space) similarities from minority samples. We can also say, it generates a random set of minority class observations to shift the classifier learning bias towards minority class.To generate artificial data, it uses bootstrapping and k-nearest neighbors.Precisely, it works this way:R has a very well defined package which incorporates this techniques. Well look at it in practical section below.It is another commonly used method to handle classification problems with imbalanced data. Its an interesting method. In simple words, this method evaluates the cost associated with misclassifying observations.It does not create balanced data distribution. Instead, it highlights the imbalanced learning problem by using cost matrices which describes the cost for misclassification in a particular scenario. Recent researches have shown that cost sensitive learning have many a times outperformed sampling methods. Therefore, this method provides likely alternative to sampling methods.Lets understand it using an interesting example: A data set of passengers in given. We are interested to knowif a person has bomb. The data set contains all the necessary information. A person carrying bomb is labeledas positive class. And, a person not carrying a bomb in labeledas negative class. The problem is to identify which class a person belongs to. Now, understand the cost matrix.There in no cost associated with identifying a person with bomb as positive and a person without negative. Right ? But, the cost associated with identifying a person with bomb as negative (False Negative) is much more dangerous than identifying a person without bomb as positive (False Positive).Cost Matrix is similarof confusion matrix. Its just, we are here more concerned about false positives and false negatives (shown below). There is no cost penalty associated with True Positive and True Negatives as they arecorrectly identified.Cost MatrixThe goal of this method is to choose a classifier with lowest total cost.Total Cost =C(FN)xFN + C(FP)xFPwhere,There are other advanced methods as well for balancing imbalanced data sets. These are Cluster based sampling, adaptive synthetic sampling, border line SMOTE, SMOTEboost, DataBoost  IM, kernel based methods and many more. The basic working on these algorithm is almost similar as explained above. There are more intuitive methods which you can try for improved predictions:Choosing a performance metric is a critical aspect of working with imbalanced data. Most classification algorithms calculate accuracy based on the percentage of observations correctly classified. With imbalanced data, the results are high deceiving since minority classes hold minimumeffect on overall accuracy.Confusion MatrixThe difference between confusion matrix and cost matrix is that, cost matrix provides information only about the misclassification cost, whereas confusion matrix describes the entire set of possibilities using TP, TN, FP, FN. In a cost matrix, the diagonal elements are zero. The most frequently used metrics are Accuracy & Error Rate.Accuracy: (TP + TN)/(TP+TN+FP+FN)Error Rate = 1 - Accuracy = (FP+FN)/(TP+TN+FP+FN)As mentioned above, these metrics may provide deceiving results and are highly sensitive to changes in data. Further, various metrics can be derived from confusion matrix. Theresultingmetrics providea bettermeasure to calculate accuracy while working on a imbalanced data set:Precision: It is a measure of correctness achieved in positive prediction i.e. of observations labeled as positive, how many are actually labeled positive.Precision = TP / (TP + FP)Recall: It is a measure of actual observations which are labeled (predicted) correctlyi.e. how many observations of positive class are labeled correctly. It is also known as Sensitivity.Recall = TP / (TP + FN)F measure: It combines precision and recall as a measure of effectiveness of classification in terms of ratio of weighted importance on either recall or precision as determined by coefficient.F measure = ((1 +) Recall  Precision) / ( Recall + Precision ) is usually taken as 1.Though, these methods are better than accuracy and error metric, but still ineffective in answering the important questions on classification. For example: precision does not tell us about negative prediction accuracy. Recall is more interesting in knowing actual positives. This suggest, we can still have a better metric to cater to our accuracy needs.Fortunately, we have a ROC (Receiver Operating Characteristics) curve to measure the accuracy of a classification prediction. Its the most widely used evaluation metric. ROC Curve is formed by plotting TP rate (Sensitivity) andFP rate (Specificity).Specificity = TN / (TN + FP)Any point on ROC graph, corresponds to the performance of a single classifier on a given distribution. It is useful because if provides a visual representation of benefits (TP) and costs (FP) of a classification data. The larger the area under ROC curve, higher will be the accuracy.There may be situations when ROC fails to deliver trustworthy performance. It has few shortcomings such as.As alternative methods, we can use other visual representation metrics include PRcurve, cost curves as well. Specifically, cost curves are known to possess the ability to describea classifiers performance over varying misclassification costs and class distributions in a visual format. In more than 90% instances, ROC curve is known to perform quite well.Till here, weve learnt about some essential theoretical aspects ofimbalanced classification. Its time to learn to implement these techniques practically. In R, packages such as ROSE and DMwR helps us to perform sampling strategies quickly. Well work on a problem of binary classification.ROSE (Random Over Sampling Examples) package helps us togenerateartificial data based on sampling methods and smoothed bootstrap approach. This package has well defined accuracy functions to do the tasks quickly.Lets get started#set path
> path <- ""C:/Users/manish/desktop/Data/March 2016""#set working directory
> setwd(path)#installpackages
> install.packages(""ROSE"")
> library(ROSE)The package ROSE comes with an inbuilt imbalanced data set named as hacide. It comprises of two files: hacide.train and hacide.test. Lets load it in R environment:> data(hacide)
> str(hacide.train)
'data.frame': 1000 obs. of 3 variables:
 $ cls: Factor w/ 2 levels ""0"",""1"": 1 1 1 1 1 1 1 1 1 1 ...
 $ x1 : num 0.2008 0.0166 0.2287 0.1264 0.6008 ...
 $ x2 : num 0.678 1.5766 -0.5595 -0.0938 -0.2984 ...As you can see, the data set contains 3 variable of 1000 observations. cls is the response variable. x1 and x2 are dependent variables. Lets check the severity of imbalance in this data set:#check table
table(hacide.train$cls)
 0   1 
980  20#check classes distribution
prop.table(table(hacide.train$cls))
 0   1 
0.98  0.02As we see, this data set contains only 2% of positive cases and 98% of negative cases. This is a severely imbalanced data set. So, how badly can this affect our prediction accuracy ? Lets build a model on this data. Ill be using decision tree algorithm for modeling purpose.> library(rpart)
> treeimb <- rpart(cls ~ ., data = hacide.train)
> pred.treeimb <- predict(treeimb, newdata = hacide.test)Lets check the accuracy of this prediction. To check accuracy, ROSE package has a function names accuracy.meas, it computes important metrics such as precision, recall & F measure.> accuracy.meas(hacide.test$cls, pred.treeimb[,2])
 Call: 
 accuracy.meas(response = hacide.test$cls, predicted = pred.treeimb[, 2])
 Examples are labelled as positive when predicted is greater than 0.5 precision: 1.000
 recall: 0.200
 F: 0.167These metrics provide an interesting interpretation. With threshold value as 0.5,Precision = 1 says there are no false positives. Recall = 0.20 is very much low and indicates that we have higher number of false negatives. Threshold values can be altered also. F = 0.167 is also low and suggests weak accuracy of this model.Well check the final accuracy of this model using ROC curve. This will give us a clear picture, if this model is worth. Using the function roc.curve available in this package:>roc.curve(hacide.test$cls, pred.treeimb[,2], plotit = F)
Area under the curve (AUC): 0.600AUC = 0.60 is a terribly low score. Therefore, it is necessary to balanced data before applying a machine learning algorithm. In this case, the algorithm gets biased toward the majority class and fails to map minority class.Well use the sampling techniques and try to improve this prediction accuracy.This package provides a function namedovun.sample which enables oversampling, undersampling in one go.Lets start with oversampling and balance the data.#over sampling
> data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = ""over"",N = 1960)$data
> table(data_balanced_over$cls)
0  1 
980 980In the code above, method over instructs the algorithm to perform over sampling. N refers to number of observations in the resulting balanced set. In this case, originally we had 980 negative observations. So, I instructed this line of code to over sample minority class until it reaches 980 and the total data set comprises of 1960 samples.Similarly, we can perform undersampling as well. Remember, undersampling is done without replacement.> data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = ""under"", N = 40, seed = 1)$data
> table(data_balanced_under$cls)
0  1 
20 20Now the data set is balanced. But, you see that weve lost significant information from the sample. Lets do both undersampling and oversampling on this imbalanced data. This can be achieved using method = both. In this case, the minority class is oversampled with replacement and majority class is undersampled without replacement.> data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = ""both"", p=0.5,               N=1000, seed = 1)$data
> table(data_balanced_both$cls)
0  1 
520 480p refers to the probability of positive class in newly generated sample.The data generated from oversampling have expected amount of repeated observations. Data generated from undersampling is deprived of important information from the original data. This leads to inaccuracies in the resulting performance. To encounter these issues, ROSE helps us to generate data synthetically as well. The data generated using ROSE is considered to provide better estimate of original data.>data.rose <- ROSE(cls ~ ., data = hacide.train, seed = 1)$data
> table(data.rose$cls)
 0  1 
520 480This generated data has size equal to the original data set (1000 observations). Now, weve balanceddata sets using 4 techniques. Lets compute the model using each data and evaluate its accuracy.#build decision tree models
> tree.rose <- rpart(cls ~ ., data = data.rose)
> tree.over <- rpart(cls ~ ., data = data_balanced_over)
> tree.under <- rpart(cls ~ ., data = data_balanced_under)
> tree.both <- rpart(cls ~ ., data = data_balanced_both)#make predictions on unseen data
> pred.tree.rose <- predict(tree.rose, newdata = hacide.test)
> pred.tree.over <- predict(tree.over, newdata = hacide.test)
> pred.tree.under <- predict(tree.under, newdata = hacide.test)
> pred.tree.both <- predict(tree.both, newdata = hacide.test)Its time to evaluate the accuracy of respective predictions. Using inbuilt function roc.curve allows us to capture roc metric.#AUC ROSE
> roc.curve(hacide.test$cls, pred.tree.rose[,2])
Area under the curve (AUC): 0.989#AUC Oversampling
roc.curve(hacide.test$cls, pred.tree.over[,2])
Area under the curve (AUC): 0.798#AUC Undersampling
roc.curve(hacide.test$cls, pred.tree.under[,2])
Area under the curve (AUC): 0.867#AUC Both
roc.curve(hacide.test$cls, pred.tree.both[,2])
Area under the curve (AUC): 0.798Here is the resultant ROC curve where:Hence, we get the highest accuracy from data obtained using ROSE algorithm. We see that the data generated using synthetic methods result in high accuracy as compared to sampling methods. This technique combined with a more robust algorithm (random forest, boosting) can lead to exceptionally high accuracy.This package also provide us methods to check the model accuracy using holdout and bagging method.This helps us to ensure that our resultant predictions doesnt suffer from high variance.> ROSE.holdout <- ROSE.eval(cls ~ ., data = hacide.train, learner = rpart, method.assess = ""holdout"", extr.pred = function(obj)obj[,2], seed = 1)
> ROSE.holdoutCall: 
ROSE.eval(formula = cls ~ ., data = hacide.train, learner = rpart, 
 extr.pred = function(obj) obj[, 2], method.assess = ""holdout"", 
 seed = 1)Holdout estimate of auc: 0.985We see that our accuracy retains at ~ 0.98 and shows that our predictions arent suffering from high variance. Similarly, you can use bootstrapping by setting method.assess to BOOT. The parameter extr.pred is a function which extracts the column of probabilities belonging to positive class.When faced with imbalanced data set, one might need to experiment with these methods to get the best suited sampling technique. In our case, we found that synthetic sampling technique outperformed the traditional oversampling and undersampling method. For better results, you can use advanced sampling methods which includes synthetic sampling with boosting methods.In this article, Ive discussed the important things one should know to deal with imbalanced data sets. For R users, dealing with such situations isnt difficult since we are blessed with some powerful and awesome packages.Did you find this article helpful ? Have you used these methods before? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/
Exploring Recommendation System (with an implementation model in R),Learn everything about Analytics|Introduction|Recommendation System in Banks  Example|What exactly is the work of a recommendation engine?|What are the types of Recommender Engines ?|How do we decide the performance metric of such engines?|Building an Item-Item collaborative filtering Recommendation Engine using R|End Notes,"You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Practical Guide to deal with Imbalanced Classification Problems in R|How to perform feature selection (i.e. pick important variables) using Boruta Package in R ?|
Tavish Srivastava
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How do we make recommendations in our lives ? We do it based on our past experiences. Now imagine, what if we start making instant recommendations based on data in our real lives? First, well feel like an intelligent adviser. Second, we will no longer be humans.Therefore, we aim tobuildintelligent softwares, which are capable ofproviding cogent recommendations.We are sub-consciously exposed to recommendation systems when we visit websites such asAmazon, Netflix, imdb and many more. Apparently, they have become an integral part of online marketing (pushing products online). Lets learn more about them here.In this article, Ive explained the working of recommendation system using a real life example, just to show you this is not limited to online marketing. It is being used by all industries. Also, well learn about its various types followed by a practical exercise in R. The term recommendation engine & recommendation system has been used interchangeably. Dont get confused!Today, every industry is making full use of recommendation systems with their own tailored versions. Lets take banking industry for an example.Bank X wants to make use of the transactions information and accordingly customize the offers they provideto their existing credit and debit card users. Here is what the end state of such analysis looks like:Customer Z walks in to a Pizza Hut. Hepays the food bill through bank Xs card. Using all the past transaction information, bank X knows that Customer Z likes to have an ice cream after his pizza. Using this transaction information at pizza hut, bank has located the exact location of the customer. Next, it finds 5 ice cream stores which are close enough to the customer and 3 of which have ties with bank X.This is the interesting part.Now, here are the deals with these ice-cream store:Store 1 : Bank profit  $2, Customer expense  $10, Propensity of customer to respond  20%
Store 2 : Bank profit  $2, Customer expense  $10, Propensity of customer to respond  20%
Store 3 : Bank profit  $5, Customer expense  $12, Propensity of customer to respond  20%
Store 4 : Bank profit  $6, Customer expense  $12, Propensity of customer to respond  20%
Store 5 : Bank profit  $4, Customer expense  $11, Propensity of customer to respond  20%Lets assume the marked prize is proportional to the desire of customer to have that ice-cream. Hence, customer struggles with the trade-off that whether to fulfil his desire at the extra cost or buy the cheaper ice cream. Bank X wants the customer to go to store 3,4 or 5 (higher profits). It can increase the propensity of the customer to respond if it gives him a reasonable deal. Lets assume that discounts are always whole numbers. For now, the expected value was :Expected value = 20%*{2 + 2 + 5 + 6 + 4 } = $ 19/5 = $3.8Can we increase the expected value by giving out discounts. Here is how the propensity varies at store (3,4,5) varies :Store 3 : Discount of $1 increases propensity by 5%, a discount of $2 by7.5% and a discount of $3 by10%Store 4: Discount of $1 increases propensity by25%, a discount of $2 by30%, a discount of $3 by35% and a discount of $4 by80%Store 5: No change with any discountBanks cannot give multiple offers at the same time with competing merchants. You need to assume that an increase in ones propensity gives equal percentage point decrease in all other propensity. Here is the calculation for the most intuitive case  Give a discount of $2 at store 4.Expected value = 50%/4 * (2 + 2 + 5 + 4) + 50% * 5 = $ 13/8+ $2.5 = $1.6 + $2.5 = $4.1Think Box : Is there any better option available which can give bank a higher profit? Id be interested to know!You see, making recommendations isnt about extracting data, writing codes and be done with it. Instead, it requires mathematics (apparently), logical thinking and a flair to use a programming language. Trust me, third one is the easiest of all. Feeling confident? Lets proceed.Previous example would have given you a fair idea.Its time to makeit crystal clear. Lets understand what alla recommendation engine can do in context of previous example (Bank X):There are broadly two types of recommender engines and based on the industry we make this choice. We have explained each of these algorithms in ourprevious articles, but here I try to put apracticalexplanationto help you understand them easily.Ive explained these algorithms in context of the industry they are used in and what makes them apt for these industries.Good question! We mustknow that performance metrics are strongly driven by business objectives. Generally, there are three possible metrics which you might want to optimise:Lets get some hands-on experience building a recommendation engine. Here, Ive demonstrated building an item-item collaborative filter recommendation engine. The data contains just 2 columns namely individual_merchant and individual_customer. The data is available to download  Download Now.The code is easy to understand. Hence, I havent explained it explicitly. If you find any part of code hard to understand, ask me in comments section below.#load libraries
> library(plyr)
> library(""arules"")
> library(readr)#load data
 #This file has two columns inidividual_merchant and inidividual_customer
> input <- read_csv(""Transaction_file.csv"")

#Get the list of merchants/items
> merchant <- unique(input$individual_merchant)
> merchant <- merchant[order(merchant)]
> target_merchants <- merchant
> sno <- 1:length(target_merchants)
> merchant_ident <- cbind(target_merchants,sno)

#Create a reference mapper for all merchant
> colnames(merchant_ident) <- c(""individual_merchant"",""sno"")

# Create a correlation matrix for these merchants
> correlation_mat = matrix(0,length(merchant),length(target_merchants))
> correlation_mat = as.data.frame(correlation_mat)
> trans = read.transactions(""Transaction_file.csv"", format = ""single"", sep = "","", cols =
c(""inidividual_customer"", ""individual_merchant""))
> c <- crossTable(trans)
> rowitem <- rownames(c)
> columnitem <- colnames(c)
> correlation_mat <- c[order(as.numeric(rowitem)),order(as.numeric(columnitem))]
> for(i in 1:9822) {
    correlation_mat[i,] <- correlation_mat[i,]/correlation_mat[i,i]
 }
> colnames(correlation_mat) <- target_merchants
> rownames(correlation_mat) <- merchant

# Now let's start recommending for individual customer
> possible_slots <- 20
> avail <- 21
> merch_rec <- matrix(0, nrow = length(target_customers), ncol = avail)
> merch_rec[,1] <- unique(input3$Cust_map)
> correlation_mat <- as.matrix(correlation_mat)
> position <- 1
> for (i in 1:length(target_customers)) {
 been_thr <- input[position : (position + customer_merch_ct[i] - 1),'individual_merchant']
 merging <- as.data.frame(merchant_ident[merchant_ident[,'individual_merchant'] %in%     been_thr,])
 corel_subset <- correlation_mat[merging$sno,] 
 will_go <- colSums(corel_subset) 
 will_go_merch <- target_merchants[order(-will_go)]
 not_been_there <- will_go_merch[!will_go_merch %in% been_thr]
 will_go_propensity <- will_go[order(-will_go)][!will_go_merch %in% been_thr]
 merch_rec[i,2:avail] <- not_been_there[1:possible_slots] 
 position <- position + customer_merch_ct[i] 
}Recommended engines have become extremely common because they solve one of the commonly found business case for all industries. Substitute to these recommendation engine are very difficult because they predict for multiple items/merchant at the same time. Classification algorithms struggle to take in so many classes as the output variable.In this article, we learnt about the use of recommendation systems in Banks. We also looked at implementing a recommendation engine in R. No doubt, they are being used across all sectors of industry, with a common aim to enhance customer experience.Did you like reading this article ? Have you built a recommendation system in past? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/exploring-building-banks-recommendation-system/
How to perform feature selection (i.e. pick important variables) using Boruta Package in R ?,Learn everything about Analytics|Introduction|What is Boruta algorithm and why such a strange name ?|How does it work?|What makes it different from traditional feature selection algorithms?|Boruta in Action in R (Practical)|Boruta vs Traditional Feature Selection Algorithm|End notes,"You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|Exploring Recommendation System (with an implementation model in R)|Course Review  Big data and Hadoop Developer Certification Course by Simplilearn|
Guest Blog
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Variable selection is an important aspect of model building which every analyst must learn. After all, it helps in building predictive models free from correlated variables, biases and unwanted noise.A lot of novice analysts assume that keeping all (or more) variables will result in the best model as you are not losing any information. Sadly, that is not true!How many times has it happened that removing a variable from the model has increased your model accuracy ?At least, it has happened tome. Such variables are often found to be correlated and hinder achieving higher model accuracy. Today,well learn one of the ways of how to get rid of such variables in R. I must say, R has an incredible CRAN repository. Out of all packages, one such availablepackage for variable selection is Boruta Package.In this article,well focus on understanding the theory and practical aspects of using Boruta Package. Ive followed a step wise approach to help you understand better.Ive also drawn a comparison of boruta with other traditional feature selection algorithms. Using this, you can arrive at a more meaningful set of features which can pave the way for a robust prediction model. The termsfeatures, variables and attributes have been used interchangeably, so dont get confused!Boruta is a feature selection algorithm. Precisely, it works as a wrapper algorithm around Random Forest. Thispackage derive its name from ademon in Slavic mythology who dwelled in pine forests.We know that feature selection is a crucial step in predictive modeling. This technique achieves supreme importance when adata set comprised ofseveral variables is given for model building.Boruta can be youralgorithm of choice to deal with such data sets. Particularly when one is interested in understanding the mechanisms related to the variable of interest, rather than just building a black box predictive model with good prediction accuracy.Belowis the step wise working of boruta algorithm:Boruta follows an all-relevant feature selection method where it captures all features which are in some circumstances relevant to the outcome variable. In contrast, most of the traditional feature selection algorithms follow a minimal optimal method where they rely on a small subset of features which yields a minimal error on a chosen classifier.While fitting a random forest model onadata set, you can recursively get rid of features in each iteration which didnt perform well in the process. This will eventually lead to a minimal optimal subset of features as the method minimizes the error of random forest model. This happens by selecting an over-pruned version of the input data set, which in turn, throws away some relevant features.On the other hand, boruta find all features which are either strongly or weakly relevant to the decision variable. This makes it well suited for biomedical applications where one might be interested to determine which human genes (features) are connected in some way to a particular medical condition (target variable).Till here, we have understood the theoretical aspects ofBoruta Package. But, that isnt enough. The real challenge starts now. Lets learn to implement this package in R.First things first. Lets install and call this package for use.> install.packages(""Boruta"")
> library(Boruta)Now, well load the data set. For this tutorial Ive taken thedata set from Practice Problem Loan Prediction> setwd(""../Data/Loan_Prediction"")
> traindata <- read.csv(""train.csv"", header = T, stringsAsFactors = F)Lets have alook at the data.> str(traindata)
> names(traindata) <- gsub(""_"", """", names(traindata))gsub() function is used to replace an expression with other one. In this case, Ive replaced the underscore(_) with blank().Lets check if this data set has missing values.> summary(traindata)We find that many variables have missing values. Its important to treat missing values prior to implementing boruta package. Moreover, this data set also has blank values. Lets clean this data set.Now well replace blank cells with NA. This will help me treat all NAs at once.> traindata[traindata == """"] <- NAHere, Im following the simplest method of missing value treatment i.e. list wise deletion. More sophisticated methods & packages of missing value imputation can be found here.> traindata <- traindata[complete.cases(traindata),]Lets convert the categorical variables into factor data type.> convert <- c(2:6, 11:13)
 > traindata[,convert] <- data.frame(apply(traindata[convert], 2, as.factor))Now is the time to implement and check the performance of boruta package. The syntax of boruta is almost similar to regression (lm) method.> set.seed(123)
>boruta.train <- Boruta(LoanStatus~.-LoanID, data = traindata, doTrace = 2)
> print(boruta.train)Boruta performed 99 iterations in 18.80749 secs.
5 attributes confirmed important: ApplicantIncome, CoapplicantIncome,
CreditHistory, LoanAmount, LoanAmountTerm.
4 attributes confirmed unimportant: Dependents, Education, Gender, SelfEmployed.
2 tentative attributes left: Married, PropertyArea.Boruta gives a crystal clear call on the significance of variables in a data set. In this case, out of 11 attributes, 4 of them are rejected and 5 are confirmed. 2 attributes are designated as tentative. Tentative attributes have importance so close to their best shadow attributes that Boruta is not able to make a decision with the desired confidence in default number of random forest runs.Now, well plot the boruta variable importance chart.By default, plot function in Boruta adds the attribute values to the x-axis horizontally where all the attribute values are not dispayed due to lack of space.Here Im adding the attributes to the x-axis vertically.> plot(boruta.train, xlab = """", xaxt = ""n"")
> lz<-lapply(1:ncol(boruta.train$ImpHistory),function(i)
boruta.train$ImpHistory[is.finite(boruta.train$ImpHistory[,i]),i])
> names(lz) <- colnames(boruta.train$ImpHistory)
> Labels <- sort(sapply(lz,median))
> axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta.train$ImpHistory), cex.axis = 0.7)Blue boxplots correspond to minimal, average and maximum Z score of a shadow attribute. Red, yellow and green boxplots represent Z scores of rejected, tentative and confirmed attributes respectively.Now is the time to take decision on tentative attributes. The tentative attributes will beclassified as confirmed or rejected by comparing the median Z score of the attributes with the median Z score of the best shadow attribute. Lets do it.> final.boruta <- TentativeRoughFix(boruta.train)
> print(final.boruta)Boruta performed 99 iterations in 18.399 secs.
Tentatives roughfixed over the last 99 iterations.
6 attributes confirmed important: ApplicantIncome, CoapplicantIncome,
CreditHistory, LoanAmount, LoanAmountTerm and 1 more.
5 attributes confirmed unimportant: Dependents, Education, Gender, PropertyArea,
SelfEmployed.Boruta result plot after the classification of tentative attributesIts time for results now. Lets obtain the list of confirmed attributes> getSelectedAttributes(final.boruta, withTentative = F)
 [1] ""Married"" ""ApplicantIncome"" ""CoapplicantIncome"" ""LoanAmount""
 [5] ""LoanAmountTerm"" ""CreditHistory""Well create adata frame of the final result derived from Boruta.> boruta.df <- attStats(final.boruta)
> class(boruta.df)
[1] ""data.frame""
> print(boruta.df)          meanImp  medianImp  minImp maxImp normHits  decisionGender 1.04104738 0.9181620 -1.9472672 3.767040 0.01010101 RejectedMarried 2.76873080 2.7843600 -1.5971215 6.685000 0.56565657 ConfirmedDependents 1.15900910 1.0383850 -0.7643617 3.399701 0.01010101 RejectedEducation 0.64114702 0.4747312 -1.0773928 3.745441 0.03030303 RejectedSelfEmployed -0.02442418 -0.1511711 -0.9536783 1.495992 0.00000000 RejectedApplicantIncome 6.05487791 6.0311639 2.9801751 9.197305 0.94949495 ConfirmedCoapplicantIncome 5.76704389 5.7920332 1.9322989 10.184245 0.97979798 ConfirmedLoanAmount 5.19167613 5.3606935 1.7489061 8.855464 0.88888889 ConfirmedLoanAmountTerm 5.50553498 5.3938036 2.0361781 9.025020 0.90909091 ConfirmedCreditHistory 59.57931404 60.2352549 51.7297906 69.721650 1.00000000 ConfirmedPropertyArea 2.77155525 2.4715892 -1.2486696 8.719109 0.54545455 RejectedLets understand the parameters used in Boruta as follows:For more complex parameters, please refer to the package documentation of Boruta.Till here, we have learnt about the concept and steps to implement boruta package in R.What if we used a traditional feature selection algorithm such as recursive feature elimination on the same data set. Do we end up with the same set of important features? Let us find out.Now, well learn the steps used to implement recursive feature elimination (RFE). In R, RFE algorithm can be implemented using caret package.Lets start by defining a control function to be used with RFE algorithm. Well load the required libraries:> library(caret)
> library(randomForest)
> set.seed(123)
> control <- rfeControl(functions=rfFuncs, method=""cv"", number=10)Here we have specified a random forest selection function through rfFuncs option (which is also the underlying algorithm in Boruta)Lets implement the RFE algorithm now.> rfe.train <- rfe(traindata[,2:12], traindata[,13], sizes=1:12, rfeControl=control)Im sure this is self explanatory. traindata[,2:12] refers to selecting all independent variablesexcept the ID variable. traindata[,13] selects only the dependent variable. It might take some time to run.We can also check the outcome of this algorithm.> rfe.trainRecursive feature selection
Outer resampling method: Cross-Validated (10 fold)
Resampling performance over subset size:Variables Accuracy  Kappa  AccuracySD  KappaSD Selected
  1   0.8083  0.4702  0.03810   0.1157   *
  2   0.8041  0.4612  0.03575   0.1099 
  3   0.8021  0.4569  0.04201   0.1240 
  4   0.7896  0.4378  0.03991   0.1249 
  5   0.7978  0.4577  0.04557   0.1348 
  6   0.7957  0.4471  0.04422   0.1315 
  7   0.8061  0.4754  0.04230   0.1297 
  8   0.8083  0.4767  0.04055   0.1203 
  9   0.7897  0.4362  0.05044   0.1464 
  10   0.7918  0.4453  0.05549   0.1564 
  11   0.8041  0.4751  0.04419   0.1336The top 1 variables (out of 1):
 CreditHistoryThis algorithm gives highest weightage to Credit History. Now, well plot the result of RFE algorithm andobtain a variable importance chart.> plot(rfe.train, type=c(""g"", ""o""), cex = 1.0, col = 1:11)Lets extract the chosen features. I am confident it would result in Credit History.>predictors(rfe.train)
[1] ""CreditHistory""Hence, we see that recursive feature elimination algorithm has selected CreditHistory as the only important feature among the 11 features in the dataset.As compared to this traditional feature selection algorithm, boruta returned a much better result of variable importance which was easy to interpret as well ! I find it awesome to work on R where one has access to so many amazing packages.Im sure there would be many other packages for feature selection. Id love to read about them.Boruta is an easy to use packageasthere arent many parameters to tune / remember. You shouldntuse a data set with missing values to check important variables using Boruta. Itll blatantly throw errors. You can use this algorithm on any classification / regression problem in hand to come up with a subset of meaningful features.In this article, Ive used a quick method to impute missing value because the scope of this article was to understand boruta (theory & practical). Id suggest you to use advanced methods of missing value imputation. After all, information available in data is all we look for ! Keep going.Did you like reading this article ? What other methods of variable selection do you use? Do share your suggestions / opinions in the comments section below.About the AuthorDebarati Dutta is MA Econometrics graduate from University of Madras. She has more than 3 years of experience in data analytics and predictive modeling across multiple domains. She has worked in companies such as Amazon, Antuit, Netlink. Currently, shes based out ofMontreal, Canada.Debarati is the first winner of Blogathon. She won amazon voucher worth INR 5000.",https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/
Course Review  Big data and Hadoop Developer Certification Course by Simplilearn,Learn everything about Analytics|Introduction|What is the objectiveof this course ?|What is the Course Structure ?|Whatsthe Faculty like ?|How good is the content ?|Who should takethis course ?|Past Students Experience|Overall Recommendation|End Notes,"You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Like this:|Related Articles|How to perform feature selection (i.e. pick important variables) using Boruta Package in R ?|Practical Guide to Principal Component Analysis (PCA) in R & Python|
Kunal Jain
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There is no question that the Big Data revolution sweeping through the world of business has made its impact on companies big and small. Multinational giants and startups alike are going out of their way to incorporate Big Data capabilities, including the likes of Google, Amazon, Airbnb, Uber, Facebook, Tesla, and more.The Big Data boom has resulted in a surge in the demand for skilled Big Data professionals not just in India  but around the world. In particular, the insurance, FMCG, online marketing, and e-commerce sectors -which deal with huge volumes of data on a regular basis  are facing a shortage of data specialists.And according to a report by McKinsey Global, the demand-supply gap is acute, with an estimated 1.5 million Big Data managers needed by 2018!Simplilearns Big Data and Hadoop Developer Certification Trainingoffers you an opportunity to start your dream career in big data from scratch.Not too long ago, I had a chance to chat with Indranil Banerjee, Associate Director, Simplilearn. I learned about their motives for launching this course andhow they are trying to transform big data industry in India. I met their course instructors and learned about their methodology of teaching this course.Recently I came to know that many students are contemplating taking this course and looking for further information on the same. To help them make a more informed choice, Ive reviewed the course and offered my input.Read on to find out more about this popular course and why its been the starting point for thousands of successful Big Data professionals the world over.The main objective of this courseis todevelop skilled professionalsin various big data technologies like Hadoop, Apache Spark, Mapreduce, Cassandra and many more. The course aims to provide comprehensive knowledge of Big Data tools to enable professionals at any level tackle data assignments with ease. The mandatory real-life industry projects that are included with the course ensure participants get adequate hands-on experience.Needless to say, all this will contribute to ones chances of employability in big data industry.This course is available in two formats: Self-paced, and Instructor-led \ Online Classroom. The online classroom instructor led program is offered via Flexipass. This feature allows access to multiple batches of training, across timezones for a period of 90 days. Both formats provide high quality e-learning content for 180 days to enable self paced training.In both cases,the self-contained pedagogy used in this course primarily benefits students withno prior knowledge of big data. For instance, big data requires knowledge of Java. But even if you have no formal training in Java, the course offers a module in Java to get you started.This course follows a basic to advanced level teaching structure. In addition to other big data tools, this courseoffers a dedicated module on Apache Spark (widely considered to be the future of Big Data). This course is thus up-to-date and is in-line with the latest industry trends and developments. The course offers training inHadoop 2.7, MapReduce, Pig, Hive, HBase, Zookeeper, Sqoop, Flume and predictive modeling in R (additional course). Check out the complete course listinghere.For an enhanced learning experience, it offers a unique CloudLab feature. This feature gives the user the experience of working on a state of the art hadoop cluster with no maintenance issues. Also, the inclusion of industry projects across retail, insurance, social media, offers the much-needed practical experienceof working on Big Data. A candidate will receive a 3 months experience certificate after the successful completion of this course.However, amidst such exemplary offerings,moving with the currents trends, I understand that they are adding Python/R in the course as well. Use of these analytic tools with Big Data will further strengthen their course, which is already among the best available today.The training faculty that host the sessions are fairly experienced and aware of the latest industry developments. I found no instances where the course instructor struggled to answer questions adequatelyI noticed that the instructors laid more emphasis on explaining technical topics with illustrations from real-world scenarios. This helps participants relate theory with practice and absorb dense technical details easily.Needles to say, if you opt for self paced classes, youd miss out on this experience. However, the choice of course is completely left toyour discretion.The pre-recorded video tutorials have nice and clear video & voice clarity. The instructors accent is easy to understand and interpret.Difficult topics areexplained using interesting real life examples.The upcoming version of the course will provide upgraded content based on the new Cloudera certification.They will also feature motion graphics and an on-screen instructor.I think that will be an exciting change as I always feel that on-screen instructor provides higher engagement.This will help increase concentration and make learning more visualized compared to voice explaining slide mode.This course is broadly suited for candidates of all domains who are keen to pursue big data as a career. The inclusion of Java module hasmade it even more appealing for non-technical candidates as well.However, as per my understanding, prior knowledge of java in this course shouldbe a major advantage. The additional java module can be used as a refresher (if required). Candidates from IT background would find this course easier to relate and understand. On the other hand, a newbie with some diligence and enough practice should be able to discern the terminologies used in different stages of this course.Hence, I would thus suggest that this course is best suited for experienced (<2 years) IT professionals. But that doesnt mean a newbie cannot benefit from the training. You might need to work harder than others, but if you truly aspire to become a big data scientist, I dont see it being impossible.Professionals who have taken the Big Data certification training in the past seem quite satisfied. Many have landed their first big gig in Big Data as a direct result of the course. Here are a few reviews I have picked from their website.Deepak Priyadarshi (Founder and CEO at Filament Factory)  a student of this course : It was an amazing experience to train on BIG DATA with Simplilearn. This course is very feasible to the beginners and the course contents are downloadable on the go.Devendra Sahu (Project Manager at Wipro)  This program is very well structured and planned to help anyone who want to understand Big Data & Hadoop concepts. I recommend it to everyone who wants to get into the Hadoop ecosystem.My final verdict is: I recommend this course to all aspiring big data candidates. This is a good starting point for your journey. Apart from the quality of the offerings, which is exceptional, the price point of the course also makes it value-for-money material.The self paced module is available for just Rs.10,999. The instructor led / online classroom module can be availed at Rs. 21,999. After looking around a bit, Ive come to realize that its difficult to find a course in this price range, considering the immense array of offerings being provided.Also, the CloudLab feature offered saves a lot of time spent in installation and configuration. This virtual hadoop environment provides an accelerated learning experience.The inclusion of industry projects across sectors make for anmuch needed practical experience on your resume.I hope this review would help you in deciding on your first or next big data course. I understand there are many courses available on big data and each has its own value proposition, Simplilearns course is a great placeto begin.If you have any questions about the program, Ill be happyto share my views & perspective. If I dont have the answer myself, Ill reach out to people who do.",https://www.analyticsvidhya.com/blog/2016/03/review-big-data-hadoop-developer-certification-simplilearn/
Practical Guide to Principal Component Analysis (PCA) in R & Python,Learn everything about Analytics|Overview|Introduction|What is Principal Component Analysis ?|What are principal components ?|Why is normalization of variables necessary ?|Implement PCA in R & Python (with interpretation)|Points to Remember|End Notes,"Predictive Modeling with PCA Components|You cantest your skills and knowledge.Check out LiveCompetitionsand compete with bestData Scientists from all over the world.|Share this:|Related Articles|Course Review  Big data and Hadoop Developer Certification Course by Simplilearn|Winning Solutions of DYD Competition  R and XGBoost Ruled|
Analytics Vidhya Content Team
|65 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Too much of anything is good for nothing!Picture this  you are working on a large scale data science project. What happens when the given data set has too many variables? Here are few possible situations which you might come across:Trust me, dealing with such situations isnt as difficult as it sounds. Statistical techniques such asfactor analysis and principal component analysis help to overcome such difficulties.In this post, Ive explained the concept of principal component analysis in detail. Ive kept the explanation to be simple and informative. For practical understanding, Ive also demonstrated using this technique in R with interpretations.Note: Understanding this concept requires prior knowledge of statisticsUpdate (as on 28th July): Process ofPredictive Modeling with PCA Components in R is added below.Practical guide to Principal Component Analysis in R & PythonIn simple words, principal component analysis is a method ofextracting important variables (in form of components) from a large set of variables available in a data set. It extracts low dimensional set of features from a high dimensional data set with a motive tocapture as much information as possible. With fewer variables,visualization also becomes much more meaningful. PCA is more useful when dealing with 3 or higher dimensional data.It is always performed on a symmetric correlation or covariance matrix. This means the matrix should be numeric and have standardized data.Lets understand it using an example:Lets say we have a data set of dimension300 (n) 50 (p).n represents the number of observations and p represents number of predictors. Since we have a large p = 50, therecan bep(p-1)/2 scatter plots i.e more than 1000 plots possible to analyze the variable relationship.Wouldnt is be a tedious job to perform exploratory analysis on this data ?In this case, it would be a lucid approach to select a subset of p(p << 50) predictor which captures as much information. Followed byplotting the observation in the resultant low dimensional space.The image below shows the transformation of a high dimensional data (3 dimension) to low dimensional data (2 dimension) using PCA. Not to forget, each resultant dimension is a linear combination of p featuresSource: nlpcaA principal component is a normalized linear combination of theoriginal predictors in a data set. In image above, PC1 and PC2 are the principal components. Lets say we have a set of predictors as X,X...,XpThe principal component can be writtenas:Z = X +X +X + .... +pXpwhere,Therefore,First principal componentis a linear combination of original predictor variables which captures the maximum variance in the data set.Itdetermines the direction of highest variability in the data.Larger the variability captured in first component, larger the information captured by component.No other component can have variability higher than first principal component.The first principal component results in a line which is closest to the data i.e. it minimizes the sum of squared distance between a data point and the line.Similarly, we can compute the second principal component also.Second principal component (Z) is also a linear combination of original predictors which captures the remaining variance in the data set and is uncorrelated with Z.In other words, the correlation between first and second component should iszero. It can be represented as:Z = X + X + X + .... + p2XpIf the two components are uncorrelated, their directions should be orthogonal (image below). This image is based on a simulated data with 2 predictors. Notice the direction of the components, as expected they are orthogonal. This suggests the correlation b/w these components in zero.All succeeding principal component follows a similar concept i.e. they capture the remaining variation without being correlated with the previous component.In general,for n pdimensional data, min(n-1, p) principal component can be constructed.The directions of these components are identified in an unsupervised way i.e. the response variable(Y) is not used to determine the component direction. Therefore, it isan unsupervised approach.Note: Partial least square (PLS) is a supervised alternative to PCA. PLS assigns higher weight to variables which are strongly related to response variable to determine principal components.The principal components are supplied with normalized version of original predictors. This is because, the original predictors may have different scales. For example: Imagine a data set with variables measuring units as gallons, kilometers, light years etc. It is definite that the scale of variances in these variables will be large.Performing PCA on un-normalized variables will lead to insanely large loadings for variables with high variance. In turn, this will lead to dependence of a principal component on the variable with high variance. This is undesirable.As shown in image below, PCA was run on a data set twice (with unscaled and scaled predictors). This data set has ~40 variables. You can see, first principal component is dominated by a variable Item_MRP. And, second principal component is dominated by a variable Item_Weight. This domination prevails due to high value of variance associated with a variable. When the variables are scaled, we get a much better representation of variables in 2D space.How many principal components to choose ? I could dive deep in theory, but it would be better to answer these question practically.For this demonstration, Ill be using the data set from Big Mart Prediction ChallengeIII.Remember, PCA can be applied only on numerical data. Therefore, if the data has categorical variables they must be converted to numerical. Also, make sure you have done the basic data cleaning prior to implementing this technique. Lets quickly finish with initial data loading and cleaning steps:#directory path
 > path <- "".../Data/Big_Mart_Sales""#set working directory
 > setwd(path)#load train and test file
 > train <- read.csv(""train_Big.csv"")
 > test <- read.csv(""test_Big.csv"")#add a column
 > test$Item_Outlet_Sales <- 1#combine the data set
 > combi <- rbind(train, test)#impute missing values with median
 > combi$Item_Weight[is.na(combi$Item_Weight)] <- median(combi$Item_Weight, na.rm = TRUE)#impute 0 with median
 > combi$Item_Visibility <- ifelse(combi$Item_Visibility == 0, median(combi$Item_Visibility),                  combi$Item_Visibility)#find mode and impute
 > table(combi$Outlet_Size, combi$Outlet_Type)
 > levels(combi$Outlet_Size)[1] <- ""Other""Till here, weve imputed missing values. Now we are left with removing the dependent (response) variable and other identifier variables( if any). As we said above, we are practicing an unsupervised learning technique, hence response variable must be removed.#remove the dependent and identifier variables
 > my_data <- subset(combi, select = -c(Item_Outlet_Sales, Item_Identifier,                    Outlet_Identifier))Lets check the available variables ( a.k.a predictors) in the data set.#check available variables
 > colnames(my_data)Since PCA works on numeric variables, lets see if we have any variable other than numeric.#check variable class
 >str(my_data)'data.frame': 14204 obs. of 9 variables:
 $ Item_Weight : num 9.3 5.92 17.5 19.2 8.93 ...
 $ Item_Fat_Content : Factor w/ 5 levels ""LF"",""low fat"",..: 3 5 3 5 3 5 5 3 5 5 ...
 $ Item_Visibility : num 0.016 0.0193 0.0168 0.054 0.054 ...
 $ Item_Type : Factor w/ 16 levels ""Baking Goods"",..: 5 15 11 7 10 1 14 14 6 6 ...
 $ Item_MRP : num 249.8 48.3 141.6 182.1 53.9 ...
 $ Outlet_Establishment_Year: int 1999 2009 1999 1998 1987 2009 1987 1985 2002 2007 ...
 $ Outlet_Size : Factor w/ 4 levels ""Other"",""High"",..: 3 3 3 1 2 3 2 3 1 1 ...
 $ Outlet_Location_Type : Factor w/ 3 levels ""Tier 1"",""Tier 2"",..: 1 3 1 3 3 3 3 3 2 2 ...
 $ Outlet_Type : Factor w/ 4 levels ""Grocery Store"",..: 2 3 2 1 2 3 2 4 2 2 ...Sadly,6 out of 9 variables are categorical in nature.We have some additional work to do now. Well convert these categorical variables into numeric using one hot encoding.#load library
> library(dummies)#create a dummy data frame
> new_my_data <- dummy.data.frame(my_data, names = c(""Item_Fat_Content"",""Item_Type"",
                ""Outlet_Establishment_Year"",""Outlet_Size"",
                ""Outlet_Location_Type"",""Outlet_Type""))To check, if we now have a data set of integer values, simple write:#check the data set
> str(new_my_data)And, we now have all the numerical values. Lets divide the data into test and train.#divide the new data
>pca.train <- new_my_data[1:nrow(train),]
> pca.test <- new_my_data[-(1:nrow(train)),]We can now go ahead with PCA.The base R function prcomp() is used to performPCA. By default, it centers the variable to have mean equals to zero. With parameter scale. = T, we normalize the variables to have standard deviation equals to 1.#principal component analysis
 > prin_comp <- prcomp(pca.train, scale. = T)
 > names(prin_comp)
 [1] ""sdev""   ""rotation"" ""center""  ""scale""  ""x""The prcomp() function results in 5 useful measures:1. center and scale refers to respective mean and standard deviation of the variables that are used for normalization prior to implementing PCA#outputs the mean of variables
 prin_comp$center#outputs the standard deviation of variables
 prin_comp$scale2. The rotation measure provides the principal component loading. Each column of rotation matrix contains the principal component loading vector. This is the most important measure we should be interested in.> prin_comp$rotationThis returns 44 principal components loadings. Is that correct ? Absolutely. In a data set, the maximum number of principal component loadings is a minimum of (n-1, p). Lets look at first 4 principal components and first 5 rows.> prin_comp$rotation[1:5,1:4]
                PC1      PC2      PC3       PC4
Item_Weight        0.0054429225  -0.001285666  0.011246194  0.011887106
Item_Fat_ContentLF    -0.0021983314  0.003768557 -0.009790094 -0.016789483
Item_Fat_Contentlow fat  -0.0019042710  0.001866905 -0.003066415 -0.018396143
Item_Fat_ContentLow Fat  0.0027936467  -0.002234328  0.028309811  0.056822747
Item_Fat_Contentreg    0.0002936319  0.001120931  0.009033254 -0.0010266153. In order to compute the principal component score vector, we dont need to multiply the loading with data. Rather, the matrix x has the principal component score vectors in a 8523  44 dimension.> dim(prin_comp$x)
[1] 8523  44Lets plot the resultant principal components.>biplot(prin_comp, scale = 0)The parameter scale = 0 ensures that arrows are scaled to represent the loadings. To make inference from image above, focus on the extreme ends (top, bottom, left, right) of this graph.We infer than first principal component corresponds to a measure of Outlet_TypeSupermarket, Outlet_Establishment_Year 2007. Similarly, it can be said that the second component corresponds to a measure of Outlet_Location_TypeTier1, Outlet_Sizeother. For exact measure of a variable in a component, you should look at rotation matrix(above) again.4. The prcomp() function also provides the facility to compute standard deviation of each principal component. sdev refers to the standard deviation of principal components.#compute standard deviation of each principal component
 > std_dev <- prin_comp$sdev#compute variance
 > pr_var <- std_dev^2#check variance of first 10 components
 > pr_var[1:10]
 [1] 4.563615 3.217702 2.744726 2.541091 2.198152 2.015320 1.932076 1.256831
 [9] 1.203791 1.168101We aim to find the components which explain the maximum variance. This is because, we want to retain as much information as possible using these components. So, higher is the explained variance, higher will be the information contained in those components.To compute the proportion of variance explained by each component, we simply divide the variance by sum of total variance. This results in:#proportion of variance explained
 > prop_varex <- pr_var/sum(pr_var)
 > prop_varex[1:20]
 [1] 0.10371853 0.07312958 0.06238014 0.05775207 0.04995800 0.04580274
 [7] 0.04391081 0.02856433 0.02735888 0.02654774 0.02559876 0.02556797
 [13] 0.02549516 0.02508831 0.02493932 0.02490938 0.02468313 0.02446016
 [19] 0.02390367 0.02371118This shows that first principal component explains 10.3% variance. Second component explains 7.3% variance. Third component explains 6.2% variance and so on. So, how do we decide how many components should we select for modeling stage ?The answer to this question is provided by a scree plot. A scree plot is used to access components or factors which explains the most of variability in the data. It represents values in descending order.#scree plot
> plot(prop_varex, xlab = ""Principal Component"",
      ylab = ""Proportion of Variance Explained"",
      type = ""b"")The plot above shows that ~ 30 components explains around 98.4% variance in the data set. In order words, using PCA we have reduced 44 predictors to 30 without compromising on explained variance. This is the power of PCA> Lets do a confirmation check, by plotting a cumulative variance plot. This will give us a clear picture of number of components.#cumulative scree plot
 > plot(cumsum(prop_varex), xlab = ""Principal Component"",
       ylab = ""Cumulative Proportion of Variance Explained"",
       type = ""b"")This plot shows that 30 components results in variance close to ~ 98%. Therefore, in this case, well select number of components as 30 [PC1 to PC30] and proceed to the modeling stage. This completes the steps to implement PCA on train data. For modeling, well use these 30 components as predictor variables and follow the normal procedures.After weve calculated the principal components on training set, lets now understand the process of predicting on test data using these components. The process is simple. Just like weve obtained PCA components on training set, well get another bunch of components on testing set. Finally, we train the model.But, few important points to understand:So,what should we do?We should do exactly the same transformation to the test set as we did to training set, including the center and scaling feature. Lets do it in R:#adda training set with principal components
> train.data <- data.frame(Item_Outlet_Sales = train$Item_Outlet_Sales, prin_comp$x)#we are interested in first 30 PCAs
> train.data <- train.data[,1:31]#run a decision tree
> install.packages(""rpart"")
> library(rpart)
> rpart.model <- rpart(Item_Outlet_Sales ~ .,data = train.data, method = ""anova"")
> rpart.model#transform test into PCA
> test.data <- predict(prin_comp, newdata = pca.test)
> test.data <- as.data.frame(test.data)#select the first 30 components
> test.data <- test.data[,1:30]#make prediction on test data
> rpart.prediction <- predict(rpart.model, test.data)#For fun, finally check your score of leaderboard
> sample <- read.csv(""SampleSubmission_TmnO39y.csv"")
> final.sub <- data.frame(Item_Identifier = sample$Item_Identifier, Outlet_Identifier = sample$Outlet_Identifier, Item_Outlet_Sales = rpart.prediction)
> write.csv(final.sub, ""pca.csv"",row.names = F)Thats the complete modeling process after PCA extraction. Im sure you wouldnt be happy with your leaderboard rank after you upload the solution. Try using random forest!For Python Users: To implement PCA in python, simply import PCA from sklearn library. The interpretation remains same as explained for R users above. Ofcourse, the result is some as derived after using R. The data set used for Python is a cleaned version where missing values have been imputed, and categorical variables are converted into numeric. The modeling process remains same, as explained for R users above.import numpy as np
 from sklearn.decomposition import PCA
 import pandas as pd
 import matplotlib.pyplot as plt
 from sklearn.preprocessing import scale
 %matplotlib inline#Load data set
 data = pd.read_csv('Big_Mart_PCA.csv')#convert it to numpy arrays
 X=data.values#Scaling the values
 X = scale(X)pca = PCA(n_components=44)pca.fit(X)#The amount of variance that each PC explains
 var= pca.explained_variance_ratio_#Cumulative Variance explains
 var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)print var1
 [ 10.37 17.68 23.92 29.7 34.7 39.28 43.67 46.53 49.27
 51.92 54.48 57.04 59.59 62.1 64.59 67.08 69.55 72.
 74.39 76.76 79.1 81.44 83.77 86.06 88.33 90.59 92.7
 94.76 96.78 98.44 100.01 100.01 100.01 100.01 100.01 100.01
 100.01 100.01 100.01 100.01 100.01 100.01 100.01 100.01]plt.plot(var1)#Looking at above plot I'm taking 30 variables
 pca = PCA(n_components=30)
 pca.fit(X)
 X1=pca.fit_transform(X)print X1For more information on PCA in python, visit scikit learn documentation.This brings me to the end of this tutorial. Without delving deep into mathematics, Ive tried to make you familiar with most important concepts required to use this technique. Its simple but needs special attention while deciding the number of components. Practically, we should strive to retain only first few k componentsThe idea behind pca is to construct some principal components( Z << Xp ) which satisfactorily explains most of the variability in the data, as well as relationship with the response variable.Did you like reading this article ? Did you understand this technique ? Do share your suggestions / opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
Winning Solutions of DYD Competition  R and XGBoost Ruled,Learn everything about Analytics|Introduction|The DYD competition|Winners of DYD Competition|Key Takeawaysfrom this Competition|End Notes,"Evaluation Metric|Rank 3  Sonny Laskar (Used ensemble of 2 XGBoost models in R )|Rank 2  Prarthana Bhat (Used ensemble of 50 XGBoost models in R)|Rank 1  Santanu Dutta ( UsedGBM in Python and Data Cleaning in R )|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Practical Guide to Principal Component Analysis (PCA) in R & Python|Statistical Analyst  Ahmedabad (2+ years of Experience)|
Analytics Vidhya Content Team
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Its all about an extra mile one is willing to walk!Winning a data science competition require 2 things: Persistence and Willingnessto try new things. There comes a moment of challenge in every competitionwhen participants feel that nothing seems to work their wayand its time to give up. Thats when aperson stands up and says, Why dont I try one more time,but this time ina different way? Thats when champions are born.Competitions organized at Data Hack are meant tochallenge yourskills & knowledgeandgive you a chanceto learn more and become a better analyst / data scientist.On a similar note, we organized Date Your DataCompetition from26th Feb 16 to 28th Feb 16.This competition enticed more than~ 2100 participants around the world. Unlike other dates (romantic ones), this date turned out to bedramatic. No signs of love were shown. Only fierceattempts to slice and dice the data withhighest level of granularity.The emerged winners (top 3) mainly used R and XGBoostto rule the leaderboard.Heres a complete solution (approach & codes) used by winners in this competition. Youll shortly see how feature engineering turned out to be a game changer in this competition.For R users, these solutions arehighly helpful and can be used as a practice material.Note: A special thanks to the winners of this competition for their immense co-operation and time.This competition surpassed our previous high of number of submissions. It recorded more than 3100 submissions. Also, we got our first female data scientist winner in this competition.This competition involved a supervised machine learning problem. Participants were required to predict the chances of a students profile to be of high relevance to employers. In simple words, the participants were required to predict whether a student will be shortlisted or not. The data set used was provided by Internshala, Indias No. 1 platform for internships.You can read the complete problem statement here: Link. The data set is available for download here. Please note that the data set is available for your practice purpose and will be accessible until 20th March 2016.The winners were judged on the basis of ROC score. ROC curve is a plot between sensitivity and (1-specificity). To know more, visit here. AUC score close to 1 is always desirable.After a live feedback session with participants held at slack, it was inferred that this competition was challenging and participants were keen to acknowledgewhat they missed!A common factor which played a crucial role in their victoryis their prolonged reverence forfeature engineering and data exploration. Boosting (XGBoost, GBM) imparted their models necessary accuracy. Ensemble modelingplayed a cameo in further enhancing their models accuracy.Since most of the coding has been done in R, this canbe a great resource to practice for R users.Sonny Laskar, currently works as a Manager  Strategy at Microland Limited. He says:Sonny says:Like everyone, I started with taking a close look at data. I call it as data discovery stage. Since there were 4 files, the chances of oversight were high. So, I realized that data has spelling mistakes. Later, I discovered some of the variables like internship profile, internship skills had good number of repetitive observations. It was evident that such observations row will dominate the prediction process.This impelled me to do one hot encoding of such variables and added them asseparate features. Later, I label encoded the binary features (0,1). In fact, majority of my time went in encoding features.But, this wasnt enough. I got a terrible score until here. Then, I created additional features with mean, percentages to supply more information to my model. It worked.I used caret package.I built 2 XGBoost models with different seed values and nrounds.Due to lack of time, I didnt do much experiment with machine learning. I then simply, ensembled my 2 XGBoost models.I think I could have achieved higher score, had I not removed duplicate rows from student experience. Im sure that lead to loss of information, but it was a race against time too. My final score was 0.700698.Link to CodePrarthana Bhat, currently works as a Data Scientist at Flutura Decision Science and Analytics. Shes the first female participant on Data Hack to secure a rank in Top 3.Prarthana says:When I looked at the data, I discerned thatfeature engineering willturn out tobe a game changer. Hence, right from the beginning I kept my focus on discovering new features.Of course, I started withthe basic hygienic steps of data cleaning. There was a lot of mix and match possible in this data set. Since the data was large, I used parallel computing in R for faster computation and also not to run out of patience. R has awesome libraries such as doParallel, doSNOW, foreach to do this job!I think the features I created were able to add significant information to the model. Thats the key to predictive modeling. One should always attempt to extract as much as information (uncorrelated) from available data.For modeling, I used XGBoost algorithm. I decided to test for its optimal potential on this data. Then, I did parameter tuning. I decided to stick with only 3 parameters namely eta, colsample_bytree, subsample. In fact, Id suggest R users to pay attention to these parameters the most for parameter tuning.Not to make it a repetitive process, I wrote some functions to do this job. This was time consuming. But, in the end, turned out to be worthy enough. My final score stood at 0.709808.Link to CodeSantanu Dutta, currently works as a Senior Associate in ACME. He is an experienced analytics professional specializing in BFSI and marketing. Hes a self learned data scientist.Santanu says:I had always been curious to know more about the science of data and how it can derive benefits in our daily lives.Since then I have been training myself to build good and stable predictive models by participating in hackathons.In this competition, the biggest challenge was shortage of time as the data set was quite hugeand dirty. Lots of data cleaning was supposed to be done before processing it to build models.An early cursory look on date variables, gave hint that pre-processing is going to be the real game changer.I have specialized myself in R. But, in last fewhackathons,I noticed that Python is quickly gearing upand is becoming the first love of hackathon winners. So, this time I promised myself to walk an extra mile. I used both R and Python to solve this problem (faster). I used R for data wrangling and Python for model building.Python was a real challenge for me. Because, in the last few months Ive badly struggled in implementing XGBoost on my windows machine. So, I selected the next best alternative i.e. GBM. In addition,I had built few variations of Random Forest, Boosting , Matrix Factorization models as well and relied on local CV to select the parameters and model.Its been a great privilege competing with leading data scientists across the globe. Learning while competing steepens the learning curve. My public lb score was 0.63 and ranking 17 and private lb score resulted in 0.72 which got me thefirst position.Link to CodeIn this competition, participants got the chance to work on real life data. Real life data comes in all shapes and dimensions. Hence, it becomes essential to develop business understanding in order to work better with data sets. In DYD, participants worked deeply with data exploration, data engineering and feature engineering techniques. Below are the key takeaways one can take home from this article:If you have thoroughly followed this article, you would have noticed that feature engineering and boosting areawfully important in winning competitions. So, the next time you would participate in a competition, make sure you dont miss out creating new features and rendersome boost. In fact, the process is simple: Clean the data, create new features, build the model, keep the best features, build the model again (boost) and done. If you have still been indecisive about, whether to learn R or Python, you can start with R from scratch.In this article, Ive shared the winning approach of top 3 winners of DYD Competition. These winners took home amazon vouchers worth INR 55k ( $800 ). For your practice, the data set is available for download until 20th March 2016. Make sure you make the most out of this opportunity.Did you like reading this article ? Did you discover what you missed in the competition?Do share your opinions / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/winning-solutions-dyd-competition-xgboost-ruled/
"Statistical Analyst  Ahmedabad (2+ years of Experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Winning Solutions of DYD Competition  R and XGBoost Ruled|Fundamentals of Deep Learning  Starting with Artificial Neural Network|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Statistical AnalystLocation  AhmedabadAbout employer ConfidentialJob description: Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.Responsibilities Qualification and Skills RequiredEducation & ExperienceInterested people can apply for this job can mail their CV to[emailprotected]with subject as Statistical Analyst  Ahmedabad",https://www.analyticsvidhya.com/blog/2016/03/statistical-analyst-ahmedabad-2-years-experience/
Fundamentals of Deep Learning  Starting with Artificial Neural Network,Learn everything about Analytics|Introduction|Table of Contents|1. What is a Neural Network?|2. How a Single Neuron works?|3. Why multi-layer networks are useful?|4. GeneralStructure of a Neural Network|5. Back-Propagation|End Notes,"Example 1: AND|Example 2: OR|Example 3:NOT|Case 1: X1 XNOR X2 = (A.B) + (A.B)|Case 2: X1 XNOR X2 =NOT [ (A+B).(A+B) ]|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Statistical Analyst  Ahmedabad (2+ years of Experience)|Senior Data Engineer  Hyderabad (2+ years of experience)|
Aarshay Jain
|42 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",a = f( -1.5 + x1 + x2 )|a = f( -0.5 + x1 + x2 )|a = f( 12*x1 )|a = f( 0.5x1  x2 )|d = a  b|e = d * c = (a-b)*c,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you know the first neural network was discovered in early 1950s ?Deep Learning (DL) and Neural Network (NN) is currently driving some of the mostingeniousinventions in todays century. Their incredible ability to learn from data and environment makes themthe first choice ofmachine learning scientists.Deep Learning and Neural Network lies in the heart of products such as self driving cars, image recognition software, recommender systems etc. Evidently, being a powerful algorithm, it ishighly adaptive to various data types as well.People think neural networkisan extremelydifficult topic to learn. Therefore, either some of them dont use it, or the ones who use it, use it as a black box. Is there any point in doing something without knowing how is it done? NO!In this article, Ive attempted to explain the concept of neural network in simple words. Understanding this article requires alittlebit of biology and lots of patience. By end of this article, you would become a confident analyst ready to start working with neural networks. In case you dont understand anything, Im always available in comments section.Note: This article is best suited for intermediate users in data science & machine learning. Beginners might find itchallenging.Neural Networks (NN), also called as Artificial Neural Network is named afteritsartificial representation of working of a human beings nervous system. Remember this diagram ? Most of us have beentaught in High School !Flashback Recap: Lets start by understanding how our nervous system works.Nervous Systemcomprises of millions of nerve cells or neurons. A neuron has the following structure:The majorcomponents are:In simple terms, each neuron takes input from numerous other neurons through the dendrites. It thenperforms the required processing on the input and sends another electrical pulse through the axiom into the terminal nodes from where it is transmitted to numerous other neurons.ANN works in a very similar fashion. The general structure of a neural network looks like:SourceThis figure depicts a typical neural network with working of a single neuron explained separately. Lets understand this.The input toeachneuron are like the dendrites. Just like inhuman nervous system, a neuron (artificial though!) collates all the inputs and performs an operation on them. Lastly, it transmits the output to all other neurons (of the next layer) to which it is connected. Neural Network is divided into layer of 3 types:Lets start by looking into the functionality of each neuron with examples.In this section, we will explore the working of a single neuron witheasy examples. The idea is to give you some intuition on how a neuron compute outputs using the inputs. A typical neuron looks like:The different components are:Here f is knownan activation function. This makes a Neural Network extremely flexible and imparts the capability to estimate complex non-linear relationships in data. It can be a gaussian function, logistic function, hyperbolic function or even a linear function in simple cases.Lets implement3 fundamental functions OR, AND, NOT using Neural Networks. This will help us understand how they work. You can assume these to be like a classification problem where well predict the output (0or1) for different combination of inputs.We will model these like linear classifiers with the following activation function:The AND function can be implemented as:The output of this neuron is:The truth table for this implementationis:Here we can see that the AND function is successfully implemented. Column a complies with X1 AND X2. Note that here the bias unit weight is -1.5. But its not a fixed value. Intuitively, we can understand it as anything which makes the total value positive only when both x1 and x2 are positive. So any value between (-1,-2) would work.The ORfunction can be implemented as:The output of this neuron is:The truth table for this implementation is:Column a complies with X1 OR X2.We can see that, just by changing the bias unit weight, we can implement an OR function. This is very similar to the one above. Intuitively, you can understand that here, the bias unit is such that the weighted sum will be positive if any of x1 or x2 becomes positive.Just like the previous cases, the NOTfunction can be implemented as:The output of this neuron is:The truth table for this implementation is:Again, the compliance with desired value proves functionality.I hope with these examples, youre getting some intuition into how a neuron inside a Neural Network works. Here I have used a very simple activation function.Note: Generally alogistic function will beused in place of what I used here because it is differentiable and makes determination of a gradient possible. Theres just 1 catch. And, that is, it outputs floating value and notexactly 0 or 1.After understanding the working of a single neuron, lets try to understand how a Neural Network can model complex relations using multiple layers. To understand this further, we will take the example of an XNOR function. Just a recap, the truth table of an XNOR function looks like:Here we can see that the output is 1 when both inputs are same, otherwise 0. This sort of a relationship cannot be modeled using a single neuron. (Dont believe me? Give it a try!) Thus we will use a multi-layer network. The idea behind using multiple layers is that complex relations can be broken into simplerfunctions and combined.Lets break down the XNOR function.Now we can implement it using any of the simplified cases. I will show you how to implement this using 2 cases.Here the challenge is todesign a neuron to model A.B . This can be easily modeled using the following:The output of this neuron is:The truth table for thisfunction is:Now that we have modeled the individual components and we can combine them using a multi-layer network. First, lets look at the semantic diagram of that network:Here we can see that in layer 1, we will determine A.B and A.B individually. In layer 2, we will take their output and implement an OR function on top. This would complete the entire Neural Network. The final network would look like this:If you notice carefully, this is nothing but a combination of the different neurons which we have already drawn. The different outputs represent different units:The functionality can be verified using the truth table:I think now you can get some intuition into how multi-layers work. Lets do another implementation of the same case.In the above example, we had to separately calculate A.B. What if we want to implement the function just using the basic AND, OR, NOT functions. Consider the following semantic:Here you can see that we had to use 3 hidden layers. The working will be similar to what we did before. The network looks like:Herethe neurons perform following actions:Note that, typically a neuron feeds into every other neuron of the next layer except the bias unit. In this case, Ive obviated few connections from layer 1 to layer 2. This is because their weights are 0 and adding them will make it visually cumbersome to grasp.The truth table is:Finally, we have successfully implemented XNOR function. This method is more complicated than case 1. Hence, you should prefer case 1 always. But the idea here is to show how complicated functions can be broken down in multiple layers. I hope the advantages of multiple layers are clearer now.Now that we had a look at some basic examples, lets define a generic structure in which every Neural Networkfalls. We will also see the equations to be followed to determine the output given an input. This is known asForward Propagation.A generic Neural Networkcan be defined as:It has L layers with 1 input layer, 1 output layer and L-2hidden layers.Terminology:Since the the output of each layer forms the input of next layer, lets define the equation to determine the output of i+1th layer using output of ith layer as input.The input to the i+1th layer are:The weights matrix from ith to i+1th layer is:The output of the i+1th layer can be calculated as:Using these equations for each subsequent layer, we can determine the final output. The number of neurons in the output layer will depend on the type of problem. It can be 1 for regression or binary classification problem or multiple for multi-class classification problems.But this is just determining the output from 1 run. The ultimate objective is to update the weights of the model in order to minimize the loss function. The weights are updated using a back-propogation algorithm which well study next.Back-propagation (BP) algorithms works by determining the loss (or error) at the output and then propagatingit back into the network. The weights are updated to minimize the error resulting from each neuron. I will not go in details of the algorithm but I will try to give you some intuition into how it works.The first step in minimizing the error is to determine the gradient of each node wrt. the final output. Since, it is a multi-layer network, determining the gradient is not very straightforward.Lets understand thegradients for multi-layer networks. Lets take a step back from neural networks and consider a very simple system as following:Here there are 3 inputs which simple processing as:Now we need to determine the gradients of a,b,c,d wrt the output e. The following cases are very straight forward:However, for determining the gradients for a and b, we need to apply the chain rule.And, this way thegradient can be computed by simply multiplying the gradient of the input to a node with that of the output of that node. If youre still confused, just read the equation carefully 5 times and youll get it!But, the actual cases are not that simple. Lets take another example. Consider a case where a single input is being fed into multiple items in the next layer as this is almost always the case with neural network.In this case, the gradients of all other will be very similar to the above example except for m because m is being fed into 2 nodes. Here, Ill show how to determine the gradient for m and rest you should calculate on your own.Here you can see that the gradient is simply the summation of the two different gradients. I hope the cloud cover is slowly vanishing and things are becoming lucid. Just understand these concepts and well come back to this.Before moving forward, lets sum up the entire process behind optimization of a neural network. The various steps involved in each iteration are:Till now we have covered #1  #3 and we have some intuition into #5. Now lets start from #4  #6. Well use the same generic structure of NN as described in section 4.#4- Find the errorHere y(i) is the actual outcome from training data#5- Back-propogating the error into the networkThe error for layer L-1 should be determined first using the following:wherei = 0,1,2, .., NL-1 (number of nodes in L-1th layer)Intuition from the concepts discussed in former half of this section:This process has to be repeated consecutively from L-1th layer to 2nd layer. Note that the first layer is just the inputs.#6- Update weights to minimize gradientwhere,I hope the convention is clear. I suggest you go through it multiple times and if still there are questions, Ill be happy to take them on through comments below.With this we have successfully understood how a neural networkworks. Please feel free to discuss further if needed.This article is focused on the fundamentals of a Neural Network and how it works. I hope now you understand the working of a neural network and wouldnt use it as a black box ever. Its really easy once you understand doing it practically as well.Therefore, in myupcoming article, Ill explain the applications of using Neural Network in Python. More than theoretical, Ill focus on practical aspect of Neural Network.Two applications come to my mind immediately:I hope you enjoyed this. I would love if you could share your feedback through comments below. Looking forward tointeracting with you further on this!",https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/
"Senior Data Engineer  Hyderabad (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Fundamentals of Deep Learning  Starting with Artificial Neural Network|What did you miss ? Complete Solution of Mini Hack Excel|
Jobs Admin
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Senior Data EngineerLocation  HyderabadAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Big Data Engineer  Hyderabad and the following details:,https://www.analyticsvidhya.com/blog/2016/03/senior-data-engineer-hyderabad-2-years-experience/
What did you miss ? Complete Solution of Mini Hack Excel,Learn everything about Analytics|Introduction|What is the problem ?|Table of Contents|1. Exploring Hidden Trends|2. Calculating Machine Productivity|3. Extrapolation and Assumptions|4. SummarizingAnswers|End Notes,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Senior Data Engineer  Hyderabad (2+ years of experience)|Python Developer  Bangalore (5+ years of experience)|
Analytics Vidhya Content Team
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Excel is a powerful and easy to use tool for data analysis. Often it is seen thatthe journey of a data analyst begins with MS Excel. When I started learning excel, I realized that its quite easy to learn. But, the difficult part is to understand when to use which command. Do you think the same ?We organized an exclusive competition namedMini Excel Hackon 5th March 2016. It was a 3 hour competition. More than 1300 participants registered for this competition. It was organized to help people learn using excel while by working on business problems. Of course, winners were rewarded with amazon vouchers worth INR 18000 ($265).The competitionwas challenging. After the competition ended, the participants were keen to know what did they miss. Hence, heres a complete solution to the business problem shared in Mini Hack Excel. This tutorial will make you proficient at using excel to find business solutions.Note: This solution requires using advanced excel methods, number crunching abilitiesand a logical approach. Prior knowledge of basic excel commands will be helpful. This solution has been provided by the client. There can be many solutions to this problem, this article revealsone of them.In short, the problem was to help Mr. Assurenaut, CEO of AssureNext with a decision on Robossurance. Robossurance is a newly created channel which consists of smart machines.These machines are installed in different location to studythe consumer purchase behaviour and recommends the best insurance plan.Mr. Assurenaut expected their sales to shoot up with this innovative product. At first, good things did happen. But, didnt sustain for long. Soon, their customer acquisition rate declined. Soon, the board members started raising questions on his decision.Based on this situation, the challenge was to answer 3 questions:Read the complete problem statementhere. Download the data set here.Lets quickly understand the information provided in the data set:Its important to understand every bit of information available in the given data. Refer to image of data fromSheet First below. The image shows business sourcing for 2 months. It explains that the machinesinstalledin the month of Januarycontributes to new policies of Jan, Feb and March. Similarly, machines installed in February contribute to new policies for Feb, March, April and so on.I hope now you have understood how these variables are related.Lets start solving now!Visualizing trend is important. Especially, when we have time series data. Ill startwith plotting simple line charts. Through this step, Ill look tofind answers for two questions:For your convenience, the solution is available for download.Letsstart withSheet Third. The idea is to discover monthly and yearly pattern in machines installed at AssureNext.Simply select the data in Sheet Third and press Alt + N + N (shortcut to activate line chart). Now, select a suitable line chart. It wouldlook like this:We can say that the company hasnt remainconsistent at installing new machines. The consistent up-down trend justifies that. However, the average trend of this plot does have a story to say. On an average, the plot shows increase in machines installed until Oct-09 and there after declines drastically. Then, it again rises and falls.For better understanding, lets create a monthly plot on this data. For this, we need to group data on monthly basis. This can be done easily.Grouping of data can be done using sumif() function. But, why to opt for difficult method when we have a simpler way? Yes! well use pivot tables. Pivot tables are immensely helpful ingrouping and summarizing data. They offerabsolutecontrolon the output with its easy to use interface.To activate pivot table, select the entire data in Sheet Third( A shortcut for selecting entire data is, Press Ctrl + Home followed by Ctrl + A ).Then, press Alt + N + V. Press Enter. This will open a new sheet which looks like this:Now, well plot a line chart which shows month wise trend of machines installed. For this, drag and drop the field Month in Rows and Machines Installed in Values (shown below).Move toany cell in the pivot table. Right click on the cell and click Group. Then, select Monthsand Press OK(Shown below).To plot the data, select any cell in the pivot table and press Alt+N+N. The plot appears like this.This is an important insight. This plotshows a seasonality pattern innumber of machines installed per month. It alsoshows that towards the end of year, company used to install more machines as compared to first half of the year. This can be corroborated with the table shown. The trend is highest in December where, on an average, the %increase is 133% followed by November at 116%. In fact, there are many ways to analyze a graph.Lets dig deeper. Now, Ill check yearly trend on this data.To check yearly trend, move toany cell in pivot table. Right click on the cell and click Group. Then, select Yearsand Press OK(Shown below).Press OK. And, the line chart will automatically convert into yearly chart.This chart also follows a pattern, but a different one than the previous. After every 2 years, there is a spike seen in number of machines installed. 2009 has a spike. Then, 2012 has a spike and so on. If we assume the same pace of growth, year 2016 and year 2017 should see a dip in the number of machines installed.Now, lets move to Sheet Second and explore the trend of average premium per year. This is simple. Just select the data and press Alt + N + N.This plotshows a consistent rise in average premiumfrom 2009 and gets nearly flattenas it reaches 2015.This can be due to various reasons. For example, there might have been a change in product distribution. Company might have started selling products which gives high premium.This is becoming interesting now!Move to Sheet First. This sheet has a lot to reveal. Lets do it one by one. Select the entire data and create a pivot table (keyboard shortcut: Alt + N + V).Lets analyze month wise new policies. For this, simply drag Business_Sourcing_Month in Rows and Total New Policies in Values. It would appear like this:Now plot this data. Click anywhere in pivot table and press Alt+N+N. The plot looks like this:This plot tells us that until 2012 there was a steep growth in number of new policies, but couldnt sustain after that. Also, a sharp decline after 2012 in seen with nearly flattenedin 2014-15. The sharp dip could be due to some prevailing regulatory changes or some internal strategy change.Lets analyze the same data on monthly basis. Simply follow the Group step as explained above. The plot comes out to be like this:This plot is important. This trend will surely influence our predicted new policies for2016-17. This plot shows that maximum newpolicies have come in month of March and declined drastically in April. Beyond April, new policies continue to sway until September from where an increase is visible.This trend actually complies with Indian insurance industry. March marks the end of financial year. Most of the people, seek to gain tax benefits by signing up for last minute insurance plans.Till here, weve gained significant insights on underlying trends of this data. These gave us enough information for making assumptions in upcoming sections. Lets movethe next part.Why am I calculating machine productivity ? Im sure many of you would have this questions. In simple terms, the value of productivity will help me to understand how efficient a machine remains after operating for so many years. Productivity, is nothing but average number of policies sold by a machine.Productivity can be calculated using this formula:Productivity = No. of Policies / No. of MachinesFrom here on, well be entering the ultra-calculation mode. Make sure you keep up with me. Well be working intensely with pivot tables, look up functions and other useful formulas.Lets start by creating a pivot table. Move to Sheet First. With this table, well get to know the number of new policies given business sourcing month and machines installed. Generally, policies are sold by humans and not machines. Humans have a tendency to become inefficientas they grow old. But, machines are more powerful and can work longer (with regular maintenance).So, does the machines installed at AssureNext lose productivity over the years? Well soon come to know.If youve been following this tutorial, I assume now youknow how to make a pivot. Select the data. Then Press Alt + N + V. Once you get used tothese shortcuts, youll realize excel is insanely fast working with shortcuts.A new sheet will open. Activate the pivot by clicking on pivot box. Now drag the following columns as shown. You output should look like this:
Table 1This table tells us that total business generatedin month of January (Business Sourcing Month) is only by machine installed in January. Total business generatedin February is by machines installed in month of both January and February. Similarly, total business generated in the month of March is by machines installed in the month of January, February and March.Then, why is there a diagonal pattern ? Its simply because the machine (yet to be installed) in Feb 09 cant bring business in Jan 09. Right ?In simple terms, ones future efforts cant bring results in present.Since, all this effort is invested to calculate productivity, well create a machine life cycle. Confused? Lets understand this. The data is available for 84 months (7 years). I wish to analyze the performance of a machine over these 84 months. Finally, this will lead me to productivity. Also, this will help me understand if a machine loses productivity over a period of 84 months or not.Lets do it.Create a table showing year & month(shown below). You can simply copy paste the pivot table from above and delete the data (shown below empty table). Row represents timeline of machines installed and columns represent timelines of new policies (new business ) arrived.Table 2Life cycle is nothing but,  for how many months has a machine contributed to business, from the day it got installed.It means, if we can find the difference between the month of machine installation and month of new policies, we can get the number of months a particular machine has remain at work.To find the difference between the month of machine installation and business sourcing, well simply write a function at cell C94: =IFERROR(DATEDIF($B94,C$93,""M""),"""")Press Enter and copy the formula to all cells. This gives us the difference between months.Table 2Now, Im interested in finding out total number of new policies,the company gotin month 0, 1, 2 and so on. This will help me in calculating new policies on yearly basis.To accomplish this, Ill use index & match function. Index match function is considered to be an alternative of vlookup function. But, works best we need to extract data from two different tables. Match function extracts the cell position, Index function extracts the data from that cell.Youll understand it better when you do it. Lets see.Create a table with years in a column and month index in a row (shown below).Table 3Now, well use this formula at cell C183: =IFERROR(INDEX($C$5:$CH$88,MATCH($B183,$B$5:$B$88,0),MATCH(C$182,$C94:$CH94,0)),"""")Copy this formula across the table and the final populated table looks like this:Table 3I think this formula deserves a quick explanation. The index function selects the table 2. Its row and column is specified by 2 match functions.First match function, looks up the date adjacent to table selected for index column. The column match look for months (0, 1, 2..) in the Table 2. The combination of these functions provide the number of new policies sold in 0 month, 1st month, 2nd month and so on.Now, lets find out the 84 months timeline of machine installed. This will help us understand the number of months (out of 84) been completed by a machine. The older the machine, lesser will it contribute to total business.Well start using Year column (shown in image above). You can extract year by using year() function.Using vlookup() function, we can extract the machines installed from sheet third. You can use the function at cell C274:=VLOOKUP($B274,Third!$A$2:$B$85,2,0)After the value is extracted, we can start building our time line. Such that, out of 84 months, machine installed will become zero only when a particular month has no business is done. A simple if statement does the trick.If statement says. if there are nopolicies (no business done) in a particular month, return zeroelse return the number of machines installed..Ive used the following formula:=IF(C183="""",0,C274)The output table looks like this (screenshot of bottom half):Table 4Till here, we have got enough monthly data. Lets consolidate it on yearly basis.Firstly, well find thenumber of machines on yearly basis. This can be achieved using Table 4 and sumif() command. Using:  =SUMIF($A$274:$A$357,$B362,C$274:C$357)The output comes as:Table 5Up til here, weve found out number of machines on yearly basis.Lets now find number of new policies on yearly basis. We are just one step away from productivity. To calculate number of new policies on yearly basis, follow the steps similar to previous calculation. For this calculation, you requireTable 3 and sumif() command.Using:=SUMIF($A$183:$A$266,$B373,C$183:C$266)The output should look like:Table 6Now, its time to calculate the productivity. Ive already disclosed its formula in the beginning of this section. The productivity (Number of Policies / Number of Machines) turns out to be:Table 7The highlighted rows in productivity table have been extrapolated, which we shall discuss in next section. Lets plot the productivity and check the trend:
This suggests that machines work wonderfully good only in initial 2-4 months. After that, their productivity starts deteriorating. Thus, these machines do lose efficiency over time. And, this is a dominating trendas it continues to prevailall 7 years. Out of 7 years, year 2012 achieved the highest productivity and year 2009 achieved the lowest. In fact, we can also see that a machine remains productive at a maximum of 34 months from its inception.Until here, we have seen enough trends and numbers to make robust assumptions. Lets proceed to the next section.Extrapolation is an art of making estimations beyond the range of available data.This is similar to regression, henceregression can also be used for solving this problem.Let understand the underlying assumptions taken for extrapolating the productivity. Precisely, Ive taken the following assumptions:Till here, we have the calculated the productivity for 2016 & 2017. If we can estimate the machines installed for 2016 and 2017, then we can easily find total number of policies . We are getting closer to our answers now!Create a new table of machines installed year wise. Ive considered the seasonality effect as seen in the plots above. Seasonality effect is captured by %monthly machines available in the table (shown below). Rest is basic arithmetic calculation.Ive assumed no. of machine installed as 5000 per year based on average derived from Sheet Third.Following are the calculations:Hold tight, we are moving towards the last stage of this solution.Amidst all calculation, the most important thingis productivity (on yearly basis). Yet, this productivity wouldnt help much unless we map it with every machine installed. Afterthis step, well come to know the productivity of every machine from 2016 to 2017. Once weve calculated this figure, we are free to make all sorts of estimations.The formula (image shown below) is quite long but easy. Let me explain it to you (Refer to the solution):The output table looks like this:Finally, by doing columnwise addition, we will get number of policies the company will get from Jan 2016 to Dec 2017. Remember, the effect of seasonality on new policies ? The data of last 7 years showed that number of policies surgedin the month of March followed by an almost sudden decline in April.It will be good practice to add the effect of seasonality to our predictions. This will make our solution more relatistic. sincethe company willoperate in the same market and industry in 2016 & 17.Monthly Seasonality can be calculated from Sheet First(shown below). The formula is:Monthly Seasonality = Sum of Total New Policies in a particular month / Average of all monthsNow, well add the effect of seasonality to number of policies obtained for 2016 and 2017. Lets plot its trend and check if we did right.This shows we have very well captured the entrenched seasonality in number of policies. We are almost done solving for answers.Lets see what do we have till now:Move to Simulation Sheet where you can find answers for all 3 questions.Answer 1: Total Premium forecasts the month wise new business in 2016 and 2017Answer 2: Using hit and trial method, make changes in number of machines installed. Try increasing it to 6000 or 7000. Simultaneously, note the change in Yearly Premium. Stop when you attain 170 crores yearly premium in 2017. This will be your answer. When I tried, I got 170 crores of business at 10800 machines.Answer 3: Update the value for machines installed and Average premium policy as mentioned in the question. Simultaneously, you can achieve quarterly premium too.With this, I come to the end of this tutorial and solution on advanced excel. If you have thoroughly followed till here, you would realize winning this competition was more about structured thinking than excel skills. I believe, its easily to perform tasks rather than thinking at what to do? Ive shared the link to download this complete solution above.Several participants tried at this competition, some succeeded and the rest learnt and got insanely curious. I enjoyed writing this tutorial. This isnt the only solution of this competition. As you can see, the assumptions are subjective. You too can make any practical assumption and get the answer. After all, business problems arent about right or wrong answer, but the thought process use behind it.Did you find this solution helpful ? Did you understand the things you missed or couldnt think of during competition? Share your suggestions / opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/complete-solution-mini-hack-excel/
"Python Developer  Bangalore (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|What did you miss ? Complete Solution of Mini Hack Excel|Data Scientist  Bangalore (6 to 8 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Python DeveloperLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asPython Developer  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/03/python-developer-bangalore-2-years-experience/
"Data Scientist  Bangalore (6 to 8 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Python Developer  Bangalore (5+ years of experience)|Business Analyst  Chennai (1.6+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation  BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/03/data-scientist-bangalore-6-8-years-experience/
"Business Analyst  Chennai (1.6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Bangalore (6 to 8 years of experience)|MIS Team Leader  Delhi (4+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business AnalystLocation  ChennaiAbout employer  ConfidentialDescriptionBusiness analysts must have excellent verbal & written communication skills and problem solving skills. He should be creative  with the ability to engage with customers to understand and respond to their needs in rapidly changing business environments.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Analyst  Chennai and the following details:,https://www.analyticsvidhya.com/blog/2016/03/business-analyst-chennai-1-6-years-experience/
"MIS Team Leader  Delhi (4+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Chennai (1.6+ years of experience)|MIS Executive  Delhi (1-3 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  MIS Team LeaderLocation  DelhiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as MIS Team Leader  Delhi and the following details:,https://www.analyticsvidhya.com/blog/2016/03/mis-team-leader-delhi-4-years-experience/
"MIS Executive  Delhi (1-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|MIS Team Leader  Delhi (4+ years of experience)|Complete Solution: How I got in Top 11% of Kaggle Telstra Competition ?|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  MIS ExecutiveLocation  DelhiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asMIS Executive  Delhi and the following details:,https://www.analyticsvidhya.com/blog/2016/03/mis-executive-delhi-1-3-years-experience/
Complete Solution: How I got in Top 11% of Kaggle Telstra Competition ?,Learn everything about Analytics|Introduction|Important Points|Table of Contents|1. Understanding theProblem andGeneratingHypothesis|2. Data Loading and Exploration|3. Data Preparation & Modelling|4. Feature Engineering & Attemptsto find the magic feature|5. Ensemble and StackingTechniques|5. Final Results and What I missed (and learnt)|End Notes,"Whats the Problem ?|File 1  train.csv (and test.csv)|File 2  event_type.csv|File 3  log_features.csv|File 4 resource.csv|Data Preparation|Modeling|Making thefinal submission|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|MIS Executive  Delhi (1-3 years of experience)|10 Questions R Users always ask while using ggplot2 package|
Aarshay Jain
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Handling Location,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Telstra is Australias largest telecommunications network. Telstra Network Disruptions(TND) Competition ended on 29th February 2016. This was a recruiting competition.At Analytics Vidhya,Ive been experimenting with several machine learning algorithms from past2 months. It was my time to testthe waters.I tried several algorithms and approaches in this competition. And, I ended up securing 106 Rank out of 974 Participants.Though I missed the benchmark of a rank in Top 10% participants, I feel satisfied after learning several tips, best practices and approaches from several Kaggle masters via discussion forums.If there is one take away from the competition, it is this  Predictive Modeling is not just about using advanced machine learning algorithms, but more about data exploration and feature engineering.Here is my complete solution for this competition. I used XGBoost and my own MLlibrary. I used Python in this competition. You can learn Python using this complete tutorial.The idea behind this step is to think broadly about the problem WITHOUT looking atthe data. This helps us to think about the problem without getting biased.Telstra wants help. The problem is to predict the severity of service disruptions on their network. In other words, we are supposed to predict if a disruption occurred is a momentary glitch or total interruption of connectivity. To make this easy, a data set of service log is provided.This will help Telsra to enhance customer experience by providing better connectivity. The accurate predictions of service disruptions will help Telstra serve customers better.For this problem, I further searched on Telstra.We know that its the largest telecommunication service providers inmobile phones, broadband, landline as well as digital TV. Also, we know the problem.Lets try to think about parameters from a users perspective which can potentially influence this factor:These are just a fewexamples to get you thinking and you can perform hypothesis further. Lets look at the data now and see what we have got!Data set is available for download here. Lets have a good look at the data and summarize the key points:Lets have a look at the individual files and derive some insights.Generally, I tend to combine both the train and test filesfor analysis by adding a source feature which keeps record of where the observation belongs. After combination, the data looks like:Columns:Keyobservations:Key inferences:Data Dimension: 31170 (rows), 2 (columns)Data Snapshot:Initial Observations:Inferences:Data Dimension:58671 (rows), 3 (columns)Data Snapshot:Initial observations:Inferences:Data Dimension: 21076(rows), 2(columns)Data Snapshot:Initial Observations:Inferences:File 5 severity_type.csvData Dimension: 18552(rows), 2(columns)Data Snapshot:Initial observations:Inferences:Lets move topreparing this data for first analysis.The first step was to clean and map the given data and prepare test and train files. The idea is not do something new but simply use the available information and make a model which will act as the baseline to test further modifications.The data preparation step involves making features from different information files and mapping them back to the train and test files. The codes can be found in the data_preparation_1iPython notebook from my GitHub repository.I adopted the following approach for event_type, resource_type and log_features file, all of which had multiple occurrencesfor each id:The overall idea is to keep categories which occur a certain minimum number of times and club the rest. This is doneto ensure that the rules are made on certain minimum number of samples. The same can also be enforced using model parameters. But, taking all the unique entities would give ~450 features and models would run really slow.Lets take an example of event_type.I created a dataframe of unique entities which looked like:First column is the count, then the percentage in train, then mode of fault_severity and then the final decision taken. This data is sorted by count and you can see that the higher counts are all kept as it is in preprocess column.If we look at the lower end of this table:Here, you find the low count entities have been combined into event_type others_0/1/2. Also, the ones only present in test file are removed. Finally sparse features are made by mappingthis table intooriginal data.Similar steps have been performed for other files as well. Some points to note:Though location should ideally be converted to sparse features, but it has too many unique values1126 to be precise.Sofollowing steps were taken:This data was used to make the first model using xgboost. I used xgboost here directly because one of my aim during this competition was to learn xgboost. Also, I started learning it around 10 days before end of competition and I didnt getmuch time to experiment. I couldnt have got a better opportunity!Ituned a xgboost model with the data set. I could see that the numeric coding of location was actually working as location was the most significant variable in my first model:The final model build was using the following parameters:The CV score was:It scored0.50710 on the public leaderboard. I was stillin the bottom 50% with this score and there was a lot to improve upon.After making my first submission, I noticed something strange. The leaderboard had apeculiartrend. The top ~15 people had a score 0.42-0.44 and below that it was all above 0.48. There was a abrupt change from0.44 to0.48 with nothing in between. This was a clear indication of leakage!If youre wondering what is leakage, it is nothing but some information which was not supposed to be present but got overlookedby mistake. It can be of various kinds, for instance:Type 1 is easy to findbecause the algorithm will automatically detect it for us. However, Type 2 is really hard to find and requires some serious exploration. In this case, it was definitely type 2 for two reasons. One, the algorithm didnt detect something very different and two, we were expecting some time information in the data which was not given. So there are high chances that people found some time-trend in the data.I tried various things on my file:The intuition for #4 and #6 came from a thesis report which I read on analyzing telecommunication networks. The pattern features came out to be important in the feature importance chart but there was no significant probably. Apparantly, xgboost was already learning about such patterns without those features.When all efforts were in vain, I created some other features which gave slight improvements in performance:You can have a look at the various things I tried in thefeature_engineering_1/2/3 iPython notebook in my GitHub repository.I was still in rangeof0.5 mark with these efforts. Towards the end, I started thinking about creating ensemble models. The most basic form is to create 10 different models with the same parameters and different seeds and average their results. I was surprised to see that my performance suddenly improved to ~0.499 with this kind of ensembling. I did this for all my good performing models and averaged them again. This also lead to some slight improvement.Then I read about stacking technique. Stacking is nothing but using the output of the model as input and then running again. But there is a catch here, you should always use out-of-fold predictions, else there will be serious over-fitting. The typical process I followed is:I tried first by stacking xgboost with another xgboost, i.e. the model made on each half was an xgboost and the model on the predicted outcomes was also an xgboost. This gave slight improvement.Then I tried making different models in intermediate step. I made random forest and extra-trees classifier model with ginni and entropy losses. Then, I used the xgboost model to stack their outcome. This gave me a good boost to ~0.498.You might be wondering that so much effort for 0.499 to 0.498! But yes this is what Kaggle competitions are all about  extracting every bit of performance possible. And it gave me a good 50-100 position boost. Also, these models are more robust and the chances of performing good on private leaderboard are high.Now I had to choose my model for final submission. While reading on how to do it, Kaggle masters suggested to determine a combination of CV and leaderboard score. Something like this:(#observations in train)*CV-score + (#observations on public)*Public-LB-scoreAnother trick I tried here was to make ensemble of all of my good models, i.e. ones ~0.498 mark. The ones which I created with ensemble to 10 models of different seeds. I was surprised to get a jump from 0.498 to 0.493 using this. I was really starting to understand the power of ensemble and how robust can different models be. I finally submitted this model and the next best. Without any surprises, the top model performed best and gave me a private leaderboard score of 0.485.This can be found in the ensemble_models iPython notebook at the GitHub repository.My final rank was 106 out of 974 participants, which is ~11% from top. I was sad that I missed the top 10% mark but was still satisfying as this was my first attempt at Kaggle. I was still very curious to see what that magic feature was which I couldnt spot. I was praying that it shouldnt turn out to be something very simple which I missed. You know, it hurts a lot.It was exactly what I didnt expect. I was looking at the wrong place. I mapped information from all the additional files into the train and test file. All my attempts were in this while the trend was safely hidden in those additional files. Those files were sorted by location and then by time. Just creating a simple index out of it gave a performance of 0.42! I was kicking myself for missing out on that, but still learnt a lot.A very interesting and informative blog has been written by the Daria Vasyukova (aka dune_dweller) who ranked 31 in the competition.Apart from this, I have hand-picked some interesting featuresmade by top performers fromKaggle forums and summarized them here for you:You can read further about the approaches fromKaggle forum.In this article, I described my approach in arecent Kaggle competition  Telstra Network Disruption, where the type of disruption had to be predicted. The competition was a good one and required some out-of-the-box thinking more than predictive modeling. Though I didnt do wonders, but it was a good learning experience. Im sure this will help you in making some informed decisions in your upcoming data science competitions.Did you like the methods shared here? Did you also participate and used some other ideas not mentioned here? Please feel free to drop a note in the comments and Ill be glad to discuss.",https://www.analyticsvidhya.com/blog/2016/03/complete-solution-top-11-telstra-network-disruptions-kaggle-competition/
10 Questions R Users always ask while using ggplot2 package,Learn everything about Analytics|Introduction|Table of Contents|Getting started|Q1. How to create a Scatter Plot ?|Q2 . How to create a Histogram ?|Q3. How to create a Bar Chart ?|Q4. How to create a Stack Bar Chart ?|Q5. How to create a Box Plot?|Q6. How to create an Area Chart ?|Q7. How to create a heat map ?|Q8. How to create a Correlogram ?|Q9. How to plot a geographical map ?|Q10. How to plot a data set in single command?|End Notes,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Complete Solution: How I got in Top 11% of Kaggle Telstra Competition ?|Tutorial on 5 Powerful R Packages used for imputing missing values|
Analytics Vidhya Content Team
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Sometimes numbers do have a beautiful story to share!Visualizing data is crucial in todays world. Without powerful visualizations, it is almost impossible to create and narrate data based stories on humongous data. These stories help us build strategies and make intelligent business decisions.R is well supportedto make data visualization easier and fun.Its already equipped withbase functions and the external support rendered by packages makes it just awesome tool to work. Thanks to our community members.Among all packages,ggplot package has become a synonym for data visualization in R. A package which allows you to get more control on charts, graphs and maps, is also known to create breathtaking graphics. I would like to sincerely thank Hadley Wickam, the father of ggplot2 package for this accomplishment.In this article, Ive answered some of the most commonly asked questions by R users while working with ggplot package. So, next time when you need to visualize data, you can pick any of the shown below.Note: This article is best suited forbeginners and intermediate R users having basic knowledge of data visualization. You can refer to this complete data visualization guide.Note: ICC T20 Cricket WorldCup is starting from 8th March 2016. Which countries are participating this year ? Well seeto it through map visualization.Lets quickly get over with pre-visualization rituals.Data Set: In this article, I have used the data set from Big Mart Sales Prediction. Data is available for download here.> path <- "".../desktop/Data/Big Mart""
> setwd(path)#Load Data
> train <- read.csv(""Train_UWu5bXk.csv"")#Look at data
> str(train)#check variable classes
 >sapply(train, class)
 Item_Identifier    Item_Weight
 ""factor""      ""numeric""
 Item_Fat_Content   Item_Visibility
 ""factor""      ""numeric""
 Item_Type      Item_MRP
 ""factor""      ""numeric""
 Outlet_Identifier  Outlet_Establishment_Year
 ""factor""       ""integer""
 Outlet_Size    Outlet_Location_Type
 ""factor""       ""factor""
 Outlet_Type     Item_Outlet_Sales
 ""factor""       ""numeric""#Install and Load Library
> install.packages(""ggplot2"")
> library(ggplot2)We are good to start now. Do keep a check on variable classes. This will help you to decide the type of plot best suited for them.When to use: Scatter Plot is used when want to see the relationship between two continuous variables.> ggplot(train, aes(Item_Visibility, Item_MRP)) + geom_point() + 
    scale_x_continuous(""Item Visibility"", breaks = seq(0,0.35,0.05))+
    scale_y_continuous(""Item MRP"", breaks = seq(0,270,by = 30))+
    theme_bw()Lets quickly understand the structure of ggplot code:We can also add a categorical variable (Item_Type) in the current plot. Do check the data to get familiar with the available in the data set.> ggplot(train, aes(Item_Visibility, Item_MRP)) + geom_point(aes(color = Item_Type)) + 
        scale_x_continuous(""Item Visibility"", breaks = seq(0,0.35,0.05))+
        scale_y_continuous(""Item MRP"", breaks = seq(0,270,by = 30))+
        theme_bw() + labs(title=""Scatterplot"")We can even make it better by creating separate scatter plot for separate Item_Type.> ggplot(train, aes(Item_Visibility, Item_MRP)) + geom_point(aes(color = Item_Type)) + 
        scale_x_continuous(""Item Visibility"", breaks = seq(0,0.35,0.05))+
        scale_y_continuous(""Item MRP"", breaks = seq(0,270,by = 30))+
        theme_bw() + labs(title=""Scatterplot"") + facet_wrap( ~ Item_Type)At your end, you need to zoom this graph for a clear view. The zoomed version looks like this. In this case, the parameter facet_wrap does the trick. It wraps facet in rectangular layout.When to use: Histogram is used when we want to plot one continuous variable.> ggplot(train, aes(Item_MRP)) + geom_histogram(binwidth = 2)+
        scale_x_continuous(""Item MRP"", breaks = seq(0,270,by = 30))+
        scale_y_continuous(""Count"", breaks = seq(0,200,by = 20))+
labs(title = ""Histogram"")When to use: Bar Chart is used when we want to plot a categorical variable or a combination of continuous and categorical variable.#Bar chart with one variable
> ggplot(train, aes(Outlet_Establishment_Year)) + geom_bar(fill = ""red"")+theme_bw()+
        scale_x_continuous(""Establishment Year"", breaks = seq(1985,2010)) + 
        scale_y_continuous(""Count"", breaks = seq(0,1500,150)) +
        coord_flip()+ labs(title = ""Bar Chart"") + theme_gray()
You can remove coord_flip() parameter to get this bar chart vertically. As you can see, I tried a different theme for this plot. You are always welcome to become experimental while using ggplot package.#Bar Chart with 2 variables
> ggplot(train, aes(Item_Type, Item_Weight)) + geom_bar(stat = ""identity"", fill = ""darkblue"") +scale_x_discrete(""Outlet Type"")
+scale_y_continuous(""Item Weight"", breaks = seq(0,15000, by = 500))
    + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
    + labs(title = ""Bar Chart"")You can zoom this graph at your end for a better visual. In this graph, I used categorical vs continuous variable on x and y axis respectively.When to use: Its an advanced version of a Bar Chart. It used when we wish to visualize a combination of categorical variables.> ggplot(train, aes(Outlet_Location_Type, fill = Outlet_Type)) + geom_bar()+
    labs(title = ""Stacked Bar Chart"", x = ""Outlet Location Type"", y = ""Count of Outlets"")When to use: Box Plots are used to plot a combination of categorical and continuous variables. This plot helps us to identify data distribution and detect outliers.> ggplot(train, aes(Outlet_Identifier, Item_Outlet_Sales)) + geom_boxplot(fill = ""red"")+
        scale_y_continuous(""Item Outlet Sales"", breaks= seq(0,15000, by=500))+
        labs(title = ""Box Plot"", x = ""Outlet Identifier"")The black points are outliers. Outlier detection and removal is an essential step of successful data exploration. Learn more about Outlier Detection from this guide.When to use: Area chart is used to show continuity across a variable or data set. Its quite similar to a line chart. It is commonly used for time series plots. Alternatively, it is used to plot continuous variables and analyze the underlying trends.> ggplot(train, aes(Item_Outlet_Sales)) + geom_area(stat = ""bin"", bins = 30, fill =     ""steelblue"") +
    scale_x_continuous(breaks = seq(0,11000,1000))+
    labs(title = ""Area Chart"", x = ""Item Outlet Sales"", y = ""Count"")When to use:Heat Map uses intensity (density) of colors to display relationship between two or three or many variables in a two dimensional image.> ggplot(train, aes(Outlet_Identifier, Item_Type))+
       geom_raster(aes(fill = Item_MRP))+
       labs(title =""Heat Map"", x = ""Outlet Identifier"", y = ""Item Type"")+
       scale_fill_continuous(name = ""Item MRP"")You can zoom this plot at your end for a better visual. The dark portion indicates Item MRP is close 50. The brighter portion indicates Item MRP is close to 250.Heat Maps can also produce visuals used forimage recognition. This can be done by adding a parameter asinterpolate = TRUE.> ggplot(train, aes(Outlet_Identifier, Item_Type))+
        geom_raster(aes(fill = Item_MRP), interpolate = T)+
        labs(title =""Heat Map"", x = ""Outlet Identifier"", y = ""Item Type"")+
        scale_fill_continuous(name = ""Item MRP"")When to use: Correlogram is used to test the level of correlation among the variable available in the data set.To create a correlogram, Ive used corrgram package instead of ggplot. I realized creating correlogram using its dedicated package is much easier than using ggplot.> install.packages(""corrgram"")
> library(corrgram)
> corrgram(train, order=NULL,
     panel=panel.shade, text.panel=panel.txt,
     main=""Correlogram"")Its quite easy to interpret too. Darker the color, higher the correlation between variables. Blue indicates positive correlation. Red indicates negative correlation. Color intensity indicates the magnitude of correlation.When to use: Maps are commonly used to visualize certain factor been influenced geographically. Its easy to plot maps in R.Lets plot the countries participating in ICC World T20 World Cup 2016. After I did my research, I found there are 16 countries participating this year. Lets see where these countries are located on the world map.Well use ggmaps package along with ggplot for creating these maps.#List of Countries
> ICC_WC_T20 <- c(""Australia"",
         ""WestIndies"",
         ""India"",
         ""SriLanka"",
         ""Pakistan"",
         ""Bangladesh"",
        ""NewZealand"",
         ""SouthAfrica"",
         ""England"",
         ""HongKong"",
        ""Zimbabwe"",
        ""Afghanistan"",
         ""Scotland"",
         ""Netherlands"",
         ""Ireland"",
         ""Oman"")#extract geo location of these countries
> countries <- geocode(ICC_WC_T20)#map longitude and latitude in separate variables
> nation.x <- countries$lon
> nation.y <- countries$lat#using ggplot
#plot the world map
> mapWorld <- borders(""world"", colour=""grey"", fill=""lightblue"")#add data points to the world map
> ggplot() + mapWorld + geom_point(aes(x=visit.x, y=visit.y) ,color=""red"", size=3)
It was easy. Isnt it? We can still embellish this map. If you are not familiar with world map, it would be difficult for you to identify countries name. Lets use functions from ggmap() package and redesign this map.#using ggmaps extract world map
> world_map <- qmap(""World"", zoom = 2)#see how map looks
> world_map#plot the data on map
> world_map + geom_point(aes(x=nation.x, y=nation.y),
       data = countries, alpha = 0.5,
size = 3, color = ""red"")This looks better. ggmap() package is linked with google map and hence extracts location detail directly. But I have one regret. If you carefully watch this map, youll realize this map is incomplete. West Indies isnt shown on this map. I tried extracting data from multiple source, but couldnt succeed in this matter. If any of you happens to solve this riddle, do share your solution.Everyone of us tries doing this step at some point in time. We all look for one command using which we can plot all variables in the data set at once. Heres your answer.You can use tabplot() package to accomplish this feat.#plotdata
> install.packages(""tabplot"")
> library(tabplot)
> tableplot(train)This end of a colorful journey! I hope it enables people to starts several new colorful journeys. You might have noticed that using ggplot2 is a lot easier. Most of the codes are repetitive, hence you will quickly get used to it. You need to be careful about picking up geoms for the charts, because thats the main design element. When I started learning this package, I asked all these questions at different point in time. Hence, the idea to line up all questions in one article came to me.In this article, I discussed 9 types of different visualizations which can be plotted using ggplot package. These visualizations are best used depending on the type of variable supplied to them. Hence, you must be careful about the type of variable you wish to plot.Did you find this article helpful ? Do you use any other package for visualization? Do share your suggestions / opinions in the comments below.",https://www.analyticsvidhya.com/blog/2016/03/questions-ggplot2-package-r/
Tutorial on 5 Powerful R Packages used for imputing missing values,Learn everything about Analytics|Overview|Introduction|List of R Packages|MICE Package|Amelia|missForest|Hmisc|mi|End Notes,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|10 Questions R Users always ask while using ggplot2 package|Data Visualizer  Gurgaon (1+ years of experience)|
Analytics Vidhya Content Team
|48 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

 4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Missing values are considered to be the first obstacle in predictive modeling. Hence, its important to master the methods to overcome them.Though, some machine learning algorithmsclaim to treat them intrinsically, but who knows how good it happens inside the black box.The choice ofmethod to impute missing values, largely influences the models predictive ability.In most statistical analysis methods, listwise deletion is the default method used to impute missing values. But, it not as good since it leads to information loss.Do you know R has robust packages for missing value imputations?Yes!R Users have something to cheer about. We are endowed with some incredible R packages for missing values imputation. These packages arrive with some inbuilt functions and a simple syntax to impute missing data at once. Some packages are known best working with continuous variables and others for categorical. With this article, you can make a better decision choose the best suited package.In this article, Ive listed 5 R packagespopularly knownfor missing value imputation. There might be more packages. But, I decided to focus on these ones. Ive tried to explain the concepts in simplistic manner with practice examples in R.Tutorial on 5 Powerful R Packages used for imputing missing valuesMICE (Multivariate Imputation via Chained Equations) is one of the commonly used package by R users. Creating multiple imputations as compared to a single imputation (such as mean) takes care of uncertainty in missing values.MICEassumes that the missing data are Missing at Random (MAR), which means that the probability that a value is missing depends only on observed value and can be predicted using them. It imputes data on a variable by variable basis by specifying an imputation model per variable.For example: Suppose we have X1, X2.Xk variables. If X1 has missing values, then it will be regressed on other variables X2 to Xk. The missing values in X1 will be then replaced by predictive values obtained. Similarly, if X2 has missing values, then X1, X3 to Xk variables will be used in prediction model as independent variables. Later, missing values will be replaced with predicted values.By default, linear regression is used to predict continuous missing values. Logistic regression is used for categorical missing values. Once this cycle is complete, multiple data sets are generated. These data sets differ only in imputed missing values. Generally, its considered to be a good practice to build models on these data sets separately and combining their results.Precisely, the methods used by this package are:Lets understand it practically now.> path <- ""../Data/Tutorial""
> setwd(path)#load data
> data <- iris#Get summary
> summary(iris)Since, MICE assumes missing at random values. Lets seed missing values in our data set using prodNA function. You can access this function by installing missForest package.#Generate 10% missing values at Random 
> iris.mis <- prodNA(iris, noNA = 0.1)#Check missing values introduced in the data
> summary(iris.mis)Ive removed categorical variable. Lets here focus on continuous values. To treat categorical variable, simply encode the levels and follow the procedure below.#remove categorical variables
> iris.mis <- subset(iris.mis, select = -c(Species))
> summary(iris.mis)#install MICE
> install.packages(""mice"")
> library(mice)mice package has a function known as md.pattern(). It returns a tabular form of missing value present in each variable in a data set.> md.pattern(iris.mis)Lets understand this table. There are 98 observations with no missing values. There are 10 observations with missing values in Sepal.Length. Similarly, there are 13 missing values with Sepal.Width and so on.This looks ugly. Right ? We can also create a visual which represents missing values.It looks pretty cool too. Lets check it out.> install.packages(""VIM"")
> library(VIM)
> mice_plot <- aggr(iris.mis, col=c('navyblue','yellow'),
          numbers=TRUE, sortVars=TRUE,
          labels=names(iris.mis), cex.axis=.7,
          gap=3, ylab=c(""Missing data"",""Pattern""))Lets quickly understand this. There are 67% values in the data set with no missing value. There are 10% missing values in Petal.Length, 8% missing values in Petal.Width and so on. You can also look at histogram which clearly depicts the influence of missing values in the variables.Now, lets impute the missing values.> imputed_Data <- mice(iris.mis, m=5, maxit = 50, method = 'pmm', seed = 500)
> summary(imputed_Data)Multiply imputed data set
Call:
mice(data = iris.mis, m = 5, method = ""pmm"", maxit = 50, seed = 500)
Number of multiple imputations: 5
Missing cells per column:
Sepal.Length Sepal.Width Petal.Length Petal.Width 
 13      14     16      15 
Imputation methods:
Sepal.Length Sepal.Width Petal.Length Petal.Width 
 ""pmm""    ""pmm""    ""pmm""    ""pmm"" 
VisitSequence:
Sepal.Length Sepal.Width Petal.Length Petal.Width 
 1       2      3      4 
PredictorMatrix:
       Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length    0     1      1      1
Sepal.Width     1     0      1      1
Petal.Length    1     1      0      1
Petal.Width     1     1      1      0
Random generator seed value: 500 Here is an explanation of the parameters used:#check imputed values
> imputed_Data$imp$Sepal.WidthSince there are 5 imputed data sets, you can select any using complete() function.#get complete data ( 2nd out of 5)
> completeData <- complete(imputed_Data,2)Also, if you wish to build models on all 5 datasets, you can do it in one go using with() command. You can also combine the result from these models and obtain a consolidated output using pool() command.#build predictive model
> fit <- with(data = iris.mis, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width))#combine results of all 5 models
> combine <- pool(fit)
> summary(combine)Please note that Ive used the command above just for demonstration purpose. You can replace the variable values at your end and try it.This package (Amelia II) is named after Amelia Earhart,the first female aviator to fly solo across the Atlantic Ocean. History says, she got mysteriously disappeared (missing) while flying over the pacific ocean in 1937, hence this package was named to solve missing value problems.This package also performs multiple imputation (generate imputed data sets) to deal with missing values. Multiple imputation helps toreduce bias and increase efficiency.It is enabled with bootstrap based EMB algorithm which makes it faster and robust to impute many variables including cross sectional, time series data etc. Also, it is enabled with parallel imputation feature using multicore CPUs.It makes the following assumptions:It works this way. First, it takes m bootstrap samples and applies EMB algorithm to each sample. The m estimates of mean and variances will be different. Finally, the first set of estimates are used to impute first set of missing values using regression, then second set of estimates are used for second set and so on.On comparing with MICE, MVN lags on some crucial aspects such as:Hence, this package works best when data has multivariable normal distribution. If not, transformation is to be done to bring data close to normality.Lets understand it practically now.#install package and load library
 > install.packages(""Amelia"")
 > library(Amelia)#load data
 > data(""iris"")The only thing that you need to be careful about isclassifying variables. It has 3 parameters:#seed 10% missing values
> iris.mis <- prodNA(iris, noNA = 0.1)
> summary(iris.mis)#specify columns and run amelia
> amelia_fit <- amelia(iris.mis, m=5, parallel = ""multicore"", noms = ""Species"")#access imputed outputs
> amelia_fit$imputations[[1]]
> amelia_fit$imputations[[2]]
> amelia_fit$imputations[[3]]
> amelia_fit$imputations[[4]]
> amelia_fit$imputations[[5]]To check a particular column in a data set, use the following commands>amelia_fit$imputations[[5]]$Sepal.Length#export the outputs to csvfiles
> write.amelia(amelia_fit, file.stem = ""imputed_data_set"")As the name suggests, missForest is an implementation of random forest algorithm. Its a non parametric imputation method applicable to various variable types. So, whats a non parametric method ?Non-parametric method does not make explicit assumptions about functional form of f (any arbitary function). Instead, it tries to estimate f such that it can be as close to the data points without seeming impractical.How does it work ? In simple words, it builds a random forest model for each variable. Then it uses the model to predict missing values in the variable with the help of observed values.It yield OOB (out of bag)imputation error estimate. Moreover, it provides high level of control on imputation process.It has options to return OOB separately (for each variable) instead of aggregating over the whole data matrix. This helps to look more closely as to how accurately the model has imputed values for each variable.Lets understand it practically. Since bagging works well on categorical variable too, we dont need to remove them here. It very well takes care of missing value pertaining to their variable types:#missForest
> install.packages(""missForest"")
> library(missForest)#load data
> data(""iris"")#seed 10% missing values
> iris.mis <- prodNA(iris, noNA = 0.1)
> summary(iris.mis)#impute missing values, using all parameters as default values
> iris.imp <- missForest(iris.mis)#check imputed values
> iris.imp$ximp#check imputation error
> iris.imp$OOBerrorNRMSE     PFC 
0.14148554 0.02985075NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values. PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.#comparing actual data accuracy
> iris.err <- mixError(iris.imp$ximp, iris.mis, iris)
>iris.err NRMSE   PFC 
0.1535103 0.0625000 This suggests that categorical variables are imputed with 6% error and continuous variables are imputed with 15% error. This can be improved by tuning the values ofmtry and ntree parameter. mtry refers to the number of variables being randomly sampled at each split. ntree refers to number of trees to grow in the forest.Hmisc is a multiple purpose package useful for data analysis, high  level graphics, imputing missing values, advanced table making,model fitting & diagnostics (linear regression, logistic regression & cox regression) etc. Amidst, the wide range of functions contained inthis package, it offers2 powerful functions for imputing missing values. These are impute() and aregImpute(). Though, it also has transcan() function, but aregImpute() is better to use.impute() function simply imputes missing value using user defined statistical method (mean, max, mean). Its default is median. On the other hand, aregImpute() allows mean imputation using additive regression, bootstrapping, and predictive mean matching.In bootstrapping, different bootstrap resamples are used for each of multiple imputations. Then, a flexible additive model (non parametric regression method) is fitted on samples taken with replacements from original data and missing values (acts as dependent variable) are predicted using non-missing values (independent variable).Then, ituses predictive mean matching (default) to impute missing values. Predictive mean matching works well for continuous and categorical (binary & multi-level) without the need for computing residuals and maximum likelihood fit.Here are some important highlights of this package:Lets understand it practically.#install package and load library
> install.packages(""Hmisc"")
> library(Hmisc)#load data
> data(""iris"")#seed missing values ( 10% )
> iris.mis <- prodNA(iris, noNA = 0.1)
> summary(iris.mis)# impute with mean value
> iris.mis$imputed_age <- with(iris.mis, impute(Sepal.Length, mean))# impute with random value
> iris.mis$imputed_age2 <- with(iris.mis, impute(Sepal.Length, 'random'))#similarly you can use min, max, median to impute missing value#using argImpute
> impute_arg <- aregImpute(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width +
Species, data = iris.mis, n.impute = 5)argImpute() automatically identifies the variable type and treats them accordingly.> impute_argThe output shows R values for predicted missing values. Higher the value, better are the values predicted. You can also check imputed values using the following command#check imputed variable Sepal.Length
> impute_arg$imputed$Sepal.Lengthmi (Multiple imputation with diagnostics) package provides several features for dealing with missing values. Like other packages, it also buildsmultiple imputation models to approximate missing values. And, uses predictive mean matching method.Though, Ive already explained predictive mean matching (pmm) above, but if you havent understood yet, heres a simpler version: For each observation in a variable with missing value, we find observation (from available values) with the closest predictive mean to that variable. The observed value from this match is then used as imputed value.Beloware some uniquecharacteristics of this package:Lets understand it practically.#install package and load library
> install.packages(""mi"")
> library(mi)#load data
> data(""iris"")#seed missing values ( 10% )
> iris.mis <- prodNA(iris, noNA = 0.1)
> summary(iris.mis)#imputing missing value with mi
> mi_data <- mi(iris.mis, seed = 335)Ive used default values of parameters namely:> summary(mi_data)Here is a snapshot o summary output by mi package after imputing missing values. As shown, it uses summary statistics to define the imputed values.So, which is the best of these 5 packages ? I am sure many of you would be asking this! Having created this tutorial, I felt Hmisc should be your first choice of missing value imputation followed by missForest and MICE.Hmisc automatically recognizes the variables types and uses bootstrap sample and predictive mean matching to impute missing values. You dont need to separate or treat categorical variable, just like we did while using MICE package. However, missForest can outperform Hmisc if the observed variables supplied contain sufficient information.In this article, I explain using 5 different R packages for missing value imputation. Such advanced methods can help you score better accuracy in building predictive models.Did you find this article useful ? Which package do you generally use to impute missing values ? Do share your experience / suggestions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/
Data Visualizer  Gurgaon (1+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Tutorial on 5 Powerful R Packages used for imputing missing values|Graph DB  Gurgaon (1+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data VisualizerLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Visualizer  Gurgaon and the following details:If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2016/03/data-visualizer-gurgaon-5-7-years-experience/
Graph DB  Gurgaon (1+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Visualizer  Gurgaon (1+ years of experience)|Data Scientist (3+ years experience)  New Delhi, India|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Graph DBLocation  GurgaonAbout employer  ConfidentialDescriptionWe are creating a niche machine learning based problem solving platform targeted at investment decision making.The platform uses big data architecture with semantic computing to process structured & unstructured data.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asGraph DB  Gurgaon and the following details:If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",https://www.analyticsvidhya.com/blog/2016/03/graph-db-gurgaon-5-7-years-experience/
"Data Scientist (3+ years experience)  New Delhi, India",Learn everything about Analytics,"If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook|Share this:|Like this:|Related Articles|Graph DB  Gurgaon (1+ years of experience)|Complete Guide to Parameter Tuning in XGBoost with codes in Python|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  New Delhi, IndiaAbout employer  ConfidentialDescriptionData Science team is responsible enhance informed decision making based on valuable insights generated from datato business clientsusing various data mining, predictive analytics and machine learning techniques. Team works closely with business executives across segments and geographies to understand the data closely. This also helps to understandbusiness objectives and helps to design analytical solutions to meet business goals.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  New Delhi, India and the following details:",https://www.analyticsvidhya.com/blog/2016/03/data-scientist-3-years-experience-delhi-india/
Complete Guide to Parameter Tuning in XGBoost with codes in Python,Learn everything about Analytics|Overview|Introduction|What should you know?|Project to apply XGBoost|Problem Statement|Table of Contents|1.The XGBoost Advantage|2. XGBoostParameters|3. Parameter Tuning with Example|End Notes,"General Parameters|Booster Parameters|Learning Task Parameters|General Approach for Parameter Tuning|Step 1: Fix learning rate and number of estimators for tuning tree-based parameters|Step 2: Tune max_depth and min_child_weight|Step 3: Tune gamma|Step 4: Tune subsample and colsample_bytree|Step 5: Tuning Regularization Parameters|Step 6: Reducing Learning Rate|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|Data Scientist (3+ years experience)  New Delhi, India|A Complete Tutorial to learn Data Science in R from Scratch|
Aarshay Jain
|99 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ifthings dont go your way in predictive modeling, use XGboost. XGBoost algorithm has become the ultimate weapon of many data scientist. Its ahighly sophisticated algorithm, powerful enough to deal with all sorts of irregularities of data.Building a model using XGBoost is easy. But, improving the model using XGBoost is difficult (at least I struggled a lot). This algorithm uses multiple parameters. To improve the model, parameter tuning is must. It is very difficult to get answers to practical questions like  Which set of parameters you should tune ? What is the ideal value of these parameters to obtain optimal output ?This article is best suited to people who are new to XGBoost. In this article, well learn the art of parameter tuning along with some useful information about XGBoost. Also, well practice this algorithm using a data setin Python.XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Since I covered Gradient Boosting Machine in detail in my previous article Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python, I highly recommend going through that before reading further. It will help you bolster your understanding of boosting in general and parameter tuning for GBM.Special Thanks: Personally, I would like to acknowledge the timeless support provided by Mr. Sudalai Rajkumar(aka SRK), currentlyAV Rank 2. This article wouldnt be possible without his help.He is helping us guide thousands of data scientists. A big thanks to SRK!HR analytics is revolutionizing the way human resources departments operate, leading to higher efficiency and better results overall. Human resources have been using analytics for years.However, the collection, processing, and analysis of data have been largely manual, and given the nature of human resources dynamics and HR KPIs, the approach has been constraining HR. Therefore, it is surprising that HR departments woke up to the utility of machine learning so late in the game. Here is an opportunity to try predictive analytics in identifying the employees most likely to get promoted.Practice NowIve always admired the boosting capabilities that this algorithm infuses in a predictive model. When I explored more about its performance and science behind its high accuracy, I discovered many advantages:I hope now you understand the sheer power XGBoost algorithm. Note that these are the points which I could muster. You know a few more? Feel free to dropa comment below and I will update the list.Did I whet your appetite ? Good.You can refer to following web-pages for a deeper understanding:The overall parameters have beendivided into 3 categories by XGBoost authors:I will give analogies to GBM here and highly recommend to read this articleto learn from the very basics.These define the overall functionality of XGBoost.There are 2 more parameters which are set automatically by XGBoost and you need not worry about them. Lets move on to Booster parameters.Thoughthere are 2 types of boosters, Ill consider onlytree boosterhere because it always outperforms the linear booster and thus the later is rarely used.These parameters are used to define the optimization objective the metric to be calculated at each step.If youve been using Scikit-Learn till now, these parameter names might not look familiar. A good news is that xgboost module in python has an sklearn wrapper called XGBClassifier. It uses sklearn style naming convention. The parameters names which will change are:You must be wondering that we have defined everything except something similar to the n_estimators parameter in GBM. Well this exists as a parameter in XGBClassifier. However, it has to be passed as num_boosting_rounds while calling the fit function in the standard xgboost implementation.I recommend you to go through the following parts of xgboost guide to better understand the parameters and codes:We will take the data set from Data Hackathon 3.x AV hackathon, same as that taken in the GBM article. The details of the problem can be found on the competition page. You can download the data set from here. I have performed the following steps:For those who have the original data from competition, you can check out these steps from the data_preparationiPython notebook in the repository.Lets start by importing the required libraries and loading the data:Note that I have imported 2 forms of XGBoost:Before proceeding further, lets define a function which will help us create XGBoostmodels and perform cross-validation. The best part is that you can take this function as it is and use it later for your own models.This code is slightly different from what I used for GBM. The focus of this article is to cover the concepts and not coding.Please feel free to drop a note in the comments if you find any challenges in understanding any part of it. Note that xgboosts sklearn wrapper doesnt have a feature_importances metric but a get_fscore() function which does the same job.We will use anapproach similar to that of GBM here. The various steps to beperformed are:Let us look at a more detailed step by step approach.In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:Please note that all the above are just initial estimates and will be tuned later. Lets take the default learning rate of 0.1 here and check the optimum number of trees using cv function of xgboost. The function defined above will do it for us.As you can see that here we got 140as the optimal estimators for 0.1 learning rate. Note that this value might be too high for you depending on the power of your system. In that case you can increase the learning rate and re-run the command to get the reduced number of estimators.Note: You willsee the test AUC as AUC Score (Test) in theoutputs here. But this would not appear if you try to run the command on your system as the data is not made public. Its provided here just for reference. The part of the code which generates this output has been removed here.We tune these first as they will have the highest impact on model outcome.To start with, lets set wider ranges and then we will perform anotheriteration for smaller ranges.Important Note: Ill be doing some heavy-duty grid searched in this section which can take 15-30 mins or even more time to run depending on your system. You can vary the number of values you are testing based on what your system can handle.Here, we have run 12combinations with wider intervals between values. The ideal values are 5for max_depth and 5for min_child_weight. Lets go one step deeper and look for optimum values. Well search for values 1 above and below the optimum values because we took an interval of two.Here, we get the optimum values as 4for max_depth and 6 for min_child_weight. Also, we can see the CV score increasing slightly. Note that as the model performance increases, it becomes exponentially difficult to achieve even marginal gains in performance. You would have noticed that here we got 6 as optimumvalue for min_child_weight but we havent tried values more than 6. We can do that as follow:.We see 6 as the optimal value.Now lets tune gamma value using the parameters already tuned above. Gammacan take various values but Ill check for 5 values here. You can go into more precise values as.This shows that our original value of gamma, i.e. 0 is the optimum one. Before proceeding, a good idea would be to re-calibrate the number of boosting rounds for the updated parameters.Here, we can see the improvement in score. So the final parameters are:The next step would be try different subsample and colsample_bytree values. Lets do this in 2 stages as well and take values 0.6,0.7,0.8,0.9 for both to start with.Here, we found 0.8 as the optimum value for both subsample and colsample_bytree.Now we should try values in 0.05 interval around these.Again we got the same values as before. Thus the optimum values are:Next step is to apply regularization toreduce overfitting. Though many people dont use this parameters much as gamma provides a substantial way of controlling complexity. But we should always try it. Ill tune reg_alpha value here and leave it upto you to try different values of reg_lambda.We can see thatthe CV score is less than the previous case. But thevalues tried arevery widespread, weshould try values closer to the optimum here (0.01) to see if we get something better.You can see that we got a better CV. Now we can apply this regularization in the model and look at the impact:Again we can see slight improvement in the score.Lastly, we should lower the learning rate and add more trees. Lets use thecv function of XGBoost to do the job again.Here is a live coding window where you can try different parameters and test the results.Now we can see a significant boost in performance and the effect of parameter tuning is clearer.As we come to the end, I would like to share2 key thoughts:You can also download the iPython notebook with all these model codes from my GitHub account. For codes in R, you can refer to this article.This article was based on developing a XGBoostmodelend-to-end. We started with discussing why XGBoost has superior performance over GBMwhich was followed by detailed discussion on the various parameters involved. We also defined a generic function which you can re-use for making models.Finally, we discussed the general approach towards tackling a problem with XGBoostand also worked outthe AV Data Hackathon 3.x problem through that approach.I hope you found this useful and now you feel more confident toapply XGBoostin solving adata science problem. You can try this out in out upcoming hackathons.Did you like this article? Would you like to share some otherhacks which you implement while making XGBoostmodels? Please feel free to drop a note in the comments below and Ill be glad to discuss.",https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
A Complete Tutorial to learn Data Science in R from Scratch,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Basics of R Programming|2. Essentials ofR Programming|3. Exploratory Data Analysis in R|4. Data Manipulation in R|5. Predictive Modeling using Machine Learning|End Notes,"Why learn R ?|How to install R / R Studio ?|How to install R Packages ?|Basic Computations in R|Data Types in R|Control Structures in R|Useful R Packages|Graphical Representation of Variables|Trouble with Continuous Variables & Categorical Variables|Label Encoding and One Hot Encoding|Linear (Multiple)Regression|Decision Trees|Random Forest|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Complete Guide to Parameter Tuning in XGBoost with codes in Python|Guide to Build Better Predictive Models using Segmentation|
Analytics Vidhya Content Team
|117 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R is a powerful language used widely for data analysis and statistical computing. It was developed in early 90s. Since then, endless efforts have been made to improve Rs user interface. The journey of R language from a rudimentary text editor to interactiveR Studio and more recentlyJupyter Notebooks has engaged many data science communities across the world.This was possible only because of generous contributions by R users globally. Inclusion of powerful packages in R has made it more and more powerful with time. Packages such as dplyr, tidyr, readr, data.table, SparkR, ggplot2 have made data manipulation, visualization and computation much faster.But, what about Machine Learning?My first impression of R was that its just a software for statistical computing. Good thing, I was wrong! R has enough provisions to implement machine learning algorithms in a fast and simplemanner.Thisis a complete tutorial to learn data science and machine learning using R.By the end of this tutorial, you will have a good exposure to building predictive models using machine learning on your own.Note: No prior knowledge of data science / analytics is required. However, prior knowledge of algebra and statistics will be helpful.A Complete Tutorial to learn Data Science in R from Scratch
Lets get started !Note: The data set used in this article is from Big Mart Sales Prediction.I dont know if I have a solid reason to convince you, but let me share what got me started. I have no prior coding experience. Actually, I never had computer science inmy subjects. I came toknow that to learn data science, onemust learn either R or Python as a starter. I chose the former. Here are some benefits I found after using R:There are many more benefits. But, these are the ones which have kept me going. If you think they are exciting, stick around and move to next section. And, if you arent convinced, you may like Complete Python Tutorial from Scratch.You could download and install the old version of R. But, Id insist you to start with RStudio. It provides much better coding experience. For Windows users, R Studio is available for Windows Vista and above versions. Follow the steps below for installing R Studio:Lets quickly understand the interface of R Studio:The sheer power of R lies in its incredible packages. In R, most data handling tasks can be performed in 2 ways: Using R packages and R base functions. In this tutorial, Ill also introduce you with themost handyand powerful R packages. To install a package, simply type:install.packages(""package name"")As a first time user, a pop mightappear to select your CRAN mirror (country server), choose accordingly and press OK.Note: You can type this either in console directly and press Enter or in R script and click Run.Lets begin with basics. To get familiar with R coding environment, start with some basic calculations. R console can be used as an interactive calculator too. Type the following in your console:> 2 + 3
> 5> 6 / 3
> 2> (3*8)/(2*3)
> 4> log(12)
> 1.07> sqrt (121)
> 11Similarly, you can experiment various combinations of calculations and get the results. In case, you want to obtain the previous calculation, this can be done in two ways. First, click in R console, and press Up / Down Arrow key on your keyboard. This will activate the previously executed commands. Press Enter.But, what if you have done too many calculations ? It would be too painful to scroll through every command and find it out. In such situations, creating variable is a helpful way.In R, you can create a variable using <- or = sign. Lets say I want to create a variable x to compute the sum of 7 and 8. Ill write it as:> x <- 8 + 7
> x
> 15Once we create a variable, you no longer get the output directly (like calculator), unless you call the variable in the next line. Remember, variables can be alphabets, alphanumeric but not numeric. You cant create numeric variables.Understand and practice this section thoroughly. This is the building block of your R programming knowledge. If you get this right, you would face less trouble in debugging.R has five basic or atomic classes of objects. Wait, what is an object ?Everything you see or create in R is an object. A vector, matrix, data frame, even a variable is an object. R treats it that way. So, R has 5 basic classesof objects. This includes:Since these classes are self-explanatory by names, I wouldnt elaborate on that. These classes have attributes. Think of attributes as their identifier, a name or number which aptly identifies them. An object can have following attributes:Attributes of an object can be accessed using attributes() function. More on this coming in following section.Lets understand the concept of object and attributes practically. The most basic object in R is known as vector. You can create an empty vector using vector(). Remember, a vector contains object of same class.For example: Lets create vectors of different classes. We can create vector using c()or concatenate command also.> a <- c(1.8, 4.5)  #numeric
> b <- c(1 + 2i, 3 - 6i) #complex
> d <- c(23, 44)  #integer
> e <- vector(""logical"", length = 5)Similarly, you can create vector of various classes.R has various type of data types whichincludes vector (numeric, integer etc), matrices, data frames and list. Lets understand them one by one.Vector: As mentioned above, a vector contains object of same class. But, you can mix objects of different classes too.When objects of different classes are mixed in a list, coercion occurs. This effect causes the objects of different types to convert into one class. For example:> qt <- c(""Time"", 24, ""October"", TRUE, 3.33) #character
> ab <- c(TRUE, 24) #numeric
> cd <- c(2.5, ""May"") #character
To check the class of any object, use class(vector name) function.> class(qt)
""character""To convert the class of a vector, you can use as. command.> bar<- 0:5
> class(bar)
> ""integer""
> as.numeric(bar)
> class(bar)
> ""numeric""
> as.character(bar)
> class(bar)
> ""character""Similarly, you can change the class of any vector. But, you should pay attention here. If you try to convert a character vector to numeric , NAs will be introduced. Hence, you should be careful to use this command.List: A list is a special type of vector which contain elements of different data types. For example:> my_list <- list(22, ""ab"", TRUE, 1 + 2i)
> my_list[[1]]
[1] 22[[2]]
[1] ""ab""[[3]]
[1] TRUE[[4]]
[1] 1+2iAs you can see, the output of a list is different from a vector. This is because, all the objects are of different types. The double bracket [[1]] shows the index of first element and so on. Hence, you can easily extract the element of lists depending on their index. Like this:> my_list[[3]]
> [1] TRUEYou can use [] single bracket too. But, that would return the list element with its index number, instead of the result above. Like this:> my_list[3]
>[[1]]
 [1] TRUEMatrices: When a vector is introduced with row and columni.e. a dimension attribute, it becomes a matrix. A matrix is represented by set of rows and columns. It is a 2 dimensional data structure. It consist of elements of same class. Lets create a matrix of 3 rows and 2 columns:> my_matrix <- matrix(1:6, nrow=3, ncol=2)
> my_matrix
[,1] [,2]
[1,] 1 4
[2,] 2 5
[3,] 3 6> dim(my_matrix)
[1] 3 2> attributes(my_matrix)
$dim
[1] 3 2As you can see, the dimensions of a matrix can be obtained using either dim()or attributes() command. To extract a particular element from a matrix, simply use the index shown above. For example(try this at your end):> my_matrix[,2]  #extracts second column
> my_matrix[,1]  #extracts first column
> my_matrix[2,]  #extracts secondrow
> my_matrix[1,]  #extractsfirst rowAs an interesting fact, you can also create a matrix from a vector. All you need to do is, assign dimension dim() later. Like this:> age <- c(23, 44, 15, 12, 31, 16)
> age
[1] 23 44 15 12 31 16

> dim(age) <- c(2,3)
> age
[,1] [,2] [,3]
[1,] 23 15 31
[2,] 44 12 16> class(age)
[1] ""matrix""You can also join two vectors using cbind() and rbind() functions. But, make sure that both vectors have same number of elements. If not, it will return NA values.> x<- c(1, 2, 3, 4, 5, 6)
> y<- c(20, 30, 40, 50, 60)
> cbind(x, y)
> cbind(x, y)
x  y
[1,] 1 20
[2,] 2 30
[3,] 3 40
[4,] 4 50
[5,] 5 60
[6,] 6 70> class(cbind(x, y))
[1] matrixData Frame: This is the most commonly usedmember of data types family. It is used to store tabular data. It is different from matrix. In a matrix, every element must have same class. But, in a data frame, you can put list of vectors containing different classes. This means, every column of a data frame acts like a list. Every time you will readdata in R, it will be stored in the form of a data frame. Hence, it is important to understand the majorly used commands on data frame:> df <- data.frame(name = c(""ash"",""jane"",""paul"",""mark""), score = c(67,56,87,91))
> df
name score
1 ash 67
2 jane 56
3 paul 87
4 mark 91

> dim(df)
[1] 4 2

> str(df)
'data.frame': 4 obs. of 2 variables:
$ name : Factor w/ 4 levels ""ash"",""jane"",""mark"",..: 1 2 4 3
$ score: num 67 56 87 91

> nrow(df)
[1] 4

> ncol(df)
[1] 2Lets understand the code above. df is the name of data frame. dim() returns the dimension of data frame as 4 rows and 2 columns. str() returns the structure of a data frame i.e. the list of variables stored in the data frame. nrow() and ncol() return the number of rows and number of columns in a data set respectively.Here you see name is a factor variable and score is numeric.In data science, a variable can be categorized into two types: Continuous and Categorical.Continuous variables are those which can take any form such as 1, 2, 3.5, 4.66 etc. Categorical variables are those which takes only discrete values such as 2, 5, 11, 15 etc. In R, categorical values are represented by factors. In df, name is a factor variable having 4 unique levels. Factor or categorical variable are specially treated in a data set. For more explanation,clickhere. Similarly, you can find techniques to deal withcontinuous variables here.Lets now understand the concept of missing values in R. This is one of the most painful yet crucial part of predictive modeling. You must be aware of all techniques to deal with them. Thecomplete explanation onsuch techniques is provided here.Missing values in R are represented by NA and NaN. Now well check if a data set has missing values (using the same data frame df).> df[1:2,2] <- NA #injecting NA at 1st, 2nd row and 2nd column of df
> df
name score
1 ash NA
2 jane NA
3 paul 87
4 mark 91

> is.na(df) #checks the entire data set for NAs and return logical output
name score
[1,] FALSE TRUE
[2,] FALSE TRUE
[3,] FALSE FALSE
[4,] FALSE FALSE
> table(is.na(df)) #returns a table of logical output
FALSE TRUE 
6   2> df[!complete.cases(df),] #returns the list of rows having missing values
name score
1 ash NA
2 jane NAMissing values hinder normal calculations in a data set. For example, lets say, we want to compute the mean of score. Since there are two missing values, it cant be done directly. Lets see:mean(df$score)
[1] NA
> mean(df$score, na.rm = TRUE)
[1] 89The use of na.rm = TRUEparameter tells R to ignore the NAs and compute the mean of remaining values in the selected column (score). To remove rows with NA values in a data frame, you can use na.omit:> new_df <- na.omit(df)
> new_df
name score
3 paul 87
4 mark 91As the name suggest, a control structure controls the flow of code / commands written inside a function. A function is a set of multiple commands written to automate a repetitive coding task.For example: You have 10 data sets. You want to find the mean of Age column present in every data set. This can be done in 2 ways: either you write the code to compute mean 10 times or you simply create a function and pass the data set to it.Lets understandthe control structures in R with simpleexamples:if, else  This structure is used to test a condition. Below is the syntax:if (<condition>){
    ##do something
} else {
    ##do something
}Example#initialize a variable
N <- 10#check if this variable * 5 is > 40
if (N * 5 > 40){
   print(""This is easy!"")
} else {
   print (""It's not easy!"")
}
[1] ""This is easy!""for  This structure is used when a loop is to be executed fixed number of times. It is commonly used for iterating over the elements of an object (list, vector). Below is the syntax:for (<search condition>){
     #do something
}Example#initialize a vector
y <- c(99,45,34,65,76,23)#print the first 4 numbers of this vector
for(i in 1:4){
  print (y[i])
}
[1] 99
[1] 45
[1] 34
[1] 65
while  It begins by testing a condition, and executes only if the conditionis found to be true. Once the loop is executed, the condition is tested again. Hence, its necessary to alter the condition such that the loop doesnt go infinity. Below is the syntax:#initialize a condition
Age <- 12#check if age is less than 17
while(Age < 17){
    print(Age)
    Age <- Age + 1 #Once the loop is executed, this code breaks the loop
}
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
There are other control structures as well but are less frequently used than explained above. Those structures are:Note: If you find the section control structures difficult to understand, not to worry. R is supported by various packages to compliment the work done by control structures.Out of ~7800 packages listed on CRAN, Ive listed some of the most powerful and commonly used packages in predictive modeling in this article. Since, Ive already explained the method of installing packages, you can go ahead and install them now. Sooner or later youll need them.Importing Data:R offers wide range of packages for importing data available in any format such as .txt, .csv, .json, .sql etc. To import large files of data quickly, it is advisable to install and use data.table, readr, RMySQL, sqldf, jsonlite.Data Visualization: R has in built plotting commands as well. They are good to create simple graphs. But, becomes complex when it comes to creating advanced graphics. Hence, you should install ggplot2.Data Manipulation: R has afantastic collection of packages for data manipulation. These packages allows you to do basic & advanced computations quickly. These packages are dplyr, plyr, tidyr, lubridate, stringr. Check out this complete tutorial on data manipulation packages in R.Modeling / Machine Learning:For modeling, caret package in R is powerful enough to cater to every need for creating machine learning model. However, you can install packages algorithms wise such as randomForest, rpart, gbm etcNote: Ive only mentioned the commonly used packages. You might like to check this interesting infographic on complete list of useful R packages.Till here, you becamefamiliar with the basic work style in R and its associated components. From next section, well begin with predictive modeling. But before you proceed. I want you to practice, what youve learnt till here.Practice Assignment: As a part of this assignment, install swirl package in package. Then type, library(swirl) to initiate the package.And, complete this interactive R tutorial. If you have followed this article thoroughly, this assignment should be an easy task for you!From this section onwards, well dive deep into various stages of predictive modeling. Hence,make sure you understand every aspect of this section. In case you find anything difficult to understand, ask me in the comments section below.Data Exploration is a crucial stage of predictive model. You cant build great and practical models unless you learn to explore the data from begin to end. This stage forms a concrete foundation for data manipulation (the very next stage). Lets understand it in R.In this tutorial, Ive taken the data set from Big Mart Sales Prediction. Before we start, you mustget familiar with these terms:Response Variable (a.k.a DependentVariable): In a data set, the response variable (y) is one on which we make predictions. In this case, well predict Item_Outlet_Sales. (Refer to image shown below)Predictor Variable (a.k.a Independent Variable): In a data set, predictor variables (Xi)are those using which the prediction is made on response variable. (Image below).Train Data: The predictive model is always built on train data set. An intuitive way to identify the train data is, that it always has the response variable included.Test Data: Once the model is built, its accuracy is tested on test data. This data always contains less number of observations than train data set. Also, it does not include response variable.Right now, you should download the data set. Take a good look at train and test data. Cross check the information shared above and then proceed.Lets now begin with importing and exploring data.#working directory
path <- "".../Data/BigMartSales""

#set working directory
setwd(path)As a beginner, Ill advise you to keep the train and test files in your working directly to avoid unnecessary directory troubles. Once the directory is set, we can easilyimport the .csv files using commands below.#Load Datasets
 train <- read.csv(""Train_UWu5bXk.csv"")
 test <- read.csv(""Test_u94Q5KV.csv"")In fact, even prior to loading data in R, its a good practice to look at the data in Excel. This helps in strategizing the complete prediction modeling process. To check if the data set has been loaded successfully, look at R environment. The data can be seen there. Lets explore the data quickly.#check dimesions ( number of row & columns) in data set
> dim(train)
[1] 8523 12

> dim(test)
[1] 5681 11We have 8523 rows and 12 columns in train data set and 5681 rows and 11 columns in data set. This makes sense. Test data should always have one column less (mentioned above right?). Lets get deeper in train data set now.#check the variables and their types in train
> str(train)
'data.frame': 8523 obs. of 12 variables:
$ Item_Identifier : Factor w/ 1559 levels ""DRA12"",""DRA24"",..: 157 9 663 1122 1298 759 697 739 441 991 ...
$ Item_Weight : num 9.3 5.92 17.5 19.2 8.93 ...
$ Item_Fat_Content : Factor w/ 5 levels ""LF"",""low fat"",..: 3 5 3 5 3 5 5 3 5 5 ...
$ Item_Visibility : num 0.016 0.0193 0.0168 0 0 ...
$ Item_Type : Factor w/ 16 levels ""Baking Goods"",..: 5 15 11 7 10 1 14 14 6 6 ...
$ Item_MRP : num 249.8 48.3 141.6 182.1 53.9 ...
$ Outlet_Identifier : Factor w/ 10 levels ""OUT010"",""OUT013"",..: 10 4 10 1 2 4 2 6 8 3 ...
$ Outlet_Establishment_Year: int 1999 2009 1999 1998 1987 2009 1987 1985 2002 2007 ...
$ Outlet_Size : Factor w/ 4 levels """",""High"",""Medium"",..: 3 3 3 1 2 3 2 3 1 1 ...
$ Outlet_Location_Type : Factor w/ 3 levels ""Tier 1"",""Tier 2"",..: 1 3 1 3 3 3 3 3 2 2 ...
$ Outlet_Type : Factor w/ 4 levels ""Grocery Store"",..: 2 3 2 1 2 3 2 4 2 2 ...
$ Item_Outlet_Sales : num 3735 443 2097 732 995 ...Lets do some quick data exploration.To begin with, Ill first check if this data has missing values. This can be done by using:> table(is.na(train))FALSE TRUE 
100813 1463In train data set, we have 1463 missing values. Lets check the variables in which these values are missing. Its important to find and locate these missing values. Many data scientists have repeatedly advised beginners to pay close attention to missing value in data exploration stages.> colSums(is.na(train))
Item_Identifier Item_Weight 
0        1463 
Item_Fat_Content Item_Visibility 
0         0 
Item_Type     Item_MRP 
0         0 
Outlet_Identifier Outlet_Establishment_Year 
0         0 
Outlet_Size    Outlet_Location_Type 
0         0 
Outlet_Type    Item_Outlet_Sales 
0         0Hence, we see that column Item_Weight has 1463 missing values. Lets getmore inferences from this data.> summary(train)Here are some quick inferences drawn from variables in train data set:These inference will help us in treating these variable more accurately.Im sure you would understand these variables better when explained visually. Using graphs, we can analyze the data in 2 ways: Univariate Analysis and Bivariate Analysis.Univariate analysis is done with one variable. Bivariate analysis is done with two variables. Univariate analysis is a lot easy to do. Hence, Ill skip that part here. Id recommend you to try it at your end. Lets now experiment doing bivariate analysis and carve out hidden insights.For visualization, Ill use ggplot2 package. These graphs would help us understand the distribution and frequency of variables in the data set.> ggplot(train, aes(x= Item_Visibility, y = Item_Outlet_Sales)) + geom_point(size = 2.5, color=""navy"") + xlab(""Item Visibility"") + ylab(""Item Outlet Sales"") + ggtitle(""Item Visibility vs Item Outlet Sales"")We can see that majority of sales has been obtained from products having visibility less than 0.2. This suggests that item_visibility < 2 must be an important factor in determining sales. Lets plot few more interesting graphs and explore such hidden stories.> ggplot(train, aes(Outlet_Identifier, Item_Outlet_Sales)) + geom_bar(stat = ""identity"", color = ""purple"") +theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = ""black"")) + ggtitle(""Outlets vs Total Sales"") + theme_bw()Here, we infer that OUT027 has contributed to majority of sales followed by OUT35. OUT10 and OUT19 have probably the least footfall, thereby contributing to the least outlet sales.> ggplot(train, aes(Item_Type, Item_Outlet_Sales)) + geom_bar( stat = ""identity"") +theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = ""navy"")) + xlab(""Item Type"") + ylab(""Item Outlet Sales"")+ggtitle(""Item Type vs Sales"")From this graph, we can infer that Fruits and Vegetables contribute to the highest amount of outlet sales followed by snack foods and household products. This information can also be represented using a box plot chart. The benefit of using a box plot is, you get to see the outlier and mean deviation of corresponding levels of a variable (shown below).> ggplot(train, aes(Item_Type, Item_MRP)) +geom_boxplot() +ggtitle(""Box Plot"") + theme(axis.text.x = element_text(angle = 70, vjust = 0.5, color = ""red"")) + xlab(""Item Type"") + ylab(""Item MRP"") + ggtitle(""Item Type vs Item MRP"")The black point you see, is an outlier. The mid line you see in the box, is the mean value of each item type. To know more about boxplots, check this tutorial.Now, we have an idea of the variables and their importance on response variable. Lets now move back to where we started. Missing values. Now well impute the missing values.We saw variable Item_Weight has missing values. Item_Weight is an continuous variable. Hence, in this case we can impute missing values with mean / median of item_weight. These arethe most commonly used methods of imputing missing value. To explore other methods of this techniques, check out this tutorial.Lets first combine the data sets. This will save our time as we dont need to write separate codes for train and test data sets. To combine the two data frames, we must make sure that they have equal columns, which is not the case.> dim(train)
[1] 8523 12

> dim(test)
[1] 5681 11Test data set has one less column (response variable). Lets first add the column. We can give this column any value. An intuitive approach would be to extract the mean value of sales from train data set and use it as placeholder for test variable Item _Outlet_ Sales. Anyways, lets make it simple for now. Ive taken a value 1. Now, well combine the data sets.> test$Item_Outlet_Sales <- 1
> combi <- rbind(train, test)Impute missing value by median. Im using median because it is known to be highly robust to outliers. Moreover, for this problem, our evaluation metric is RMSEwhich is also highly affected by outliers. Hence, median is better in this case.> combi$Item_Weight[is.na(combi$Item_Weight)] <- median(combi$Item_Weight, na.rm = TRUE)
> table(is.na(combi$Item_Weight))FALSE 
14204Its important to learn to deal with continuous and categorical variables separately in a data set. In other words, they need special attention. In this data set, we have only 3 continuous variables and rest are categorical in nature. If you are still confused, Ill suggest you to once again look at the data set using str() and proceed.Lets take up Item_Visibility. In the graph above, we saw item visibility haszero value also, which is practically not feasible. Hence, well consider it as a missing value and once again make the imputation using median.> combi$Item_Visibility <- ifelse(combi$Item_Visibility == 0,
             median(combi$Item_Visibility), combi$Item_Visibility)Lets proceed to categorical variables now. During exploration, we saw there are mis-matched levels in variables which needs to be corrected.> levels(combi$Outlet_Size)[1] <- ""Other""
> library(plyr)
>combi$Item_Fat_Content <- revalue(combi$Item_Fat_Content,
c(""LF"" = ""Low Fat"", ""reg"" = ""Regular""))
> combi$Item_Fat_Content <- revalue(combi$Item_Fat_Content, c(""low fat"" = ""Low Fat""))
> table(combi$Item_Fat_Content)
 Low Fat Regular 
 9185  5019Using the commands above, Ive assigned the name Other to unnamed level in Outlet_Size variable. Rest, Ive simply renamed the various levels of Item_Fat_Content.Lets call it as, the advanced level of data exploration. In this section well practically learn about feature engineering and other useful aspects.Feature Engineering: This component separates an intelligent data scientist from a technically enabled data scientist. You might have access to large machines to run heavy computations and algorithms, but the power delivered by new features, just cant be matched. We create new variables to extract and provide as much new information to the model, to help it make accurate predictions.If you have been thinking all this time, great. But now is the time to think deeper. Look at the data set and ask yourself, what else (factor) could influence Item_Outlet_Sales ? Anyhow, the answer is below. But, I want you to try it out first, before scrolling down.1. Count of Outlet Identifiers  There are 10 unique outlets in this data. This variable will give us information on count of outlets in the data set. More the number of counts of an outlet, chances are more will be the sales contributed by it.> library(dplyr)
> a <- combi%>%
      group_by(Outlet_Identifier)%>%
      tally()> head(a)
Source: local data frame [6 x 2]
Outlet_Identifier n
 (fctr)      (int)
1 OUT010     925
2 OUT013     1553
3 OUT017     1543
4 OUT018     1546
5 OUT019     880
6 OUT027     1559> names(a)[2] <- ""Outlet_Count""
> combi <- full_join(a, combi, by = ""Outlet_Identifier"")As you can see, dplyr package makes data manipulation quite effortless. You no longer need to write long function. In the code above, Ive simply stored the new data frame in a variable a. Later, the new column Outlet_Countis added in our original combi data set. To know more about dplyr, follow this tutorial.2. Count of Item Identifiers Similarly, we can compute count of item identifiers too. Its a good practice to fetch more information fromuniqueID variables using their count.This will help us to understand, which outlet has maximum frequency.> b <- combi%>%
     group_by(Item_Identifier)%>%
    tally()> names(b)[2] <- ""Item_Count""
> head (b)
Item_Identifier  Item_Count
(fctr)      (int)
1 DRA12       9
2 DRA24       10
3 DRA59       10
4 DRB01       8
5 DRB13       9
6 DRB24       8> combi <- merge(b, combi, by = Item_Identifier)3. Outlet Years  This variable represent the information of existence of a particular outlet since year 2013. Why just 2013? Youll find the answer in problem statement here. My hypothesis is, older the outlet, more footfall, large base of loyal customers and larger the outlet sales.> c <- combi%>%
     select(Outlet_Establishment_Year)%>%
     mutate(Outlet_Year = 2013 - combi$Outlet_Establishment_Year)
> head(c)
 Outlet_Establishment_Year Outlet_Year
1 1999            14
2 2009            4
3 1999            14
4 1998            15
5 1987            26
6 2009            4>combi <- full_join(c, combi)This suggests that outlets established in 1999 were 14 years old in 2013 and so on.4. Item Type New  Now, pay attention to Item_Identifiers.We are about to discover a new trend.Look carefully, there is a pattern in the identifiers starting with FD,DR,NC. Now, check the corresponding Item_Types to these identifiers in the data set. Youll discover, items corresponding to DR, are mostly eatables. Items corresponding to FD, are drinks. And, item corresponding to NC, are products which cant be consumed, lets call them non-consumable. Lets extract these variables into a new variable representing their counts.Here Ill use substr(), gsub() function to extract and rename the variables respectively.> q <- substr(combi$Item_Identifier,1,2)
> q <- gsub(""FD"",""Food"",q)
> q <- gsub(""DR"",""Drinks"",q)
> q <- gsub(""NC"",""Non-Consumable"",q)
> table(q)
 Drinks Food Non-Consumable 
 1317  10201 2686Lets now add this information in our data set with a variable name Item_Type_New.> combi$Item_Type_New <- qIll leave the rest of feature engineering intuition to you. You can think of more variables which could add more information to the model. But make sure, the variable arent correlated. Since, they are emanating from a same set of variable, there is a high chance for them to be correlated. You can check the same in R using cor() function.Just, one last aspect of feature engineering left. Label Encoding and One Hot Encoding.Label Encoding, in simple words, is the practice of numerically encoding (replacing) different levels of a categorical variables. For example: In our data set, the variable Item_Fat_Contenthas 2 levels: Low Fat and Regular. So, well encode Low Fat as 0 and Regular as 1. This will help us convert a factor variable in numeric variable. This can be simply done using if else statement in R.> combi$Item_Fat_Content <- ifelse(combi$Item_Fat_Content == ""Regular"",1,0)One Hot Encoding, in simple words, is the splittinga categorical variable into its unique levels, andeventually removing the original variable from data set. Confused ? Heres an example: Lets take any categorical variable, say, Outlet_ Location_Type. It has 3 levels. One hot encoding of this variable, will create 3 different variables consisting of 1s and 0s. 1s will represent the existence of variable and 0s will represent non-existence of variable. Let look at a sample:> sample <- select(combi, Outlet_Location_Type)
>demo_sample <- data.frame(model.matrix(~.-1,sample))
> head(demo_sample)
Outlet_Location_TypeTier.1 Outlet_Location_TypeTier.2 Outlet_Location_TypeTier.3
1       1             0            0
2       0             0            1
3       1             0            0
4       0             0            1
5       0             0            1
6       0             0            1model.matrix creates a matrix of encoded variables.  ~. -1 tells R, to encode all variables in the data frame, but suppress the intercept. So, what will happen if you dont write -1 ? model.matrix will skip the first level of the factor, thereby resulting in just 2 out of 3 factor levels (loss of information).This was the demonstration of one hot encoding. Hope you have understood the concept now. Lets now apply this technique to all categorical variables in our data set (excluding ID variable).>library(dummies)
>combi <- dummy.data.frame(combi, names = c('Outlet_Size','Outlet_Location_Type','Outlet_Type', 'Item_Type_New'), sep='_')With this, I have shared 2 different methods of performing one hot encoding in R. Lets check if encoding has been done.> str (combi)
$ Outlet_Size_Other : int 0 1 1 0 1 0 0 0 0 0 ...
 $ Outlet_Size_High : int 0 0 0 1 0 0 0 0 0 0 ...
 $ Outlet_Size_Medium : int 1 0 0 0 0 0 1 1 0 1 ...
 $ Outlet_Size_Small : int 0 0 0 0 0 1 0 0 1 0 ...
 $ Outlet_Location_Type_Tier 1 : int 1 0 0 0 0 0 0 0 1 0 ...
 $ Outlet_Location_Type_Tier 2 : int 0 1 0 0 1 1 0 0 0 0 ...
 $ Outlet_Location_Type_Tier 3 : int 0 0 1 1 0 0 1 1 0 1 ...
 $ Outlet_Type_Grocery Store : int 0 0 1 0 0 0 0 0 0 0 ...
 $ Outlet_Type_Supermarket Type1: int 1 1 0 1 1 1 0 0 1 0 ...
 $ Outlet_Type_Supermarket Type2: int 0 0 0 0 0 0 0 1 0 0 ...
 $ Outlet_Type_Supermarket Type3: int 0 0 0 0 0 0 1 0 0 1 ...
 $ Item_Outlet_Sales : num 1 3829 284 2553 2553 ...
 $ Year : num 14 11 15 26 6 9 28 4 16 28 ...
 $ Item_Type_New_Drinks : int 1 1 1 1 1 1 1 1 1 1 ...
 $ Item_Type_New_Food : int 0 0 0 0 0 0 0 0 0 0 ...
 $ Item_Type_New_Non-Consumable : int 0 0 0 0 0 0 0 0 0 0 ...As you can see, after one hot encoding, the original variables are removed automatically from the data set.Finally, well drop the columns which have either been converted using other variables or are identifier variables. This can be accomplished using select from dplyr package.> combi <- select(combi, -c(Item_Identifier, Outlet_Identifier, Item_Fat_Content,                Outlet_Establishment_Year,Item_Type))
> str(combi)In this section, Ill cover Regression, Decision Trees and Random Forest. A detailed explanation of these algorithms is outside the scope of this article. These algorithms have been satisfactorily explained in our previous articles.Ive provided the links for useful resources.As you can see, we have encoded all our categorical variables. Now, this data set is good to takeforward to modeling. Since, we started from Train and Test, lets now divide the data sets.> new_train <- combi[1:nrow(train),]
> new_test <- combi[-(1:nrow(train)),]Multiple Regression is used when response variable is continuous in nature and predictors are many. Had it been categorical, we would have used Logistic Regression.Before you proceed, sharpen yourbasics of Regression here.Linear Regressiontakes following assumptions:Lets now build out first regression model on this data set. R uses lm() function for regression.> linear_model <- lm(Item_Outlet_Sales ~ ., data = new_train)
> summary(linear_model)Adjusted R measures the goodness of fit of a regression model. Higher the R, better is the model. Our R = 0.2085. It means we really did something drastically wrong.Lets figure it out.In our case, I could find our new variables arent helping much i.e. Item count, Outlet Count and Item_Type_New. Neither of these variables are significant. Significant variables are denoted by * sign.As we know, correlated predictor variables brings down the model accuracy. Letsfind out the amount of correlation present in our predictor variables. This can be simply calculated using:> cor(new_train)Alternatively, you can also use corrplot package for some fancy correlation plots. Scrolling through the long list of correlation coefficients, I could find a deadly correlation coefficient:cor(new_train$Outlet_Count, new_train$`Outlet_Type_Grocery Store`)
[1] -0.9991203Outlet_Count is highly correlated (negatively) with Outlet Type Grocery Store. Here are some problems I could find in this model:Lets try to create a more robust regression model. This time, Ill be using a building a simple model without encoding and new features. Below is the entire code:#load directory
> path <- ""C:/Users/manish/desktop/Data/February 2016""> setwd(path)#load data
> train <- read.csv(""train_Big.csv"")
> test <- read.csv(""test_Big.csv"")#create a new variable in test file 
> test$Item_Outlet_Sales <- 1#combine train and test data
> combi <- rbind(train, test)#impute missing value in Item_Weight
> combi$Item_Weight[is.na(combi$Item_Weight)] <- median(combi$Item_Weight, na.rm = TRUE)#impute 0 in item_visibility
> combi$Item_Visibility <- ifelse(combi$Item_Visibility == 0, median(combi$Item_Visibility),             combi$Item_Visibility)#rename level in Outlet_Size
> levels(combi$Outlet_Size)[1] <- ""Other""#rename levels of Item_Fat_Content
> library(plyr)
> combi$Item_Fat_Content <- revalue(combi$Item_Fat_Content,c(""LF"" = ""Low Fat"", ""reg"" =                  ""Regular""))
> combi$Item_Fat_Content <- revalue(combi$Item_Fat_Content, c(""low fat"" = ""Low Fat""))#create a new column 2013 - Year
> combi$Year <- 2013 - combi$Outlet_Establishment_Year#drop variables not required in modeling
> library(dplyr)
> combi <- select(combi, -c(Item_Identifier, Outlet_Identifier, Outlet_Establishment_Year))#divide data set
> new_train <- combi[1:nrow(train),]
> new_test <- combi[-(1:nrow(train)),]#linear regression
> linear_model <- lm(Item_Outlet_Sales ~ ., data = new_train)
> summary(linear_model)Now we have got R = 0.5623. This teaches us that, sometimes all you need is simple thought process to get high accuracy. Quite a good improvement from previous model. Next, time when you work on any model, always remember to start with a simple model.Lets check out regression plot to find out more ways to improve this model.> par(mfrow=c(2,2))
> plot(linear_model)You can zoom these graphs in R Studio at your end. All these plots have a different story to tell. But the most important story is being portrayed by Residuals vs Fitted graph.Residual values are the difference between actual and predicted outcome values. Fitted values are the predicted values. If you see carefully, youll discover it as a funnel shape graph (from right to left ). The shape of this graph suggests that our model is suffering from heteroskedasticity (unequal variance in error terms). Had there been constant variance, there would be no pattern visible in this graph.A common practice to tackle heteroskedasticity is by taking the log of response variable. Lets do it and check if we can get further improvement.> linear_model <- lm(log(Item_Outlet_Sales) ~ ., data = new_train)
> summary(linear_model)And, heres a snapshot of my model output. Congrats! We have got an improved model with R = 0.72. Now, we are on the right path. Once again you can check the residual plots (you might zoom it). Youll find there is no longer a trend in residual vs fitted value plot.This model can be further improved by detecting outliers and high leverage points. For now, I leave that part to you! I shall write a separate post on mysteries of regression soon. For now, lets check our RMSE so that we can compare it with other algorithms demonstrated below.To calculate RMSE, we can load a package named Metrics.> install.packages(""Metrics"")
> library(Metrics)
> rmse(new_train$Item_Outlet_Sales, exp(linear_model$fitted.values))
[1] 1140.004Lets proceed to decision tree algorithm and try to improve our RMSE score.Before you start, Id recommend you to glance through the basics of decision tree algorithms. To understand what makes it superior than linear regression, check this tutorialPart 1and Part 2.In R, decision tree algorithm can be implemented using rpart package.In addition, well use caret package for doing cross validation. Cross validation is a technique to build robust modelswhich are not prone tooverfitting. Read more about Cross Validation.In R, decision tree uses a complexity parameter (cp). It measures the tradeoff between model complexity and accuracy on training set. A smaller cp will lead to a bigger tree, which might overfit the model. Conversely, a large cp value might underfit the model. Underfitting occurs whenthe model does not capture underlying trends properly. Lets find out the optimum cp value for our model with 5 fold cross validation.#loading required libraries
> library(rpart)
> library(e1071)
> library(rpart.plot)
> library(caret)#setting the tree control parameters
> fitControl <- trainControl(method = ""cv"", number = 5)
> cartGrid <- expand.grid(.cp=(1:50)*0.01)#decision tree
> tree_model <- train(Item_Outlet_Sales ~ ., data = new_train, method = ""rpart"", trControl = fitControl, tuneGrid = cartGrid)
> print(tree_model)The final value for cp = 0.01. You can also check the table populated in console for more information. The model with cp = 0.01 has the least RMSE. Lets now build a decision tree with 0.01 as complexity parameter.> main_tree <- rpart(Item_Outlet_Sales ~ ., data = new_train, control = rpart.control(cp=0.01))
> prp(main_tree)Here is the tree structure of our model. If you have gone through the basics, you would now understand that this algorithm has marked Item_MRP as the most important variable (being the root node). Lets check the RMSE of this model and see if this is any better than regression.> pre_score <- predict(main_tree, type = ""vector"")
> rmse(new_train$Item_Outlet_Sales, pre_score)
[1] 1102.774As you can see, our RMSE has further improvedfrom 1140 to 1102.77 with decision tree. To improve this score further, you can further tune the parameters for greater accuracy.Random Forest is a powerful algorithm which holistically takes care of missing values, outliers and other non-linearities in the data set. Its simply a collection of classification trees, hence the name forest. Id suggest you to quickly refresh your basics of random forest with this tutorial.In R, random forest algorithm can be implement using randomForest package. Again, well use train package for cross validation and finding optimum value of model parameters.For this problem, Ill focus on two parameters of random forest. mtry and ntree.ntree is the number of trees to be grown in the forest. mtryis the number of variables taken at each node to build a tree. And, well do a 5 fold cross validation.Lets do it!#load randomForest library
> library(randomForest)#set tuning parameters
> control <- trainControl(method = ""cv"", number = 5)#random forest model
> rf_model <- train(Item_Outlet_Sales ~ ., data = new_train, method = ""parRF"",trControl =         control,prox = TRUE,allowParallel = TRUE)#check optimal parameters
> print(rf_model)If you notice, youll see Ive used method = parRF. This is parallel random forest. This is parallel implementation of random forest.This package causesyour local machine to take less time in random forest computation. Alternatively, you can also use method = rf as a standard random forest function.Now weve got the optimal value of mtry = 15. Lets use 1000 trees for computation.#random forest model
> forest_model <- randomForest(Item_Outlet_Sales ~ ., data = new_train, mtry = 15, ntree = 1000)
> print(forest_model)
> varImpPlot(forest_model)This model throws RMSE = 1132.04 which is not an improvement over decision tree model. Random forest has a feature of presenting the important variables. We see that the most important variable is Item_MRP (also shown by decision tree algorithm).This model can be further improved by tuning parameters. Also,Lets make out first submission with our best RMSE score by decision tree.> main_predict <- predict(main_tree, newdata = new_test, type = ""vector"")
> sub_file <- data.frame(Item_Identifier = test$Item_Identifier, Outlet_Identifier = test$Outlet_Identifier,    Item_Outlet_Sales = main_predict)
> write.csv(sub_file, 'Decision_tree_sales.csv')When predicted on out of sample data, our RMSE has come out to be 1174.33.Here are some things you can do to improve this model further:Do implement the ideas suggested above and share your improvement in the comments section below. Currently, Rank 1 on Leaderboard has obtained RMSE score of 1137.71. Beat it!This brings us to the end of this tutorial. Regret for not so happy ending. But, Ive given you enough hints to work on. The decision to not use encoded variables in the model, turned out to be beneficial until decision trees.The motive of this tutorial was to get your started with predictive modeling in R. We learnt few uncanny things such as build simple models. Dont jump towards building a complex model. Simple models give you benchmark score and a threshold to work with.In this tutorial, I have demonstrated the steps used in predictive modeling in R. Ive covered data exploration, data visualization, data manipulation and building models using Regression, Decision Trees and Random Forest algorithms.Did you find this tutorial useful ? Are you facing any trouble at any stage of this tutorial ? Feel free to mention your doubts in the comments section below. Do share if you get a better score.Edit: On visitors request, the PDF version of the tutorial is available for download. You need to create a log in account to download the PDF. Also, you can bookmark this page for future reference. Download Here.",https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/
Guide to Build Better Predictive Models using Segmentation,Learn everything about Analytics|Introduction|What is Segmentation ?|Techniques of Segmentation|How to createsegments for model development ?|Using Machine Learning for Segmentation|End Notes,"Objective Segmentation|Non-Objective Segmentation|1. Commonly adopted methodology|2. The Appropriate Methodology|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|A Complete Tutorial to learn Data Science in R from Scratch|Quick Insights: India Analytics and Big Data Salary Report 2016|
Guest Blog
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",About the Authors,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We use linear or logistic regression technique for developing accurate models for predicting an outcome of interest. Often, we create separate models for separate segments. To judge their effectiveness, we even make use of segmentation methods such as CHAID or CRT.But, is that necessary ? Cant we create a single model and enable it with some segmentation variable as an input to the model ?May be, we could. Particularly because, creating separate model for separate segments may be time consuming and not worth the effort.But, creating separate model for separate segmentsmay provide higher predictive power.In this article, not only Ive given an answer to the question above, but have also shared a perfect guideline for doing optimal segmentation for model development. Furthermore, this articlealso explores the possibilities of leveraging a segment model approach using complex techniques likestochastic gradient boosting or random forest into a simple logistic or linear regression framework (albeit to a very limited extent).Doing this, brings the essence of interaction effects in modeling process and replicated the advantages of complex techniques stated above.I could take you down in the depth of marketing to explain this concept. But, whyto complicate thingsif they could be made simpler?So, I define Market Segmentation as,Dividing the target market or customers on the basis of some significant features which could help a company sell more products in less marketing expenses.Companies have limited marketing budgets. Yet, the(marketing team) isexpected to makeslarge number of sales to ensure rising revenue & profits.In limited marketing budgets, how is it made possible? Answer is, usingsegmentation.Lets move a step back and understand how do companiescreate a product which people buy.Actually, a product is created in two ways Once the product is created, the ball shifts to the marketing teams court. As mentioned above, they make use of market segmentation techniques. This ensures the product is positioned to the right segment of customers withhigh propensity to buy.There are two broad set of methodologies for segmentation: Objective (supervised) and Non-Objective (unsupervised) segmentation methodologies. As the name indicates, a supervised methodology requires the objective to be stated as the basis for segmentation.Given below are some examples of objective and non-objective approaches.Hence, it is critical that the segments created on the basis of an objective segmentation methodology must be different with respect to the stated objective (e.g. response to an offer).However, in case of a non-objective methodology, the segments are different with respect to the generic profile of observations belonging to each segment, but not with regards to any specific outcome of interest.The most common techniques used for building an objective segmentation are CHAID and CRT. Each of these techniques attempt to maximize the difference among segments with regards to the stated objective (sometimes referred to as the target for segmentation). CHAID uses a chi square statistic for the same while CRT uses Gini impurity.The most common techniques for building non-objective segmentation are cluster analysis, K nearest neighbor techniques etc.Each of these techniques uses a distance measure (e.g. Euclidian distance, Manhattan distance, Mahalanobis distance etc.) This is doneto maximize the distance between the two segments. Thisimplies maximum difference between the segments with regards to a combination of all the variables (or factors).If you have thoroughly followed this article till here, we are now good to delve into the methodology adopted for creating segments. Of course, with the sole objective of building separate models for each segments.Let us consider an example.Here well build a logistic regression model for predicting likelihood of a customer to respond to an offer. A very similar approach can also be used for developing a linear regression model.Ive discussed it in the followingsection.Logistic regression: It uses1 or 0 indicator in the historical campaign data, which indicateswhether the customer has responded to the offer or not.Usually, one usesthe target (or Y known as dependent variable) that has been identified for model development to undertake anobjective segmentation. Remember,a separate model will be built for each segment. A segmentation scheme which provides the maximum difference between the segments with regards to the objective is usually selected. Below is a simple example of this approach.Fig-1: Sample segmentation for building a logistic regression  commonly adopted methodologyThe above segmentation scheme is the best possible objective segmentation developed, because the segments demonstrate the maximum separation with regards to the objectives (i.e. response rate).In the above tree, each separation should represent a statistically significant difference between the nodes with respect to the target. If a CHAID algorithm is used for developing the segmentation tree, then Chi Square value for each separation should be significantly different from zero (as measured by the p value of the separation).In addition, it is the common business intuition (which may not always have a sound statistical rationale), to develop separate models if the difference in response rates between adjacent node is at least 30% (e.g. if the response rate in a particular node is 0.7% and the same for the adjacent node is 0.5% then the difference in response rate is ~30%)The commonly adopted approach would suggest that one should build separate models for each of the terminal (or end) nodes, which have been depicted in green in Fig-1. But, is this the best approach from a modeling perspective? To answer that question, we need to find out a measure for evaluating a segmentation scheme from a modeling perspective.The most effective measure for evaluating a segmentation scheme for the purpose of building separate models is the lift in predictive power that can be achieved by building segmented models. The following example isused to illustrate the same.Let us assume that a logistic model is developed on the entire population to predict the likelihood of response.Let us designate this as Model-1 (mostly analysts describe this as the parent model), let the Gini for this model be 0.57. Now as part of the segmented model development approach, five separate models are built, one for each end node (mostly analysts describe them as child models).After building 5 separate models, the score or the predicted probability is calculated for each observation (or record) and the 5 data sets (for each end node) are appended.The Gini of the combined data set is compared with the Gini of model-1. Then, the ratio of the two is designated as the lift in predictive power. For instance, if the Gini of the combined data set is 0.6, then the lift will be 1.05. It can be seen that, though segmentation is the best possible objective segmentation, yet it provides only 5% extra lift in predictive power.Let us find out why this may be the case. It should be noted that,whenone is developing a linear model, thelift in Adjusted R Square should be considered instead of lift in Gini.While building the overall model (Model-1), one can always use appropriate dummy variables to represent the segmentation. For instance, one can use the following dummies (it should be noted that due to degree of freedom constraint there will be one less than all possible number of dummies)The predictive power of the model will be even better if one uses dummies to replicate the segmentation treeThese dummies would provide the same differentiation in response rate as that of the five individual segments. Hence, it can be seen that the differentiation in response rate whichis provided by the segmentation can easily be replicated by using a set of dummy variables in an overall regression model.However, this is not the complete explanation behind the low lift in predictive power.It is also important to considerthe set of segmented models or the child models. Fig-2 provides the list of variables in the child models.The variables in each model are ordered in accordance to their predictive strength (as measured by the Wald Chi Square and the Standardized Betas). The same color has been used to depict a particular variable across segments so as to make the comparison easier.Fig-2: Variables across the 5 child modelsIt can be observed that the variables in each of the five child models are quite similar, though the relative ordering of the variables is slightly different. This implies that the drivers of response are similar across segments. In addition, if one considers the predictive pattern of a particular variable across segments, one can observe something even more interesting.Fig-3 depicts the predictive pattern for the variable Number of purchase in the last 12 months.For depicting the predictive pattern, the Weight of Evidence (WoE) has been plotted. Weight of Evidence is a common measure used for understanding if a particular range of value for a variable has a relatively higher or lower concentration of the desired target. A positive value of WoE indicates that there is a higher concentration of the target and vice-versa. In this case, for higher values of number of purchases, one observes a higher WoE; indicating a relatively higher concentration of therespondents. Whilebuilding a linear model, the average value of the target acrosseach range of the variable should be used for understanding the predictive pattern.A visual inspection of the graph reveals that though the individual WoEs are different across the segments, yet the trend is very similar. This implies that the predictive pattern of the variable is same across segments. Therefore, the impact of variable in overall model is not very different as compared to the segment wise impact.In other words, it means that there is no interaction effect between the segmenting variables (i.e. age and income) aJnd the predictor variable Number of purchase in the last 12 months. Hence, the segmentation is not expected to yield any benefit with regards to lift in predictive power.It should also be noted that in this case the information value of the variables are also similar across segments. In case of a linear model, partial R square can be used instead of information value. If this is the case for most of highly predictive variables, then segmentation would add limited value to the overall predictive power.Fig-3: Predictive Pattern of the Variable Number of purchase in the last 12 months Across the Five SegmentsIn order to harness the interaction effect between the segmenting variables and the predictor variables, it is important to devise a segmentation scheme where predictors andpredictive patterns of the variables are different across segments.This will help one to create a scenario where the predictive power of the segmented models is higher than the predictive power of the overall model. Fig-4 provides an alternate segmentation scheme for the problem described earlier.Fig-4: Sample segmentation for building a logistic regression  alternate methodologyIn this case, one would develop the following segmented models (child models):The variables in the child models have been depicted in Fig-5. As in earlier case, the common variables have been highlighted in same color. It can be observed that in this case, the extent of overlap of the variables between segments is very limited. Therefore, each segment represents a homogeneous set of customers for whom the driver of response are almost completely different.Hence, in this case the Gini of the segmented system of models is significantly higher compared to the Gini of the overall model. This segmentation provides a significantly superior predictive power which is created due to the interaction of the segmenting variables and the predictor variables.Fig-5: Variables across the 4 child modelsFig-6: Predictive Pattern of the Variable Number of purchase in the last 24 months Across the Five SegmentsIn this case, one can observe that the predictive pattern of a particular variable is significantly different across segments. The lines in Fig-5 are much more dispersed and look different from one another as compared to Fig-3, which indicates that the predictive pattern of the variable is different across segments.Therefore the impact of the variable in the overall model is quite very different as compared to the segment wise impact. In other words, it means that there is significant interaction effect between the segmenting variables and the predictor variable No. of different categories of purchase made in last 18 months.Hence, the segmentation is expected to produce superior lift in predictive power. It should also be noted that in this case the information value of the variables are also different across segments.There is another interesting aspect to it. Forsegmentation, it is also good to consider machine learning algorithms based on multiple trees. Multiple Additive Regression Tree, Random Forest and Stochastic Gradient Boosting are techniques that use a multitude of trees and an ensemble of the same for making predictions.For instance if one considers stochastic gradient boosting, at a very simplistic (and possibly amateurish level), the method involves building an ensemble of trees wherein the residual from the first tree is used as the target for the second tree and so on till no further improvement in predictive power is observed.Each tree in this case consists of a few nodes and ensures that it does not over fit the data. In reality, each of these tree is expected to capture the interaction effect instead of fitting too closely to the target at hand. One can relate this philosophy (at a broad level) with the idea behind creation of segments for developing models, wherein the objective of the segmentation is not to achieve a closer fit with the target but to identify interaction effects.In fact, a possible way of identifying the segments for developing separate models may involve considering the nodes of the first few trees in a stochastic gradient boosting ensemble of trees and consider if those are appropriate for building segmented models.In this article, we learnt the following aspects:Did you find the article useful ? Have you used any of these techniques in your market segmentation process? Do share your opinions and suggestions in the comments section below.Sandhya Kuruganti and Hindol Basu are authors of a book on business analytics titled Business Analytics: Applications to Consumer Marketing, recently published by McGraw Hill. The book is available on Flipkart and Amazon India/UK/Canada/US. They are seasoned analytics professionals with a collective industry experience of more than 30 years.",https://www.analyticsvidhya.com/blog/2016/02/guide-build-predictive-models-segmentation/
Quick Insights: India Analytics and Big Data Salary Report 2016,Learn everything about Analytics|Introduction|Download Complete Report,"Share this:|Like this:|Related Articles|Guide to Build Better Predictive Models using Segmentation|Hewlett Packard Enterprise in association with Imarticus Learning presents LIVE WEBINAR on NEXT BEST OFFER|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are several burning questions which run down in the mind of an experienced / aspiring analytics professionals. The popular ones are:And many moreAfter the US, India has thelargest demand of analytics & data science professionals. Cities like Delhi, Bangalore, Hyderabad, Mumbai etc are leading theway right now. The uniquefusion of big data and machine learning has promised a bright future for IT professionals in our country and around the world. As a result, more and more people are shifting their jobs and striving to get a head start inanalytics industry.We (Analytics Vidhya) alongwith Jigsaw Academy have created a salary report which provides a comprehensive overview of current trends, skills, salary earned, companies and many other factors which can play a crucial role in writing your success story in analytics.We have created an exclusive infographic which highlights the major insights generated from this report. For view these insights in detail, you can download the complete report below.( Click To Enlarge )
Please enter your Email ID
Are you :

Recruiter
Data Science Professional

Your City

Mumbai
Delhi-NCR
Bengaluru
Chennai
Hyderabad
Kolkata
Pune
Ahmedabad
Indore
Others



* By filling this form you agree to receive jobs alerts and industry newsletter from Analytics Vidhya. Furthermore your account with Analytics Vidhya will also be created, the details of which shall be mailed to you.
Dont worry we will not spam you. ",https://www.analyticsvidhya.com/blog/2016/02/quick-insights-analytics-big-data-salary-report-2016/
Hewlett Packard Enterprise in association with Imarticus Learning presents LIVE WEBINAR on NEXT BEST OFFER,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Quick Insights: India Analytics and Big Data Salary Report 2016|India Exclusive: Analytics and Big Data Salary Report 2016|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imarticus Learning and Hewlett Packard Enterprise (HPE) present to you a joint webinar on the Next Best Offer Analytics. In this webinar, we focus on how organizations connect with their customers continuously, resulting in a higher level of engagement. This creates multiple touch-points and opens up a wide-range of opportunities to cross sell various products and build a steady revenue stream.After a brief introduction about HPE, we will focus on the approaches and techniques in operationalizing analytics for Next Best Offers. Staying in tune with Imarticus Learnings experiential form of learning, we go on to explore a case study for Next Best Offer and finally move towards the methods of deployment. We end this webinar by opening up the floor for some question and answers. This webinar can be accessed via your laptop, tablet or mobile.Hewlett Packard Enterprise is a leading analytics and technology solution provider. We have their Senior Director of Analytics, R. Raghavendra, who will conduct this session, along with his colleagues, Rajeshwari Sinha and Kumar Saurabh.REGISTER NOWWe welcome you to be part of this webinar on Thursday, Feb 25 at 5pm, IST.",https://www.analyticsvidhya.com/blog/2016/02/hewlett-packard-enterprise-association-imarticus-learning-presents-live-webinar-next-offer/
India Exclusive: Analytics and Big Data Salary Report 2016,Learn everything about Analytics|Introduction|Launching Analytics and Big Data Salary Report for India|Why is this report important ?|Some Insights from the Report|Methodology usedfor study|Download Complete Report,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|Hewlett Packard Enterprise in association with Imarticus Learning presents LIVE WEBINAR on NEXT BEST OFFER|Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python|
Kunal Jain
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
"," A candidate having knowledge of R earns Rs. 10.20L yearly as compared to Python with Rs. 9.36L yearly.|A professional with working knowledge of data science and big data earns 8% more than fellow data scientist|Mumbai pays the highest salary among all cities in absolute numbers at Rs. 12.19L yearly. But, it is the costliest too.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let us go a few years back (remember the pre-iphone era?), none of us imagined we would live in a world full of connected devices. Did we?Today, we are living in a world where your refrigerator would text you saying, Hey! I am running out of Apple Juice. Please bring some while coming home. Just imagine, such an effortless life our near future holds. And, this would all be made possible through our own data and connected devices!Companies have recognized the immense business value which can be delivered using data. Google, Amazon, Facebook, Baidu are just some of the companies which have made investments in data products such as self-driving cars, voice / image recognition, wearable devices etc.This has caused a huge demand of skilled professional in data related jobs around the world. Job profiles such as Data Scientist, Data Analyst, Big Data Engineer, Statistician are being largely hunted by companies. Not only they are being handsomely paid, but a career in analytics has much more to promise.And, this is just the beginning!Analytics Vidhya in association with Jigsaw Academy has created this salary report.We have interacted with more than a million data science professionals across the globe over the last 12 months. Through these interactions, we inevitably get a flavor of changes happening in analytics and data science industry. We are excited to launch our first ever Salary report for India. We believe that this report should bring out a lot of interesting trends about Analytics industry and its evolution in India.We believe that this is the first time, when a study like this has been done at this scale (spanning more than 60,000 analytics professionals from India) and this should help people making critical career decisions about their future.After the U.S., India has the largest demand of analytics / big data / data science professionals. Amidst such demand, people find themselves confused to select an appropriate job profile for the best future.Have you also asked yourself, What set of skills do I need toincrease my chances of getting higher salary ?And, many suchquestion which you would have asked yourself at some point in time, this report has answers to all of your ingenuous questions.This salary report is exclusively designed to give you a clear picture of analytics industry (job specific) in India. This will help people to understand the current trends in jobs, salaries, skills, locations prior to taking any job related decision in analytics.Heres a quick look at some essential insights fromthis report:R is still the most used tool in analytics industry. No doubt, python is catching up fast. Hence, it is highly recommended once you have gained significant knowledge in one tool, move to the next. These days companies look for varied set of skills and talents.Predictive modeling combined with big data results in a formidable combination of skill set. A candidate with Big Data and Data Science skills earn Rs. 13.10L yearly as compared to a candidate with only big data skills who earns Rs. 9.80 L yearly.Good news for people staying nearby Mumbai ! Yes, your city pays the highest amount of salary to a data scientist. So, if you live nearby Mumbai, you should look for jobs in your city instead of thinking of relocating to a new city.For more insights, you can download the complete report.The data points include people who came into contact withAnalytics Vidhya through ourwebsite, job platform, hiring competitions and other sources. This number also includes, butis not limited to, people applying for jobs on the Analytics Vidhya website, confidentialsearches, hackathons, and tie-ups with skill-enhancement partners.People with only advanced analytics / data science skills were considered for the study. Forexample, people who were only experienced in MIS have been excluded from the study.Once filtered, the skill sets of the survey participants were appropriately categorized in orderto gain insights into the data.The findings are open to biases arising from the nature of the jobs / competitions hosted onAnalytics Vidhyas website. However we believe that even with this bias, this first-of-its-kindstudy of the Indian Analytics industry and the thousands of analytics professionals itemploys, reveals many fascinating ground realities.Please enter your Email ID
Are you :

Recruiter
Data Science Professional

Your City

Mumbai
Delhi-NCR
Bengaluru
Chennai
Hyderabad
Kolkata
Pune
Ahmedabad
Indore
Others



* By filling this form you agree to receive jobs alerts and industry newsletter from Analytics Vidhya. Furthermore your account with Analytics Vidhya will also be created, the details of which shall be mailed to you.
Dont worry we will not spam you. ",https://www.analyticsvidhya.com/blog/2016/02/analytics-big-data-salary-report-2016/
Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. How Boosting Works ?|2. GBM Parameters|3. Parameter Tuning with Example|End Notes,"General Approach for Parameter Tuning|Fix learning rate and number of estimators for tuning tree-based parameters|Tuning tree-specific parameters|Tuning subsample and making models with lower learning rate|If you like this article and want to read a similar post for XGBoost, check this out Complete Guide to Parameter Tuning in XGBoost|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|India Exclusive: Analytics and Big Data Salary Report 2016|BML Munjal University launches MBA in Business Analytics to create future leaders!|
Aarshay Jain
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have been using GBM as a black box till now, maybe its time for you to open it and see, how it actually works!This article is inspired by Owen Zhangs (Chief Product Officer at DataRobot and Kaggle Rank 3) approach sharedatNYC Data Science Academy. He delivered a~2 hours talk and I intend to condense it and present the most precious nuggets here.Boosting algorithms play a crucial role in dealing with bias variance trade-off. Unlike bagging algorithms, which only controls forhigh variance in a model, boosting controls both the aspects (bias & variance), andis considered to be more effective.A sincere understanding of GBM here should give you much needed confidence to deal with such critical issues.In this article, Ill disclose the science behind using GBM usingPython. And, most important, how you can tune its parameters and obtain incredible results.Special Thanks: Personally, I would like to acknowledge the timeless support provided by Mr. Sudalai Rajkumar, currentlyAV Rank 2. This article wouldnt be possible without his guidance.I am sure the whole community will benefit from the same.Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in PythonBoosting is a sequential technique which works on the principle of ensemble. It combines a set of weak learnersand deliversimproved prediction accuracy. At any instant t, the model outcomesare weighed based on the outcomes of previous instant t-1. The outcomespredicted correctly are given a lower weight and the ones miss-classified are weighted higher. This techniqueis followed fora classification problem while a similar technique is used for regression.Lets understand it visually:Observations:Similar trend can be seenin box 3 as well. This continues for many iterations. In the end, all models are given a weight depending on their accuracy and a consolidated result is generated.Did I whet your appetite ? Good.Refer to these articles (focus on GBM right now):The overall parameters can be divided into 3 categories:Illstart with tree-specific parameters. First, lets look at the general structure of a decision tree:The parameters used for defining a tree are further explained below. Note that Im using scikit-learn (python) specific terminologies here which might be different in other software packages like R. But the idea remains the same.Before moving on toother parameters, lets see the overall pseudo-code of the GBM algorithm for 2 classes:This is an extremely simplified (probably naive) explanation of GBMs working. The parameters which we have considered so far will affect step 2.2, i.e. model building. Lets consider another set of parameters for managing boosting:Apart from these, there are certain miscellaneous parameters which affect overall functionality:I know its a long list of parameters but I have simplified it for you inan excel file which you can download from my GitHub repository.We will take the dataset from Data Hackathon 3.x AV hackathon. The details of the problem can be found on the competition page. You can download the data set from here. I have performed the following steps:For those who have the original data from competition, you can check out these steps from the data_preparationiPython notebook in the repository.Lets start by importing the required libraries and loading the data:Before proceeding further, lets define a function which will help us create GBM models and perform cross-validation.The code is pretty self-explanatory. Please feel free to drop a note in the comments if you find any challenges in understanding any part of it.Lets start by creating a baseline model. In this case, the evaluation metric is AUC so using any constant value will give 0.5 as result. Typically, a good baseline can be a GBM model with default parameters, i.e. without any tuning. Lets find out what it gives:So, the mean CV score is 0.8319 and we should expect our model to do better than this.As discussed earlier, there are two types of parameter to be tuned here  tree based and boosting parameters. There are no optimum values for learning rate as low values always work better, given that we train on sufficient number of trees.Though, GBM is robust enough to not overfit with increasing trees, but a high number for pa particular learning rate can lead to overfitting. But as we reduce the learning rate and increase trees, the computation becomes expensive and would take a long time to run on standard personal computers.Keeping all this in mind, we can take the following approach:In order to decide on boosting parameters, we need to set some initial values of other parameters. Lets take the following values:Please note that all the above are just initial estimates and will be tuned later. Lets take the default learning rate of 0.1 here and check the optimum number of trees for that. For this purpose, we can do a grid search and test out values from 20 to 80 in steps of 10.The output can be checked using following command:As you can see that here we got 60 as the optimal estimators for 0.1 learning rate. Note that 60 is a reasonable value and can be used as it is. But it might not be the same in all cases. Other situations:Now lets move onto tuning the tree parameters. I plan to do this in following stages:The order of tuning variables should be decided carefully. You should take the variables with a higher impact on outcome first. For instance, max_depth and min_samples_split have a significant impact and were tuning those first.Important Note: Ill be doing some heavy-duty grid searched in this section which can take 15-30 mins or even more time to run depending on your system. You can vary the number of values you are testing based on what your system can handle.To start with, Ill test max_depth values of 5 to 15 in steps of 2 and min_samples_split from 200 to 1000 in steps of 200. These are just based on my intuition. You can set wider ranges as well and then perform multiple iterations for smaller ranges.Here, we have run 30 combinations and the ideal values are 9 for max_depth and 1000 for min_samples_split. Note that, 1000 is an extreme value which we tested. There is a fare chance that the optimum value lies above that. So we should check for some higher values as well.Here, Ill take the max_depth of 9 as optimum and not try different values for higher min_samples_split. It might not be the best idea always but here if you observe the output closely, max_depth of 9 works better in most of the cases. Also, we can test for 5 values of min_samples_leaf, from 30 to 70 in steps of 10, along with higher min_samples_split.Here we get the optimum values as 1200 for min_samples_split and 60 for min_samples_leaf. Also, we can see the CV score increasing to 0.8396 now. Lets fit the model again on this and have a look at the feature importance.If you compare the feature importance of this model with the baseline model, youll find that now we are able to derive value from many more variables. Also, earlier it placedtoo much importanceon some variables but now it has beenfairlydistributed.Now lets tune the last tree-parameters, i.e. max_features by trying 7 values from 7 to 19 in steps of 2.Here, we find that optimum value is 7, which is also the square root. So our initial value was the best. You might be anxious to check for lower values and you should if you like. Ill stay with 7 for now. With this we have the final tree-parameters as:The next step would be try different subsample values. Lets take values 0.6,0.7,0.75,0.8,0.85,0.9.Here, we found 0.85 as the optimum value. Finally, we have all the parameters needed. Now, we need to lower the learning rate and increase the number of estimators proportionally. Note that these trees might not be the most optimum values but a good benchmark.As trees increase, it will become increasingly computationally expensive to perform CV and find the optimum values. For you to get some idea of the model performance, I have included the private leaderboard scores for each. Since the data is not open, you wont be able to replicate that but itll good for understanding.Lets decrease the learning rate to half, i.e. 0.05 with twice (120) the number of trees.Private LB Score: 0.844139Now lets reduce to one-tenth of the original value, i.e. 0.01 for 600 trees.Private LB Score: 0.848145Lets decrease to one-twentieth of the original value, i.e. 0.005 for 1200 trees.Private LB Score: 0.848112Here we see that the score reduced very slightly. So lets run for 1500 trees.Private LB Score: 0.848747Therefore, now you can clearly see that this is a very important step as private LB scored improved from ~0.844 to ~0.849 which is a significant jump.Another hack that can be used here is the warm_start parameter of GBM. You can use it to increase the number of estimators in small steps and test different values without having to run from starting always. You can also download the iPython notebook with all these model codes from my GitHub account.This article was based on developing a GBM modelend-to-end. We started with an introduction to boosting which was followed by detailed discussion on the various parameters involved. The parameters were divided into 3 categories namely the tree-specific, boosting and miscellaneous parameters depending on their impact on the model.Finally, we discussed the general approach towards tackling a problem with GBM and also worked outthe AV Data Hackathon 3.x problem through that approach.I hope you found this useful and now you feel more confident toapply GBM in solving adata science problem. You can try this out in out upcoming signature hackathon Date Your Data.Did you like this article? Would you like to share some otherhacks which you implement while making GBM models? Please feel free to drop a note in the comments below and Ill be glad to discuss.",https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/
BML Munjal University launches MBA in Business Analytics to create future leaders!,Learn everything about Analytics,"KJ:What was the idea behind launching an exclusive MBA in Business Analytics? What do you intend to accomplish?|KJ:How is your course different from other business analytics programs in India?|KJ: Do you think that students enrolling in this course will have an edge over students of other courses? If yes, how?|KJ: Who is the target audience of this program?|KJ:What is the structure of the program  total classroom time, expectation from the candidate and the areas covered as part of the course?|KJ:What is the teaching methodology adopted for this Program  Online / Offline / Hybrid?|KJ:How much of the program is devoted to Industry Interaction and Live Project?|KJ: Is there a capstone project / industry project at the end of the course? If yes, how much time are students expected to devote on it?|KJ: Tell us about the Profile of the Faculties of this program.|KJ: How about placements? Would placement support be available for this program?|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathons to compete with many Data Scientists from all over the world.|Share this:|Like this:|Related Articles|Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python|Free Must Read Books on Statistics & Mathematics for Data Science|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The need for data science talent is increasing by the day. While this demand is increasing, the supply of the talent with the right skills is still catching up. According to the famous McKinsey report,by 2018 the United States alone will experience a shortage of 190,000 skilled data scientists, and 1.5 million managers and analysts capable of reaping actionable insights from the big data deluge. You can imagine what the global shortage would be!In order to address this need of managers capable of reaping actionable insights for business, various institutes are trying to come up with new programs to prepare leaders for tomorrow. One such program was launched byBML Munjal University (BMU) recently.As usual, we decided to take a closer look at the program to make sure our audience gets the right information to make their career choices. So, I reached out to the program director with my questions and here are the excerpts from the discussion.ANSWER:Advancement of technology is impacting our world and todays economy. It is also diverting researchers from academia and industry to fulfill the gap and create a bridge between theory and practice with specific expertise in analytics. Business analytics provides skill based knowledge to students and prepares them to deal with the current world of business which as we all know is a very dynamic field. Nowadays, a massive amount of data is generated from different business processes belonging to diverse business environments.For the useful information extraction from huge, overloaded, and variety of unstructured data there rises a demand for strategic and skillful managers for better decision making. This course will prepare students in different domains of business analytics that uses data for statistical and quantitative analysis, predictive modelling, prescriptive analytics, and content based analysis etc. This course will provide a platform for students which combine both theory and practice according to the requirement of industry.ANSWER:Nowadays, business analytics is a need not only for Decision Science and Information System, but other specific areas, like Marketing, Finance, HR, Accounts, Communication and Strategy etc. have a strong demand for these skills and expertise. We are focusing on individual areas and will provide to-the-point expertise to the students so that they can cope up with a number of specific business needs.ANSWER:Of course, if a student possesses proficiency in Business Analytics (BA) apart from other skills then they can easily fit themselves into the current business scenario. Students will be more focused because with their expertise they can target specific goals and they will be preferred by the industry.ANSWER:In this program, we are targeting students who have the requirements for pursuing a postgraduate program and they should have additional prerequisites such as knowledge of basic statistics, mathematics, and basic computers skills. We are also focusing on some students from industry who have the required analytical skills and practical exposure.For example, we focus on students such as:ANSWER:In our University, we follow a module system where the first year is common for all students and second year on-wards the students have the option of choosing electives. In this context, we will offer some specific area courses to our students for providing them the necessary skills and expertise. In the second year of the program, we are planning to offer the below mentioned courses in association with IBM. We have divided our course into three different segments:Basic Analytics CoursesApplication CoursesDomain CoursesANSWER:The curriculum for MBA in Business Analytics has been designed in association with IBM. IBM has established an IBM Business Analytics Lab at the BMU campus, through which they are providing industry relevant software and training on Predictive Analytics using IBM SPSS, Descriptive Analytics using IBM Cognos and Big Data & Analytics using IBM Info Sphere Big Insight etc.We follow and believe on case based learning process where we have a mix of core, electives, skill, and perspective courses. We are following a hybrid course structure where most of the courses will be taught in class but some online courses will also be a part of the curriculum. More than 40% courses will be conducted by industry experts, consultants, and visiting faculty from International and reputed Indian UniversitiesANSWER:Approximately 20% of the program is hands-on and project based. In these modules, professionals working in analytics and big data will guide students on how to deal with real-world problems not only from a business point of view but also on basic social issues incorporating live data. We do have empirical study as a course where students are involved in live research projects for enhancing their research thinking and analysis capabilities.ANSWER:Our students compulsorily go for the summer practice school where they work under a company and work on a live project. This summer internship will take 8 weeks. After completion of projects, students have to submit their report and which will then be graded by assigned instructors from industry and the school.ANSWER:Dr Gopal Chaudhurihastaught at several reputed Institutes in India and abroad including Indiana University at Indianapolis, USA, IIM Ahmedabad, IIM Kozhikode and IIPM Kolkata before joining BMU. He was a visiting faculty seconded by the Government of India at Asian Institute of Technology, Bangkok.He served the Thomson Consumer Electronics at Indianapolis as a Statistical Consultant. Dr Chaudhuri obtained his M Stat from Indian Statistical Institute and PhD (Statistics) from Indian Institute of Technology Kanpur. He teaches courses in Business Statistics, Operations Research, Six Sigma, and Statistical Methods in Data Analysis and conducts MDP on Advanced Data Analysis for Marketing Decisions.His research area includes Reliability Statistics and Statistical Applications in Management. He has published 25 research papers in national and international journals of repute. He was awarded a US Patent for his contribution to the System Reliability Prediction in 2003. He is featured in the Marquis International Whos Who published in the USA.Dr Shrawan Kumar Trivedicompleted and submitted his research as a Fellow (FPM) in Indian Institute of Management Indore. Prior to this, he did his M.Tech (IT) from Indian Institute of Information Technology Allahabad, M.Sc. (Electronics) from University Institute of Technology, C.S.J.M. University Kanpur and B.Sc. (PCM) form C.S.J.M. University Kanpur. He has expertise in Data Mining, Text Mining and Big Data. His area of interest includes Business Intelligence and Analytics, Management Information Systems, Enterprise Resource Planning, Knowledge Management, IT Strategy etc. He also has expertise in several software tools like SPSS, SAS, MATLAB.In addition to this, he published his research papers in reputed international journals and presented his research at many international conferences. In his FPM, he worked on Text mining and developed some new algorithms for classification. His research interest includes Text Mining, Data Mining, Big Data, Inter Organisation Systems and Applications like Spam Classification, Sentiment Analysis, Credit Scoring etc.Dr. Anil Kumar completed his PhD in Management Science from Indian Institute of Information Technology and Management, Gwalior. He did his MBA from ICFAI University, Dehradun, M.Sc. (Mathematics) from Department of Mathematics (Kurukshetra University, Kurukshetra) and Graduation in Mathematics-Hons from University College (Kurukshetra University, Kurukshetra). He also qualified UGC-NET in Management.He is actively involved as a resource person of Faculty Development Programs (FDPs) and has more than 25 research papers/book chapters and 2 books to his credit. His books include Fuzzy Optimization and Multi-Criteria Decision Making in Digital Marketing (Ed.) and The Economics of Small Scale Industries in India: The Role of Private Equity and Debt Market. He is a member of many scientific societies like International Society on Multiple Criteria Decision Making (ISMCDM), Marketing Science Institute (MSI), American Marketing Association (AMA), and Society of Information Sciences (SIS) etc. He has his expertise in Marketing Analytics, Mathematics and Statistics for Management, Multi-Criteria Decision Making, Fuzzy Multi-Criteria Decision Making, Marketing Research, Research Methodology, Business Analytics, Quantitative Technique, Marketing Management, and Consumer Behavior.Mr. Saroj Kanta Jenahas over 17 years of experience in teaching various Computer Sciences and Engineering courses, particularly in the fields of Data Structures, Database Management Systems, and Data Mining, Software Engineering and Operating Systems in both the undergraduate and postgraduate levels. He also has teaching experience in Business Mathematics, Quantitative Techniques for Managers, Business Analytics, Operation Research and Management Information Systems for undergraduate and postgraduate courses of management studies. His teaching interests also include Advertising Management and Consumer Buying Behavior.ANSWER:As this course is a skill based course and analytics is a sought after skill set by organisations of today. After completion of this course our students can begin their careers with the following profiles:Our focus is to provide skills and expertise to students so that the respective skill based profile can be achieved by them. At BMU, we want to maintain a good placement team specifically for this area so that we can target related domain companies.Our potential recruiters are Infosys, TCS, HCL Technologies, Cognizant, IBM, Dell, Wipro, Accenture, SAS etc.I thank the faculty and the program Director at BMU for spending time in providing us these useful details.If you have any questions / queries about the program, feel free to write them in comments below. We will make sure, we get an answer to them!You can get more details and apply here.",https://www.analyticsvidhya.com/blog/2016/02/business-analytics-bml-munjal-university/
Free Must Read Books on Statistics & Mathematics for Data Science,Learn everything about Analytics|Introduction|Statistics|Mathematics|Additional Resources|End Notes,"Introduction to Statistical Learning|Elements of Statistical Learning|Think Stats|From Algorithms to Z Scores|Introduction to Bayesian Statistics|Discovering Statistics using R|Introduction to Linear Algebra|Matrix Computation|A Probabilistic Theory of Pattern Recognition|Introduction of Math of Neural Networks|Advanced Engineering Mathematics|Cookbook on Probability and Statistics|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Like this:|Related Articles|BML Munjal University launches MBA in Business Analytics to create future leaders!|Advanced Learning Path  Now Learn R with Best Online Resources|
Analytics Vidhya Content Team
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The selection process of data scientists at Google gives higher priority tocandidates with strong background in statistics and mathematics. Not just Google,othertopcompanies (Amazon, Airbnb, Uber etc) in the world also prefer candidates withstrong fundamentals rather than mere know-howin data science.If you tooaspire to work forsuch top companies in future, it is essential for you to develop a mathematical understanding of data science.Data science is simply the evolved versionof statistics and mathematics, combined with programming and business logic.Ive met many data scientists who struggle to explain predictive models statistically.More than just deriving accuracy, understanding & interpreting every metric, calculation behind that accuracy is important.Remember, every single variable has a story to tell. So, if not anything else, try tobecome a great story explorer!In this article, Ive compiled a list of must read books on statistics and mathematics. I understand, mathematics has no extreme. Hence, Ive enlist only those books which will help you toconnect with data science better.Note: Books which are made free to access by the registered authorities have been mentioned in this article. If not, a link to amazon bookstore isprovided.This is a highly recommended book for practicing data scientists. The focus of this books is kepton connecting statistics concept with machine learning. Hence, youll learn about all popularsupervised and unsupervised machine learning algorithms. R userswill get an advantage, since the practical aspects of algorithms have been demonstrated using R. In addition to theory, this book also lay emphasis on using ML algorithms in real life setting.Available: Free DownloadThis book is an advanced level of previous book. It is written by Trevor Hastie and Rob Tibshirani, Professors at Stanford University. Their first book Introduction to Statistical Learning uncoverthe basics of statistics and machine learning. This book, will introduce you to higher level algorithms such as Neural Networks, Bagging & Boosting, Kernel methods etc. The algorithms have been implemented in R programming.Available: Free DownloadThe author of this book is Alien B Downey. It is based on perform statistical analysis practically in Python. Hence,make sure youve gotsome basic knowledge of Python before buying this book. It focuses entirely on understanding real life influence of statistics using popular case studies. Since, stats and math are closely connected, it also has dedicated chapters on topic like bayesian estimation.Available: Buy from AmazonDid you know the about crucial role of statistics in programming ? The author of this book is Norm Matloff, Professor, University of California. This book explains using probabilistic concepts and statistical measures in R. Again, a good practice source for R users. It teaches the art of dealing with probabilistic models and choosing the best one for final evaluation. It is a highly recommended book (specially for R users).Available: Free DownloadThis is a highly recommended book for freshersin data science. The author of this book is William M Bolstad. Its a must read for people who find mathematics boring. Having been written in a conversational style (rare to find math this way), this book is a great introductory resource on statistics. It begins with scientific methods of data gathering and end up delivering dedicated chapters on bayesian statistics.Available: Free DownloadThis book is written by Andy Field, Jeremy Miles and Zoe Field. I would highly recommend this book tonewbies in data science. To start with statistics, this book has a great content which goes in depth detail of its topics. Along with, the statistical concept are explained in conjunction with R which makes it even more useful. It offers a step by step understanding, with a parallel support of interestingpractice examples.Available: Buy on AmazonThis is one of the most recommended book on Linear Algebra. The author of this book is Gilbert Strang, Professor, MIT. Gilbert unique way of delivering knowledge would give you the intuition and excitement to move forward after every chapter. This book will help you to build a strong mathematical foundation for machine learning. It enlists all the necessary chapters such as vectors, linear equations, determinants, eigenvalues, matrix factorization etc in great depth.Available: Buy on AmazonMatrix and Data frames are essential components of machine learning. The author of this book is Gene H Golub and Charles F Van Loan. This book provides a nice head start to students with concepts of matrix computations. The author covers most of the important topics such as gaussian elimination, matrix factorization, lancoz method, error analysis etc. Every chapter is supported by intuitive practice problems. The pseudo codes are available in Matlab.Available: Free DownloadThis is a complete resource to learn application of mathematics. This is a must read book for intermediate and advanced practitioners in machine learning.This book is written by Luc Devroye, Laszlo Gyorfi and Gabor Lugosi.It covers a wide range of topics varying from bayes error, linear discrimination to epsilon entropy & neural networks.It provides a convincing explanation to complex theorems with section wise practice problems.Available: Free DownloadIf you have innate interest in learning about neural network, this should be your place to start. The author of this book is Jeff Heaton. The author has beautifully simplified the difficult concepts of neural networks. This book introduces you to basics of underlying maths in neural networks. It assumes reader has prior knowledge of algebra, calculus and programming. It demonstratesvarious mathematical tools which can be applied to neural networks.Available: Buy on AmazonThis is probably the most comprehensive book available on mathematics for machine learning users. The author of this book is Erwin Kreyszig. As a matter of fact, this book is highly recommended to college students as well. If you havent been good at maths till now, follow this book religiously and you should surely see significant improvements in your math understanding. Along with derivations & practice example, this book has dedicated sections of calculus, algebra, probability etc. Definitely, a must read book for all levels of practitioners in data science.Available: Free DownloadThis cookbook is must have in your digital bookshelf. This isnt exactly a text book youd discover, but a quick digital guide on mathematical equations. The author of this book is Matthias Vallentin.After you finish with essentials of mathematics, this book will help you connect various theorem and algorithm quickly with their formulae. Its difficult to derive equations instantly, this book will help you to quickly navigate to your desired problem and solve.Available: Free DownloadBored of reading too much ? Here are is a list of highly recommended tutorials (video) / resources on mathematics and statistics. They are FREE to access.The books listed in this article are selected on the basis of their reviews and depth of topics covered. This is not an exhaustive list of books. But, I found its almost too easy to get confused while deciding from where to begin? In such situations, it is advisable to start with this list.In this article, Ive listed some most helpful books on statistics and machine learning. It has been found that people tend of neglect these topics in pursuit of quick success. But, thats not the right way. Hence, if you aim for a long term success in data science, make sure you learn to create stories out of maths and statistics.Have you read any of these books ? Which book on mathematics and statistics has helped you the most ? Please share your suggestions / reviews in the comments section below.",https://www.analyticsvidhya.com/blog/2016/02/free-read-books-statistics-mathematics-data-science/
Advanced Learning Path  Now Learn R with Best Online Resources,Learn everything about Analytics|Introduction|Visit Here  Learning Path on R,"You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathons to compete with many Data Scientists from all over the world.|Share this:|Like this:|Related Articles|Free Must Read Books on Statistics & Mathematics for Data Science|Data Analyst, Risk Analytics & Modeling  Chennai (2+ years of experience)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This good news is onlyfor (future) R Users!If you are new to data science, and keen to begin your career, bookmarking this page should be your first step.To be successful in data science, among all other skills, employerspay special attention to programming skills. And, if you have experience in an open-source programming language, you couldnt ask for a better start.The two most popular programming languages in data science are Python and R. We launched our learning paths some time back and got tremendous response from our audience.In less than a year,our Learning path on Pythonand LeaRning path on R have helped tens of thousandsof people globally to learn these language.However, this is a fast moving world and there is a lot of content which gets created by some smart people across the globe. Hence, to keep these learning paths useful, we plan to update them regularly. This time we also got help from our friends at DataCamp to create this awesome learning path.Analytics Vidhya and DataCamp have created a exclusive learning path on R with updated tutorials, practice questions, exercises etc. We have done all the hard work, you just need to follow this path with discipline. And guess what, this learning pathis FREE to access !This learning path enlists covers the following topics:We have added only the best of resources so that you dont need to waste time on finding alternative resources. In case you have any thoughts / suggestions, feel free to share them in comments section below.",https://www.analyticsvidhya.com/blog/2016/02/advanced-learning-path-learn-online-resources/
"Data Analyst, Risk Analytics & Modeling  Chennai (2+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Advanced Learning Path  Now Learn R with Best Online Resources|Approach and Solution to break in Top 20 of Big Mart Sales prediction|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data Analyst, Risk Analytics & ModelingLocation  ChennaiAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Analyst, Risk Analytics & Modelling  Chennai and the following details:",https://www.analyticsvidhya.com/blog/2016/02/data-analyst-risk-analytics-modelling-chennai-2-years-experience/
Approach and Solution to break in Top 20 of Big Mart Sales prediction|Introduction|End Notes,Learn everything about Analytics|1. Hypothesis Generation|2. Data Exploration|3. Data Cleaning|4. Feature Engineering|4. Model Building,"The Problem Statement|The Hypotheses|Imputing Missing Values|Step 1: Consider combining Outlet_Type|Step 2: Modify Item_Visibility|Step 3: Create a broad category of Type of Item|Step 4: Determine the years of operation of a store|Step 5: Modify categories of Item_Fat_Content||Step 6: Numerical and One-Hot Coding of Categorical variables|Step 7: Exporting Data|Linear Regression Model|Ridge Regression Model:||Decision Tree Model||Random Forest Model|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathons to compete with many Data Scientists from all over the world.|Share this:|Like this:|Related Articles|Data Analyst, Risk Analytics & Modeling  Chennai (2+ years of experience)|Business Analyst  Bangalore (2-3 years of experience)|
Aarshay Jain
|57 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",You can also check out a full hands-on solution to this practice problem on our Trainings platform.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Practice problems or data science projects are one of the best ways to learn data science. You dont learn data science until you start working on problems yourself.BigMart Sales Prediction practice problem was launched about a month back, and 624 data scientists have already registered with 77 among those making submissions. If youre finding it difficult to start or if you feel stuck somewhere, this article is meant just for you. Today I am going to take you through the entire journey of getting started with this data set.I hope that this article will help more and more people start their data science journey!We will explore the problem in following stages:Without further ado, lets get started!This is a very pivotal step in the process of analyzing data. This involves understanding the problem and making some hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analysiswhich we can potentially perform if data is available. Read more about hypothesis generation here.Understanding the problem statement is the first and foremost step. You can view this in the competition page but Ill iterate the same here:The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.So the idea is to find out the properties of a product, and store which impacts the sales of a product. Lets think about some of the analysis that can be done and come up with certain hypothesis.I came up with the following hypothesis while thinking about the problem. These are just my thoughts and you can come-up with many more of these. Since were talking about stores and products, lets make different setsfor each.Store Level Hypotheses:Product Level Hypotheses:These are just some basic 15 hypothesis I have made, but you can think further and create some of your own. Remember that the data might not be sufficient to test all of these, but forming these gives us a better understanding of the problemand we can even look for open source information if available.Lets move on to the data exploration where we will have a look at the data in detail.Well be performing some basic data exploration here and come up with some inferences about the data. Well try to figure out some irregularities and address them in the next section. If you are new to this domain, please refer our Data Exploration Guide.The first step is to look at the data and try to identify the information which we hypothesized vs the available data. A comparison between the data dictionary on the competition page and out hypotheses is shown below:We can summarize the findings as:You will invariable find features which you hypothesized,butdata doesnt carry and vice versa. You should look for open source data to fill the gaps if possible.Lets start by loading the required libraries and data. You can download the data from thecompetition page.Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe data with a source column specifying where each observation belongs.Thus we can see that data has same #columns but rows equivalent to both test and train. One of the key challenges in any data set is missing values. Lets start by checking whichcolumns contain missing values.Note that the Item_Outlet_Sales is the target variable and missing values are ones in the test set. So we need not worry about it. But well impute the missing values in Item_Weight and Outlet_Size in the data cleaning section.Lets look at some basic statistics for numerical variables.Some observations:Moving to nominal (categorical) variable, lets have a look at the number of unique values in each of them.This tells us that there are 1559 productsand 10 outlets/stores (which was also mentioned in problem statement). Another thing that should catch attention is that Item_Type has 16 unique values. Lets explore further using the frequency of different categories in each nominal variable. Ill exclude the ID and source variables for obvious reasons.The output gives us following observations:This step typically involves imputing missing values and treating outliers. Though outlier removal is very important in regression techniques, advanced tree based algorithms are impervious to outliers. So Ill leave it to you to try it out. Well focus on the imputation step here, which is a very important step.Note: Well be using some Pandas library extensively here. If youre new to Pandas, please go through this article.We found two variables with missing values  Item_Weight and Outlet_Size. Lets impute the formerby the average weight of the particular item. This can be done as:This confirms that the column has no missing values now. Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet.This confirms that there are no missing values in the data. Lets move on to feature engineering now.Weexplored somenuances in the data in the data exploration section.Lets move on to resolving them and making our data ready for analysis. We willalso create some new variables using the existing ones in this section.During exploration, we decided to consider combining the Supermarket Type2 and Type3 variables. But is that a good idea? A quick way to check that could be to analyze the mean sales by type of store. If they have similar sales, then keeping them separate wont help much.This shows significant difference between them and well leave them as it is. Note that this is just one way of doing this, you can perform some other analysis in different situations and also do the same forother features.We noticed that the minimum value here is 0, which makes no practical sense. Lets consider it like missing information and impute it with mean visibility of that product.So we can see that there are no values which are zero.In step 1we hypothesized that products with higher visibility are likely to sell more. But along with comparing products on absolute terms, we should look at the visibility of the product in that particular store as compared to the mean visibility of that product across allstores. This will give some idea about how much importance was given to that product in a store as compared to other stores. We can use the visibility_avg variable made above to achieve this.Thus the new variable has been successfully created. Again, this is just 1 example of how to create new features. I highly encourage you to try more of these, as good features can drastically improve model performance and they invariably prove to be the difference between the best and the average model.Earlier we saw that the Item_Type variable has 16 categories which might prove to be very useful in analysis. So its a good idea to combine them. One way could be to manually assign a new category to each. But theres a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. So Ive used the Item_Identifier variable to create a new column:Another idea could be to combine categories based on sales. The ones with high average sales could be combined together. I leave this for you to try.We wanted to make a new column depicting the years of operation of a store. This can be done as:This shows stores which are 4-28 years old. Notice Ive used 2013. Why? Read the problem statement carefully and youll know.We found typos and difference in representation in categories of Item_Fat_Content variable. This can be corrected as:Now it makes more sense. But hang on, in step 4 we saw there were some non-consumables as well and a fat-content should not be specified for them. So we can also create a separate category for such kind of observations.Since scikit-learn accepts only numerical variables, I converted all categories of nominal variables into numeric types. Also, I wanted Outlet_Identifier as a variable as well. So I created a new variable Outlet same as Outlet_Identifier and coded that. Outlet_Identifier should remain as it is, because it will be required in the submission file.Lets start with coding all categorical variables as numeric using LabelEncoder from sklearns preprocessing module.One-Hot-Coding refers to creating dummy variables, one for each category of a categorical variable. For example, the Item_Fat_Content has 3 categories  Low Fat, Regular and Non-Edible. One hot coding will remove this variable and generate 3 new variables. Each will have binary numbers  0 (if the category is not present) and 1(if category is present). This can be done using get_dummies function of Pandas.Lets look at the datatypes of columns now:Here we can see that all variables are now float and each category has a new variable. Lets look at the 3 columns formed from Item_Fat_Content.You can notice that each row will have only one of the columns as 1 corresponding to the category in the original variable.Final step is to convert data back into train and test data sets. Its generally a good idea to export both of these as modified data sets so that they can be re-used for multiple sessions. This can be achieved using following code:With this we come to the end of this section. If you want all the codes for exploration and feature engineering in an iPython notebook format, you can download the same from my GitHub repository.Now that we have the data ready, its time to start making predictive models. I will take you through 6 models including linear regression, decision tree and random forest which can get you into Top 20 ranks in this competition (I mean ranks as of today because after reading this article, Im sure many new leaders will emerge).Lets start by making a baseline model. Baseline model is the one which requires no predictive model and its like an informed guess. For instance, in this case lets predict the sales as the overall average sales. This can be done as:Public Leaderboard Score: 1773Seems too naive for you? If you look at the public LB now, youll find 4 players below this number. So making baseline models helps in setting a benchmark. If your predictive algorithm is below this, there is something going seriously wrong and you should check your data.If you participated in AV datahacks or other short duration hackathons, youll notice first submissions coming in within 5-10 mins of data being available. These are nothing but baseline solutions and no rocket science.Taking overall mean is just the simplest way. You can also try:These should give better baseline solutions.Since Ill be making many models, instead of repeating the codes again and again, I would like to define a generic function which takes the algorithm and data as input and makes the model, performs cross-validation and generates submission. If you dont like functions, you can choose the longer way as well. But I have a tendency of using functions a lot (actually I over-use sometimes :D). So here is the function:Ive put in self-explanatory comments. Please feel free to discuss in comments if you face difficulties in understanding the code. If youre new to the concept of cross-validation, read more about ithere.Lets make our first linear-regression model. Readmore on Linear Regression here.Public LB Score: 1202We can see this is better than baseline model. But if you notice the coefficients, they are very large in magnitude which signifies overfitting. To cater to this, lets use a ridge regression model. You should read this article if you wish to learn more about Ridge & Lasso regression techniques.Public LB Score: 1203Though the regression coefficient look better now, the score is about the same. You can tune the parameters of the model for slightly better results but I dont think there will be a significant improvement. Even the cross-validation score is same so we cant expect way better performance.Lets try out a decision tree model and see if we get something better.Public LB Score: 1162Here you can see that the RMSE is 1058 and the mean CV error is 1091. This tells us that the model is slightlyoverfitting. Lets try making a decision tree with just top 4 variables, a max_depth of 8 and min_samples_leaf as 150.Public LB Score: 1157You can fine tune the model further using other parameters. Ill leave this to you.Lets try a random forest model as well and see if we get some improvements. Read more about random forest here.Public LB Score: 1154You might feel this is a very small improvement but as our model gets better, achieving even minute improvements becomes exponentially difficult. Lets try another random forest with max_depth of 6 and 400 trees. Increasing the number of trees makes the model robust but is computationally expensive.LB Score: 1152Again this is an incremental change but will help you get a jump of 5-10 ranks on leaderboard. You should try to tune the parameters further to get higher accuracy. But this is good enough to get you into the top 20 on the LB as of now. I tried a basic GBM with little tuning and got into the top 10. I leave it to you to refine with score with better algorithms like GBM and XGBoost and try ensemble techniques.With this we come to the end of this section. If you want all the codes for model buildingin an iPython notebook format, you can download the same from myGitHub repository.This article took us through the entire journey of solving a data science problem. We started with making some hypothesis about the data without looking at it. Then we moved on to data explorationwherewe found out some nuances in the data which required remediation. Next, we performed data cleaning andfeature engineering, where we imputed missing values and solved otherirregularities, made new features and also made the data model-friendly by one-hot-coding. Finally we made regression, decision tree and random forest model and got a glimpse of how to tune them for better results.I believe everyone reading this article should attain a good score in BigMart Sales now. For beginners, you should achieve at least a score of 1150 and for the ones already on the top, you can use some feature engineering tips from here to go further up. All the best to all!Did you find this article useful? Couldyou makesome more interesting hypothesis? What other features did you create? Were you able to get a better score with GBM & XGBoost?Feel free to discuss your experiences in comments below or on the discussion portal and well be more than happy to discuss.",https://www.analyticsvidhya.com/blog/2016/02/bigmart-sales-solution-top-20/
"Business Analyst  Bangalore (2-3 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Approach and Solution to break in Top 20 of Big Mart Sales prediction|Step by step guide to building sentiment analysis model using graphlab|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Business AnalystLocation  BangaloreAbout employer  ConfidentialDescriptionQualification and Skills RequiredKey Skills: Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asBusiness Analyst  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/02/business-analyst-bangalore-2-3-years-experience/
Step by step guide to building sentiment analysis model using graphlab,Learn everything about Analytics,"Problem Statment|Lets get started|End Notes|If you like what you just read & want to continue youranalytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Business Analyst  Bangalore (2-3 years of experience)|What I learnt about Time Series Analysis in 3 hour Mini DataHack?|
Tavish Srivastava
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have been using graph lab for quite some time now. The first Kaggle competition I used it for was Click Trough Rate (CTR) and I was amazed to see the speed at which it can crunch such big data. Over last few months, I have realised much broader applications of GraphLab. In this article I will take up the text mining capability of GraphLab and solve one of the Kaggle problems. I will be referring to this problem with a small tweak. The problem will classify the sentiment of each phrase into one of the 5 buckets. For simplicity, I will convert the sentiment into two category variable (with or without sentiment more than 3). This way I will be able to use all the classification algorithm of GraphLab in this exercise.If you are completely new to GraphLab, I will recommend you to read this article before doing this exercise. Lets look at the problem statement first.This problem is a very advanced problem of sentiment analysis, because it focuses to capture not so obvious trends. For instance sarcasm or frustration etc. And we do it by breaking down the sentence. For instance, if I say The movie was OK but not that awesome . What do you think is the sentiment of this sentence. It looks neutral as it was expected to be awesome but it was just fine. Now try this The movie was OK. Now the meaning of the sentence is quite different as now we dont know what was author expecting of the movie. This problem gives many sentences broken down to phrases and sentiments of these phrases. This dataset just enables you to create your own sentiment miner algorithm. Here is how the problem reads on Kaggle Lets load the data and GraphLab.The entire data set loads in less than 1 minute, which is amazing compared to 7 minutes on my R setup.Now its time to convert sentiment into a two class flag variable.Now we start with the text mining on these phrases.Now that we have our first model ready, lets evaluate our model on various metrics.Now, we will try a few more advanced techniques.And we evaluate each of these models in the same way we did for logistic.As you can observe from all the evaluations, with accuracy as the primary metric, GBM comes out to be the best model on the test data.My journey with GraphLab has been amazing till date. I will be soon publishing another article with Photo classification using GraphLab which I am currently working on.Did you like this article? Have you used GraphLab before? If yes, how was the experience. Share with us applications you have used the tool GraphLab for.",https://www.analyticsvidhya.com/blog/2016/02/step-step-guide-building-sentiment-analysis-model-graphlab/
What I learnt about Time Series Analysis in 3 hour Mini DataHack?,Learn everything about Analytics|What was Mini DataHack?|Action packed 3 hours|Learnings from Mini DataHack,"Approach from SRK:|Note from Vopani:|End Note:|Share this:|Like this:|Related Articles|Step by step guide to building sentiment analysis model using graphlab|A comprehensive beginners guide to create a Time Series Forecast (with Codes in Python and R)|
Aarshay Jain
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Last weekend, I participated in the Mini DataHack by Analytics Vidhya and I learnt more about Time Series in those 3 hours than I did by spending many hours leading up to the event. Hence, I thought I will share my learnings with all of you.In short, Analytics Vidhya came up with an idea to shorten up their Signature hackathons and the result was Mini DataHack. It was basically a 3 hour hackathon, where the problem area was released upfront. The philosophy behind this Mini Hackathon was to provide a power pack of learnings on a focused area in a short duration.My preparationsSince it was already decided that the problem would be about Time Series  I made sure I was well equipped with knowledge and packages about Time Series. Infact, I even wrote a guide to Time Series in Python.If AV Signature hackathon is equivalent to an ODI in cricket, Mini DataHack was like a T20 match  shorter, action packed and full of twists!Close to 900 people registered for the Mini DataHack  a very high number, given that it was floated only 6 days before the event! It was a very intense competition from the word GO. Vopani made first submission in under 5 minutes and SRK (who won the competition) was not on top till a few minutes before the finish time.Honestly, it was very difficult to guess the outcome until it happened.Needless to say, I learnt a lot about Time series in these 3 hours. Here is a brief summary ofmy learnings:I used both xgboost and linear regression to get to my final score.Variables used in the models are:
1. Day of the month
2. Hour of the day
3. Day of the week
4. Ordinal date (Number of days from January 1 of year 1)
At first, I plotted the DV using a scatter plot and here were some of my observations:I then trained a xgboost model on the full dataset which I think helped to capture the overall trend of all these input variables. This is the one which scored a rmse of 139 on the public LB. But since xgboost is a space splitting algorithm, I thought it wont be able to capture the increasing trend and so it may not be able to extrapolate the same in test set.
So, I decided to run a linear regression model to capture the increasing trend. Since the initial part has a different pattern compared to the later part, training the linear regression model only on the later part of the training set made more sense to me and I did that. This one scored a rmse of about 182 on the public LB.
I averaged both of these models and made the final submission which scored 155 rmse in public LB and 196 rmse in the Private LB.
One more inference from the modeling is that:
Including month of the year, week of the year variables in XGB gave good results in public LB. But when I checked the plot of predicted counts in test set, it took a dip after a certain time period due to the way in which the xgb captures the information. So including these variables might give a good public LB score but most probably will not give a good private LB score. So I dropped these variables while building the models.
Codes are present in my github and the link is
https://github.com/SudalaiRajkumar/ML/tree/master/AV_MiniHack1
I think each word in the approach above can be weighed in Gold! A natural question coming to my mind was How can XGBoost perform better than Time Series methods? And here is what Vopani added:Im not surprised XGB and linear models performed so well. I tried out a lot of models and found XGB far superior than any other.
Ive had a good exposure to time series problems since I worked on many such projects, and in almost all of them I converted the problem into a structure which would fit any supervised algorithm, like what was done here by most people, including SRK and me.
Its no fault of the dataset, its just that XGB is way too clever and powerful, and is able to capture linear and seasonal trends pretty well with the basic date features.
A real time-series challenge is one where the values are given in order without the date variable. Then, you cant really use an XGB-type model and thats when the power of the ARIMA-type models comes into the picture.
Unfortunately, its pointless keeping out the date variable since there is a lot of useful information there which can boost accuracy and hence, ultimately, XGB ends up the winner.In Summary:I learnt a lotfrom participating in this Mini DataHack and I cant help wanting more of these! I hope AV comes up with the next action packed weekend soon!",https://www.analyticsvidhya.com/blog/2016/02/hand-learn-time-series-3-hours-mini-datahack/
A comprehensive beginners guide to create a Time Series Forecast (with Codes in Python and R)|Overview|Introduction|1. What makes Time Series Special?|2. Loading and Handling Time Series in Pandas|3. How to Check Stationarity of a Time Series?|4. How to make a Time Series Stationary?|5. Forecasting a Time Series|End Notes,Learn everything about Analytics|Estimating & Eliminating Trend|Time Series Forecast in R|Projects,"Moving average|Eliminating Trend and Seasonality|AR Model|MA Model|Combined Model|Taking it back to original scale|Step 1: Reading data and calculating basic summary||Output||Step 2: Checking the cycle of Time Series Data and Plotting the Raw Data|Output|Step 3: Decomposing the time series data|Output|Step 4: Test the stationarity of data|Output||Step 5: Fitting the model||Output||Step 6: Forecasting|Output|Note  The discussions of this article are going on at AVs Discuss portal.Join here!|Share this:|Related Articles|What I learnt about Time Series Analysis in 3 hour Mini DataHack?|Mini DataHack and the tactics of the three Last Man Standing!|
Aarshay Jain
|72 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Differencing|Decomposing,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time Series (referred as TS from now) is considered to be one of the less known skills in the data science space (Even I had little clue about it a couple of days back). I set myself on a journey to learn the basic steps for solving a Time Series problem and here I am sharing the same with you. These will definitely help you get a decent model in any future project you take up!complete guide to create a time series forecast with pythonBefore going through this article, I highly recommend reading A Complete Tutorial on Time Series Modeling in Rand taking the free Time Series Forecasting course. It focuses on fundamental concepts and I will focus on using these concepts in solving a problem end-to-end along with codes in Python. Many resources exist for time series in R but very few are there for Python so Ill be using Python in this article.Our journey would go through the following steps:As the name suggests, TS is a collection of data points collected at constant time intervals. These are analyzed to determine the long term trend so as to forecast the future or perform some other form of analysis. But what makes a TS different from say a regular regression problem? There are 2 things:Because of the inherent properties of a TS, there are various steps involved in analyzing it. These are discussed in detail below. Lets start by loading a TS object in Python. Well be using the popular AirPassengers data set which can be downloaded here.Please note that the aim of this article is to familiarize you with the various techniques used for TS in general. The example considered here is just for illustration and I will focus on coverage a breadth of topics and not making a very accurate forecast.Pandas has dedicated libraries for handling TS objects, particularly the datatime64[ns]class which stores time information and allows us to perform some operations really fast. Lets start by firing up the required libraries:Lets understand the arguments one by one:Now we can see that the data has time object as index and #Passengers as the column. We can cross-check the datatype of the index with the following command:Notice the dtype=datetime[ns] which confirms that it is a datetime object.As a personal preference, I would convert the column into a Series object to prevent referring to columns names every time I use the TS. Please feel free to use as a dataframe is that works better for you.ts = data[#Passengers]
ts.head(10)Before going further, Ill discuss some indexing techniques for TS data. Lets start by selecting a particular value in the Series object. This can be done in following 2 ways:Both would return the value 112 which can also be confirmed from previous output. Suppose we want all the data upto May 1949. This can be done in 2 ways:Both would yield following output:There are 2 things to note here:Consider another instance where you need all the values of the year 1949. This can be done as:The month part was omitted. Similarly if you all days of a particular month, the day part can be omitted.Now, lets move onto the analyzing the TS.A TS is said to be stationary if its statistical properties such as mean, variance remain constant over time. But why is it important? Most of the TS models work on the assumption that the TS is stationary. Intuitively, we can sat that if a TS has a particular behaviour over time, there is a very high probability that it will follow the same in the future. Also, the theories related to stationary series are more mature and easier to implement as compared to non-stationary series.Stationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:Ill skip the details as it is very clearly defined in this article. Lets move onto the ways of testing stationarity. First and foremost is to simple plot the data and analyze visually. The data can be plotted using following command:It is clearly evident that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences (well see such cases later). So, more formally, we can check stationarity using the following:These concepts might not sound very intuitive at this point. I recommend going through the prequel article. If youre interested in some theoretical statistics, you can refer Introduction to Time Series and Forecastingby Brockwell and Davis. The book is a bit stats-heavy, but if you have the skill to read-between-lines, you can understand the concepts and tangentially touch the statistics.Back to checking stationarity, well be using the rolling statistics plots along with Dickey-Fuller test results a lot so I have defined a function which takes a TS as input and generated them for us. Please note that Ive plotted standard deviation instead of variance to keep the unit similar to mean.The code is pretty straight forward. Please feel free to discuss the code in comments if you face challenges in grasping it.Lets run it for our input series:Though the variation in standard deviation is small, mean is clearlyincreasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values. Note that the signed values should be compared and not the absolute values.Next, well discuss the techniques that can be used to take this TS towards stationarity.Though stationarity assumption is taken in many TS models, almost none of practical time series are stationary. So statisticians have figured out ways to make series stationary, which well discuss now. Actually, its almost impossible to make a series perfectly stationary, but we try to take it as close as possible.Lets understand what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS:
1. Trend  varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.
2. Seasonality  variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.The underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.Note: Ill be discussing a number of methods. Some might work well in this case and others might not. But the idea is to get a hang of all the methods and not focus on just the problem at hand.Lets start by working on the trend part.One of the first tricks to reduce trend can be transformation. For example, in this case we can clearly see that the there is a significant positive trend. So we can apply transformation which penalize higher values more than smaller values. These can be taking a log, square root, cube root, etc. Lets take a log transform here for simplicity:In this simpler case, it is easy to see a forward trend in the data. But its not very intuitive in presence of noise. So we can use some techniques to estimate or model this trend and then remove it from the series. There can be many ways of doing it and some of most commonly used are:I will discuss smoothing here and you should try other techniques as well which might work out for other problems. Smoothing refers to taking rolling estimates, i.e. considering the past few instances. There are can be various ways but I will discuss two of those here.In this approach, we take average of k consecutive values depending on the frequency of time series. Here we can take the average over the past 1 year, i.e. last 12 values. Pandas has specific functions defined for determining rolling statistics.The red line shows the rolling mean. Lets subtract this from the original series. Note that since we are taking average of last 12 values, rolling mean is not defined for first 11 values. This can be observed as:Notice the first 11 being Nan. Lets drop these NaN values and check the plots to test stationarity.This looks like a much better series. The rolling values appear to be varying slightly but there is no specific trend. Also, the test statistic is smaller than the 5% critical values so we can say with 95% confidence that this is a stationary series.However, adrawback in this particular approach is that the time-period has to be strictly defined.In this casewe can take yearly averages but in complex situations like forecasting a stock price, its difficult to come up with a number. So we take a weighted moving average where more recent values are given a higher weight. There can be many technique for assigning weights.A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor. Find details here. This can be implemented in Pandas as:Note that here the parameter halflife is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain. Other parameters like span and center of mass can also be used to define decay which are discussed in the link shared above. Now, lets remove this from series and check stationarity:This TS has even lesser variations in mean and standard deviation in magnitude. Also, the test statistic is smaller than the 1% critical value, which is better than the previous case.Note that in this case there will be no missing values as all values from starting are given weights. So itll work even with no previous values.Thesimple trend reduction techniques discussed before dont work in all cases, particularly the ones with high seasonality. Lets discuss two ways of removing trend and seasonality:One of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the observation at a particular instant with that at the previous instant. This mostly works well in improving stationarity. First order differencing can be done in Pandas as:This appears to have reduced trend considerably. Lets verify using our plots:We can see that the mean and std variations have small variations with time. Also, the Dickey-Fuller test statistic is less than the 10% critical value, thus the TS is stationary with 90% confidence. We can alsotake second or third order differences which might get even better results in certain applications. I leave it to you to try them out.In this approach, both trend and seasonality are modeled separately and the remaining part of the series is returned. Ill skip the statistics and come to the results:Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:The Dickey-Fuller test statistic is significantly lower than the 1% critical value. So this TS is very close to stationary. You can try advanced decomposition techniques as well which can generate better results. Also, you should note that converting the residuals into original values for future data in not very intuitive in this case.We saw different techniques and all of them worked reasonably well for making the TS stationary. Lets make model on theTS after differencing as it is a very popular technique. Also, itsrelatively easier to add noise and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:Let me give you a brief introduction to ARIMA. I wont go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:An importance concern here is how to determine the value of p and q. We use two plots to determine these numbers. Lets discuss them first.The ACF and PACF plots for the TS after differencing can be plotted as:In this plot, the two dotted lines on either sides of 0 are the confidence interevals. These can be used to determine the p and q values as:Now, lets make 3 different ARIMA models considering individual as well as combined effects. I will also print the RSS for each. Please note that here RSS is for the values of residuals and not actual series.We need to load the ARIMA model first:The p,d,q values can be specified using the order argument of ARIMA which take a tuple (p,d,q). Let model the 3 cases:Here we can see that the AR and MA models have almost the same RSS but combined is significantly better. Now, we are left with 1 last step, i.e. taking these values back to the original scale.Since the combined model gave best result, lets scale it back to the original values and see how well it performs there. First step would be to store the predicted results as a separate series and observe it.Notice that these start from 1949-02-01 and not the first month. Why? This is because we took a lag by 1 and first element doesnt have anything before it to subtract from. The way to convert the differencing to log scale is to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. The cumulative sum can be found as:You can quickly do some back of mind calculations using previous output to check if these are correct. Next weve to add them to base number. For this lets create a series with all values as base number and add the differences to it. This can be done as:Here the first element is base number itself and from thereon the values cumulatively added. Last step is to take the exponent and compare with the original series.Thats all in Python. Well, lets learn how to implement a time series forecast in R.the p-value is 0.01 which is <0.05, therefore, we reject the null hypothesis and hence time series is stationary.The maximum lag is at 1 or 12 months, indicates a positive relationship with the 12-month cycle.Autoplot the random time series observations from 7:138 which exclude the NA valuesFinally we have a forecast at the original scale. Not a very good forecast I would say but you got the idea right? Now, I leave it upto you to refine the methodology further and make a better solution.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Test the techniques discussed in this post and accelerate your learning in Time Series Analysis with the following Practice Problems:Through this article I have tried to give you a standard approach for solving time series problem. This couldnt have come at a better time as today is our Mini DataHack which will challenge you to solve a similar problem. Weve covered concepts of stationarity, how to take a time series closer to stationarity and finally forecasting the residuals. It was a long journey and I skipped some statistical details which I encourage you to refer using the suggested material. If you dont want to copy-paste, you can download the iPython notebook with all the codes from my GitHub repository.I hope this article will help you achieve a good first solution today. All the best guys!Did you like the article? How helpful was it in the hackathon today? Somethings bothering you which you wish to discuss further? Please feel free to post a comment and Ill be more than happy to discuss.",https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/
Mini DataHack and the tactics of the three Last Man Standing!,Learn everything about Analytics|Introduction:|Launching Mini DataHack:|Last man standing:|End Notes,"Problem Statement:|Evaluation Metric:|Winning strategies from the competition:|Rank 3: Mark Landry|Rank 2: Vopani|Rank 1: Bishwarup Bhattacharjee|Takeaways from Last Man Standing|Highlights of the hackathon:|Updated user rankings:|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|A comprehensive beginners guide to create a Time Series Forecast (with Codes in Python and R)|Solution Expert (Analytics- Solutions-Delivery Manager)  Noida (3-6 years of experience)|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",The Toxic Pesticides,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"February started on a high for us. Last Man Standing saw more than1600 Data Scientists compete from all over the world making more than5000 submissions in 3 days. Depending on the metric you look at, this was 40  60% higher participation and engagement compared to our previous hackathon  Black Friday.Guess what  we are loving this high and are in no mood to come off the peak! We love the action over the weekend so much that we feel a void with out it. I know some of you also feel about hackathons in similar way and would love what I am going to announce next!There are 2 aspects of our hackathons which people love about our hackathons:So, this time we are going to take both these aspects one notch up!I am pleased to launch Mini DataHack, our shortest form of hackathon ever. The idea is very simple  we will design a problem, which would be focused on solving a type of problem. For example, the first Mini DataHack would be featuring a Time Series problem. By focusing on a single aspect and shortening the duration, we hope that the learning will grow tremendously!We hope that this experience will give you more learning than you can imagine in 3 hours. So, make sure you register for our first ever Mini DataHack and go through the concepts of Time Series before hand.One of the feedbackwe heard from people during our last hackathons was that feature engineering wasnt playing an important role in improving the score. Most of the time it was just the improvements coming from algorithms. We wanted to change this score this time. Hence, we designed a problem, where the key to winning would lie in feature engineering.Though, many of us dont appreciate much, but a farmers job is real test of endurance and determination. Once the seeds are sown, he works days and nights to make sure that he cultivates a good harvest at the end of season. A good harvest is ensured by several factors such as availability of water, soil fertility, protecting crops from rodents, timely use of pesticides & other useful chemicals and nature. While a lot of these factors are difficult to control for, the amount and frequency of pesticides is something the farmer can control.Pesticides are also special, because while they protect the crop with the right dosage. But, if you add more than required, they may spoil the entire harvest. A high level of pesticide can deem the crop dead / unsuitable for consumption among many outcomes. This data is based on crops harvested by various farmers at the end of harvest season. To simplify the problem, you can assume that all other factors like variations in farming techniques have been controlled for.You need to determine the outcome of the harvest season, i.e. whether the crop would be healthy (alive), damaged by pesticides or damaged by other reasons.Evaluation metrics for this challenge was Confusion_Matrix. A confusion matrix is an N X N matrix, where N is the number of classes being predicted. Here are a few definitions, you need to remember for a confusion matrix :Mark says:I went fora vectorized way to calculate the difference between the current value of any feature and the one that was N before it or after it. I used 1-6 on both sides (+ and -) for the insect and dose features (for 24 total). Then I also added a few of those together (diffOfThisandThis+1 + diffOfThisandThis+2). Those started to overfit a tiny bit.I really didnt see what the models were doing to solve the problem, but once they were calculated, the H2O GBMs I was using started getting much more accurate. I noticed it because if you look at the insect feature, you can see a pattern of a similar value repeated several times, and sometimes it varies backwards just a tiny bit. Again, I cant claim to have seen how that helps solve the problemthat part really was ML for my model. My job was to get the features in there.Link to the CodeVopani says:I particularly enjoyed creating the features and seeing it steadily improve the CV and LB. I found the time-series pattern pretty much straight away and from there, it was only uphill.
You can go through the code and get some ideas.This model scored 0.9604 on the public LB and was ranked 2nd.Certain tips I followed:-Link to CodeBishwarup says:ThiswasmyfirstcompetitioninAVandIthoroughlyenjoyedworkingonit.Thedatawasquitestandard anddidntdemandmuchofpreprocessing.Istartedwithanxgboostmodelwithafewengineeredfeatures:The first three didnt offer any improvement but the rest four of the features helped my model to some extent and at that point my LB was around 0.849.I tried tuning the hyper parameters with a 5-fold stratified CV, but that also didnt help much in my case. One more thing that didnt work out is one-hot-encoding the nominal features like Season and Pesticide_Use_Category.
I also tried Keras for my NN model, which gave me a CV of 0.8414 and LB around 0.842.Being stuck at this point, I went back to exploring the data and started searching for potential signals for engineering more features. While scanning the data carefully I came across the fact that the data consists of multiple batches of similar estimated insect counts and the batches are strictly non-decreasing in the response values. There were one or more fixed patterns present in the data which are as below:Here are a few key learning I would emphasize from the competition:Here are a few visualizations which were shared on the slack channel during the competitionWhile the entire experience was enriching, here are a couple of highlights for those who missed the action:If you havent noticed, the ranks and badges have been updated. Here is how the points of top 5 users stack up pre and post Last Man Standing (LMS):Aayushmnit & SRK are now neck to neck for overall ranking. Also, Vopani has jumped from rank 15 to rank 6. You can see the updated rankings here.I hope you learnt a lot from this hackathon. I want to thank all the participants and the community members for the success of this hackathon and would see you around the next innovation tomorrow  Mini DataHack.The next Hackathons are LIVE now for registrations. Participate Now!Short Hackathon: Mini DataHackSignature Hackathon: Date Your Data",https://www.analyticsvidhya.com/blog/2016/02/secrets-winners-signature-hackathon-last-man-standing/
"Solution Expert (Analytics- Solutions-Delivery Manager)  Noida (3-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Mini DataHack and the tactics of the three Last Man Standing!|Data Scientist  Noida (1 to 3 Years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Solution Expert (Analytics- Solutions-Delivery Manager)Location  NoidaAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredPreferred CandidateInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Solution Expert (Analytics- Solutions-Delivery Manager)  Noida and the following details:,https://www.analyticsvidhya.com/blog/2016/02/analytics-delivery-manager-noida-3-5-years-experience/
"Data Scientist  Noida (1 to 3 Years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Solution Expert (Analytics- Solutions-Delivery Manager)  Noida (3-6 years of experience)|Launching learning path to master D3.js|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Data ScientistLocation  NoidaAbout employer  ConfidentialDescriptionData Science team is responsible for providing decision insights to business users using data mining, predictive analytics and machine learning techniques. Team works closely with business executives across segments and geographies in understanding business objectives and designing analytical solutions to meet business goals. Candidate will be expected to achieve following goals.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Data Scientist  Noida and the following details:",https://www.analyticsvidhya.com/blog/2016/02/data-scientist-noida-1-3-years-experience/
Launching learning path to master D3.js,Learn everything about Analytics,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Noida (1 to 3 Years of experience)|How to use Multinomial and Ordinal Logistic Regression in R ?|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We launched our learning paths last year and to say the least, they were a runaway hit!The aim of these learning paths is to take away confusion in learning for newbies. If you havent checked them out and are learning a tool / technique, you should definitely check them out.Today, I am pleased to announce the launch of learning path to master D3.js. For those of you who dont know about D3.js, D3.js is a tool to create interactive data stories over the web.So, go out and check what it takes to learn D3.js.",https://www.analyticsvidhya.com/blog/2016/02/newbie-expert-complete-learning-path-master-d3-js/
How to use Multinomial and Ordinal Logistic Regression in R ?,Learn everything about Analytics|Introduction|What is Multinomial Regression ?|How does Multinomial Regression works ?|What is Ordinal Regression ?|Examples|Multinomial Logistic Regression (MLR) in R|Ordinal Logistic Regression (OLR) in R|End Notes,"Case 1 (Multinomial Regression)|Case 2 (Ordinal Regression)|Interpretation|Interpretation|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Launching learning path to master D3.js|A Complete Tutorial on Ridge and Lasso Regression in Python|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most of us have limited knowledge of regression. Of which, linear and logistic regression are our favorite ones. As an interesting fact, regressionhas extendedcapabilities to deal with different types of variables. Do you know, regression has provisions for dealing with multi-level dependent variables too? Im sure, you didnt. Neither did I. Until I was pushed to explore this aspect of Regression.For multi-level dependent variables, there are many machine learning algorithms which can do the job for you; such as naive Bayes, decision tree, random forest etc. For starters,these algorithm can be a bitdifficult to understand. But, if you very well understand logistic regression, mastering this new aspect of regression should be easy for you!In this article, Ive explained the method of using multinomial and ordinal regression. Also, for practical purpose, Ive demonstrated this algorithm in a step wise fashion in R. This article draws inspiration from a detailed article here. I have added my own take on it.Note: This article is best suited for R users having prior knowledge of logistic regression. However, if you use python, you can still get a overall understanding of this regression method.Multinomial Logistic Regression (MLR) is a form of linear regression analysis conductedwhen the dependent variable is nominal with more than two levels. It is used to describe data and to explain the relationship between one dependent nominal variable and one or more continuous-level (interval or ratio scale) independent variables. You can understand nominal variable as, a variable which has no intrinsic ordering.For example: Types of Forests: Evergreen Forest, Deciduous Forest, Rain Forest. As you see, there is no intrinsic order in them, but each forest represent a unique category.In other words,multinomial regression isan extension of logistic regression, which analyzes dichotomous (binary) dependents.The multinomial logistic regression estimates a separate binary logistic regression model for each dummy variables. The result is M-1 binary logistic regression models. Each modelconveysthe effect of predictors on the probability of success in that category, in comparison to the reference category.Each model has its own intercept and regression coefficientsthe predictors can affect each category differently.Lets compare this part with our classics  Linear and Logistic Regression.Standard linear regression requires the dependent variable to be of continuous-level (interval or ratio) scale. However, logistic regression jumps the gap by assuming that the dependent variable is a stochastic event. And the dependent variable describes the outcome of this stochastic event with a density function (a function of cumulated probabilities ranging from 0 to 1). Statisticians then argue one event happens if the probability is less than 0.5 and the opposite event happens when probability is greater than 0.5.Now we know thatMLR extends the binary logistic model to a model with numerous categories(in dependent variable). However, it has one limitation. The category to which an outcome belongs to, does not assume any order in it. For example, if we have N categories, all have an equal probability. In reality, we come across problems where categories have a natural order.So, what to do when we have a natural order in categories of dependent variables ? In such situation, Ordinal Regression comes to our rescue.Ordinal Regression ( also known as Ordinal Logistic Regression) is another extension of binomial logistics regression. Ordinal regression is used to predict thedependent variablewithordered multiple categories and independent variables. In other words, it is used to facilitate the interaction of dependent variables (having multiple ordered levels) with one or more independent variables.For example: Let us assume a survey is done. We asked a question to respondent where their answer lies between agree or disagree. The responses thus collected didnt help us to generalize well.Later, we added levels to our responses such asStrongly Disagree, Disagree, Agree, Strongly Agree.This helped us toobserve a natural order in the categories. For our regression model to be realistic, we must appreciate this order instead of being naiveto it, as in the case of MLR. Ordinal Logistic Regression addresses this fact. Ordinal means order of the categories.Before we perform these algorithm in R, lets ensure that we have gained a concrete understanding using the cases below:The modeling of program choices made by high school students can be done using Multinomial logit. The program choices are general program, vocational program and academic program. Their choicecanbe modeled using their writing score and their social economic status.Based on a variety of attributes such as social status, channel type, awards and accoladesreceived by the students, gender, economic status and how well they are able to read and write in the subjects given, the choice on the type of program can be predicted. Choice of programs with multiple levels (unordered) is the dependent variable. This case is suited for usingMultinomial Logistic Regression technique.A study looks at factors whichinfluence the decision of whether to apply to graduate school. College juniors are asked if they are unlikely, somewhat likely, or very likely to apply to graduate school. Hence, our outcome variable has three categories i.e. unlikely, somewhat likely and very likely.Data on parental educational status, class of institution (private or state run), current GPA arealso collected. The researchers have reason to believe that the distances between these three points are not equal. For example, the distance between unlikely and somewhat likely may be shorter than the distance between somewhat likely and very likely. In such case, well use Ordinal Regression.Read the file> library(""foreign"")
> ml <- read.dta(""http://www.ats.ucla.edu/stat/data/hsbdemo.dta"")Re-leveling data> head(ml)## id female ses schtyp prog read write math science socst
 ## 1 45 female low public vocation 34 35 41 29 26
 ## 2 108 male middle public general 34 33 41 36 36
 ## 3 15 male high public vocation 39 39 44 26 42
 ## 4 67 male low public vocation 37 37 42 33 32
 ## 5 153 male middle public vocation 39 31 40 39 51
 ## 6 51 female high public general 42 36 42 31 39
 ## honors awards cid
 ## 1 not enrolled 0 1
 ## 2 not enrolled 0 1
 ## 3 not enrolled 0 1
 ## 4 not enrolled 0 1
 ## 5 not enrolled 0 1
 ## 6 not enrolled 0 1> ml$prog2 <- relevel(ml$prog, ref = ""academic"")Now well execute a multinomial regression with two independent variable.> library(""nnet"")
> test <- multinom(prog2 ~ ses + write, data = ml)## # weights: 15 (8 variable)
 ## initial value 219.722458 
 ## iter 10 value 179.982880
 ## final value 179.981726 
 ## converged> summary(test)## Call:
 ## multinom(formula = prog2 ~ ses + write, data = ml)
 ## 
 ## Coefficients: 
 ## (Intercept) sesmiddle seshigh write
 ## general 2.852198 -0.5332810 -1.1628226 -0.0579287
 ## vocation 5.218260 0.2913859 -0.9826649 -0.1136037
 ## 
 ## Std. Errors:
 ## (Intercept) sesmiddle seshigh write
 ## general 1.166441 0.4437323 0.5142196 0.02141097
 ## vocation 1.163552 0.4763739 0.5955665 0.02221996
 ## 
 ## Residual Deviance: 359.9635 
 ## AIC: 375.96351. Model execution output shows some iteration history and includes the final negative log-likelihood 179.981726. This value is multiplied by two as shown in the model summary as the Residual Deviance.2. The summary output has a block of coefficients and another block of standard errors. Each blocks has one row of values corresponding to one model equation. In the block of coefficients, we see that the first row is being compared to prog = generalto our baselineprog = academicand the second row toprog = vocationto our baselineprog = academic.3. A one-unit increase in write decreases the log odds of being in general program vs. academic program by 0.05794. A one-unit increase in write decreases the log odds of being in vocation program vs. academic program by 0.11365. The log odds of being in general program than in academic program will decrease by 1.163 if moving from ses=low to ses=high.6. On the other hand, Log odds of being in general program than in academic program will decrease by 0.5332 if moving from ses=low to ses=middle7. The log odds of being in vocation program vs. in academic program will decrease by 0.983 if moving from ses=low to ses=high8. The log odds of being in vocation program vs. in academic program will increase by 0.291 if moving from ses=low to ses=middleNow well calculate Z score and p-Value for the variables in the model.> z <- summary(test)$coefficients/summary(test)$standard.errors
> z## (Intercept) sesmiddle seshigh write
 ## general 2.445214 -1.2018081 -2.261334 -2.705562
 ## vocation 4.484769 0.6116747 -1.649967 -5.112689> p <- (1 - pnorm(abs(z), 0, 1))*2
> p## (Intercept) sesmiddle seshigh write
 ## general 0.0144766100 0.2294379 0.02373856 6.818902e-03
 ## vocation 0.0000072993 0.5407530 0.09894976 3.176045e-07> exp(coef(test))## (Intercept) sesmiddle seshigh write
 ## general 17.32582 0.5866769 0.3126026 0.9437172
 ## vocation 184.61262 1.3382809 0.3743123 0.8926116The p-Value tells us that ses variables are not significant. Now well explore the entire data set, and analyze if we can remove any variables which do not add to model performance.> names(ml)## [1] ""id"" ""female"" ""ses"" ""schtyp"" ""prog"" ""read"" ""write"" 
 ## [8] ""math"" ""science"" ""socst"" ""honors"" ""awards"" ""cid"" ""prog2""> levels(ml$female)## [1] ""male"" ""female""> levels(ml$ses)## [1] ""low"" ""middle"" ""high""> levels(ml$schtyp)## [1] ""public"" ""private""> levels(ml$honors)## [1] ""not enrolled"" ""enrolled""Lets now build a multinomial model on the entire data set after removing id and prog variables.> test <- multinom(prog2 ~ ., data = ml[,-c(1,5,13)])## # weights: 39 (24 variable)
 ## initial value 219.722458 
 ## iter 10 value 178.757016
 ## iter 20 value 155.866327
 ## iter 30 value 154.365307
 ## final value 154.365305 
 ## converged> summary(test)## Call:
 ## multinom(formula = prog2 ~ ., data = ml[, -c(1, 5, 13)])
 ## 
 ## Coefficients:
 ## (Intercept) femalefemale sesmiddle seshigh schtypprivate
 ## general 5.692368 0.1547445 -0.2809824 -0.9632924107 -0.5872049
 ## vocation 9.839107 0.4076641 1.2246933 0.0008659972 -1.9089941
 ## read write math science socst
 ## general -0.04421300 -0.05434029 -0.1001477 0.10397170 -0.02486526
 ## vocation -0.04124332 -0.05149742 -0.1209839 0.06341246 -0.07012002
 ## honorsenrolled awards
 ## general -0.5963679 0.26104317
 ## vocation 1.0986972 -0.08573852
 ## 
 ## Std. Errors:
 ## (Intercept) femalefemale sesmiddle seshigh schtypprivate
 ## general 2.385383 0.4514339 0.5224132 0.5934146 0.5597181
 ## vocation 2.566895 0.4993567 0.5764471 0.6885407 0.8313621
 ## read write math science socst
 ## general 0.03076523 0.05109711 0.03514069 0.03153073 0.02697888
 ## vocation 0.03451435 0.05358824 0.03902319 0.03252487 0.02912126
 ## honorsenrolled awards
 ## general 0.8708913 0.2969302
 ## vocation 0.9798571 0.3708768
 ## 
 ## Residual Deviance: 308.7306 
 ## AIC: 356.7306Lets check for fitted values now.> head(fitted(test))## academic general vocation
 ## 1 0.08952937 0.1811189 0.7293518
 ## 2 0.05219222 0.1229310 0.8248768
 ## 3 0.54704495 0.0849831 0.3679719
 ## 4 0.17103536 0.2750466 0.5539180
 ## 5 0.10014015 0.2191946 0.6806652
 ## 6 0.27287474 0.1129348 0.6141905Once we have build the model, well use it forprediction. Let us create a new data set with different permutation and combinations.> expanded=expand.grid(female=c(""female"", ""male"", ""male"", ""male""),
          ses=c(""low"",""low"",""middle"", ""high""),
          schtyp=c(""public"", ""public"", ""private"", ""private""),
          read=c(20,50,60,70),
          write=c(23,45,55,65),
          math=c(30,46,76,54),
          science=c(25,45,68,51),
          socst=c(30, 35, 67, 61),
          honors=c(""not enrolled"", ""not enrolled"", ""enrolled"",""not enrolled""),
          awards=c(0,0,3,0,6))> head(expanded)## female ses schtyp read write math science socst honors awards
 ## 1 female low public 20 23 30 25 30 not enrolled 0
 ## 2 male low public 20 23 30 25 30 not enrolled 0
 ## 3 male low public 20 23 30 25 30 not enrolled 0
 ## 4 male low public 20 23 30 25 30 not enrolled 0
 ## 5 female low public 20 23 30 25 30 not enrolled 0
 ## 6 male low public 20 23 30 25 30 not enrolled 0> predicted=predict(test,expanded,type=""probs"")
> head(predicted)## academic general vocation
 ## 1 0.01357216 0.1759060 0.8105219
 ## 2 0.01929452 0.2142205 0.7664850
 ## 3 0.01929452 0.2142205 0.7664850
 ## 4 0.01929452 0.2142205 0.7664850
 ## 5 0.01357216 0.1759060 0.8105219
 ## 6 0.01929452 0.2142205 0.7664850Now, well calculate the prediction values. The parameter type=probs, specifies our interest in probabilities. In order to plot predicted probabilities for intuitive understanding, we add predicted probability values to data.> bpp=cbind(expanded, predicted)Now well calculate the mean probabilities within each level of ses.> by(bpp[,4:7], bpp$ses, colMeans)## bpp$ses: low
 ## read write math science 
 ## 50.00 47.00 51.50 47.25 
 ## -------------------------------------------------------- 
 ## bpp$ses: middle
 ## read write math science 
 ## 50.00 47.00 51.50 47.25 
 ## -------------------------------------------------------- 
 ## bpp$ses: high
 ## read write math science 
 ## 50.00 47.00 51.50 47.25Ive used themelt() function from reshape2 package. It melts data with the purpose of each row being a unique id-variable combination.> library(""reshape2"")## Warning: package 'reshape2' was built under R version 3.1.3> bpp2 =melt(bpp,id.vars=c(""female"", ""ses"",""schtyp"",""read"",""write"",""math"",""science"",""socst"",""honors"", ""awards""),value.name=""probablity"")> head(bpp2)## female ses schtyp read write math science socst honors awards
 ## 1 female low public 20 23 30 25 30 not enrolled 0
 ## 2 male low public 20 23 30 25 30 not enrolled 0
 ## 3 male low public 20 23 30 25 30 not enrolled 0
 ## 4 male low public 20 23 30 25 30 not enrolled 0
 ## 5 female low public 20 23 30 25 30 not enrolled 0
 ## 6 male low public 20 23 30 25 30 not enrolled 0
 ## variable probablity
 ## 1 academic 0.01357216
 ## 2 academic 0.01929452
 ## 3 academic 0.01929452
 ## 4 academic 0.01929452
 ## 5 academic 0.01357216
 ## 6 academic 0.01929452Now, we will be plotting graphs to explore the distribution of dependent variable vs independent variables, using ggplot() function. In ggplot, the first parameter in this function is the data values to be plotted. The second part is where (aes()) binds variables to x and y axis. We tell the plotting function to draw a line using geom_line(). We are differentiating the school type by plotting them in different colors.> library(""ggplot2"")> ggplot(bpp2, aes(x = write, y = probablity, colour = ses)) +
   geom_line() + facet_grid(variable ~ ., scales=""free"")Till here, we have learnt to use multinomial regression in R. As mentioned above, if you have prior knowledge of logistic regression, interpreting the results wouldnt be too difficult. Lets now proceed to understand ordinal regression in R.Below are the steps to perform OLR in R:Load the Libraries> require(foreign)
> require(ggplot2)
> require(MASS)
> require(Hmisc)
> require(reshape2)Load the data> dat <- read.dta(""http://www.ats.ucla.edu/stat/data/ologit.dta"")
> head(dat)## apply pared public gpa
 ## 1 very likely 0 0 3.26
 ## 2 somewhat likely 1 0 3.21
 ## 3 unlikely 1 1 3.94
 ## 4 somewhat likely 0 0 2.81
 ## 5 somewhat likely 0 0 2.53
 ## 6 unlikely 0 1 2.59Lets quickly understand the data.The data set has a dependent variable known as apply.It has 3 levels namelyunlikely, somewhat likely, and very likely, coded in 1, 2, and 3 respectively. 3 being highest and 1 being lowest. This situation is best for using ordinal regressionbecause of presence of ordered categories. Pared (0/1) refers to at least one parent has a graduate degree; public (0/1) refers to the type of undergraduate institute.For building this model, we will be using the polr command to estimate an ordered logistic regression. Then, wellspecify Hess=TRUE to let the model output show the observed information matrix from optimization which is used to get standard errors.> m <- polr(apply ~ pared + public + gpa, data = dat, Hess=TRUE)
> summary(m)## Call:
 ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE)
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 1.04769 0.2658 3.9418
 ## public -0.05879 0.2979 -0.1974
 ## gpa 0.61594 0.2606 2.3632
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 2.2039 0.7795 2.8272
 ## somewhat likely|very likely 4.2994 0.8043 5.3453
 ## 
 ## Residual Deviance: 717.0249 
 ## AIC: 727.0249We see the usual regression output coefficient table including the value of each coefficient, standard errors, t values, estimates for the two intercepts, residual deviance and AIC. AIC is the information criteria. Lesser the better.Now well calculate some essential metrics such as p-Value, CI, Odds ratio> ctable <- coef(summary(m))## Value Std. Error t value
 ## pared 1.04769010 0.2657894 3.9418050
 ## public -0.05878572 0.2978614 -0.1973593
 ## gpa 0.61594057 0.2606340 2.3632399
 ## unlikely|somewhat likely 2.20391473 0.7795455 2.8271792
 ## somewhat likely|very likely 4.29936315 0.8043267 5.3452947> p <- pnorm(abs(ctable[, ""t value""]), lower.tail = FALSE) * 2
> ctable <- cbind(ctable, ""p value"" = p)## Value Std. Error t value p value
 ## pared 1.04769010 0.2657894 3.9418050 8.087072e-05
 ## public -0.05878572 0.2978614 -0.1973593 8.435464e-01
 ## gpa 0.61594057 0.2606340 2.3632399 1.811594e-02
 ## unlikely|somewhat likely 2.20391473 0.7795455 2.8271792 4.696004e-03
 ## somewhat likely|very likely 4.29936315 0.8043267 5.3452947 9.027008e-08# confidence intervals
> ci <- confint(m)## Waiting for profiling to be done...## 2.5 % 97.5 %
 ## pared 0.5281768 1.5721750
 ## public -0.6522060 0.5191384
 ## gpa 0.1076202 1.1309148> exp(coef(m))## pared public gpa
 ## 2.8510579 0.9429088 1.8513972## OR and CI
> exp(cbind(OR = coef(m), ci))## OR 2.5 % 97.5 %
 ## pared 2.8510579 1.6958376 4.817114
 ## public 0.9429088 0.5208954 1.680579
 ## gpa 1.8513972 1.1136247 3.0984901. One unit increase in parental education, from 0 (Low) to 1 (High), the odds of very likely applying versus somewhat likely or unlikely applying combined are 2.85 greater .2. The odds very likely or somewhat likely applying versus unlikely applying is 2.85 times greater .3. For gpa, when a students gpa moves 1 unit, the odds of moving from unlikely applying to somewhat likely or very likley applying (or from the lower and middle categories to the high category) are multiplied by 1.85.Lets now try to enhance this model to obtain better prediction estimates.> summary(m)## Call:
 ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE)
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 1.04769 0.2658 3.9418
 ## public -0.05879 0.2979 -0.1974
 ## gpa 0.61594 0.2606 2.3632
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 2.2039 0.7795 2.8272
 ## somewhat likely|very likely 4.2994 0.8043 5.3453
 ## 
 ## Residual Deviance: 717.0249 
 ## AIC: 727.0249> summary(update(m, method = ""probit"", Hess = TRUE), digits = 3)## Call:
 ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE, 
 ## method = ""probit"")
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 0.5981 0.158 3.7888
 ## public 0.0102 0.173 0.0588
 ## gpa 0.3582 0.157 2.2848
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 1.297 0.468 2.774 
 ## somewhat likely|very likely 2.503 0.477 5.252 
 ## 
 ## Residual Deviance: 717.4951 
 ## AIC: 727.4951> summary(update(m, method = ""logistic"", Hess = TRUE), digits = 3)## Call:
 ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE, 
 ## method = ""logistic"")
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 1.0477 0.266 3.942
 ## public -0.0588 0.298 -0.197
 ## gpa 0.6159 0.261 2.363
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 2.204 0.780 2.827 
 ## somewhat likely|very likely 4.299 0.804 5.345 
 ## 
 ## Residual Deviance: 717.0249 
 ## AIC: 727.0249> summary(update(m, method = ""cloglog"", Hess = TRUE), digits = 3)## Call:
 ## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE, 
 ## method = ""cloglog"")
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 0.517 0.161 3.202
 ## public 0.108 0.168 0.643
 ## gpa 0.334 0.154 2.168
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 0.871 0.455 1.912 
 ## somewhat likely|very likely 1.974 0.461 4.287 
 ## 
 ## Residual Deviance: 719.4982 
 ## AIC: 729.4982Lets add interaction terms here.> head(predict(m, dat, type = p))## unlikely somewhat likely very likely
 ## 1 0.5488310 0.3593310 0.09183798
 ## 2 0.3055632 0.4759496 0.21848725
 ## 3 0.2293835 0.4781951 0.29242138
 ## 4 0.6161224 0.3126888 0.07118879
 ## 5 0.6560149 0.2833901 0.06059505
 ## 6 0.6609240 0.2797117 0.05936430> addterm(m, ~.^2, test = ""Chisq"")## Single term additions
 ## 
 ## Model:
 ## apply ~ pared + public + gpa
 ## Df AIC LRT Pr(Chi)
 ## <none> 727.02 
 ## pared:public 1 727.81 1.21714 0.2699
 ## pared:gpa 1 728.98 0.04745 0.8276
 ## public:gpa 1 728.60 0.42953 0.5122> m2 <- stepAIC(m, ~.^2)
> m2## Start: AIC=727.02
 ## apply ~ pared + public + gpa
 ## 
 ## Df AIC
 ## - public 1 725.06
 ## <none> 727.02
 ## + pared:public 1 727.81
 ## + public:gpa 1 728.60
 ## + pared:gpa 1 728.98
 ## - gpa 1 730.67
 ## - pared 1 740.60
 ## 
 ## Step: AIC=725.06
 ## apply ~ pared + gpa
 ## 
 ## Df AIC
 ## <none> 725.06
 ## + pared:gpa 1 727.02
 ## + public 1 727.02
 ## - gpa 1 728.79
 ## - pared 1 738.60> summary(m2)## Call:
 ## polr(formula = apply ~ pared + gpa, data = dat, Hess = TRUE)
 ## 
 ## Coefficients:
 ## Value Std. Error t value
 ## pared 1.0457 0.2656 3.937
 ## gpa 0.6042 0.2539 2.379
 ## 
 ## Intercepts:
 ## Value Std. Error t value
 ## unlikely|somewhat likely 2.1763 0.7671 2.8370 
 ## somewhat likely|very likely 4.2716 0.7922 5.3924 
 ## 
 ## Residual Deviance: 717.0638 
 ## AIC: 725.0638> m2$anova## Stepwise Model Path 
 ## Analysis of Deviance Table
 ## 
 ## Initial Model:
 ## apply ~ pared + public + gpa
 ## 
 ## Final Model:
 ## apply ~ pared + gpa
 ## 
 ## 
 ## Step Df Deviance Resid. Df Resid. Dev AIC
 ## 1 395 717.0249 727.0249
 ## 2 - public 1 0.03891634 396 717.0638 725.0638> anova(m, m2)## Likelihood ratio tests of ordinal regression models
 ## 
 ## Response: apply
 ## Model Resid. df Resid. Dev Test Df LR stat.
 ## 1 pared + gpa 396 717.0638 
 ## 2 pared + public + gpa 395 717.0249 1 vs 2 1 0.03891634
 ## Pr(Chi)
 ## 1 
 ## 2 0.8436145Time to plot this model.> m3 <- update(m, Hess=TRUE)
> pr <- profile(m3)
> confint(pr)## 2.5 % 97.5 %
 ## pared 0.5281772 1.5721695
 ## public -0.6522008 0.5191415
 ## gpa 0.1076189 1.1309092> plot(pr)> pairs(pr)I enjoyed writing this article. Id suggest you to pay attention to interpretation aspect of themodel. Coding is relatively easy, but unless you know whats resulting, you learning will be incomplete.There are many essential factors such asAIC, Residuals values to determine the effectiveness of the model. If you still struggle to understand them, Id suggest you to brush your Basics of Logistic Regression. This should help you in understanding this concept better.In this article, I shared my understanding of using multinomial and ordinal regression in R. These techniques are used when the dependent variable has levels either ordered or unordered.Did you find this article helpful ? Have you used this technique to build any models ? Do share your experience and suggestions in the comments section below.About the AuthorSray Agarwalis the chief manager ofBennett Coleman and Co. Ltd. (Times Group)and works as Subject Matter Expert (SME) forBusiness Analytics program.He hasmore than 8.5 years of experience in data science and BA.He holds a degree inBusiness Analytics from Indian School of Business (ISB), Hyderabad. and graduated with an award of Academic Excellence and has been the part of the Deans List. Along with this he is a SAS certified Predictive Modeller.",https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/
A Complete Tutorial on Ridge and Lasso Regression in Python,Learn everything about Analytics|Overview|Introduction|Project to Apply your Regression Skills|Problem Statement|Table of Contents|1. Brief Overview|2. Why Penalize the Magnitude of Coefficients?|3. Ridge Regression|4. Lasso Regression|5. Sneak Peak into Statistics (Optional)|6. Conclusion|End Notes,"1. Simple Linear Regression|2. Ridge Regression|3. Lasso Regression|1. Key Difference|2. Typical Use Cases|3. Presence of Highly Correlated Features|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|How to use Multinomial and Ordinal Logistic Regression in R ?|My AMA & our biggest ever hackathon  less than 24 hours away!|
Aarshay Jain
|52 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Objective = RSS +  * (sum of square of coefficients)|Objective = RSS +  * (sum of absolute value of coefficients),ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When we talk about Regression, we often end up discussingLinear and Logistic Regression. But, thats not the end. Do you know there are 7 types ofRegressions?Linear and logistic regression isjust the most loved membersfrom the family of regressions.Last week, I saw a recorded talk at NYC Data Science Academy from Owen Zhang, Chief Product Officer at DataRobot. He said, if you are using regression without regularization, you have to be very special!. I hope you get what a person of his stature referred to.I understood it very well anddecided to explore regularization techniques in detail.In this article,I have explained the complex science behindRidge Regression and Lasso Regression which are the most fundamental regularization techniques used in data science, sadlystill not used by many.The overall idea of regression remains the same. Its the way in which the model coefficients are determined which makes all the difference. I strongly encourage you to go through multiple regression before reading this. You can take help from this article or any other preferred material.Demand forecasting is a key component of every growing online business. Without proper demand forecasting processes in place, it can be nearly impossible to have the right amount of stock on hand at any given time. A food delivery service has to deal with a lot of perishable raw materials which makes it all the more important for such a company to accurately forecast daily and weekly demand.Too much inventory in the warehouse means more risk of wastage, and not enough could lead to out-of-stocks  and push customers to seek solutions from your competitors. In this challenge, get a taste of demand forecasting challenge using a real dataset.Practice NowRidge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a large number of features. Here large can typically mean either of two things:Though Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If youve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called regularization techniques. The key difference is in how they assign penalty to the coefficients:Note that here LS Obj refers to least squares objective, i.e. the linear regression objective without regularization.If terms like penalty and regularization seem very unfamiliar to you, dont worry well talk about these in more detail through the course of this article. Before digging further into how they work, lets try to get some intuition into why penalizing the magnitude of coefficients should work in the first place.Lets try to understand the impact of model complexity on the magnitude of coefficients. As an example, I have simulated a sine curve(between 60 and 300) and added some random noise using the following code:The input-output looks like:
This resembles a sine curve but not exactly because of the noise. Well use this as an example to test different scenarios in this article. Lets try to estimate the sine function using polynomial regression with powers of x from 1 to 15. Lets add a column for each power upto 15 in our dataframe. This can be accomplished using the following code:The dataframe looks like:
Now that we have all the 15 powers, lets make 15 different linear regression models with each model containing variables with powers of x from 1 to the particular model number. For example, the feature set of model 8 will be  {x, x_2, x_3,  ,x_8}.First, well define a generic function which takes in the required maximum power of x as an input and returns a list containing  [ model RSS, intercept, coef_x, coef_x2,  upto entered power ]. Here RSS refers to Residual Sum of Squares which is nothing but the sum of square of errors between the predicted and actual values in the training data set. The python code defining the function is:Note that this function will not plot the model fit for all the powers but will return the RSS and coefficients for all the models. Ill skip the details of the code for now to maintain brevity. Ill be happy to discuss the same through comments below if required.Now, we can make all 15 models and compare the results. For ease of analysis, well store all the results in a Pandas dataframe and plot 6 models to get an idea of the trend. Consider the following code:We would expect the models with increasing complexity to better fit the data and result in lower RSS values. This can be verified by looking at the plots generated for 6 models:This clearly aligns with our initial understanding. As the model complexity increases, the models tends to fit even smaller deviations in the training data set. Though this leads to overfitting, lets keep this issue aside for some time and come to our main objective, i.e. the impact on the magnitude of coefficients. This can be analysed by looking at the data frame created above.Python Code:The output looks like:
It is clearly evident thatthe size of coefficients increase exponentially with increase in model complexity. I hope this gives some intuition into why putting a constraint on the magnitude of coefficients can be a good idea to reduce model complexity.Lets try to understand this even better.What does a large coefficient signify? It means that were putting a lot of emphasis on that feature, i.e. the particular feature is a good predictor for the outcome. When it becomes too large, the algorithm starts modelling intricate relations to estimate the output and ends up overfitting to the particular training data.I hope the concept is clear. Ill be happy to discuss further in comments if needed.Now, lets understand ridge and lasso regression in detail and see how well they work for the same problem.As mentioned before, ridge regression performs L2 regularization, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:Here,  (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients.  can take various values:I hope this gives some sense on how  would impact the magnitude of coefficients. One thing is for sure that any non-zero value would give values less than that of simple linear regression. By how much? Well find out soon. Leaving the mathematical details for later, lets see ridge regression in action on the same problem as above.First, lets define a generic function for ridge regression similar to the one defined for simple linear regression. The Python code is:Note the Ridge function used here. It takes alpha as a parameter on initialization. Also, keep in mind that normalizing the inputs is generally a good idea in every type of regression and should be used in case of ridge regression as well.Now, lets analyze the result of Ridge regression for 10 different values of  ranging from 1e-15 to 20. These values have been chosen so that we can easily analyze the trend with change in values of . These would however differ from case to case.Note that each of these 10 models will contain all the 15 variables and only the value of alpha would differ. This is different from the simple linear regression case where each model had a subset of features.Python Code:This would generate the following plot:
Here we can clearly observe that as the value of alpha increases, the model complexity reduces. Though higher values of alpha reduce overfitting, significantly high values can cause underfitting as well (eg. alpha = 5). Thus alpha should be chosen wisely. A widely accept technique is cross-validation, i.e. the value of alpha is iterated over a range of values and the one giving higher cross-validation score is chosen.Lets have a look at the value of coefficients in the above models:Python Code:The table looks like:
This straight away gives us the following inferences:The first 3 are very intuitive. But #4 is also acrucialobservation. Lets reconfirm the same by determining the number of zeros in each row of the coefficients data set:Python Code:Output:
This confirms that all the 15 coefficients are greater than zero in magnitude (can be +ve or -ve). Remember this observation and have a look again until its clear. This will play an important role in later while comparing ridge with lasso regression.LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesnt give much of an idea but there are 2 key words here  absolute and selection.Lets consider the former first and worry about the latter later.Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:Here,  (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge,  can take various values. Lets iterate it here briefly:Yes its appearing to be very similar to Ridge till now. But just hang on with me and youll know the difference by the time we finish. Like before, lets run lasso regression on the same problem as above. First well define a generic function:Notice the additional parameters defined inLasso function  max_iter. This is the maximum number of iterations for which we want the model to run if it doesnt converge before. This exists for Ridge as as well but setting this to a higher than default value was required in this case. Why? Ill come to this in next section, just keep it in the back of the envelope.Lets check the output for 10 different values of alpha using the following code:This gives us the following plots:
This again tells us that the model complexity decreases with increase in the values of alpha. But notice the straight line at alpha=1. Appears a bit strange to me. Lets explore this further by looking at the coefficients:Apart from the expected inference of higher RSS for higher alphas, we can see the following:Inferences #1,2 might not generalize always but will hold for many cases. The real difference from ridge is coming out in the last inference. Lets check the number of coefficients which are zero in each model using following code:Output:
We can observe that even for a small value of alpha, a significant number of coefficients are zero. This also explains the horizontal line fit for alpha=1 in the lasso plots, its just a baseline model! This phenomenon of most of the coefficients being zero is called sparsity. Although lasso performs feature selection, this level of sparsity is achieved in special cases only which well discuss towards the end.This has some really interesting implications on the use cases of lasso regression as compared to that of ridge regression. But before coming to the final comparison, lets take a birds eye view of the mathematics behind why coefficients are zero in case of lasso but not ridge.I personally love statistics but many of you might not. Thats why I have specifically marked this section as OPTIONAL. If you feel you can handle the algorithms without going into the maths behind them, I totally respect the decision and you can feel free to skip this section.But I personally feel that getting some elementary understanding of how the thing works can be helpful in the long run.As promised, Ill keep it to a birds eye view. If you wish to get into the details, I recommend taking a good statistics textbook. One of my favorites is the Elements of Statistical Learning. The best part about this is that it has been made available for free by the authors.Lets start by reviewing the basic structure of data in a regression problem.In this infographic, you can see there are 4 data elements:Here, N is the total number of data points available and M is the total number of features. X has M+1 columns because of M features and 1 intercept.The predicted outcome for any data point i is:Itis simply the weighted sum of each data point with coefficients as the weights.This prediction is achieved by finding the optimum value of weights based on certain criteria, which depends on the type of regression algorithm being used. Lets consider all 3 cases:The objective function (also called as the cost) to be minimized is justthe RSS (Residual Sum of Squares), i.e. the sum of squared errors of the predicted outcome as compared to the actual outcome. This can be depicted mathematically as:In order to minimize this cost, we generally use a gradient descent algorithm. Ill not go into the details right now but you can refer this. The overall algorithm works as:Here the important step is #2.1.1 where we compute the gradient. Gradient is nothing but a partial differential of the cost with respect to a particular weight (denoted as wj). The gradient for the jth weight will be:This is formed from 2 parts:Step #2.1.2 involves updating the weights using the gradient. This update step for simple linear regression looks like:I hope you are able to follow along. Note the +ve sign in the RHS is formed after multiplication of 2 -ve signs. I would like to explain point #2 of the gradient descent algorithm mentioned above iterate till not converged. Here convergence refers to attaining the optimum solution within pre-defined limit.It is checked using the value of gradient. If the gradient is small enough, that means we are very close to optimum and further iterations wont have a substantial impact on coefficients. The lower-limit on gradient can be changed using the tol parameter.Lets consider the case of ridge regression now.The objective function (also called the cost) to be minimized is the RSS plus the sum of square of the magnitude of weights. This can be depicted mathematically as:In this case, the gradient would be:Again in the regularization part of gradient, only wj remains and all other would become zero. The corresponding update rule is:Here we can see that second part of the RHS is same as that of simple linear regression. Thus, ridge regression is equivalent to reducing the weight by a factor of (1-2) first and then applying the same update rule as simple linear regression. I hope thisgives some intuition into why the coefficients get reduced to small numbers but never become zero.Note that the criteria for convergence in this case remains similar to simple linear regression, i.e. checking the value of gradients. Lets discussLasso regression now.The objective function (also called the cost) to be minimized is the RSS plus the sum of absolute value of the magnitude of weights. This can be depicted mathematically as:In this case, the gradient is not defined as the absolute function is not differentiable at x=0. This can be illustrated as:We can see that the parts on the left and right side of 0 are straight lines with defined derivates but the function cant be differentiatedat x=0.In this case, we have to use a different technique called as coordinate descent which is based on the concept of sub-gradients. One of the coordinate descent follows the following algorithms (this is also the default in sklearn):#2.1.1 might look too generalized. But Im intentionally leaving the details and jumping to the update rule:Here g(w-j) represents (but not exactly)the difference between actual outcome and the predicted outcome considering all EXCEPT the jth variable. If this value is small, it means that the algorithm is able to predict the outcome fairly well even without the jth variable and thus it can be removed from the equation by setting a zero coefficient. This gives us some intuition into why the coefficients become zero in case of lasso regression.In coordinate descent, checking convergence is another issue. Since gradients are not defined, we need an alternate method. Many alternatives exist but the simplest one is to check the step size of the algorithm. We can check the maximum difference in weights in any particular cycle over all feature weights (#2.1 of algo above).If this is lower than tol specified, algo willstop. The convergence is not as fast as gradient descent and we might have to set the max_iter parameter if a warning appears saying that the algo stopped before convergence. This is why I specified this parameter in the Lasso generic function.Lets summarize our understanding by comparing the coefficients in all the threecases using the following visual, which shows how the ridge and lasso coefficients behave in comparison to the simple linear regression case.Apologies for the lack of visual appeal. But I think it is good enough to re-inforced the following facts:Before going further,one important issue in case of both ridge and lasso regression is intercept handling. Generally, regularizing the intercept is not a good idea and it should be left out of regularization. This requires slight changes in the implementation, which Ill leave for you to explore.If youre still confused and things are a bit fuzzy, I recommend taking the course on Regression which is part of theMachine Learning Specializationby University of Washington at Coursera.Now, lets come to the concluding part where we compare the Ridge and Lasso techniques and see where these can be used.Now that we have a fair idea of how ridge and lasso regression work, lets try to consolidate our understanding by comparing them and try to appreciate their specific use cases. I will also compare them with some alternate approaches. Lets analyze these under three buckets:Traditionally, techniques like stepwise regression were used to perform feature selection and make parsimonious models. But with advancements in Machine Learning, ridge and lasso regression provide very good alternatives as they give much better output, require fewer tuning parameters and can be automated to a large extend.Its not hard to see why the stepwise selection techniques become practically very cumbersome to implement in high dimensionality cases. Thus, lasso provides a significant advantage.This disadvantage of lasso can be observed in the example we discussed above. Since we used a polynomial regression, the variables were highly correlated. ( Not sure why? Check the output of data.corr() ). Thus, we saw that even small values of alpha were giving significantsparsity (i.e. high #coefficients as zero).Along with Ridge and Lasso, Elastic Net is another useful techniques which combines both L1 and L2 regularization. It can be used to balance out the pros and cons of ridge and lasso regression. I encourage you to explore it further.In this article, I gave anoverview of regularization using ridge and lasso regression. Then, I focused on reasons behindpenalizing the magnitude of coefficients should give us parsimonious models. Next, we went into details of ridge and lasso regression and sawtheir advantages over simple linear regression. We got some intuition into why they should work and also how they work. If you read the optional mathematical part, you probably understood the underlying fundamentals.Regularization techniques are really useful and I encourage you to implement them. If youre ready to take the challenge, why not try them on the BigMart Sales Prediction problem and share your results in the discussion forum.Did you findthe article useful? Wasit was tooconvoluted for you or just a walk in the park? Is there something you would like me to improve upon. Please share your valuable feedback and help me treat you with better content in future.",https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/
My AMA & our biggest ever hackathon  less than 24 hours away!,Learn everything about Analytics,"AMA with Analytics Vidhya community|Last Man Standing  Our biggest ever hackathon|See you soon|Share this:|Like this:|Related Articles|A Complete Tutorial on Ridge and Lasso Regression in Python|Top Certification Courses in SAS, R, Python, Machine Learning, Big Data, Spark|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you are a regular here, you would know that my excitement is going through the roof. If you are asking Why?, it is because we cant wait to see the action in next 4 days. In fact, there are 2 big events happening in the next 24 hours.I am doing an open AMA tonight (8:30 p.m.  9:30 p.m. India time, GMT + 5:30). I hope to interact with the community and talk about data science, machine learning, our hackathons, blogathon, entrepreneurship and careers in analytics.Join the AMA here.I also hope to hear from you how we can make Analytics Vidhya more helpful for you.I hope to see you all soon.Our largest ever hackathon, Last Man Standing starts mid night (India time, GMT + 5:30). More than 1100 data scientists have already registered for this competition and we hope that more than 1500 data scientists will compete for the awards shortly. The competition will happen on the new platform and you will be able to form teams during the hackathon.The prize money is Amazon vouchers worth INR 30,000 (~$450), the learning unparalleled and now you will get AV points for hackathons, which will decide user rankings and will reflect in your profile pages as well!Thats it for now  lot of action coming up shortly  stay focused and stay tuned to these awesome events.Excited to interact with you tonight!",https://www.analyticsvidhya.com/blog/2016/01/ama-biggest-hackathon-24-hours-away/
"Top Certification Courses in SAS, R, Python, Machine Learning, Big Data, Spark",Learn everything about Analytics|Introduction|Which courses are included in these rankings ?|Certification Courses in SAS Programming|Certification in Data Science Using SAS|Certification Courses in R Programming|Certifications in Data Science using R Programming|Certification Courses in Python|Certification Courses in Machine Learning|Certification Courses inBig Data|Certification Courses in Spark|Certification Courses in MS Excel / Data Visualization|What is the criteria used for ranking these programs?|End Notes,"1.Base SAS Programmer Certification  SAS Institute|2.Certified SAS Professional  Analytix Lab|3.Certified AI and ML BlackBelt  Analytics Vidhya|4.Certification in SAS  Imarticus|5. SAS (Base & Advance)  Dexlab|1.SAS Institute  Predictive Modeler|2.Foundation Course in Analytics by Jigsaw Academy|3.Data Science using SAS & R  Analytix Lab|4.Certification in SAS Programming by Imarticus|5.Certified Business Analytics Professionals by EdvancerEduventures|1. R Courses on DataCamp|2.R Programming  Coursera|3.Introduction to R Programming  edX|4. Dataquest|5.Certified R Programmer  Edvancer|1. Data Science Specialization by Coursera|2. Analytics Edge by edX|3.Introduction to Data Science  Udemy|4. Data Science with R from Jigsaw Academy|5. Data Science  Edureka|1. Intro to Data Science by Udacity|2. From 0 to 1: Machine Learning, NLP & Python  Udemy|3. Mastering Python by Edureka|4. Certification in Python by Imarticus|5. Data Science in Python by Dezyre|1. Learning from Data by Yaser Abu Mostafa|2. Machine Learning by Andrew Ng|3. Intro to Machine Learning by Udacity|4. Machine Learning Specialization  Coursera|5. Data Camp  Machine Learning|1.CCP Data Scientist  Cloudera|2. Big Data University|3. Data Science and Big Data Analytics  EMC|4. Data Science at Scale Specialization by Coursera|5. Big Data Specialist from Jigsaw Academy|1. Spark  Big Data University|2. CCA Spark and Hadoop Developer Certification|3.Introduction to Big Data with Apache Spark|4. Taming Big Data with Apache Spark  Udemy|5. Apache Spark and Scala  Edureka|Data Visualization|1. Become QlikView Developer from Scratch|2. Tableau Desktop 9 Associate Certification|MS Excel|1. Excel to MySQL: Analytic Techniques for Business Specialization|2. Advanced MS 2010  Edureka|3. Microsoft Excel 2010  Advanced Training  Udemy|4. MS Excel Certification  Dexlab|5. Advanced Excel with Macros and VBA  Dezyre|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|My AMA & our biggest ever hackathon  less than 24 hours away!|How to use XGBoost algorithm in R in easy steps|
Kunal Jain
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What could be more convenient than upgrading skills online ?There are plenty ofcourses / certifications available to kick-start your career in analytics. These courses are provided in online, offline or hybrid mode. The only difficulty students face is to decide the best out of these courses.With somenewly introducedcourses, it has become even more difficult to make a convincing decision.The fear of investing in unworthy courses continues to remain the biggest hurdle for students. Last year, Ireceived thousand of emails after I published Top Certifications on SAS, R, Python, Machine Learning. Later, I came to know that my analysis helped many people in deciding the best course for themselves.The year 2016 is no different either. I am back with my thoroughanalysis and rankings of best certifications courses in India. I assure you these rankings are unbiased. Last month, we released our rankings on Top Business Analytics Programs in India 2015-16. If you too are planning for a degree in analytics, you may like to consider these institutes.In this article, Ill focus on ranking short duration and certification courses.Ive considered the courses which are delivered in online or hybrid mode. Course running in hybrid mode are carried out only in India. For now, Ive filtered out the courses being delivered in other countries. If you want me to do an analysis of analytics courses in your country, leave your consent in the comment section below.In India, there is an abundance of online / offline certification courses. It has made more difficult for candidates to make an informed decision. And, this is where these rankings will help you out!I understand, there is no one for all analytics course. Hence, Ive categorized these courses on the basis of course material they have covered. Ive categorized these courses into the following 9 categories:This will help you to choose the best available certification in these segments according to your need. Considering the rising popularity of MOOCs, Ive included only the best ones which provide a comprehensive knowledge and thorough detail ona subject.Below is the ranking of certifications:This certification is a good place to start with SAS for newbies. Yes! If you ever wanted to learn SAS from the basics, thiscouldbe your first step. This is the only certification in SAS which is recognized globally. The classes are held in Mumbai, India. Live online classes are also available. It covers the basics of SAS and various data manipulation techniques. However, the cost of this certification is relatively higher than other certifications listed in this category. Fee for online classes in INR30,000. Fee for offline classes in Mumbai is INR 48000.This certification by Analytix Lab is costed at a lower price but covers all the essential aspects of SAS. Though less recognized than the former, this certification is good enough fora beginner to get started as data analyst in SAS. It covers data exploration, data manipulation, operations, data analysis etc. It teaches both base SAS and advanced SAS. This certification is available in two forms: Instructor Led (Fee  INR 16000) and Self Paced (Fee  INR 12000). Some additional feature includes free mock tests, question bank, doubt clearing session with faculty etc.This comprehensive certified program combines the power of data science, machine learning and deep learning to help you become an AI & ML BlackBelt! Go from a complete beginner to gaining in-demand industry-relevant AI skills.You will get access to ALL the courses Analytics Vidhya has curated and designed as part of AI & ML Blackbelt. What are you waiting for? Start your AI journey today!This is a classroom certification program which follows an intensive pedagogy of teaching SAS using case studies & real life business problems. It covers Bases SAS & Advanced SAS. It includes 100 hours of classroom interaction. Additionally, a students gets 24/7 access to learning material, assessment and study aids. This certification is not limited to SAS, but also helps a students develop necessary skills to grab a job in this industry. Students undergo dedicated session by industry experts for job readiness. Fee of this certification is INR 40000.This certification program is available in a hybrid (classroom and online training) format. The duration of this classes is 30 hours. Currently, this course is available in Delhi and Gurgaon (India).It teaches both base SAS and advanced SAS. Though, the content covered in this course could be made comprehensive. Fee of this certification ins INR 18000. Being a newly launched course, this course is yet to make a mark in analytics industry.The course listed in this section are designed to teach data science using SAS. Its a good place to start data science for people who dont find coding / programming comfortable.This course is best suited for candidates having prior knowledge of Base SAS. It isnt suitable for beginners, but must for intermediate SAS users. It is a 6 day program held in online & offline format. The course fee is Rs. 66000, which isquite higher than other courses. However, if you are determined to get this certification, Id suggest you complete their free online courses first. Of all the certificates, this certification has the widest global recognition.This would provide you a much need hands on experience on SAS.Undoubtedly, this is the best coursefor beginnersto learn SAS. It has retained its position this year as well. The duration of this course is 16 weeks. It is available in both self paced and instructor led formats. This course cover basics of analytics, machine learning modeling, model building in SAS. Hence, its a comprehensive course on getting started with data science in SAS. It is best suited to a fresher candidate who has no prior experience of analytics.This certification is designed to capture a thorough learning experience of data science using Excel, SAS and R. If you dream of becoming a data scientist, this could be a place where you can get all starting material. Its a detailed course which covers predictive modeling along with tools used. This course is available in dual formats: Self paced and Instructor led. Fee of self paced classes is INR 25000. Fee of instructor led classes in INR 30000. Here youll get access to case studies, weekly assignments and much more to keep yourself engaged.This is a good course to get started with SAS. It covers both Base and Advanced module in detail. However, it doesnt touch upon predictive modeling in SAS. However, it does touch upon basics of statistics in later half. Hence, ranked on 3rd position. If you wish to get acquainted with coding environment in SAS, this could be your choice. You can get a flavor of predictive modeling, using the case studies provided. Other benefits associated with this course includes career assistance, industry mentorship and much more.This SAS certification by Edvancer is delivered through interactive online classes. The classes are held on weekends. It provides a thorough knowledge of data science using R and SAS.Fee of this course is INR 26990. More than just class session, students are also provided with practice assignments & case studies to get better. It teaches predictive modeling in SAS, not in R. In R, it teaches some basic data manipulation skills.Students get the facility to connect with faculties to get doubts & queries cleared.The certification listed in this category will enable you with the knowledge required become skilled at using R Programming technically. These do not teach to use R Programming for data science or predictive modeling. Hence, if you are required to use R for operations other than related to data science, these certification should help you to get started.DataCamp offers a wide variety of courses on R Programming. If you are a beginner, this can be a good place to start. It offers an interactive platform in your web browser where you learn R by doing. After you sign up, initial certifications are free to avail. As you proceed towards the advanced level, youll be required to a pay fee. The concepts are explained in an easy to understand method. The course material is comprehensive and includes timely assessments to check your understanding.This course is a part of Data Science Specialization by John Hopkins University. It very well teaches the technical aspects of programming in R for effective data analysis. The concepts are explained using various examples and in a simple manner. The duration of this course is 4 weeks. It is available online. This course is free to access. However, if you want a certification, the fee of this certification is $29.This course is delivered by DataCamp and Microsoft together on edX.The course content is free to access. However, the certification can be availed for $49. The course content is great to watch and learn. No doubt about course quality too. The duration of this course is 4 weeks. The course is designed to help students master the concept of data structures in R.The curriculum also covers data visualization, data analysis in R along with basic data handling techniques.Dataquest started with Python. And, now it has launched R tutorials too. Thesetutorials are free to access for beginners. However, if you wish to upscale your knowledge in R, youd enter the premium paid section. The subscription for self paced learners is available at $49/month. It offers an online interactive interface to learn R coding while doing. There are various missions or levels which will ensure your progress. Students will face timely challenges and projects.This is a self paced course which includes 15 hours of detailed course content. This course covers the basics of R along with statistical methods of dealing with data inR. The breadth of content coverage is fantastic. Being a self paced course, you need to be self motivated to complete this course. While, a necessarysupport wouldcome from faculties, but it lacks the massive community to support learning.Above all, to ensure an enrichinglearning experience, the course is equippedwith challenging quizzes, exercises and other useful course material. Fee of this certification is INR 8000.This specialization is delivered by John Hopkins University. It consists of 9 courses plus a capstone project. Each course covers one topic in detail. Its a one stop destination for anyone willing to learn R and machine learning from scratch. Hence, if you ever want to become a master indata science using R, this is yourstart line. However, If you ever get stuck, you wouldnt have any instructor to rescue you. But, you can seek active forum to shout out. And, if you have busy work schedule, great. This course demands just 3-5 hours of dedication per week.This is probably the best course available to learn R. But, this course requires you to dedicate 10 hours a week. This course teaches machine learning in R using popular case studies such as moneyball, IBM super computer, twitter mining etc. It assumes you are familiar with basics of R. Itis archived now. You surely can access the course content but wouldnt be evaluated. The certifications open in Summer and Spring Season. If you wish to learn R, Id recommend you to subscribe its updates.This is an enriching course on R Programming which teaches predictive modeling using commonly used machine learning algorithms. Also, it covers data exploration & data manipulation techniques in R.The lectures are delivered using video content. This course expects a prior knowledge of basic statistics.It includes working on data science project to get practical experience of R. Fee of this certification is $24.This is priced higher than previous course. But, its undoubtedly a comprehensive course on data science in R. It is priced at INR 38000 (Instructor Led) and INR 25000 (Self-paced). This course is best suited for complete freshers. It covers statistics, r programming, modeling techniques and much more. Its a blend of theoretical and practical learning on R. The video quality is nice and instructors have does a satisfactory job in simplifying these concepts.This course is specially designed for students aspiring to become a data scientist. It covers the complete data life cycle from data storage, data manipulation to model building and validation. This is a comprehensive course which teaches R with basic statistics, predictive modeling, machine learning, big data techniques etc. Its a lot of content to use, hence it is made self-pace. The certification can be availed after the successful completion of real time project (provided in course). Fee of this certification course is INR 21164.There arent many python for data science courses available to teach python from scratch. Hence, before taking up these courses, Id suggest you gain basic python knowledge from Python at Codecademy. Its an easy and quick course to get you started with these courses. Though some of these course do teach basics, but I always suggest to learn by doing it.Its a free course. It is not available for certification. If you are interested for certification in python, stop by and check out this course. Once you have finished it successfully, you can switch to a paid course. This course is great to get you started on your data scientist path. It assumes youve basic knowledge of python and statistics. This course touches upon basics of machine learning, statistical modeling and big data. Dont expect for in depth knowledge, but enough to whetyour learning appetite.This is a comprehensive machine learning course delivered in python. This course covers machine learning, natural language processing, neural networks too. The course content is broadly structured with central focus on exploring python with every technique. Its a good to go course for python in data science. As a part of this certification, quizzes & assignments are given at every stage for evaluation purpose.This course is available for $24.Did you ever want to learn to use Python on Big Data? If yes, you search should end here. For a beginner, this is a judicious course to start with python in big data. This course is well designed to give you hands on experience on machine learning in python, and big data too. It also involves working on small projects necessary to test learning equally. This course is delivered online supported by assignments, projects and instructors support.This certification course by Imarticusis a well designed to kick-start your career in analytics with Python. It assumes you have basic knowledge of Python. Its a 7 week course which teaches python basics, data manipulation, data visualization, predictive modeling, machine learning, big data and much more. Being a beginners course, dont expect in-depth knowledge on a topic, but breadth of topics pertaining to data science in python.This is a practical course on learning python using data science hackathons and real time projects. It is priced at INR 23900. It is well structure course which teaches NumPy, SciPy, Pandas, Scikit learn, matplotlib and much more. This course allows you to work on industry problems with industry experts. Along with expert support, youll get 36 hours of live webinar, 1 on 1 weekly meetings with mentor and much more.Start learning python today.This course is no longer available for certification. Yet,Ive mentioned this course because,a beginner would surely miss out on exploring the real mathematics in machine learning, if one skips this course. This course provides an in-depth of knowledge of machine learning algorithms. You wouldnt learn about any tool (R or Python), but the real side of machine learning and its related concepts. This course is delivered by Yaser Abu Mostafa, Professor, Electrical and Computer Science Department, California Institute of Technology.Its the most popular course on Machine Learning. It covers basics as well as practical aspects of machine learning using Octave (programming language).To earn this course certificate, it is essential to complete the course with 180 days of payment made. This course isnt always available for certification. Good news is, this course is starting from January 25, 2016. Hence, if you have been planning long for this certification, dont miss this opportunity.This course acquaints you with both theoretical and practical aspects of machine learning. Having been delivered by Sebastian Thun,the man behind self driving cars, this course very well makes machine learning even more interesting to learn.Italso gives you programming experience in Python. It is a FREE courseand certification is not available.This is one of the newly launched certification courses on machine learning at Coursera. This machine learning specialization in Python consists of 6 courses. It is focused at building machine learning application, primarily using deep learning. Its a great course for people trying to walk one step beyond traditional machine learning methods. This complete specialization is available at $354. At the end of this course, a capstone project accomplishes the extend of learning acquired.This certification on Machine Learning is best suited for R Professionals. It expects prior working knowledge of R Programming. This course is focused on delivering the useful knowledge of using machine learning for training models effectively. The course content is delivered through a blend of videos & interactive web browser coding. After few initial free modules, the complete course is available at $25 monthly.This big data certification by Cloudera is globally recognized. This makes it one of the most sought our certification worldwide. This course requires prior experience of working on big data tools, data science techniques and experience in solving real world data science problems. It includes clearing 3 exams to prove your expertise. These are based on statistics, big data and machine learning.It features a data science challenge solution kit which acts as your study material for this certification.This online university is built to provide every bit of knowledge available on big data. Majority of the courses are free to access. The course quality is good. Instead of finding detailed courses, youll find more short term courses. Theres everything in this mall you want to shop for big data courses. However, only a few courses provide certificates.This comprehensively designed course on big data is focused onteaching variousbig data technologies such as MapReduce, Hadoop used in industry today. The course curriculum includes basic of big data, usingR, machine learning algorithms, database management, big data tools and lab based analytics project. Fee of this certification course is INR 325000. After the successful completion of this course, a student becomes ready forProven Professional Data Scientist Associate (EMCDSA) certification exam.Enrollments in this specialization has started from 4th January 2016. Having been recently launched by coursera, this specialization includes 3 courses and a capstone project. It delivers knowledge and hands on experience on SQL, NoSQL, Spark, Hadoop, MapReduce and much more. This course requires you to work on amazon AWS. Hence, if you are unwilling to use AWS, you may want to reconsider your decision before paying for this course. Overall, its a nice definitive course to get started in big data.This course is best suited for people having prior experience of R. This course is designed for R beginners interested to shift in big data. It starts with basics of big data and teaches in-depth concepts of big data in R using interesting case studies. For a period of 6 months, youll also get access to connect with jigsaw faculty. The quality of content is great. Instructors have done a satisfactory job in simplifying big data concepts. It teaches to use big data technologies such as Hadoop, Pig, Impala, MapReduce. RHadoop and much more.As mentioned above, for free access, big data university has to be thefirst halt in your learning journey. It comprises of various spark tutorials of short durations which are good enough to get you started.You can start with Spark Fundamentals I and Spark Fundamentals II. This should give you a nice head start on its related topics before heading towards the advanced concepts.This certification course by Cloudera is globally recognized. This is a comprehensively designed course which includes every possible aspect of big data technology being used these days including Apache Spark, Impala, Hive, YARN, Sqoop, HDFS, Avro and building apache applications. For every determined applicant, this course should be top of his/her list. By the end of this course, a candidate is expected to successfully clear CCA Spark and Hadoop Developer exams.This certification course is provided by Berkeley, University of California on edX. The course content is free to access. The course certification can be available for $49. It requires prior working knowledge of Python. This course is designed to help candidates learn using Apache Spark for data analysis, using parallel programming, log mining, collaborative filtering and much more. This course would have been the best, had they covered in depth topics of apache spark. Yet, this is a good course to go with.This course is designed to teach using Apache Spark to carve up huge datasets and make some useful sense out of them. It teaches concepts from scratch. This includes solving real life examples helpful to practice side by side. It includes using machine learning libraries, spark sql, GraphX to solve various multi stage issues. This course is best suited for people having some background in software development with working knowledge in Python.It is available for $24.This course is focused on delivering the essentials of large scale data processing using Scala, Hadoop, RDD, Spark, Spark SQL, Mlib, GraphX etc. It enables a candidates to prepare for dealing with various data set challenges emerging in industry today. Using these techniques, the unstructured data can be tamed for analysis use to a higher extent. It includes a project work too. This course is available for INR 15125.Thanks to Udemy. This course is available at $10 only. This course is divided in 9 sections. Every section deals with certain special feature of QlikView, thus ensuring a comprehensive coverage of this tool. Each section comprises of a quiz to test your knowledge.After successful completion of this course, a candidate is expected to build QlikView data models independently. 8According to Gartner 2015 Magic Quadrant, Tableau is the market leader with highest execution ability. It this instigates your appetite, tableau is one visualization tool you should master. This course delivers the necessary knowledge to create interactive dashboard in tableau. Italso explains the adept use of tableau in various stage of business analysis. It is priced at INR 29899.This is one of the recently launched specialization at Coursera. It is best suited for candidates interested to upgrade their excel knowledge towards Big Data. This course involves use of Excel, Tableau and MySQL to analyze and visualize data. This specialization consists of 4 courses and a capstone project. More than theoretical aspects, it involves dealing with industry problems with extra emphasis on visualization and analysis.This course is developed to teach excel functions used to perform financial, statistical and mathematical calculations. Additionally, youll also become smart at using pivot tables, macros, VBA, charts and much more. It includes a project to evaluate your learning towards the end. Fee of this course is INR 5099This is a detailed course on Excel operations. It includes functions, arrays, pivot tables, visualizing data and much more. This course is best suited to students having basic working knowledge of excel. The course content is delivered through pre recorded sessions. The topics have been simply explained using some interesting data sets. This course is available for $24.This is a complete course from basics to advanced level on Excel. It covers basics such as row column operations, functions, arrays, pivot tables, data visualization, VBA, Macros, Dashboards etc. This course is delivered in Classroom and Online modes. This course is available for INR 7000. This course is best suited for candidates aspiring for MIS analyst roles.This course is suited only for candidates having keen to upgrade their excel skills. The course is exceptionally comprehensive which consists of 28 modules and covers excel tillcreating interactive dashboards in excel. Every aspect of VBA has been covered in detail. This is one the advanced level course available for excel analyst. This course only suffers a high fee drawback. It is available for INR 12935.We have ranked these courses based on four factors: Coverage Score, Quality Score, Industry Recognition and Value Score. These courses have beenintensely evaluated based on these factors. We tried to consider user reviews also, but couldnt find satisfactory ones. Then, we assigned certain weights to these factors. It allowed us to completethe analysis with appropriate rankings. Precisely, these factors comprises of following attributes:The courses listed in this article, have been solely selected on the basis of factors listed above. The rankings have been created under no influence or coercion. I hope, this would help you to make decision in choosing the best course for you! Ideally, you should follow these rankings in order. But, if there are financial constrains, Id rather leave the decision upto you.Have you enrolled or finished any of the course listed above? Do share your experience / learning in the comment section below. Id love you discuss with you.",https://www.analyticsvidhya.com/blog/2016/01/top-certification-courses-sas-r-python-machine-learning-big-data-spark-2015-16/
How to use XGBoost algorithm in R in easy steps,Learn everything about Analytics|Overview|Introduction|What is XGBoost?|Preparation of Data for usingXGBoost|Building Model using Xgboost on R|Parameters used in Xgboost|Advancedfunctionalityof xgboost|Testing whether the results make sense|End Notes,"Step 1: Load all the libraries|Step 2 : Load the dataset|Step 3:Data Cleaning & Feature Engineering|Step 4: Tune and Run the model|Step 5: Score the Test Population|General Parameters|Booster Parameters|Linear Booster Specific Parameters|Learning Task Parameters|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Top Certification Courses in SAS, R, Python, Machine Learning, Big Data, Spark|Improvising Hackathon platform, Blogathon, Profile pages, Points and much more|
Tavish Srivastava
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Did you knowusing XGBoost algorithm is one of the popular winning recipe ofdata science competitions ?So, what makes it more powerful than a traditional Random Forest or Neural Network? In broad terms, its the efficiency, accuracy and feasibility ofthis algorithm. (Ive discussed this part in detail below).In the last few years, predictive modeling has become much faster and accurate. I remember spending long hours on feature engineering for improving model byfew decimals. A lot of that difficult work, can now be done by using better algorithms.Technically, XGBoost is a short form for Extreme Gradient Boosting. It gained popularityin data scienceafter the famous Kaggle competition called Otto Classification challenge.The latest implementation on xgboost on R was launched in August 2015. We will refer to this version (0.4-2) in this post.In this article, Ive explained a simple approach to use xgboost in R. So, next time when you build a model, do consider this algorithm. Im sure it would be a moment of shock and then happiness!How to use XGBoost algorithm in R in easy stepsExtreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more efficient. It has both linear model solver and tree learning algorithms. So, what makes it fast is its capacity to doparallel computation on a single machine.This makes xgboost at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.Since it isvery high inpredictive power but relatively slow with implementation, xgboost becomes an ideal fit for many competitions. It also hasadditional features for doingcross validation and finding important variables. There are many parameters which needs tobe controlled to optimize the model. We will discuss about these factors in the next section.XGBoost only works with numericvectors. Yes! you need to work on data types here.Therefore, you need to convert all other forms of data into numeric vectors. Asimple method to convert categorical variable into numeric vector is One Hot Encoding.This term emanatesfrom digital circuit language, where it means an array of binary signals and only legal values are 0s and 1s.In R, one hot encoding is quite easy. This step (shown below) will essentially make a sparse matrix using flags on every possible value of that variable. Sparse Matrix is a matrix where most of the values of zeros. Conversely, a dense matrix is a matrix where most of the values are non-zeros.Lets assume, you have a dataset named campaign and want to convert all categorical variables into such flags except the response variable. Here is how you do it :Now lets break down this codeas follows:To convert the target variables as well, you can use following code:Here is what thecodedoes:Here are simple steps you can use to crack any data problem using xgboost:(Here I use a bank data where we need to find whether a customer is eligible for loan or not).I am using a list of variables in feature_selected to beused by the model. I have shared aquick and smartway to choose variables later in this article.And thats it! You now have an object xgb which is an xgboost model. Here is how you score a test population :I understand, by now, you would be highly curious to know about various parameters used in xgboost model. So, there arethree types of parameters: General Parameters, Booster Parameters and Task Parameters.Lets understand these parameters in detail. I require you to pay attention here. This is the most critical aspect of implementing xgboost algorithm:The tree specific parameters Compared toother machine learning techniques, I find implementation of xgboost really simple. If you did all we have done till now, you already have a model.Lets take it one step further and try to find the variable importance in the model and subset our variable list.As you can observe, many variables are just not worth usinginto our model. You can conveniently remove these variables and run the model again. This time you can expect a better accuracy.Lets assume, Age was the variable which came out to be most important from the above analysis. Here is a simple chi-square test which you can do to see whether the variable is actually important or not.We can do the same process for all important variables. This will bring out the fact whether the model has accurately identified all possible important variables or not.With this article, you can definitely builda simple xgboost model. You will be amazed to see the speed of this algorithm against comparable models. In this post, I discussed various aspects of using xgboost algorithm in R. Most importantly, you must convert your data type to numeric, otherwise this algorithm wont work.Also, I would suggest you to pay attention to these parameters as they can make or break any model. If you still find these parameters difficult to understand, feel free to ask me in the comments section below.Did you find the article useful? Have you used this technique before? How did the model perform? Do you use some better (easier/faster) techniques for performing the tasks discussed above? Can you replicate the codes inPython? Well be glad if you share your thoughts as comments below.",https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/
"Improvising Hackathon platform, Blogathon, Profile pages, Points and much more",Learn everything about Analytics,"New features of our DataHack platform:|1. Functionality to participate in teams in AV hackathons|2. Badges for activities on Analytics Vidhya|3. Point system for activities on Analytics Vidhya|4. Rankings based on points|5. Profile pages|6. Competitions of different formats|Above all, Thank You So Much Avians!||Get Involved!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|How to use XGBoost algorithm in R in easy steps|Tutorial  Python List Comprehension With Examples|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",How does this help you?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Continuous improvement is better than delayed perfection!-Mark TwainI cant stop smiling and deeply appreciating this statement from Mark Twain. He has summarized the life of an entrepreneur in a statement as simple as this!Some time mid last year, we decided to conduct an online data science competition for our community. The only experience we had was by conducting a few offline hackathons and Kaggle competitions we had participated in the past.There were a lot of uncertainties attached to the execution of an online data science competition. We faced several questions  how to distribute data? How to decide the winner?We decided to keep things simple and functional.This is how the first hackathon screen looked afterthe overnight hack I had done:I am sure the designers and UI / UX specialists out there would discern at this screen, but there was some thing magical about it. It just did what it was supposed to  tell people the accuracy of there solutions through a simple interface. The simplicity and responsiveness of platformclicked with the users and we had close to thousand submissions over a 2 day period!We havent looked back since! This ishow the platform looks today.I am glad to announce our First Update of Hackathon Platform for the year. It has come after some toil at our end. But it has come in good time  just before last man standing. And this particular update is very close to my heart. Why?The short answer is that this unifies everything we do at Analytics Vidhya. Ill come back to this again. For now, lets look at the list of updateswhich are addedin this version.All right! Ahem! Mike testing  Hello 1, 2, 3! Yep  this is working.Now that I am at the center of this stage, let me reveal the updates briefly!Let us look at details of each of these new features:This was definitely one of the most awaited feature on our hackathon platform. We hadreceived several requests for enabling this and now we have this feature in the platform.When I look back at the projects where I had the highest amount of learning in data science, most of them happened to be in team. We were a team of analysts fighting out a business problem or the other with tight deadline. And this is what we have simulated on our platform.If you believe you learnt a lot in any of the past hackathon  wait and watch what a team can do for you. The math is very simple  your prize splits and your learning / chances of winning increase multi-fold. You need to select the right partner, of course.A few specifics about participating as a team:Wow  I cant wait to see teams in action during our upcoming hackathon.Badges will be provided for all activities on Analytics Vidhya  starting from blogging, discussion portals and hackathons. Here is the complete list of badges for each of these:Over time, the best badges would be able to define your profile in a crisp manner.While badges define your AV profile at a macro level, points is what differentiates the best among the good ones. Points are meant to push ourselves to achieve more than what we would normally do.Here is how the point system is defined currently:where,Total Points: The numbers of points allocated to a contest. This can vary depending on the contest. The bigger the stage, higher the points. All signature hackathons till now carry 1,000 points each.
Number of TeamMates: The number of members in a team
Rank: Your team final standing on private leader board
Percentile: Your team relative position to other participantsNote: As of now, the maximum number of teammates allowed in a team are 2.Here is how rankings page looks on our most visited page:Profile Pages have been carefully designed toreveal the true credibilityof a participant. This includes rank, badges earned, points, last activity, hackathon participation, likes etc. In this update, the focus has been kept more on presenting important metrics of a participant.We will continue to evolve this to make it more useful for you.Currently, this is how the profile page looks:To ensure a thorough learning, we have introduced various forms of competition suited for people of different competitive level. For example, if you are a beginner, you might find hackathons intimidating, instead you can participate in practice problems. Needless to say, the advanced user would love to test their water in the real battlefield. In addition, we have MCQ challenges, assessment test, blogathon to keep you engaged.How do these changes sound? If you are thinking this was a lot of work in small time  we bet! If you have feedback, thoughts, suggestions  we are all ears. Drop a comment, reach out to us, post it on our FB page, tweet us and we will respond. If you like love the changes we have made  leave a high five in the comments below.While I have done most of the talking writing, I wanted to present stories and testimonials from some of the users.It gives us immense satisfaction and deep pleasure that we are able to impact more and more people across the globe every day.This journeywould not have been possible with out support of our community, the team at Analytics Vidhya, our mentors, families and friends. I cant thank them enough and I promise that the fun has just begun!With out much delay here is what some AVians had to say about Analytics Vidhya:Sudalai RajkumarAV is one of the best knowledge portal for Data Science I know about and certainly the best in India. It caters to the need of all sorts of people, from beginners to well experienced professionals. The articles are in-depth in nature and cover wide-variety as well which is very difficult to find else where. Discuss portals help people to clarify their doubts while hackathons give a flavor of real world experience. Thank you AV and keep up the good work.!VikashAV is the best community driven DS group out there.
Whether its data-hacks, meetups or forums, the community is always there to help.
Have learnt a lot being an AVian. Thanks guys Kumar AbhijeetThe blog posts are very informative, the community is very supportive and the regular hackathons are arguably the best short-term data science competitions on the internet. I recommend Analytics Vidhya to everyone interested in data scienceThe blog posts are very informative, the community is very supportive and the regular hackathons are arguably the best short-term data science competitions on the internet. I recommend Analytics Vidhya to everyone interested in data scienceWhat are you waiting for? There hasnt been a better time in history to be a data science professional. We believe, there hasnt been a knowledge platform better than Analytics Vidhya to learn. And remember  the journey has just begun!Come, join the community. Register for our signature hackathon  last man standing, blogathonor participate in the practice problems out there.",https://www.analyticsvidhya.com/blog/2016/01/update-introducing-hackathon-blogathon-profile-pages-points/
Tutorial  Python List Comprehension With Examples,Learn everything about Analytics|Introduction|Table of Contents|1. What is List Comprehension (LC)?|2. The Time advantage|3. Generator Expressions (GE)|4. Using this concept inData Science|End Notes,"Example 1: Flatten a Matrix|Example 2: Removing vowels from a sentence|Example 3: Dictionary Comprehension|map function|Comparing Runtime|A Simple Example|Taking a step forward|Getting into crux of the matter|Why use Generator in Python?|Example 4: Reading a list of list|Example 5: Creating powers of a columns for Polynomial regression|Example 6: Filtering column names|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Improvising Hackathon platform, Blogathon, Profile pages, Points and much more|[Infographic] 10 Popular TV Shows on Data Science and Artificial Intelligence|
Aarshay Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"List comprehension is powerful and must know the concept in Python. Yet, this remains one of the most challenging topic for beginners. With this post, I intend help each one of you who is facing this trouble in python. Mastering this concept would help you in two ways:Do you know List Comprehensions are 35% faster than FOR loop and 45% faster than map function? I discovered these and many other interesting facts in this post.If you are reading this, Im sure either you want to learn it from scratch or become better at this concept. Both ways, this post will help you.P.S  This article is best suited for beginners and intermediates users of Python for Data Science. If you are already an experienced python user, this post might not help you much.When I first used LC, it took me down the memory lane and reminded me of the set-builder form.Yes you got me right. Set builder is a concept mostly taught in Class XI in India.(check this). Ive picked some of easy examples from this text book to explain this concept.Lets look at some of basic examples:Now lets look at the corresponding Python codes implementing LC (in the same order):Looks intuitive right?Dont worry even if it feels a bit foggy. Just hang on with me, things will become more clear as we proceed. Lets take a step back and see whats happening here. Each of the above example involves 3 things  iteration, conditional filtering and processing. Hence, you might sayits just another representation ofFOR loop.In a general sense, a FOR loop works as:Consider another example: { x: x is a natural number less than or equal to100, x is a perfect square }
This can be solved using a for-loop as:Now, to create the LC code, we need to just plug in the different parts:I hope it is making more sense now. Once you get a hang of it, LC is a simple but powerful technique and can help you accomplish a variety of tasks with ease. Things to keep in mind:Lets look at some more examples to understand this in detail.Predefined functions might exist for carrying out the tasks explained below, but these are good examples to get a grasp of LC.Note: I will be confining all my codes in functions. Its generally a good practice as it enhances re-usability but I carrya specialmotive here whichIve revealed below.Aim: Take a matrix as input and return a list with each row placed on after the other.Python codes with FOR-loop and LC implementations:Aim: Take a string as input and return a string with vowels removed.Python codes with FOR-loop and LC implementations:Lets define a matrix and test the results:Output:Aim: Take two list of same length as input and return a dictionary with one as keys and other as values.Python codes with FOR-loop and LC implementations:Lets define a matrix and test the results:Output:I believe with the color convention in place, things are getting pretty much self-explanatory by now. Till now, we have focused on 1 aspect of LC, i.e. its brevity and readability. But thats not it! LC are also faster in various scenarios as compared to some of the alternatives available. Lets explore this further.In this section, we will find out the time taken by LC in comparison to similar techniques. We will also try to unravel the situations in which LC works better and where it should be avoided. Along with runtime, we will compare the readability of the different approaches.Before jumping into comparing the time taken by different techniques, lets take a refresher on the Python map function.It is used to apply a function to each element of a list or any other iterable.
Syntax: map(function, Python iterable)For example, we can multiply each element of a list of integers with the next number using the following code:An important part of this exercise is the ability to compare the running time of a code fragment. We will be using %timeit, an in-built magic function of iPython notebook environment. Alternatively, you can use the time or timeit modules.We will be comparing the runtimes of implementation of same result using LC, FOR-loop and map function. Also, we shall focus on the relative run times and not the absolute values because it is subject to the machine specs. FYI, I am using a Dell XPS 14Z system with following specs:
2nd Gen i5 (2.5GHz) | 4GB RAM | 64-bit OS | Windows 7 Home PremiumLets start with a simpleexample  squaring each element of a list. The Python codes and runtimes for each of the 3 implementations are:Output: 100000 loops, best of 3: 2.69 s per loopOutput: 100000 loops, best of 3: 3.16 s per loopOutput: 100000 loops, best of 3: 1.67 s per loopRunTime: We can clearly see that in this case LC is ~35% faster than for-loop and ~45% faster than map function.
Readability: Both LC and map functions are fairly simple and readable, but for-loop is a bit bulky but not much.So can we say LC is the fastest method? Not necessary! We cant generalize our conclusions at this stage as it might be specific to the problem at hand. Lets consider a slightly trickyexample.Lets include a catch in above problem. What if we want the square of only even numbers in the list? Now, the three functions would look like:Output: 100000 loops, best of 3: 2.31 s per loopOutput: 100000 loops, best of 3: 5.25 s per loopOutput: 100000 loops, best of 3: 1.74 s per loopRunTime: LC is ~25% faster than for-loop and ~65% faster than map function.
Readability: LC is most concise and elegant. Map function became cumbersome with additional lambda function and for-loop is no better.I think now we have gained some more confidence on LC. Even I cant waitto compare the runtime of advanced applications. But hang on! I am not totally convinced.Why is LC faster? After all each technique is doing the same thing on the same scale  iteration, conditional filtering and execution. So what makes LC special? Will it be faster in all scenarios or are these special cases? Lets try to find out!Lets compare each algorithm on 3 fronts:First, lets call a simple function that has no obligation of doing anything and primary role is iteration. The codes and runtime of each technique is:Output: 1000000 loops, best of 3: 714 ns per loopOutput: 1000000 loops, best of 3: 2.29 s per loopOutput: 1000000 loops, best of 3: 1.18 s per loopRunTime: FOR-loop is fastest. This is because in a for-loop, we need not return an element and just move onto next iteration using pass. In both LC and map, returning an element is necessary. The codes here return None. But still map takes more than twice the time. Intuitively, we can think that map involves a definite function call at each iteration which can be the reason behind the extra time. Lets explore this at later stage.Now, lets perform a simple operation of multiplying the number by 2 but we need not store the result:Output: 1000000 loops, best of 3: 1.07 s per loopOutput: 100000 loops, best of 3: 2.61 s per loopOutput: 1000000 loops, best of 3: 1.44 s per loopRunTime: Here we see a similar trend as before. So, till the point of iterating and making slight modifications, for-loop is clear winner. LC is close to for-loop but again map takes around twice as much time. Note that here the difference between time will also depend on the complexity of the function being applied to each element.An intuition for higher time of map and LC can be that both methods have a compulsion of storing information and we are actually performing all 3 steps for them. So lets check runtime of for-loop with step 3:Output: 100000 loops, best of 3: 2.55 s per loopThis is interesting! The runtime of FOR jumps to ~2.5 times just because of storing the information. The reason being that we have to define an empty list and append the result to each in each iteration.After all 3 steps, LC seems to be the clear winner. But are you 100% sure ? Im not sure about you, but I am still not convinced. Both LC and map have an elegant implementation (in this case) and I dont see why map is so slow.One possible reason could beprobably map is slower because it has to make function calls at each step. LC might just be calculating the value of the same expression for all elements. We can quickly check this out. Lets make a compulsory function call in LC as well:def x2_lc(arr): def mul(x): return x*2 [mul(i) for i in arr] %timeit x2_lc(range(1,11))Output: 100000 loops, best of 3: 2.67 s per loopAha! So the intuition was right. When we force LC to make function calls, it ends up taking similar time as the map function.Lets summarize our findings now:I have compared the runtime of 3 examples we saw earlier and the results are:
In the first two cases, LC is faster because they involve storing values in a list. However, in case 3 FOR-loop is faster because it involves dictionary comprehension. Sincedictionaries are implemented as hash tables, adding new values is significantly faster as compared to list. Thus we see that the first three examples resonate with our initial findings.Do you want to explore this further? Why not try the following 3 problems and compare the run-time of FOR-loop implementation vs List Comprehension and we can discuss in comments:Now, we will look at another facet of LC which will allow you to make it even faster in certain specific applications.Never heard of generators in Python? Lets try to understand.A generator is an iterator but it returns a single value at a time and maintains the iteration count between calls. Its like bowling an over in cricket. The various steps are:A generator works in similar way. Key things to note about a generator:Lets consider a simple example  generate integers from 0 to the number N passed as argument to generator.#Generator definition:Now what happens when we print it? Lets try. (Dont worry about the yieldkeyword. Well come to that in a bit)It doesnt return a list like a traditional for-loop. In fact, it just returns an object. So that way it works is that we have assign this object to a variable and then get numbers using a nextstatement. In this example, the flow would be:Output: 0Output: 1Output: 2I hope its making sense. But how does it know which value to return? Thats where the yieldstatement comes in. Whatever nextis called, generator executes till it encounters a yield statement. The value written after yield will get returned and execution stops. If yield is not found, it returns an error. Lets consider another example to see the flow of information:The outputs for subsequent calls are:
I hope things are very much clear now. If you think about an over in cricket, everything will start making sense. There is just one problem, we dont know when it ends unless it returns an error. One solution is using try and except and check for StopIteration error. But, Python automatically manages this when generators are called within functions which accept an iterator. For example:Output: 45Note:This starts from current state and works to end. If a next statement is called before sum, it wont take the numbers already returned into account. Example:Output: 39Having understood the basics of generators, lets come to Generator Expressions (GE). Essentially, this can simply be done by replacing the [] with (). For example:Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Output: at 0x0000000003F7A5A0>Again, we can apply some function which takes an iterable as an argument to the LC:Output: 45Generator is a very useful concept and I encourage you to explore it further.You must be wondering why should one use a generator! It works one-by-one, has some inherent properties to be kept in mind and it is not compatible with many functions and even additional libraries.The biggest advantage is that generators have very low memory requirements as they return numbers on the fly. On the other hand, using simple LC will first create a list in memory and then sum it. However, generators might not perform well on small problems because they make many function calls making it slower. It should be used for solving computations which are too heavy on memory.Consider following functions:I ranthese functions for 3 different values of N and the runtimes are illustrated below:
Conclusions:Moving towards the end, lets discuss some applications of these concepts immensely useful for adata scientist. A quick recap, remember the general structure of LC:You might have faced situations in which each cell of a column in a data set contains a list. I have tried to simulate this situation using a simple example based on this data. This contains a dataframe having2 columns:Here, you can see the different sports a person plays mentioned in the same column. To make this data usable in a predictive model, its generally a good idea to create a new column for each sport and mark it as 1 or 0 depending on whether the person plays it or not.First step is to convert the text into a list so that individual entries can be accessed.Next, we need a unique list of games to identify the different number of columns required. This can be achieved through set comprehension. Sets are collection of unique elements. Refer to this article for basics of python data structures.Note that, here we have used generator expression so that each value gets updated on the fly and storing is not required. Now we will generate a matrix using LC containing 5 columns with 0-1 tags corresponding to each sport.Note that, here again one instance of LC has been nested into another. The inner LC is highlighted using bold square brackets. Please follow the color convention for understanding. If still unclear, I recommend trying a for-loop implementation and then converting to LC. Feel free to leave a comment below in case you face challenges at this stage.The last step is to convert this into Pandas dataframes column which can be achieved as:Polynomial regression algorithm requires multiple powers of the same variable, which can be created using LC. As much as 15-20 powers of the same variable might be used and modeled along with Ridge regression to reduce overfitting.Lets create a 1-column data set for simplicity:Lets define a variable deg containing the number of degrees required. Keeping it dynamic is generally a good practice to allow flexibility. Our first step is to create a matrix containing the different powers of number variable.Now we will add this to the dataframe similar to the previous example. Note that, we need a list of column names in this case and again LC can be used to get it fairly easily:Thus the powers have been successfully added.Personally, I have faced this issue numerous times of selecting a subset of columns in a dataframe for the purpose of setting as predictors ina predictive model. Lets consider a situation where the total columns are:These can be understood as:Depending on the analysis being performed or the model being used, there might be different use cases:These can be done using following code.This brings me to an end of this tutorial.Here we took a deep dive into List Comprehension in Python. We learned about time advantage of LC, comparison of LC with map and FOR loop, generator expressions and explored some advanced applications.If you have reached this point, I believe that now you would be able to appreciate the importance of LC. You should try to incorporate this technique in daily practice. Though it might be a bit time consuming in beginning, but trust me you are going to enjoy it as you proceed further.Did you find this article helpful ?Did I miss something ? Do you have some more interesting applications where you think LC will be useful? Please share your comments and I will be more than happy to discuss.",https://www.analyticsvidhya.com/blog/2016/01/python-tutorial-list-comprehension-examples/
[Infographic] 10 Popular TV Shows on Data Science and Artificial Intelligence,Learn everything about Analytics|Introduction,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Tutorial  Python List Comprehension With Examples|A Complete Python Tutorial to Learn Data Science from Scratch|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The development of full artificial intelligence could spell the end of human race.  Stephen HawkingThe world is now rapidly moving towards achieving this finest technology breakthrough ever.It is expected that AI would enrich humans with more power and opportunities. Another group of people (including Stephen Hawking and Elon Musk) believe that this might lead to human destruction (if not handled carefully).I think, its too early for us to envisage such uncertain future. Good news is, companies like Google, Microsoft, Baidu have already started creating products based on AI. It wont belong enough to experiencethe influence of AI in our daily lives.Accidentally, my exploration of AI started with movie Her. The influence was so powerful that I ended up creating an infographic on 10 Movies on Data Science and Machine Learning. May be a ~ 2 hours movie didnt nourish my appetite well.Few days later, I came across aTV Series Intelligence. So far, this is one of the best show Ive ever watched.Later, I found many other shows which beautifullydescribes a future world driven by data, technology, numbers and artificial intelligence. Here Ive shared the best of them.Below are 10 Popular TV Shows on Data Science and Artificial Intelligence ( in no particular order). If you havent seen any of them, Id suggest you to begin with Intelligence.",https://www.analyticsvidhya.com/blog/2016/01/10-popular-tv-shows-data-science-artificial-intelligence/
A Complete Python Tutorial to Learn Data Science from Scratch,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Basics of Python for Data Analysis|2. Python libraries and Data Structures|3. Exploratory analysis in Python using Pandas|4. Data Munging in Python : Using Pandas|5. Building a Predictive Model in Python|Projects|End Notes,"Why learn Python for data analysis?|Python 2.7 v/s 3.4|How to install Python?|Warming up: Running your firstPython program|Few things to note|Python Data Structures|Python Iteration and Conditional Constructs|Python Libraries|Introduction to Series and Dataframes|Practicedata set Loan Prediction Problem|Lets begin with the exploration|Importing libraries and thedata set:|Quick Data Exploration|Distribution analysis|Categorical variable analysis|Data munging  recap of the need|Check missing values in the dataset|How to fill missing values in LoanAmount?|How to treat for extreme valuesin distribution of LoanAmount and ApplicantIncome?|Logistic Regression|Decision Tree|Random Forest|Note  The discussions of this article are going on at AVs Discuss portal. Join here!|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|[Infographic] 10 Popular TV Shows on Data Science and Artificial Intelligence|Model Monitoring Senior Business Analyst/Assistant Manager  Gurgaon (5-6 years of experience)|
Kunal Jain
|54 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",Why Python 2.7?|Why Python 3.4?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It happened a few years back. After working on SAS for more than 5 years, I decided to move out of my comfort zone.Being a data scientist, my hunt for other useful tools was ON! Fortunately, it didnt take me long to decide  Python was my appetizer.I always had an inclination for coding. This was the time to do what I really loved. Code. Turned out, coding was actually quite easy!I learned the basics of Python within a week. And, since then, Ive not only explored this language to the depth, but also have helped many other to learn this language. Python was originally a general purpose language. But, over the years, with strong community support, this language got dedicated library for data analysis and predictive modeling.Due to lack of resource on python for data science, I decided to create this tutorial to help many others to learn python faster.In this tutorial, we will take bite sized information about how to use Python for Data Analysis, chew it till we are comfortable and practice it at our own end.A complete python tutorial from scratch in data science.You can also check out the Introduction to Data Science course  a comprehensive introduction to the world of data science. It includes modules on Python, Statistics and Predictive Modeling along with multiple practical projects to get your hands dirty.Lets get started!Python has gathered a lot of interest recently as a choice of language for data analysis. I had basics of Python some time back. Here are some reasons which go in favour of learning Python:Needless to say, it still has few drawbacks too:This is one of the most debated topics in Python. You will invariably cross paths with it, specially if you are a beginner. There is no right/wrong choice here. It totally depends on the situation and your need to use. I will try to give you some pointers to help you make an informed choice.There is no clear winnerbutI suppose the bottom line is thatyou should focus on learning Python as a language. Shifting between versions should just be a matter of time. Stay tuned for a dedicated article on Python 2.x vs 3.x in the near future!There are 2 approaches to install Python:Second method provides a hassle free installation and hence Ill recommend that to beginners. The imitation of this approach is you have to wait for the entire package to be upgraded, even if you are interested in the latest version of a single library. It should not matter until and unless, until and unless, you are doing cutting edge statistical research.Choosing a development environmentOnce you have installed Python, there are various options for choosing an environment. Here are the 3 most common options:IDLE editor for PythonWhile the right environment depends on your need, I personally prefer iPython Notebooks a lot. It provides a lot of good features for documenting while writing the code itself and you can choose to run the code in blocks (rather than the line by line execution)We will use iPython environment forthis complete tutorial.You can use Python as a simple calculator to start with:Before we deep dive into problem solving, lets take a step back and understand the basics of Python. As we know that data structures and iteration and conditional constructs form the crux of any language. In Python, these include lists, strings, tuples, dictionaries, for-loop, while-loop, if-else, etc. Lets take a look at some of these.Following are some data structures, which are used in Python. You should be familiar with them in order to use them as appropriate.Here is a quick example to define a list and then access it:Since Tuples are immutable and can not change, they are faster in processing as compared to lists. Hence, if your list is unlikely to change, you should use tuples, instead of lists.Like most languages, Python also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax:Here Python Iterable can be a list, tuple or other advanced data structures which we will explore in later sections. Lets take a look at a simple example, determining the factorial of a number.Coming to conditional statements, these are used to execute code fragments based on a condition. The most commonly used construct is if-else, with following syntax:For instance, if we want to print whether the number N is even or odd:Now that you are familiar with Python fundamentals, lets take a step further. What if you have to perform the following tasks:If you try to write code from scratch, its going to be a nightmare and you wont stay on Python for more than 2 days! But lets not worry about that. Thankfully, there are many libraries with predefined which we can directly import into our code and make our life easy.For example, consider the factorial example we just saw. We can do that in a single step as:Off-course we need to import the math library for that. Lets explore the various libraries next.Lets takeone step ahead in ourjourney to learn Python by getting acquainted with some useful libraries. The first step is obviously to learn to import them into our environment. There are several ways of doing so in Python:In the first manner, we have defined an alias m to library math. We can now use various functions from math library (e.g. factorial) by referencing it using the alias m.factorial().In the second manner, you have imported the entire name space in math i.e. you can directly use factorial() without referring to math.Following are a list of libraries, you will need for any scientific computations and data analysis:Additional libraries, you might need:Now that we are familiar with Python fundamentals and additional libraries, lets take a deep dive into problem solving through Python. Yes I mean making a predictive model! In the process, we use some powerful libraries and also come across the next level of data structures. We willtake you through the 3 key phases:In order to explore our data further, let me introduce you to another animal (as if Python was not enough!)  PandasImage Source: WikipediaPandas isone of the most useful data analysis library in Python (I know these names sounds weird, but hang on!). They have been instrumental in increasing the use of Python in data science community. We will now use Pandas to read a data set from an Analytics Vidhya competition, perform exploratory analysis and build our first basic categorization algorithm for solving this problem.Before loading the data, lets understand the 2 key data structures in Pandas  Series and DataFramesSeries can be understood as a 1 dimensional labelled / indexed array. You can access individual elements of this series through these labels.A dataframe is similar to Excel workbook  you have column names referring to columns and you have rows, which can be accessed with use of row numbers. The essential difference being that column names and row numbers are known as column and row index, in case of dataframes.Series and dataframes form the core data model for Pandas in Python. The data sets are first to read into these dataframes and then various operations (e.g. group by, aggregation etc.) can be applied very easily to its columns.More:10 Minutes to PandasYou can download the dataset from here. Here is the description of the variables:To begin, start iPython interface in Inline Pylab mode by typing following on your terminal/windows command prompt:This opens up iPython notebook in pylab environment, which has a few useful libraries already imported. Also, you will be able to plot your data inline, which makes this a really good environment for interactive data analysis. You can check whether the environment has loaded correctly, by typing the following command (and getting the output as seen in the figure below):I am currently working in Linux, and have stored the dataset in the following location:/home/kunal/Downloads/Loan_Prediction/train.csvFollowing are the libraries we will use during this tutorial:Please note that you do not need to import matplotlib and numpy because of Pylab environment. I have still kept them in the code, in case you use the code in a different environment.After importing the library, you read the dataset using function read_csv(). This is how the code looks like till this stage:Once you have read the dataset, you can have a look at few top rows by using the function head()This should print 10 rows. Alternately, you can also look at more rows by printing the dataset.Next, you can look at summary of numerical fields by using describe() functiondescribe() function would provide count, mean, standard deviation (std), min, quartiles and max in its output (Read this article to refresh basic statistics to understand population distribution)Here are a few inferences, you can draw by looking at the output of describe() function:Please note that we can get an idea of a possible skew in the data by comparing the mean to the median, i.e. the 50% figure.For the non-numerical values (e.g. Property_Area, Credit_History etc.), we can look at frequency distribution to understand whether they make sense or not. The frequency table can be printed by following command:Similarly, we can look at unique values of port of credit history. Note that dfname[column_name] is a basic indexing technique to acessa particular column of the dataframe. It can be a list of columns as well. For more information, refer to the 10 Minutes to Pandas resource shared above.Now that we are familiar with basic data characteristics, let us study distribution of various variables. Let us start with numeric variables  namely ApplicantIncome and LoanAmountLets start by plotting the histogram of ApplicantIncome using the following commands:Here we observe that there are few extreme values. This is also the reason why 50 bins are required to depict the distribution clearly.Next, we look at box plots to understand the distributions. Box plot for fare can be plotted by:This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:We can see that there is no substantial different between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers.Now, Lets look at the histogram and boxplot of LoanAmount using the following command:Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging. LoanAmount has missing and well as extreme values values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections.Now that we understand distributions for ApplicantIncome and LoanIncome, let us understand categorical variables in more details. We will use Excel style pivot table and cross-tabulation. For instance, let us look at the chances of getting a loan based on credit history. This can be achieved in MS Excel using a pivot table as:Note: here loan status has been coded as 1 for Yes and 0 for No. So the mean represents the probability of getting loan.Now we will look at the steps required to generate a similar insight using Python.Please refer to this article for getting a hang of the different data manipulation techniques in Pandas.Now we can observe that we get a similar pivot_table like the MS Excel one. This can be plotted as a bar chart using the matplotlib library with following code:This shows that the chances of getting a loan are eight-fold if the applicant has a valid credit history. You can plot similar graphs by Married, Self-Employed, Property_Area, etc.Alternately, these two plots can also be visualized by combining them in a stacked chart::You can also add genderinto the mix (similar to the pivot table in Excel):If you have not realized already, we have just created two basic classification algorithms here, one based on credithistory, while other on 2categorical variables (including gender). You can quickly code this to create your first submission on AV Datahacks.We justsaw how we can do exploratory analysis in Python using Pandas. I hope your love for pandas (the animal) would have increased by now  given the amount of help, the library can provide you in analyzing datasets.Next letsexplore ApplicantIncome and LoanStatusvariables further, perform data munging and create a dataset for applying various modeling techniques. I would strongly urge that you take another dataset and problem and go through an independent example beforereading further.For those, who have been following, here are your must wearshoes to start running.
While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as Data Munging. Here are the problems, we are already aware of:In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Educationand Dependentsto see, if they contain any useful information.If you are new to Pandas, I would recommend readingthis articlebefore moving on. It detailssome useful techniques of data manipulation.Let us look at missing values in all the variables because most of the models dont work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the datasetThis command should tell us the number of missing values in each column as isnull() returns 1, if the value is null.Though the missing values are not very high in number, but many variables have them and each one of these should be estimated and added in the data. Get a detailed view on different imputation techniques through this article.Note: Remember that missing values may not always be NaNs.For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and youre right. So we should check for values which are unpractical.There are numerous ways to fill the missing values of loan amount the simplest being replacement by mean, which can be done by following code:The other extreme could be to build a supervised learning model to predict loan amounton the basis of other variables and then use age along with other variables to predict survival.Since, the purpose nowis to bring out the steps in data munging, Ill rather take an approach, which lies some where in between these 2 extremes. Akey hypothesis is that thewhether a person is educated orself-employed can combine to give a good estimate of loan amount.First, lets look at the boxplot to see if a trend exists:Thus we see some variations in the median of loan amount for each group and this can be used to impute the values. But first, we have to ensure that each of Self_Employed and Education variables should not have a missing values.As we say earlier, Self_Employed has some missing values. Lets look at the frequency table:Since ~86% values are No, it is safe to impute the missing values as No as there is a high probability of success. This can be done using the following code:Now, we will create a Pivot table, which provides us median values for all the groups of unique values of Self_Employed and Education features. Next, we define a function, which returns the values of these cells and apply it to fill the missing values ofloan amount:This should provide you a good way to impute missing values of loan amount.NOTE : This method will work only if you have not filled the missing values in Loan_Amount variable using the previous approach, i.e. using mean.Lets analyze LoanAmount first. Since the extreme values are practically possible, i.e. some people might apply for high value loans due to specific needs. So instead of treating them as outliers, lets try a log transformation to nullify their effect:Looking at the histogram again:
Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.Coming to ApplicantIncome. One intuition can be that some applicants have lower income but strong support Co-applicants. So it might be a good idea to combine both incomes as total income and take a log transformation of the same.Now we see that the distribution is much better than before. I will leave it upto you toimpute the missing values forGender, Married, Dependents, Loan_Amount_Term, Credit_History. Also, I encourage you to think about possible additional information which can be derived from the data. For example, creating a column for LoanAmount/TotalIncome might make sense as it gives an idea of how well the applicant is suited to pay back his loan.Next, we will look at making predictive models.After, we have made the data useful for modeling, lets now look at the python code to create a predictive model onour data set. Skicit-Learn (sklearn) is the most commonly used library in Python for this purpose and we will follow the trail. I encourage you to get a refresher on sklearn through this article.Since, sklearn requires all inputs to be numeric, we should convert all our categorical variables into numeric by encoding the categories. Before that we will fill all the missing values in the dataset. This can be done using the following code:Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores. Since this is an introductory article, I will not go into the details of coding. Please refer to this article for getting details of the algorithms with R and Python codes. Also, itll be good to get a refresher oncross-validation through this article, as it is a very important measure of power performance.Lets make ourfirst Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting (dont worry if youre unaware of this terminology yet). In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. Read more about Logistic Regression.We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for:So lets make our first model with Credit_History.Accuracy : 80.945% Cross-Validation Score : 80.946%Accuracy : 80.945% Cross-Validation Score : 80.946%Generally we expect the accuracy to increase on adding variables. But this is a more challenging case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now:Decision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more about Decision Trees.Accuracy : 81.930% Cross-Validation Score : 76.656%Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Lets try a few numerical variables:Accuracy : 92.345% Cross-Validation Score : 71.009%Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Lets try an even more sophisticated algorithm and see if it helps:Random forest is another algorithm for solving the classification problem.Read more about Random Forest.An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features.Accuracy : 100.000% Cross-Validation Score : 78.179%Here we see that the accuracy is 100% for the training set. This is the ultimate case of overfitting and can be resolved in two ways:Lets try both of these. First we see the feature importance matrix from which well take the most important features.Lets use the top 5 variables for creating a model. Also, we will modify the parameters of random forest model a little bit:Accuracy : 82.899% Cross-Validation Score : 81.461%Notice that although accuracy reduced, but the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark.You would have noticed that even after some basic parameter tuning on random forest, we have reached a cross-validation accuracy only slightly better than the original logistic regression model. This exercise gives us some very interesting and unique learning:You can access the dataset and problem statement used in this post at this link:Loan Prediction ChallengeNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems:I hope this tutorial will helpyou maximize your efficiency when starting with data science in Python. I am sure this not only gave you an idea aboutbasic data analysis methods but it also showed you how to implement some of the more sophisticated techniques available today.You should also check out our free Python course and then jump over to learn how to apply it for Data Science.Python is really a great tool and is becoming an increasingly popular language among the data scientists. The reason being, its easy to learn, integrates well with other databases and tools like Spark and Hadoop. Majorly, it has the great computational intensity and has powerful data analytics libraries.So, learn Python to perform the full life-cycle of any data science project. It includes reading, analyzing, visualizing and finally making predictions.If you come across any difficulty while practicing Python, or you have any thoughts /suggestions/feedback on the post, please feel free to post them through comments below.",https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
"Model Monitoring Senior Business Analyst/Assistant Manager  Gurgaon (5-6 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|A Complete Python Tutorial to Learn Data Science from Scratch|Model Development Business Analyst/Manager  Gurgaon (4-5 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Model Monitoring Senior Business Analyst/Assistant ManagerLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Model Monitoring Senior Business Analyst/Assistant Manager  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/01/model-monitoring-senior-business-analystassistant-manager-gurgaon-5-6-years-experience/
"Model Development Business Analyst/Manager  Gurgaon (4-5 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Model Monitoring Senior Business Analyst/Assistant Manager  Gurgaon (5-6 years of experience)|Data Scientist  Bangalore (5+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Model Development Business Analyst/ManagerLocation  GurgaonAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject as Model Development Business Analyst/Manager  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/01/model-development-business-analystmanager-gurgaon-4-5-years-experience/
"Data Scientist  Bangalore (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Model Development Business Analyst/Manager  Gurgaon (4-5 years of experience)|20 Powerful Images which perfectly captures the growth of Data Science|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data ScientistLocation BangaloreAbout employer  ConfidentialDescriptionResponsibilitiesQualification and Skills RequiredKey Skills: Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Scientist  Bangalore and the following details:,https://www.analyticsvidhya.com/blog/2016/01/data-scientist-bangalore-5-years-experience/
20 Powerful Images which perfectly captures the growth of Data Science,Learn everything about Analytics|Introduction|What do these images show ?|20 Pictures|End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|Data Scientist  Bangalore (5+ years of experience)|A Comprehensive Guide to Data Exploration|
Analytics Vidhya Content Team
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data cant make your past better. However, it you surely can createan awesome future.In recent years, companies have invested millions of dollars in the field of data science. Theyhave shown immense faith in its potential to create a better world, a better lifeand a better future.The powerful trio of mathematics, computer science and domain expertise has re-defined theprocess of decision making. Intuition or gut no longer remains the key to make complicated decisions.What was considered to be path breaking invention few years back, has now become obsolete. Data Science has empowered us with possibilities beyond imagination. Over the years, lots of things have decayed& evolved.Still, the best of technology is yet to come. Im excited to see it in front of my eyes!We all knowimages are easy to comprehend and conveys more information than text. Basically, these images depict the journey of data science as a field. These include the developments, inventions, achievements and everything that has made an impact on our daily lives.You talk about politics, economics, science, life, sports, almost everything is blessed by services of data science. And, Ive tried to captured the best of themin these pictures.Hope you like them!1.This event marked the beginning of data scientist revolution. After this research report got published, the world immediately acknowledged the potential needed to make sense out of the data.2. Mckinsey further fueled this revolution with their research report on growing imbalance in demand and supply of talent with analytical talent. Though, this was focused only on US market, but its ripples were felt in worldwide industries. (Image Source: McKinsey Report)3. This came as the first breakthrough in Artificial Intelligence. IBM created its first artificially intelligent computer IBM Watson. It was created to compete in the popular game showJeopardy. Eventually, Watson defeated two of these greatest Jeopardy champions (Ken & Brad). This marked the beginning of next level artificial intelligence.4. Data science plays a significant role in baseball. This caused sudden up surge in the demand and availability of players data near 2000. It is said that the years after 2000 marked the beginning of data revolution in baseball. (Image Source : Datanami )5. In 2009, Netflix organized a competition to improve the accuracy and relevance of its content recommended system. This team solved the mystery. Won $1,000,000 prize money. Surely, this is one of the most notable event in data science history.6. This is statistics at its best. Peter Brand (Moneyball) explains statistically the number game which can get them wins in the upcoming baseball season. His popular dialogue In baseball, the goal shouldnt be to buy players. The goal should be to buy wins. In order to buy wins, you need to buy runs !7. In 2012 Presidential Elections, Nate Silver correctly predicted 50out of 50 states. He used probability, graph theory, bayes theoremtechniques to achieve this feat. This level of accuracy brought using statistics completely changed the way making political predictions.
8. Will this be the decade of Deep Learning ?This image shows the number of deep learning projects taken by google in last 4 years. (Image Source : Bloomberg)9. Its a competition between Google Image Recognition Software and a Soduku Champion (human). While the champion does her analysis andfigures out the right numbers, google goggles solves it withinseconds.10. Industrial Robots in action in a car manufacturing factory. These robots provide high precision, low error margin and faster response rate than humans. Auto manufacturing industry is pacing towards this automated form of labor.11. Record Stress. Yes!Stress can also be recorded in form ofdata. Now, there are several devices and applications available in the market which can record stress level and predict probable health issues.12. We talk about new methods of data collection, heres another one. Companies have created apps & software to track your fitness levels and recommend health products accordingly. Everything you do today, generates data.13. Expedition made easier.ThisImage recognition software translates text instantly. You no longer need to face troubles of unfamiliar languages which travelling to a new country.14. In 2012, another breakthrough research happened in the field of data science. At google, a computer learnt to identify cats using a Neural Networkcreatedusing 16,000 computer processors.15. Unmanned Aerial Vehicle (UAV), also known as Drones are aircrafts which are pre-programmed for a certain mission. It can be security (spy camera), delivery of goods, monitoring etc.Its an advanced way of producing data which was earlier considered to be difficult to capture.16. Self Driving Cars. Researches from Google, Baidu, Ford are diligentlyworking on this project. This is a perfect example of a machinewhich learnsfrom its surroundings. You see, this car detects a person crossing the road ?17. Every moment, artificial intelligence is becoming betterto challenge human intuition. Robotscan nowlook & think like humans. However, extra sensory perception yet remains a challenge. Dont get baffled if you see lot of these species in our surroundings in near future.18. Internet of Things promises an incredible future forhuman beings. Creating a network of connected devices will make human lives faster and more convenient. (Image Source: The Connectivist)19. Machine Learning will now promote life. Google continues to inspire humans with their projects. This is google project Verily. It aims to make robots better surgical assistant using machine learning and advanced image processing. (Image Source: Verily)20. Google Trends shows a promising growth of Data Science in 2016 also.This article is about 20pictures which depicts the remarkable growth of data science and machine learning. Nobody wondered that the blend of data and technology could design a fantastic future. Yet, this decade will experience this future.One thing that I learnt while compiling this article is, if someone wants to enter data science and analytics, you cant waitfor a better time. This is it. Do it now. Companies are furiously hunting for talented candidates. Be the one.",https://www.analyticsvidhya.com/blog/2016/01/20-powerful-images-perfectly-captures-growth-data-science/
A Comprehensive Guide to Data Exploration,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Steps of Data Exploration and Preparation|2. Missing Value Treatment|3. Techniques of Outlier Detection and Treatment|4. The Art of Feature Engineering|End Notes,"Lets get started.|Variable Identification|Univariate Analysis|Bi-variateAnalysis|Why missing values treatment is required?|Why my data has missing values?|Which are the methods to treat missing values ?|What is an Outlier?|What are the types of Outliers?||What causes Outliers?|What is the impact of Outliers on a dataset?||How to detect Outliers?||How to remove Outliers?|What is Feature Engineering?|What is the process of Feature Engineering ?|What is Variable Transformation?|When should we use Variable Transformation?|What are the common methods of Variable Transformation?|What is Feature /Variable Creation & its Benefits?|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|20 Powerful Images which perfectly captures the growth of Data Science|The Ultimate Plan to Become a Data Scientist in 2016|
Sunil Ray
|102 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
","For further read, here is a list of transformation / creation ideas which can be applied to your data.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are no shortcuts for data exploration. If you are in a state of mind, that machine learning can sail you away from every data storm, trust me, it wont. After some point of time, youll realize that you are strugglingat improving models accuracy. In such situation, data exploration techniques will come to your rescue.I can confidently say this, because Ive been through such situations, a lot.I have been a Business Analytics professional for close to three years now. In my initial days, one of my mentor suggested me to spend significant time on exploration and analyzing data. Following his advice has served me well.Ive created this tutorial to help you understand the underlying techniques of data exploration. As always, Ive tried my best to explain these concepts in the simplest manner. For better understanding, Ive taken up few examples to demonstrate the complicated concepts.Remember the quality of your inputs decide the quality of your output. So, once you have got your business hypothesis ready, it makes sense to spend lot oftime and efforts here. Withmy personal estimate, data exploration, cleaning and preparation cantakeup to 70% of your total project time.Below are the steps involved to understand, clean and prepare your data for building your predictive model:Finally, we will need to iterate over steps 4  7 multiple times before we come up with our refined model.Lets now study each stage in detail:-First, identify Predictor (Input) and Target (output) variables. Next, identify the data type and category of the variables.Lets understand this step more clearly by taking an example.Example:- Suppose, we want to predict, whether the students will play cricket or not (refer below data set). Here you need to identify predictor variables, target variable, data type of variables and category of variables.Below, the variables have been defined in different category:At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable typeis categorical orcontinuous.Lets look at these methods and statistical measures for categorical and continuous variables individually:Continuous Variables:-In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:Note:Univariate analysisis also used to highlight missing and outlier values. In theupcoming part of this series, we will look at methods to handle missing and outlier values. To know more about these methods, you can refer coursedescriptive statistics from Udacity.Categorical Variables:- For categorical variables, well use frequency table to understand distribution ofeach category. We can also read as percentage of valuesunder each category. It can be be measured using two metrics, Count and Count% against each category. Bar chart can be used as visualization.Bi-variate Analysis finds out the relationship between two variables. Here, we look forassociation and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.Lets understand the possible combinations in detail:Continuous & Continuous: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way tofind out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.Scatter plot shows the relationship between two variable but does not indicatesthe strength of relationship amongst them. To find the strength of the relationship, we useCorrelation. Correlation varies between -1 and +1.Correlation can be derived using following formula:Correlation = Covariance(X,Y) / SQRT( Var(X)* Var(Y))Various tools have function or functionality to identify correlation between variables. In Excel, function CORREL() is used to return the correlation between two variables and SAS uses procedure PROC CORR to identify the correlation. These function returns Pearson Correlation value to identify the relationship between two variables:In above example, we have goodpositive relationship(0.65) between two variables X and Y.Categorical & Categorical:To find the relationship between two categorical variables, we can use following methods:Probability of 0: It indicates that both categorical variable are dependentProbability of 1: It shows that both variables are independent.Probability less than 0.05: It indicates that the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:where O represents the observed frequency. E is the expected frequency under the null hypothesis and computed by:

From previous two-way table, the expected count for product category 1 to be of small size is0.22. It is derived bytaking the row total for Size(9) times the column total for Product category (2) then dividing by the sample size (81). This is procedure is conducted for each cell. Statistical Measures used to analyze the power of relationship are:Different data science language and tools have specific methods to perform chi-square test. In SAS, we can use Chisqas an option with Proc freq to perform this test.Categorical & Continuous: While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables. If levels are smallin number, it will not show the statistical significance. To look at the statistical significance we can perform Z-test, T-test or ANOVA.Example: Suppose, we want to test the effect of five different exercises. For this, we recruit 20 men and assign one type of exercise to 4 men (5 groups). Their weights are recorded after a few weeks.We need to find out whether the effect of these exercises on them is significantly different or not. This can be done by comparing the weights of the 5 groups of 4 men each.Till here, we have understoodthe first three stages of Data Exploration, Variable Identification, Uni-Variate and Bi-Variate analysis. We also looked at various statistical and visual methods to identify the relationship between variables.Now, we will look at the methods ofMissing values Treatment.More importantly, we will also look at why missing values occur in our data and why treating them is necessary.Missing data in the training data set can reduce the power / fit of a model or can lead toa biased model because we have not analysedthe behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.Notice the missing values in the image shown above: In the left scenario, we have not treated missing values. The inference from this data set is that the chances of playing cricket by males is higher than females. On the other hand, if you look at the second table, which shows data after treatment of missing values (based on gender), we can see that females have higher chances of playing cricket compared to males.We looked at the importance of treatment of missing values in a dataset. Now, lets identify the reasons for occurrence of these missing values. Theymay occur at two stages:
After dealing with missing values, the next task is to deal with outliers. Often, we tend to neglect outliers while building models. This is a discouraging practice. Outliers tend to make your data skewed and reduces accuracy. Lets learn more about outlier treatment.Outlier is a commonly usedterminology by analysts and data scientists asit needsclose attention else itcan result in wildly wrongestimations. Simply speaking,Outlier is an observation that appears far away and diverges from an overall pattern in a sample.Lets take an example, we do customer profiling and find out that the average annual income of customers is $0.8 million. But, there are two customers having annual income of $4 and $4.2 million. These two customers annual income is much higher thanrest of the population. These two observations will be seen asOutliers.Outlier can be of two types:Univariate andMultivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.Let us understand this with an example. Let us say we are understanding the relationship between height and weight. Below, we have univariate and bivariate distribution for Height, Weight. Take a look atthe box plot. We do not have any outlier (above and below 1.5*IQR, most common method). Now look at the scatter plot. Here, we have two valuesbelowand one above the average in a specific segment of weight and height.Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:Lets understand various types of outliers in more detail:Outliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:To understand the impact deeply, lets take an example to checkwhat happens to a data set with and without outliersin thedata set.Example:As you can see, data set with outliers has significantly different mean and standard deviation. In the first scenario, we will say that average is 5.45. But with the outlier, average soarsto30. This would change the estimate completely.Most commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot (above, we have used box plot and scatter plot for visualization). Some analysts also various thumb rules to detectoutliers. Some of them are:Most of the waysto dealwith outliers are similar to the methods ofmissing values like deleting observations, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we willdiscuss thecommon techniques used to deal with outliers:Deleting observations:We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.Transforming and binning values:Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allowsto deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.Imputing:Likeimputationof missing values, we can also imputeoutliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier observation and after that we can impute it with predicted values.Treat separately:If there are significant number of outliers, we should treat them separatelyin the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and thencombine the output.
Till here, we have learnt about steps of data exploration, missing value treatment and techniques of outlier detection and treatment. These 3 stages will make your raw data better in terms of information availability and accuracy. Lets now proceed to the final stage of data exploration. It is Feature Engineering.Feature engineering is the science (and art) of extracting more information from existing data. You are not adding any new data here, but you are actually making the data you already have more useful.For example, lets say you are trying to predict foot fall in a shopping mall based on dates. If you try and use the dates directly, you may not be able to extract meaningful insights from the data. This is because the foot fall is less affected by the day of the month than it is by the day of the week. Now this information about day of week is implicit in your data. You need to bring it out to make your model better.This exercising of bringing out information from data in known as feature engineering.You perform feature engineering once you have completed the first 5 steps in data exploration Variable Identification,Univariate, Bivariate Analysis,Missing Values ImputationandOutliers Treatment. Feature engineering itself can be divided in 2 steps:These two techniques are vital in data exploration andhavea remarkableimpact on the power of prediction.Letsunderstand each of this step in more details.In data modelling, transformation refers tothe replacement of a variable by a function. For instance, replacing a variable x by the square / cube root or logarithm x is a transformation. In other words, transformation is a process that changes the distribution or relationship of a variable with others.Lets look at the situations when variable transformation is useful.Below are the situations where variable transformation is arequisite:There are various methods used to transform variables. As discussed, some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Lets look at these methods in detailbyhighlightingthe pros and cons of these transformation methods.Feature / Variable creation is a process to generate a new variables / features based on existing variable(s). For example, say, wehave date(dd-mm-yy) as an input variable in adata set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. This step is used to highlight the hidden relationship in a variable:There are various techniques to create new features. Lets look at the some of the commonly used methods:As mentioned in the beginning, quality and efforts invested in data exploration differentiates a good model from a bad model.This ends our guideon data exploration and preparation. In this comprehensive guide, we looked at the seven steps of data exploration in detail. The aim of this series was to provide an in depth and step by step guide to an extremely important process in data science.Personally, I enjoyed writing this guideand would love to learn from your feedback. Didyou findthis guideuseful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/
The Ultimate Plan to Become a Data Scientist in 2016,Learn everything about Analytics|Introduction|Click Here > Link to Resources,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|A Comprehensive Guide to Data Exploration|3 Tricky Puzzles which most people get Wrong in Job Interviews|
Kunal Jain
|46 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Scientist is one of the hottest jobs of this decade. The demand for data scientists is much higher than available candidates (Source). So, there is a lot of incentive for people to look up to data science as a career option, and that is not going to change in near future.However, if you do one search on Google, you will see your dream vanishing. There are too many resources, advice and paths suggested by various people, which makes it impossible for a beginner to take right decisions.If you are facing a similar problem, lets accomplish this in 2016. If you aspire to become a data scientist, this annualplan would make things much easier and faster for you. Ive mentioned only the best resources you should follow. This plan is designed to make you a data scientist by December 2016 (conservative pace). If you can devote more time, great. Youd could achieve this feat much faster or with more depth by looking at additional resources (orange bullet).",https://www.analyticsvidhya.com/blog/2016/01/ultimate-plan-data-scientist-2016/
3 Tricky Puzzles which most people get Wrong in Job Interviews,Learn everything about Analytics,"Introduction|Scoring Marks|Puzzle 1 Who took that Coconut?|Puzzle 2 Hat Riddle|Puzzle 3  Are you ready for the final one|End Notes|If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Like this:|Related Articles|The Ultimate Plan to Become a Data Scientist in 2016|AV Blogathon is Live  Inspiring a new breed of Data Scientists|
Tavish Srivastava
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you ever go for an interview, prepare well for puzzles and guess estimate questions. Theyd surely be thrown at you. Sometimes, the puzzlesappear to be difficult, but the solutions turns out to be extremely simple. Ive got stumbled many a times by such questions. But, over the years, Ive learned the art of solving such tricky puzzles.Heres the secret to solve them. Be Logical. Thats it. You need nothing else, no qualification, no reputed degree to solve them. Such questions are meant to test your common sense and logical aptitude. Dont think too hard decoding the logic behind them, but think out of the box. Think smart.In this article, I have chosen 3 most challenging puzzles which most people get wrong in interviews. Since these questions are tricky to understand at first, its alright evenif you do not find their answer in first attempt. But dont give up easily.None of these puzzles have been crafted by me. But, the solutions provided are my own solutions. These puzzle might possibly have more than one solution. Id suggest you to explore them from all possible directions. Do share your solutions in the comments section below.Puzzles are just one part of the entire data science interview process. Theres a whole lot more that goes into landing your first role in data science. Check out the Ace Data Science Interviews course where we provide tons of materials including videos and resources (cheatsheets, checklists, a MASSIVE interview book) to help you land your first data science role.In order to evaluate your progress, Ive created a score chart. Here is marking sheet for these puzzles:Maximum score on each puzzle : 10Unattempted puzzle : 0 marksPuzzle solved but withwrong answer : 1  5 marks (self assessment)Right answer but methodology not very clear : 6  8marks(self assessment)Right answer and right methodology : 9 marksRight answer solved using multiple methodology : 10 marksOnce you have score for all three, add them up and get a score out of 30. In case you know a puzzle : skip it and scale your score to a base of 30. With this score map your self from the following table :Note: Be true to yourself in scoring.Here we go with the puzzles :Ten people land on a deserted island. There they find lots of coconuts and a monkey. During their first day they gather coconuts and put them all in a community pile. After working all day they decide to sleep and divide them into ten equal piles the next morning.That night one castaway wakes up hungry and decides to take his share early. After dividing up the coconuts he finds he is one coconut short of ten equal piles. He also notices the monkey holding one more coconut. So he tries to take the monkeys coconut to have a total evenly divisible by 10. However when he tries to take it the monkey conks him on the head with it and kills him.Later another castaway wakes up hungry and decides to take his share early. On the way to the coconuts he finds the body of the first castaway, which pleases him because he will now be entitled to 1/9 of the total pile. After dividing them up into nine piles he is again one coconut short and tries to take the monkeys slightly bloodied coconut. The monkey conks the second man on the head and kills him.One by one each of the remaining castaways goes through the same process, until the 10th person to wake up gets the entire pile for himself. What is the smallest number of possible coconuts in the pile, not counting the monkeys?Answer:2519Logic: LCM (Lowest Common Multiple) of 10,9,8,7,6,5,4,3,2,1 -1. LCM would give the least number which is divisible by all of these number and subtracting one would give us the number of coconuts which were initially there.A stark raving mad king tells his 100 wisest men he is about to line them up and he will place either a red or blue hat on each of their heads. Once lined up, they must not communicate amongs themselves. Nor may they attempt to look behind them or remove their own hat.The king tells the wise men that they will be able to see all the hats in front of them. They will not be able to see the color of their own hat or the hats behind them, although they will be able to hear the answers from all those behind them.The king will then start with the wise man in the back and ask what color is your hat? The wise man will only be allowed to answer red or blue, nothing more. If the answer is incorrect then the wise man will be silently killed. If the answer is correct then the wise man may live but must remain absolutely silent.The king will then move on to the next wise man and repeat the question.The king makes it clear that if anyone breaks the rules then all the wise men will die, then allows the wise men to consult before lining them up. The king listens in while the wise men consult each other to make sure they dont devise a plan to cheat. To communicate anything more than their guess of red or blue by coughing or shuffling would be breaking the rules.What is the maximum number of men they can be guaranteed to save?Answer :99.The first wise man counts all the red hats he can see (A) and then answers blue if the number is odd or red if the number is even. Each subsequent wise man keeps track of the number of red hats known to have been saved from behind (B), and counts the number of red hats in front (C).If Awas even, and if B&C are either both even or are both odd, then the wise man would answer blue. Otherwise the wise man would answer red.If Awas odd, and if B&C are either both even or are both odd, then the wise man would answer red. Otherwise the wise man would answer blue.A king wants his daughter to marry the smartest of 3 extremely intelligent young Prince, and so the kings wise men devised an intelligence test.All the prince are gathered into a room and seated, facing one another, and are shown 2 black hats and 3 white hats. They are blindfolded, and 1 hat is placed on each of their heads, with the remaining hats hidden in a different room.The king tells them that the first prince to deduce the color of his hat without removing it or looking at it will marry his daughter. A wrong guess will mean death. The blindfolds are then removed.You are one of the prince. You see 2 white hats on the other princes heads. After some time you realize that the other prince are unable to deduce the color of their hat, or are unwilling to guess. What color is your hat?Note: You know that your competitors are very intelligent and want nothing more than to marry the princess. You also know that the king is a man of his word, and he has said that the test is a fair test of intelligence and bravery.Answer:White.The king would not select two white hats and one black hat. This would mean two prince would see one black hat and one white hat. You would be at a disadvantage if you were the only prince wearing a black hat.If you were wearing the black hat, it would not take long for one of the other princes to deduce he was wearing a white hat.If an intelligent prince saw a white hat and a black hat, he would eventually realize that the king would never select two black hats and one white hat. Any prince seeing two black hats would instantly know he was wearing a white hat. Therefore if a prince can see one black hat, he can work out he is wearing white.Therefore the only fair test is for all three prince to be wearing white hats. After waiting some time just to be sure, you can safely assert you are wearing a white hat.I hope you scored 27 + . In case you did, we will love to hear your solutions. Write your score and your solutions in the comment section below. Make sure you bring out, what challenge did you face and how did you think around the challenge.Did you find this article useful ? Share your views and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2016/01/3-tricky-puzzles-people-wrong-job-interviews/
AV Blogathon is Live  Inspiring a new breed of Data Scientists,Learn everything about Analytics|Do you also have the passion to write and inspire ?|Announcing AV Blogathon|Registration Process|Win Prizes!|Rules & Regulations|Guidelines for Writing Articles,"A Short Story|Share this:|Like this:|Related Articles|3 Tricky Puzzles which most people get Wrong in Job Interviews|Digital Marketing Strategist  Ahmedabad (6+ years of experience)|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Writing is an art which lets you convey a piece of information without any criticism. Sometime, its difficult to speak and deliver an idea, but you can write it anytime. Have you felt this before?Jon and Harry felt the same. They both shared a common passion to write. They both wanted to share their knowledge. In other words, both of them wanted to write and inspire people. But, both chose different ways. Jon decided to create his website to share his knowledge. It costed him a lot. More than that, his website demanded dedicated effort and hardwork for scaling it. He failed to keep up.On the other hard, Harry played smart. He searched for some popularblogs in his domain. He, then connected with one of them. And, published his article on their website. With in few months, he become one of the most sought after thought leader in the industry. He got the recognition which he had never ever dream of!Moral of the Story:If you are in analytics, take this opportunity and become the next Harry!I am delighted to announce launch of AV Blogathon.We are inviting bloggers to position themselves as data hackers & thought leaders in the industry. Dont get mislead by the term blogger. For us, you are a blogger, if you know the art of expressing yourself in writing.All you have to do is pen down your article and make a submission.We will not only publish the best of articleson Analytics Vidhya, but also provide feedback to every writer about their article.We have grown this community through blogs and if there is some one who would understand the value of a well written article  it is us!Here is a once in a lifetime opportunity when you can make a mark on our audience and stand a chance to win exciting prizes at the same time!You can drop in your registrationhere. You will need to create a profile. If your article gets selected, it wouldbe published with the same id.Dont let this opportunity pass by you. Its better to act and repent than not to act and regret. Get ready now and start making plans to spread your knowledge.Sounds like fun? Join the competition and get ready to take away a bundle of knowledge and expertise!",https://www.analyticsvidhya.com/blog/2016/01/announcing-av-blogathon-inspiring-data-scientists/
"Digital Marketing Strategist  Ahmedabad (6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|AV Blogathon is Live  Inspiring a new breed of Data Scientists|Data Software Engineer  Ahmedabad (5-12 years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Designation  Digital Marketing StrategistLocation  AhmedabadAbout employer  ConfidentialDescriptionDo you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.ResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asDigital Marketing Strategist  Ahmadabad and the following details:",https://www.analyticsvidhya.com/blog/2016/01/digital-marketing-strategist-ahmadabad-6-years-experience/
"Data Software Engineer  Ahmedabad (5-12 years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Digital Marketing Strategist  Ahmedabad (6+ years of experience)|12 Useful Pandas Techniques in Python for Data Manipulation|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Data Software EngineerLocation  AhmedabadAbout employer  ConfidentialDescriptionResponsibilitiesHelp in building an enterprise-level data processing and analytics platformQualification and Skills RequiredTechnologiesExperience:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject asData Software Engineer  Ahmadabad and the following details:,https://www.analyticsvidhya.com/blog/2016/01/data-software-engineer-ahmadabad-5-12-years-experience/
12 Useful Pandas Techniques in Python for Data Manipulation,Learn everything about Analytics|Overview||Introduction|Lets get started|#1  Boolean Indexing|#2  Apply Function|#3  Imputing missing files|#4  Pivot Table|#5  Multi-Indexing|#6. Crosstab|#7  Merge DataFrames|#8  Sorting DataFrames|#9  Plotting (Boxplot & Histogram)|#10  Cut function for binning|#11  Coding nominal data|#12  Iterating over rows of a dataframe|Projects||End Notes,"If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Data Software Engineer  Ahmedabad (5-12 years of experience)|Semantic Web Experts  Gurgaon (6+ years of experience)|
Aarshay Jain
|55 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Python is fast becoming the preferredlanguage in data science  and for good reason(s). It provides the larger ecosystem of a programming language and the depth of good scientific computation libraries. If you are starting to learn Python, have a look at learning path on Python.Among its scientific computation libraries, I found Pandas to be the most useful for data science operations. Pandas, along with Scikit-learn provides almost the entire stack needed by a data scientist. This article focuses on providing 12 ways for data manipulation in Python. Ive also shared some tips & tricks which will allow you to work faster.I would recommend that you look at the codes for data explorationbefore going ahead.To help you understand better, Ive taken a data set to perform these operations and manipulations.If youre just starting out your data science journey, they youll love the Introduction to Data Science course. It covers the basics of Python, comprehensive introduction to statistics and several machine learning algorithms. A must-have course!Data Set: Ive used the data setof Loan Prediction problem. Download the dataset and get started.Illstart by importing modules and loading the data set into Python environment:What do you do, if you want to filter values of a column based on conditions from another set of columns?For instance, we want a list of all femaleswho are not graduate and got a loan. Boolean indexing can help here. You can use the following code:Read More: Pandas Selecting and IndexingIt is one of the commonly usedfunctions forplaying with data and creating new variables. Apply returns some value after passing each row/column of a data frame withsome function. The function can be both default or user-defined. For instance, here it can be used to find the #missing values in each row and column.Thus we get the desired result.Note: head() function is used in second output because it contains many rows.
Read More: Pandas Reference (apply)fillna() does it in one go. It is used for updating missing values with the overall mean/mode/median of the column. Lets impute the Gender, Married and Self_Employed columns with their respective modes.Output: ModeResult(mode=array([Male], dtype=object), count=array([489]))This returns both mode and count. Remember that mode can be an array as there can be multiple values with high frequency. We will take the first one by default always using:Now we can fill the missing values and check using technique #2.Hence, it is confirmed that missing values are imputed. Please note that this is the most primitive form of imputation. Other sophisticated techniques include modeling the missing values, using grouped averages (mean/mode/median). Ill cover that part in my next articles.Read More: Pandas Reference (fillna)Pandas can be used to create MS Excel style pivot tables. For instance, in this case, a key column is LoanAmount which has missing values. We can impute it using mean amount of each Gender, Married and Self_Employed group. The mean LoanAmount of each group can be determined as:More: Pandas Reference (Pivot Table)If you notice the output of step #3, it has a strangeproperty. Each index is made up of a combination of 3 values. This is called Multi-Indexing. It helps in performingoperations really fast.Continuing the example from #3, we have the values for each group but they have not beenimputed.
This can be done using the various techniques learned till now.Note:This function isused to get an initial feel (view) of the data. Here, we can validate some basic hypothesis. For instance, in this case, Credit_History is expected to affect the loan status significantly. This can be tested using cross-tabulation as shown below:These are absolute numbers. But, percentages can be more intuitive in making some quick insights. We can do this using the apply function:Now, it is evident that people with a credit history have much higher chances of getting a loan as 80% people with credit history got a loan as compared to only 9% without credit history.But thats not it. It tells an interesting story. Since I know that having a credit history is super important, what if I predict loan status to be Y for ones with credit history and N otherwise. Surprisingly, well be right 82+378=460 times out of 614 which is a whopping 75%!I wont blame you if youre wondering why the hell do we need statistical models. But trust me, increasing the accuracy by even 0.001% beyond this mark is a challenging task. Would you take this challenge?Note: 75% is on train set. The test set will be slightly different but close. Also, I hope this gives some intuition into why even a 0.05% increase in accuracy can result in jump of 500 ranks on theKaggle leaderboard.Read More: Pandas Reference (crosstab)Merging dataframes become essential when we have information coming from different sources to be collated. Consider a hypothetical case where the average property rates (INR per sq meters) is available for different property types. Lets define a dataframe as:Now we can merge this information with the original dataframe as:The pivot table validates successful merge operation. Note that the values argument is irrelevant here because we are simply counting the values.ReadMore: Pandas Reference (merge)Pandas allow easy sorting based on multiple columns. This can be done as:Note: Pandas sort function is now deprecated. We should use sort_values instead.More: Pandas Reference (sort_values)Many of you might be unaware that boxplots and histograms can be directly plotted in Pandas and calling matplotlib separately is not necessary. Its just a 1-line command. For instance, if we want to compare the distribution of ApplicantIncome by Loan_Status:This shows that income is not a big deciding factor on its own as there is no appreciable difference between the people who received and were denied the loan.Read More: Pandas Reference (hist) | Pandas Reference (boxplot)Sometimes numerical values make more sense if clustered together. For example, if were trying to model traffic (#cars on road) with time of the day (minutes). The exact minute of an hour might not be that relevant for predicting traffic as compared to actual period of the day like Morning, Afternoon, Evening, Night, Late Night. Modeling traffic this way will be more intuitive and will avoid overfitting.Here we define a simple function which can be re-used for binning any variable fairly easily.Read More: Pandas Reference (cut)Often, we finda case where weve to modify the categories of a nominal variable. This can be due to various reasons:Here Ive defined a generic function which takes in input as a dictionary and codes the values using replace function in Pandas.Similar counts before and after proves the coding.Read More: Pandas Reference (replace)This is not a frequently used operation.Still, you dont want to get stuck. Right? At times you may need to iterate through all rows using a for loop. For instance, one common problem we face isthe incorrect treatment of variablesinPython. This generally happens when:So its generally a good idea to manually define the column types. If we check the data types of all columns:Here we see that Credit_History is a nominal variable but appearing as float. A good way to tackle such issuesis to create a csv file with column names and types. This way, we can make a generic function to read the file and assign column data types. For instance, here I have created acsv file datatypes.csv.After loading this file, we can iterate through each row and assign the datatype using column type to the variable name defined in the feature column.Now the credit history column is modified to object type which is used for representing nominal variables in Pandas.
Read More: Pandas Reference (iterrows)Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems:In this article, we covered various functions of Pandas which can make our life easy while performing data exploration and feature engineering. Also, we defined some generic functions which can be reused for achieving similar objective on different datasets.Also See: If you have any doubts pertaining to Pandas or Python in general, feel free to discuss with us.Did you find the article useful? Do you use some better (easier/faster) techniques for performing the tasks discussed above? Do you think there are better alternatives to Pandas in Python? Well be glad if you share your thoughts as comments below.",https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/
"Semantic Web Experts  Gurgaon (6+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|12 Useful Pandas Techniques in Python for Data Manipulation|Text Mining / Text Analytics / NLP  Sr Resource  Gurgaon (5+ years of experience)|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Semantic Web ExpertsLocation  GurgaonAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredSkills:Interested people can apply for this job by sending their updated CV to[emailprotected]with subject as Semantic Web Experts  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/01/semantic-web-experts-gurgaon-6-years-experience/
"Text Mining / Text Analytics / NLP  Sr Resource  Gurgaon (5+ years of experience)|If you want to stay updated onlatest analytics jobs,follow our job postings on twitteror like ourCareers in Analytics pageon Facebook",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Semantic Web Experts  Gurgaon (6+ years of experience)|Here comes a year full of knowledge & learning!|
Jobs Admin
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
|

4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,Designation  Text Mining / Text Analytics / NLP  Sr ResourceLocation  GurgaonAbout employer  ConfidentialResponsibilitiesQualification and Skills RequiredInterested people can apply for this job by sending their updated CV to[emailprotected]with subject asText Mining / Text Analytics / NLP  Sr Resource  Gurgaon and the following details:,https://www.analyticsvidhya.com/blog/2016/01/text-mining-text-analytics-nlp-sr-resource-gurgaon-6-years-experience/
