Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
Key Highlights in Data Science / Deep Learning / Machine Learning 2017 and What can we Expect in 2018?,Learn everything about Analytics|Introduction|Interesting Snippets of Year 2017|Neuralink : A high bandwidth and safe Brain-Machine Interface|What more can we expect in 2018?|End Notes,"PowerBlox developed a scalable energy device capable of storing and distributing electricity from a variety of inputs|Face Recognition for payment transaction in KFC China|Release of Deeplearn.js: Harness Machine Learning in Your Browser |Release of CatBoost: A machine learning library to handle categorical data automatically|IBM Watson to aid in filing taxes|Shelf Engine: A startup developing AI to prevent food wastage|Body Labs – a start-up acquired by Amazon develops 3D models of individual human bodies from images|Data Science competition platform “Kaggle” joins Google Cloud|Capsule Networks – an improved deep learning architecture has been introduced|Canada bets big on artificial intelligence with AI institute|Baidu trained an AI agent to navigate the world like a parent teaches a baby|Machine learning creates living atlas of the planet|Disney understanding audience personality to better target them|“Entrupy” uses Deep learning to spot product authenticity|Detecting heart disease using Deep Learning|Facebook has decreased the training for visual recognition models |IBM Watson automatically generated highlight reels of Wimbledon|DeepMind created artificial agents that can imagine and plan ahead|Replika, a chatbot that creates a digital representation of you the more you interact with it|Using Twitter as a tool to forecast crime|HireVue is using AI to analyze word choice, tone, and facial movement of job applicants who do video interviews|E&Y using Email and Calendar data to understand how employees work|Carnegie Mellon’s ‘Superhuman AI’ bests leading Texas Hold’em poker professionals|Deep Learning to emulate cyber threats|Mozilla’s releases Speech Recognition model and Voice dataset|Course content for Fast.ai’s “Cutting Edge Deep Learning for Coders – Part 2”|China will allow self-driving cars to be tested on public roads|Emergence of Automated Machine Learning |Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Introductory Guide – Factorization Machines & their application on huge datasets (with codes in Python)|11 most read Machine Learning articles from Analytics Vidhya in 2017|
Faizan Shaikh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"2017 has been a really exciting year for a data science professional. This is pretty evident from the new technologies that have been emerging day-by-day such as Face-ID which has revolutionized the way we secure information in our mobile phones. Self-driving cars had been a myth, but now they are very much a reality, the adoption of which can be seen by governments throughout the world.Data science is a field wherein ground-breaking research is happening at a much faster pace, in comparison to any other emergent technologies ever before. The time between contemplating a research idea and actually implementing it has come down significantly. . This is also fueled by the immense amount of resources freely available to everyone – which essentially enables even a normal person to contribute to research in their own way. For example, GitHub (a collaborative platform for software development) is now paving the way for research ideas to be shared in an implementation format. As Andrew Ng saidData is the new Oil
AI is the new ElectricityPersonalization and Automation is the talk of the day and more and more industries such as Financial Services, Healthcare, Pharmaceuticals and Automotive are adapting to the developments being brought upon by better Machine learning / Deep Learning models. This article specially focuses on the defining moments of Data Science in 2017. We have kept a few criterion in mind when we curated the list, namely:Also, we have shared our predictions in Data Science industry for the year 2018, which we believe would be really something to look forward to.Enjoy! Tags – Startup, Renewable EnergyA young company PowerBlox is using algorithms to give energy grids “swarm intelligence.” Power-Blox gives grids “the ability to adapt automatically to fluctuating electrical loads and an ever-changing roster of power sources.” This technology is extremely important as we move to grids sourced by wind, solar, and wave energy that fluctuate minute-by-minute, thus making renewable sources of energy more usable.
 Tags – Startup, Innovation2017 probably saw its biggest announcement from Elon Musk who announced creation of a company called Neuralink that aims at building high bandwidth and safe Brain-Machine interfaces. It looks like Elon has finally decided to bring Matrix into reality where learning new skills for example flying a helicopter is just a matter of plugging a wire into your neocortex. All this may sound fiction and over exaggerated but Elon has been known to make things real, take Tesla and SpaceX for example.If it all comes true, humans may soon have the technology to study and map the brain in its entirety. This has far reaching implications to vastly improving healthcare to augmenting human capacity. And Elon seems serious too. The company recently received a  $27 M funding and is looking to raise another $100M through its shares. For us mortal minds, open the link in the heading to understand how it all started and how the mission of Neuralink is grandeur enough to change the future of humanity if brought to reality. Tags – Innovation, Retail Industry, Computer VisionAlipay and KFC China are allowing customers to pay via facial recognition plus their phone
numbers. No cash, credit cards or smarts phones are necessary.This is the first retailer in the world to do so. Tags – Product release, machine learning, open source softwareDeeplearn.js an open source WebGL-accelerated JavaScript library for machine learning that runs entirely in a browser.Software engineers Nikhil Thorat and Daniel Smilkov noted, “There are many reasons to bring machine learning into the browser. A client-side ML library can be a platform for interactive explanations, for rapid prototyping and visualization, and even for offline computation. And if nothing else, the browser is one of the world’s most popular programming platforms.” Tags – machine learning, open source softwareYou have seen errors while dealing with categorical variables to build machine learning models using library “sklearn”, at least in the initial days “ValueError: could not convert string to float”. This error occurs when dealing with categorical (string) variables. In “sklearn”, you are required to convert these categories in the numerical format. In order to do this conversion, we use several pre-processing methods like “label encoding”, “one hot encoding” and others. “CatBoost”, a recently open-sourced library, developed and contributed by Yandex does this for you automatically.There are many such open source tools/ libraries which have been released in the last year. This article captures a few of the most popular ones. Tags – Company Collaboration, FinanceH & R Block, a tax preparation company is partnering with IBM Watson, to develop internal systems to help its employees’ to file customer’s taxes. The US tax code is 74,000 pages long which is difficult for any one person to know. IBM Watson will help tax professionals with prompts and questions to aid the tax interview process. At the end of this year’s tax process, this will also leave Watson with a massive library of tax data, which it can then analyze. Tags – Startup, Food IndustryShelf Engine is a startup developing robust models to help grocery stores enable their category managers to match orders of hundreds or thousands of products to demand. In the company’s case study, it explains how many managers often make orders based on their current waste numbers — a flawed method, because “that decision isn’t based on a cumulation of waste and deliveries.” Shelf Engine uses an order prediction engine and probability models that analyze historical order and sales data, gross margins, and shelf life information. The more a customer uses the system, the more accurate its recommendations become. The startup is backed by by Initialized Capital (Reddit co-founder Alexis Ohanian is a general partner), with participation from Founder’s Co-op, Liquid 2 Ventures (Joe Montana is a general partner), and others. Tags – Company Acquisition, Fashion RetailBody labs, a computer vision startup has developed applications that takes any input, whether that’s 2D photos, 3D scans or actual body measurements and predicts full, 3D visual body shapes. The implications of this built-out technology are massive, spanning not only commercial opportunities in fashion and apparel, but fitness, gaming, health and manufacturing. This could actually solve the fitting issues with the customers especially in the ecommerce which is marred by huge return requests due to size issues. Body Labs was founded by Michael Black, William J. O’Farrell, Eric Rachlin, and Alex Weiss who were connected at Brown University and Max Planck Institute for Intelligent Systems. Tags – Company AcquisitionGoogle acquired Kaggle, a competition platform for data scientists in March, 2017. Kaggle is known for hosting data science and machine learning competitions as well. Google is claimed to have acquired Kaggle in an attempt to enhance the AI and machine learning functionalities and to take advantage of the 600,000 data scientists at Kaggle’s community. With this acquisition, Kaggle has continued to provide services as before, but the product enhancements seen in Kaggle platform has increased multifold. For example, “kernels” which are online coding environments are a lot smoother and provide a lot more functionality than before – such as longer runtimes. Tags – Research, Deep LearningGeoffrey Hinton, one of the pioneers in deep learning, explains how capsule networks can be useful to improve over the traditional convolutional neural network architecture. If this technique is brought into application, it could easily beat the benchmarks of previous techniques until now.Actually, this technique was also discovered previously – but it has now been implemented in a stable way and can be seen to perform better. Tags – Industry collaborationUnder Geoffery Hinton, Canada Government and big companies like Google and Facebook has invest $150 milliion in Vector Institute to churn out 1000 graduates in AI every year. The Vector Institute intends propel Canada to the forefront of the global shift to artificial intelligence (“AI”) by promoting and maintaining Canadian excellence in deep learning and machine learning Tags – Innovation, RoboticsBaidu taught an AI agent to navigate 2D space using only natural language, the same basic feedback mechanism a human parent uses with an infant. This is a progress towards building AI that can be taught in the same way as humans. Next target for the Chinese giant is to teach a physical robot navigate in a 3D space which is more realistic. This application based on reinforcement learning has huge implications for the robotics industry. Tags – Startup, Food ManagementDescartes Labs, a start-up in Mexico, uses satellite imagery and AI to predict food supplies and crisis-level food shortages months in advance. This leaves enough time to mount orderly humanitarian responses or optimize food supply networks. By processing these images and data via their advanced machine learning algorithm, Descartes Labs collect remarkably in-depth information such as being able to distinguish individual crop fields and determining the specific field’s crop by analyzing how the sun’s light is reflecting off its surface. After the type of crop has been established, the machine learning program then monitors the field’s production levels. Tags – Innovation, Behavioural AnalyticsDisney Research’s Maarten Bos explains about how his team of behavioral scientists conducted a series of studies on how people react to targeted marketing using images and discusses the ways this information might be used within Disney and beyond. If done intelligently and diligently, this could revolutionize the way we do brand marketing. Tags – Startup, Computer Vision, Deep Learning“Entrupy” is a start-up that uses computer vision algorithms to detect counterfeit products. They created a portable scanning device that instantly detects imitation designer bags by taking microscopic pictures that take into account details of the material, processing, workmanship, serial number, and wears/tear. It then employs the technique of deep learning to compare the images against a vast database that includes top luxury brands and if the bag is deemed authentic, users immediately get a Certificate of Authenticity. Tags – Startup, HealthcareCardiogram, in partnership with University of California San Francisco, modified an Apple watch that can detect atrial fibrillation — the most common heart arrhythmia — with higher accuracy than previously validated methods. They achieved this feat using deep learning techniques. As soon as the disease is detected, the device could send you a notification: “We noticed an abnormality in your heartbeat. Want to chat with a cardiologist?” which can potentially decrease the time between the onset of the disease, its detection, and its care. Tags – Innovation, Deep LearningEvery minute spent training a deep learning model is a minute not doing something else, and in today’s fast-paced world of research, that minute is worth a lot. Facebook published a paper this morning detailing its personal approach to this problem. The company says it has managed to reduce the training time of a ResNet-50 deep learning model on ImageNet from 29 hours to one. Tags – Innovation, JournalismPreviously, the task of creating highlight packages and annotating photographs would be the responsibility of a human. But this year round, the job was placed in the hands of the Watson AI.Watson can generate highlight packages without any human input. It can watch a video feed and identify the most pertinent parts of a match. This can be seen by players shaking hands, gesticulating in celebration, or something as simple as the levels of volume from the audience. Tags – Innovation, Reinforcement learningDeepMind researchers created what they’re calling “imagination-augmented agents,” or I2As, that have a neural network trained to extract any information from its environment that could be useful in making decisions later on. These agents can create, evaluate and follow through on plans. To construct and evaluate future plans, the I2As “imagine” actions and outcomes in sequence before deciding which plan to execute. They can also choose how they want to imagine, options for which include trying out different possible actions separately or chaining actions together in a sequence. Tags – Innovation, Artificial IntelligenceReplika is a shadow-bot that tracks what you’re up to on your computer and mimics your style, attitude, and tendencies in order to text like you would. To give an example, the inventor used it to mimic the presence of a dearly departed friend. Tags – Predictive Analysis, Twitter MiningUniversity of Virginia Assistant Professor Matthew Gerber is using Twitter data to predict crime in order to give police a digital spotlight on geographical crime hotspots. He demonstrated that using some old forecasting models and new tweets, he was able to predict 19 of 25 types of crime. This is another step detect sentiment of people on social media and taking emergency precautionary measures. Tags – Human Resources, Computer Vision, Natural Language ProcessingHireVue is using AI in the human resources space for making hiring decisions. This company uses video from job interviews to assess candidate’s facial expressions, body language, tone of voice, and keywords to predict which applicants are going to be the best employees. This technology will completely revolutionize the HR industry Tags – Behavioural AnalyticsErnst and Young, one of the largest accounting firms in the US, is using a calendar and email data from its employees to identify patterns around who is engaging with whom, which parts of the organization are under stress, and which individuals are most active in reaching across company boundaries.  Tags – Innovation, Game agentsCMU releases there a secret recipe of how they built a Superhuman AI that beats professional players at Poker. This is significant because no-limit Texas Hold’em is what’s called an “imperfect-information game,” which means that not all information about all elements in play is available to all players at all times. That’s in contrast to games like Go and Chess, both of which feature a board which contains all the pieces in play, plainly visible to both competitors.
 Tags – Innovation, Cyber Security, Deep LearningScientists have harnessed the power of artificial intelligence (AI) to create a program that, combined with existing tools, figures out passwords. The work could help average users and companies measure the strength of passwords, says Thomas Ristenpart, a computer scientist who studies computer security at Cornell Tech in New York City but was not involved with the study. “The new technique could also potentially be used to generate decoy passwords to help detect breaches.” Tags – Product Release, Speech Recognition, Deep LearningTo speed up advancements in the audio domain, Mozilla has released the worlds second largest publicly available voice dataset, along with open sourcing the cutting edge technology of speech recognition. This release will certainly affect the advancements of speech recognition in general.
 Tags – Deep LearningVideos and course content for Course “Cutting Edge Deep Learning for Coders – Part 2” is now available for general public. For those who haven’t had a chance to see Part 1 of the course – the course introduces you to basics of Deep learning in a practical manner. Part 2 enables you to get into the details of Deep Learning and introducing you to the cutting edge research that is going on in the industry
 Tags – Self-driving cars, TransportationChina is opening up its roads to self-driving cars. The Beijing Municipal Transport Commission released a statement saying that on certain roads and under certain conditions, companies registered in China will be able to test their autonomous vehicles. Tags – Automated Machine LearningAutomated Machine Learning is the new deal. It does most of the heavy lifting required to complete a data science lifecycle on its own. This is a very powerful idea; while we previously have had to worry about tuning parameters and hyperparameters, automated machine learning systems can learn the best way to tune these for optimal outcomes by a number of different possible methods.
 There is so much happening in the industry that its difficult to keep up with the current trends. And it will continue to grow, as soon as the cutting-edge research is transformed into use for the ordinary man. To give an example, you can see the impact that deep learning research has done to computer vision, where we see applications like Face-ID, self-driving car just around the corner. In the future, you will certainly see a boom in the applications that can be driven by deep learning techniques.A few things that I am particularly looking forward to,As Andrej Karpathy, an eminent personality in data science industry explains,Neural networks are not just another classifier, they represent the beginning of a fundamental shift in how we write software. They are Software 2.0. Software 2.0 is not going to replace the software we know now, but it is going to take over increasingly large portions of what software is responsible for today. This article list down a few of the groundbreaking events that have happened in the year 2017 in data science industry, and the implications they have on a data science professional. It also shows a path that can be seen by these advancements in the near future.If you know a ground-breaking event and want to share it with the community, please do write your comment below.",https://www.analyticsvidhya.com/blog/2017/12/reminiscing-2017-defining-moments-and-future-of-data-science/
11 most read Machine Learning articles from Analytics Vidhya in 2017,Learn everything about Analytics|Introduction|Top 11 Machine Learning articles from Analytics Vidhya in 2017|End Notes,"Ultimate Guide to Understand & Implement Natural Language Processing (with codes in Python)|Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning|A comprehensive beginners guide for Linear, Ridge and Lasso Regression

|Natural Language Processing Made Easy – using SpaCy ( in Python)|How to build Ensemble Models in machine learning? (with code in R)|Which algorithm takes the crown: Light GBM vs XGBOOST?|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)||Comprehensive Tutorial to Learn Data Science with Julia from Scratch|CatBoost: A machine learning library to handle categorical (CAT) data automatically||Solving Multi-Label Classification problems (Case studies included)||Tutorial on Automated Machine Learning using MLBox||Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Key Highlights in Data Science / Deep Learning / Machine Learning 2017 and What can we Expect in 2018?|11 most read Deep Learning Articles from Analytics Vidhya in 2017|
NSS
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The next post at the end of the year 2017 on our list of best-curated articles on – “Machine Learning”. These curated articles will be a one stop solution for people who are getting started with Machine Learning or who already have. This article contains all the best articles of 2017 which gathered the interest of the Machine Learning community.Similar to the previous article on -“Best Deep Learning articles in 2017”, I have added the used tool and the level of difficulty for each article to facilitate you with the choice. If you wish to include any other learning resource/article here, please mention them in the comments. A large amount of unstructured data present today is in the form of text, for example : Medical documents, legal agreements, tweets, blogs, newspapers, chat conversions etc. These text informations are the storehouse of new innovative products that can revolutionise the way we interact with the technology and live our lives. A few of the examples are:This is just the tip of the iceberg for what is possible if Natural Language is exploited.This article explains the basic concepts behind Natural Language Processing such as Text Processing, Feature Extraction from text etc. along with their codes in Python.This is a must read article for someone getting started into the field of Natural Language Processing.Tool: PythonLevel: Beginner Machine Learning has been with us since a long time ago, but it picked up pace about a decade back, part in thanks to the advancements in the hardware and in part to the Algorithms.This article is about one such Algorithm which is extremely popular in the field of Machine Learning – Gradient Descent. This article explains in detail about how Gradient Descent works, the problems in the original Gradient Descent and the variants of Gradient Descent for overcoming the problem along with the implementation.Level: Intermediate An operations manager working at a Supermarket chain in India knows about the amount of preparation the store chain needs to do before the Indian festive season (Diwali) kicks in. It is for them to estimate/predict which product will sell like hotcakes and which would not prior to the purchase. A bad decision can leave your customers to look for offers and products in the competitor stores. The challenge does not finish there – he also needs to estimate the sales of products across a range of different categories for stores in varied locations and with consumers having different consumption techniques. This article tells you everything you need to know about regression models and how they can be used to solve prediction problems like the one mentioned above.Tools: Python
Level: Intermediate There are many libraries out in the industry which provides methods for exploiting the text data to make sense out of it. Some of the examples being like Stanford CoreNLP, NLTK etc. and Python has been the go-to choice for working with text data.But these libraries lacking in the sense that they are bulky and with too much overhead like NLTK which downloads thousands and thousands of files for performing any NLP task.This is where SpaCy comes in – an industrial grade superfast NLP library which can perform almost all the NLP tasks with the breeze. This article makes you aware of the syntax of SpaCy and teaches you to perform some very common NLP tasks like PoS tagging, NER etc with minimal lines of code. The article also introduces the concept of Word vectors which are currently the state-of-the-art in features extracted from the text.Tools: PythonLevel: Intermediate If you are an active participant in the Data Science Competitions or have just started participating in the competitions and have gone through the solutions of the winners, you will notice that most of them use a blend of different models to extract that last drop of performance from the models.This blend of models is what is called – Ensemble Learning, where you combine the learnings of different models to create a better-learned model. In this article, you will learn about the different Ensembling techniques along with how you can code them up in R to ace your Data Science Competitions.Tools: RLevel: Intermediate For active members of the Data Science Competitions, XGBOOST almost became the go-to algorithm for performance and winning the competitions. It has the best of both the boosting machines and regularised methods.But it suffers from one problem: Given a huge amount of data, it takes a very long time to train. This is where LightGBM comes in.This article explains about LightGBM and compares it with XGBOOST in terms of performance and speed. This article is a must for people looking to reduce their training time in the competition without losing on the performance of the model.Tool: PythonLevel: Expert We as data scientists and machine learning engineers spend a lot of time trying to come up with the best performing model for solving a problem and most of the time we do get successful. But all these investments of time and mind will become useless if do not put the model in the real life.For example, an algorithm that can detect cataract just by looking at a photo is useless if the end user or person with cataract cannot input the image into the model. After all, models are created to solve a problem. Running a model shouldn’t be a problem for an end customer.This is where this article comes in. This article explains how you can deploy a machine learning model and use it to solve problems.Tools: PythonLevel: Expert There is a quote about Julia that says – “Walks like python. Runs like C.”The above line tells a lot about why creating ripples in the numerical computing space, even though it was in its early stages. Julia is a work straight out of MIT, a high-level language that has a syntax as friendly as Python and performance as competitive as C. This is not all, it provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.This article is about how can you utilize it in your workflow as a data scientist without going through hours of confusion which usually comes when we come across a new language.Tool: JuliaLevel: Beginner You have seen below error while building your machine learning models using “sklearn” – at least in the initial days.
 This error occurs when dealing with categorical (string) variables. In “sklearn”, you are required to convert these categories in the numerical format.In order to do this conversion, we use several pre-processing methods like “label encoding”, “one hot encoding” and others.This article discusses a recently open-sourced library ” CatBoost” developed and contributed by Yandex. As said by Mikhail Bilenko, Yandex’s head of machine intelligence and research, “This is the first Russian machine learning technology that’s an open source”! Pretty interesting right?
Tool – PyhtonLevel – Intermediate If we consider the image below – does this image contain a house? The option will be YES or NO.Consider another case, like what all things (or labels) are relevant to this picture?These types of problems, where we have a set of target variables, are known as multi-label classification problems. This article explains in detail what this problem entails and how to deal with it in the form of case studiesTool: Python
Level: Expert As soon as the library was released on GitHub, many data scientists were extremely excited to try it out. In this article, we have talked about an automated machine learning library “MLBox”.“MLBox is a powerful Automated Machine Learning Python library. It provides the following features:The library automates the machine learning and feature engineering process itself. Just to give you an example, with just 8 lines of code – the creator of the library broke into top 1% of data science hackathon. This article gives you hands-on practice of the library MLBox.Tool: PythonLevel: Expert I hope you found the resources useful. Machine Learning is already helpful in solving many problems in different fields. I hope that we have been helpful on your journey to learn this year and we promise to do so in the coming year as well.The Analytics Vidhya family wishes you Merry Christmas and very happy new year. May the new year bring the best of health, wealth and knowledge for you. In the meanwhile, if you have any suggestions / feedback, do share them with us. If you have any questions, feel free to drop your comments below.",https://www.analyticsvidhya.com/blog/2017/12/11-machine-learning-articles-analytics-vidhya-2017/
11 most read Deep Learning Articles from Analytics Vidhya in 2017,Learn everything about Analytics|Introduction|Top 11 Deep learning articles from Analytics Vidhya in 2017|End Notes,"1. Understanding and coding Neural Networks From Scratch in Python and R|2. Getting started with Deep Learning using Keras and TensorFlow in R|3. Architecture of Convolutional Neural Networks(CNN) demystified|4. Hands-on with Deep Learning – Solution for Age Detection Practice Problem|5. Debugging & Visualizing training of Neural Network with TensorBoard|6. Transfer learning & The art of using Pre-trained Models in Deep Learning| 7. Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks|8. An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|9. Getting Started with Audio Data Analysis using Deep Learning (with case study)|10. An introductory guide to Generative Adversarial Networks (GANs) and their promise!|11. 6 Deep Learning Applications a beginner can build in minutes (using Python)|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|11 most read Machine Learning articles from Analytics Vidhya in 2017|15 Trending Data Science GitHub Repositories you can not miss in 2017|
Dishashree Gupta
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"This is that time of year, when you reflect on the year gone by. 2017 has been a year of growth for us at Analytics Vidhya. By any means / metric – we have seen growth, be it web traffic, number of hackathons, number of discussions, team size, a journey of meetup to the large-scale summit.This year, we covered breadth as well as depth on various data science topics including machine learning, deep learning, and reinforcement learning. You will see lot more articles coming your way in 2018. It gives us immense satisfaction to be able to create something which is helping more and more people every day.Whether you are charting your learning plan for 2018 or reflecting back on your learning this year – curation of best resources can be immensely helpful. This is why, we present our first curation on best AV articles published on Deep Learning in 2017. If you missed these articles, you should go through them. If you are new to deep learning – this is going to be the best resources curated at a single place for you.To help you, we have also mentioned the level of article and the tools used in the article.So stay warm, keep your machines running and keep learning as the new year sets in. If you followed any other resources on deep learning this year, please feel free to mention them in the comments below.  Neural networks are believed to be black boxes. People run far away from understanding their inner working. Reading this article would completely change that.This article simplifies the architecture of a neural network beginning from what a perceptron is going all the way to code them in numpy and in R.By end of this article, you will understand how Neural networks work, how do we initialize weights and how do we update them using back-propagation. It’s a must read article for those who wish to completely understand the working of neural networks in and out.Tools Used: Python(numpy), RLevel: Intermediate It has always been a debatable topic to choose between R and Python. The Machine Learning world has been divided over the preference of one language over the other. But with the explosion of Deep Learning, the balance shifted towards Python as it had an enormous list of Deep Learning libraries and frameworks which R lacked (till now).With launch of Keras in R, this fight is back at the center. Python was slowly becoming the de-facto language for Deep Learning models. But with the release of Keras library in R with tensorflow (CPU and GPU compatibility)  at the backend as of now, it is likely that R will again fight Python for the podium even in the Deep Learning space.In this article, you will see how to install Keras with Tensorflow in R and build your first Neural Network model on the classic MNIST dataset in RStudio.Library used – keras in RLevel – Intermediate In this article we discussed the architecture behind Convolutional Neural Networks, which are designed to address image recognition and classification problems. Every image is an arrangement of pixels arranged in a special order. If you change the order or value of a pixel, the image would change as well. To understand an image for a network it’s extremely important to understand how the pixels are arranged. Convolutional networks are designed specifically to conserve the arrangement of these pixels and obtain various features from the images. This article would help you understand the various layers in a convolutional neural network and would also enable you to implement an image classification task using CNNs.Library used: KerasLevel: Intermediate It is one thing to learn data science by reading or watching a video / MOOC and other to apply it on problems. You need to do both the things to learn the subject effectively. If you are questioning, why learn or apply deep learning – you have most likely come out of a cave just now. Deep learning in already powering face detection in cameras, voice recognition on mobile devices to deep learning cars. This article encourages you to solve fun and interesting problem – to detect the age of a person using Deep Learning.Library Used: kerasLevel: Advanced If you have tried to train a neural network, you must know the plight of figuring out why does it not converge. It could range from a simple data transformation issue to a model creation issue. This article focuses on a workflow to debug a neural network. The purpose is guide to you as to how would you approach to solve the problem.  The article also introduces a tool which is a useful addition to the deep learning toolbox – TensorBoard.Library used: Keras, TensorBoardLevel: Intermediate In today’s world, RAM on a machine is pretty cheap and is easily available with little investment. You need hundreds of GBs of RAM to run a super complex supervised machine learning problem. On the other hand, access to GPUs is not that cheap. If you need access to a hundred GB VRAM on GPUs – it won’t be straight forward and would involve significant costs. When we try to solve complex real life problems on areas like image and voice recognition, once you have a few hidden layers in your model, adding another layer of hidden layer would need immense resources and time. There is something called “Transfer Learning” which enables us to use pre-trained models from other people for our own problems by some tweaking. In this article, I am going to tell how we can use pre-trained models to accelerate our solutions and use pre-trained models like VGG16 for image classification.Library used: KerasLevel: Advanced There are multiple such cases wherein the sequence of information defines the data itself. If we are trying to use such data for any reasonable output, we need a network which has access to some prior knowledge about the data to completely understand it. Recurrent neural networks thus come into play. In this article we introduce recurrent neural networks. To make things simple for you we have shown the working for a recurrent neuron in Excel. We also discuss the Backpropagation in time along with some shortcomings of the recurrent neural networks. To say in a few words it’s an introductory guide that shall enable you to understand and to use recurrent neural networks in a problem of your own. Library used: KerasLevel: Intermediate When we look at an example such as typing a sentence in google translate in English and getting an equivalent Chinese conversion, we see an application of text processing. Text processing deals with humongous amount of text to perform different range of tasks like clustering, classification and Machine Translation. Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing these tasks. Whereas for a computer, it is very hard to perform the tasks which humans can do quite effectively. Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? The answer to the question lies in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. All of these are implemented by using “Word Embeddings” or numerical representations of texts so that computers may handle them. In the article, you will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results.Library used – gensimLevel – Advanced When you get started with data science, you start simple. You go through simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format. In other words, you are spoon-fed the hardest part in data science pipeline. The datasets in real life are much more complex. You first have to understand it, collect it from various sources and arrange it in a format which is ready for processing. This is even more difficult when the data is in an unstructured format such as image or audio. This is so because you would have to represent image/audio data in a standard way for it to be useful for analysis.Interestingly, unstructured data represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice. In this article, an overview of audio / voice processing with a case study is covered so that you would get a hands-on introduction to solving audio processing problems.Library used: KerasLevel: Intermediate Neural Networks have made great progress. They now recognize images and voice at levels comparable to humans. They are also able to understand natural language with a good accuracy. But even then, the talk of automating human tasks with machines looks a bit far fetched. After all, we do much more than just recognizing image / voice or understanding what people around us are saying – don’t we? Let us see a few examples where we need human creativity (at least as of now):Do you think, these tasks can be accomplished by machines? Well – the answer might surprise you 🙂 These are definitely difficult to automate tasks, but Generative Adversarial Networks (GANs) have started making some of these tasks possible.If you feel intimidated by the name GAN – don’t worry! You will feel comfortable with them by end of this article. This article introduces you to the concept of GANs and explains how they work along with the challenges. It will also let you know of some cool things people have done using GANs and give you links to some of the important resources for getting deeper into these techniques.Library used: KerasLevel: Advanced Deep Learning has been the most researched and talked about topic in data science recently. And it deserves the attention it gets, as some of the recent breakthroughs in data science are emanating from deep learning. It’s predicted that many deep learning applications will affect your life in the near future. However, if you have been looking at deep learning from the outside, it might look difficult and intimidating. Terms like TensorFlow, Keras, GPU based computing might scare you. But it’s actually not that difficult! While cutting edge deep learning will take time and effort to follow, applying them in simple day to day problems is very easy.This article showcases 6 such applications – which might look difficult at the outset, but can be achieved using Deep Learning implementation in less than an hour, to give you a taste of how they work.Tools Used: Deep Learning APIs and Open source Deep learning softwareLevel: Beginner I hope you found the resources useful. Deep learning is spreading its wings and this year has made us learn a lot in this domain. I hope that we have been helpful on your journey to learn this year and we promise to do so in the coming year as well.The Analytics Vidhya family wishes you Merry Christmas and very happy new year. May the new year bring the best of health, wealth and knowledge for you. In the meanwhile, if you have any suggestions / feedback, do share them with us. If you have any questions, feel free to drop your comments below.",https://www.analyticsvidhya.com/blog/2017/12/11-deep-learning-analytics-vidhya-2017/
15 Trending Data Science GitHub Repositories you can not miss in 2017,Learn everything about Analytics|Introduction|Table of Contents|1. Learning Resources|2. Open Source Softwares|End Notes,"1.1 Awesome Data Science|1.2 Machine Learning / Deep Learning Cheat Sheet|1.3 Oxford Deep Natural Language Processing Course Lectures|1.4 PyTorch – Tutorial|1.5 Resources of NIPS 2017|2.1 TensorFlow|2.2 TuriCreate – A Simplified Machine Learning Library|2.3 OpenPose|2.4 DeepSpeech|2.5 Mobile Deep Learning|2.6 Visdom||2.7 Deep Photo Style Transfer|2.8 CycleGAN|2.9 Seq2seq|2.10 Pix2code|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|11 most read Deep Learning Articles from Analytics Vidhya in 2017|Introduction to Computational Linguistics and Dependency Trees in data science|
Sunil Ray
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"GitHub is much more than a software versioning tool, which it was originally meant to be. Now people from different backgrounds and not just software engineers are using it to share their tools / libraries they developed on their own, or even share resources that might be helpful for the community.Following the best repos on GitHub can be an immense learning experience. You not only see what are the best open contributions, but also see how their code was written and implemented.Being an avid data science enthusiast, I have curated a list of repositories that have been particularly famous in the year 2017. Enjoy and Keep learning!  This GitHub repository is an ultimate resource guide to data science. It is built upon multiple contributions over the years with links to resources ranging from getting-started guides, infographics to people to follow on social networking sites like twitter, facebook, Instagram etc. There are plenty of resources waiting to be viewed, irrespective of whether you are a beginner or a veteran.A look at table of contents of the repo says it all about the depth of resources in the repository:Link to repository: https://github.com/bulutyazilim/awesome-datascience This repository consists of the commonly used tools and techniques compiled in the form of cheatsheets. The cheatsheets range from very simple tools like pandas to techniques like Deep Learning. After giving a star or forking the repository, you won’t need to google the most commonly used tips and tricks.To give you a glimpse, the different types of cheatsheets are pandas, numpy, scikit learn, matplotlib, ggplot, dplyr, tidyr, pySpark and Neural Networks .Link to repository: https://github.com/kailashahirwar/cheatsheets-ai Stanford NLP has always been a golden course for people wanting to venture out into the field of Natural Language Processing. But with the advent of Deep Learning, NLP has seen tremendous progress, all thanks to the capabilities of Deep Learning Architectures such as RNN and LSTMs.This repository based on Oxford NLP Lectures take the education of NLP to next level. A practical course, these lectures covers the techniques and terminologies to advance material such as using RNNs for Language Modeling, Speech Recognition, Text to Speech etc. This repository is a one stop shop for all the materials of the Oxford Lectures providing Lecture materials to Practical assignments.Link to repository: https://github.com/oxford-cs-deepnlp-2017/lectures As of now, PyTorch is the sole competitor to Tensorflow and it is doing a good job of maintaining its reputation. With the ease of Pythonic style coding, Dynamic Computations, and faster prototyping, PyTorch has garnered enough attention of the Deep Learning Community.This repository contains codes for Deep Learning tasks ranging from learning basic of creating a Neural Network in PyTorch to coding RNNs, GANs and Neural Style Transfers. Most of the models have been implemented with as few as 30 lines of code. This speaks volume about the abstraction provided by PyTorch so that researchers may focus on finding the right model quickly rather than getting entangled in the nitty gritty of programming language or tool choice.Link to repository: https://github.com/yunjey/pytorch-tutorial This repository is a list of resources and slides of all invited talks, tutorials, and workshops in NIPS 2017 conference. For those who do not know what NIPS is, it is an annual conference specifically for Machine learning and Computational Neuroscience.Most of the breakthrough research that has happened in the data science industry in the last couple of years has been a result of the research that has been presented at this conference. If you want to stay ahead of the curve, this is the right resource to follow!Link to repository: https://github.com/hindupuravinash/nips2017 It has been 2 years since the official release of TensorFlow, but it has maintained the status of being the top Machine Learning / Deep Learning library. Google Brain and the community behind the development of TensorFlow has been actively contributing and keeping it abreast with the latest developments especially in Deep Learning domain.TensorFlow was originally built as a library for numerical computation using data flow graphs. But looking at its current state, it can be pretty much said to be a complete library for building Deep Learning models. Although TensorFlow majorly supports Python, it also provides support for languages such as C, C++, Java and many more. And a cherry on the cake, it can also be run on a mobile platform!Link to Repository: https://github.com/tensorflow/tensorflow A recent open source contribution by Apple, TuriCreate is the talk of the day. It boasts of easy-to-use creation and deployment of machine learning models for complex tasks such as object detection, activity classification, and recommendation systems.Being a data science enthusiast for some time now, I remember that void that had been created when Turi (the company that created GraphLab Create – an amazing machine learning library) was acquired by Apple. Everyone in the data science industry had been waiting for this kind of explosion to happen!TuriCreate is developed specially for python. One of the best features that TuriCreate provides is its easy deployability of machine learning models to Core ML (another open source software by Apple) for use in iOS, macOS, watchOS, and tvOS appsLink to repository: https://github.com/apple/turicreate OpenPose is a multi-person keypoint detection library which helps you to detect positions of a person in an image or video at real-time speed. Developed by CMU’s perceptual computing lab, OpenPose is a fine example of how open sourced research can be easily inculcated in the industry.One of the use cases that OpenPose helps to solve is activity detection. For example, an activity done by an actor can be captured in real time. Then these key points and their motions can be used to create animated films.OpenPose has a C++ API which can be used to access the library. But it also has a simple command line interface to process images or videos.Link to repository: https://github.com/CMU-Perceptual-Computing-Lab/openpose DeepSpeech library is an open source implementation of the state-of-the-art technique for Speech-to-Text synthesis by Baidu Research. It is based on TensorFlow and can be used specifically for Python, but it also has bindings for NodeJS and can be used on the command line too.Mozilla has been one of the main workforces for building DeepSpeech from scratch and open sourcing the library. “There are only a few commercial quality speech recognition services available, dominated by a small number of large companies. This reduces user choice and available features for startups, researchers or even larger companies that want to speech-enable their products and services, Together with a community of like-minded developers, companies, and researchers, we have applied sophisticated machine learning techniques and a variety of innovations to build a speech-to-text engine …” Sean White, vice president of technology strategy at Mozilla, wrote in a blog post.I believe that the library is worth checking out. Do let me know if you use it in the comments below.Link to repository: https://github.com/mozilla/DeepSpeech This repository brings about the state-of-the-art technique in data science to the mobile platform. Developed by Baidu Research, the repository aims to deploy Deep Learning models on mobile devices such as Android and IOS with low complexity and high speed. A simple use case as explained in the repository itself is object detection. It can identify the exact location of an object such as mobile in an image. Pretty cool right?Link to repository: https://github.com/baidu/mobile-deep-learning Visdom is a library that supports broadcasting of plots, images, and text among collaborators. You can organize your visualization space programmatically or through the UI to create dashboards for live data, inspect results of experiments, or debug experimental code.The exact inputs into the plotting functions vary, although most of them take as input a tensor X than contains the data and an (optional) tensor Y that contains optional data variables (such as labels or timestamps). It supports all basic plot types to create visualizations that are powered by Plotly.Visdom supports Torch and Numpy within Python.Link to repository: https://github.com/facebookresearch/visdomThis repository is based on a research paper that introduces a deep learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. The approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including the transfer of the time of day, weather, season, and artistic edits. This code is based on torch.Link to Repository: https://github.com/luanfujun/deep-photo-styletransfer CycleGAN is a fun but powerful library which shows the potential of the state-of-the-art technique.  Just to give an example, the image below is a glimpse of what the library can do – adjusting the depth perception of the image. The catch here is that you haven’t told the algorithm which part of the image to focus upon. It does this on its own!The library is currently written in Lua, but it can be used in command line too.Link to repository: https://github.com/junyanz/CycleGAN Seq2seq was initially built for Machine Translation, but have since been developed to be used for a variety of other tasks, including Summarization, Conversational Modeling, and Image Captioning. As long as a problem can be moulded as encoding input data in one format and decoding it into another format, this framework can be used. It is programmed using the all popular Tensorflow library for Python.Link to Repository: https://github.com/google/seq2seq This one is a really exciting project using deep learning that attempts to automatically generate code for a given GUI. When building a website or a mobile interface, front-end engineers typically have to write repetitive code that is time consuming and non-productive. This essentially prevent developers from dedicating the majority of their time to implement the actual functionality and logic of the software they are building. Pix2code intends to remedy this by automating the process. It is based on a novel approach allowing the generation of computer tokens from a single GUI screenshot as input.Here is a video explaining the use case of pix2code.Pix2code is written in python and can be used to convert image captures of both mobile and web interfaces to code. The project can be accessed in the link below.Link to Repository: https://github.com/tonybeltramelli/pix2code I hope you got to know a few of the new open source tools/technologies that have been released on GitHub in the year 2017. I have also listed down resources that have been trending on GitHub. If you have seen more such useful repositories in the past, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/12/15-data-science-repositories-github-2017/
Introduction to Computational Linguistics and Dependency Trees in data science,Learn everything about Analytics|Introduction||Computational linguistics|Applications of Dependency Trees|Dependency Trees using Spacy||End Notes,"|Example – Problem with Neural Networks|Named Entity Recognition|Coreference Resolution or Anaphora Resolution|Question Answering|Other Tasks that uses Computational Linguistics|Generating Dependency Trees using Stanford Core NLP|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|15 Trending Data Science GitHub Repositories you can not miss in 2017|Essentials of Deep Learning : Introduction to Long Short Term Memory|
Shivam Bansal
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"In recent years, the amalgam of deep learning fundamentals with Natural Language Processing techniques has shown a great improvement in the information mining tasks on unstructured text data.The models are now able to recognize natural language and speech comparable to human levels. Despite such improvements, discrepancies in the results still exist as sometimes the information is coded very deep in the syntaxes and syntactic structures of the corpus.For example, a conversation system which is trained using recurrent neural network produces the following results in two scenarios:User: Hi, I took a horrible picture in a museum, can you tell where is it located?
Bot Reply 1: The museum is located at a horrible placeUser: Hi, I took a horrible picture in a museum, can you tell where is it located?
Bot Reply 2: The horrible museum is located at this placeThe two responses have virtually similar tokens but different structures which completely changes the context. In this article, I will discuss the interdisciplinary field of Computational Linguistics which deals with the structural aspects of the text that are used to solve common text related problems. Some examples are Named entity extraction, coreference resolution, and machine translation.Computational linguistics often overlaps with the field of Natural Language Processing as most of the tasks are common to both the fields. While Natural Language Processing focuses on the tokens/tags and uses them as predictors in machine learning models, Computational Linguistics digs further deeper into the relationships and links among them.Structural aspects of the text refer to the organization of tokens in a sentence and the how the contexts among them are interrelated. This organization is often depicted by the word-to-word grammar relationships which are also known as dependencies. Dependency is the notion that syntactic units (words) are connected to each other by directed links which describe the relationship possessed by the connected words.These dependencies map directly onto a directed graph representation, in which words in the sentence are nodes in the graph and grammatical relations are edge labels. This directed graph representation is also called as the dependency tree. For example, the dependency tree of the sentence is shown in the figure below:AnalyticsVidhya is the largest community of data scientists and provides best resources for understanding data and analytics.Another way to represent this tree is following:-> community-NN (root)
-> AnalyticsVidhya-NNP (nsubj)
-> is-VBZ (cop)
-> the-DT (det)
-> largest-JJS (amod)
-> scientists-NNS (pobj)
-> of-IN (prep)
-> data-NNS (case)
-> and-CC (cc)
-> provides-VBZ (conj)
-> resources-NNS (dobj)
-> best-JJS (amod)
-> understanding-VBG (pcomp)
-> for-IN (mark)
-> data-NNS (dobj)
-> and-CC (cc)
-> analytics-NNS (conj)These trees can be generated in python using libraries such as NLTK, Spacy or Stanford-CoreNLP and can be used to obtain subject-verb-object triplets, noun and verb phrases, grammar dependency relationships, and part of speech tags etc for example –-> of-IN (prep)-> data-NNS (nn)POS: IN – NNS – NNSPhrase: of data scientist-> for-IN (prep)-> data-NNS (dobj)-> and-CC (cc)-> analytics-NNS (conj)POS: NNS – CC – NNSPhrase: data and analytics Grammar: <prep> <pcomp> <prep> <dobj>POS: VBG – IN – NNSPhrase: for understanding data and analytics Named-entity recognition (NER) is the process of locating and classifying named entities in a textual data into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.To recognize the named entities, one needs to parse the dependency tree in a top to bottom manner and identify the noun phrases. Noun phrases are the subtrees which are connected by a relation from nouns family such as nsubj, nobj, dobj etc nmod etc.  For example –Donald Trump will be visiting New Delhi next summer for a conference at Google-> visiting-VBG (root)
-> Trump-NNP (nsubj)
-> Donald-NNP (compound)
-> will-MD (aux)
-> be-VB (aux)
-> Delhi-NNP (dobj)
-> New-NNP (compound)
-> summer-NN (tmod)
-> next-JJ (amod)
-> conference-NN (nmod)
-> for-IN (case)
-> a-DT (det)
-> Google-NNP (nmod)
-> at-IN (case)In the above trees, following noun phrases are are detected from the grammar relations of noun family such as:Trump <-> visiting <by> nsubj
Delhi <-> visiting <by> dobj
summer <-> Delhi <by> nmod
conference <-> visiting <by> nmod
Google <-> conference <by> nmodNamed entities can be obtained by identifying the NNP (proper noun) part of speech tag of the root node. Example – Trump, Delhi, and Google have the part of speech tag NNP. To generate the proper noun phrase linked with root node, one needs to parse the subtree linked with the root nodes.  Using the grammar rules, following named entities are obtained:<compound> <nsubj> : Donald Trump
<compound> <dsubj> : New Delhi
<nmod> : GoogleOther tasks such as phrase chunking and entity wise sentiment analysis can be performed using similar processes. For example, one sentence may contain multiple sentiments, contexts and entities and dictionary based models may not perform well. In the following sentence, there are two contexts:Sentence – His acting was good but the script was poorContext 1: His acting was good
Context 2: the script was poorBoth of the contexts have different sentiments, one is negative while other is positive. In the dependency tree of this sentence, there are two sub-trees which correspond to different contexts and can be used to extract them. The dependency tree of the sentence isThe subtrees can be extracted and evaluated individually from this dependency tree to compute the dependency tree. This paper from Stanford and this paper from Singapore University describes the efficient approaches to perform NER using dependencies. Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.The coreference processJohn telephoned Bill. He lost his laptop.Np -> noun phrases nodes
Hw -> headwords
Rl -> grammar relations
Lv -> level in the tree
Pos -> part of speech tag
Gen -> gender of part of speech tagUsing Named Entity Recognizer, identified named entities are Bill, JohnUsing gender api such as this, gender of named entities are Male entities.Features of sentences:3.1 map the tokens with same gender of pronoun and named entity3.2 map the tokens with same singularity / plurality3.3 map the tokens with same grammar relationsThis paper from Soochow University describes the use of dependency trees in coreference resolution. Another important task in which computational linguistics can help to obtain results with high relevance is the Question Answering which is treated as one of the hardest tasks involved with text data. Question answering systems based on computational linguistics uses the syntactic structures of the query questions and matches them with the responses having similar syntactic structures. The similar syntactic structures contribute the answer set to a particular question. For exampleQuestion: What is the capital of India?Answer: New Delhi is the capital of India-> capital-NN (root)
-> what-WP (nsubj)
-> is-VBZ (cop)
-> the-DT (det)
-> of-IN (prep)
-> India-NNP (pobj) -> capital-NN (root)
-> Delhi-NNP (nsubj)
-> New-NNP (nn)
-> is-VBZ (cop)
-> the-DT (det)
-> of-IN (prep)
-> India-NNP (pobj) Both the question and answer dependency trees have similar patterns and can be used to generate the answer responses to specific queries. This paper1 and paper2 describe the approaches to perform question answering using dependency trees.    Note: If you wish to explore this field further, then have a look at our detailed video course on NLP.In this article, I discussed the field of computational linguistics and how grammar relations among the sentences can be used in different tasks related to text data. If you feel, there are any other resources, tasks related to dependency trees and computational linguistics that I have missed, please feel free to comment with your suggestions and feedback.",https://www.analyticsvidhya.com/blog/2017/12/introduction-computational-linguistics-dependency-trees/
Essentials of Deep Learning : Introduction to Long Short Term Memory,"Learn everything about Analytics|Introduction|Table of Contents|1. Flashback: A look into Recurrent Neural Networks (RNN)
|2. Limitations of RNNs|3. Improvement over RNN: LSTM (Long Short-Term Memory) Networks|4. Architecture of LSTMs|5. Text generation using LSTMs|End Notes","4.1 Forget Gate|4.2 Input Gate|4.3 Output Gate|Importing dependencies |Loading text file and creating character to integer mappings|Preparing dataset|Reshaping of X|Defining the LSTM model|Fitting the model and generating characters|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Introduction to Computational Linguistics and Dependency Trees in data science|Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks|
Pranjal Srivastava
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Sequence prediction problems have been around for a long time. They are considered as one of the hardest problems to solve in the data science industry. These include a wide range of problems; from predicting sales to finding patterns in stock markets’ data, from understanding movie plots to recognizing your way of speech, from language translations to predicting your next word on your iPhone’s keyboard. With the recent breakthroughs that have been happening in data science, it is found that for almost all of these sequence prediction problems, Long short Term Memory networks, a.k.a LSTMs have been observed as the most effective solution.
LSTMs have an edge over conventional feed-forward neural networks and RNN in many ways. This is because of their property of selectively remembering patterns for long durations of time.  The purpose of this article is to explain LSTM and enable you to use it in real life problems.  Let’s have a look!Note: To go through the article, you must have basic knowledge of neural networks and how Keras (a deep learning library) works. You can refer the mentioned articles to understand these concepts:  Take an example of sequential data, which can be the stock market’s data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price of the stock depends on these features, it is also largely dependent on the stock values in the previous days. In fact for a trader, these values in the previous days (or the trend) is one major deciding factor for predictions.In the conventional feed-forward neural networks, all test cases are considered to be independent. That is when fitting the model for a particular day, there is no consideration for the stock prices on the previous days.This dependency on time is achieved via Recurrent Neural Networks. A typical RNN looks like:This may be intimidating at first sight, but once unfolded, it looks a lot simpler:Now it is easier for us to visualize how these networks are considering the trend of stock prices, before predicting the stock prices for today. Here every prediction at time t (h_t) is dependent on all previous predictions and the information learned from them. RNNs can solve our purpose of sequence handling to a great extent but not entirely. We want our computers to be good enough to write Shakespearean sonnets. Now RNNs are great when it comes to short contexts, but in order to be able to build a story and remember it, we need our models to be able to understand and remember the context behind the sequences, just like a human brain. This is not possible with a simple RNN.Why? Let’s have a look. Recurrent Neural Networks work just fine when we are dealing with short-term dependencies. That is when applied to problems like:RNNs turn out to be quite effective. This is because this problem has nothing to do with the context of the statement. The RNN need not remember what was said before this, or what was its meaning, all they need to know is that in most cases the sky is blue. Thus the prediction would be:However, vanilla RNNs fail to understand the context behind an input. Something that was said long before, cannot be recalled when making predictions in the present. Let’s understand this as an example:Here, we can understand that since the author has worked in Spain for 20 years, it is very likely that he may possess a good command over Spanish. But, to make a proper prediction, the RNN needs to remember this context. The relevant information may be separated from the point where it is needed, by a huge load of irrelevant data. This is where a Recurrent Neural Network fails!The reason behind this is the problem of Vanishing Gradient. In order to understand this, you’ll need to have some knowledge about how a feed-forward neural network learns. We know that for a conventional feed-forward neural network, the weight updating that is applied on a particular layer is a multiple of the learning rate, the error term from the previous layer and the input to that layer. Thus, the error term for a particular layer is somewhere a product of all previous layers’ errors. When dealing with activation functions like the sigmoid function, the small values of its derivatives (occurring in the error function) gets multiplied multiple times as we move towards the starting layers. As a result of this, the gradient almost vanishes as we move towards the starting layers, and it becomes difficult to train these layers.A similar case is observed in Recurrent Neural Networks. RNN remembers things for just small durations of time, i.e. if we need the information after a small time it may be reproducible, but once a lot of words are fed in, this information gets lost somewhere. This issue can be resolved by applying a slightly tweaked version of RNNs – the Long Short-Term Memory Networks.    When we arrange our calendar for the day, we prioritize our appointments right? If in case we need to make some space for anything important we know which meeting could be canceled to accommodate a possible meeting. Turns out that an RNN doesn’t do so. In order to add a new information, it transforms the existing information completely by applying a function. Because of this, the entire information is modified, on the whole, i. e. there is no consideration for ‘important’ information and ‘not so important’ information. LSTMs on the other hand, make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. This way, LSTMs can selectively remember or forget things. The information at a particular cell state has three different dependencies. We’ll visualize this with an example. Let’s take the example of predicting stock prices for a particular stock. The stock price of today will depend upon: These dependencies can be generalized to any problem as:Another important feature of LSTM is its analogy with conveyor belts! That’s right!Industries use them to move products around for different processes. LSTMs use this mechanism to move information around. We may have some addition, modification or removal of information as it flows through the different layers, just like a product may be molded, painted or packed while it is on a conveyor belt. The following diagram explains the close relationship of LSTMs and conveyor belts.Source
Although this diagram is not even close to the actual architecture of an LSTM, it solves our purpose for now.Just because of this property of LSTMs, where they do not manipulate the entire information but rather modify them slightly, they are able to forget and remember things selectively. How do they do so, is what we are going to learn in the next section? The functioning of LSTM can be visualized by understanding the functioning of a news channel’s team covering a murder story. Now, a news story is built around facts, evidence and statements of many people. Whenever a new event occurs you take either of the three steps.Let’s say, we were assuming that the murder was done by ‘poisoning’ the victim, but the autopsy report that just came in said that the cause of death was ‘an impact on the head’. Being a part of this news team what do you do? You immediately forget the previous cause of death and all stories that were woven around this fact.What, if an entirely new suspect is introduced into the picture. A person who had grudges with the victim and could be the murderer? You input this information into your news feed, right?Now all these broken pieces of information cannot be served on mainstream media. So, after a certain time interval, you need to summarize this information and output the relevant things to your audience. Maybe in the form of “XYZ turns out to be the prime suspect.”.Now let’s get into the details of the architecture of LSTM network:SourceNow, this is nowhere close to the simplified version which we saw before, but let me walk you through it. A typical LSTM network is comprised of different memory blocks called cells
(the rectangles that we see in the image).  There are two states that are being transferred to the next cell; the cell state and the hidden state. The memory blocks are responsible for remembering things and manipulations to this memory is done through three major mechanisms, called gates. Each of them is being discussed below.Taking the example of a text prediction problem. Let’s assume an LSTM is fed in, the following sentence:As soon as the first full stop after “person” is encountered, the forget gate realizes that there may be a change of context in the next sentence. As a result of this, the subject of the sentence is forgotten and the place for the subject is vacated. And when we start speaking about “Dan” this position of the subject is allocated to “Dan”. This process of forgetting the subject is brought about by the forget gate.
A forget gate is responsible for removing information from the cell state. The information that is no longer required for the LSTM to understand things or the information that is of less importance is removed via multiplication of a filter. This is required for optimizing the performance of the LSTM network. This gate takes in two inputs; h_t-1 and x_t.h_t-1 is the hidden state from the previous cell or the output of the previous cell and x_t is the input at that particular time step. The given inputs are multiplied by the weight matrices and a bias is added. Following this, the sigmoid function is applied to this value. The sigmoid function outputs a vector, with values ranging from 0 to 1, corresponding to each number in the cell state. Basically, the sigmoid function is responsible for deciding which values to keep and which to discard. If a ‘0’ is output for a particular value in the cell state, it means that the forget gate wants the cell state to forget that piece of information completely. Similarly, a ‘1’ means that the forget gate wants to remember that entire piece of information. This vector output from the sigmoid function is multiplied to the cell state.  Okay, let’s take another example where the LSTM is analyzing a sentence:Now the important information here is that “Bob” knows swimming and that he has served the Navy for four years. This can be added to the cell state, however, the fact that he told all this over the phone is a less important fact and can be ignored. This process of adding some new information can be done via the input gate.Here is its structure:The input gate is responsible for the addition of information to the cell state. This addition of information is basically three-step process as seen from the diagram above. Once this three-step process is done with, we ensure that only that information is added to the cell state that is important and is not redundant.  Not all information that runs along the cell state, is fit for being output at a certain time. We’ll visualize this with an example:In this phrase, there could be a number of options for the empty space. But we know that the current input of ‘brave’, is an adjective that is used to describe a noun. Thus, whatever word follows, has a strong tendency of being a noun. And thus, Bob could be an apt output.This job of selecting useful information from the current cell state and showing it out as an output is done via the output gate. Here is its structure:The functioning of an output gate can again be broken down to three steps:The filter in the above example will make sure that it diminishes all other values but ‘Bob’. Thus the filter needs to be built on the input and hidden state values and be applied on the cell state vector. We have had enough of theoretical concepts and functioning of LSTMs. Now we would be trying to build a model that can predict some n number of characters after the original text of Macbeth. Most of the classical texts are no longer protected under copyright and can be found here. An updated version of the .txt file can be found here.We will use the library Keras, which is a high-level API for neural networks and works on top of TensorFlow or Theano. So make sure that before diving into this code you have Keras installed and functional. Okay, so let’s generate some text! We import all the required dependencies and this is pretty much self-explanatory. The text file is open, and all characters are converted to lowercase letters. In order to facilitate the following steps, we would be mapping each character to a respective number. This is done to make the computation part of the LSTM easier.Data is prepared in a format such that if we want the LSTM to predict the ‘O’ in ‘HELLO’  we would feed in [‘H’, ‘E‘ , ‘L ‘ , ‘L‘ ] as the input and [‘O’] as the expected output. Similarly, here we fix the length of the sequence that we want (set to 50 in the example) and then save the encodings of the first 49 characters in X and the expected output i.e. the 50th character in Y. A LSTM network expects the input to be in the form [samples, time steps, features] where samples is the number of data points we have, time steps is the number of time-dependent steps that are there in a single data point, features refers to the number of variables we have for the corresponding true value in Y. We then scale the values in X_modified between 0 to 1 and one hot encode our true values in Y_modified. A sequential model which is a linear stack of layers is used. The first layer is an LSTM layer with 300 memory units and it returns sequences. This is done to ensure that the next LSTM layer receives sequences and not just randomly scattered data. A dropout layer is applied after each LSTM layer to avoid overfitting of the model. Finally, we have the last layer as a fully connected layer with a ‘softmax’ activation and neurons equal to the number of unique characters, because we need to output one hot encoded result. The model is fit over 100 epochs, with a batch size of 30. We then fix a random seed (for easy reproducibility) and start generating characters. The prediction from the model gives out the character encoding of the predicted character, it is then decoded back to the character value and appended to the pattern.  This is how the output of the network would look likeEventually, after enough training epochs, it will give better and better results over the time. This is how you would use LSTM to solve a sequence prediction task. LSTMs are a very promising solution to sequence and time series related problems. However, the one disadvantage that I find about them, is the difficulty in training them. A lot of time and system resources go into training even a simple model. But that is just a hardware constraint! I hope I was successful in giving you a basic understanding of these networks. For any problems or issues related to the blog, please feel free to comment below. ",https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks,Learn everything about Analytics|Introduction|Table of Contents|Need for a Neural Network dealing with Sequences|What are Recurrent Neural Networks?|Understanding a Recurrent Neuron in Detail|Forward Propagation in a Recurrent Neuron in Excel|Back propagation in a Recurrent Neural Network(BPTT)|Implementation of Recurrent Neural Networks in Keras|Vanishing and Exploding Gradient Problem|Other RNN architectures|End Notes,"Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Essentials of Deep Learning : Introduction to Long Short Term Memory|Introduction to Altair – A Declarative Visualization Library in Python|
Dishashree Gupta
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Let me open this article with a question – “working love learning we on deep”, did this make any sense to you? Not really – read this one – “We love working on deep learning”. Made perfect sense! A little jumble in the words made the sentence incoherent. Well, can we expect a neural network to make sense out of it? Not really! If the human brain was confused on what it meant I am sure a neural network is going to have a tough time deciphering such text.There are multiple such tasks in everyday life which get completely disrupted when their sequence is disturbed. For instance, language as we saw earlier- the sequence of words define their meaning, a time series data – where time defines the occurrence of events, the data of a genome sequence- where every sequence has a different meaning. There are multiple such cases wherein the sequence of information determines the event itself. If we are trying to use such data for any reasonable output, we need a network which has access to some prior knowledge about the data to completely understand it. Recurrent neural networks thus come into play.In this article I would assume that you have a basic understanding of neural networks, in case you need a refresher please go through this article before you proceed.  Before we deep dive into the details of what a recurrent neural network is, let’s ponder a bit on if we really need a network specially for dealing with sequences in information. Also what are kind of tasks that we can achieve using such networks.The beauty of recurrent neural networks lies in their diversity of application. When we are dealing with RNNs they have a great ability to deal with various input and output types.So RNNs can be used for mapping inputs to outputs of varying types, lengths and are fairly generalized in their application. Looking at their applications, let’s see how the architecture of an RNN looks like. Let’s say the task is to predict the next word in a sentence. Let’s try accomplishing it using an MLP. So what happens in an MLP. In the simplest form, we have an input layer, a hidden layer and an output layer. The input layer receives the input, the hidden layer activations are applied and then we finally receive the output.Let’s have a deeper network, where multiple hidden layers are present. So here, the input layer receives the input, the first hidden layer activations are applied and then these activations are sent to the next hidden layer, and successive activations through the layers to produce the output. Each hidden layer is characterized by its own weights and biases.Since each hidden layer has its own weights and activations, they behave independently. Now the objective is to identify the relationship between successive inputs. Can we supply the inputs to hidden layers? Yes we can! Here, the weights and bias of these hidden layers are different. And hence each of these layers behave independently and cannot be combined together. To combine these hidden layers together, we shall have the same weights and bias for these hidden layers.We can now combines these layers together, that the weights and bias of all the hidden layers is the same. All these hidden layers can be rolled in together in a single recurrent layer.So it’s like supplying the input to the hidden layer. At all the time steps weights of the recurrent neuron would be the same since its a single neuron now. So a recurrent neuron stores the state of a previous input and combines with the current input thereby preserving some relationship of the current input with the previous input. Let’s take a simple task at first. Let’s take a character level RNN where we have a word “Hello”. So we provide the first 4 letters i.e. h,e,l,l and ask the network to predict the last letter i.e.’o’. So here the vocabulary of the task is just 4 letters {h,e,l,o}. In real case scenarios involving natural language processing, the vocabularies include the words in entire wikipedia database, or all the words in a language. Here for simplicity we have taken a very small set of vocabulary.Let’s see how the above structure be used to predict the fifth letter in the word “hello”. In the above structure, the blue RNN block, applies something called as a recurrence formula to the input vector and also its previous state. In this case, the letter “h” has nothing preceding it, let’s take the letter “e”. So at the time the letter “e” is supplied to the network, a recurrence formula is applied to the letter “e” and the previous state which is the letter “h”. These are known as various time steps of the input. So if at time t, the input is “e”, at time t-1, the input was “h”. The recurrence formula is applied to e and h both. and we get a new state.The formula for the current state can be written as –Here, Ht is the new state, ht-1 is the previous state while xt is the current input. We now have a state of the previous input instead of the input itself, because the input neuron would have applied the transformations on our previous input. So each successive input is called as a time step.In this case we have four inputs to be given to the network, during a recurrence formula, the same function and the same weights are applied to the network at each time step.Taking the simplest form of a recurrent neural network, let’s say that the activation function is tanh, the weight at the recurrent neuron is Whh and the weight at the input neuron is Wxh, we can write the equation for the state at time t as –The Recurrent neuron in this case is just taking the immediate previous state into consideration. For longer sequences the equation can involve multiple such states. Once the final state is calculated we can go on to produce the outputNow, once the current state is calculated we can calculate the output state as-Let me summarize the steps in a recurrent neuron for you-Let’s take a look of how we can calculate these states in Excel and get the output. Let’s take a look at the inputs first –The inputs are one hot encoded. Our entire vocabulary is {h,e,l,o} and hence we can easily one hot encode the inputs.Now the input neuron would transform the input to the hidden state using the weight wxh. We have randomly initialized the weights as a 3*4 matrix –Step 1:Now for the letter “h”, for the the hidden state we would need Wxh*Xt. By matrix multiplication, we get it as –Step 2:Now moving to the recurrent neuron, we have Whh as the weight which is a 1*1 matrix as  and the bias which is also a 1*1 matrix as For the letter “h”, the previous state is [0,0,0] since there is no letter prior to it.So to calculate ->  (whh*ht-1+bias)Step 3:Now we can get the current state as –Since for h, there is no previous hidden state we apply the tanh function to this output and get the current state –Step 4:Now we go on to the next state. “e” is now supplied to the network. The processed output of ht, now becomes ht-1, while the one hot encoded e, is xt. Let’s now calculate the current state ht.Whh*ht-1 +bias will be –Wxh*xt will be –Step 5:Now calculating ht for the letter “e”,Now this would become ht-1 for the next state and the recurrent neuron would use this along with the new character to predict the next one.Step 6:At each state, the recurrent neural network would produce the output as well. Let’s calculate yt for the letter e.Step 7:The probability for a particular letter from the vocabulary can be calculated by applying the softmax function. so we shall have softmax(yt)If we convert these probabilities to understand the prediction, we see that the model says that the letter after “e” should be h, since the highest probability is for the letter “h”. Does this mean we have done something wrong? No, so here we have hardly trained the network. We have just shown it two letters. So it pretty much hasn’t learnt anything yet.Now the next BIG question that faces us is how does Back propagation work in case of a Recurrent Neural Network. How are the weights updated while there is a feedback loop? To imagine how weights would be updated in case of a recurrent neural network, might be a bit of a challenge. So to understand and visualize the back propagation, let’s unroll the network at all the time steps. In an RNN we may or may not have outputs at each time step.In case of a forward propagation, the inputs enter and move forward at each time step. In case of a backward propagation in this case, we are figuratively going back in time to change the weights, hence we call it the Back propagation through time(BPTT).In case of an RNN, if yt is the predicted value ȳt is the actual value, the error is calculated as a cross entropy loss –Et(ȳt,yt) = – ȳt log(yt)E(ȳ,y) = – ∑ ȳt log(yt)We typically treat the full sequence (word) as one training example, so the total error is just the sum of the errors at each time step (character). The weights as we can see are the same at each time step. Let’s summarize the steps for backpropagation The unrolled network looks much like a regular neural network. And the back propagation algorithm is similar to a regular neural network, just that we combine the gradients of the error for all time steps. Now what do you think might happen, if there are 100s of time steps. This would basically take really long for the network to converge since after unrolling the network becomes really huge.In case you do not wish to deep dive into the math of backpropagation, all you need to understand is that back propagation through time works similar as it does in a regular neural network once you unroll the recurrent neuron in your network. However, I shall be coming up with a detailed article on Recurrent Neural networks with scratch with would have the detailed mathematics of the backpropagation algorithm in a recurrent neural network.Let’s use Recurrent Neural networks to predict the sentiment of various tweets. We would like to predict the tweets as positive or negative. You can download the dataset here.We have around 1600000 tweets to train our network. If you’re not familiar with the basics of NLP, I would strongly urge you to go through this article. We also have another detailed article on word embedding which would also be helpful for you to understand word embeddings in detail.Let’s now use RNNs to classify various tweets as positive or negative.If  you would run this model, it may not provide you with the best results since this is an extremely simple architecture and quite a shallow network.  I would strongly urge you to play with the architecture of the network to obtain better results. Also, there are multiple approaches to how to preprocess your data. Preprocessing shall completely depend on the task at hand. RNNs work upon the fact that the result of an information is dependent on its previous state or previous n time steps. Regular RNNs might have a difficulty in learning long range dependencies. For instance if we have a sentence like “The man who ate my pizza has purple hair”. In this case, the description purple hair is for the man and not the pizza. So this is a long dependency.If we backpropagate the error in this case, we would need to apply the chain rule. To calculate the error after the third time step with respect to the first one –∂E/∂W = ∂E/∂y3 *∂y3/∂h3 *∂h3/∂y2 *∂y2/∂h1 .. and there is a long dependency.Here we apply the chain rule and if any one of the gradients approached 0, all the gradients would rush to zero exponentially fast due to the multiplication. Such states would no longer help the network to learn anything. This is known as the vanishing gradient problem.Vanishing gradient problem is far more threatening as compared to the exploding gradient problem, where the gradients become very very large due to a single or multiple gradient values becoming very high.The reason why Vanishing gradient problem is more concerning is that an exploding gradient problem can be easily solved by clipping the gradients at a predefined threshold value. Fortunately there are ways to handle vanishing gradient problem as well. There are architectures like the LSTM(Long Short term memory) and the GRU(Gated Recurrent Units) which can be used to deal with the vanishing gradient problem. As we saw, RNNs suffer from vanishing gradient problems when we ask them to handle long term dependencies. They also become severely difficult to train as the number of parameters become extremely large. If we unroll the network, it becomes so huge that its convergence is a challenge.Long Short Term Memory networks – usually called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber. They work tremendously well on a large variety of problems, and are now widely used. LSTMs also have this chain like structure, but the repeating module has a slightly different structure. Instead of having a single neural network layer, there are multiple layers, interacting in a very special way. They have an input gate, a forget gate and an output gate. We shall be coming up with detailed article on LSTMs soon.Another efficient RNN architecture is the Gated Recurrent Units i.e. the GRUs. They are a variant of LSTMs but are simpler in their structure and are easier to train. Their success is primarily due to the gating network signals that control how the present input and previous memory are used, to update the current activation and produce the current state. These gates have their own sets of weights that are adaptively updated in the learning phase. We have just two gates here, the reset an the update gate. Stay tuned for more detailed articles on GRUs. I hope that this article would have given you a head start with the Recurrent Neural Networks.  In the upcoming articles we shall deep dive into the complex mathematics of Recurrent Neural Networks along with the detailed descriptions of LSTMs and GRUs. Try playing with the architecture of these RNNs and be amazed by their performance and applications. Do share your findings and approach in the comments section.",https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
Introduction to Altair – A Declarative Visualization Library in Python,Learn everything about Analytics|Introduction|Table of Contents|1. Overview of Altair|2. Exploring a Real World Problem|3. Pros and Cons|4. End Notes,"1.1 What is Altair?|1.2 Installation|2.1 Understanding the dataset|2.2 Histograms|2.3 Scatterplot|2.4 Stacked Bar Plot|2.5 Line Chart|2.6 Heatmap|Share this:|Like this:|Related Articles|Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks|Heart Sound Segmentation using Deep Learning – A doctor in making?|
Shubham Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Visualization is one of the most exciting parts of data science. Plotting huge amounts of data to unveil underlying relationships has its own fun.Whether you’re identifying relationships between features or simply understanding the working of a model, visualizations are usually the best way to go about it. Visualizations also help explain your work to your customers and stakeholders.Python provides a lot of libraries, specifically for plotting and visualization and I usually have a tough time picking out which one to use for my problem statement.I recently come across with Altair, a visualization library in Python and I was amazed by its capabilities. It is a very user-friendly library which actually performs a lot of things with the minimal amount of code.Please note that Altair is still in development phase, so things might change over time. We can still do a lot of exciting work on it and the future potential really excites me – hence this article!So, let’s get started.  Altair is a declarative statistical visualization library in Python, based on  Vega-lite  .I am sure many of you would be asking these two questions by now:By declarative, we mean that while plotting any chart, you only need to declare links between data columns to the encoding channels, such as x-axis, y-axis, colour, etc. and rest all of the plot details are handled automatically. Let’s understand it by an example.Take a look at the plot below and above that is the code required for generating that plot in Altair.So, if you notice, we just mentioned x,y and color and rest of the things like legend, axis labels, range etc. all are automatically set.Now, take a look at the image below which is the matplotlib implementation of the same visualization we previously did with Altair. Here, we need to explicitly use groupby function, define axis names, legends etc. which becomes a lot of extra work while you are doing an exploratory analysis.Therefore being declarative makes Altair simple, friendly and consistent. It produces beautiful and effective visualizations with the minimal amount of code. Therefore you can spend more time understanding your data rather than spending your time on setting the legends, defining axes and so on.I suppose by now you must have got the answer to the second question also. Altair can be installed via conda as follows:Or we can also install it via pip with the following:Great, now you have Altair installed on your system. I always feel that the best way to learn a library is to practice it on a real-life problem. So, without any further delay, let us quickly begin! For this purpose, I am using The Big Mart Sales dataset. Download the training file and load it into your working environment.In the data set, we have product wise Sales for Multiple outlets of a chain.  Let’s take a look at the dataset. The total data frame consists of 12 columns, out of which 7 are categorical and rest are numerical. Here, we have Item_Outlet_Sales as the target feature.NOTE: I have only used the first 1000 rows, just to provide you with the sense of different plots using Altair. During the exploration journey, we generally start with the univariate analysis using histograms.So, let’s start with the Item_Outlet_Sales feature, by plotting its histogram using Altair.To understand the code, let’s understand some basic terminologies, to begin with.Here, we have shown bar mark in order to visualization our data points as a bar chart.Above defined are the common parameters which are used in every Altair visualization. Let’s take a look at some specific attributes which are used in the above code.That was seriously very difficult to grasp after reading it for the first time. But, surely we will get hands-on it by the end of this article.Okay, now if you look at the above chart, the label for x-axis is not defined properly by the Altair, and we wish to change it. You can do this by defining another parameter called axis.It comes out to be a right-skewed distribution for the sales in our dataset. If I wish to do the same visualization using matplotlib, the implementation is shown below.You may consider the above code simple, but the visualization obtained from Altair is far more appealing than the one obtained from matplotlib.We can also change the color of the histogram in Altair by simply adding a color attribute. Now, let’s try to do some bivariate analysis using Altair.Now, let us look at the relationship between MRP and Sales features.We can clearly see a linear relationship between MRP and sales in the above plot. Let’s introduce some other encoding of Altair in order to draw more inferences from the visualizations.Here, we have defined another encoding called color, which is used to differentiate the data points and helps us to understand the relationship better.We can see that the sales for the grocery stores are pretty low as compared to sales in supermarkets, where supermarket type3 showing the highest sales.To draw more information from the above plot, let’s add another encoding called row.So, we can now notice that items in supermarket type 1 have comparatively more weight than the grocery store and supermarket type 2.These were different variates of scatter plot that can be drawn using the Altair. Let’s now look at some barplot charts to do some bivariate analysis. For this, let’s look at the relationship between Outlet_location_type and size of the outlet.Above drawn is the stacked bar plot, which denotes that the outlets in tier 2 cities only have outlets of small size, while only tier 3 cities have outlets which are high in size.Notice that here we have defined the count aggregator in the X encoding, in order to produce a horizontal stack chart. Now, let us look at the price of the item across the years. Generally, for the time series related visualizations, we prefer the use of line charts which is provided by Altair by using the line mark.Altair supports date and discretization of dates when setting the type of it as temporal (T). Notice: Here, we have separately defined our transformation using the aggregate attribute. Altair provides another type of chart known as heat map, which uses text mark attribute.Similarly, you can draw more patterns in data by this and you can also try some other data transformations and plots to find various trends and relationships between features.Note that, Altair is currently in the development phase and sooner it will include more and better visualizations in the coming time. Pros:Cons: I hope that you had a fun time learning this new library where you can do a lot of things with the minimal amount of code. So, try using it yourself on the dataset you are working on, and share your experience or doubts in the comment section below.I would also recommend you to once the visit the gallery page of the Altair documentation for getting more idea about the type of visualizations possible using Altair.Happy Exploring!",https://www.analyticsvidhya.com/blog/2017/12/introduction-to-altair-a-declarative-visualization-in-python/
Heart Sound Segmentation using Deep Learning – A doctor in making?,Learn everything about Analytics|Introduction|Table of Contents|What is a Segmentation problem (in general)?|Supervised Segmentation approach|Understanding our problem – What do you mean by “Heart Sound”?|Implementation of Heart Sound Segmentation|End Notes,"Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Introduction to Altair – A Declarative Visualization Library in Python|Introductory guide to Information Retrieval using kNN and KDTree|
Faizan Shaikh
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The idea of doing a project on heart sound segmentation came from a recent breakthrough I heard over the internet. One of the influencers I follow – Andrew Ng published a research paper a while back – which essentially is a state-of-the-art method for detecting heart disease.Heart disease? ML diagnoses from ECG better than cardiologist! @awnihannun @pranavrajpurkar @iRhythmTech @willknight https://t.co/bZAm8Y5Z09— Andrew Ng (@AndrewYNg) July 7, 2017It was an intriguing idea for me, so I went through all the materials published on the subject to understand what was the original idea behind.To keep the story short, the authors said that they used deep learning – a technique which has been in the news for a while now, for extracting the patterns that experts used to identify a diseased patient. This algorithm after training, became so good at the task that the authors claim to surpass even seasoned doctors. This idea influenced me that even I – albeit small – could have an impact on the substantial advancements that these researchers are having!This article focuses on audio segmentation problem in ECG signals and how we leverage deep learning to solve the task. I will first discuss a bit about segmentation problem in general and then show you the ways that can be used to solve the problem. I will also discuss what “heart sound” is and then show you an implementation of heart sound segmentation.So let’s get on with it!Note: This article assumes that you have a basic knowledge of audio data analysis. If you want to brush up the concepts – you can go through the article  Before we dive into heart sound segmentation, let us go back and understand what segmentation problem entails. Segmentation literally means dividing a particular object into parts (or segments) based on a defined set of characteristics. This object can be anything – ranging from concrete things like a frame of an image or an audio signal, to abstract objects like market or consumers.You may ask, why would you segment objects? The answer is simple – if you break down an object, it becomes an easier task extract information from it. For example in Customer Management, working with averages never reveals actionable insights until broken down in segments. As mentioned in the article, this is an example of customer segmentation of credit card usage on the basis of their age. Now that you know Segmentation as a problem, let us understand the approaches to solve a segmentation problem.Segmentation, specially for audio data analysis, is an important pre-processing step. This is because you can segment a noisy and lengthy audio signal into short homogeneous segments, which are handy short sequences of audio used for further processing. Now to solve a segmentation problem, you can either do it directly using unsupervised methods or convert it into a supervised problem and then group it according to its class.To explain this more intuitively, lets take an example of Image Segmentation task.Suppose you have an image of a cat in a field as we can see below. What you want is to divide the image into chunks – so that one individual object can be separately identified from the other. You can do this in two waysAlthough both the approaches has its pros and cons, the decision to start out with either of the approach will depend upon how hard it is to get training examples to go on with the supervised approach. Without wasting any time, let us jump on to what our actual problem is and try to solve it. Quoting the challenge page itself,According to the World Health Organisation, cardiovascular diseases (CVDs) are the number one cause of death globally: more people die annually from CVDs than from any other cause. An estimated 17.1 million people died from CVDs in 2004, representing 29% of all global deaths. Of these deaths, an estimated 7.2 million were due to coronary heart disease. Any method which can help to detect signs of heart disease could therefore have a significant impact on world health. This challenge is to produce methods to do exactly that.The task in the challenge is to find a method that can locate sounds particular to a heart (aka lub & dub, which are technically called S1 and S2) within audio data and then segment the audio files on the basis of these sounds. After segmenting the sounds, the challenge then asks us to produce a method that can classify heartbeat into normal and diseased categories. For the purpose of this article, we will take up only the first task of the challenge, i.e. to segment heart audio.
To give you a practical glimpse, this is how the heart sounds likeA normal heart sound has a clear “lub dub, lub dub” pattern, with the time from “lub” to “dub” shorter than the time from “dub” to the next “lub” (when the heart rate is less than 140 beats per minute). A temporal description of “lub” and “dub” locations over time in the following illustration:
  The very basic step you need to do whenever you start up on a problem is to understand the data and go through it record by record. Let us start with this:Note: you can download the required dataset from this webpage. Only download Dataset A of challenge 1 (Atraining_normal.zip and Atraining_normal_seg.csv)We see that there are cycles of heartbeat, with a higher intensity sound followed by a lower intensity sound.from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPool1D
from keras.models import Sequentialmodel = Sequential()model.add(InputLayer(input_shape=data_x.shape[1:]))model.add(Conv1D(filters=50, kernel_size=10, activation=’relu’))
model.add(MaxPool1D(strides=8))
model.add(Conv1D(filters=50, kernel_size=10, activation=’relu’))
model.add(MaxPool1D(strides=8))
model.add(Flatten())
model.add(Dense(units=1, activation=’softmax’))model.compile(optimizer=’adam’, loss=’binary_crossentropy’, metrics=[‘accuracy’])Our model will have this type of architectureWe are restricting the training for only 1 epoch here. But you can increase this to make your model perform better.And voila! You have a trained model which can be used to perform segmentation task. Now to get the durations where you should segment a heartbeat, just divide your raw test file into multiple parts and get the top prediction out of it. The model would give a prediction like this  I hope this article gave you a glimpse of how advancements in audio analysis can help us creating amazing technologies that can change our lives. The possibilities it opens up for humans can be huge.I have specially included an implementation of the technique so that you can use it to try it out locally. If you find the article helpful or have any suggestions, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/11/heart-sound-segmentation-deep-learning/
Introductory guide to Information Retrieval using kNN and KDTree,Learn everything about Analytics|Introduction|Table of Contents|What is Information Retrieval?|Where is Information Retrieval used?|KNN for Information Retrieval|Improvement over KNN: KD Trees for Information Retrieval||Intuitive Explanation of KD Trees|Implementation of KD Trees||Advantages and Disadvantages of KD trees|End Notes,"Use Case 1: Digital Library|Use Case 2: Search Engine|Use Case 3: Image retrieval|Advantages of KD Trees|Disadvantages of KD Tree|Share this:|Like this:|Related Articles|Heart Sound Segmentation using Deep Learning – A doctor in making?|FlashText – A library faster than Regular Expressions for NLP tasks|
Gurchetan Singh
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor  
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I love cricket as much as I love data science. A few years back (on 16 November 2013 to be precise), my favorite cricketer – Sachin Tendulkar retired from International Cricket. I spent that entire day reading articles and blogs about him on the web.By the end of the day, I had read close to 50 articles about him. Interestingly, while I was reading these articles – none of the websites suggested me articles outside of Sachin or cricket. Was it a co-incidence? Surely not.I was being suggested the next article based on what I was currently reading. The technique behind this process is known as “Information Retrieval”.In this article, I would take you through the basics of Information Retrieval and two common algorithms used to implement it, KNN and KD Tree. By end of this article, you will be able to create your own information retrieval systems, which can be implemented in any digital library / search.Let’s get going!  In the past few decades, the availability of cheap and effective storage devices and information systems has prompted the rapid growth of graphical and textual databases. Information collection and storage efforts have become easier, but the effort required to retrieve relevant information has become significantly greater, especially in large-scale databases. This situation is critical for textual databases, with so much of textual information around us – for instance business applications (e.g., manuals, newsletters, and electronic data interchanges), scientific applications (e.g., electronic community systems and scientific databases) etc.SourceTo aid the users to access these databases and extract the relevant knowledge or documents, Information Retrieval is used.Information Retrieval (IR) is the process by which a collection of data is represented, stored, and searched for the purpose of knowledge discovery as a response to a user request (query). This process involves various stages initiating with representing data and ending with returning relevant information to the user. The intermediate stage includes filtering, searching, matching and ranking operations. The main goal of information retrieval system (IRS) is to “find the relevant information or a document that satisfies user’s information needs” A digital library is a library in which collection of data are stored in digital formats and accessible by computers. The digital content may be stored locally, or accessed remotely via computer networks. A digital library is a type of information retrieval system.A search engine is one of the most the practical applications of information retrieval techniques to large scale text collections.An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images.Very famous example of image retrieval system is https://reverse.photos/ which uses image as the search query and returns similar images.Source One of the most common algorithms that most of the Data scientists use for retrieval of information is KNN. K Nearest Neighbour (KNN) is one of the simplest algorithms that calculates the distance between the query observation and each data point in the train dataset and finds the K closest observations.When we use Nearest neighbour search algorithm, it compares all the data points with the mentioned query point and finds the closest points.There are many ways in which we can find the distance between two data points. Most commonly used distance metrics are “Euclidean distance” and “Hamming distance”.This research paper focuses on them.Imagine a situation where you have thousands of queries, and every time the algorithm compares the query point with all the data points. Isn’t it very computationally intensive?Also, greater the data points, higher will be the amount of computation needed. Obviously!SourceSo, KNN  search has O(N) time complexity for each query where N= Number of data points. For KNN with K neighbor search, the time complexity will be O(log(K)*N) only if we maintain a priority queue to return the closest K observations. You can read more about KNN here.So, for a dataset with millions of rows and thousands of queries, KNN seems to be computationally very demanding. So is there any alternative to KNN which uses similar approach but can be time efficient also?KD Tree is one such algorithm which uses a mixture of Decision trees and KNN to calculate the nearest neighbour(s). KD-trees are a specific data structure for efficiently representing our data. In particular, KD-trees helps organize and partition the data points based on specific conditions.Let’s say we have a data set with 2 input features. We can represent our data as- Now, we’re going to be making some axis aligned cuts, and maintaining lists of points that fall into each one of these different bins.   SourceAnd what this structure allows us to do as we’re going to show, is efficiently prune our search space so that we don’t have to visit every single data point.Now the question arises of how to draw these cuts?Then a question is when do you stop?There are a couple of choices that we have.So again, this second criteria would ignore the actual data in the box whereas the first one uses facts about the data to drive the stocking criterion. We can use the same distance metrics(“Euclidean distance” and “Hamming distance”) that we used while implementing KNN.Suppose we have a data set with only two features.Let’s split data into two groups.We do it by comparing x with mean of max and min value.Value = (Max + Min)/2= (0.96 + 0.11)/2= 0.53     At each node we will save 3 things.Similarly dividing the structure into more parts on the basis of alternate dimensions until we get maximum 2 data points in a Node. So now we plotted the points and divided them into various groups.Let’s say now we have a query point ‘Q’ to which we have to find the nearest neighbor. Using the tree we made earlier, we traverse through it to find the correct Node. When using Node 3 to find the Nearest Neighbor.But we can easily see, that it is in fact not the Nearest neighbor to the Query point.We now traverse one level up, to Node 1. We do this because the nearest neighbor may not necessarily fall into the same node as the query point.Do we need to inspect all remaining data points in Node 1 ?We can check this by checking if the tightest box containing all the points of Node 4 is closer than the current near point or not.This time, the bounding box for Node 4 lies within the circle, indicating that Node 4 may contain a point that’s closer to the query.When we calculate the distance of the points within the Node 4 and previous closest point with the query point, we find that point lying above the query point is actually the nearest neighbor within the given points.We now traverse one level up, to Root.Do we need to inspect all remaining data points in Node 2 ?We can check this by checking if the tightest box containing all the points of Node 2 is closer than the current Near point or not.We can see that the Tightest box is far from the current Nearest point. Hence, we can prune that part of the tree. Since we’ve traversed the whole tree, we are done: data point marked is indeed the true nearest neighbour of the query. For the implementation of KD Tree, we will use the most common form of IR ie Document Retrieval. Based on the current document, document retrieval returns the most similar document(s) to the user.We will use the dataset which consists of articles on famous personalities. We would implement KD Tree to help us retrieve articles most similar to that of the “Barack Obama”.You can download the dataset in the form of csv from here.source  Hence we can see that articles of Bill Clinton and Donald Flower who share the same field of politics as Barack Obama are similar.So now we look at the advantages of KD Tree : Well, KD-trees are really cool. They’re a very intuitive way to think about storing data, and as we saw, they could lead to help us find relevant information way sooner.But there are few issues.Source KD Tree can prove to be a better Retrieval algorithm on a specific Dataset that matches its condition. Though there are more models such as Locality Sensitive Hashing which can overcome its limitations. We shall explore them as well in the upcoming articles.Did you find the article useful? Do you plan to use KD Tree or LSH in near future in Python or R? If yes, share with us how you plan to go about it.",https://www.analyticsvidhya.com/blog/2017/11/information-retrieval-using-kdtree/
FlashText – A library faster than Regular Expressions for NLP tasks|Introduction,Learn everything about Analytics|Table of Contents|1. What is FlashText?|2. How is FlashText so fast?|3. When do you use FlashText?|4. Installing FlashText|5. Searching for words in a text document||6. Replacing words in a text document|7. End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Introductory guide to Information Retrieval using kNN and KDTree|A Step Towards Reproducible Data Science : Docker for Data Science Workflows|
NSS
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"People like me working in the field of Natural Language Processing almost always come across the task of replacing words in a text. The reasons behind replacing the words may be different. Some of them are.Now, if the number of words to replace and the corpus of text is not huge i.e within thousands, then Regular Expressions have always been my solution. But as I started working on bigger and bigger datasets with tens of thousands of documents and sometimes millions, I noticed that performing the above tasks started taking days. In today’s fast-moving world, this is not the amount of time one would want to invest in a very simple but important task. So earlier, it would come down to optimizing the number of words necessary to be changed and time required to replace these words.But in the early November, I found FlashText – a super blazingly fast library that reduces days of replacement computation time to minutes.  FlashText is a Python library created specifically for the purpose of searching and replacing words in a document. Now, the way FlashText works is that it requires a word or a list of words and a string. The words which FlashText calls keywords are then searched or replaced in the string.Let us check out in detail about FlashText working. When keywords are passed to FlashText for searching or replacing, they are stored as a Trie Data Structure which is very efficient at Retrieval tasks. Below is an example of how a Trie Data Structure looks like. Above is a Trie Data Structure for the words (their, there, answer, any, and bye).Now, in case of searching the keywords, FlashText will return the keywords that are present in the string. In case of replacing, FlashText will create a new string with the keywords replaced. Both these operations happen over a single pass. Now it is important to understand the concept of the single pass.An example of single pass replacement looks like this-String = “spamham sha”
Replace “spam” with “eggs” and “sha” with “md5”Now let’s see how does the String look like with and without a single pass.Single-pass
String = “eggsham md5”Without Single-pass
String = “eggmd5m md5”Above, you can see the difference between the single pass and without the single pass.Now, we have an overview of how FlashText works. It is imperative that we understand – What is that FlashText has which Regular Expressions don’t? After all, Regular Expressions are mostly considered the one-stop solution for string manipulation in terms of both speed and variety of manipulations that can be done. This can be better understood with the help of an example by the author of the FlashText package himself.Suppose, there is a string called sample = ” This is a sample sentence” and a collection of words called keywords = {sample, sameer, pony, time}. Now, if we are to perform searching of the keywords in the sample,  there are 2 ways of doing it.Method 1:for each word in keywords:                                                                 Line 1
check if word exists in sample                                                        Line 2Now the above method has a loop which will run n times, where n is the number of words in the keywords.  Also, there will be significant time consumption in Line 2 which checks whether a particular word is present in a string or not.Method 2:for each word in sample:                                                                     Line 1
check if word exists in the keywords dictionary                            Line 2Now, the loop in this method will run m times, where m is the number of words in the sample. The major advantage is the execution time of Line 2. Checking a key in a dictionary is a significantly faster process than checking for a word in a string.FlashText uses Method 2 for faster searching and replacing and is inspired by the Aho-Corasick Algorithm and Trie Data Structure. Let’s have a look at the way it works:First, a Trie dictionary is created from the keywords which looks like below.Start and EOT  represent the word boundaries such as space, new_line etc. The idea is that words which have word boundaries on both ends should only match. So, pam will not match to pamella.  Now let’s see how we can perform searching,Since we have our keywords dictionary with us. We pass in the input string sample. Now, the comparison will look like below.Is <start>This<EOT>in the keywords dictionary?                                       No
Is <start>is<EOT>in the keywords dictionary?                                            No
Is <start>a<EOT>in the keywords dictionary?                                            No
Is <start>sample<EOT>in the keywords dictionary?                                  Yes
Is <start>sentence<EOT>in the keywords dictionary?                               NoVisually it looks like below.An important thing to keep in mind is that checking for a word in the dictionary happens at a character level. So for example, while checking for the word “is”, it will search for <start>”i”. Since “i” does not exist anywhere after <start> in the Keywords trie dictionary, therefore the entire word is skipped. This character level matching and skipping are what gives FlashText its speed. Pretty much every time since it is super fast. Below is the image of the benchmarking done by FlashText for searching and replacing against regular expressions.SearchingLooking at the image, it looks like when the number of keywords to be searched is below 500 then, regular expressions provide a little edge over FlashText. But as soon as number of keywords cross 500, FlashText surpasses regular expression performance by a wide margin.Conclusion: Use FlashText when you have to search for a large number of keywords, preferably more than 500.ReplacingWhile Regular Expressions presented stiff competition to FlashText in the sub 500 keyword domain for searching, when it comes to replacing FlashText beats Regular Expressions hands down. While there is a linear increase in time as the number of keywords increase, FlashText stays constant.Conclusion: FlashText is any day better than regular expressions for replacing keywords in a document.These are the codes for benchmarking of searching and replacing by the author.There are still a few caveats of using FlashText. As of now, FlashText does not support partial word matching or special characters matching. For that, Regular Expressions are the best. Installing FlashText is as easy as any other package. In command prompt/ terminal, type:pip install flashtext Below is the code to find keywords in a document.from flashtext import KeywordProcessor
document = """"""Batman is a fictional superhero appearing in American comic books published by DC Comics. The character was created by artist Bob Kane and writer Bill Finger,[4][5] and first appeared in Detective Comics #27 (1939). Originally named the ""Bat-Man"", the character is also referred to by such epithets as the Caped Crusader, the Dark Knight, and the World's Greatest Detective.[6]Batman's secret identity is Bruce Wayne, a wealthy American playboy, philanthropist, and owner of Wayne Enterprises. After witnessing the murder of his parents Dr. Thomas Wayne and Martha Wayne as a child, he swore vengeance against criminals, an oath tempered by a sense of justice. Bruce Wayne trains himself physically and intellectually and crafts a bat-inspired persona to fight crime.[7]Batman operates in the fictional Gotham City with assistance from various supporting characters, including his butler Alfred, police commissioner Gordon, and vigilante allies such as Robin. Unlike most superheroes, Batman does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. A large assortment of villains make up Batman's rogues gallery, including his archenemy, the Joker.The character became popular soon after his introduction in 1939 and gained his own comic book title, Batman, the following year. As the decades went on, differing interpretations of the character emerged. The late 1960s Batman television series used a camp aesthetic, which continued to be associated with the character for years after the show ended. Various creators worked to return the character to his dark roots, culminating in 1986 with The Dark Knight Returns by Frank Miller. The success of Warner Bros.' live-action Batman feature films have helped maintain the character's prominence in mainstream culture.[8]An American cultural icon, Batman has garnered enormous popularity and is among the most identifiable comic book characters. Batman has been licensed and adapted into a variety of media, from radio to television and film, and appears on various merchandise sold around the world, such as toys and video games. The character has also intrigued psychiatrists, with many trying to understand his psyche. In 2015, FanSided ranked Batman as number one on their list of ""50 Greatest Super Heroes In Comic Book History"".[9] Kevin Conroy, Bruce Greenwood, Peter Weller, Anthony Ruivivar, Jason O'Mara, and Will Arnett, among others, have provided the character's voice for animated adaptations. Batman has been depicted in both film and television by Lewis Wilson, Robert Lowery, Adam West, Michael Keaton, Val Kilmer, George Clooney, Christian Bale, and Ben Affleck. """"""# searching for a single word in the document
processor = KeywordProcessor()
processor.add_keyword('batman')
found = processor.extract_keywords(document)
print(found)output:# Replacing all occurences of word 'batman'(case insensitive) with Bruce Wayne
processor = KeywordProcessor(case_sensitive = False)
processor.add_keywords('batman','Bruce Wayne')
found = processor.replace_keywords(document)
print(found)output:‘Bruce Wayne is a fictional superhero appearing in American comic books published by DC Comics. The character was created by artist Bob Kane and writer Bill Finger,[4][5] and first appeared in Detective Comics #27 (1939). Originally named the “Bat-Man”, the character is also referred to by such epithets as the Caped Crusader, the Dark Knight, and the World\’s Greatest Detective.[6]\n\nBruce Wayne\’s secret identity is Bruce Wayne, a wealthy American playboy, philanthropist, and owner of Wayne Enterprises. After witnessing the murder of his parents Dr. Thomas Wayne and Martha Wayne as a child, he swore vengeance against criminals, an oath tempered by a sense of justice. Bruce Wayne trains himself physically and intellectually and crafts a bat-inspired persona to fight crime.[7]\n\nBruce Wayne operates in the fictional Gotham City with assistance from various supporting characters, including his butler Alfred, police commissioner Gordon, and vigilante allies such as Robin. Unlike most superheroes, Bruce Wayne does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. A large assortment of villains make up Bruce Wayne\’s rogues gallery, including his archenemy, the Joker.\n\nThe character became popular soon after his introduction in 1939 and gained his own comic book title, Bruce Wayne, the following year. As the decades went on, differing interpretations of the character emerged. The late 1960s Bruce Wayne television series used a camp aesthetic, which continued to be associated with the character for years after the show ended. Various creators worked to return the character to his dark roots, culminating in 1986 with The Dark Knight Returns by Frank Miller. The success of Warner Bros.\’ live-action Bruce Wayne feature films have helped maintain the character\’s prominence in mainstream culture.[8]\n\nAn American cultural icon, Bruce Wayne has garnered enormous popularity and is among the most identifiable comic book characters. Bruce Wayne has been licensed and adapted into a variety of media, from radio to television and film, and appears on various merchandise sold around the world, such as toys and video games. The character has also intrigued psychiatrists, with many trying to understand his psyche. In 2015, FanSided ranked Bruce Wayne as number one on their list of “50 Greatest Super Heroes In Comic Book History”.[9] Kevin Conroy, Bruce Greenwood, Peter Weller, Anthony Ruivivar, Jason O\’Mara, and Will Arnett, among others, have provided the character\’s voice for animated adaptations. Bruce Wayne has been depicted in both film and television by Lewis Wilson, Robert Lowery, Adam West, Michael Keaton, Val Kilmer, George Clooney, Christian Bale, an’So this was all about FlashText – an efficient library for searching and replacing of keywords in millions of document. If you are into the NLP field and it is your day to day job of dealing with this kind of problem of text cleaning and modification then, I would really suggest you try the library once. The source for the library is present here for any reference.If you try out this amazing library, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/11/flashtext-a-library-faster-than-regular-expressions/
A Step Towards Reproducible Data Science : Docker for Data Science Workflows,Learn everything about Analytics|Introduction|Table of Contents|1. What is Docker?|2. Use Cases for Data Science|3. Docker Terminology|4. Docker: Hello-World|5. Data Science tools without installation:|6. Your first Docker Image|7. Docker Eco-system|End Notes,"Distribution and setup of Data Science tools and software:|Sharing reproducible analysis and code via Docker Images:|Sharing Data Science applications directly without a dedicated DevOps team:|About the Author|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|FlashText – A library faster than Regular Expressions for NLP tasks|2 days to go for the DataHack Summit|
Guest Blog
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"My first encounter with Docker was not to solve a Data Science problem, but to install MySQL. Yes, to install MySQL! Quite an anti-climatic start, right? At times, you stumble upon jewels while going through StackOverflow and Docker was one of them. What started with a one-off use case, ended up becoming a useful tool in my daily workflow.I got a taste of docker when I tried to install TensorFlow in my system. Just to give you the context, TensorFlow is a deep learning library which requires a series of steps that you ought to do for system setup. Especially it is extremely complex to install Nvidia Graphics drivers. I literally had to reinstall my Operating system countless number of times. That loop stopped only when I shifted to docker, thankfully!Docker provides you with an easy way to share your working environments including libraries and drivers. This enables us to create reproducible data science workflows.This article aims to provide the perfect starting point to nudge you to use Docker for your Data Science workflows! I will cover both the useful aspects of Docker – namely, setting up your system without installing the tools and creating your own data science environment.  Docker is a software technology providing containers, promoted by the company Docker, Inc. Docker provides an additional layer of abstraction and automation of operating-system-level virtualization on Windows and Linux.The underlying concept that Docker promotes is usage of containers, which are essentially “boxes” of self-contained softwares. Containers have been in existence before Docker & quite successful, but 2015 saw a huge adoption by the software community in terms of containerization to solve the day-to-day issues. When you walk into a cubicle of Data Science folks, they are either doing data processing or struggling to setup something on their work-stations/laptops. Okay, that might be an exaggeration but you get the sense of helplessness. To give a small example, for someone to setup a Caffe environment there are more than 30 unique ways. And trust me, you’ll end up creating a new blogpost just for showing all the steps!Source: xkcd comicsYou get the idea. Anaconda distribution has made virtual environments and replicating environments using a standardized method a reality…yet things do get muddled and sometimes we miss the bullet points in the README file, carefully created to replicate those.To solve the above problem, bash scripts and makefiles are added which adds to more confusion. It becomes as simple as untangling earphones, phew!Docker’s learning curve might be a bit steep, but it helps to solve:The Caffe example we discussed is one of the pain points that everyone experiences in their Data Science journey. Not only Docker helps to set a consistent platform via which these tools can be shared, the time wasted in searching for operating system specific installers/libraries is eliminated.Along with sharing the tools (docker images as installers), we can share Jupyter notebooks or scripts along with their results baked inside a Docker image. All the other person/colleague needs to do is run the Docker image to find out what’s there!In my last article, we looked at how wrapping ML model in an API helps to make it available to your consumers. This is just one part of it. With small teams with no independent DevOps team to take care of deployments, Docker and the eco-system around it — docker-compose, docker-machine helps to ease the problems at a small scale.Sales guys needs to present an RShiny application but don’t want to run the code? Docker can help you with that! I’ve been going on about containers, containerization in the previous section. Let’s understand the Docker terminologies first.For starters, containers can be thought of as mini-VMs that are light-weight, disposable. Technically though, they are just processes (threads if you might say) that are created when you fire Docker commands in your terminal via their Command Line Interface (CLI).Docker also provides: images that are essentially snapshots of the containers whose running state is saved using Docker CLI or generated using Dockerfile.A Dockerfile can be considered as an automated setup file. This small file helps to create/modify Docker images. All the talk makes no sense, until there’s some proof. Let’s dive in and fire up your terminals.Think of the process of creating a Docker image as creating a layered cake. Dockerfile being your recipe, Docker image created out of various layers.In the next few sections, we’ll try to get a feel of Docker and work with its command line commands. Also, we’ll create our own Docker Image. This is all we need to execute a docker image. hello-world is simple, it has to be, but let’s move on to better things. Those that will help more, next section is all about that: Data Science tools without installation, our first use case. You have a clean laptop and you need to install TensorFlow in your system, but you are lazy (yes we all are sometimes). You want to procrastinate and not install things on your laptop, but you have Docker installed already as a standard company practice. Hmm, interesting times, you ponder!You go to Dockerhub and search for the official Docker image for TensorFlow. All you need to run on your terminal is: docker pull tensorflow/tensorflowAs discussed above (in Docker Terminology section), the tensorflow docker image is also a layered object that forms images. Once all the intermediate layers are downloaded, run: docker images to check whether our docker pull was successful.To run the image, run the command: docker run -it -p 8888:8888 tensorflow/tensorflowNow the above docker run command packs in a few more command line argurments. A few which you need to know better are as follows:Now since a docker container is created, you can visit: http://localhost:8889 where you can try out tensorflow.Wasn’t that easy? Now as a exercise, replace -it in the docker run command by -d. See whether you can get the tensorflow jupyter environment again or not?You should get the following outputs as in the screenshot below:Exercise: Create more containers with different ports using the docker run command and see how many get created. We as Data Science folks are picky about what tools we use for our analysis, some like to work with R while others prefer Python. Personally, I’d whine about the above TensorFlow image. I don’t know what’s there in it (unless I look at the source code i.e. the Dockerfile aka recipe). Tensorflow isn’t enough on it’s own, suppose you want to use OpenCV too and maybe scikit-learn & matplotlib.Let’s see how to create your own custom TensorFlow image! Docker provides a good support to build up from a prototype level scale to production levels. Purely from a deployments perspective: docker-machine, docker-compose & docker-swarm are components that help achieve that. Starting off with a new habit is a difficult task. But once the learning curve smoothens out, things start to work out and new ideas open up with the usage. It is the same with Docker, hoping that this primer makes you think about using it in your daily Data Science workflows. Comment down below, how do you plan to use Docker, starting today!Prathamesh Sarang works as a Data Scientist at Lemoxo Technologies. Data Engineering is his latest love, turned towards the *nix faction recently. Strong advocate of “Markdown for everyone”.",https://www.analyticsvidhya.com/blog/2017/11/reproducible-data-science-docker-for-data-science/
2 days to go for the DataHack Summit,Learn everything about Analytics|Introduction|What is the DataHack Summit?|What to expect @The DataHack Summit|Conference Talks|Hack Sessions|Workshops|Speed Networking|Focus Areas|Why DataHack Summit is the place to be this week !|End Notes,"Profiles|Participating Organisations|Share this:|Like this:|Related Articles|A Step Towards Reproducible Data Science : Docker for Data Science Workflows|Exclusive Interview – Six years of Analytics Education – what has changed, and what hasn’t?|
Dishashree Gupta
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"What do you think of when you hear the word “Summit” or “Conference”? You think, knowledgeable talks and theoretical buzz. Research and case studies. Reports and insights.A lot of conferences follow the same guidelines and eventually get redundant. As the times change, so should the way in which conferences are conducted.In a glorious attempt to redefine those two words and to eradicate redundancy; if Data Science & Machine Learning have always been your calling and you want to meet the biggest names of the industry who have redefined the data science-scape, we bring to you Data Hack Summit 2017 and it definitely is the place to be this week.Crucial to science education is hands-on involvement: showing, not just telling; real experiments and field trips and not just ‘virtual reality.’We at Analytics Vidhya feel passionately about enhancing data science culture in India. We want people to talk, showcase and learn data science. With the Datahack Summit, we aim to create a conference which leaves you in the company of several like minded passionate people.And not just meeting these people, we have 30+ talks, 10 Hack Sessions, 6 full day workshops and much more. We get you in touch with people with expertise in several domains involving Machine learning, Deep Learning, Artificial intelligence, Business Analytics, BioGenomics, Geo-spatial coding and various other topics. At the DataHack Summit, you don’t just hear people talk, you also witness multiple live coding sessions. You have 10 hack sessions where the speaker gives a live demo for you to understand their approach to various problems.Let’s go into bit more detail of what to expect in these 3 days!The Conference is not just aimed at having Data Scientists. We have people from various domains and Profiles, we have people from managers to professors, Analysts to CEOs. Let’s take a look at the distribution of the profiles of people you shall meet at the DATAHACK SUMMIT.DATAHACK SUMMIT marks the presence of 200+ organisations under a single roof. We shall have 800+ people from organisations like American Express, Fidelity Investments, Accenture, Shell, HP, Intel and many other organisations. You can take a look at the distribution of all these participating organisations.The first step towards learning about a specific topic would be to hear what it is all about. Who can do this better than the best in the field?Her’s a list of a few of our speakers. You can check the entire list here.             Our Keynote speaker, Dr. Kirk Borne is one of the most influential people in the field of Data Science. He worked 18 years supporting NASA projects in various roles, including Data Archive Project Scientist for the Hubble Space Telescope which was once informally dubbed as one of the greatest achievements of mankind. He has a PhD in Astronomy from Caltech and a BS in Physics from LSU.Another one of our famed speakers, Bharat Kaul, has been at Intel for 17+ years, a brand that most people associate very closely with. Bharat’s lab focuses on Application driven Architectural Performance leadership for Intel for multi-core/many architectures with special focus on Artificial Intelligence and High Performance Computing. The lab works in close collaboration with leading academic and industry partners with 30+ papers in last years in Tier-1 conferences.This is something unique we can up with. We don’t just want to audience to hear experts talk, we would also want the participants to see experts coding. The DataHack Summit marks 10, one hour hack sessions where experts would be doing a live problem solving in front of the audienceSome of the Hack Sessions include –You can see the entire list of the hack sessions here.Now that you’ve acquainted yourself with the latest advancements of the field and domain, how do you understand where and when to apply these?The 8 hour workshop sessions are specially curated for beginners and experts alike, where you’ll be taught about a certain topic and guided through hands on coding by experts as you get the most out of your learning. You will be taught by industry experts and Kaggle grandmasters to learn tips and tricks you never knew about!Workshops we would be having at the DataHack Summit:More than anything else the conference aims to summon all data scientists, CXOs, Directors, Business Consultants, Startup Founders and Architects under one roof and get them talking about what’s going on new in the industry.At this mini event, we give you time to exchange your cards, get familiar with as many people as you can. We will pair up consultants with consultees, trainers with trainees and most importantly, job seekers with employers!Following will be the areas of focus of discussions at the DataHack Summit 2017:Data Hack Summit 2017 is India’s biggest conference on Data science , ML and AI. Visit www.analyticsvidhya.com/datahacksummit to know more about the conference and how you can be a part of the learning extravaganza. We are extremely excited to organize the first ever largest AI/ML conference in India. See you at the DataHack Summit!",https://www.analyticsvidhya.com/blog/2017/11/2-days-to-go-for-the-datahack-summit/
"Exclusive Interview – Six years of Analytics Education – what has changed, and what hasn’t?",Learn everything about Analytics|,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|2 days to go for the DataHack Summit|A Guide To Conduct Analysis Using Non-Parametric Statistical Tests|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Analytics and Data Science is a fast evolving landscape. What was relevant few years back has become obsolete today. Over the last few years, Data Science and Analytics have progressively become part of the vocabulary of professionals and students in the country.We thought it’s a good time to see how this domain has evolved over the past few years – from the point of view of an early player in India’s Analytics training landscape. Praxis Business School was one of the pioneer in recognizing the need to create a pipeline of trained resources for this science.We spoke to Charanpreet Singh, Co-founder and Director, Praxis Business School Foundation, and Dr. Prithwis Mukerjee, Director, Business Analytics Program, Praxis Business School to examine how different does the analytics landscape looks today compared to 2011, when Praxis started its full-time program in this field. Analytics Vidhya (AV): What triggered your starting a full-time post graduate program in business analytics in November 2011, and what was the response to this move? How has the program progressed in the last 6 years?Charanpreet: We were in Bangalore, meeting companies for recruitment for our business management students. In two of our meetings, we were told by two completely unconnected people to start a course in analytics – they believed that there would be a great demand for trained professionals and Praxis had the capability to deliver a program in this area. We did our own research and decided to go ahead with the idea – as it would help us differentiate ourselves among the several thousand B-schools!The response we got was overwhelmingly meager – we started with a batch of 8 students. As the awareness of this domain increased, not the least due to the efforts of communities like yours, and as successive batches saw real jobs at the end of the program, the demand picked up. Today we have 2 intakes a year – January and July batches running out of Kolkata and Bangalore – and also run the analytics program at the International Institute of Digital Technologies. AV: How has the analytics landscape changed over the last 6 years from a Praxis point of view?Prithwis: There are changes across all stakeholders: the most obvious change of course is the number of people seeking to skill themselves in analytics. There are two important reasons for this:On the program front – technology has the nasty habit of changing just when you thought that you have mastered it. This is no surprise to anyone who has been at the cutting edge – as is the case for many of us at Praxis. But instead of being resigned to this ‘bleeding” edge we thrive on it by keeping the Praxis curriculum in “perpetual beta”.When we began in 2011, SAS was central to our curriculum and this was supplemented with domain knowledge from partners like ICICI Bank. Within a year or two we realised that the world was moving into the open source tools like R for analysis and Hadoop for managing “Big Data”, and altered our curriculum accordingly. In another two years with the advent of Spark and TensorFlow, Python has become the dominant platform because of its native affinity for these tools and so our curriculum was modified again – in fact in the middle of the 2016-17 session!There has been a continual change in the way the subjects are taught as well – there are more used cases available, more hackathons and problems in the public domain that we can get our students to work on. So, both from content and pedagogy perspectives, the analytics program at Praxis has undergone huge changes to remain contemporary and effective. AV: What about the demand side of things – the industry?Charanpreet: Three trends are clearly visible:At present, our placements are witnessing the dual advantage of two factors – repeat purchase by ‘loyal’ companies and a significant number of new companies joining the fray. I am sure this must be the case industry-wide. KJ: Now that we have a hold on the journey so far, what do you think lies ahead?Prithwis: There is no doubt that the future belongs to artificial intelligence – of the kind demonstrated in self driving cars and automatic face and voice recognition. All these are extensions of what the data scientist refers to as data mining or machine learning and there are two aspects to this change. Obviously, subjects like artificial neural networks, deep learning and cognitive learning will become increasingly important and we are introducing them into our curriculum.The other subtle change is that this same AI will be used to make data science simpler and easier to use. Managers will be able to use GUI driven tools to carry out data science tasks without having to learn about the algorithms and data-structures that support machine learning models. The challenge lies in being able to harmonise these two widely different scenarios in a manner that caters to the aspirations and expectations of all our future students. KJ: Finally, what would you say to an aspiring data scientist?Charanpreet: An aspiring data scientist has already made the right decision – so we encourage him whole-heartedly to join this exciting world. As the subject is sufficiently complex, my advice would be to invest adequate time in a process that strengthens your knowledge and skill levels and gives you a sound conceptual base from which you can chart your growth. Too many aspirants think they can juggle several things in life along with learning – our experience has been that even as a full-time pursuit fathoming this science is a challenge.My appeal to everyone who believes he/ she is a good problem solver, analytical and number friendly – analytics and data science is pretty much the best option you have for an interesting, remunerative and sustainable career – because data generation is poised to grow at unprecedented rates, and the requirement for people who can make sense out of this data is similarly poised for an explosion! With the fear of repeatable tasks in the IT workplace getting automated, you would likely be taking a big risk if you do not re-skill yourself. Thanks Charanpreet & Prithwis for spending your valuable time with us and sharing these thought with us. We wish you all the best for coming batches and hope to see you at the DataHack Summit 2017.",https://www.analyticsvidhya.com/blog/2017/11/six-years-of-analytics-education-what-changed/
A Guide To Conduct Analysis Using Non-Parametric Statistical Tests,Learn everything about Analytics|Introduction|Table of Contents|1. How are Non-Parametric tests different from Parametric tests?|2. When can I apply non-parametric tests?|3. Pros and Cons of using non-parametric test|4. Hypothesis testing with non-parametric tests|5. Non-Parametric Tests|End Notes,"Pros|Cons|1. The first step is to set up hypothesis and opt a level of significance|2. Set a test statistic|3. Set decision rule| 4. Calculate test statistic|5. Compare the test statistic to the decision rule|Mann Whitney U test|Wilcoxon Sign-Rank Test|Sign Test|Kruskal-Wallis Test|Spearman Rank Correlation|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Exclusive Interview – Six years of Analytics Education – what has changed, and what hasn’t?|A Comprehensive Tutorial to Learn Data Science with Julia from Scratch|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The average salary package of an economics honors graduate at Hansraj College during the end of the 1980s was around INR 1,000,000 p.a. The number is significantly higher than people graduating in early 80s or early 90s.What could be the reason for such a high average? Well, one of the highest paid Indian celebrity, Shahrukh Khan graduated from Hansraj College in 1988 where he was pursuing economics honors. This, and many such examples tell us that average is not a good indicator of the center of the data. It can be extremely influenced by Outliers. In such cases, looking at median is a better choice. It is a better indicator of the center of the data because half of the data lies below the median and the other half lies above it.So far, so good – I am sure you have seen people make this point earlier. The problem is no one tells you how to perform the analysis like hypothesis testing taking median into consideration. Statistical tests are used for making decisions. To perform analysis using median, we need to use non-parametric tests. Non-parametric tests are distribution independent tests whereas parametric tests assume that the data is normally distributed. It would not be wrong to say parametric tests are more infamous than non-parametric tests but the former does not take median into account while the latter makes use of median to conduct the analysis.Without wasting any more time, let’s dive into the world of non-parametric tests.Note: This article assumes that you have prerequisite knowledge of hypothesis testing, parametric tests, one-tailed & two-tailed tests.  If you read our articles on probability distributions and hypothesis testing, I am sure you know that there are several assumptions attached to each probability distribution. Parametric tests are used when the information about the population parameters is completely known whereas non-parametric tests are used when there is no or few information available about the population parameters. In simple words, parametric test assumes that the data is normally distributed. However, non-parametric tests make no assumptions about the distribution of data.But what are parameters? Parameters are nothing but characteristics of the population that can’t be changed. Let’s look at an example to understand this better.A teacher calculated average marks scored by the students of her class by using the formula shown below:
Look at the formula given above, the teacher has considered the marks of all the students while calculating total marks. Assuming that the marking of students is done accurately and there are no missing scores, can you change the total marks scored by the students? No. Therefore, average marks is called a parameter of the population since it cannot be changed.  Let’s look at some examples.1. A winner of the race is decided by the rank and rank is allotted on the basis of crossing the finish line. Now, the first person to cross the finish line is ranked 1, the second person to cross the finish line is ranked 2 and so on. We don’t know by what distance the winner beat the other person so the difference is not known. 2. A sample of 20 people followed a course of treatment and their symptoms were noted by conducting a survey. The patient was asked to choose among the 5 categories after following the course of treatment. The survey looked somewhat like thisNow, if you look carefully the values in the above survey aren’t scalable, it is based on the experience of the patient. Also, the ranks are allocated and not calculated. In such cases, parametric tests become invalid.For a nominal data, there does not exist any parametric test. 3. Limit of detection is the lowest quantity of a substance that can be detected with a given analytical method but not necessarily quantitated as an exact value. For instance, a viral load is the amount of HIV in your blood. A viral load can either be beyond the limit of detection or it can a higher value.4. In the example above of average salary package, Shahrukh’s income would be an outlier. What is an outlier? The income of Shahrukh lies at an abnormal distance from the income of other economics graduates. So the income of Shahrukh here becomes an outlier because it lies at an abnormal distance from other values in the data.To summarize, non-parametric tests can be applied to situations when:The point to be noted here is that if there exists a parametric test for a problem then using nonparametric tests will yield highly inaccurate answers. In the above discussion, you may have noticed that I mentioned few points where using non-parametric tests could be beneficial or disadvantageous so now let’s look at these points collectively.  The pros of using non-parametric tests over parametric tests are 1. Non-parametric tests deliver accurate results even when the sample size is small.2. Non-parametric tests are more powerful than parametric tests when the assumptions of normality have been violated.3. They are suitable for all data types, such as nominal, ordinal, interval or the data which has outliers.1. If there exists any parametric test for a data then using non-parametric test could be a terrible blunder.2. The critical value tables for non-parametric tests are not included in many computer software packages so these tests require more manual calculations. Now you know that non-parametric tests are indifferent to the population parameters so it does not make any assumptions about the mean, standard deviation etc of the parent population. The null hypothesis here is as general as the two given populations are equal. Steps to follow while conducting non-parametric tests: Now, let’s look at what these two areHypothesis: My prediction is that Rahul is going to win the race and the other possible outcome is that Rahul isn’t going to win the race. These are our hypothesis. Our alternative hypothesis is Rahul will win the race because we set alternative hypothesis equal to what we want to prove. The null hypothesis is the opposite one, generally null hypothesis is the statement of no difference. For example,
Level of significance: It is the probability of making a wrong decision. In the above hypothesis statement, null hypothesis indicates no difference between sample and population mean. Say there’s a 5% risk of rejecting the null hypothesis when there is no difference between the sample and population mean. This risk or probability of rejecting the null hypothesis when it’s true is called level of significance.In a non-parametric test, the test hypothesis can be one tailed or two tailed depending on the interest of research. To understand what a statistic is, let’s look at an example. A teacher computed the average marks, say 36, scored by the students in section A, and she used the average marks scored by the students in section A to represent the average marks scored by the students in sections B, C and D. The point to be noted here is that the teacher did not make the use of total marks scored by the students in all the sections instead she used the average marks of section A. Here, the average marks is called a statistic since the teacher did not make use of the entire data. In a non-parametric test, the observed sample is converted into ranks and then ranks are treated as a test statistic. A decision rule is just a statement that tells when to reject the null hypothesis. In non-parametric tests, we use the ranks to compute the test statistic.
 Here, you’ll be accepting or rejecting your null hypothesis on the basis of comparison. We will dig deeper into this section while discussing types of non-parametric tests.  Also known as Mann Whitney Wilcoxon and Wilcoxon rank sum test and, is an alternative to independent sample t-test. Let’s understand this with the help of an example.A pharmaceutical organization created a new drug to cure sleepwalking and observed the result on a group of 5 patients after a month. Another group of 5 has been taking the old drug for a month. The organization then asked the individuals to record the number of sleepwalking cases in the last month. The result was:If you look at the table, the number of sleepwalking cases recorded in a month while taking the new drug is lower as compared to the cases reported while taking the old drug.Look at the graphs given below. 
Now, here you see that the frequency of sleepwalking cases is lower when the person is taking new drugs. 

Understood the problem? Let’s see how Mann Whitney U test works here. We are interested in knowing whether the two groups taking different drugs report the same number of sleepwalking cases or not. The hypothesis is given below:
I am selecting 5% level of significance for this test. The next step is to set a test statistic. 
For Mann Whitney U test, the test statistic is denoted by U which is the minimum of U1 and U2.Now, we will compute the ranks by combining the two groups. The question isHow to assign ranks?Ranks are a very important component of non-parametric tests and therefore learning how to assign ranks to a sample is considerably important. Let’s learn how to assign ranks.1. We will combine the two samples and arrange them in ascending order. I am using OD and ND for Old Drug and New Drug respectively.The lowest value here is assigned the rank 1 and the second lowest value is assigned the rank 2 and so on.But notice that the numbers 1, 4 and 8 are appearing more than once in the combined sample. So the ranks assigned are wrong.How to assign ranks when there are ties in the sample?Ties are basically a number appearing more than once in a sample. Look at the position of number 1 in the sample after sorting the data. Here, the number 1 is appearing at 1st and 2nd position. In such a case, we take the mean of 1 and 2 (because the number 1 is appearing at 1st and 2nd position) and assign the mean to the number 1 as shown below. We follow the same steps for number 4 and 8. The number 4 here is appearing at position 5th and 6th and their mean is 5.5 so we assign rank 5.5 to the number 4. Calculate rank for number 8 along these lines.  We assign the mean rank when there are ties in a sample to make sure that the sum of ranks in each sample of size n is same. Therefore, the sum of ranks will always be equal to2. The next step is to compute the sum of ranks for group 1 and group 2.R1 = 15.5
R2 = 39.53. Using the formula of U1 & U2, compute their values.U1 = 24.5
U2 = 0.5Now, U = min(U1, U2) = 0.5Note: For Mann Whitney U test, the value of U lies in the range(0, n1*n2) where 0 indicates that the two groups are completely different from each other and n1*n2 indicates some relation between the two groups. Also, U1 + U2 is always equal to n1*n2. Notice that the value of U is 0.5 here which is very close to 0. Now, we determine a critical value (denoted by p), using the table for critical values, which is a point derived from the level of significance of the test and is used to reject or accept the null hypothesis. In Mann Whitney U test, the test criteria areU < critical value, therefore, we reject the null hypothesis and conclude that the there’s no significant evidence to state that two groups report same number of sleepwalking cases. This test can be used in place of paired t-test whenever the sample violates the assumptions of a normal distribution. A teacher taught a new topic in the class and decided to take a surprise test on the next day. The marks out of 10 scored by 6 students were as follows:
Note: Assume that the following data violates the assumptions of normal distribution.Now, the teacher decided to take the test again after a week of self-practice. The scores wereLet’s check if the marks of the students have improved after a week of self-practice.In the table above, there are some cases where the students scored less than they scored before and in some cases, the improvement is relatively high (Student 4). This could be due to random effects. We will analyse if the difference is systematic or due to chance using this test.The next step is to assign ranks to the absolute value of differences. Note that this can only be done after arranging the data in ascending order.In Wilcoxon sign-rank test, we need signed ranks which basically is assigning the sign associated with the difference to the rank as shown below.                                        Easy, right? Now, what is the hypothesis here?The hypothesis can be one-sided or two-sided and I am considering one-sided hypothesis and using 5% level of significance. Therefore, The test statistic for this test is W is the smaller of W1 and W2 defined below:W1 = 17.5
W2 = 3.5
W = min(W1, W2 ) = 3.5Here, if W1 is similar to W2 then we accept the null hypothesis. Otherwise, in this example, if the difference reflects greater improvement in the marks scored by the students, then we reject the null hypothesis. The critical value of W can be looked up in the table. The criteria to accept or reject null hypothesis areHere, W > critical value=2, therefore we accept the null hypothesis and conclude that there’s no significant difference between the marks of two tests.  This test is similar to Wilcoxon sign-rank test and this can also be used in place of paired t-test if the data violates the assumptions of normality. I am going to use the same example that I used in Wilcoxon sign-rank test, assuming that it does not follow the normal distribution, to explain sign test. Let’s look at the data again.In sign test, we don’t take magnitude into consideration thereby ignoring the ranks. The hypothesis is same as before.Here, if we see a similar number of positive and negative differences then the null hypothesis is true. Otherwise, if we see more of positive signs then the null hypothesis is false.Test Statistic:  The test statistic here is smaller of the number of positive and negative signs. Determine the critical value and the criteria for rejecting or accepting null hypothesis isHere, the smaller number of + & – signs = 2 < critical value = 6. Therefore, we reject the null hypothesis and conclude that there’s no significant evidence to state that the median difference is zero. This test is extremely useful when you are dealing with more than 2 independent groups and it compares median among k populations. This test is an alternative to One way ANOVA when the data violates the assumptions of normal distribution and when the sample size is too small.

Note: The Kruskal-Wallis test can be used for both continuous and ordinal-level dependent variables.Let’s look at an example to enhance our understanding of Kruskal-Wallis test. Patients suffering from Dengue were divided into 3 groups and three different types of treatment were given to them. The platelet count of the patients after following a 3-day course of treatment is given below.Notice that the sample size is different for the three treatments which can be tackled using Kruskal-Wallis test.Sample sizes for treatments 1, 2 and 3 are as follows:Treatment 1; n1 = 5
Treatment 2; n2 = 3
Treatment 3; n3 = 4
n = n1 + n2 + n3 = 5+3+4 = 12The hypothesis here is given below and I have selected 5% level of significance.Order these samples from smallest to largest and then assign ranks to the clubbed sample.Recall that the sum of ranks will always be equal to n(n+1)/2.Here, sum of ranks = 78
n(n+1)/2 = (12*13)/2 = 78We have to check if there is a difference between 3 population medians so we will summarize the sample information in a test statistic based on ranks. Here, the test statistic is denoted by H and given by the following formulaThe next step is to determine the critical value of H using the table of critical values and the test criteria is given by:H comes out to be 6.0778 and the critical value is 5.656. Therefore, we reject our null hypothesis and conclude that there’s no significant evidence to state that the three population medians are same. Note: In a Kruskal-Wallis test, if there are 3 or more independent comparison groups with 5 or more observations in each group then the test statistic H approximates a chi-square distribution with k-1 degree of freedom. Therefore, in such a case, you can find the critical value of the test in the chi-square distribution table for critical values. 
 I went to the market to buy a skirt and coincidently my friend bought the same skirt from the market near her place but she paid a higher price for it. The market near my friend’s place is more expensive as compared to mine. So does a region affect the price of a commodity? If it does then there is a link between the region and price of the commodity. We make use of Spearman rank correlation here because it establish if there is the correlation between two datasets. The prices of vegetables differ across areas. We can check if there’s a relation between the price of a vegetable and area by using Spearman rank correlation. The hypothesis here is:
Here, the trend line suggests a positive correlation between the price of vegetable and area. However, Spearman’s rank correlation method should be used to check the direction and strength of correlation.Spearman rank correlation is a non-parametric alternative to Pearson correlation coefficient and is denoted by . The value of  lies in the range (-1,1) where 
-1 represents a negative correlation between ranks
0 represents no correlation between ranks
1 represents a positive correlation between ranks

After assigning ranks to the sample, use the following formula to calculate Spearman rank correlation coefficient.Let’s understand the application of these formula using a sample data.The following table includes marks of students in math and science.
The null hypothesis states that there is no relationship between the marks and alternative hypothesis states that there is a relationship between the marks. I have selected 5% level of significance for this test. Now calculate rank and d which is the difference between ranks and n is the sample size = 10. This is done as follows:Now, use the formula to calculate Spearman rank correlation coefficient. Hence, the Spearman rank correlation comes out to be 0.67 which indicates a positive relation between the ranks students obtained in maths and science test which implies that the higher you ranked in maths, the higher you ranked in science and vice-versa.You can also check this by determining the critical values using the significance level and sample size. The criteria to reject or accept null hypothesis is given byNote: The degree of freedom here is n-2. 
The critical value is found to be 0.033 which is lower than 0.67. Hence, we reject our null hypothesis. Non-parametric tests are more powerful when the assumptions for parametric tests are violated and can be used for all data types such as nominal, ordinal, interval and also when data has outliers. If any of the parametric tests is valid for a problem then using non-parametric test will give highly inaccurate results. To summarize,Mann Whitney U test is used for testing the difference between two independent groups with ordinal or continuous dependent variable.Wilcoxon sign rank test is used for testing the difference between two related variables which takes into account the magnitude and direction of difference, however, Sign test ignores the magnitude and only considers the direction of the difference.Kruskal-Wallis test compares the outcome among more than 2 independent groups by making use of the medians.Spearman Rank Correlation technique is used to check if there is a relationship between the two data sets and it also tells about the type of relationship. I hope that you find this article useful and if you would like to see more articles on non-parametric or parametric tests then write down in the comment section below.",https://www.analyticsvidhya.com/blog/2017/11/a-guide-to-conduct-analysis-using-non-parametric-tests/
A Comprehensive Tutorial to Learn Data Science with Julia from Scratch,Learn everything about Analytics|Introduction|Table of Contents|Installation|Basics of Julia for Data Analysis|Exploratory Analysis using Julia (Analytics Vidhya Hackathon)|Data Munging in Julia|Building a predictive ML model|Calling R and Python libraries in Julia||End Notes,"Running your first Julia program|Few things to note|Julia Data Structures|Loops, Conditionals in Julia|Introduction to DataFrames.jl|Practice dataset: Loan Prediction Problem|Importing libraries and the data set|Quick Data Exploration|Visualisation in Julia using Plots.jl|Distribution analysis|Bonus: Interactive visualizations using Plotly|Data munging – recap of the need|Check missing values in the dataset|How to fill missing values?|Label Encoding categorical data|Logistic Regression|Decision Tree|Random Forest|Using pandas with Julia|Using ggplot2 in Julia|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|A Guide To Conduct Analysis Using Non-Parametric Statistical Tests|The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks)|
Mohd Sanad Zaki Rizvi
|27 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Installing Julia|Installing IJulia and Jupyter Notebook|Installing Julia Packages,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Recently, I came across a quote about Julia:“Walks like python. Runs like C.”The above line tells a lot about why I chose to write this article. I came across Julia a while ago even though it was in its early stages, it was still creating ripples in the numerical computing space. Julia is a work straight out of MIT, a high-level language that has a syntax as friendly as Python and performance as competitive as C. This is not all,  It provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.But this article isn’t about praising Julia, it is about how can you utilize it in your workflow as a data scientist without going through hours of confusion which usually comes when we come across a new language. Read more about Why Julia? here.  Before we can start our journey into the world of Julia, we need to set up our environment with the necessary tools and libraries for data science. https://julialang.org/downloads/  https://julialang.org/downloads/platform.html  Jupyter notebook has become an environment of choice for data science since it is really useful for both fast experimenting and documenting your steps. There are other environments too for Julia like  Juno IDE  but I recommend to stick with the notebook. Let’s look at how we can setup the same for Julia.Go to the Julia prompt and type the following codeNote that Pkg.add() command downloads files and package dependencies in the background and installs it for you. For this, you should have an active internet connection. If your internet is slow, you might have to wait for some time.After ijulia is successfully installed you can type the following code to run it,By default, the notebook “dashboard” opens in your home directory ( homedir() ), but you can open the dashboard in a different directory with notebook(dir=”/some/path”) .There you have your environment all set up. Let’s install some important Julia libraries that we’d be needing for this tutorial. A simple way of installing any package in Julia is using the command Pkg.add(“..”). Like Python or R, Julia too has a long list of packages for data science. I thought instead of installing all the packages together it would be better if we install them as and when needed, that’d give you a good sense of what each package does. So we will be following that process for this article. Julia is a language that derives a lot of syntax from other data analysis tools like R, Python, and MATLAB. If you are from one of these backgrounds, it would take you no time to get started with it. Let’s learn some of the basic syntaxes. If you are in a hurry here’s a cheat sheet comparing syntax of all the three languages:There, you created your first Julia notebook! Just like you use jupyter notebook for R or Python, you can write Julia code here, train your models, make plots and so much more all while being in the familiar environment of jupyter. Go ahead and play around a bit with the notebook to get familiar. The following are some of the most common data structures we end up using when performing data analysis on Julia:Note that in Julia the indexing starts from 1, so if you want to access the first element of an array you’ll do A[1].Notice that “=>” operator is used to link key with their respective values. You access the values of the dictionary using its key. Like most languages, Julia also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax:Here “Julia Iterable” can be a vector, string or other advanced data structures which we will explore in later sections. Let’s take a look at a simple example, determining the factorial of a number ‘n’.Julia also supports the while loop and various conditionals like if, if/else, for selecting a bunch of statements over another based on the outcome of the condition. Here is an example,The above code snippet performs a check on N and prints whether it is a positive or a negative number. Note that julia is not indentation sensitive like Python but it is a good practice to indent your code that’s why you’ll find code samples in this article well indented. Here is a list of Julia conditional constructs compared to their counterparts in MATLAB and Python.You can learn more about Julia basics  here  .Now that we are familiar with Julia fundamentals, let’s take a deep dive into problem-solving. Yes, I mean making a predictive model! In the process, we use some powerful libraries and also come across the next level of data structures. We will take you through the 3 key phases: The first step in any kind of data analysis is exploring the dataset at hand. There are two ways to do that, the first is exploring the data tables and applying statistical methods to find patterns in numbers and the second is plotting the data to find patterns visually.The former requires an advanced data structure that is capable of handling multiple operations and at the same time is fast and scalable. Like many other data analysis tools, Julia provides one such structure called DataFrame. You need to install the following package for using it:A dataframe is similar to Excel workbook – you have column names referring to columns and you have rows, which can be accessed with the use of row numbers. The essential difference is that column names and row numbers are known as column and row index, in case of dataframes . This is similar to pandas.DataFrame in Python or data.table in R.Let’s work with a real problem. We are going to analyze an Analytics Vidhya Hackathon as a practice dataset. You can download the dataset from  here  . Here is the description of variables:In Julia we import a library by the following command:Let’s first import our DataFrames.jl library and load the train.csv file of the data set: Once the data set is loaded, we do preliminary exploration on it. Such as finding the size(number of rows and columns) of the data set, the name of columns etc. The function size(train) is used to get the number of rows and columns of the data set and names(train) is used to get the names of columns(features).The data set is not that large(only 614 rows) knowing the size of data set sometimes affect the choice of our algorithm. There are 13 columns(features) we have that is also not much, in case of a large number of features we go for techniques like dimensionality reduction etc. Let’s look at the first 10 rows to get a better feel of how our data looks like? The head(,n) function is used to read the first n rows of a dataset.A number of preliminary inferences can be drawn from the above table such as:Note that these inferences are just preliminary they will either get rejected or updated after further exploration.I am interested in analyzing the LoanAmount column, let’s have a closer look at that.describe() function would provide the count(length), mean, median, minimum, quartiles and maximum in its output (Read  this article  to refresh basic statistics to understand population distribution).Please note that we can get an idea of a possible skew in the data by comparing the mean to the median, i.e. the 50% figure.For the non-numerical values (e.g. Property_Area, Credit_History etc.), we can look at frequency distribution to understand whether they make sense or not. The frequency table can be printed by the following command:Similarly, we can look at unique values of credit history. Note that dataframe_name[:column_name] is a basic indexing technique to access a particular column of the dataframe. A column can also be accessed by its index. For more information, refer to the documentation . Another effective way of exploring the data is by doing it visually using various kind of plots as it is rightly said, “A picture is worth a thousand words” .Julia doesn’t provide a plotting library of its own but it lets you use any plotting library of your own choice in Julia programs. In order to use this functionality you need to install the following package:The package “Plots.jl” provides a single frontend(interface) for any plotting library(matplotlib, plotly, etc.) you want to use in Julia. “StatPlots.jl” is a supporting package used for Plots.jl. “PyPlot.jl” is used to work with matplotlib of Python in Julia. Now that we are familiar with basic data characteristics, let us study the distribution of various variables. Let us start with numeric variables – namely ApplicantIncome and LoanAmountLet’s start by plotting the histogram of ApplicantIncome using the following commands:Here we observe that there are few extreme values. This is also the reason why 50 bins are required to depict the distribution clearly.Next, we look at box plots to understand the distributions. Box plot for fare can be plotted by:This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:We can see that there is no substantial difference between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers.Now, Let’s look at the histogram and boxplot of LoanAmount using the following command:Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging. LoanAmount has missing and well as extreme values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections.That was a lot of useful visualizations, to learn more about creating visualizations in Julia using Plots.jl Plots.jl Documentation Now’s the time where awesomeness of Plots.jl comes into play. The visualizations we created till now were all good but while exploration it is useful if the plot is interactive. We can create interactive plots in Julia using Plotly as a backend. Type the following codeYou can do much more with Plots.jl and various backends it supports. Read Plots.jl Documentation 🙂 For those, who have been following, here you must wear your shoes to start running. While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as “Data Munging”. Here are the problems, we are already aware of:In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Education and Dependents to see, if they contain any useful information. Let us look at missing values in all the variables because most of the models don’t work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the datasetThough the missing values are not very high in number, many variables have them and each one of these should be estimated and added to the data.Note: Remember that missing values may not always be NaNs. For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and you’re right. So we should check for values which are unpractical. There are multiple ways of fixing missing values in a dataset. Take LoanAmount for example, there are numerous ways to fill the missing values – the simplest being replacement by the mean.The other extreme would be to build a supervised learning model to predict loan amount on the basis of other variables and then use age along with other variables to predict survival.We would be taking the simpler approach to fix missing values in this article:I have basically replaced all missing values in numerical columns with their means and with the mode in categorical columns. Let’s understand the code little closely,I hope this gives you a better understanding of the code part that is used to fix missing values.As discussed earlier, there are better ways to perform data imputation and I encourage you to learn as many as you can. Get a detailed view of different imputation techniques through  this article  . Now that we have fixed all missing values, we will be building a predictive machine learning model. We will also be cross-validating it and saving it to the disk for future use. The following packages are required for doing so:This package is an interface to Python’s scikit-learn package so python users are in for a treat. The interesting thing about using this package is you get to use the same models and functionality as you used in Python. Sklearn requires all data to be of numeric type so let’s label encode our data,Those who have used sklearn before will find this code to be familiar, we are using LabelEncoder to encode the categories. I have used the index of columns with categorical data.Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores. Since this is an introductory article and julia code is very similar to python, I will not go into the details of coding. Please refer to  this article  for getting details of the algorithms with R and Python codes. Also, it’ll be good to get a refresher on cross-validation through  this article  , as it is a very important measure of power performance. Let’s make our first Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting (don’t worry if you’re unaware of this terminology yet). In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. Read more about  Logistic Regression  .We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for:So let’s make our first model with ‘Credit_History’.Accuracy : 80.945% Cross-Validation Score : 80.957%Accuracy : 80.945% Cross-Validation Score : 80.957%Generally, we expect the accuracy to increase by adding variables. But this is a more challenging case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now: A decision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more about Decision Trees.Accuracy : 80.945% Cross-Validation Score : 76.656%Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Let’s try a few numerical variables:Accuracy : 99.345% Cross-Validation Score : 72.009%Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Let’s try an even more sophisticated algorithm and see if it helps: Random forest is another algorithm for solving the classification problem. Read more about Random Forest.An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features.Accuracy : 100.000% Cross-Validation Score : 78.179%Here we see that the accuracy is 100% for the training set. This is the ultimate case of overfitting and can be resolved in two ways:The updated code would now beAccuracy : 82.410% Cross-Validation Score : 80.635%Notice that although accuracy reduced, the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark.You would have noticed that even after some basic parameter tuning on the random forest, we have reached a cross-validation accuracy only slightly better than the original logistic regression model. This exercise gives us some very interesting and unique learning:So are you ready to take on the challenge? Start your data science journey with Loan Prediction Problem. Julia is a powerful language with interesting libraries but it may so happen that you want to use library of your own from outside Julia. One such reason can be lack of functionality in existing Julia libraries(it is still very young). For situations like this, Julia provides ways to call libraries from R and Python. Let’s see how can we do that? Install the following package:There is something interesting about using a Python library as smoothly in another language.Pandas is a very mature and performant library, it is certainly a bliss that we can use it wherever the native DataFrames.jl falls short. Install the following packages: I hope this tutorial will help you maximize your efficiency when starting with data science in Julia. I am sure this not only gave you an idea about basic data analysis methods but it also showed you how to implement some of the more sophisticated techniques available today.Julia is really a great tool and is becoming an increasingly popular language among the data scientists. The reason being, it’s easy to learn, integrates well with other tools, gives C like speed and also allows using libraries of existing tools like R and Python.So, learn Julia to perform the full life-cycle of any data science project. It includes reading, analyzing, visualizing and finally making predictions.Also note, all the code used in this article is available on GitHub.If you come across any difficulty while practicing Julia, or you have any thoughts/suggestions/feedback on the post, please feel free to post them in comments below.",https://www.analyticsvidhya.com/blog/2017/10/comprehensive-tutorial-learn-data-science-julia-from-scratch/
The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks),Learn everything about Analytics|Introduction|Why did I create this Guide?|Table of Contents|1. Stemming|2. Lemmatisation|3. Word Embeddings|4. Part-Of-Speech Tagging|5. Named Entity Disambiguation|6. Named Entity Recognition|7. Sentiment Analysis|8. Semantic Text Similarity|9. Language Identification|10.  Text Summarisation|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|A Comprehensive Tutorial to Learn Data Science with Julia from Scratch|Fundamentals of Deep Learning – Activation Functions and When to Use Them?|
NSS
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Organizations today deal with huge amount and wide variety of data – calls from customers, their emails, tweets, data from mobile applications and what not. It takes a lot of effort and time to make this data useful. One of the core skills in extracting information from text data is Natural Language Processing (NLP).Natural Language Processing (NLP) is the art and science which helps us extract information from text and use it in our computations and algorithms. Given then increase in content on internet and social media, it is one of the must have still for all data scientists out there.Whether you know NLP or not, this guide should help you as a ready reference for you. Through this guide, I have provided you with resources and codes to run the most common tasks in NLP.Once you have gone through this guide, feel free to have a look at our video course on Natural Language Processing (NLP). After having been working on NLP problems for some time now, I have encountered various situations where I needed to consult hundred of different of sources to study about the latest developments in the form of research papers, blogs and competitions for some of the common NLP tasks.So, I decided to bring all these resources to one place and make it a One-Stop solution for the latest and the important resources for these common NLP tasks. Below is the list of tasks covered in this article along with their relevant resources. Let’s get started.  What is Stemming?: Stemming is the process of reducing the words(generally modified or derived) to their word stem or root form. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word. For example, in the English language-Paper: The original paper by Martin Porter on Porter Algorithm for stemming.Algorithm: Here is the Python implementation of Porter2 stemming algorithm.Implementation: Here is how you can stem a word using the Porter2 algorithm from the stemming library.#!pip install stemming
from stemming.porter2 import stem
stem(""casually"") What is Lemmatisation?: Lemmatisation is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence, the meaning of the word in the nearby sentences etc. before reducing the word to its lemma. For example, in the English Language-Paper 1: This paper discusses different methods for performing lemmatisation in great detail. A must read if you want to know hoe traditional lemmatisers work.Paper 2: This is an excellent paper which addresses the problem of lemmatisation for variation rich languages using Deep Learning.Dataset: This is the link for Treebank-3 dataset which you can use if you wish to create your own Lemmatiser.Implementation: Below is an implementation of an English Lemmatiser using spacy.#!pip install spacy
#python -m spacy download en
import spacy
nlp=spacy.load(""en"")
doc=""good better best""for token in nlp(doc):
    print(token,token.lemma_) What is Word Embeddings?: Word Embeddings is the name of the techniques which are used to represent Natural Language in vector form of real numbers. They are useful because of computers’ inability to process Natural Language. So these Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100.So for example-A word “man” might be represented in a 5-dimension vector as where each of these numbers is the magnitude of the word in a particular direction. Blog: Here is an article which explains Word Embeddings in great detail.Paper: A very good paper which explains Word Vectors in detail. A must-read for an in-depth understanding of Word Vectors.Tool: A browser based tool for visualising Word Vectors.Pre-trained Word Vectors: Here is an exhaustive list of pre-trained Word Vectors in 294 languages by facebook.Implementation: Here is how you can obtain pre-trained Word Vector of a word using the gensim package.Download the Google News pre-trained Word Vectors from here.#!pip install gensim
from gensim.models.keyedvectors import KeyedVectors
word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)
word_vectors['human']Implementation: Here is how you can train your own word vectors using gensimsentence=[['first','sentence'],['second','sentence']]
model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4) What is Part-Of-Speech Tagging?: In Simplistic terms, Part-Of-Speech Tagging is the process of marking up of words in a sentence as nouns, verbs, adjectives, adverbs etc. For example, in the sentence-“Ashok killed the snake with a stick”The Parts-Of-Speech are identified as –Ashok PROPN
killed VERB
the DET
snake NOUN
with ADP
a DET
stick NOUN
. PUNCTPaper 1: This paper by choi aptly titled The Last Gist to the State-of-the-Art presents a novel method called Dynamic Feature Induction which achieves state-of-the-art on POS Tagging taskPaper 2: This paper presents performing unsupervised POS Tagging using Anchor Hidden Markov Models.Implementation: Here is how we can perform POS Tagging using spacy.#!pip install spacy
#!python -m spacy download en 
nlp=spacy.load('en')
sentence=""Ashok killed the snake with a stick""
for token in nlp(sentence):
   print(token,token.pos_) What is Named Entity Disambiguation?: Named Entity Disambiguation is the process of identifying the mentions of entities in a sentence. For example, in the sentence-“Apple earned a revenue of 200 Billion USD in 2016”It is the task of Named Entity Disambiguation to infer that Apple in the sentence is the company Apple and not a fruit.Named Entity, in general, requires a knowledge base of entities which it can use to link entities in the sentence to the knowledge base.Paper 1: This paper by Huang makes use of Deep Semantic Relatedness models based on Deep Neural Networks along with Knowledgebase to achieve a state-of-the-art result on Named Entity Disambiguation.Paper 2: This paper by Ganea and Hofmann make use of Local Neural Attention along with Word Embeddings and no manually crafted features. What is Named Entity Recognition?: Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example, a NER would take in a sentence like –“Ram of Apple Inc. travelled to Sydney on 5th October 2017”and return something likeRam
of
Apple ORG
Inc. ORG
travelled
to
Sydney GPE
on
5th DATE
October DATE
2017 DATEHere, ORG stands for Organisation and GPE stands for location.The problem with current NERs is that even state-of-the-art NER tend to perform poorly when they are used on a domain of data which is different from the data, the NER was trained on.  Paper: This excellent paper makes use of bi-directional LSTMs and combines Supervised and Unsupervised learning methods to achieve a state-of-the-art result in Named Entity Recognition in 4 languages.Implementation: Here is how you can perform Named Entity Recognition using spacy.import spacy
nlp=spacy.load('en')sentence=""Ram of Apple Inc. travelled to Sydney on 5th October 2017""
for token in nlp(sentence):
   print(token, token.ent_type_) What is Sentiment Analysis?: Sentiment Analysis is a broad range of subjective analysis which uses Natural Language processing techniques to perform tasks such as identifying the sentiment of a customer review, positive or negative feeling in a sentence, judging mood via voice analysis or written text analysis etc. For example-“I did not like the chocolate ice-cream” – is a negative experience of ice-cream.“I did not hate the chocolate ice-cream” – may be considered as a neutral experienceThere is a wide range of methods which are used to perform sentiment analysis starting from taking a count of negative and positive words in a sentence to using LSTMs with Word Embeddings.Blog 1: This article focuses on performing sentiment analysis on movie tweetsBlog 2: This article focuses on performing sentiment analysis of tweets during the Chennai flood.Paper 1: This paper takes the Supervised Learning method approach with Naive Bayes method to classify IMDB reviews.Paper 2: This paper makes use of Unsupervised Learning method with LDA to identify aspects and sentiments of user-generated reviews. This paper is outstanding in the sense that it addresses the problem of shortage of annotated reviews.Repository: This is an awesome repository of the research papers and implementation of sentiment analysis in various languages.Dataset 1: Multi-Domain sentiment dataset version 2.0Dataset 2: Twitter Sentiment analysis DatasetCompetition: A very good competition where you can check the performance of your models on the sentiment analysis task of movie reviews of rotten tomatoes.Perform Twitter Sentiment Analysis your self.What is Semantic Text Similarity?: Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.For example –Car and Bus are similar but Car and fuel are related.Paper 1: This paper presents the different approaches to measuring text similarity in detail. A must read paper to know about the existing approaches at a single place.Paper 2: This paper introduces CNNs to rank a pair of two short textsPaper 3: This paper makes use of Tree-LSTMs which achieve a state-of-the-art result on Semantic Relatedness of texts and Semantic Classification. What is Language Identification?: Language identification is the task of identifying the language in which the content is in.  It makes use of statistical as well as syntactical properties of the language to perform this task. It may also be considered as a special case of text classification.Blog: In this blog post by fastText, they introduce a new tool which can identify 170 languages under 1MB of memory usage.Paper 1: This paper discusses 7 methods of language identification of 285 languages.Paper 2: This paper describes how Deep Neural Networks can be used to achieve state-of-the-art results on Automatic Language Identification. What is Text Summarisation?: Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.Paper 1: This paper describes a Neural Attention Model based approach for Abstractive Sentence Summarization.Paper 2: This paper describes how sequence-to-sequence RNNs can be used to achieve state-of-the-art results on Text Summarisation.Repository: This repository by Google Brain team has the codes for using a sequence-to-sequence model customised for Text Summarisation. The model is trained on Gigaword dataset.Application: Reddit’s autotldr bot uses Text Summarisation to summarise articles into the comments of a post. This feature turned out to be very famous amongst the Reddit users.Implementation: Here is how you can quickly summarise your text using the gensim package.from gensim.summarization import summarize

sentence=""Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.""

summarize(sentence) So this was all about the most common NLP tasks along with their relevant resources in the form of blogs, research papers, repositories and applications etc. If you feel, there is any great resource on any of these 10 tasks that I have missed or you want to suggest adding another task, then please feel free to comment with your suggestions and feedback.Happy Learning!",https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/
Fundamentals of Deep Learning – Activation Functions and When to Use Them?,Learn everything about Analytics|Introduction|Table of Contents|Brief overview of neural networks|What is an Activation Function?|Can we do without an activation function?|Popular types of activation functions and when to use them|Choosing the right Activation Function|Projects|End Notes,"Binary Step Function|Linear Function|Sigmoid|Tanh|ReLU|Leaky ReLU|Softmax|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks)|The Art of Story Telling in Data Science and how to create data stories?|
Dishashree Gupta
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Internet provides access to plethora of information today. Whatever we need is just a Google (search) away. However, when we have so much of information, the challenge is to segregate between relevant and irrelevant information.When our brain is fed with a lot of information simultaneously, it tries hard to understand and classify the information between useful and not-so-useful information. We need a similar mechanism to classify incoming information as useful or less-useful in case of Neural Networks.This is a very important in the way a network learns because not all information is equally useful. Some of it is just noise. Well, activation functions help the network do this segregation. They help the network use the useful information and suppress the irrelevant data points.Let us go through these activation functions, how they work and figure out which activation functions fits well into what kind of  problem statement.  Before I delve into the details of activation functions, let’s do a little review of what are neural networks and how they function. A neural network is a very powerful machine learning mechanism which basically mimics how a human brain learns. The brain receives the stimulus from the outside world, does the processing on the input, and then generates the output.As the task gets complicated multiple neurons form a complex network, passing information among themselves.Using a artificial neural network, we try to mimic a similar behavior. The network you see below is a neural network made of interconnected neurons.The black circles in the picture above are neurons. Each neuron is characterized by its weight, bias and activation function. The input is fed to the input layer. The neurons do a linear transformation on the input by the weights and biases. The non linear transformation is done by the activation function. The information moves from the input layer to the hidden layers. The hidden layers would do the processing and send the final output to the output layer. This is the forward movement of information known as the forward propagation. But what if the output generated is far away from the expected value? In a neural network, we would update the weights and biases of the neurons on the basis of the error. This process is known as back-propagation. Once the entire data has gone through this process, the final weights and biases are used for predictions. Activation functions are an extremely important feature of the artificial neural networks. They basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored.The activation function is the non linear transformation that we do over the input signal. This transformed output is then sen to the next layer of neurons as input. Now the question which arises is that if the activation function increases the complexity so much, can we do without an activation function?When we do not have the activation function the weights and bias would simply do a linear transformation. A linear equation is simple to solve but is limited in its capacity to solve complex problems. A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. We would want our neural networks to work on complicated tasks like language translations and image classifications. Linear transformations would never be able to perform such tasks.Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. Without the differentiable non linear function, this would not be possible. The first thing that comes to our mind when we have an activation function would be a threshold based classifier i.e. whether or not the neuron should be activated. If the value Y is above a given threshold value then activate the neuron else leave it deactivated.It is defined as –= 0, x<0The binary function is extremely simple. It can be used while creating a binary classifier. When we simply need to say yes or no for a single class, step function would be the best choice, as it would either activate the neuron or leave it to zero.The function is more theoretical than practical since in most cases we would be classifying the data into multiple classes than just a single class. The step function would not be able to do that.Moreover, the gradient of the step function is zero. This makes the step function not so useful since during back-propagation when the gradients of the activation functions are sent for error calculations to improve and optimize the results. The gradient of the step function reduces it all to zero and improvement of the models doesn’t really happen. We saw the problem with the step function, the gradient being zero, it was impossible to update gradient during the backpropagation. Instead of a simple step function, we can try using a linear function. We can define the function as-f(x)=axWe have taken a as 4 in the figure above. Here the activation is proportional to the input. The input x, will be transformed to ax. This can be applied to various neurons and multiple neurons can be activated at the same time. Now, when we have multiple classes, we can choose the one which has the maximum value. But we still have an issue here. Let’s look at the derivative of this function.The derivative of a linear function is constant i.e. it does not depend upon the input value x.This means that every time we do a back propagation, the gradient would be the same. And this is a big problem, we are not really improving the error since the gradient is pretty much the same. And not just that suppose we are trying to perform a complicated task for which we need multiple layers in our network. Now if each layer has a linear transformation, no matter how many layers we have the final output is nothing but a linear transformation of the input. Hence, linear function might be ideal for simple tasks where interpretability is highly desired. Sigmoid is a widely used activation function. It is of the form-Let’s plot this function and take a look of it.This is a smooth function and is continuously differentiable. The biggest advantage that it has over step and linear function is that it is non-linear. This is an incredibly cool feature of the sigmoid function. This essentially means that when I have multiple neurons having sigmoid function as their activation function – the output is non linear as well. The function ranges from 0-1 having an S shape. Let’s take a look at the shape of the curve. The gradient is very high between the values of -3 and 3 but gets much flatter in other regions. How is this of any use?This means that in this range small changes in x would also bring about large changes in the value of Y. So the function essentially tries to push the Y values towards the extremes. This is a very desirable quality when we’re trying to classify the values to a particular class.Let’s take a look at the gradient of the sigmoid function as well.It’s smooth and is dependent on x. This means that during backpropagation we can easily use this function. The error can be backpropagated and the weights can be accordingly updated.Sigmoids are widely used even today but we still have a problems that we need to address. As we saw previously – the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region the gradients become very small. This means that the gradient is approaching to zero and the network is not really learning.Another problem that the sigmoid function suffers is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin and the values received are all positive. So not all times would we desire the values going to the next neuron to be all of the same sign. This can be addressed by scaling the sigmoid function. That’s exactly what happens in the tanh function. let’s read on. The tanh function is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function.It can be directly written as –Tanh works similar to the sigmoid function but is symmetric over the origin. it ranges from -1 to 1.It basically solves our problem of the values all being of the same sign. All other properties are the same as that of the sigmoid function. It is continuous and differentiable at all points. The function as you can see is non linear so we can easily backpropagate the errors.Let’s have a look at the gradient of the tan h function.The gradient of the tanh function is steeper as compared to the sigmoid function. Our choice of using sigmoid or tanh would basically depend on the requirement of gradient in the problem statement. But similar to the sigmoid function we still have the vanishing gradient problem. The graph of the tanh function is flat and the gradients are very low. The ReLU function is the Rectified linear unit. It is the most widely used activation function. It is defined as-It can be graphically represented as-ReLU is the most widely used activation function while designing networks today. First things first, the ReLU function is non linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. What does this mean ? If you look at the ReLU function if the input is negative it will convert it to zero and the neuron does not get activated. This means that at a time only a few neurons are activated making the network sparse making it efficient and easy for computation.Let’s look at the gradient of the ReLU function.But ReLU also falls a prey to the gradients moving towards zero. If you look at the negative side of the graph, the gradient is zero, which means for activations in that region, the gradient is zero and the weights are not updated during back propagation. This can create dead neurons which never get activated. When we have a problem, we can always engineer a solution. Leaky ReLU function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which made the neurons die for activations in that region. Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x less than 0, we define it as a small linear component of x. It can be defined as-What we have done here is that we have simply replaced the horizontal line with a non-zero, non-horizontal line. Here a is a small value like 0.01 or so. It can be represented on the graph as-The main advantage of replacing the horizontal line is to remove the zero gradient. So in this case the gradient of the left side of the graph is non zero and so we would no longer encounter dead neurons in that region. The gradient of the graph would look like –Similar to the Leaky ReLU function, we also have the Parameterised ReLU function. It is defined similar to the Leaky ReLU as –However, in case of a parameterised ReLU function, ‘a‘ is also a trainable parameter. The network also learns the value of ‘a‘ for faster and more optimum convergence. The parametrised ReLU function is used when the leaky ReLU function still fails to solve the problem of dead neurons and the relevant information is not successfully passed to the next layer. The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems. The sigmoid function as we saw earlier was able to handle just two classes. What shall we do when we are trying to handle multiple classes. Just classifying yes or no for a single class would not help then. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs. This essentially gives the probability of the input being in a particular class. It can be defined as –Let’s say for example we have the outputs as-The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input. Now that we have seen so many activation  functions, we need some logic / heuristics to know which activation function should be used in which situation. Good or bad – there is no rule of thumb.However depending upon the properties of the problem we might be able to make a better choice for easy and quicker convergence of the network. Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:In this article I have discussed the various types of activation functions and what are the types of problems one might encounter while using each of them.I would suggest to begin with a ReLU function and explore other functions as you move further. You can also design your own activation functions giving a non-linearity component to your network. If you have used your own activation function which worked really well, please share it with us and we shall be happy to incorporate it into the list.",https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
The Art of Story Telling in Data Science and how to create data stories?,Learn everything about Analytics|Introduction|Table of Contents|The need for storytelling|How to create stories?|Types of Data and Suitable Charts|Storytelling during the steps of predictive modeling|Best Practices for Story Telling|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Fundamentals of Deep Learning – Activation Functions and When to Use Them?|8 Essential Tips for People starting a Career in Data Science|
Analytics Vidhya Content Team
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The idea of storytelling is fascinating; to take an idea or an incident, and turn it into a story. It brings the idea to life and makes it  more interesting. This happens in our day to day life. Whether we narrate a funny incident or our findings, stories have always been the “go-to” to draw interest from listeners and readers alike.For instance; when we talk of how one of our friends got scolded by a teacher, we tend to narrate the incident from the beginning so that a flow is maintained.Let’s take an example of the most common driving distractions by gender. There are two ways to tell this.The first is that I give you some statistics as follows:Another way to recreate similar statistics is this visual from kids4kars.org.Which one do you think tells a better story?  The art of storytelling is simple and complex at the same time. Stories provoke thought and bring out insights that could not have been understood or explained before. It’s often overlooked in data driven operations as we believe it’s a trivial task.
What we fail to understand is that the best stories not presented well end up being useless!In several firms, the first step towards analyzing anything is story-boarding it.  Questions like why do we have to analyze it? what decisions can we make out of it? Sometimes, data alone tells such visual and intricate stories that we don’t need to run complex correlations to confirm it.The best example of needing stories and visuals to explain data is the Anscombe’s Quartet. The Anscombe’s Quartet is a set of four datasets with very similar statistical summaries, but completely different when you visualize them.These are the four datasets used during the depiction of the Anscombe’s Quartet. If we look at mere numbers, we find that their summary statistics are almost identical.Let’s see how they appear when we visualize them.Did you ever think these four quartets would have such varying visuals? To create a story or a plot is the first step to selling your ideas with a strong foot forward. Most people fail to think their stories through and cannot differentiate themselves from mediocrity. Let me take an example and guide you through the steps of creating stories.We will be exploring a dataset that has news headlines and details of every stock price from the NASDAQ 100 tech companies. The columns selected are as follows.1. Begin with a pen-paper approachVisually engaging presentations will inspire your audience, but they definitely need more work to be put in. One of the best presentations have been created on rough pages and tissue papers.Scripting down your ideas and flow before you start structuring your story is very essential to your final product.The single most important thing you can do to dramatically improve your analytics is to have a story to tell. A flow that you can generate can have a lot of friction in your end result.Aristotle’s classic five point plan that helps deliver strong impacts is:The way I structured my report was by involving plots that would give me a better understanding of my data.The first idea that I had was, how can I make better business decisions of stocks by using the data that I have?Involving a line graph would help me analyse trend lines of specific stock prices.As I can see, February 2016 has been a drop for all stocks. This would help me scrape news articles only from that period to identify what caused the drop. Now, how do I select which news source to scrape from?By identifying which news source reported most about a particular stock, we would have reason to believe that this is a good source for the specific stock. 2. Dig deeper to identify the sole purpose of your story 3. Use powerful headings 4. Design a Road-Map5. Conclude with brevityNow that you have put forward all points of your story, your conclusion should be short and powerful. In my report, I mentioned small 3-4 liner summaries to conclude why to buy a particular stock. Let us see the common types of data we encounter and how to tell stories from those, by selecting the best fit charts.Commonly encountered types of data:1. Textual Data When data is found in this form, it’s usually good to be finding how often a word has been used or what the sentiment of the text is. Stories can be told best using this form of data.One of the best suited visualizations for textual data is the WordCloud. The wordcloud brings the more frequent ones to the center and enlarges them, giving us a clear picture of what the general idea of the text depicts.For example, the wordcloud in this article displayed above gives a representation of twitter dataset. It shows that love is the most frequent positive term used in the tweets. 2. Mixed DataWhen our data consists of numeric or any other variety of formats, we need to know which ones are important and give us better insights from our dataset.The preferred visual for this kind of data can vary; here I will show you how to use facet grids for the data. I will be using the Titanic Passenger Data. As this plot shows us, females and first-class passengers tend to have a higher survival chance than men who are a part of the crew or lower boarding classes.Isn’t that what had really happened on the Titanic?Another way to visualize this kind of data is by trying a multivariate plot. The dataset in use for this plot is the Car Performance and Specifications dataset.  Here we can see how Cars that have a heavier built are slower than the ones with lighter bodies. Makes sense, right? 3. Numeric DataWhen we encounter this kind of data, we’re usually looking for trends or lines that depict numbers. The visual that would suit numeric data best would be a line or a step graph.Here, we can very clearly see the rise of prices at a local attraction for adults and children. See how easy it is to see the growth at each year interval? 4. StocksOne of the datasets that we also encounter are related to stocks. Stock market data is primarily a time series data of numeric values, but as a trader or an investor, I would like to understand each date and drop carefully.The most visually captivating charts in this regard is the Candlestick chart. 
Here, we take the example of Tesla’s stocks. The candlestick charts can be used to maneuver across each date and see the lows and highs of stocks individually. This could help us take better investment decisions based on current or past market trends.As the graph shows us, February 2016 was a drop for Tesla’s stocks. We could now use this information to understand other market conditions and economic situations to make decisions about their stock. 5. Geographic DataWhen we have data pertaining to specific locations and areas, we use maps to add clarity and meaning to our analysis. In this example, we can see how countries fared at and after the 2002 World Cup. Germany has scored the maximum number of goals, being one of the most dominant teams in world football ever since. Often, we would be questioned about how our stories and visuals can work or help when it’s time to create mathematical models. During all stages of predictive modeling, storytelling could be a vital addition to your analysis.Let us understand the basic steps involved in creating models out of our data and go through telling stories within them. 1. Data ExplorationThe first step of model building is understanding your data. I’ll give you instances and show you how you can explore your data without computing complex statistics.Let’s consider a dataset on Wine Quality. This is the structure of the dataset is as follows Here, we can see the associated summary statistics of the dataset in use. So, if we need to see whether there is any correlation between alcohol volumes and wine qualities, how do we do it?We could either compute Pearson’s ‘r’. It would help us in building a model, but would not help us in analyzing much.This shows a very strong correlation between Alcohol content and wine quality. But does it tell you anything else?Ideally, it doesn’t. So, what does?Let’s see how we can visualize these and tell a lot more from them.First, we’ll begin by seeing how Wine Quality relates to Alcohol content.Here, we can see that the higher alcohol volumes relate to better wine qualities and it helps us come to a better understanding of our data. We can also spot outliers better in this scenario.Next, would you wonder how acid contents in your wine affect its quality? This would be one way to visualise the effects of acid. As the Violin Plot expands horizontally, it shows that there are higher numbers of data points within those areas. 2. Feature VisualizingAfter you generate features, how do you see how well one is predicting?Graphs tell us how far away our predicted points are from our fitted line.Another example where we might have to visualize newly created visuals is the Principal Component Analysis. If you want to get an in-depth understanding of PCA, you can go through this article.This is the Iris dataset found in RStudio.When we run the principal component analysis on this dataset, we find these statistics. Although when we plot this, we find that the resulting visual is much more informative than the statistics.3. Model Creation and ComparisonComing to the model creation phase, we usually find the need to understand how our data is being fitted.This is a model that predicts whether the car should go fast or slow, based on the grade of the road and bumpiness.As you can see, the decision boundary clearly classifies most of the data but an accuracy of 88.21% doesn’t tell much of a story. Here we can even see how far the misclassified points are from the decision boundary.We can also compare certain algorithms and techniques by looking at their decision boundaries as we did above.Another example using the Iris dataset is shown below. Here, there’s not much information to derive valuable insights about our model.To learn more about Support Vector Machines, you can go through this article.On the other hand, this plot shows us a clear classification boundary where the Species separate from each other. Now that you know the scenarios where we can use story telling to explain our point, I will give you a few practical tips when you take this up on your own. Storytelling is more than what it has been used for. It can uncover insights from your data that you might have missed before. Relations between features and data that numbers can never clearly depict, can be shown using stories and charts.In this piece, we’ve elaborated on how stories are used in almost all avenues to explain a detail better. Starting from how they’re used in the steps of model building, we’ve gradually gone on to which charts suit specific data types well.Hope you had a great time reading the article. Eager to hear your data stories!",https://www.analyticsvidhya.com/blog/2017/10/art-story-telling-data-science/
8 Essential Tips for People starting a Career in Data Science,Learn everything about Analytics|Introduction|End Notes,"1. Choose the right role|2. Take up a Course and Complete it|3. Choose a Tool / Language and stick to it|4. Join a peer group|5. Focus on practical applications and not just theory|6. Follow the right resources|7. Work on your Communication skills|8. Network, but don’t waste too much time on it!|Learn, engage, compete, and get hired!|Related Articles|The Art of Story Telling in Data Science and how to create data stories?|Introductory guide to Linear Optimization in Python (with TED videos case study)|
Faizan Shaikh
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Learning data science can be intimidating. Specially so, when you are just starting your journey. Which tool to learn – R or Python? What techniques to focus on? How much statistics to learn? Do I need to learn to code? These are some of the many questions you need to answer as part of your journey.That is why I thought that I would create this guide, which could help people starting in Analytics or Data Science. The idea was to create a simple, not very long guide which can set your path to learn data science. This guide would set a framework which can help you learn data science through this difficult and intimidating period.Just follow through these tips, and enroll in the below courses to get a head start in your data science journey!So let’s get started! There are a lot of varied roles in data science industry. A data visualization expert, a machine learning expert, a data scientist, data engineer etc are a few of the many roles that you could go into. Depending on your background and your work experience, getting into one role would be easier than another role. For example, if you a software developer, it would not be difficult for you to shift into data engineering. So, until and unless you are clear about what you want to become, you will stay confused about the path to take and skills to hone.What to do, if you are not clear about the differences or you are not sure what should you become? I few things which I would suggest are:Here is a descriptive comparison done by Analytics Vidhya a few months back on what is it like being a Data Scientist vs Data Engineer vs Statistician. I’m sure it will help you reach your decision.A point to keep in mind when choosing a role: don’t just hastily jump on to a role. You should first understand clearly what the field requires and prepare for it. Now that you have decided on a role, the next logical thing for you is to put in dedicated effort to understand the role. This means not just going through the requirements of the role. The demand for data scientists is big so thousands of courses and studies are out there to hold your hand, you can learn whatever you want to. Finding material to learn from isn’t a hard call but learning it may become if you don’t put efforts.What you can do is take up a MOOC which is freely available, or join an accreditation program which should take you through all the twists and turns the role entails. The choice of free vs paid is not the issue, the main objective should be whether the course clears your basics and brings you to a suitable level, from which you can push on further.When you take up a course, go through it actively. Follow the coursework, assignments and all the discussions happening around the course. For example, if you want to be a machine learning engineer, you can take up Machine learning by Andrew Ng. Now you have to diligently follow all the course material provided in the course. This also means the assignments in the course, which are as important as going through the videos. Only doing a course end to end will give you a clearer picture of the field. As I mentioned before, it is important for you to get an end-to-end experience of whichever topic you pursue. A difficult question which one faces in getting hands-on is which language/tool should you choose?This would probably be the most asked question by beginners. The most straight-forward answer would be to choose any of the mainstream tool/languages there is and start your data science journey. After all, tools are just means for implementation; but understanding the concept is more important.Still the question remains, which would be a better option to start with? There are various guides / discussions on the internet which address this particular query. The gist is that start with the simplest of language or the one with which you are most familiar with. if you are not as well versed with coding, you should prefer GUI based tools for now. Then as you get a grasp on the concepts, you can get your hands on with the coding part.You can learn Python for Data Science here. Now that you know that which role you want to opt for and are getting prepared for it, the next important thing for you to do would be to join a peer group. Why is this important? This is because a peer group keeps you motivated. Taking up a new field may seem a bit daunting when you do it alone, but when you have friends who are alongside you, the task seems a bit easier.The most preferable way to be in a peer group is to have a group of people you can physically interact with.  Otherwise you can either have a bunch of people over the internet who share similar goals, such as joining a Massive online course and interacting with the batch mates.Even if you don’t have this kind of peer group, you can still have a meaningful technical discussion over the internet. There are online forums which give you this kind of environment. I will list a few of them: While undergoing courses and training, you should focus on the practical applications of things you are learning. This would help you not only understand the concept but also give you a deeper sense on how it would be applied in reality.A few tips you should do when following a course:Create your first Time Series Forecast using Python here. To never stop learning, you have to engulf each and every source of knowledge you can find. The most useful source of this information is blogs run by most influential Data Scientists. These Data Scientists are really active and update the followers on their findings and frequently post about the recent advancement in this field.Read about data science every day and make it a habit to be updated with the recent happenings. But there may be many resources, influential data scientists to follow, and you have to be sure that you don’t follow the incorrect practices. So it is very important to follow the right resources.Here is a list of Data Scientists that you can follow. People don’t usually associate communication skills with rejection in data science roles. They expect that if they are technically profound, they will ace the interview. This is actually a myth. Ever been rejected within an interview, where the interviewer said thank you after listening to your introduction?Try this activity once; make your friend with good communication skills hear your intro and ask for honest feedback. He will definitely show you the mirror!Communication skills are even more important when you are working in the field. To share your ideas to a colleague or to prove your point in a meeting, you should know how to communicate efficiently. Initially, your entire focus should be on learning. Doing too many things at initial stage will eventually bring you up to a point where you’ll give up.Gradually, once you have got a hang of the field, you can go on to attend industry events and conferences, popular meetups in your area, participate in hackathons in your area – even if you know only a little. You never know who, when and where will help you out!Actually, a meetup is very advantageous when it comes down to making your mark in the data science community. You get to meet people in your area who work actively in the field, which provides you networking opportunities along with establishing a relationship with them will in turn help you advance your career heavily. A networking contact might: The demand of data science is huge and employers are investing significant time and money in Data Scientists. So taking the right steps will lead to an exponential growth. This guide provides tips that can get you started and help you to avoid some costly mistakes.If you went through a similar experience in the past and want to share this with the community, do comments below! ",https://www.analyticsvidhya.com/blog/2017/10/tips-people-starting-career-data-science/
Introductory guide to Linear Optimization in Python (with TED videos case study),"Learn everything about Analytics|Introduction|Table of Contents||Introduction to Linear Optimization|The Problem – Creating the Watch List for TED videos
|Step 1 – Import relevant packages|Step 2 – Create a dataframe for TED talks|Step 3 – Set up the Linear Optimization Problem|Step 4 – Convert the Optimization results into an interpretable format|End Notes","Share this:|Like this:|Related Articles|8 Essential Tips for People starting a Career in Data Science|25 Questions to test a Data Scientist on Image Processing|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Data Science & Machine Learning are being used by organizations to solve a variety of business problems today.  In order to create a real business impact, an important consideration is to bridge the gap between the data science pipeline and business decision making pipeline.The outcome of data science pipeline is uaully predictions, patterns and insights from data (typically without any notion of constraints) but that alone is insufficient for business stakeholders to take decisions. Data science output has to be fed into the business decision making pipeline which involves some sort of optimization involving constraints and decision variables which model key aspects of the business.For example, if you are running a Super Market chain – your data science pipeline would forecast the expected sales. You would then take those inputs and create an optimised inventory / sales strategy.In this article, we will show one such example of Linear optimization for selecting which TED videos to watch. Among optimization techniques, Linear Optimization using the Simplex Method is considered one of the most powerful ones and has been rated as one of the Top 10 algorithms of the 20th century. As data science practitioners, it is important to have hands-on knowledge in implementing Linear Optimization and this blog post is to illustrate its implementation using Python’s PuLP package.To make things interesting & simpler to understand, we will learn this optimization technique by applying it on a practical, day-to-day problem. Having said that, what we learn is applicable to a variety of business problems as well.Note: This article assumes you have a basics knowledge of linear programming. You can go through this article if you want to review the topic. TED is a nonprofit devoted to spreading ideas. TED began in 1984 as a conference where Technology, Entertainment and Design converged, and today covers almost all topics — from science to business to global issues — in more than 100 languages. TED talks are delivered by experts passionate about work in their chosen domains and have a wealth of information.Now, for the purpose of this blog post, imagine a situation where one is interested to create their watch list of the most popular TED talks given their constraints (time that can be allotted to viewing and the number of talks). We will see how to implement the Python program to help us create the watchlist in the optimal manner.The code of the article can be found here. Screenshots from my Jupyter notebook are shown below: PuLP is a free open source software written in Python. It is used to describe optimisation problems as mathematical models. PuLP can then call any of numerous external LP solvers (CBC, GLPK, CPLEX, Gurobi etc) to solve this model and then use python commands to manipulate and display the solution. By default, CoinMP solver is bundled with PuLP. Dataset having all the TED talks (2550) is downloaded from Kaggle and read into a dataframe. A subset of relevant columns is selected and the resulting dataset has the following details – Index of the talk, Name of the talk, TED Event Name, Talk duration (in minutes), Number of Views (Proxy for Popularity of the talk) Start with defining the LP Object. The prob variable is created to contain the problem formulationStep 3.1: Create the decision variablesIterate over each row of the data frame to create the decision variables, such that each talk becomes one decision variable. Since each talk can either be selected or not selected as part of the final watch list, the decision variable is binary in nature (1=Selected, 0=Not Selected)Step 3.2: Define the Objective FunctionThe objective function is the sum over all rows of the views for each talk. The views serve as a proxy for the popularity of the talk and so in essence we are trying to maximize the views (popularity) by selecting appropriate talks (decision variables)Step 3.3: Define the ConstraintsIn the problem, we have 2 constraints:a) We only have fixed amount of total time that can be allocated to view the talksb) We don’t want to view more than a certain number of talks to avoid information overloadStep 3.4: The Final Format (for problem formulation)The final format of the problem formulated is written out into a .lp file. This will list the objective function, the decision variables and the constraints imposed on the problem.Step 3.5: The Actual OptimizationThe actual optimization is a single line of code that calls ‘prob.solve’. Assert statement is inserted to ascertain whether an optimal result was obtained for the problem. The optimization results which indicates the specific decision variables (talks) that were selected to maximize the outcome has to be converted into a format of a watch list, as shown below: This article provides an example of utilizing Linear Optimization techniques available in Python to solve the everyday problem of creating video watch list. The concepts learned are also applicable in more complex business situations involving thousands of decision variables and many different constraints.Every data science practitioner needs to add “Optimization techniques” to their body of knowledge so that they can use advanced analytics to solve real world business problems and this article is intended to help you take the first step in that direction. Karthikeyan Sankaran is currently a Director at LatentView Analytics which provides solutions at the intersection of Business, Technology & Math to business problems across a wide range of industries. Karthik has close to two decades of experience in the Information Technology industry having worked in multiple roles across the space of Data Management, Business Intelligence & Analytics.This story was received as part of “Blogathon” contest on Analytics Vidhya. Karthikeyan’s entry was one of the winning entries in the competition.",https://www.analyticsvidhya.com/blog/2017/10/linear-optimization-in-python/
25 Questions to test a Data Scientist on Image Processing,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Introductory guide to Linear Optimization in Python (with TED videos case study)|25 Questions to test a Data Scientist on Support Vector Machines|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Extracting useful information from unstructured data has always been a topic of huge interest in the research community. One such example of unstructured data is an image, and analysis of image data has applications in various aspects of business.This skilltest is specially designed for you to test your knowledge on the knowledge on how to handle image data, with an emphasis on image processing. More than 300 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboard for the participants who took the test. Here are some resources to get in depth knowledge in the subject. 1) Match the following image formats to their correct number of channelsNone
A) RGB -> I, GrayScale-> III
B) RGB -> IV, GrayScale-> II
C) RGB -> III, GrayScale -> I
D) RGB -> II, GrayScale -> ISolution: CGrayscale images have one number per pixel, and are stored as an m × n matrix, whereas Color images  have 3 numbers per pixel – red, green, and blue brightness (RGB) 2) Suppose you have to rotate an image. Image rotation is nothing but multiplication of image by a specific matrix to get a new transformed image. For simplicity, we consider one point in the image to rotate with co-ordinates as (1, 0) to a co-ordinate of (0, 1), which of the following matrix would we have to multiply with?A) 
B) 
C) 
D) Solution: CThe calculation of would be like this; 3) [True or False] To blur an image, you can use a linear filterA) TRUE
B) FALSESolution: BBlurring compares neighboring pixels in a filter and smooth them. For this, you cannot use a linear filter. 4) Which of the following is a challenge when dealing with computer vision problems?A) Variations due to geometric changes (like pose, scale etc)
B) Variations due to photometric factors (like illumination, appearance etc)
C) Image occlusion
D) All of the aboveSolution: DAll the above mentioned options are challenges in computer vision 5) Suppose we have an image given below.Our task is to segment the objects in the image. A simple way to do this is to represent the image in terms of the intensity of pixels and the cluster them according to the values. On doing this, we got this type of structure.A) 1
B) 2
C) 3
D) 4Solution: CThree clusters will be formed; points in the circle, points in the square and the points excluding both of these objects 6) In this image, you can find an edge labelled in the red region. Which form of discontinuity creates this kind of edge?A) Depth Discontinuity
B) Surface color Discontinuity
C) Illumination discontinuity
D) None of the aboveSolution: AThe chair and wall are far from each other, causing an edge in the image. 7) Finite difference filters in image processing are very susceptible to noise. To cope up with this, which of the following methods can you use so that there would be minimal distortions by noise?A) Downsample the image
B) Convert the image to grayscale from RGB
C) Smooth the image
D) None of the aboveSolution: CSmoothing helps in reducing noise by forcing pixels to be more like their neighbours 8) Consider and image with width and height as 100×100. Each pixel in the image can have a color from Grayscale, i.e. values. How much space would this image require for storing?Note: No compression is done.A) 2,56,00,000
B) 25,60,000
C) 2,56,000
D) 8,00,000
E) 80,000
F) 8,000Solution: EThe answer will be 8x100x100 because 8 bits will be required to represent a number from 0-256 9) [True or False] Quantizing an image will reduce the amount of memory required for storage.A) TRUE
B) FALSESolution: AThe statement given is true. 10) Suppose we have a grayscale image, with most of the values of pixels being same. What can we use to compress the size of image?A) Encode the pixels with same values in a dictionary
B) Encode the sequence of values of pixels
C) No compression can be doneSolution: AEncoding same values of pixels will greatly reduce the size for storage 11) [True or False] JPEG is a lossy image compression techniqueA) TRUE
B) FALSESolution: AThe reason for JPEG being a lossy compression technique is because of the use of quantization. 12) Given an image with only 2 pixels and 3 possible values for each pixel, what is the number of possible image histograms that can be formed?A) 3
B) 6
C) 9
D) 12Solution: CThe permutations possible of the histograms would be 9. 13) Suppose we have a 1D image with values as Now we apply average filter on this image of size 3. What would be the value of the last second pixel? A) The value would remain the same
B) The value would increase by 2
C) The value would decrease by 2
D) None of the above
Solution: A(8+5+2)/3 will become 5. So there will be no change. 14) fMRI (Functional magnetic resonance imaging) is a technology where volumetric scans of the brain are acquired while the subject is performing some cognitive tasks over time. What is the dimensionality of fMRI output signals?A) 1D
B) 2D
C) 3D
D) None of the aboveSolution: D
The question itself mentions “volumetric scans” over time, so it would be a series of 3D scans 15) Which of the following methods is used as a model fitting method for edge detection?A) SIFT
B) Difference of Gaussian detector
C) RANSAC
D) None of the aboveSolution: CRANSAC is used to find the best fit line in edge detection 16)Suppose we have an image which is noisy. This type of noise in the image is called salt-and-pepper noiseA) TRUE
B) FALSESolution: AMedian filter technique helps reduce noise to a good enough extent 17) If we convolve an image with the matrix given below, what would be the relation between the original and modified image?Solution: AI would suggest you to try this yourself and see the result! 18) Which of the following is a correct way to sharpen an image?A)B)C)D) None of the aboveSolution: BOption B gives a correct way to sharpen an image 19) Below given images are two operations performed on a signal. Can you identify which is which?Solution: ACorrelation and convolution are two different methods with give different result. Convolution defines how much the signals overlap, whereas correlation tries to find the relation between the signals 20) [True or False] By using template matching along with cross correlation, you can build a vision system for TV remote controlA) TRUE
B) FALSESolution: AThis is a excellent example of cross correlation in computer vision. Refer paper “Computer Vision for Interactive Computer Graphics,” W.Freeman et al, IEEE Computer Graphics and Applications 21) Suppose you are creating a face detector in the wild. Which of the following features would you select for creating a robust facial detector?1. Location of iris, eyebrow and chin
2. Boolean feature: Is the person smiling or not
3. Angle of orientation of face
4. Is the person sitting or standingA) 1, 2
B) 1, 3
C) 1, 2, 3
D) 1, 2, 3, 4Solution: BOptions 1, 3 would be relevant features for the problem, but 2, 4 may not be 22) Which of the following is example of low level feature in an image?A) HOG
B) SIFT
C) HAAR features
D) All of the aboveSolution: DAll the above are examples of low-level features 23) In RGBA mode of color representation, what does A represent?A) Depth of an image
B) Intensity of colors
C) Opacity of an image
D) None of the aboveSolution: COpacity can be mentioned by introducing it as a fourth parameter in RGB 24) In Otsu thresholding technique, you remove the noise by thresholding the points which are irrelevant and keeping those which do not represent noise.In the image given, at which point would you threshold on?A) A
B) B
C) C
D) DSolution: BLine B would catch most of the noise in the image. 25) Which of the following data augmentation technique would you prefer for an object recognition problem?`
A) Horizontal flipping
B) Rescaling
C) Zooming in the image
D) All of the aboveSolution: DAll the mentioned techniques can be used for data augmentation. Below is the distribution of the scores of the participants:You can access the scores here. More than a hundered people participated in the skill test and the highest score obtained was a 22. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.",https://www.analyticsvidhya.com/blog/2017/10/image-skilltest/
25 Questions to test a Data Scientist on Support Vector Machines,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution| |End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|25 Questions to test a Data Scientist on Image Processing|Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R)|
Ankit Gupta
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"You can think of machine learning algorithms as an armory packed with axes, sword and blades. You have various tools, but you ought to learn to use them at the right time. As an analogy, think of ‘Regression’ as a sword capable of slicing and dicing data efficiently, but incapable of dealing with highly complex data. On the contrary, ‘Support Vector Machines’ is like a sharp knife – it works on smaller datasets, but on them, it can be much more stronger and powerful in building models.This skilltest was specially designed for you to test your knowledge on SVM techniques and its applications. More than 550 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboard for the participants who took the test. Here are some resources to get in depth knowledge in the subject. Question Context: 1 – 2Suppose you are using a Linear SVM classifier with 2 class classification problem. Now you have been given the following data in which some points are circled red that are representing support vectors.1) If you remove the following any one red points from the data. Does the decision boundary will change? A) Yes
B) NoSolution: AThese three examples are positioned such that removing any one of them introduces slack in the constraints. So the decision boundary would completely change. 2) [True or False] If you remove the non-red circled points from the data, the decision boundary will change?A) True
B) FalseSolution: BOn the other hand, rest of the points in the data won’t affect the decision boundary much. 3) What do you mean by generalization error in terms of the SVM?A) How far the hyperplane is from the support vectors
B) How accurately the SVM can predict outcomes for unseen data
C) The threshold amount of error in an SVMSolution: BGeneralisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data. 4) When the C parameter is set to infinite, which of the following holds true?A) The optimal hyperplane if exists, will be the one that completely separates the data
B) The soft-margin classifier will separate the data
C) None of the aboveSolution: AAt such a high level of misclassification penalty, soft margin will not hold existence as there will be no room for error. 5) What do you mean by a hard margin?A) The SVM allows very low error in classification
B) The SVM allows high amount of error in classification
C) None of the aboveSolution: AA hard margin means that an SVM is very rigid in classification and tries to work extremely well in the training set, causing overfitting. 6) The minimum time complexity for training an SVM is O(n2). According to this fact, what sizes of datasets are not best suited for SVM’s?A) Large datasets
B) Small datasets
C) Medium sized datasets
D) Size does not matterSolution: ADatasets which have a clear classification boundary will function best with SVM’s. 7) The effectiveness of an SVM depends upon:A) Selection of Kernel
B) Kernel Parameters
C) Soft Margin Parameter C
D) All of the aboveSolution: DThe SVM effectiveness depends upon how you choose the basic 3 requirements mentioned above in such a way that it maximises your efficiency, reduces error and overfitting. 8) Support vectors are the data points that lie closest to the decision surface.A) TRUE
B) FALSESolution: AThey are the points closest to the hyperplane and the hardest ones to classify. They also have a direct bearing on the location of the decision surface. 9) The SVM’s are less effective when:A) The data is linearly separable
B) The data is clean and ready to use
C) The data is noisy and contains overlapping pointsSolution: CWhen the data has noise and overlapping points, there is a problem in drawing a clear hyperplane without misclassifying. 10) Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?A) The model would consider even far away points from hyperplane for modeling
B) The model would consider only the points close to the hyperplane for modeling
C) The model would not be affected by distance of points from hyperplane for modeling
D) None of the aboveSolution: BThe gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well. 11) The cost parameter in the SVM means:A) The number of cross-validations to be made
B) The kernel to be used
C) The tradeoff between misclassification and simplicity of the model
D) None of the aboveSolution: CThe cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. It is also simply referred to as the cost of misclassification. 12)Suppose you are building a SVM model on data X. The data X can be error prone which means that you should not trust any specific data point too much. Now think that you want to build a SVM model which has quadratic kernel function of polynomial degree 2 that uses Slack variable C as one of it’s hyper parameter. Based upon that give the answer for following question.What would happen when you use very large value of C(C->infinity)?Note: For small C was also classifying all data points correctly
A) We can still classify data correctly for given setting of hyper parameter C
B) We can not classify data correctly for given setting of hyper parameter C
C) Can’t Say
D) None of theseSolution: AFor large values of C, the penalty for misclassifying points is very high, so the decision boundary will perfectly separate the data if possible. 13) What would happen when you use very small C (C~0)?A) Misclassification would happen
B) Data will be correctly classified
C) Can’t say
D) None of theseSolution: AThe classifier can maximize the margin between most of the points, while misclassifying a few points, because the penalty is so low. 14) If I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on validation set, what should I look out for?A) Underfitting
B) Nothing, the model is perfect
C) OverfittingSolution: CIf we’re achieving 100% training accuracy very easily, we need to check to verify if we’re overfitting our data. 15) Which of the following are real world applications of the SVM?A) Text and Hypertext Categorization
B) Image Classification
C) Clustering of News Articles
D) All of the aboveSolution: DSVM’s are highly versatile models that can be used for practically all real world problems ranging from regression to clustering and handwriting recognitions. Question Context: 16 – 18Suppose you have trained an SVM with linear decision boundary after training SVM, you correctly infer that your SVM model is under fitting.16) Which of the following option would you more likely to consider iterating SVM next time?A) You want to increase your data points
B) You want to decrease your data points
C) You will try to calculate more variables
D) You will try to reduce the featuresSolution: CThe best option here would be to create more features for the model. 17) Suppose you gave the correct answer in previous question. What do you think that is actually happening?1. We are lowering the bias
2. We are lowering the variance
3. We are increasing the bias
4. We are increasing the variance
A) 1 and 2
B) 2 and 3
C) 1 and 4
D) 2 and 4Solution: C
Better model will lower the bias and increase the variance 18) In above question suppose you want to change one of it’s(SVM) hyperparameter so that effect would be same as previous questions i.e model will not under fit? A) We will increase the parameter C
B) We will decrease the parameter C
C) Changing in C don’t effect
D) None of theseSolution: AIncreasing C parameter would be the right thing to do here, as it will ensure regularized model 19) We usually use feature normalization before using the Gaussian kernel in SVM. What is true about feature normalization?1. We do feature normalization so that new feature will dominate other
2. Some times, feature normalization is not feasible in case of categorical variables
3. Feature normalization always helps when we use Gaussian kernel in SVMA) 1
B) 1 and 2
C) 1 and 3
D) 2 and 3Solution: BStatements one and two are correct. Question Context: 20-22Suppose you are dealing with 4 class classification problem and you want to train a SVM model on the data for that you are using One-vs-all method. Now answer the below questions?20) How many times we need to train our SVM model in such case?A) 1
B) 2
C) 3
D) 4Solution: DFor a 4 class problem, you would have to train the SVM at least 4 times if you are using a one-vs-all method. 21) Suppose you have same distribution of classes in the data. Now, say for training 1 time in one vs all setting the SVM is taking 10 second. How many seconds would it require to train one-vs-all method end to end?A) 20
B) 40
C) 60
D) 80Solution: B
It would take 10×4 = 40 seconds 22) Suppose your problem has changed now. Now, data has only 2 classes. What would you think how many times we need to train SVM in such case?A) 1
B) 2
C) 3
D) 4Solution: ATraining the SVM only one time would give you appropriate results Question context: 23 – 24Suppose you are using SVM with linear kernel of polynomial degree 2, Now think that you have applied this on data and found that it perfectly fit the data that means, Training and testing accuracy is 100%.23) Now, think that you increase the complexity(or degree of polynomial of this kernel). What would you think will happen?A) Increasing the complexity will overfit the data
B) Increasing the complexity will underfit the data
C) Nothing will happen since your model was already 100% accurate
D) None of theseSolution: AIncreasing the complexity of the data would make the algorithm overfit the data. 24) In the previous question after increasing the complexity you found that training accuracy was still 100%. According to you what is the reason behind that?1. Since data is fixed and we are fitting more polynomial term or parameters so the algorithm starts memorizing everything in the data
2. Since data is fixed and SVM doesn’t need to search in big hypothesis spaceA) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the given statements are correct. 25) What is/are true about kernel in SVM?1. Kernel function map low dimensional data to high dimensional space
2. It’s a similarity functionA) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the given statements are correct. Below is the distribution of the scores of the participants:You can access the scores here. More than 350 people participated in the skill test and the highest score obtained was a 25. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.",https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/
Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R),Learn everything about Analytics|Introduction|Table of Contents|Setting up the System|Trading with Bollinger Bands|Analyzing the Volatility of Bank Stocks||Prediction of Stock Prices|End Notes,"What are Bollinger Bands?|Aspects of Bollinger Bands|Keep it Clean!|Identification of Patterns|Learn, engage,compete, and get hired!|Share this:|Like this:|Related Articles|25 Questions to test a Data Scientist on Support Vector Machines|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)|
Guest Blog
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
","|Code Break up for Quandl and Quandl API key,|1. Signal: W – Bottoms|2. Signal: M-Tops",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Finding underlying patterns and taking decisions is very critical in Stock market. The same skill can be applied to many parallel domains. For example, I met some one who was doing the same thing with Cryptocurrency recently. Risk & Unemployment prediction in banks, customer churn in telecom and spend analysis are all examples of similar problems.That is why I decided to create this series of articles. By following this series, you will understand some of the techniques used in stock market. You can also apply them to the parallel domains I mentioned before.In the last article (Part I) , we started with descriptive analysis for comparison on stocks. In this post, we will emphasize on identifying patterns in order to know how a stock behaves. This behavior, as you will see later on, is very important for stock trading. In the latter part of the article, I will show how to predict stock prices using the conventional ARIMA (Auto-Regressive intensive Moving Average Method) methodology from Time Series Analysis and Regression Model.So lets get on with it!  I will mention below the packages necessary to get on hands on in this article. Make sure you have them set up in your system before you continue. Perhaps the most important thing when you get into stock market trading is to know what Bollinger Bands are. In this section, I will mention what they are and how they were discovered.The Bollinger Band was introduce by John Bollinger in 1980s. These Bands depict the volatility of stock as it increases or decreases. The bands are placed above and below the moving average line of the stocks. The wider the gap between the bands, higher is the degree of volatility.On the other hand, as the width within the band decreases, lower is the degree of volatility of the stock. At times, the width within the band is constant over a period of time, which shows the constant behavior of a certain stock over that period of time.There are three lines in the Bollinger Band,Note: SMA is Simple Moving Average, Standard Deviation, K and N period is usually set at 20 days. The upper and lower band are placed 2 units above and below respectively.Below image is the typical example for Bollinger Band. This shows the volatility of Axis Bank stock for the period of 1 year from 1st September, 2016 to 1st September, 2017. The gap was higher in the months of September till December.In this section, we will discuss few aspects of Bollinger Band. This information can be used in different stock trading.The study will discuss the above points along with the identification of popular patterns like “W – Bottoms” & “M – Tops” in Bollinger band. We will keep the data clean with tidyverse. In this section we will first download the data with the help of Quandl package and then manipulate the dataframe with tidyverse to get our desired dataset,If you haven’t gone through this in my previous post on Comparative Stock Analysis Vol-I , lets setup up the Quandl APIThere are patterns which are usualy seen in stock market data. These patterns (or signals) help us identify the behavior of stocks. Let us quickly understand the two most popular ones (W-Bottoms and M-Tops)A “W-Bottom” forms in a downtrend and involves two reaction lows. In particular, Bollinger looks for W-Bottoms where the second low is lower than the first but holds above the lower band. There are four steps to confirm Bollinger “W – Bottoms”,Below image is the “W-Bottoms” identification for BOB (Bank of Baroda). Both of the W-Bottoms are followed by strong northward move in February and May, 2017 respectively.An M-Top is similar to a double top. M-Tops are reversal signals from upward trend into a downward trend. The first high can be higher or lower than the second high. Initially there is a wave higher, which gets close to or move above the upper band. Then price will move downward to middle band and then continues northward journey, might or might not touch the upper band (at times it goes above the previous high) and then does not close above the upper band.Bollinger suggests looking for signs of non-confirmation when a security is making new highs.A non-confirmation occurs with three steps.Below is the image for MTOPS signal for SBI (State Bank Of India) stock in NSE over the period of one year starting from 1st sept, 2016 to 1st sept, 2017. Each of the MTops are followed by decline in prices in the months of Nov-Dec, May and August.If you want to get more information on the Bollinger Band and related identification patterns. Below are the links to the resources,Let us visualize the volatility (gap between the upper and lower band) and also try to identify the patterns / signals in our six selected bank stocks.In this section, we will predict the prices for two selected bank PNB and Axis Bank. In stock market, generally the prices are dynamic and depends on various factors like news, weather, public policy, interest rate. It is difficult to predict the stock price behavior as it depends on lots of factor. In order to get more accuracy in prediction, we’ve used two different approach to come to prediction.In the last post, we have seen that the stock prices is also dependent on the traded quantity, but direction can be either ways. In our analysis, we will take consideration of these movements. We will also analyze the random part of the stock price movement, so called white noise and will include in our prediction model.There is also available study on white noise on Analytics Vidhya by Tavish Srivastava.The following points are the steps to arrive at PredictionsNote: ggplot shows the prediction and actual prices. Predcition prices has the band for lower and upper limit.In this article, I have focused on Predictive Analysis of bank stocks. I have summarized a bit on Bollinger Bands, which probably is the most important topic in stock analysis. I have also walked you through the volatility of bank stocks and ways to see through this volatility.This ends our journey of comparative analysis of stock market data. I hope it will help you to make your mark in the world of stocks. Good Luck!Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning.",https://www.analyticsvidhya.com/blog/2017/10/comparative-stock-market-analysis-in-r-using-quandl-tidyverse-part-i/
Tutorial to deploy Machine Learning models in Production as APIs (using Flask),Learn everything about Analytics|Introduction|Table of Contents|1. Options to implement Machine Learning models|2. What are APIs?|3. Python Environment Setup & Flask Basics|4. Creating a Machine Learning Model|5. Saving Machine Learning Model : Serialization & Deserialization|6. Creating an API using Flask|End Notes,"About the Author|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R)|How to build your first Machine Learning model on iPhone (Intro to Apple’s CoreML)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I remember the initial days of my Machine Learning (ML) projects. I had put in a lot of efforts to build a really good model. I took expert advice on how to improve my model, I thought about feature engineering, I talked to domain experts to make sure their insights are captured. But, then I came across a problem!How do I implement this model in real life? I had no idea about this. All the literature I had studied till now focussed on improving the models. But I didn’t know what was the next step.This is why, I have created this guide – so that you don’t have to struggle with the question as I did. By end of this article, I will show you how to implement a machine learning model using Flask framework in Python.  Most of the times, the real use of our Machine Learning model lies at the heart of a product – that maybe a small component of an automated mailer system or a chatbot. These are the times when the barriers seem unsurmountable.For example, majority of ML folks use R / Python for their experiments. But consumer of those ML models would be software engineers who use a completely different stack. There are two ways via which this problem can be solved: In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software.You can read this article to understand why APIs are a popular choice amongst developers:Majority of the Big Cloud providers and smaller Machine Learning focussed companies provide ready-to-use APIs. They cater to the needs of developers / businesses that don’t have expertise in ML, who want to implement ML in their processes or product suites.One such example of Web APIs offered is the Google Vision APIAll you need is a simple REST call to the API via SDKs (Software Development Kits) provided by Google. Click here to get an idea of what can be done using Google Vision API.Sounds marvellous right! In this article, we’ll understand how to create our own Machine Learning API using Flask, a web framework in Python.NOTE:Flask isn’t the only web-framework available. There is Django, Falcon, Hug and many more. For R, we have a package called plumber. Viola! You wrote your first Flask application. As you have now experienced with a few simple steps, we were able to create web-endpoints that can be accessed locally.Using Flask, we can wrap our Machine Learning models and serve them as Web APIs easily. Also, if we want to create more complex web applications (that includes JavaScript *gasps*) we just need a few modifications.To follow the process on how we ended up with this estimator, refer this notebookWe’ll create a pipeline to make sure that all the preprocessing steps that we do are just a single scikit-learn estimator.To search for the best hyper-parameters (degree for Polynomial Features & alpha for Ridge), we’ll do a Grid Search:Our pipeline is looking pretty swell & fairly decent to go the most important step of the tutorial: Serialize the Machine Learning ModelIn computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment.In Python, pickling is a standard way to store objects and retrieve them as their original state. To give a simple example:When we load the pickle back:We can save the pickled object to a file as well and use it. This method is similar to creating .rda files for folks who are familiar with R Programming.NOTE: Some people also argue against using pickle for serialization(1). h5py could also be an alternative.We have a custom Class that we need to import while running our training, hence we’ll be using dill module to packup the estimator Class with our grid object.It is advisable to create a separate training.py file that contains all the code for training the model (See here for example).So our model will be saved in the location above. Now that the model is pickled, creating a Flask wrapper around it would be the next step.Before that, to be sure that our pickled file works fine – let’s load it back and do a prediction:Since, we already have the preprocessing steps required for the new incoming data present as a part of the pipeline, we just have to run predict(). While working with scikit-learn, it is always easy to work with pipelines.Estimators and pipelines save you time and headache, even if the initial implementation seems to be ridiculous. Stitch in time, saves nine!We’ll keep the folder structure as simple as possible:There are three important parts in constructing our wrapper function, apicall():HTTP messages are made of a header and a body. As a standard, majority of the body content sent across are in json format. We’ll be sending (POST url-endpoint/) the incoming data as batch to get predictions.(NOTE: You can send plain text, XML, csv or image directly but for the sake of interchangeability of the format, it is advisable to use json)Once done, run: gunicorn --bind 0.0.0.0:8000 server:appLet’s generate some prediction data and query the API running locally at https:0.0.0.0:8000/predictThere are a few things to keep in mind when adopting API-first approach:Next logical step would be creating a workflow to deploy such APIs out on a small VM. There are various ways to do it and we’ll be looking into those in the next article.Code & Notebooks for this article: pratos/flask_apiSources & Links: Prathamesh Sarang works as a Data Scientist at Lemoxo Technologies. Data Engineering is his latest love, turned towards the *nix faction recently. Strong advocate of “Markdown for everyone”. ",https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/
How to build your first Machine Learning model on iPhone (Intro to Apple’s CoreML),"Learn everything about Analytics|Introduction|Table of Contents|1. What is CoreML?|2. Setting up the System
|3. Case Study: Implementing a spam message classifier for iPhone|4. Pros and Cons of CoreML|End Notes","A little context for CoreML|Enter CoreML|What else does CoreML provide?|Converting your machine learning model into CoreML format|Integrating the model with our app|Pros|Cons|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)|Why we are so excited about DataHack Summit 2017?|
Mohd Sanad Zaki Rizvi
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",What is mlmodel?|About Spam Collection dataset |Build a basic model|What happened here?|Downloading the project|Adding a pre-trained model into your app|Compiling the model|Using the model in code|But why is tfidf() required?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The data scientist in me is living a dream – I can see top tech companies coming out with products close to the area I work on.If you saw the recent Apple iPhone X launch event, iPhone X comes with some really cool features like FaceID, Animoji, Augmented Reality out of box, which use the power of machine learning. The hacker in me wanted to get my hands dirty and figure out what it takes to build a system like that? When probed further, the answer was CoreML which is Apple’s official machine learning kit for developers. It works with iPhone, Macbook, Apple TV, Apple watch, in short, every Apple device.Another interesting announcement was that Apple has designed a custom GPU and an advanced processing chip A11 Bionic with a neural engine optimised for Machine Learning in the latest iPhones. With a powerful computing engine at its core, the iPhone will now be open to new avenues of machine learning and the significance of CoreML will only rise in the coming days.By end of this article, you will see what Apple CoreML is and why it is gaining the momentum. We will also look into implementation details of CoreML by building a message spam classification app for iPhone. We will finish off the article by objectively looking at pros and cons of the same.   Apple launched CoreML this year in their annual developer conference WWDC(which is equivalent of Google I/O conference) with a lot of hype. In order to better understand CoreML’s role, we have to know a bit of context. Interestingly this is not the first time that Apple has come out with a framework for machine learning on its devices. Last year it launched a bunch of libraries for the same:The point of difference was, one was optimized for CPU while the other was optimized for the GPU. The reason for this is sometimes during inference the CPU can be faster than the GPU. While during training almost every time GPU is faster.These multiple frameworks created a lot of confusion among developers and since they were quite close to the hardware(for high performance), they were difficult to program in. CoreML provides another layer of abstraction over the previous two libraries and gives an easy interface to achieve the same level of efficiency. Another benefit is that CoreML takes care of context switching between the CPU and GPU itself while your app is running. That is to say, for example, you have a memory heavy task that involves dealing with text (natural language processing), CoreML will automatically run it on the CPU whereas if you have compute heavy tasks like image classification, it will use the GPU. If you have both the functionalities in your app, it will also take care of that automatically so that you can get best of both the worlds. CoreML also comes with three libraries built on top of it :All of the above libraries, again are very easy to use and provide a simple interface to do a bunch of tasks. With the above libraries, the final structure of CoreML would look something like thisNotice that the above design gives a nice modular structure to your iOS application. You have different layers for different tasks and you can make use of them in a variety of ways (for example, using NLP with image classification in your app). You can read more about these libraries here : Vision, Foundation and GameplayKit. Well, that was enough theory for now, it is time to get our hands dirty! To make full use of the CoreML , you need the following requirements set up:Once you log in, you will have to verify your apple ID. You will receive the notification regarding the same on the device that is registered with your apple ID.Select “Allow” and type the given 6 digit passcode in the website Once you do this step, you will be shown a download option and you can download Xcode from there. Now that we have set up our system and all ready let’s move on to the implementation part! We will be looking at two important ways to utilize the power of CoreML by building them. Let’s start then!Now, one of the strengths of CoreML or rather should I say wise decision of its creators was to support conversion of trained machine learning models built in other popular frameworks like sklearn, caffe, xgboost etc. This didn’t alienate the data science community from trying out CoreML because they can experiment, train their models in their favourite environment and then simply import it to use in their iOS/MacOS app. The following are the frameworks that CoreML supports right out of the box:In order to make the conversion process simple, Apple designed its own open format for representing cross framework machine learning models called mlmodel. This model file contains a description of the layers in your model, the inputs and outputs, the class labels, and any preprocessing that needs to be done on the data. It also contains all the learned parameters (the weights and biases).The conversion flow looks like this :For this example, we will be building a spam message classifier in sklearn and then port the same model to CoreML.The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-encoded messages, tagged according to being legitimate (ham) or spam. You can download the dataset from here.We will be building a basic model using LinearSVC in sklearn. Also, I have used TF-IDF on the text of the message as a feature for the model. TF-IDF is a technique used in natural language processing that helps classify documents based on words that uniquely identify a document over other. If you would like to learn more about NLP and tf-idf, you can read this article. This would be the code for that: That builds our model. Let’s test it with a spam message,Interesting, our model works well! let’s add some cross-validation too,Now that we have built our model, we need to port it to .mlmodel format in order to make it compatible with CoreML. We will use the coremltools package that we installed earlier for that. The following code would convert our model into .mlmodel formatWe first import the coremltools package in python. Then we use one of the converters to convert our model, in this case, we used converters.sklearn because we have to convert a model built in sklearn. We then pass the model object, input variable name, and the output variable name in .convert(). We then set the parameters of the model to add more info about the inputs, outputs and finally call .save() to save our model file.When you double click on the model file, it should open in a Xcode window. As you can see, the model file shows details about the type of model, its inputs, and outputs, their types etc. I have highlighted all this information in the above figure. You can match the description with the ones we provided while converting to .mlmodel. That is how easy it is to import your model into CoreML. Now your model is into Apple ecosystem, that’s when the real fun starts!Note: The complete code file for this step can be found here. Read more about coremltools here and different types of converters available here.Now that we have trained our model and ported it to CoreML, let us use that model and build a spam classifier app for iPhone!We would be running our app on a simulator. A simulator is a software that shows how an app will look and work as if it was really running on the phone. This saves a lot of time because we can experiment with our code and fix all bugs before trying the app on an actual phone. Have a look at what the end product would be like:I have already built a basic UI for our app and it is available on GitHub. Use the following commands to get it up and running:This will open our project using Xcode. I have highlighted three main regions in the Xcode window :Let’s first run our app and see what happens. Click on the play button on the top left that will run our app in the simulator. Try typing some text in the box and clicking on the Predict button. What happens?For now, our app doesn’t do much it just prints whatever has been typed in the box.  This bit is fairly easy, Here is the whole process for reference :Before we can start making inferences from our model, we need to tell Xcode to compile the model during the build stage. For that follow the following steps:Now every time we run our app, Xcode will compile our machine learning model so that it can be used for making predictions.Any application that is to be developed for an Apple device is programmed in swift. For following this tutorial you don’t need to learn swift but if afterward it interests you and you want to go deeper, you can follow this tutorial.The above code checks whether the user has entered any message in the text box. If he has, it calculates the tfidf of the text by calling a function tfidf(). It then creates an object of our SpamMessageClassifier and then calls the .prediction() function. This is equivalent of .predict() in sklearn. It then displays an appropriate message based on the prediction.Remember that we trained our model based on tf-idf representation of text so our model expects the input in the same format. Once we get the message entered in the text box we are calling tfidf() function on it to do the same. Let’s write code for it, copy the following code just below the predictSpam() function:The above code finds the tfidf representation of the message entered in the text box for that it reads the original dataset file SMSSpamCollection.txt and returns the same. Once you save the program and re-run the simulator, your app should be working fine now. Like every library in development, it has its pros and cons. Let us state them explicitly. In this article, we learned more about CoreML and its application in building a machine learning app for iPhone. CoreML is a relatively new library and hence has its own share of pros and cons. A very useful feature provided here is it runs on the device locally thus giving more speed and providing data privacy. At the same time, it can’t be thought of as a full-fledged data scientist friendly library yet. We will have to wait and see how does it evolve in the coming releases.For those of you who got stuck at some step, all of the code for this article is available on GitHub.Also, if you want to explore CoreML in more depth these are some of the resources:",https://www.analyticsvidhya.com/blog/2017/09/build-machine-learning-iphone-apple-coreml/
Why we are so excited about DataHack Summit 2017?,Learn everything about Analytics|The DataHack Summit philosophy|Speakers @ DataHack Summit 2017|Talks @ DataHack Summit 2017|Areas of Focus|Buy your Tickets before they vanish!,"Share this:|Like this:|Related Articles|How to build your first Machine Learning model on iPhone (Intro to Apple’s CoreML)|Introduction to Pseudo-Labelling : A Semi-Supervised learning technique|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Recently I attended a Data Science conference – full with celebrated & experienced speakers, nice looking agenda having multiple tracks and attended by a large number of industry professionals.What I took away from the conference was 2 days of networking, bunch of exchanged visiting cards and may be one good session which was useful for the data scientist inside me to some degree. Most of the sessions talked about Analytics / Data Science from a very high level to be useful for me. The same use cases – Alexa using Deep Learning or Target being able to predict a pregnancy in advance or Google driverless cars being slapped in sessions after sessions!This is not the first time this happened to me – this actually happens almost every time I attend an event like this. I know there are many more practitioners who feel the same about Data Science events in India.This is exactly what we will change in DataHack Summit 2017 in Bengaluru November 9 – 11, 2017 DataHack Summit (DHS) is a conference created for the data science practitioner. It is a conference which celebrates the awesome work being done by data scientists across the globe and showcases it to the rest of the community. It is a conference where we talk data science, we breathe data science and we experience data science like never before.DataHack Summit is a festival for those who don’t see numbers as just numbers. It is a festival for those who see numbers as designs and patterns and trends, those who see and appreciate this art of dealing with data.DHS 2017 aims to show the bleeding edge, the horizon and the impact of data science to the professionals who perform it every day. It aims to inspire you with the new tools, techniques and applications by bringing the best in data science together. Our philosophy reflect directly in our speakers. All the speakers are Data Science practitioners and leaders.These are the people who have built scalable data science solutions and have solved real life problems using cutting edge tools and techniques. The spread of speakers include Dr. Kirk D. Borne who brings 30 years of data science wisdom to young Axel De Romblay, talking about Automated Machine Learning and the open source project he created.Check out the latest speakers for yourself on our speakers page. Every session, every talk, every workshop in DataHack Summit will be created with the same diligence you see in our articles. Hands on, practical knowledge delivered in simple and exciting manner.We will not have a session unless Analytics Vidhya team sees value in that session for themselves.We will make sure that you walk away with practical knowledge and industry use case when you come out of DataHack Summit 2017. Following will be the areas of focus of discussions at DataHack Summit 2017:The detailed agenda would be out in a few days. We have to thank our community for such an awesome response. The workshop tickets are sold out already. The regular tickets will sell out shortly as well. Make sure you book your tickets before they vanish.If you have any questions / suggestions, feel free to reach out to us at [email protected]See you at the DataHack Summit 2017!",https://www.analyticsvidhya.com/blog/2017/09/excited-datahack-summit-2017/
Introduction to Pseudo-Labelling : A Semi-Supervised learning technique,"Learn everything about Analytics|Introduction|Table of Contents|1. What is Semi-Supervised Learning (SSL) ?|2. How Unlabelled data can help?|3. Introduction Pseudo Labeling|4. Implementation of SSL
|5. Dependence of Sampling Rate|6. Applications of SSL||End Notes","1. Multimodal semi-supervised learning for image classification|2. SSL for detecting human trafficking|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Why we are so excited about DataHack Summit 2017?|6 Common Probability Distributions every data science professional should know|
Shubham Jain
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"We have made huge progress in solving Supervised machine learning problems. That also means that we need a lot of data to build our image classifiers or sales forecasters. The algorithms search patterns through the data again and again.But, that is not how human mind learns. A human brain does not require millions of data for training with multiple iterations of going through the same image for understanding a topic. All it needs is a few guiding points to train itself on the underlying patterns. Clearly, we are missing something in current machine learning approaches.Thankfully, there is a line of research which specifically caters to this question. Can we build a system capable of requiring minimal amount of supervision which can learn majority of the tasks on its own. In this article, I would like to cover one such technique called pseudo-labelling. I will give an intuitive explanation of what pseudo-labelling is and then provide a hands-on implementation of it.Are you ready?Note: I assume you have clarity on the basic topics of machine learning. If not, I would suggest you to go through a basic machine learning article first and then come back to this one.  Let’s say, we have a simple image classification problem. So, our training data consists of two labelled images as shown below.So, we need to classify images of eclipse from the non-eclipse images. But, the problem is that we need to build our model on just a training set of just two images.Therefore, in order to apply any supervised learning algorithm we need more data to build a robust model. To solve this purpose, we find a simple solution that we download some images from the web to increase our training data.But, for the supervised approach we also need labels for these images. So, we manually classify each image into a category as shown below.After running supervised algorithm on this data, our model will definitely out-perform the model just containing two images in the training data.But this approach is only valid for small purposes because human annotation to a large dataset can be very hard and expensive.So, to solve these type of problems, we define a different type of learning known as semi-supervised learning, which is used both labelled data (supervised learning) and unlabelled data (unsupervised learning).Source: linkTherefore, let us understand how unlabelled data can help to improve our model. Consider a situation as shown below.
You have only two data points belonging to two different categories, and the line drawn is the decision boundary of any supervised model. Now, let’s say we add some unlabelled data to this data as shown in the image below.Images source: link If we notice the difference between the above two images, you can say that after adding unlabelled data, the decision boundary of our model has become more accurate.So, the advantage of using unlabelled data are:Now, we have a basic understanding that what is semi-supervised learning. There are different techniques of applying SSL, in this article we will try to understand one such technique known as Pseudo Labeling. In this technique, instead of manually labeling the unlabelled data, we give approximate labels on the basis of the labelled data. Let’s make it simpler by breaking into steps as shown in the image below.
Source: linkI suppose, you understood the steps mentioned in the above image. So, the final model trained in the third step is used for the final predictions on the test data.For better understanding, I always prefer understanding a concept by its implementation on a real world problem. Here, we will be using Big Mart Sales problem from AV data hack platform. So, let’s get start by downloading the train and test file present in the data section.So, let’s get start by importing the basic libraries.Now, let’s read train and test file that we have downloaded and do some basic preprocessing in order to form modelling.Starting with different supervised learning algorithm, let us check which algorithm gives us the best results.We can see XGB gives us the best model performance. Note here, I have not tuned parameter of any algorithm for the simplicity of this article.Now, let’s us implement Pseudo-labelling, for this purpose I will be using test data as the unlabelled data.  This look quite complex, but you need not to worry about this as it is the same implementation of the method we learned above. So, copy the same code every time you need to perform pseudo labeling.So, now let’s us now check the results of pseudo labeling on the dataset.In this case, we a get rmse value which comes out to lesser than any of the supervised learning algorithm.If you have notice sample_rate was one of the parameter, which denotes the percentage of unlabelled data to be used as the pseudo labelled for the modelling purpose.Therefore, let’s us check the dependance of sample_rate on the performance of the pseudo labelling. In order to find out the dependence of sample_rate on the performance of the pseudo labelling, let us plot a graph between those two.Here, I am using only two algorithm to show you the dependence because of the time constraint, but you can try for other algorithms too.So, we can see that the rmse is minimum for a particular value of sample_rate, which is different for both the algorithm.Therefore, it is important to tune sample_rate in order to achieve better results while using pseudo labeling. In past, there are limited number of applications of semi-supervised learning, but currently there is lot of working going on in this field.Some applications which I found interesting are listed below.Generally, in image categorisation, the goal is to classify an image whether it belongs to the category or not. In this paper, not only images are used for modelling but the keywords associated with labelled and unlabelled images are also used to improve the classifier using semi-supervised learning.Source: linkHuman trafficking is one of the most atrocious crimes and among the challenging problems facing law enforcement which demands attention of global magnitude. Semi-supervised learning is to applied to use both labelled and unlabelled data in order to produce better results than the normal approaches.Source: linkI hope that now you have a understanding what semi-supervised learning is and how to implement it in any real world problem. Therefore, try to explore it further and learn other types of semi-supervised learning technique and share with the community in the comment section.You can find the full code of this article from my github repository.Also, did you find this article helpful? Please share your opinions / thoughts in the comments section below. ",https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/
6 Common Probability Distributions every data science professional should know,Learn everything about Analytics|Introduction|Table of Contents|Common Data Types|Types of Distributions|Relations between the Distributions|Test your knowledge|End Notes,"Bernoulli Distribution|Uniform Distribution|Binomial Distribution|Normal Distribution|Poisson Distribution|Exponential Distribution|Relation between Bernoulli and Binomial Distribution|Relation between Poisson and Binomial Distribution|Relation between Normal and Binomial Distribution & Normal and Poisson Distribution:|Relation between Exponential and Poisson Distribution:|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Introduction to Pseudo-Labelling : A Semi-Supervised learning technique|Comparative Stock Market Analysis in R using Quandl & tidyverse – Part I|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Welcome to the world of Probability in Data Science! Let me start things off with an intuitive example.Suppose you are a teacher at a university. After checking assignments for a week, you graded all the students. You gave these graded papers to a data entry guy in the university and tell him to create a spreadsheet containing the grades of all the students. But the guy only stores the grades and not the corresponding students.He made another blunder, he missed a couple of entries in a hurry and we have no idea whose grades are missing. Let’s find a way to solve this.One way is that you visualize the grades and see if you can find a trend in the data.The graph that you have plot is called the frequency distribution of the data. You see that there is a smooth curve like structure that defines our data, but do you notice an anomaly? We have an abnormally low frequency at a particular score range. So the best guess would be to have missing values that remove the dent in the distribution.This is how you would try to solve a real-life problem using data analysis. For any Data Scientist, a student or a practitioner, distribution is a must know concept. It provides the basis for analytics and inferential statistics.While the concept of probability gives us the mathematical calculations, distributions help us actually visualize what’s happening underneath.In this article, I have covered some important probability distributions which are explained in a lucid as well as comprehensive manner.Note: This article assumes you have a basic knowledge of probability. If not, you can refer this probability distributions.  Before we jump on to the explanation of distributions, let’s see what kind of data can we encounter. The data can be discrete or continuous.Discrete Data, as the name suggests, can take only specified values. For example, when you roll a die, the possible outcomes are 1, 2, 3, 4, 5 or 6 and not 1.5 or 2.45.Continuous Data can take any value within a given range. The range may be finite or infinite. For example, A girl’s weight or height, the length of the road. The weight of a girl can be any value from 54 kgs, or 54.5 kgs, or 54.5436kgs.Now let us start with the types of distributions. Let’s start with the easiest distribution that is Bernoulli Distribution. It is actually easier to understand than it sounds!All you cricket junkies out there! At the beginning of any cricket match, how do you decide who is going to bat or ball? A toss! It all depends on whether you win or lose the toss, right? Let’s say if the toss results in a head, you win. Else, you lose. There’s no midway.A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial. So the random variable X which has a Bernoulli distribution can take value 1 with the probability of success, say p, and the value 0 with the probability of failure, say q or 1-p.Here, the occurrence of a head denotes success, and the occurrence of a tail denotes failure.
Probability of getting a head = 0.5 = Probability of getting a tail since there are only two possible outcomes.The probability mass function is given by: px(1-p)1-x  where x € (0, 1).
It can also be written asThe probabilities of success and failure need not be equally likely, like the result of a fight between me and Undertaker. He is pretty much certain to win. So in this case probability of my success is 0.15 while my failure is 0.85Here, the probability of success(p) is not same as the probability of failure. So, the chart below shows the Bernoulli Distribution of our fight. Here, the probability of success = 0.15 and probability of failure = 0.85. The expected value is exactly what it sounds. If I punch you, I may expect you to punch me back. Basically expected value of any distribution is the mean of the distribution. The expected value of a random variable X from a Bernoulli distribution is found as follows:E(X) = 1*p + 0*(1-p) = pThe variance of a random variable from a bernoulli distribution is:V(X) = E(X²) – [E(X)]² = p – p² = p(1-p)There are many examples of Bernoulli distribution such as whether it’s going to rain tomorrow or not where rain denotes success and no rain denotes failure and Winning (success) or losing (failure) the game. When you roll a fair die, the outcomes are 1 to 6. The probabilities of getting these outcomes are equally likely and that is the basis of a uniform distribution. Unlike Bernoulli Distribution, all the n number of possible outcomes of a uniform distribution are equally likely.A variable X is said to be uniformly distributed if the density function is:The graph of a uniform distribution curve looks likeYou can see that the shape of the Uniform distribution curve is rectangular, the reason why Uniform distribution is called rectangular distribution.For a Uniform Distribution, a and b are the parameters. 
The number of bouquets sold daily at a flower shop is uniformly distributed with a maximum of 40 and a minimum of 10.Let’s try calculating the probability that the daily sales will fall between 15 and 30.The probability that daily sales will fall between 15 and 30 is (30-15)*(1/(40-10)) = 0.5Similarly, the probability that daily sales are greater than 20 is  = 0.667The mean and variance of X following a uniform distribution is:Mean -> E(X) = (a+b)/2Variance -> V(X) =  (b-a)²/12The standard uniform density has parameters a = 0 and b = 1, so the PDF for standard uniform density is given by: Let’s get back to cricket.  Suppose that you won the toss today and this indicates a successful event. You toss again but you lost this time. If you win a toss today, this does not necessitate that you will win the toss tomorrow. Let’s assign a random variable, say X, to the number of times you won the toss. What can be the possible value of X? It can be any number depending on the number of times you tossed a coin.There are only two possible outcomes. Head denoting success and tail denoting failure. Therefore, probability of getting a head = 0.5 and the probability of failure can be easily computed as: q = 1- p = 0.5.A distribution where only two outcomes are possible, such as success or failure, gain or loss, win or lose and where the probability of success and failure is same for all the trials is called a Binomial Distribution.The outcomes need not be equally likely. Remember the example of a fight between me and Undertaker? So, if the probability of success in an experiment is 0.2 then the probability of failure can be easily computed as q = 1 – 0.2 = 0.8.Each trial is independent since the outcome of the previous toss doesn’t determine or affect the outcome of the current toss. An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial.On the basis of the above explanation, the properties of a Binomial Distribution are The mathematical representation of binomial distribution is given by:A binomial distribution graph where the probability of success does not equal the probability of failure looks likeNow, when probability of success = probability of failure, in such a situation the graph of binomial distribution looks likeThe mean and variance of a binomial distribution are given by:Mean -> µ = n*pVariance -> Var(X) = n*p*q Normal distribution represents the behavior of most of the situations in the universe (That is why it’s called a “normal” distribution. I guess!). The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application. Any distribution is known as Normal distribution if it has the following characteristics:A normal distribution is highly different from Binomial Distribution. However, if the number of trials approaches infinity then the shapes will be quite similar.The PDF of a random variable X following a normal distribution is given by:The mean and variance of a random variable X which is said to be normally distributed is given by:Mean -> E(X) = µVariance -> Var(X) = σ^2Here, µ (mean) and σ (standard deviation) are the parameters.
The graph of a random variable X ~ N (µ, σ) is shown below.A standard normal distribution is defined as the distribution with mean 0 and standard deviation 1.  For such a case, the PDF becomes: Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. Some more examples areYou can now think of many examples following the same course. Poisson Distribution is applicable in situations where events occur at random points of time and space wherein our interest lies only in the number of occurrences of the event.A distribution is called Poisson distribution when the following assumptions are valid:1. Any successful event should not influence the outcome of another successful event.
2. The probability of success over a short interval must equal the probability of success over a longer interval.
3. The probability of success in an interval approaches zero as the interval becomes smaller.Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are:Here, X is called a Poisson Random Variable and the probability distribution of X is called Poisson distribution.Let µ denote the mean number of events in an interval of length t. Then, µ = λ*t.The PMF of X following a Poisson distribution is given by:The mean µ is the parameter of this distribution. µ is also defined as the λ times length of that interval. The graph of a Poisson distribution is shown below:The graph shown below illustrates the shift in the curve due to increase in mean.It is perceptible that as the mean increases, the curve shifts to the right.The mean and variance of X following a Poisson distribution:Mean -> E(X) = µ
Variance -> Var(X) = µ Let’s consider the call center example one more time. What about the interval of time between the calls ? Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls.Other examples are:1. Length of time beteeen metro arrivals,
2. Length of time between arrivals at a gas station
3. The life of an Air ConditionerExponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result.A random variable X is said to have an exponential distribution with PDF:f(x) = { λe-λx,  x ≥ 0and parameter λ>0 which is also called the rate.For survival analysis, λ is called the failure rate of a device at any time t, given that it has survived up to t.Mean and Variance of a random variable X following an exponential distribution:Mean -> E(X) = 1/λVariance -> Var(X) = (1/λ)²Also, the greater the rate, the faster the curve drops and the lower the rate, flatter the curve. This is explained better with the graph shown below. To ease the computation, there are some formulas given below.
P{X≤x} = 1 – e-λx, corresponds to the area under the density curve to the left of x.P{X>x} = e-λx, corresponds to the area under the density curve to the right of x.P{x1<X≤ x2} = e-λx1 – e-λx2, corresponds to the area under the density curve between x1 and x2. 1. Bernoulli Distribution is a special case of Binomial Distribution with a single trial.2. There are only two possible outcomes of a Bernoulli and Binomial distribution, namely success and failure.3. Both Bernoulli and Binomial Distributions have independent trails. Poisson Distribution is a limiting case of binomial distribution under the following conditions: Normal distribution is another limiting form of binomial distribution under the following conditions:The normal distribution is also a limiting case of Poisson distribution with the parameter λ →∞. If the times between random events follow exponential distribution with rate λ, then the total number of events in a time period of length t follows the Poisson distribution with parameter λt. You have come this far. Now, are you able to answer the following questions? Let me know in the comments below!1. The formula to calculate standard normal random variable is:a. (x+µ) / σ
b. (x-µ) / σ
c. (x-σ) / µ 2. In Bernoulli Distribution, the formula for calculating standard deviation is given by:a. p (1 – p)
b. SQRT(p(p – 1))
c. SQRT(p(1 – p)) 3. For a normal distribution, an increase in the mean will:a. shift the curve to the left
b. shift the curve to the right
c. flatten the curve 4. The lifetime of a battery is exponentially distributed with λ = 0.05 per hour. The probability for a battery to last between 10 and 15 hours is:a.0.1341
b.0.1540
c.0.0079 Probability Distributions are prevalent in many sectors, namely, insurance, physics, engineering, computer science and even social science wherein the students of psychology and medical are widely using probability distributions. It has an easy application and widespread use. This article highlighted six important distributions which are observed in day-to-day life and explained their application. Now you will be able to identify, relate and differentiate among these distributions.If you have any doubts and want to see more articles on distributions, please do write in the comment section below. For a more in-depth write up of these distributions, you can refer this resource.I hope this article helps you in your data science journey. Was it explanatory? Let me know in the comment section.",https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/
Comparative Stock Market Analysis in R using Quandl & tidyverse – Part I,Learn everything about Analytics|Introduction|Objective of this Tutorial|Table of Contents|Setting Up The System|Getting Started with Comparative Analysis|End Notes,"Creating the Dataset|Visualization of monthly prices|Discovering the Relation between Total Traded Quantity vs Close Price|Finding the Density Distribution of Deviation of High Price from Open Price|Observing the Autocorrelation lags|Learn, engage,compete, and get hired!|Share this:|Like this:|Related Articles|6 Common Probability Distributions every data science professional should know|Understanding Support Vector Machine algorithm from examples (along with code)|
Guest Blog
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"What differentiates the best data scientists from others? It is their focus on application of data science. The best data scientists I know of, see data science and its application every where they look. They look at this world as an outcome of flow of data and information.On the other hand, most beginners often ask the question – how do we apply our learning on real life problems?In this post (and another one following this), I have picked up a real life dataset (Stock Markets in India) and showed how I would use this data to come out with useful insights.I hope that you will find this useful. The idea is show the vast opportunities present in data science in a simple yet powerful manner. If you can think of more examples like this – let me know in comments below!For the best results, I would strongly recommend to build the application yourself as you follow the tutorial. In this article, we will analyze stock market in banking segment based on the bank stocks which are listed in NSE India. Our objective is to find the trends (Seasonal or cyclic) in banking stocks.In our comparative analysis we will use several packages and the primary focus will be on tidy verse package. The emphasis will be given on grouping with the help of tibble dataframe from tidy verse package. This will help to perform similar operation on multiple groups at a time, hence reducing the code length and computational time.This article also focuses on API Key, database code search using quandl, and finally how to directly download the data from R Console.So lets get started!Note: The code that has been mentioned below is to be run on the R command line for best results.  There are a few things you should take care of before you go on further. Below mentioned are the packages you need to install in the systemIf you don’t have any of the packages, then use the below code to install the packages. Modify the packages variable if any of the above packages are already installed.You can then call the necessary packages using the code below We will be using Quandl is online repository for the core financial, macroeconomic statistics and forex. Quandl has a vast collection of free and open data collected from a variety of organizations: central banks, governments, multinational organizations and more. You can use it without payment and with few restrictions.Both Free and Premium data are available. Authenticated Free users have a limit of 300 calls per 10 seconds, 2,000 calls per 10 minutes and a limit of 50,000 calls per day. Premium data subscribers have a limit of 5,000 calls per 10 minutes and a limit of 720,000 calls per day.We will use this online repository to get our data using “Quandl” package directly from the R Console. Quandl package directly interacts with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search.For More information on Quandl Package, please visit this page.To get started with Quandl, create an account and get the quandl API key. Please click here to create an account. Then click on the Login button provided on the top right corner of the screen. Once the registration is complete, please click here to get the API Key.In our analysis, we have selected following banksWe have selected these banks as they are in the price band of Rs 200 to Rs 500. We will use the following codes to get the data into R console.The parameters we use are as follows:Now we will download the data, add a column “Stock” for the stock identifier, and then we paste the respective stock name in the downloaded dataset. We will then consolidate all stock data into one Master Data frame for analysisLet us look at Monthly and Daily price pattern for Stocks using ggplot package. For this we will need to group the master dataframe according by Stock.We have heavily manipulated the theme section of ggplot to get the desired plot. More information on plot is provided here.
 Usually, traded quantity increases if the stock price increases or decreases too rapidly on a given day. This parameter is important for our model for prediction. So we should take some time out to identify the relation between them in our data. We have an idea of trend of the stock price, but not much is clear from the Monthly prices. Axis Bank share price improved in september and stayed at Rs750 for a month. whereas all other Banks were consistent and did not show much of volatility. Now we will see the density distribution of High Price from Open Price in order to get an understanding that how much price is deviating in either direction (North or South) on weekly basis. This gives us an idea of price range for any stock in intraday trading.We will use the transmute_tq() function from tidyquant package to compute the weekly prices. Please click here to get more information.For this add a new column with the difference of high and open price using mutate function. Add another new column with the difference of low and open price using mutate function. Calculate the weekly average of differences using “tq_transmute()” function from tidyverse package. Visualize both density plots with dot distribution on ggplot The lag operator (also known as backshift operator) is a function that shifts (offsets) a time series such that the “lagged” values are aligned with the actual time series. The lags can be shifted any number of units, which simply controls the length of the backshift.Here, “k” is denoted as lag. We will see the lag of 180 days period and see how stocks behave.These are the steps for ComputationIts apparent from the ACF plot, that there is no weekly or monthly pattern. This article contains descriptive analysis of stocks in terms of Daily/Weekly Price fluctuations. It also includes analysis on deviation from High and Low Price. The focus is also given on the relationship between the daily traded quantity of shares & close price and to check for the relationship. In the later part, the main focus is on xts package for the computation of Auto-correaltion. In the article, the focus is provided on finding lag and acf plot using ggplot rather than using the conventional time series package. This includes the analysis on ACF using different lags and to check if there is any pattern in the series.You can read part 2 of this article here. Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning. ",https://www.analyticsvidhya.com/blog/2017/09/comparative-stock-analysis/
Understanding Support Vector Machine algorithm from examples (along with code),Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Support Vector Machine?|How does it work?|How to implement SVM in Python and R?|Pros and Cons associated with SVM||Practice Problem|End Notes,"Problem Statement|How to tune Parameters of SVM?||If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.|Share this:|Related Articles|Comparative Stock Market Analysis in R using Quandl & tidyverse – Part I|Python vs. R vs. SAS – which tool should I learn for Data Science?|
Sunil Ray
|93 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Note: This article was originally published on Oct 6th, 2015 and updated on Sept 13th, 2017 Mastering machine learning algorithms isn’t a myth at all. Most of the beginners start by learning regression. It is simple to learn and use, but does that solve our purpose? Of course not! Because, you can do so much more than just Regression!Think of machine learning algorithms as an armory packed with axes, sword, blades, bow, dagger etc. You have various tools, but you ought to learn to use them at the right time. As an analogy, think of ‘Regression’ as a sword capable of slicing and dicing data efficiently, but incapable of dealing with highly complex data. On the contrary, ‘Support Vector Machines’ is like a sharp knife – it works on smaller datasets, but on them, it can be much more stronger and powerful in building models.By now, I hope you’ve now mastered Random Forest, Naive Bayes Algorithm and Ensemble Modeling. If not, I’d suggest you to take out few minutes and read about them as well. In this article, I shall guide you through the basics to advanced knowledge of a crucial machine learning algorithm, support vector machines.If you’re a beginner looking to start your data science journey, you’ve come to the right place! Check out the below comprehensive courses, curated by industry experts, that we have created just for you:Understanding Support Vector Machine algorithm from examples (along with code)  “Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well (look at the below snapshot).Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line).You can look at support vector machines and a few examples of its working here. Above, we got accustomed to the process of segregating the two classes with a hyper-plane. Now the burning question is “How can we identify the right hyper-plane?”. Don’t worry, it’s not as hard as you think!Let’s understand:Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let’s look at the below snapshot:
Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification. Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.In SVM, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, SVM has a technique called the kernel trick. These are functions which takes low dimensional input space and transform it to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then find out the process to separate the data based on the labels or outputs you’ve defined.When we look at the hyper-plane in original input space it looks like a circle:
Now, let’s look at the methods to apply SVM algorithm in a data science challenge. In Python, scikit-learn is a widely used library for implementing machine learning algorithms, SVM is also available in the scikit-learn library and follow the same structure (Import library, object creation, fitting model and prediction).Now, let us have a look at a real-life problem statement and dataset to understand how to apply SVM for classificationDream Housing Finance company deals in all home loans. They have a presence across all urban, semi-urban and rural areas. Customer first applies for home loan after that company validates the customer eligibility for a loanCompany wants to automate the loan eligibility process (real-time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.Use the coding window below to predict the loan eligibility on the test set. Try changing the hyperparameters for the linear SVM to improve the accuracy. The e1071 package in R is used to create Support Vector Machines with ease. It has helper functions as well as code for the Naive Bayes Classifier. The creation of a support vector machine in R and Python follow similar approaches, let’s take a look now at the following code: Tuning parameters value for machine learning algorithms effectively improves the model performance. Let’s look at the list of parameters available with SVM.I am going to discuss about some important parameters having higher impact on model performance, “kernel”, “gamma” and “C”.kernel: We have already discussed about it. Here, we have various options available with kernel like, “linear”, “rbf”,”poly” and others (default value is “rbf”).  Here “rbf” and “poly” are useful for non-linear hyper-plane. Let’s look at the example, where we’ve used linear kernel on two feature of iris data set to classify their class.Example: Have linear kernelExample: Have rbf kernelChange the kernel type to rbf in below line and look at the impact.I would suggest you to go for linear kernel if you have large number of features (>1000) because it is more likely that the data is linearly separable in high dimensional space. Also, you can RBF but do not forget to cross validate for its parameters as to avoid over-fitting.gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem.Example: Let’s difference if we have gamma different gamma values like 0, 10 or 100.C: Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.We should always look at the cross validation score to have effective combination of these parameters and avoid over-fitting.In R, SVMs can be tuned in a similar fashion as they are in Python. Mentioned below are the respective parameters for e1071 package: Find right additional feature to have a hyper-plane for segregating the classes in below snapshot:Answer the variable name in the comments section below. I’ll shall then reveal the answer. In this article, we looked at the machine learning algorithm, Support Vector Machine in detail.  I discussed its concept of working, process of implementation in python, the tricks to make the model efficient by tuning its parameters, Pros and Cons, and finally a problem to solve. I would suggest you to use SVM and analyse the power of this model by tuning the parameters. I also want to hear your experience with SVM, how have you tuned parameters to avoid over-fitting and reduce the training time?Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
Python vs. R vs. SAS – which tool should I learn for Data Science?,Learn everything about Analytics|Overview|Introduction|Hasn’t a lot already been said on this topic?|Background|Attributes For Comparison|Conclusion,"|1. Availability / Cost|2. Ease of Learning||3. Data Handling Capabilities|4. Graphical Capabilities|5. Advancements in Tool|6. Job Scenario|7. Customer Service Support & Community|8. Deep Learning Support|Other Factors:|Learn, engage, compete, and get hired!|Share this:|Related Articles|Understanding Support Vector Machine algorithm from examples (along with code)|6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R|
Kunal Jain
|191 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found," Note: This article was originally published on Mar 27th, 2014 and updated on Sept 12th, 2017We love comparisons!From Samsung vs. Apple vs. HTC in smartphones; iOS vs. Android vs. Windows in mobile OS to comparing candidates for upcoming elections or selecting captain for the world cup team, comparisons and discussions enrich us in our life. If you love discussions, all you need to do is pop up a relevant question in middle of a passionate community and then watch it explode! The beauty of the process is that everyone in the room walks away as a more knowledgeable person.I am sparking something similar here. SAS vs. R has probably been the biggest debate the data science industry might have witnessed. Python is one of the fastest growing languages now and has come a long way since it’s inception. The reason for me to start this discussion is not to watch it explode (that would be fun as well though). I know that we all will benefit from the discussion.This has also been one of the most commonly asked questions to me on this blog. So, I thought I’ll discuss it with all my readers and visitors! Probably yes! But I still feel the need for discussion for following reasons:So, without any further delay, let the combat begin! Here is a brief description about the 3 ecosystems: I’ll compare these languages on following attributes:I am comparing these from point of view of an analyst. So, if you are looking for purchasing a tool for your company, you may not get complete answer here. The information below will still be useful. For each attribute I give a score to each of these 3 languages (1 – Low; 5 – High).The weightage for these parameters will vary depending on what point of career you are in and your ambitions.SAS is a commercial software. It is expensive and still beyond reach for most of the professionals (in individual capacity). However, it holds the highest market share in Private Organizations. So, until and unless you are in an Organization which has invested in SAS, it might be difficult to access one. Although, SAS has brought in a University Edition that is free to access but it has some limitations. You can also use Jupyter Notebooks in there!R & Python, on the other hand are completely free. Here are my scores on this parameter:SAS – 3R – 5Python – 5 SAS is easy to learn and provides easy option (PROC SQL) for people who already know SQL. Even otherwise, it has a good stable GUI interface in its repository. In terms of resources, there are tutorials available on websites of various university and SAS has a comprehensive documentation. There are certifications from SAS training institutes, but they again come at a cost.R has the steepest learning curve among the 3 languages listed here. It requires you to learn and understand coding. R is a low level programming language and hence simple procedures can take longer codes.Python is known for its simplicity in programming world. This remains true for data analysis as well. While there are no widespread GUI interfaces as of now, I am hoping Python notebooks will become more and more mainstream. They provide awesome features for documentation and sharing.SAS – 4.5R – 2.5Python – 3.5This used to be an advantage for SAS till some time back. R computes every thing in memory (RAM) and hence the computations were limited by the amount of RAM on 32 bit machines. This is no longer the case. All three languages have good data handling capabilities and options for parallel computations. This I feel is no longer a big differentiation. They’ve all also brought on Hadoop and Spark integrations, with them also supporting Cloudera and Apache Pig.SAS – 4R – 4Python – 4 SAS has decent functional graphical capabilities. However, it is just functional. Any customization on plots are difficult and requires you to understand intricacies of SAS Graph package.R has highly advanced graphical capabilities along with Python. There are numerous packages which provide you advanced graphical capabilities.With the introduction of Plotly in both the languages now and with Python having Seaborn, making custom plots has never been easier.SAS – 3R – 4.5Python – 4.5 All 3 ecosystems have all the basic and most needed functions available. This feature only matters if you are working on latest technologies and algorithms.Due to their open nature, R & Python get latest features quickly. SAS, on the other hand updates its capabilities in new version roll-outs. Since R has been used widely in academics in past, development of new techniques is fast.Having said this, SAS releases updates in controlled environment, hence they are well tested. R & Python on the other hand, have open contribution and there are chances of errors in latest developments.SAS – 4R – 4.5Python – 4.5 Globally, SAS is still the market leader in available corporate jobs. Most of the big organizations still work on SAS. R / Python, on the other hand are better options for start-ups and companies looking for cost efficiency. Also, number of jobs on R / Python have been reported to increase over last few years. Here is a trend widely published on internet, which shows the trend for R and SAS jobs. Python jobs for data analysis will have similar or higher trend as R jobs:The graph below shows R in Blue and SAS in Orange.This one on the other hand, now shows R in Blue and Python in Orange.Overall, the market based on languages can be pictured as such: SAS – 4R – 4.5Python – 4.5 R and Python have the biggest online communities but no customer service support. So if you have trouble, you are on your own. You will get a lot of help though.SAS on the other hand has dedicated customer service along with the community. So, if you have problems in installation or any other technical challenges, you can reach out to them.SAS – 4R – 3.5Python – 3.5 Deep Learning in SAS is still in it’s beginning phase and there’s a lot to work on it.On the other hand, Python has had great advancements in the field and has numerous packages like Tensorflow and Keras.R has recently added support for those packages, along with some basic ones too. The kerasR and keras packages in R act as an interface to the original Python package, Keras.SAS – 2Python – 4.5R – 3 Following are some more points worthy to note: We see the market slightly bending towards Python in today’s scenario. It will be pre-mature to place bets on what will prevail, given the dynamic nature of industry. Depending on your circumstances (career stage, financials etc.) you can add your own weights and come up with what might be suitable for you. Here are a few specific scenarios:Strategically, corporate setups that require more hands-on assistance and training choose SAS as an option.Researchers and statisticians choose R as an alternative because it helps in heavy calculations. As they say, R was meant to get the job done and not to ease your computer.Python has been the obvious choice for startups today due to its lightweight nature and growing community. It is the best choice for deep learning as well.Here is the final scorecard:These are my views on this comparison. Now, it’s your turn to share your views through the comments below.",https://www.analyticsvidhya.com/blog/2017/09/sas-vs-vs-python-tool-learn/
6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R,Learn everything about Analytics|Overview|Introduction|Project to apply Naive Bayes|Problem Statement|Table of Contents|What is Naive Bayes algorithm?|How Naive Bayes algorithm works?|What are the Pros and Cons of Naive Bayes?|4 Applications of Naive Bayes Algorithms|How to build a basic model using Naive Bayes in Python and R?|Tips to improve the power of Naive Bayes Model|End Notes,"Python Code:|R Code:|Learn, engage, compete, and get hired!|Share this:|Related Articles|Python vs. R vs. SAS – which tool should I learn for Data Science?|4 Essential Tools any Data Scientist can use to improve their productivity|
Sunil Ray
|41 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Note: This article was originally published on Sep 13th, 2015 and updated on Sept 11th, 2017 Here’s a situation you’ve got into in your data science project:You are working on a classification problem and have generated your set of hypothesis, created features and discussed the importance of variables. Within an hour, stakeholders want to see the first cut of the model.What will you do? You have hundreds of thousands of data points and quite a few variables in your training data set. In such a situation, if I were in your place, I would have used ‘Naive Bayes‘, which can be extremely fast relative to other classification algorithms. It works on Bayes theorem of probability to predict the class of unknown data sets.In this article, I’ll explain the basics of this algorithm, so that next time when you come across large data sets, you can bring this algorithm to action. In addition, if you are a newbie in Python or R, you should not be overwhelmed by the presence of available codes in this article.HR analytics is revolutionizing the way human resources departments operate, leading to higher efficiency and better results overall. Human resources have been using analytics for years.However, the collection, processing, and analysis of data have been largely manual, and given the nature of human resources dynamics and HR KPIs, the approach has been constraining HR. Therefore, it is surprising that HR departments woke up to the utility of machine learning so late in the game. Here is an opportunity to try predictive analytics in identifying the employees most likely to get promoted.Practice Now  It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:Above, Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.Step 1: Convert the data set into a frequency tableStep 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.Problem: Players will play if weather is sunny. Is this statement is correct?We can solve it using above discussed method of posterior probability.P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes. Pros:Cons:  Again, scikit learn (python library) will help here to build a Naive Bayes model in Python. There are three types of Naive Bayes model under the scikit-learn library:Gaussian: It is used in classification and it assumes that features follow a normal distribution.Multinomial: It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.Bernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.Try out the below code in the coding window and check your results on the fly!Above, we looked at the basic Naive Bayes model, you can improve the power of this basic model by tuning parameters and handle assumption intelligently. Let’s look at the methods to improve the performance of Naive Bayes Model. I’d recommend you to go through this document for more details on Text classification using Naive Bayes. Here are some tips for improving power of Naive Bayes Model: In this article, we looked at one of the supervised machine learning algorithm “Naive Bayes” mainly used for classification. Congrats, if you’ve thoroughly & understood this article, you’ve already taken you first step to master this algorithm. From here, all you need is practice.Further, I would suggest you to focus more on data pre-processing and feature selection prior to applying Naive Bayes algorithm.0 In future post, I will discuss about text and document classification using naive bayes in more detail.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
4 Essential Tools any Data Scientist can use to improve their productivity,Learn everything about Analytics|Introduction|Table of Contents|What does a “data science stack” look like?|Case Study of a Deep Learning problem: Getting started with Python Ecosystem|Overview of Jupyter: A Tool for Rapid Prototyping|Keeping Tab of Experiments: Version Control with GitHub|Running Multiple Experiments at the Same Time: Overview Tmux|Deploying the Solution: Using Docker to Minimize Dependencies|Summarization of tools|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R|Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems|
Faizan Shaikh
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Many people have asked me this question that whenever they get started with data science, they get stuck with the manifold of tools available to them.Although there are handful of guides available out there concerning the problem such as “19 Data Science Tools for people who aren’t so good at Programming” or “A Complete Tutorial to Learn Data Science with Python from Scratch“, I would like to show what tools I generally prefer for my day-to-day data science needs.Read on if you are interested!Note: I usually work in Python and in this article I intend to cover the tools used in python ecosystem on Windows.  I come from a software engineering background. There people are inquisitive about what kind of “stack” people currently working on? Their intention here is to find which tools are trending and what are the pros and cons while using it. Its because each and everyday new tools come out which try to eliminate the nagging problems people face everyday.In data science, I can probably say that there is more inclination on what kinds of techniques you use to solve a problem rather than the tools to use. Still, its wise to get to know what kind of tools are available to you. A survey was done keeping this in mind. Below image summarizes these findings.Source Instead of blatantly saying which the tools to be used, I will give you a rundown of these tools with a practical example. We will do this exercise on “Identify the Digits” practice problem.Let us first get to know what the problem entails. The home page mentions “Here, we need to identify the digit in given images. We have total 70,000 images, out of which 49,000 are part of train images with the label of digit and rest 21,000 images are unlabeled (known as test images). Now, we need to identify the digit for test images.”This essentially means that the problem is an image recognition problem.The first step here would be to setup your system for the problem. I usually create a specific folder structure for each problem I start (yes I’m a windows user 😐 ) and start working from there.For this kind of problems, I have a tendency to use the kit mentioned below: Fortunately, most of the things above can be accessed using a single software called Anaconda. I’m accustomed to using Anaconda because of its comprehensiveness of data science packages and ease of use.To setup anaconda in your system, you have to simply download the appropriate version for your platform. More specifically, I have the python 3.6 version of anaconda 4.4.0.Now to use the newly install python ecosystem, open the anaconda command prompt and type “python” As I said earlier,most of the things come pre-installed in anaconda. The only libraries left are tensorflow and keras. A smart thing to do here which anaconda provides a feature for is creating an environment. You do this because even if you do something wrong when setting up, it won’t affect your original system. This is like creating a sandbox for all your experiments. To do this, go to the anaconda command prompt and typeNow not install , the remaining packages by typingNow you can start writing your codes in your favorite text editor and run the python scripts! The problem in working with a plain text editor is that each time you update something, you have to run the code from the start again. Suppose you have a small code for reading data and processing. The code for data reading is functioning correctly but takes an hour to run. Now if you try to change the code for processing, you have to wait for the code for data reading to run and then see if your update works. This is tiresome and it wastes your time a lot.To get over this issue, you can use jupyter notebooks. Jupyter notebooks essentially save your progress and let you continue from where you left off. Here you can write your code in a structured way so that you can resume the code and update it whenever you want to.In the section above, you had setup your system with anaconda software. As I mentioned, anaconda has jupyter preinstalled in it. To open jupyter notebook,  open the anaconda prompt, go to the directory that you created and typeThis opens up jupyter notebook in you web browser (Mozilla Firefox for me). Now to create a notebook, click on “New”-> “Python 3”Now what i usually do is divide the code into small blocks of code, so that it would be easier for me to debug. I always keep these in mind when I write code:Here is a sample of code I wrote to solve the deep learning problem mentioned. You can follow along if you like! A data science project requires multiple iterations of experimentation and testing. It is very difficult to remember all the things you tried out, which one of those worked and which did not.One method to control this issue is to take notes of all the experiments that you did and summarize them. But this too is a bit tedious and manual work. A workaround for this (and my personal choice) is to use GitHub. Originally GitHub was used as a code sharing platform for software engineers, but now it is gradually being accepted by the data science community. What GitHub does is provides you with a framework for saving all the changes you did in the code and reverting back to it anytime you want. This gives you the flexibility that you need to do a data science project efficiently.I will give you an example of how to use GitHub. But first, let us install it in our system. Go to the download link and click on the version as per your system. Let it download and then install it.This will install git in your system, along with command prompt for it called git bash. The next step is to configure the system with your git account. Once you have signed up for GitHub, you can use this to setup your system. Open up you git bash and type the following commands.   Now there’s two tasks you have to do.To accomplish the tasks, follow on to the steps mentioned belowStep 1: Add a new repository on GitHubStep 2: Give proper descriptions to setup the repository.Step 3: Get link to the repositoryStep 4: Connect you local repository with the repository on GitHub with the commands below.Now if you want GitHub to ignore the changes in the files you can mention them in .gitignore file. Open it and add the file or folder you want to ignore.Now each time you want to save your progress, run these commands againThis will ensure that you can go back to where you left off! Jupyter notebooks are very helpful for our experimentations. But what if you wanted to multiple experiments at the same time? You would have to wait for the previous command to finish before you can run another one.I usual have many ideas that I want to test out. And there are times I want to scale up. I use the company’s deep learning box we built a few months back. So I prefer to use tmux for it. tmux or Terminal Multiplexer lets you switch easily between several programs in the same terminal. Here is my command center right now 🙂We setup Ubuntu in the monster because it seemed like a good idea at that time. Setting up tmux in your system is pretty easy, you just have to install it using the command belowsudo apt-get install tmuxTo open tmux, type “tmux” in your command line.To make a new window, just press control + B and then shift + 5Now if you want to let this experiment continue and do something else, just typeand it will give you the terminal again. And if you want to come back to the experiment again, typeAs simple as that! Note that setting up tmux in windows is a bit different. You can refer this article to set it up in your system. Now after all the implementations are done, we still have to deploy the solutions so that the end user such as a developer can access it. But the issue we always face is that the system we have might not be the same as that of the user. There will always be installation and setting up issues in their system.This is a very big problem when it comes to deployment of products in market. So to curb this issue, you can rely on a tool called docker. Docker is works on the idea that you can package code along with its dependencies into a self-contained unit. This unit can then be distributed to the end user.I usually do toy problems on my local machine, but when it comes to final solutions, I rely on the monster. I had setup docker in that system using these commandsIntegrating GPU on docker has some additional steps:Now comes the main part, installing DL libraries on docker. We built the docker from scratch, following this excellent guide (https://github.com/saiprashanths/dl-docker/blob/master/README.md)To run your code on docker, open your docker’s command prompt using the commandNow whenever I have a complete working model, I throw it in the docker system to try it out on scale. You can use the dockerfile in a different system to install the same softwares and libraries in their system. This ensures that there isn’t any installation problem. To conclude these are the tools I use and their usagesHope these tools will help you in your experiments too! In this article, I have covered what I feel are the necessarily tools for doing a data science project. To summarize, I use Python environment in anaconda stack, Jupyter notebooks for experimentations, Github for saving the crucial experiments, Tmux for running multiple experiments at once, and docker for deployment.If you have any more tools to suggest or if you use a completely different stack, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/09/essential-tools-data-scientist-improve-productivity/
"Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems",Learn everything about Analytics,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|4 Essential Tools any Data Scientist can use to improve their productivity|Commonly used Machine Learning Algorithms (with Python and R Codes)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"“Corporate by chance, Entrepreneur by choice”– Interview with Pankaj Kulshreshtha, CEO, Scienaptic SystemsIf you have to learn experience of other people, I can recommend 2 sure shot ways of doing it:Because of my role at Analytics Vidhya, I am lucky to have multiple opportunities of the second type. Today, I am sharing my interaction with one such leader – Pankaj Kulshreshtha, CEO, Scienaptic SystemsPankaj has 20+ years of experience in analytics space. He holds a PhD in Quantitative Methods from IIM-Bangalore. He has grown multiple businesses in past and is still a researcher at heart.Read on to know about his journey & story of setting up Scienaptic Systems. P.S. If you have not bought tickets for DataHack Summit 2017 – this could be your opportunity to interact with thought leaders from across the globe. Kunal: Hi Pankaj, thanks for taking time out for this interview. One of the fears I had when I was in my job was that if I stay too long in a job / secure environment, I might not become an entrepreneur later on. When I look at you, you started your venture after spending close to 20 years in corporate. How did this journey start?Pankaj: Every day we read or hear stories about a group of people who wanted to be Entrepreneurs right from the beginning. But, that was not my journey – In fact, even my entry into the corporate world was a chance encounter.When I was finishing my doctoral work at IIM-B, GE advertised for a ‘Decision Science’ center and this was quite unique in 1997-98. At that time, predictive modeling roles and working with credit bureau data were unheard of in India. Even though my interest lay in academics, I decided to experience the corporate world for a while, and began with a catchy designation: Assistant Manager – Modelling. The experience of working in analytics was great, timing was just right. I did hands on work that still keeps me somewhat sharp in geeky discussions! The more interesting aspect of growth though was on other dimensions. GE is legendary for developing leadership skills and I was fortunate to grow in a very conducive environment. After about 4-5 years, it was pretty clear that I wasn’t going to academic world, at least not full time!I am a very passionate believer that use of data and analytics will dramatically change not just the corporations but also our societies. Building and growing the analytics business at Genpact was very fulfilling and enjoyable, but I started getting a serious inner push to start thinking about starting a new venture in the last couple of years. I chewed on it and discussed the ideas widely. Came to a conclusion that there is much friction among technologies, processes and people that slows down the adoption of predictive analytics. Once that clarity descended, I finally decided to start on the entrepreneurial journey and here I am today – a very unlikely entrepreneur at an unlikely age! Kunal: So does that mean your ideal clients are MNC’s who have huge amount of data and they may not be having the right infrastructure. Basically, they want to accelerate or transform the process. Pankaj: Our target audience is big organizations, especially banks, financial services and insurance companies that have been investing large amount of resources, both in terms of monetary investments for buying new data infrastructure and also setting up machine learning teams.But what we have observed is that in spite of making these investments for 5-10 years now, these organizations still struggle to deliver intelligent customer interactions. This is what we want to change, by enabling organizations to use and enrich omnichannel data and leveraging ML at industrial scale.We have created a platform (“Ether”) that companies can use without having to spend tens of millions of dollars on long range projects but still deliver superior intelligence in their customer interactions. We work in such a way that we leverage the existing technology investments of our client along with our platform and accelerate the development of customer management strategies embedded with advanced ML algorithms. Everything we do is geared to produce significant business impact in short agile cycles of 2-3 months. Kunal: In my experience, I have also seen companies making huge investments in data infrastructure and data science. But, I haven’t seen a marked difference from that in customer interactions. What do you think are the flaws in their strategies or common challenges your clients face and how do you solve them?Pankaj: This is a matter of opinion to some extent. I have worked with global banks & financial companies in the past and I have seen a huge revolution in the way they use & manage data. I think there has been massive progress made over the years.There is a plethora of young, Fintech startups that have dramatically improved the customer experience in the past few years. This kind of innovation in customer experience is hard to achieve for larger financial organizations because of a few reasons:Larger organizations have multiple cross-functional transactions which makes it difficult for them to create a centralized & intelligent customer view at the speed of smaller startups. Hence, creating cross functional strategies to drive customer experience takes up a lot of time and effort in these organizations.The other big problem organizations deal with is around creating a unified customer experience. e.g. If I reach out to a call center of a bank, they should know who I am, what my past transactions have been, whether I have visited the bank branch to resolve my issue etc. All that I (the customer) want is a unified customer experience irrespective of the channel. However, even as banks are getting better at building applications, the data and technology is not yet there to create a unified customer viewThe third issue is the fact that most banks are still organize their P&Ls by products like mortgage, credit cards, loans, etc. Each of these products are separate business units, many times with their own data warehouses. But essentially, these separate business units are targeting the same customer / prospect pool. The challenge here, is to integrate the different business units across customer lifecycle to ensure appropriate product and price targeting so that customer feels an intelligent approach to how they are being offered and reached for new offers.  Kunal: How did you get your first set of clients? Pankaj: Our very first client was an ex colleague who had gone on to become the CEO of a loans business. When he heard about our company, he wanted us to handle analytics and data science for his company. Then another client came in as a referral from one of my ex-colleagues.To be honest, I have been fortunate. The people I have worked with in the industry had a certain level of trust in our ability to get things done and chose to partner with us. I believe that hard work never remains unnoticed; building relationships along your journey always helps. Kunal: Can you tell us any use case which came in as a problem from client’s end and how did you solve it?Pankaj: We worked with a large US cards company to help them manage their fraud. This client was experiencing surging fraud trends because of check-kiting and synthetic ID based credit bust-outs and the projection was incremental fraud losses of ~ $100MM. Historically they had used a set of rules to control the frauds, but these rules also had the flip side of killing a lot of good sales because of the high false positive rates. Within 6 weeks of our engagement, we designed a machine learning strategy for managing the losses and increasing the sales which were otherwise going on hold. We were able to deliver a clear impact of 20 million dollars in the fraud reduction and were able to reduce their False Positive Rates by 1/5th. Kunal: How did you build your initial team?Pankaj: I went about it the traditional way. I approached people from my network with whom I had worked in the past. I spoke to few of them and they decided to join us. Then the team approached their ex-colleagues & hired few more people.Even as we hire more folks to build bigger teams, we still find that looking through our networks is the most effective way to hire the initial team. Kunal: What was the most challenging time for you in the journey to build Scienaptic?Pankaj: I started Scienaptic with a couple of friends and when one of my co-founders decided to leave the company after 9 months because it was not the right thing for them, the world came crashing down on me! I had to evaluate my willingness to even run the company. More than the loss of the co-founder, it was the loneliness in that situation that affected me. At that time, I did ask myself whether I really wanted to go ahead with my startup.What kept me going was my belief in our idea. We were a team of 20 people at that time and I was totally convinced of the product as there was enough evidence that the market needed it. After that, some senior folks joined us, and since then, the company has been growing. Today, we have a great team and I feel energized by just being around them Kunal: How do you define Scienaptic today, is it a product or services driven company and what is the culture like?Pankaj: Before we hired any analytical folks, we hired engineers to build the platform. It was clear from day one that we were building a platform based company. The premise of starting Scienaptic was that the friction among technologies, processes and humans has to be dramatically reduced to enable ML to get embedded in organizations at scale and that needed a platform based approach. Our vision is still the same and we continue to make huge investments in developing our platform capabilities. Based on the serious traction that we are seeing, we feel our strategy is working out!Our culture is hands-on and very tech savvy. Whole of our senior leadership is hands on and leads with personal example. We focus on building new things every day. We ensure that every piece of work that we do, delivers a significant impact to our clients. We strive to create a ‘future-ready’ workplace for ourselves, where there is an immense opportunity and flexibility to learn and grow, both professionally and personally. We are building a work culture which the millennials look forward to! Kunal: Can you tell us more about “Ether”?Pankaj: Ether is a platform that has data management, visualization, machine learning and workflow capabilities that come together to solve very specific business problems. We have designed Ether with several deeply technical innovations. It is a natural language based platform where working with structured & unstructured data is seamless. There is a significant push to democratize machine learning. Ether lets users work on complicated machine learning problems and evaluate the impact rather than worrying about writing thousands of lines of codes to solve the problem.One of our core solution built on Ether is Credit Decisioning and Fraud Management. We understand that as technology evolves within Banks and Financial Institutions, there is an emerging need to also evolve the Credit Decisioning process and our solution is designed to do just that. Our advanced Machine Learning driven approach radically outperforms the traditional approach. This solution populates a rich multidimensional “Customer Consciousness” that can evolve to provide intelligent signals to other processes like customer service and collections. Kunal: What are your top priorities today as the CEO of Scienaptic?Pankaj: We intend to change the way organizations deploy and leverage machine learning in their customer engagements. As part of that vision, we are creating class-defining software that reduces friction among technologies, processes and humans. Another key element of our strategy is to make sure that everything we do delivers impact in very short agile cycles. Towards that we are continually innovating on our operating and engagement models. Kunal: What are the different skillsets you look at while hiring in the US and in India?Pankaj: We look for people who are hands-on and versatile with technology. Our engineering team has been based in Bangalore till now and our core developers are skilled in Java, Scala, Spark, Hadoop etc. As we deploy our platform to more US clients, we are building out an engineering team in New York as well.The other team common to both New York and Bangalore comprises of people who have significant understanding of our platform and Machine Learning techniques. These folks primarily work with clients to understand their problems and create appropriate solutions using technology.Kunal: This is kind of different than the regular practices in the industry. What was the thought process that went behind this?Pankaj: The delivery & execution of work actually happens in both New York and Bangalore in line with what makes sense for the client. We don’t think of ourselves as an off-shoring company. We want to reduce the friction that can be caused in managing remote work. Our aim is to solve problems and transform the costs using technology and machine learning rather than off-shoring. Kunal: Apart from India & US, are there any other markets where you have your presence or looking for expansion. Pankaj: I am very conscious about geographical diversifications as we don’t want to thin out the intensity. Right now, we are focused on the US and UK as our primary markets. Most of our customers and prospects are Fortune 500 corporations and they have presence on both sides of the Atlantic. And because we have presence in Bangalore, we are also working with a few progressive Indian companies.  Kunal: You were one of the key members of the team which scaled up the analytics operations at Genpact. Tell us a bit more about that journey. Pankaj: As I mentioned, I started with GE when analytics wasn’t known in India, about two decades ago. One good thing was that I started building models myself and that is the reason people still think I am technically solid. Over time the Analytics Center of Excellence at GE became popular across GE business units and I got to managing teams, started with 3-4 people team and by 2004 I was leading a team of about 250 people. At that time I decided to move into a functional role as Chief Risk Officer with GE Money, UK.  People would often ask me if it made sense to move from leading a team of 250 to leading one with 8 people! But, I think that was the most powerful career move I made because I actually got to see analytics live in action! What I learnt in that time & the relationships I built, helped me a lot in my journey. And living in another country was very enriching, it opens up our mind to the variety and possibilities.In 2008, I came back to what had become Genpact. With hindsight, the timing was great as the meltdown of 2008 followed pretty quickly! Genpact was a tremendously rewarding and enjoyable experience. We grew 3 times while I was the business leader. I believe those six years taught me how to think big, scale organizations, and sell! Kunal: These days machine learning, big data & AI is getting a lot of attention. How do you think things will change in the next 5 years? Pankaj: I actually think all the hoopla around machine learning & AI is good. In my last few trips to New York, I have heard many conversations about Hadoop, Big data & AI on the streets in midtown. Unlike few years back big data analytics is being talked about a lot today. I think people still underestimate how much our lives will change as a result of big data and how insights will be generated and consumed.I believe that enterprise engagement with customers in an interactive way through multiple channels must become a reality in the coming future. I also believe that Machine Learning will not just be a hyped-up curiosity. Rather, people will actually be using ML in real cases to solve real world problems.The other thing that I think will change, is that more and more people will start getting comfortable with working with a set of technologies rather than just 1 or 2 primary enterprise platforms. Kunal: What would be your advice to people already in data science industry & the beginners entering the industry?Pankaj: There is a lot of talk around Big Data & Machine Learning in the industry. I believe that there is essentially only one problem that everyone is trying to solve – most organizations want to make sure their customers are happy and give them more business over time.Today we have vast data available because of the digitization & one has the ability to do large computations on that data using machine learning & AI. There are numerous resources available to learn techniques and practice hands-on. My advice to people is to focus on solving business problems using machine learning – there is a scarcity of skilled professional who do that well enough.I think there is no substitute for intensity & passion. Even if you have a niche skillset, you need to keep evolving your skills. We are living in times where ability to learn is lot more important than “knowing stuff”. Kunal: Thanks Pankaj for taking time out for this interview. We hope to see you around in our community interactions in future. ",https://www.analyticsvidhya.com/blog/2017/09/interview-pankaj-kulshreshtha-ceo-scienaptic-systems/
Commonly used Machine Learning Algorithms (with Python and R Codes),"Learn everything about Analytics|Overview|Introduction|Who can benefit the most from this guide?|Broadly, there are 3 types of Machine Learning Algorithms||List of Common Machine Learning Algorithms|1. Linear Regression|2. Logistic Regression|3. Decision Tree|4. SVM (Support Vector Machine)|5. Naive Bayes|6. kNN (k- Nearest Neighbors)|7. K-Means|8. Random Forest|9. Dimensionality Reduction Algorithms|10. Gradient Boosting Algorithms|Projects||End Notes","1. Supervised Learning|2. Unsupervised Learning|3. Reinforcement Learning:||Furthermore..|10.1. GBM|10.2. XGBoost|10.3. LightGBM|10.4. Catboost|If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.|Share this:|Related Articles|Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems|Building Machine Learning Model is fun using Orange|
Sunil Ray
|76 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
","What I am giving out today is probably the most valuable guide, I have ever created.||Python Code|Python Code|Python Code|R Code|Python Code|R Code",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Note: This article was originally published on August 10, 2015 and updated on Sept 9th, 2017
 Google’s self-driving cars and robots get a lot of press, but the company’s real future is in machine learning, the technology that enables computers to get smarter and more personal.– Eric Schmidt (Google Chairman)We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to cloud. But what makes it defining is not what has happened, but what is coming our way in years to come.What makes this period exciting and enthralling for someone like me is the democratization of the various tools and techniques, which followed the boost in computing. Welcome to the world of data science!Today, as a data scientist, I can build data-crunching machines with complex algorithms for a few dollars per hour. But reaching here wasn’t easy! I had my dark days and nights.Are you a beginner looking for a place to start your data science journey? Presenting two comprehensive courses, full of knowledge and data science learning, curated just for you to learn data science (using Python) from scratch: The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world. Through this guide, I will enable you to work on machine learning problems and gain from experience. I am providing a high-level understanding of various machine learning algorithms along with R & Python codes to run them. These should be sufficient to get your hands dirty.Essentials of machine learning algorithms with implementation in R and PythonI have deliberately skipped the statistics behind these techniques, as you don’t need to understand them at the start. So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. But, if you are looking to equip yourself to start building machine learning project, you are in for a treat. How it works: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc. How it works: In this algorithm, we do not have any target or outcome variable to predict / estimate. It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means. How it works: Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision ProcessHere is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.In this equation:These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.Look at the below example. Here we have identified the best fit line having linear equation y=0.2811x+13.9. Now using this equation, we can find the weight, knowing the height of a person.Linear Regression is mainly of two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding the best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.Here’s a coding window to try out your hand and build your own linear regression model in Python:R Code Don’t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).Again, let us try and understand this through a simple example.Let’s say your friend gives you a puzzle to solve. There are only 2 outcome scenarios – either you solve it or you don’t. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).Now, you may ask, why take a log? For the sake of simplicity, let’s just say that this is one of the best mathematical way to replicate a step function. I can go in more details, but that will beat the purpose of this article.Build your own logistic regression model in Python here and check the accuracy:R Code There are many different steps that could be tried in order to improve the model: This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: Decision Tree Simplified.source: statsexchangeIn the image above, you can see that population is classified into four different groups based on multiple attributes to identify ‘if they will play or not’. To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.The best way to understand how decision tree works, is to play Jezzball – a classic game from Microsoft (image below). Essentially, you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.So, every time you split the room with a wall, you are trying to create 2 different populations with in the same room. Decision trees work in very similar fashion by dividing a population in as different groups as possible.More: Simplified Version of Decision Tree AlgorithmsLet’s get our hands dirty and code our own decision tree in Python!R Code It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, that’s what class we can classify the new data as.More: Simplified Version of Support Vector MachineThink of this algorithm as playing JezzBall in n-dimensional space. The tweaks in the game are:Try your hand and design an SVM model in Python through this coding window:R Code It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:
Here,Example: Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’. Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.Step 1: Convert the data set to frequency tableStep 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.Problem: Players will pay if weather is sunny, is this statement is correct?We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.Code a Naive Bayes classification model in Python:R Code It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.More: Introduction to k-nearest neighbors : Simplified.KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!Things to consider before selecting kNN:R Code It is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!How K-means forms cluster:How to determine value of K:In K-means, we have clusters and each cluster has its own centroid. Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. Here, we can find the optimum number of cluster.R Code Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).Each tree is planted & grown as follows:For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:Introduction to Random forest – SimplifiedComparing a CART model to Random Forest (Part 1)Comparing a Random Forest to a CART model (Part 2)Tuning the parameters of your Random Forest modelPython Code:R Code In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.To know more about this algorithms, you can read “Beginners Guide To Learn Dimension Reduction Techniques“. GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.More: Know about Boosting algorithms in detailGradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the difference between these two algorithms.Another classic gradient boosting algorithm that’s known to be the decisive choice between winning and losing in some Kaggle competitions.The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.The support includes various objective functions, including regression, classification and ranking.One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process.To learn more about XGBoost and parameter tuning, visit https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/.Python Code:R Code: LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:The framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.Also, it is surprisingly very fast, hence the word ‘Light’.Refer to the article to know more about LightGBM: https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/Python Code:R Code:If you’re familiar with the Caret package in R, this is another way of implementing the LightGBM.CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML.The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.Make sure you handle missing data well before you proceed with the implementation.Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.Learn more about Catboost from this article: https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/Python Code:R Code: Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems: By now, I am sure, you would have an idea of commonly used machine learning algorithms. My sole intention behind writing this article and providing the codes in R and Python is to get you started right away. If you are keen to master machine learning, start right away. Take up problems, develop a physical understanding of the process, apply these codes and see the fun!Did you find this article useful ? Share your views and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
Building Machine Learning Model is fun using Orange,Learn everything about Analytics|Introduction|Table of Contents:|1. Why Orange?||2. Setting up your System|3. Creating Your First Workflow|4. Familiarising yourself with the basics|5. How do you clean your data?|6. Training your First Model|End Notes,"4.1 Problem|4.2 Importing the data files|4.3 Understanding our Data|Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Commonly used Machine Learning Algorithms (with Python and R Codes)|Creating & Visualizing Neural Network in R|
Analytics Vidhya Content Team
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"With growing need of data science managers, we need tools which take out difficulty from doing data science and make it fun. Not everyone is willing to learn coding, even though they would want to learn / apply data science. This is where GUI based tools can come in handy.Today, I will introduce you to another GUI based tool – Orange. This tool is great for beginners who wish to visualize patterns and understand their data without really knowing how to code.In my previous article, I presented you with another GUI based tool KNIME. If you do not want to learn to code but still apply data science, you can try out any of these tools.By the end of this tutorial, you’ll be able to predict which person out of a certain set of people is eligible for a loan with Orange!  Orange is a platform built for mining and analysis on a GUI based workflow. This signifies that you do not have to know how to code to be able to work using Orange and mine data, crunch numbers and derive insights.You can perform tasks ranging from basic visuals to data manipulations, transformations, and data mining. It consolidates all the functions of the entire process into a single workflow.The best part and the differentiator about Orange is that it has some wonderful visuals. You can try silhouettes, heat-maps, geo-maps and all sorts of visualizations available.Orange comes built-in with the Anaconda tool if you’ve previously installed it. If not, follow these steps to download Orange.Step 1: Go to https://orange.biolab.si and click on Download. Step 2: Install the platform and set the working directory for Orange to store its files. This is what the start-up page of Orange looks like. You have options that allow you to create new projects, open recent ones or view examples and get started.Before we delve into how Orange works, let’s define a few key terms to help us in our understanding:You can also go to “Example Workflows” on your start-up screen to check out more workflows once you have created your first one.For now, click on “New” and let’s start building your first workflow. This is the first step towards building a solution to any problem. We need to first understand what steps we need to take in order to achieve our final goal. After you clicked on “New” in the above step, this is what you should have come up with.  This is your blank Workflow on Orange. Now, you’re ready to explore and solve any problem by dragging any widget from the widget menu to your workflow. Orange is a platform that can help us solve most problems in Data Science today. Topics that range from the most basic visualizations to training models. You can even evaluate and perform unsupervised learning on datasets: The problem we’re looking to solve in this tutorial is the practice problem Loan Prediction that can be accessed via this link on Datahack. We begin with the first and the necessary step to understand our data and make predictions: importing our dataStep 1: Click on the “Data” tab on the widget selector menu and drag the widget “File” to our blank workflow.Step 2: Double click the “File” widget and select the file you want to load into the workflow. In this article, as we will be learning how to solve the practice problem Loan Prediction, I will import the training dataset from the same.Step 3: Once you can see the structure of your dataset using the widget, go back by closing this menu.Step 4: Now since we have the raw .csv details, we need to convert it to a format we can use in our mining. Click on the dotted line encircling the “File” widget and drag, and then click anywhere in the blank space.Step 5: As we need a data table to better visualize our findings, we click on the “Data Table” widget.Step 6: Now double click the widget to visualize your table.Neat! Isn’t it?Let’s now visualize some columns to find interesting patterns in our data. 4.3.1 Scatter Plot
Click on the semicircle in front of the “File” widget and drag it to an empty space in the workflow and select the “Scatter Plot” widget.Once you create a Scatter Plot widget, double click it and explore your data like this! You can select the X and Y axes, colors, shapes, sizes and a lot of other manipulations.The plot I’ve explored is a Gender by Income plot, with the colors set to the education levels. As we can see in males, the higher income group naturally belongs to the Graduates!Although in females, we see that a lot of the graduate females are earning low or almost nothing at all. Any specific reason? Let’s find out using the scatterplot. One possible reason I found was marriage. A huge number graduates who were married were found to be in lower income groups; this may be due to family responsibilities or added efforts. Makes perfect sense, right? 4.3.2 DistributionAnother way to visualize our distributions would be the “Distributions” widget. Click on the semi-circle again, and drag to find the widget “Distributions”.Now double click on it and visualize!What we see is a very interesting distribution. We have in our dataset, more number of married males than females. 4.3.3 Sieve diagramHow does income relate to the education levels? Do graduates get paid more than non-grads?Let’s visualize using a sieve diagram.Click and drag from the “File” widget and search for “Sieve Diagram”. Once you place it, double click on it and select your axes!This plot divides the sections of distribution into 4 bins. The sections can be investigated by hovering the mouse over it.For example, graduates and non-graduates are divided 78% by 22%. Then subdivisions of 25% each are made by splitting the applicant incomes into 4 equal groups. Here the task for you, generate insight from these charts and share in the comment section.Let’s now look at how to clean our data to start building our model. Here for cleaning purpose, we will impute missing values. Imputation is a very important step in understanding and making the best use of our data.Click on the “File” widget and drag to find the “Impute” widget.When you double click on the widget after placing it, you will see that there are a variety of imputation methods you can use. You can also use default methods or choose individual methods for each class separately. Here, I have selected the default method to be Average for numerical values and Most Frequent for text based values (categorical).You can select from a variety of imputations like:The other things you can include in your approach to training your model are Feature Extraction and Generation.For further understanding, follow this article on Data Exploration and Feature Engineering (https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/) Beginning with the basics, we will first train a linear model encompassing all the features just to understand how to select and build models.Step 1: First, we need to set a target variable to apply Logistic Regression on it.Step 2: Go to the “File” widget and double click it.Step 3: Now, double click on the Loan_Status column and select it as the target variable. Click Apply.Step 4: Once we have set our target variable, find the clean data from the “Impute” widget as follows and place the “Logistic Regression” widget.Step 5: Double click the widget and select the type of regularization you want to perform.For a better understanding of these, please visit the link about Ridge and Lasso regressions https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/I have chosen Ridge for my analysis, you are free to choose between the two.Step 6: Next, click on the “Impute” or the “Logistic Regression” widget and find the “Test and Score” widget. Make sure you connect both the data and the model to the testing widget.Step 7: Now, click on the “Test and Score” widget to see how well your model is doing.Step 8: To visualize the results better, drag and drop from the “Test and Score” widget to fin d “Confusion Matrix”.Step 9: Once you’ve placed it, click on it to visualize your findings!This way, you can test out different models and see how accurately they perform.Let’s try to evaluate, how a Random Forest would do? Change the modeling method to Random Forest and look at the confusion matrix.Looks decent, but the Logistic Regression performed better.We can try again with a Support Vector Machine.Better than the Random Forest, but still not as good as the Logistic Regression model.Sometimes the simpler methods are the better ones, isn’t it?This is how your final workflow would look after you are done with the complete process.For people who wish to work in groups, you can also export your workflows and send it to friends who can work alongside you!The resulting file is of the (.ows) extension and can be opened in any other Orange setup. Orange is a platform that can be used for almost any kind of analysis but most importantly, for beautiful and easy visuals. In this article, we explored how to visualize a dataset. Predictive modeling was undertaken as well, using a logistic regression predictor, SVM, and a random forest predictor to find loan statuses for each person accordingly.Hope this tutorial has helped you figure out aspects of the problem that you might not have understood or missed out on before. It is very important to understand the data science pipeline and the steps we take to train a model, and this should surely help you build better predictive models soon!",https://www.analyticsvidhya.com/blog/2017/09/building-machine-learning-model-fun-using-orange/
Creating & Visualizing Neural Network in R,Learn everything about Analytics|Introduction|Table of Contents|The Basics of Neural Network|Fitting Neural Network in R|Cross Validation of a Neural Network|End Notes,"Share this:|Like this:|Related Articles|Building Machine Learning Model is fun using Orange|30 Questions to test a data scientist on Tree Based Models|
Guest Blog
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Neural network is an information-processing machine and can be viewed as analogous to human nervous system. Just like human nervous system, which is made up of interconnected neurons, a neural network is made up of interconnected information processing units. The information processing units do not work in a linear manner. In fact, neural network draws its strength from parallel processing of information, which allows it to deal with non-linearity. Neural network becomes handy to infer meaning and detect patterns from complex data sets.Neural network is considered as one of the most useful technique in the world of data analytics. However, it is complex and is often regarded as a black box, i.e. users view the input and output of a neural network but remain clueless about the knowledge generating process. We hope that the article will help readers learn about the internal mechanism of a neural network and get hands-on experience to implement it in R.  A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans. The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e. trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome.A perceptron, viz. single layer neural network, is the most basic form of a neural network.  A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule.There are many learning rules that are used with neural network:a) least mean square;
b) gradient descent;
c) newton’s rule;
d) conjugate gradient etc.The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit.  The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding. Figure 1 A simple neural network model Now we will fit a neural network model in R. In this article, we use a subset of cereal dataset shared by Carnegie Mellon University (CMU). The details of the dataset are on the following link: http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. The objective is to predict rating of the cereals variables such as calories, proteins, fat etc. The R script is provided side by side and is commented for better understanding of the user. .  The data is in .csv format and can be downloaded by clicking: cereals. Please set working directory in R using setwd( ) function, and keep cereal.csv in the working directory. We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. We divide the data into training and test set. Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60% of the dataset as training set. The assignment of the data to training and test set is done using random sampling. We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and   maintain consistency. We will use the index variable while fitting neural network to create training and test data sets. The R script is as follows:  Now we fit a neural network on our data. We use neuralnet library for the analysis. The first step is to scale the cereal dataset. The scaling of data is essential because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. Unlike Z-score normalization and median and MAD method, the min-max method retains the original distribution of the variables. We use min-max normalization to scale the data. The R script for scaling the data is as follows.  The scaled data is used to fit the neural network. We visualize the neural network with weights for each of the variable. The R script is as follows. Figure 3 visualizes the computed neural network. Our model has 3 neurons in its hidden layer. The black lines show the connections with weights. The weights are calculated using the back propagation algorithm explained earlier. The blue line is the displays the bias term.Figure 2 Neural NetworkWe predict the rating using the neural network model. The reader must remember that the predicted rating will be scaled and it must me transformed in order to make a comparison with real rating. We also compare the predicted rating with real rating using visualization. The RMSE for neural network model is 6.05. The reader can learn more about RMSE in another article, which can be accessed by clicking here. The R script is as follows: Figure 3: Predicted rating vs. real rating using neural network We have evaluated our neural network method using RMSE, which is a residual method of evaluation. The major problem of residual evaluation methods is that it does not inform us about the behaviour of our model when new data is introduced.   We tried to deal with the “new data” problem by splitting our data into training and test set, constructing the model on training set and evaluating the model by calculating RMSE for the test set. The training-test split was nothing but the simplest form of cross validation method known as holdout method. A limitation of the holdout method is the variance of performance evaluation metric, in our case RMSE, can be high based on the elements assigned to training and test set.The second commonly cross validation technique is k-fold cross validation. This method can be viewed as a recurring holdout method. The complete data is partitioned into k equal subsets and each time a subset is assigned as test set while others are used for training the model. Every data point gets a chance to be in test set and training set, thus this method reduces the dependence of performance on test-training split and reduces the variance of performance metrics. The extreme case of k-fold cross validation will occur when k is equal to number of data points. It would mean that the predictive model is trained over all the data points except one data point, which takes the role of a test set. This method of leaving one data point as test set is known as leave-one-out cross validation. Now we will perform k-fold cross-validation on the neural network model we built in the previous section. The number of elements in the training set, j, are varied from 10 to 65 and for each j, 100 samples are drawn form the dataset. The rest of the elements in each case are assigned to test set. The model is trained on each of the 5600 training datasets and then tested on the corresponding test sets. We compute RMSE of each of the test set. The RMSE values for each of the set is stored in a Matrix[100 X 56]. This method ensures that our results are free of any sample bias and checks for the robustness of our model. We employ nested for loop. The R script is as follows: The RMSE values can be accessed using the variable Matrix.RMSE. The size of the matrix is large; therefore we will try to make sense of the data through visualizations. First, we will prepare a boxplot for one of the columns in Matrix.RMSE, where training set has length equal to 65. One can prepare these box plots for each of the training set lengths (10 to 65). The R script is as follows. Figure 4 BoxplotThe boxplot in Fig. 4 shows that the median RMSE across 100 samples when length of training set is fixed to 65 is 5.70. In the next visualization we study the variation of RMSE with the length of training set. We calculate the median RMSE for each of the training set length and plot them using the following R script. Figure 5 Variation of RMSEFigure 5 shows that the median RMSE of our model decreases as the length of the training the set. This is an important result. The reader must remember that the model accuracy is dependent on the length of training set. The performance of neural network model is sensitive to training-test split. The article discusses the theoretical aspects of a neural network, its implementation in R and post training evaluation. Neural network is inspired from biological nervous system. Similar to nervous system the information is passed through layers of processors. The significance of variables is represented by weights of each connection. The article provides basic understanding of back propagation algorithm, which is used to assign these weights. In this article we also implement neural network on R. We use a publically available dataset shared by CMU. The aim is to predict the rating of cereals using information such as calories, fat, protein etc. After constructing the neural network we evaluate the model for accuracy and robustness. We compute RMSE and perform cross-validation analysis. In cross validation, we check the variation in model accuracy as the length of training set is changed. We consider training sets with length 10 to 65. For each length a 100 samples are random picked and median RMSE is calculated. We show that model accuracy increases when training set is large.  Before using the model for prediction, it is important to check the robustness of performance through cross validation. The article provides a quick review neural network and is a useful reference for data enthusiasts. We have provided commented R code throughout the article to help readers with hands on experience of using neural networks. Bio: Chaitanya Sagar  is the Founder and CEO of Perceptive Analytics. Perceptive Analytics is one of the top analytics companies in India. It works on Marketing Analytics for ecommerce, Retail and Pharma companies.",https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/
30 Questions to test a data scientist on Tree Based Models,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution| |End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|Creating & Visualizing Neural Network in R|30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm|
Ankit Gupta
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Decision Trees are one of the most respected algorithm in machine learning and data science. They are transparent, easy to understand, robust in nature and widely applicable. You can actually see what the algorithm is doing and what steps does it perform to get to a solution. This trait is particularly important in business context when it comes to explaining a decision to stakeholders.This skill test was specially designed for you to test your knowledge on decision tree techniques. More than 750 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboard for the participants who took the test. Here are some resources to get in depth knowledge in the subject. 1) Which of the following is/are true about bagging trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features and samples. 2) Which of the following is/are true about boosting trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: BIn boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. Bagging and boosting both can be consider as improving the base learners results. 3) Which of the following is/are true about Random Forest and Gradient Boosting ensemble methods?A) 1
B) 2
C) 3
D) 4
E) 1 and 4Solution: EBoth algorithms are design for classification as well as regression task. 4) In Random forest you can generate hundreds of trees (say T1, T2 …..Tn) and then aggregate the results of these tree. Which of the following is true about individual(Tk) tree in Random Forest?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: ARandom forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees. 5) Which of the following is true about “max_depth” hyperparameter in Gradient Boosting?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: AIncrease the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always prefer the small depth in final model building. 6) Which of the following algorithm doesn’t uses learning Rate as of one of its hyperparameter?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: DRandom Forest and Extra Trees don’t have learning rate as a hyperparameter. 7) Which of the following algorithm would you take into the consideration in your final model building on the basis of performance?Suppose you have given the following graph which shows the ROC curve for two different classification algorithms such as Random Forest(Red) and Logistic Regression(Blue)A) Random Forest
B) Logistic Regression
C) Both of the above
D) None of theseSolution: ASince, Random forest has largest AUC given in the picture so I would prefer Random Forest 8) Which of the following is true about training and testing error in such case?Suppose you want to apply AdaBoost algorithm on Data D which has T observations. You set half the data for training and half for testing initially. Now you want to increase the number of data points for training T1, T2 … Tn where T1 < T2…. Tn-1 < Tn.A) The difference between training error and test error increases as number of observations increases
B) The difference between training error and test error decreases as number of observations increases
C) The difference between training error and test error will not change
D) None of TheseSolution: BAs we have more and more data, training error increases and testing error de-creases. And they all converge to the true error. 9) In random forest or gradient boosting algorithms, features can be of any type. For example, it can be a continuous feature or a categorical feature. Which of the following option is true when you consider these types of features?A) Only Random forest algorithm handles real valued attributes by discretizing them
B) Only Gradient boosting algorithm handles real valued attributes by discretizing them
C) Both algorithms can handle real valued attributes by discretizing them
D) None of theseSolution: CBoth can handle real valued features. 10) Which of the following algorithm are not an example of ensemble learning algorithm?A) Random Forest
B) Adaboost
C) Extra Trees
D) Gradient Boosting
E) Decision TreesSolution: EDecision trees doesn’t aggregate the results of multiple trees so it is not an ensemble algorithm. 11) Suppose you are using a bagging based algorithm say a RandomForest in model building. Which of the following can be true?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: ASince Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building.  Random Forest is a black box model you will lose interpretability after using it. Context 12-15 Consider the following figure for answering the next few questions. In the figure, X1 and X2 are the two features and the data point is represented by dots (-1 is negative class and +1 is a positive class). And you first split the data based on feature X1(say splitting point is x11) which is shown in the figure using vertical line. Every value less than x11 will be predicted as positive class and greater than x will be predicted as negative class.12) How many data points are misclassified in above image?A) 1
B) 2
C) 3
D) 4Solution: AOnly one observation is misclassified, one negative class is showing at the left side of vertical line which will be predicting as a positive class. 13) Which of the following splitting point on feature x1 will classify the data correctly?A) Greater than x11
B) Less than x11
C) Equal to x11
D) None of aboveSolution: DIf you search any point on X1 you won’t find any point that gives 100% accuracy. 14) If you consider only feature X2 for splitting. Can you now perfectly separate the positive class from negative class for any one split on X2?A) Yes
B) NoSolution: BIt is also not possible. 15) Now consider only one splitting on both (one on X1 and one on X2) feature. You can split both features at any point. Would you be able to classify all data points correctly?A) TRUE
B) FALSESolution: BYou won’t find such case because you can get minimum 1 misclassification. Context 16-17 Suppose, you are working on a binary classification problem with 3 input features. And you chose to apply a bagging algorithm(X) on this data. You chose max_features = 2 and the n_estimators =3. Now, Think that each estimators have 70% accuracy.Note: Algorithm X is aggregating the results of individual estimators based on maximum voting16) What will be the maximum accuracy you can get? A) 70%
B) 80%
C) 90%
D) 100%Solution: DRefer below table for models M1, M2 and M3.     17) What will be the minimum accuracy you can get? A) Always greater than 70%
B) Always greater than and equal to 70%
C) It can be less than 70%
D) None of theseSolution: CRefer below table for models M1, M2 and M3. 18) Suppose you are building random forest model, which split a node on the attribute, that has highest information gain. In the below image, select the attribute which has the highest information gain? 
A) Outlook
B) Humidity
C) Windy
D) TemperatureSolution: AInformation gain increases with the average purity of subsets. So option A would be the right answer. 19) Which of the following is true about the Gradient Boosting trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth are true and self explanatory 20) True-False: The bagging is suitable for high variance low bias models?
A) TRUE
B) FALSE 
Solution: AThe bagging is suitable for high variance low bias models or you can say for complex models.
21) Which of the following is true when you choose fraction of observations for building the base learners in tree based algorithm?
A) Decrease the fraction of samples to build a base learners will result in decrease in variance
B) Decrease the fraction of samples to build a base learners will result in increase in variance
C) Increase the fraction of samples to build a base learners will result in decrease in variance
D) Increase the fraction of samples to build a base learners will result in Increase in variance
Solution: AAnswer is self explanatory Context 22-23Suppose, you are building a Gradient Boosting model on data, which has millions of observations and 1000’s of features. Before building the model you want to consider the difference parameter setting for time measurement.
22) Consider the hyperparameter “number of trees” and arrange the options in terms of time taken by each hyperparameter for building the Gradient Boosting model?Note: remaining hyperparameters are sameA) 1~2~3
B) 1<2<3C) 1>2>3
D) None of theseSolution: BThe time taken by building 1000 trees is maximum and time taken by building the 100 trees is minimum which is given in solution B 23) Now, Consider the learning rate hyperparameter and arrange the options in terms of time taken by each hyperparameter for building the Gradient boosting model?Note: Remaining hyperparameters are same1. learning rate = 1
2. learning rate = 2
3. learning rate = 3
A) 1~2~3
B) 1<2<3C) 1>2>3
D) None of theseSolution: ASince learning rate doesn’t affect time so all learning rates would take equal time. 24) In greadient boosting it is important use learning rate to get optimum output. Which of the following is true abut choosing the learning rate?A) Learning rate should be as high as possible
B) Learning Rate should be as low as possible
C) Learning Rate should be low but it should not be very low
D) Learning rate should be high but it should not be very highSolution: CLearning rate should be low but it should not be very low otherwise algorithm will take so long to finish the training because you need to increase the number trees.25) [True or False] Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.A) TRUE
B) FALSESolution: A 26) When you use the boosting algorithm you always consider the weak learners. Which of the following is the main reason for having weak learners?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: ATo prevent overfitting, since the complexity of the overall learner increases at each step. Starting with weak learners implies the final classifier will be less likely to overfit. 27) To apply bagging to regression trees which of the following is/are true in such case?A) 1 and 2
B) 2 and 3
C) 1 and 3
D) 1,2 and 3Solution: DAll of the options are correct and self explanatory 28) How to select best hyperparameters in tree based models?A) Measure performance over training data
B) Measure performance over validation data
C) Both of these
D) None of theseSolution: BWe always consider the validation results to compare with the test result. 29) In which of the following scenario a gain ratio is preferred over Information Gain?A) When a categorical variable has very large number of category
B) When a categorical variable has very small number of category
C) Number of categories is the not the reason
D) None of theseSolution: AWhen high cardinality problems, gain ratio is preferred over Information Gain technique. 30) Suppose you have given the following scenario for training and validation error for Gradient Boosting. Which of the following hyper parameter would you choose in such case? A) 1
B) 2
C) 3
D) 4Solution: BScenario 2 and 4 has same validation accuracies but we would select 2 because depth is lower is better hyper parameter. Below is the distribution of the scores of the participants:You can access the scores here. More than 350 people participated in the skill test and the highest score obtained was 28. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test. For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/
30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Tree Based Models|How to create jaw dropping Data Visualizations on the web with D3.js?|
Sunil Ray
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"If you were to ask me 2 most intuitive algorithms in machine learning – it would be k-Nearest Neighbours (kNN) and tree based algorithms. Both of them are simple to understand, easy to explain and perfect to demonstrate to people. Interestingly, we had skill tests for both these algorithms last month.If you are new to machine learning, make sure you test yourself on understanding of both of these algorithms. They are simplistic, but immensely powerful and used extensively in industry.  This skill test will help you test yourself on k-Nearest Neighbours. It  is specially designed for you to test your knowledge on kNN and its applications.More than 650 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions. Here is the leaderboard for the participants who took the test. Here are some resources to get in depth knowledge in the subject. 1) [True or False] k-NN algorithm does more computation on test time rather than train time.
A) TRUE
B) FALSE 
Solution: AThe training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.In the testing phase, a test point is classified by assigning the label which are most frequent among the k training samples nearest to that query point – hence higher computation. 2) In the image below, which would be the best value for k assuming that the algorithm you are using is k-Nearest Neighbor. Solution: BValidation error is the least when the value of k is 10. So it is best to use this value of k 3) Which of the following distance metric can not be used in k-NN?
A) Manhattan
B) Minkowski
C) Tanimoto
D) Jaccard
E) Mahalanobis
F) All can be used 
Solution: FAll of these distance metric can be used as a distance metric for k-NN.
4) Which of the following option is true about k-NN algorithm?
A) It can be used for classification
B) It can be used for regression
C) It can be used in both classification and regression 
Solution: CWe can also use k-NN for regression problems. In this case the prediction can be based on the mean or the median of the k-most similar instances.
5) Which of the following statement is true about k-NN algorithm?A) 1 and 2
B) 1 and 3
C) Only 1
D) All of the aboveSolution: DThe above mentioned statements are assumptions of kNN algorithm 6) Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuous variables?
A) K-NN
B) Linear Regression
C) Logistic Regression 
Solution: Ak-NN algorithm can be used for imputing missing value of both categorical and continuous variables.
7) Which of the following is true about Manhattan distance?
A) It can be used for continuous variables
B) It can be used for categorical variables
C) It can be used for categorical as well as continuous
D) None of these 
Solution: AManhattan Distance is designed for calculating the distance between real valued features.
8) Which of the following distance measure do we use in case of categorical variables in k-NN?A) 1
B) 2
C) 3
D) 1 and 2
E) 2 and 3
F) 1,2 and 3Solution: ABoth Euclidean and Manhattan distances are used in case of continuous variables, whereas hamming distance is used in case of categorical variable. 9) Which of the following will be Euclidean Distance between the two data point A(1,3) and B(2,3)?A) 1
B) 2
C) 4
D) 8Solution: Asqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) = 1 10) Which of the following will be Manhattan Distance between the two data point A(1,3) and B(2,3)?A) 1
B) 2
C) 4
D) 8Solution: Asqrt( mod((1-2)) + mod((3-3))) = sqrt(1 + 0) = 1 Context: 11-12Suppose, you have given the following data where x and y are the 2 input variables and Class is the dependent variable. Below is a scatter plot which shows the above data in 2D space. B) – ClassC) Can’t sayD) None of theseSolution: AAll three nearest point are of +class so this point will be classified as +class. 12) In the previous question, you are now want use 7-NN instead of 3-KNN which of the following x=1 and y=1 will belong to?B) – ClassC) Can’t saySolution: BNow this point will be classified as – class because there are 4 – class and 3 +class point are in nearest circle. Context 13-14:Suppose you have given the following 2-class data where “+” represent a postive class and “” is represent negative class. 13) Which of the following value of k in k-NN would minimize the leave one out cross validation accuracy?A) 3
B) 5
C) Both have same
D) None of theseSolution: B5-NN will have least leave one out cross validation error. 14) Which of the following would be the leave on out cross validation accuracy for k=5?A) 2/14
B) 4/14
C) 6/14
D) 8/14
E) None of the aboveSolution: EIn 5-NN we will have  10/14 leave one out cross validation accuracy. 15) Which of the following will be true about k in k-NN in terms of Bias?A) When you increase the k the bias will be increases
B) When you decrease the k the bias will be increases
C) Can’t say
D) None of theseSolution: Alarge K means simple model, simple model always condider as high bias 16) Which of the following will be true about k in k-NN in terms of variance?A) When you increase the k the variance will increases
B) When you decrease the k the variance will increases
C) Can’t say
D) None of theseSolution: BSimple model will be consider as less variance model 17) The following two distances(Eucludean Distance and Manhattan Distance) have given to you which generally we used in K-NN algorithm. These distance are between two points A(x1,y1) and B(x2,Y2).Your task is to tag the both distance by seeing the following two graphs. Which of the following option is true about below graph ?Solution: BLeft is the graphical depiction of how euclidean distance works, whereas right one is of Manhattan distance. 18) When you find noise in data which of the following option would you consider in k-NN?A) I will increase the value of k
B) I will decrease the value of k
C) Noise can not be dependent on value of k
D) None of theseSolution: ATo be more sure of which classifications you make, you can try increasing the value of k. 19) In k-NN it is very likely to overfit due to the curse of dimensionality. Which of the following option would you consider to handle such problem?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CIn such case you can use either dimensionality reduction algorithm or the feature selection algorithm 20) Below are two statements given. Which of the following will be true both statements?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth are true and self explanatory 21) Suppose you have given the following images(1 left, 2 middle and 3 right), Now your task is to find out the value of k in k-NN in each image where k1 is for 1st, k2 is for 2nd and k3 is for 3rd figure.Solution: D
Value of k is highest in k3, whereas in k1 it is lowest 22) Which of the following value of k in the following graph would you give least leave one out cross validation accuracy?Solution: BIf you keep the value of k as 2, it gives the lowest cross validation accuracy. You can try this out yourself. 23) A company has build a kNN classifier that gets 100% accuracy on training data. When they deployed this model on client side it has been found that the model is not at all accurate. Which of the following thing might gone wrong? Note: Model has successfully deployed and no technical issues are found at client side except the model performance

A) It is probably a overfitted model
B) It is probably a underfitted model
C) Can’t say
D) None of theseSolution: AIn an overfitted module, it seems to be performing well on training data, but it is not generalized enough to give the same results on a new data. 24) You have given the following 2 statements, find which of these option is/are true in case of k-NN?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the options are true and are self explanatory. 25) Which of the following statements is true for k-NN classifiers?A) The classification accuracy is better with larger values of k
B) The decision boundary is smoother with smaller values of k
C) The decision boundary is linear
D) k-NN does not require an explicit training stepSolution: DOption A: This is not always true. You have to ensure that the value of k is not too high or not too low.Option B: This statement is not true. The decision boundary can be a bit jaggedOption C: Same as option BOption D: This statement is true 26) True-False: It is possible to construct a 2-NN classifier by using the 1-NN classifier?A) TRUE
B) FALSESolution: AYou can implement a 2-NN classifier by ensembling 1-NN classifiers 27) In k-NN what will happen when you increase/decrease the value of k?A) The boundary becomes smoother with increasing value of K
B) The boundary becomes smoother with decreasing value of K
C) Smoothness of boundary doesn’t dependent on value of K
D) None of theseSolution: A
The decision boundary would become smoother by increasing the value of K 28) Following are the two statements given for k-NN algorthm, which of the statement(s)is/are true?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the statements are true Context 29-30:Suppose, you have trained a k-NN model and now you want to get the prediction on test data. Before getting the prediction suppose you want to calculate the time taken by k-NN for predicting the class for test data.
Note: Calculating the distance between 2 observation will take D time.29) What would be the time taken by 1-NN if there are N(Very large) observations in test data?A) N*D
B) N*D*2
C) (N*D)/2
D) None of theseSolution: AThe value of N is very large, so option A is correct 30) What would be the relation between the time taken by 1-NN,2-NN,3-NN. A) 1-NN >2-NN >3-NN
B) 1-NN < 2-NN < 3-NN
C) 1-NN ~ 2-NN ~ 3-NN
D) None of theseSolution: CThe training time for any value of k in kNN algorithm is the same. Below is the distribution of the scores of the participants:
You can access the scores here. More than 250 people participated in the skill test and the highest score obtained was 24. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test. For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/
How to create jaw dropping Data Visualizations on the web with D3.js?,"Learn everything about Analytics|Introduction|Table of Contents|1. Refreshing previous concepts of D3.js|2. A glance at advanced concepts: Scales, Axes and Reading data from external sources|3. Building basic charts and code reusability|4. Visualizing the Game of Thrones Social Network: Force-Directed Graph in action!|5. Case Studies – Different Charts using D3.js|6. A Brief Introduction to dimple.js – D3 made easy!|End Notes","2.1 Scales|2.2 Axes|2.3 Loading data from external sources|3.1 Line Chart|3.2 Area Chart|3.3 Chart as a modular structure|3.4 Case Study|Task – Add Step 7. Extras to the bar chart|Bubble Chart – Visualizing 3 Dimensional data|Concept Map – Relationship between concepts|Map Chart – Visualizing Demographical Data|Sankey Diagrams|Parallel Coordinates|Share this:|Related Articles|30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm|Solving Multi-Label Classification problems (Case studies included)|
Mohd Sanad Zaki Rizvi
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Selections|Adding Elements|Setting attributes|Appending data to DOM|About Dataset|Data preprocessing – Handling date and type conversions|Plotting line|Bringing it all together|Step 1. Basic HTML and CSS|Step 2. Setting the stage|Step 3. Visualization specific code|Step 4. Create the SVG|Step 5. Loading external data|Step 6. Bringing it all together|Tooltips|Interactivity|Animation|A little context about Game of Thrones|About the dataset|Data format for force-directed graph,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Data Visualization is the way a data scientist expresses himself / herself. Creating a meaningful visualization requires you to think about the story, the aesthetics of the visualization and various other aspects.If you are planning to create custom visualizations on the web, chances are that you’d have already heard about D3.js. A web based visualization library that features a plethora of APIs to handle the heavy lifting of creating advanced, dynamic and beautiful visualization content on the web. In this article, we would learn some of D3’s powers and use them to create magic of our own! By end of the article, you will be able to create awesome stories for yourself!This article is a continuation of my previous article, Beginner’s guide to build data visualizations on the web with D3.js . Which I would recommend you to read before going further for a better understanding 🙂 Note: The code in this article uses D3 v3.
  Let’s quickly refresh some of the important concepts we have learned in the previous article!In order to perform an operation on an element in the DOM(our HTML page), we would first need to “select” it. We do that using d3.select(..) method. If you want to select the <body> tag, you would do : In case you want to add an element to your DOM, you use .append(..) method. For example, to add a <svg> element to your body, you would do :Notice that here we use another important D3 concept, method chaining. Here, we reuse the output of the d3.select(“body”) and apply the .append() method to add a <svg> element to it. D3 gives you a clean way to add/remove attributes for a selected DOM element. For example, you’d like to set the width and height of your <svg> :  In D3, if you want to add some data to DOM elements, you would create what is called a “Enter, Update, Exit” cycle. For example, You want to create a set of rectangles each having a height as given in an array you’d do : Now, having refreshed some of the important concepts, let’s learn some new topics. These are some of the most useful features that come out of the box with D3. We will cover them one by one:As data enthusiasts, you had probably come across cases where you have data in different scales. For any meaningful inference, it had to be “scaled” to a common range. For example, when you normalize for values between 0 and 1. Here if you want to rescale a data set, you’d need to create a linear scale with fixed domain and range values. Then call the scale with the required value:Here domain(..) is used to set the max and min values of the input data and range(..) is used to set the max and min values of the output needed. A domain of [0,50] and a range of [0,1] means we have created a linear scale that’d map any value in [0,50] to a value in [0,1]. Let’s check how well our scale works : It looks the scale is working quite well! D3 supports plenty of scales to cover almost all of your data needs. Read more here. Axes are a very important part of a visualization. They add a great deal of useful information like the magnitude of data, measurement unit and the direction of magnitude etc. We will create an axis with the domain of data [0,50] and the range of our scale will be in [10,400] which represent the pixels. Let’s look at the code to fit into our <svg> :Let’s beautify the axis a little by adding the following code to the CSS :That looks quite better! I personally feel CSS is like “parameter tuning” of visualization with D3.What happened here?We created an axis using d3.svg.axis() method. The scale() method then sets the scale to our newly created scale. ticks()  sets the number of ticks our axis will have, the default is 10.  The above axis without ticks(5) would look like :An axis a combination of many SVG elements that for its scale, ticks, labels etc. So it is better to group them using <g> in the SVG. That’s we appended a group and called xAxis() function to plot the axis. You can read more about axes in D3 here. Now that we know how to plot axes, let’s learn how to load external data into D3! As data scientists , we deal with a variety of data formats like JSON, CSV, TSV, XML,HTML etc. D3 supports all of these formats and much more by default. For example, loading data from a tsv is as simple as calling a function :Things to note:Now that we have learned how to create the building blocks of a visualization, let’s create some charts using D3. All of the concepts that we have learned till now are used as modules for creating a visualization in D3. We will create two charts of our own, a line chart and an area chart. Line charts are one of the most widely used chart types when it comes to showing time series based data to depict trends over time. Let’s create the above line chart! We will be using a sample data set in the form of tab-separated-values. You can find it here. There are three columns in our data set – index , date and close. While index is the index of the entry, date denotes the recorded date of stock and close is the closing price of the stock at the given date. Notice that both the date and price is in string format. Before we can use them to make plots, we need to convert them into usable formats. Make the following changes to your earlier code of loading external data:Let’s view our data once again : Looks like we have successfully formatted our data. Let’s take a closer look at the code:Now that our data is properly preprocessed, we are ready to convert the data into visuals. When we plot a line in an SVG, we give coordinates of the path to follow but that’s a tedious process. Enter D3, here we just need to provide data values and the scale they should follow and D3 does all the heavy lifting of calculating coordinates for us! The following code will create a reusable line generator for our data : Now that we have all the pieces together, let’s build the final chart! The following code should make the above chart: One of the magical things about D3 is if you design your code smartly, a lot of it can be reused in other visualizations. An area chart can be thought as a modified version of the line chart, here we shade the region of the chart that comes under the line. The interesting thing here is, you can create the above chart by changing 4-5 lines in the previous chart’s code! The final code for area chart would be : Notice that we have only made the following changes : Now that we have successfully built some visualizations let’s look at some case studies with D3. You have already built a bunch of charts using D3, did you notice any pattern or structure in them? After working with D3 for a while, I have noticed a general structure that I follow in my code to create charts. The following structure is how I logically layout my code :Note that sometimes you won’t be able to follow the order because of other factors and that is alright. This is not a rigid rule but just a way to divide and conquer the process of building a chart in D3 so that it can be easily understood and reused whenever possible. Confused much? Let’s understand it with a practice problem! This section is very important because here, it’s your turn to build! You will be building the following beautiful bar chart. After the basic chart is built a huge task awaits you, give your best. This chart is built on a data set of character frequency. The dataset can be found here. Let’s follow the general structure that we just learned and try to build this chart!  In the above code, we started out by setting up basic margins and width, height values. Then we created a percentage formatter using D3’s format(..) so that we can convert our y axis labels to %. Then we defined scales for x and y axis and defined the range for the same. Lastly, since we would be needing color for our bars, we will use one of D3’s color bands, category10. The category10 contains the following 10 colors : D3 has more color scales. Read more here. For the above simple bar chart, we don’t need any visualization specific code. Now that we have set the stage, it is time to create the SVG along with the above settings: Since we have a URL to load data from and it is in tsv format, we can just make a single function call to load data:Remember some preprocessing of data? Now that we get our data, we need to set the domain of our x and y scales which we couldn’t set earlier because we didn’t have the data: We have our data nicely formatted and ready, we also have our axes, colors ready. Let’s bring it all together and attach them to our chart. Notice the new code added in external data loading function : Now that you have the basic chart drawn, it is time for you to jump into action and do something creative! This is the step where you come in. After all, who doesn’t like something extra? Visualization is no different! Since we already have a basic bar chart, I challenge you to go ahead and add animation/interactivity to this chart. The following is a list of common effects that you can easily add to your chart using D3:     The above is just a minor set of options. You’ll find enough resources in the endnotes to refer. You can post about your approach/discuss your doubts with the community here :https://discuss.analyticsvidhya.com/Go ahead, surprise me with your creativity! Now that you have pretty much learnt all the basics of D3 and made plenty of charts on your own, it is time to move to the next level and do something that is niche to D3, you’ll be building one of the case study – Force-Directed Graph to visualize a very popular TV Series, Game of Thrones. The TV sensation Game of Thrones is based on George R. R. Martin’s epic fantasy novel series, “A Song of Ice and Fire.” The series is famous for its sweeping plotlines, its cast of hundreds of characters, and its complex web of character dynamics. In addition, “Game of Thrones” is an ensemble piece, featuring many prominent characters with intertwined relationships.Here, interaction among characters and the strength of their relationship is important. Also, there are some characters who are much more influential than others and steer the course of the story. The dataset for this visualization is based on Andrew Beveridge’s data set of “A Storm of Swords”, the third book in the series. In Andrew’s words,“We represented each character in the book as a vertex. We then added a link between two characters whenever their names appeared within 15 words of one another. So a link between characters means that they interacted, spoke of one another, or that another character spoke of them together. Characters that interacted frequently are connected multiple times.”Using the above data set, I calculated the influence of character based on the “the number of times her/his interaction has appeared in the book”. For example, if Sansa has 6 records where she is the source and 4 records where she is the target of an interaction, her influence will be 6+4=10.I then formatted the data so that D3 can easily use it. The result was a simple JSON file here. The force directed graph can be divided into two major components – nodes and links. The nodes represent entities whose relationship is to be plotted and similarly links are those relationships. These are analogous to vertex and edge in a graph. D3 expects two arrays for force layout. The first one should be an array of nodes and the second one an array of links. The links refer to the index of the node and should have two attributes “source” and “target”. This is exactly how I have laid out our JSON. Now that we are versed in the basics, let’s build our Social Network Graph by following the earlier steps : Step 1. Basic HTML and CSS Step 2. Setting the stageStep 3. No visualization specific codeStep 4. Create the SVGStep 5. Load external dataStep 6. Bringing it all together – force layout, nodes, linksStep 7. Adding extras – Labels, Starting the simulationWhat’s happening here?First, we perform steps 1 to 4 of building a D3 chart as discussed earlier. Then we create a force layout here  : When the above code is executed, it asks D3 to make the necessary calculations for the position of each node, the distance between them, calculating coordinates of links joining them etc. All of this happens in the background and D3 takes care of it all. While the above calculations are being done, we need to keep updating the position of the nodes and links. We attach a listener to our force layout : The above code basically tells D3 for every “tick” (single step of the simulation), redraw(updated) all the nodes and their links. Now when everything is done, we start the simulation : If you want to dig deeper into force layouts, you can read its documentation here. With this, you have your very own GoT Social Graph!Live DemoFew things to note about the visualization: D3 has been used on a versatile set of visualization problems. Let’s look into some of the interesting ones.Source LinkWhat would you do if you want to show 3-dimensional data in 2 dimensions? You’d use a bubble chart! The area of the circle and the x and y coordinates are used to encode all the 3 dimensions. Source LinkA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as causes, requires, or contributes to. In this example, D3 is used to show an improvised concept map to better convey the story. Notice how the branches get highlighted when you select a topic. This is a good example of interactivity D3 enables in your visualizations. Source LinkD3 provides amazing inbuilt support to create interactive, map-based visualizations that can be used to show demographically distributed data. Source Link Sankey diagrams visualize the magnitude of flow between nodes in a network. This intricate diagram shows a possible scenario for UK energy production and consumption in 2050: energy supplies are on the left, and demands are on the right. Intermediate nodes group related forms of production and show how energy is converted and transmitted before it is consumed (or lost!). The thickness of each link encodes the amount of flow from source to target. Source Link Parallel coordinates is a visualization technique used to plot individual data elements across many dimensions. Each of the dimensions corresponds to a vertical axis and each data element is displayed as a series of connected points along the dimensions/axes. In the above example, the very famous iris dataset is plotted. D3 code can be quite verbose. That begs the question, is there a simpler way of creating basic charts using D3 but with fewer lines of code? That’s exactly what dimple.js lets you do. It is a library that is directly built on top of D3 and provides an easy interface to build visualizations. It can be loosely compared to the relationship between keras and tensorflow in python. Let’s build a basic visualization using dimple :What happened here?Note: that with only a few lines of code, you were able to create a beautiful visualization. Also, dimple took care of basic interaction and animation. If you want to explore dimple more, check out these other examples created using dimple.js. In this article, we refreshed some of D3’s basics and further learned a lot of techniques and new functionalities. We also learned how to preprocess data and successfully build a line chart and modified it into an area chart. We used a practice problem to create a colorful bar chart. We also saw some of the most useful and amazing visualizations created using D3.js. We ended up analyzing Game of Thrones data using one of the case studies – Force-Directed Graph and with this, I hope, this article has given you the necessary impetus on your D3 journey! All of the code used in this article is available at GitHub.Here are some useful links for D3.js : ",https://www.analyticsvidhya.com/blog/2017/08/visualizations-with-d3-js/
Solving Multi-Label Classification problems (Case studies included),Learn everything about Analytics|Introduction|Table of Contents|1. What is Multi-Label Classification?|2. Multi-Label v/s Multi-Class|3. Loading and Generating Multi-Label Datasets|4. Techniques for Solving a Multi-Label classification problem|5. Case Studies|6. End Notes,"4.1 Problem Transformation|4.2 Adapted Algorithm|4.3 Ensemble Approaches|1. Audio Categorization|2. Image Categorization|3. Bioinformatics|4. Text Categorization|Share this:|Like this:|Related Articles|How to create jaw dropping Data Visualizations on the web with D3.js?|Getting Started with Audio Data Analysis using Deep Learning (with case study)|
Shubham Jain
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",|4.1.1 Binary Relevance||4.1.2 Classifier Chains||4.1.3 Label Powerset,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"For some reason, Regression and Classification problems end up taking most of the attention in machine learning world. People don’t realize the wide variety of machine learning problems which can exist.I, on the other hand, love exploring different variety of problems and sharing my learning with the community here.Previously, I shared my learnings on Genetic algorithms with the community. Continuing on with my search, I intend to cover a topic which has much less widespread but a nagging problem in the data science community – which is multi-label classification.In this article, I will give you an intuitive explanation of what multi-label classification entails, along with illustration of how to solve the problem. I hope it will show you the horizon of what data science encompasses. So lets get on with it!  Let us take a look at the image below. What if I ask you that does this image contains a house? The option will be YES or NO. Consider another case, like what all things (or labels) are relevant to this picture? These types of problems, where we have a set of target variables, are known as multi-label classification problems. So, is there any difference between these two cases? Clearly, yes because in the second case any image may contain a different set of these multiple labels for different images.But before going deep into multi-label, I just wanted to clear one thing as many of you might be confused that how this is different from the multi-class problem. So, let’s us try to understand the difference between these two sets of problems.  Consider an example to understand the difference between these two. For this, I hope that below image makes things quite clear. Let’s try to understand it. For any movie, Central Board of Film Certification, issue a certificate depending on the contents of the movie.  For example, if you look above, this movie has been rated as ‘U/A’ (meaning ‘Parental Guidance for children below the age of 12 years’) certificate. There are other types of certificates classes like ‘A’ (Restricted to adults) or ‘U’ (Unrestricted Public Exhibition), but it is sure that each movie can only be categorized with only one out of those three type of certificates.  In short, there are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem. Again, if you look back at the image, this movie has been categorized into comedy and romance genre. But there is a difference that this time each movie could fall into one or more different sets of categories.  Therefore, each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels. Great! Now you can distinguish between a multi-label and multi-class problem. So, let’s start how to deal with these types of problems.  Scikit-learn has provided a separate library scikit-multilearn for multi label classification. For better understanding, let us start practicing on a multi-label dataset. You can find a real-world data set from the repository provided by MULAN package. These datasets are present in ARFF format. So, for getting started with any of these datasets, look at the python code below for loading it onto your jupyter notebook. Here I have downloaded the yeast data set from the repository.There is how the data set looks like.Here, Att represents the attributes or the independent variables and Class represents the target variables.For practice purpose, we have another option to generate an artificial multi-label dataset.   Let us understand the parameters used above. sparse: If True, returns a sparse matrix, where sparse matrix means a matrix having a large number of zero elements. n_labels:  The average number of labels for each instance. return_indicator: If ‘sparse’ return Y in the sparse binary indicator format.allow_unlabeled: If True, some instances might not belong to any class.You must have noticed that we have used sparse matrix everywhere, and scikit-multilearn also recommends to use data in the sparse form because it is very rare for a real-world data set to be dense. Generally, the number of labels assigned to each instance is very less.Okay, now we have our datasets ready so let us quickly learn the techniques to solve a multi-label problem. Basically, there are three methods to solve a multi-label classification problem, namely:  In this method, we will try to transform our multi-label problem into single-label problem(s). This method can be carried out in three different ways as: This is the simplest technique, which basically treats each label as a separate single class classification problem. For example, let us consider a case as shown below. We have the data set like this, where X is the independent feature and Y’s are the target variable.
In binary relevance, this problem is broken into 4 different single class classification problems as shown in the figure below.We don’t have to do this manually, the multi-learn library provides its implementation in python. So, let’s us quickly look at its implementation on the randomly generated data.NOTE: Here, we have used Naive Bayes algorithm but you can use any other classification algorithm.Now, in a multi-label classification problem, we can’t simply use our normal metrics to calculate the accuracy of our predictions. For that purpose, we will use accuracy score metric. This function calculates subset accuracy meaning the predicted set of labels should exactly match with the true set of labels.So, let us calculate the accuracy of the predictions.It is most simple and efficient method but the only drawback of this method is that it doesn’t consider labels correlation because it treats every target variable independently.In this, the first classifier is trained just on the input data and then each next classifier is trained on the input space and all the previous classifiers in the chain.  Let’s try to this understand this by an example. In the dataset given below, we have X as the input space and Y’s as the labels.In classifier chains, this problem would be transformed into 4 different single label problems, just like shown below. Here yellow colored is the input space and the white part represent the target variable.This is quite similar to binary relevance, the only difference being it forms chains in order to preserve label correlation. So, let’s try to implement this using multi-learn library.We can see that using this we obtained an accuracy of about 21%, which is very less than binary relevance. This is maybe due to the absence of label correlation since we have randomly generated the data. In this, we transform the problem into a multi-class problem with one multi-class classifier is trained on all unique label combinations found in the training data. Let’s understand it by an example.In this, we find that x1 and x4 have the same labels, similarly, x3 and x6 have the same set of labels. So, label powerset transforms this problem into a single multi-class problem as shown below. So, label powerset has given a unique class to every possible label combination that is present in the training set. Let’s us look at its implementation in python. This gives us the highest accuracy among all the three we have discussed till now. The only disadvantage of this is that as the training data increases, number of classes become more. Thus, increasing the model complexity, and would result in a lower accuracy. Now, let us look at the second method to solve multi-label classification problem.  Adapted algorithm, as the name suggests, adapting the algorithm to directly perform multi-label classification, rather than transforming the problem into different subsets of problems. For example, multi-label version of kNN is represented by MLkNN. So, let us quickly implement this on our randomly generated data set.  Great! You have achieved an accuracy score of 69% on your test data. Sci-kit learn provides inbuilt support of multi-label classification in some of the algorithm like Random Forest and Ridge regression. So, you can directly call them and predict the output. You can check the multi-learn library if you wish to learn more about other types of adapted algorithm.  Ensemble always produces better results. Scikit-Multilearn library provides different ensembling classification functions, which you can use for obtaining better results. For the direct implementation, you can check out here.  Multi-label classification problems are very common in the real world. So, let us look at some of the areas where we can find the use of them.We have already seen songs being classified into different genres. They are also been classified on the basis of emotions or moods like “relaxing-calm”, or “sad-lonely” etc. Source: linkMulti-label classification using image has also a wide range of applications. Images can be labeled to indicate different objects, people or concepts.Multi-Label classification has a lot of use in the field of bioinformatics, for example, classification of genes in the yeast data set.It is also used to predict multiple functions of proteins using several unlabeled proteins. You can check this paper for more information. You all must once check out google news. So, what google news does is, it labels every news to one or more categories such that it is displayed under different categories. For example, take a look at the image below. Image source: Google newsThat same news is present under the categories of India, Technology, Latest etc. because it has been classified into these different labels. Thus making it a multi label classification problem.There are plenty of other areas, so explore and comment down below if you wish to share it with the community.  In this article, I introduced you to the concept of multi-label classification problems. I have also covered the approaches to solve this problem and the practical use cases where you may have to handle it using multi-learn library in python.
I hope this article will give you a head start when you face these kinds of problems. If you have any doubts/suggestions, feel free to reach out to me below!",https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/
Getting Started with Audio Data Analysis using Deep Learning (with case study),Learn everything about Analytics|Introduction|Table of Contents|What do you mean by Audio data?|Data Handling in Audio domain|Let’s solve the UrbanSound challenge!|Intermission: Our first submission|Let’s solve the challenge! Part 2: Building better models|Future steps to explore|End Notes,"The abundance on unstructured data|Applications of Audio Processing|Step 1 and  2 combined: Load audio files and extract features|Step 3: Convert the data to pass it in our deep learning model|Step 4: Run a deep learning model and get results|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Solving Multi-Label Classification problems (Case studies included)|Building your first machine learning model using KNIME (no coding required!)|
Faizan Shaikh
|40 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"When you get started with data science, you start simple. You go through simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format. In other words, you are spoon-fed the hardest part in data science pipeline.The datasets in real life are much more complex.You first have to understand it, collect it from various sources and arrange it in a format which is ready for processing. This is even more difficult when the data is in an unstructured format such as image or audio. This is so because you would have to represent image/audio data in a standard way for it to be useful for analysis. Interestingly, unstructured data represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. It also contains a lot of useful & powerful information. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice.Also the body language of the person can show you many more features about a person, because actions speak louder than words! So in short, unstructured data is complex but processing it can reap easy rewards.In this article, I intend to cover an overview of audio / voice processing with a case study so that you would get a hands-on introduction to solving audio processing problems.Let’s get on with it!  Directly or indirectly, you are always in contact with audio. Your brain is continuously processing and understanding audio data and giving you information about the environment. A simple example can be your conversations with people which you do daily. This speech is discerned by the other person to carry on the discussions. Even when you think you are in a quiet environment, you tend to catch much more subtle sounds, like the rustling of leaves or the splatter of rain. This is the extent of your connection with audio.So can you somehow catch this audio floating all around you to do something constructive? Yes, of course! There are devices built which help you catch these sounds and represent it in computer readable format. Examples of these formats areIf you give a thought on what an audio looks like, it is nothing but a wave like format of data, where the amplitude of audio change with respect to time. This can be pictorial represented as follows.Although we discussed that audio data can be useful for analysis. But what are the potential applications of audio processing? Here I would list a few of themHere’s an exercise for you; can you think of an application of audio processing that can potentially help thousands of lives? As with all unstructured data formats, audio data has a couple of preprocessing steps which have to be followed before it is presented for analysis.. We will cover this in detail in later article, here we will get an intuition on why this is done.The first step is to actually load the data into a machine understandable format. For this, we simply take values after every specific time steps. For example; in a 2 second audio file, we extract values at half a second. This is called sampling of audio data, and the rate at which it is sampled is called the sampling rate. Another way of representing audio data is by converting it into a different domain of data representation, namely the frequency domain. When we sample an audio data, we require much more data points to represent the whole data and also, the sampling rate should be as high as possible.On the other hand, if we represent audio data in frequency domain, much less computational space is required. To get an intuition, take a look at the image belowSourceHere, we separate one audio signal into 3 different pure signals, which can now be represented as three unique values in frequency domain.There are a few more ways in which audio data can be represented, for example. using MFCs (Mel-Frequency cepstrums. PS: We will cover this in the later article). These are nothing but different ways to represent the data.Now the next step is to extract features from this audio representations, so that our algorithm can work on these features and perform the task it is designed for. Here’s a visual representation of the categories of audio features that can be extracted.After extracting these features, it is then sent to the machine learning model for further analysis. Let us have a better practical overview in a real life project, the Urban Sound challenge. This practice problem is meant to introduce you to audio processing in the usual classification scenario.The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, namely:Here’s a sound excerpt from the dataset. Can you guess which class does it belong to?To play this in the jupyter notebook, you can simply follow along with the code.Now let us load this audio in our notebook as a numpy array. For this, we will use librosa library in python. To install librosa, just type this in command lineNow we can run the following code to load the dataWhen you load the data, it gives you two objects; a numpy array of an audio file and the corresponding sampling rate by which it was extracted. Now to represent this as a waveform (which it originally is), use the following  codeThe output comes out as followsLet us now visually inspect our data and see if we can find patterns in the dataWe can see that it may be difficult to differentiate between jackhammer and drilling, but it is still easy to discern between dog_barking and drilling. To see more such examples, you can use this code  We will do a similar approach as we did for Age detection problem, to see the class distributions and just predict the max occurrence of all test cases as that class.Let us see the distributions for this problem.We see that jackhammer class has more values than any other class. So let us create our first submission with this idea.This seems like a good idea as a benchmark for any challenge, but for this problem, it seems a bit unfair. This is so because the dataset is not much imbalanced. Now let us see how we can leverage the concepts we learned above to solve the problem. We will follow these steps to solve the problem.Step 1: Load audio files
Step 2: Extract features from audio
Step 3: Convert the data to pass it in our deep learning model
Step 4: Run a deep learning model and get resultsBelow is a code of how I implemented these steps Now let us train our modelThis is the result I got on training for 5 epochsSeems ok, but the score can be increased obviously. (PS: I could get an accuracy of  80% on my validation dataset). Now its your turn, can you increase on this score? If you do, let me know in the comments below! Now that we saw a simple applications, we can ideate a few more methods which can help us improve our score In this article, I have given a brief overview of audio processing with an case study on UrbanSound challenge. I have also shown the steps you perform when dealing with audio data in python with librosa package. Giving this “shastra” in your hand, I hope you could try your own algorithms in Urban Sound challenge, or try solving your own audio problems in daily life. If you have any suggestions/ideas, do let me know in the comments below!Podcast: Play in new window | Download",https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/
Building your first machine learning model using KNIME (no coding required!),Learn everything about Analytics|Introduction|1. Setting Up Your System|2. Introducing KNIME|3. How do you clean your Data?|4. Training your First Model|5. Submitting your Solution|6. Limitations|End Notes,"Why KNIME?|Table of Contents|1.1 Creating your First Workflow|2.1 Importing the data files|2.2 Visualization and Analysis:|3.1 Finding Missing Values|3.2 Imputations|4.1 Implementing a Linear Model|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Getting Started with Audio Data Analysis using Deep Learning (with case study)|Finding chairs the data scientist way! (Hint: using Deep Learning) – Part I|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"One of the biggest challenges for beginners in machine learning / data science is that there is too much to learn simultaneously. Especially so, if you do not know how to code. You need to quickly get used to Linear Algebra, Statistics, other mathematical concepts and learn how to code them! It might end up being a bit overwhelming for the new users.If you have no background in coding and find it difficult to cope with, you can start learning data science with a tool which is GUI driven. This enables you to focus your efforts on learning the subject in initial days. Once you are comfortable with basic concepts, you can always learn how to code later on.In today’s article, I will get you started with one such GUI based tool – KNIME. By end of this article, you will be able to predict sales for a retail store without writing a piece of code!Let’s get started! KNIME is a platform built for powerful analytics on a GUI based workflow. This means, you do not have to know how to code to be able to work using KNIME and derive insights.You can perform functions ranging from basic I/O to data manipulations, transformations and data mining. It consolidates all the functions of the entire process into a single workflow.   To begin with KNIME, you first need to install it and set it up on your PC.Step 1: Go to  www.knime.com/downloadsStep 2: Identifying the right version for your PCStep 3: Install the platform and set the working directory for KNIME to store its files.This is how your home screen at KNIME would look like. Before we delve more into how KNIME works, let’s define a few key terms to help us in our understanding and then see how to open up a new project in KNIME.Node: A node is the basic processing point of any data manipulations. It can do a number of actions based on what you choose in your workflow.Workflow: A workflow is the sequence of steps or actions you take in your platform to accomplish a particular task.The workflow coach on the left top corner will show you what percentage of the community of KNIME recommends a particular node for usage. The node repository will display all nodes that a particular workflow can have, depending on your needs. You can also go to “Browse Example Workflows” to check out more workflows once you have created your first one. This is the first step towards building a solution to any problem.To setup a workflow, you can follow these steps.Step 1: Go to File menu, and click on New. Step 2: Create a new KNIME Workflow in your platform and name it “Introduction”.Step 3: Now when you click on Finish, you should have successfully created your first KNIME workflow.This is your blank Workflow on KNIME. Now, you’re ready to explore and solve any problem by dragging any node from the repository to your workflow. KNIME is a platform that can help us solve any problem that we could possibly think of, in the boundaries of data science today. Topics that range from the most basic visualizations or linear regressions to advanced deep learning, KNIME can do it all.As a sample use case, the problem we’re looking to solve in this tutorial is the practice problem Big Mart Sales that can be accessed at Datahack.The problem statement is as follows,The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales. Let us start with the first yet a very important step in understanding the problem; importing our data.Drag and drop the “file reader” node to the workflow and double click on it. Next, browse the file you need to import into your workflow.In this article, as we will be learning how to solve the practice problem Big Mart Sales, I will import the training dataset from Big Mart Sales.This is what the preview would look like, once you import the dataset.Let us visualize some relevant columns and find the correlation between them. Correlation helps us find what columns might be related to each other and have a higher predictive power to help us in our final results.To create a correlation matrix, we type “Linear Correlation” in the node repository, then drag and drop it to our workflow.After we drag and drop it like shown, we will connect the output of the file reader to the input of the node “Linear Correlation”.Click the green button “Execute” on the topmost panel. Now right click the correlation node and select “View: Correlation Matrix” to generate the image below.This will help you select the features that are important and required for better predictions by hovering over the particular cell.Next, we will visualize the range and patterns of the dataset to understand it better. One of the primary things we would like to know from our data would be that what item is sold the maximum out of the others.There would be two ways to interpret the information:Search for “Scatter Plot” under the Views tab in our node repository. Drag and drop it in a similar fashion to your workflow, and connect the output of File Reader to this node.Next, configure your node to select how many rows of the data you need and wish to visualize. [I chose 3000]
Click execute, and then View: Scatter Plot.I have selected the X axis to be Item_Type and the Y axis to be Item_Outlet_Sales.The plot above represents the sales of each item type individually, and shows us that fruits and vegetables are sold in the highest numbers.To understand an average sales estimate of all product types in our database, we will use a pie chart.Click on the Pie Chart node under Views and connect it to your File Reader. Choose the columns you need for segregation and choose your preferred aggregation methods, then apply.This chart shows us that sales were averagely divided over all kinds of products. “Starchy Foods” amassed the highest average sales of 7.7%.I have used only two types of visuals although you can explore the data in numerous forms while you browse through the “Views” tab. You can use histograms, line plots etc. to better visualize your data. The other things you can include in your approach before training your model are Data Cleaning and Feature Extraction. Here I will cover an overview of data cleaning steps in KNIME. For further understanding, follow this article on Data Exploration and Feature Engineering. Before we impute values, we need to know which ones are missing.Go to the node repository again, and find the node “Missing Values”. Drag and drop it, and connect the output of our File Reader to the node. To impute values, select the node Missing Value and click configure. Select the appropriate imputations you want for your data depending on the type of data it is, and “Apply”.Now when we execute it, our complete dataset with imputed values is ready in the output port of the node “Missing Value”. For my analysis, I have chosen the imputation methods as:String: Most Frequent ValueNumber (Double): MedianNumber (Integer): MedianYou can choose from a variety of imputation techniques such as:String: Number (Double and Integer): Let us take a look at how we would build a machine learning model in KNIME. To start with the basics, we will first train a Linear Model encompassing all the features of the dataset just to understand how to select features and build a model.Go to your node repository and drag the “Linear Regression Learner” to your workflow. Then connect the clean data that you gathered in the “Output Port” of the “Missing Value” node.This should be your screen visual as of now. In the configuration tab, exclude the Item_Identifier and select the target variable on top. After you complete this task, you need to import your Test data to run your model.Drag and drop another file reader to your workflow and select the test data from your system.As we can see, the Test data contains missing values as well. We will run it through the “Missing Value” node in the same way we did for the Training data.After we’ve cleaned our Test data as well, we will now introduce a new node “Regression Predictor”.Load your Model into the predictor by connecting the learner’s output to the predictor’s input. In the predictor’s second input, load your test data. The predictor will automatically adjust the prediction column based on your learner, but you can alter it manually as well.KNIME has the capability to train some very specialised models as well under the “Analytics” tab. Here is an in-exhaustive list After you execute your predictor now, the output is almost ready for submission.Find the node “Column Filter” in your node repository and drag it to your workflow. Connect the output of your predictor to the column filter and configure it to filter out the columns you need. In this case, you need Item_Identifier, Outlet_Identifier and the Prediction of Outlet_Sales.Execute the “Column Filter” and finally, search for the node “CSV Writer” and document your predictions on your hard drive.Adjust the path to set it where you want the .csv file stored, and execute this node. Finally, open the .csv file to correct the column names as according to our solution. Compress the .csv file into a .zip file and submit your solution!This is the final workflow diagram that was obtained.KNIME workflows are very handy when it comes to portability. They can be sent to your friends or colleagues to build on together, adding to the functionality of your product!To export a KNIME workflow, you can simply click on File -> Export KNIME WorkflowAfter that, select the suitable workflow that you need to export and click finish!This will create a .knwf file that you can send across to anyone and they will be able to access it with one click! KNIME being a very powerful open source tool, has its own set of limitations. The primary ones being: KNIME is a platform that can be used for almost any kind of analysis. In this article, we explored how to visualise a dataset and extract important features from it. Predictive modelling was undertaken as well, using a linear regression predictor to estimate sales for each item accordingly. Finally, we filtered out the required columns and exported it to a .csv file.Hope this tutorial has helped you uncover aspects of the problem that you might have overlooked before. It is very important to understand the data science pipeline and the steps we take to train a model, and this should surely help you build better predictive models soon. Good luck with your endeavors!",https://www.analyticsvidhya.com/blog/2017/08/knime-machine-learning/
Finding chairs the data scientist way! (Hint: using Deep Learning) – Part I,Learn everything about Analytics|Introduction|Table of Contents|Why did I choose to solve this problem?|Simplifying the problem: Chair Recognition in clear image|Taking a step further: Detecting chair location|Challenges and Future Steps|End Notes,"Tasks in the problem|Solving the chair – desk problem|Applying YOLO for Chair Detection|Issue 1|Issue 2|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Building your first machine learning model using KNIME (no coding required!)|CatBoost: A machine learning library to handle categorical (CAT) data automatically|
Faizan Shaikh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I have been going through the deep learning literature for quite some time now. I have also participated in a few challenges to get my hands dirty.But what I enjoy the most is to apply deep learning in a real life problem. A real life problem which encompasses my daily life. This is partly why I picked up this problem of chair count recognition, to finally solve a problem which was unsolved till now!In this article, I will cover how I defined the problem. I will also mention what were the steps I took to solve the problem. Consider it as a raw uncut version of my experience as I tried to solve the problem 🙂  Let me provide a bit of background about why I wanted to count chairs in a photograph.At Analytics Vidhya, we usually have 10-15 people in the office. But in summers, interns crowd our habitat. So, if we have to do an all team meetings in Summers – we end up pulling chairs from all other roomsGiven my laziness, I thought – what if there was an algorithm which could suggest us which room has an unoccupied chair? This will save us the hassle of going from one room to the other in search of a chair.This seemed to be a simple and mundane enough problem, but I saw it as a chance to try out my newly acquired prowess! Can deep learning be used to solve this problem? Well, honestly I don’t know how much it could help, but no harm in trying it out right? Now you know what the problem was, let me explain to you my thought process in solving the problem. We can break down the problem into four tasks –I decided that I should go from a comparatively easy problem to a more complex problem to reach my goal. That is the reason I divided the problem into these specific four tasks. In this article, I will cover how I attempted the first two tasks and then in the subsequent article, I will show you my attempts for the next two problems. The first and the simplest task for our problem is to find out whether we have a chair in the picture clicked in a room. As of now, I simplified the problem by ignoring the need of video feed by manually taking pictures of the room.For example, if I give you two images, can you tell which one is of a chair?     If you have guessed correctly, it’s the first one. So how did you guess it?You have probably seen a chair so many times that it is not difficult for you to infer if there is a chair in the image or not. In short, you have prior knowledge of what a chair looks like in reality. Similarly, we can have a trained artificial neural network which can do the exact thing for us.By the way, we choose to use artificial neural network over other algorithms because right now, Neural nets are the most powerful and state-of-the-art techniques for solving image processing problems.So what I did was, I took an out-of-the-box pre-trained neural network and applied it to these images. This network was previously trained on ImageNet dataset, which has an assortment of all sorts of classes that are found in the reality.But there was an issue when I let the model recognize an object in the image. It could not correctly classify what object was present in the image. For example, here is an output for the image given belowOn the contrary, it predicted that the image contained a desk rather than a chair. This seemed disheartening because a desk and a chair have very few similarities. A desk is much broader in shape than a chair.  As mentioned in m previous article, whenever I encounter a problem when building neural networks, I go through a stepwise approach to tackle the issue. I’ll just list down the steps:Step 1: Check the architecture
Step 2: Check the hyper-parameters of neural network
Step 3: Check the Complexity of network
Step 4: Check the Structure of Input data
Step 5: Check the Distribution of dataHere after evaluation, I found that the image input I was giving to the model was incorrect. I was not properly handling the aspect ratio of the image. So to take care of this, I added a custom code which was mentioned in one the keras’s issues on github. The updated image looked like this.After taking care of the issue, the model started working correctly and giving out right results. Now that we have recognized that our image contains a chair, the next step was to identify where in the image is the chair present. Along with the chair, we also have to recognize and identify a person in the image. We need to identify a person to discern the occupancy of the chair. Both of these tasks (task 2 and task 3 respectively) will help us to solve a much bigger task of finding out if the chair is occupied or not.For this too, as with the previous task, we will use a pre-trained network which will give us an acceptable score out-of-the-box. For object detection, currently, YOLO network is one the best models which gives a great performance in real time. I have covered a bit about YOLO and how it works in this article. Let us look at how we can leverage this to solve our problem. To setup YOLO in the system, the following simple steps can be followed:Step 1:Step 2:Now to run this to solve our problem, you have to type the below command and give the location of your own imageAfter applying YOLO on our images, I saw that it gave pretty good results. Let me show you some examples of what it can do.                                                                      Although we have a decent start, there are still some issues which would hinder the deployment of the project as a full-fledged product. I will list down a few of them:The YOLO model still made some mistakes, i.e. it was not a 100% accurate model. For example, in the image below; even a dustbin is categorized as a person!  What if in an image, a chair obstructs the view of another chair? Would our algorithm be able to identify the hidden chair? This is a point to ponder upon. Along with these issues, there are some more practical implementation details, like how much time does our algorithm take to recommend a solution, what kind of hardware does it require to run etc. These all things are certainly to be considered before selling our algorithm as a product!Also, as I said earlier that we have only considered the first two tasks and haven’t touched upon the next two tasks. Our next steps would be to identify the count of chairs in the room and then build an end-to-end product. In this article, I described my personal experience of solving a real life problem. This article covers object detection and recognition in an image; the object specifically being a chair.  For recognition, we used a simple pre-trained model for predicting the object in an image. On the other hand, for detection, we used YOLO, which is a state-of-the-art real time technique for object detection.I will continue on with chair count in the next part of the article, where we will cover how to calculate the count of chairs. I hope this will help you solve your own problem someday. Good luck!",https://www.analyticsvidhya.com/blog/2017/08/finding-chairs-deep-learning-part-i/
CatBoost: A machine learning library to handle categorical (CAT) data automatically,Learn everything about Analytics|Introduction|Table of Contents|1. What is CatBoost?|2. Advantages of CatBoost Library|3. CatBoost – Comparison to other boosting libraries|4. Installing CatBoost|5. Solving ML challenge using CatBoost|6. End Notes,"Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Finding chairs the data scientist way! (Hint: using Deep Learning) – Part I|Mining frequent items bought together using Apriori Algorithm (with code in R)|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"How many of you have seen this error while building your machine learning models using “sklearn”?
I bet most of us! At least in the initial days.This error occurs when dealing with categorical (string) variables. In sklearn, you are required to convert these categories in the numerical format.In order to do this conversion, we use several pre-processing methods like “label encoding”, “one hot encoding” and others.In this article, I will discuss a recently open sourced library ” CatBoost” developed and contributed by Yandex. CatBoost can use categorical features directly and is scalable in nature.“This is the first Russian machine learning technology that’s an open source,” said Mikhail Bilenko, Yandex’s head of machine intelligence and research.P.S. You can also read this article written by me before “How to deal with categorical variables?“.  CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.It is especially powerful in two ways:“CatBoost” name comes from two words “Category” and “Boosting”.As discussed, the library works well with multiple Categories of data, such as audio, text, image including historical data.“Boost” comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.Here is a video message of Mikhail Bilenko, Yandex’s head of machine intelligence and research and Anna Veronika Dorogush, Head of Tandex machine learning systems.  We have multiple boosting libraries like XGBoost, H2O and LightGBM and all of these perform well on variety of problems. CatBoost developer have compared the performance with competitors on standard ML datasets:The comparison above shows the log-loss value for test data and it is lowest in the case of CatBoost in most cases. It clearly signifies that CatBoost mostly performs better for both tuned and default models.In addition to this, CatBoost does not require conversion of data set to any specific format like XGBoost and LightGBM. CatBoost is easy to install for both Python and R. You need to have 64 bit version of python and R.Below is installation steps for Python and R:4.1 Python Installation:4.2 R Installation The CatBoost library can be used to solve both classification and regression challenge. For classification, you can use “CatBoostClassifier” and for regression, “CatBoostRegressor“.Here’s a live coding window for you to play around the CatBoost code and see the results in real-time:﻿In this article, I’m solving “Big Mart Sales” practice problem using CatBoost. It is a regression challenge so we will use CatBoostRegressor, first I will read basic steps (I’ll not perform feature engineering just build a basic model).Now, you’ll see that we will only identify categorical variables. We will not perform any preprocessing steps for categorical variables:As you can see that a basic model is giving a fair solution and training & testing error are in sync. You can tune model parameters, features to improve the solution.Now, the next task is to predict the outcome for test data set.That’s it! We have built first model with CatBoost In this article, we saw a recently open sourced boosting library “CatBoost” by Yandex which can provide state of the art solution for the variety of business problems.One of the key features which excites me about this library is handling categorical values automatically using various statistical methods.We have covered basic details about this library and solved a regression challenge in this article. I’ll also recommend you to use this library to solve a business solution and check performance against another state of art models.",https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/
Mining frequent items bought together using Apriori Algorithm (with code in R),Learn everything about Analytics|Introduction:|Table of Contents:  |1. The Approach(Apriori Algorithm)|2. Implementing Apriori Algorithm and Key Terms and Usage|3. Interpretations and Analysis|4. End Notes and Summary,"1.1 Handling and Readying The Dataset|1.2 Structural Overview and Prerequisites|3.1 The Item Frequency Histogram|3.2 Graphical Representation|3.3 Individual Rule Representation|3.4 Interactive Scatterplot|Share this:|Like this:|Related Articles|CatBoost: A machine learning library to handle categorical (CAT) data automatically|10 Advanced Deep Learning Architectures Data Scientists Should Know!|
Analytics Vidhya Content Team
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"We live in a fast changing digital world. In today’s age customers expect the sellers to tell what they might want to buy. I personally end up using Amazon’s recommendations almost in all my visits to their site.This creates an interesting threat / opportunity situation for the retailers.If you can tell the customers what they might want to buy – it not only improves your sales, but also the customer experience and ultimately life time value.On the other hand, if you are unable to predict the next purchase, the customer might not come back to your store.In this article, we will learn one such algorithm which enables us to predict the items bought together frequently. Once we know this, we can use it to our advantage in multiple ways.  When you go to a store, would you not want the aisles to be ordered in such a manner that reduces your efforts to buy things?For example, I would want the toothbrush, the paste, the mouthwash  & other dental products on a single aisle – because when I buy, I tend to buy them together. This is done by a way in which we find associations between items.In order to understand the concept better, let’s take a simple dataset (let’s name it as Coffee dataset) consisting of a few hypothetical transactions. We will try to understand this in simple English.The Coffee dataset consisting of items purchased from a retail store.Coffee dataset:The Association Rules: For this dataset, we can write the following association rules: (Rules are just for illustrations and understanding of the concept. They might not represent the actuals).Rule 1: If Milk is purchased, then Sugar is also purchased.Rule 2:  If Sugar is purchased, then Milk is also purchased.Rule 3: If Milk and Sugar are purchased, Then Coffee powder is also purchased in 60% of the transactions.Generally, association rules are written in “IF-THEN” format. We can also use the term “Antecedent” for IF (LHS) and “Consequent” for THEN (RHS).From the above rules, we understand the following explicitly:For example, if we see {Milk} as a set with one item and {Coffee} as another set with one item, we will use these to find sets with two items in the dataset such as {Milk,Coffee} and then later see which products are purchased with both of these in our basket.Therefore now we will search for a suitable right hand side or Consequent. If someone buys Coffee with Milk, we will represent it as {Coffee} => {Milk} where Coffee becomes the LHS and Milk the RHS.When we use these to explore more k-item sets, we might find that {Coffee,Milk} => {Tea}.That means the people who buy Coffee and Milk have a possibility of buying Tea as well.Let us see how the item sets are actually built using the Apriori.Apriori envisions an iterative approach where it uses k-Item sets to search for (k+1)-Item sets. The first 1-Item sets are found by gathering the count of each item in the set. Then the 1-Item sets are used to find 2-Item sets and so on until no more k-Item sets can be explored; when all our items land up in one final observation as  visible in our last row of the table above. One exploration takes one scan of the complete dataset.An Item set is a mathematical set of products in the basket. The first part of any analysis is to bring in the dataset. We will be using an inbuilt dataset “Groceries” from the ‘arules’ package to simplify our analysis.All stores and retailers store their information of transactions in a specific type of dataset called the “Transaction” type dataset.The ‘pacman’ package is an assistor to help load and install the packages. we will be using pacman to load the arules package.The p_load() function from “pacman” takes names of packages as arguments.If your system has those packages, it will load them and if not, it will install and load them.Example:pacman::p_load(PACKAGE_NAME)pacman::p_load(arules, arulesViz)ORLibrary(arules)Library(arulesViz)data(“Groceries"") Before we begin applying the “Apriori” algorithm on our dataset, we need to make sure that it is of the type “Transactions”.str(Groceries) The structure of our transaction type dataset shows us that it is internally divided into three slots: Data, itemInfo and itemsetInfo.The slot “Data” contains the dimensions, dimension names and other numerical values of number of products sold by every transaction made.These are the first 12 rows of the itemInfo list within the Groceries dataset. It gives specific names to our items under the column “labels”. The “level2” column segregates into an easier to understand term, while “level1” makes the complete generalisation of Meat.The slot itemInfo contains a Data Frame that has three vectors which categorizes the food items in the first vector “Labels”.The second & third vectors divide the food broadly into levels like “baby food”,”bags” etc.The third slot itemsetInfo will be generated by us and will store all associations. This is what the internal visual of any transaction dataset looks like and there is a dataframe containing products bought in each transaction in our first inspection. Then, we can group those products by TransactionID like we did in our second inspection to see how many times each is sold before we begin with associativity analysis.The above datasets are just for a clearer visualisation on how to make a Transaction Dataset and can be reproduced using the following code:data <- list(
c(""a"",""b"",""c""),
c(""a"",""b""),
c(""a"",""b"",""d""),
c(""b"",""e""),
c(""b"",""c"",""e""),
c(""a"",""d"",""e""),
c(""a"",""c""),
c(""a"",""b"",""d""),
c(""c"",""e""),
c(""a"",""b"",""d"",""e""),
c(""a"",'b','e','c')
)
data <- as(data, ""transactions"")inspect(data)#Convert transactions to transaction ID liststl <- as(data, ""tidLists"")
inspect(tl) Let us check the most frequently purchased products using the summary function.summary(Groceries)The summary statistics show us the top 5 items sold in our transaction set as “Whole Milk”,”Other Vegetables”,”Rolls/Buns”,”Soda” and “Yogurt”. (Further explained in Section 3)To parse to Transaction type, make sure your dataset has similar slots and then use the as() function in R. rules <- apriori(Groceries,parameter = list(supp = 0.001, conf = 0.80))We will set minimum support parameter (minSup) to .001.We can set minimum confidence (minConf) to anywhere between 0.75 and 0.85 for varied results.I have used support and confidence in my parameter list. Let me try to explain it:Support: Support is the basic probability of an event to occur. If we have an event to buy product A, Support(A) is the number of transactions which includes A divided by total number of transactions.Confidence: The confidence of an event is the conditional probability of the occurrence; the chances of A happening given B has already happened.Lift: This is the ratio of confidence to expected confidence.The probability of all of the items in a rule occurring together (otherwise known as the support) divided by the product of the probabilities of the items on the left and right side occurring as if there was no association between them.The lift value tells us how much better a rule is at predicting something than randomly guessing. The higher the lift, the stronger the association.Let’s find out the top 10 rules arranged by lift.inspect(rules[1:10]) As we can see, these are the top 10 rules derived from our Groceries dataset by running the above code.The first rule shows that if we buy Liquor and Red Wine, we are very likely to buy bottled beer. We can rank the rules based on top 10 from either lift, support or confidence.Let’s plot all our rules in certain visualisations first to see what goes with what item in our shop. Let us first identify which products were sold how frequently in our dataset. These histograms depict how many times an item has occurred in our dataset as compared to the others.The relative frequency plot accounts for the fact that “Whole Milk” and “Other Vegetables” constitute around half of the transaction dataset; half the sales of the store are of these items.arules::itemFrequencyPlot(Groceries,topN=20,col=brewer.pal(8,'Pastel2'),main='Relative Item Frequency Plot',type=""relative"",ylab=""Item Frequency (Relative)"")This would mean that a lot of people are buying milk and vegetables!What other objects can we place around the more frequently purchased objects to enhance those sales too?For example, to boost sales of eggs I can place it beside my milk and vegetables. Moving forward in the visualisation, we can use a graph to highlight the support and lifts of various items in our repository but mostly to see which product is associated with which one in the sales environment.plot(rules[1:20],method = ""graph"",control = list(type = ""items""))This representation gives us a graph model of items in our dataset.The size of graph nodes is based on support levels and the colour on lift ratios. The incoming lines show the Antecedants or the LHS and the RHS is represented by names of items. The above graph shows us that most of our transactions were consolidated around “Whole Milk”.We also see that all liquor and wine are very strongly associated so we must place these together.Another association we see from this graph is that the people who buy tropical fruits and herbs also buy rolls and buns. We should place these in an aisle together. The next plot offers us a parallel coordinate system of visualisation. It would help us clearly see that which products along with which ones, result in what kinds of sales.As mentioned above, the RHS is the Consequent or the item we propose the customer will buy; the positions are in the LHS where 2 is the most recent addition to our basket and 1 is the item we previously had.The topmost rule shows us that when I have whole milk and soups in my shopping cart, I am highly likely to buy other vegetables to go along with those as well.plot(rules[1:20],method = ""paracoord"",control = list(reorder = TRUE))If we want a matrix representation, an alternate code option would be:plot(rules[1:20],method = ""matrix"",control = list(reorder = TRUE) These plots show us each and every rule visualised into a form of a scatterplot. The confidence levels are plotted on the Y axis and Support levels on the X axis for each rule. We can hover over them in our interactive plot to see the rule. Plot: arulesViz::plotly_arules(rules)The plot uses the arulesViz package and plotly to generate an interactive plot. We can hover over each rule and see the Support, Confidence and Lift.As the interactive plot suggests, one rule that has a confidence of 1 is the one above. It has an exceptionally high lift as well, at 5.17. By visualising these rules and plots, we can come up with a more detailed explanation of how to make business decisions in retail environments.Now, we would place “Whole Milk” and “Vegetables” beside each other; “Wine” and “Bottled Beer” alongside too.I can make some specific aisles now in my store to help customers pick products easily from one place and also boost the store sales simultaneously.Aisles Proposed: This analysis would help us improve our store sales and make calculated business decisions for people both in a hurry and the ones leisurely shopping.Happy Association Mining!",https://www.analyticsvidhya.com/blog/2017/08/mining-frequent-items-using-apriori-algorithm/
10 Advanced Deep Learning Architectures Data Scientists Should Know!,Learn everything about Analytics|Introduction||Table of Contents|What do we mean by an Advanced Architecture?||Types of Computer Vision Tasks|List of Deep Learning Architectures||||||||End Notes,"1. AlexNet|2. VGG Net|3. GoogleNet|4. ResNet|5. ResNeXt|6. RCNN (Region Based CNN)|7. YOLO (You Only Look Once)|8. SqueezeNet|9. SegNet|10. GAN (Generative Adversarial Network)|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Mining frequent items bought together using Apriori Algorithm (with code in R)|DataHack Summit 2017 – India’s largest conference for data science practitioners|
Faizan Shaikh
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"It is becoming very hard to stay up to date with recent advancements happening in deep learning. Hardly a day goes by without a new innovation or a new application of deep learning coming by. However, most of these advancements are hidden inside a large amount of research papers that are published on mediums like ArXiv / SpringerTo keep ourselves updated, we have created a small reading group to share our learnings internally at Analytics Vidhya. One such learning I would like to share with the community is a a survey of advanced architectures which have been developed by the research community.This article contains some of the recent advancements in Deep Learning along with codes for implementation in keras library. I have also provided links to the original papers, in case you are interested in reading them or want to refer them.To keep the article concise, I have only considered the architectures which have been successful in Computer Vision domain.If you are interested, read on!P.S.: This article assumes the knowledge of neural networks and familiarity with keras. If you need to catch up on these topics, I would strongly recommend you read the following articles first:  Deep Learning algorithms consists of such a diverse set of models in comparison to a single traditional machine learning algorithm. This is because of the flexibility that neural network provides when building a full fledged end-to-end model.Neural network can sometimes be compared with lego blocks, where you can build almost any simple to complex structure your imagination helps you to build.We can define an advanced architecture as one that has a proven track record of being a successful model. This is mainly seen in challenges like ImageNet, where your task is to solve a problem, say image recognition, using the data given. Those who don’t know what ImageNet is, it is the dataset which is provided in ILSVR (ImageNet Large Scale Visual Recognition) challenge.Also as described in the below mentioned architectures, each of them has a nuance which sets them apart from the usual models; giving them an edge when they are used to solve a problem. These architectures also fall in the category of “deep” models, so they are likely to perform better than their shallow counterparts. This article is mainly focused on Computer Vision, so it is natural to describe the horizon of computer vision tasks. Computer Vision; as the name suggests is simply creating artificial models which can replicate the visual tasks performed by a human. This essentially means what we can see and what we perceive is a process which can be understood and implemented in an artificial system.The main types of tasks that computer vision can be categorised in are as follows: Now that we have understood what an advanced architecture is and explored the tasks of computer vision, let us list down the most important architectures and their descriptions:AlexNet is the first deep architecture which was introduced by one of the pioneers in deep learning – Geoffrey Hinton and his colleagues. It is a simple yet powerful network architecture, which helped pave the way for groundbreaking research in Deep Learning as it is now. Here is a representation of the architecture as proposed by the authors.When broken down, AlexNet seems like a simple architecture with convolutional and pooling layers one on top of the other, followed by fully connected layers at the top. This is a very simple architecture, which was conceptualised way back in 1980s. The things which set apart this model is the scale at which it performs the task and the use of GPU for training. In 1980s, CPU was used for training a neural network. Whereas AlexNet speeds up the training by 10 times just by the use of GPU.Although a bit outdated at the moment, AlexNet is still used as a starting point for applying deep neural networks for all the tasks, whether it be computer vision or speech recognition. The VGG Network was introduced by the researchers at Visual Graphics Group at Oxford (hence the name VGG). This network is specially characterized by its pyramidal shape, where the bottom layers which are closer to the image are wide, whereas the top layers are deep.As the image depicts, VGG contains subsequent convolutional layers followed by pooling layers. The pooling layers are responsible for making the layers narrower. In their paper, they proposed multiple such types of networks, with change in deepness of the architecture.The advantages of VGG are :On the other hand, its main disadvantage is that it is very slow to train if trained from scratch. Even on a decent GPU, it would take more than a week to get it to work. GoogleNet (or Inception Network) is a class of architecture designed by researchers at Google. GoogleNet was the winner of ImageNet 2014, where it proved to be a powerful model.In this architecture, along with going deeper (it contains 22 layers in comparison to VGG which had 19 layers), the researchers also made a novel approach called the Inception module.As seen above, it is a drastic change from the sequential architectures which we saw previously. In a single layer, multiple types of “feature extractors” are present. This indirectly helps the network perform better, as the network at training itself has many options to choose from when solving the task. It can either choose to convolve the input, or to pool it directly.The final architecture contains multiple of these inception modules stacked one over the other. Even the training is slightly different in GoogleNet, as most of the topmost layers have their own output layer. This nuance helps the model converge faster, as there is a joint training as well as parallel training for the layers itself.The advantages of GoogleNet are :GoogleNet does not have an immediate disadvantage per se, but further changes in the architecture  are proposed, which make the model perform better. One such change is termed as an Xception Network, in which the limit of divergence of inception module (4 in GoogleNet as we saw in the image above) are increased. It can now theoretically be infinite (hence called extreme inception!) ResNet is one of the monster architectures which truly define how deep a deep learning architecture can be. Residual Networks (ResNet in short) consists of multiple subsequent residual modules, which are the basic building block of ResNet architecture. A representation of residual module is as followsIn simple words, a residual module has two options, either it can perform a set of functions on the input, or it can skip this step altogether.Now similar to GoogleNet, these residual modules are stacked one over the other to form a complete end-to-end network.A few more novel techniques which ResNet introduced are:The main advantage of ResNet is that hundreds, even thousands of these residual layers can be used to create a network and then trained. This is a bit different from usual sequential networks, where you see that there is reduced performance upgrades as you increase the number of layers. ResNeXt is said to be the current state-of-the-art technique for object recognition. It builds upon the concepts of inception and resnet to bring about a new and improved architecture. Below image is a summarization of how a residual module of ResNeXt module looks like. Region Based CNN architecture is said to be the most influential of all the deep learning architectures that have been applied to object detection problem. To solve detection problem, what RCNN does is to attempt to draw a bounding box over all the objects present in the image, and then recognize what object is in the image. It works as follows:The structure of RCNN is as follows: YOLO is the current state-of-the-art real time system built on deep learning for solving image detection problems. As seen in the below given image, it first divides the image into defined bounding boxes, and then runs a recognition algorithm in parallel for all of these boxes to identify which object class do they belong to. After identifying this classes, it goes on to merging these boxes intelligently to form an optimal bounding box around the objects.All of this is done in parallely, so it can run in real time; processing upto 40 images in a second.Although it gives reduced performance than its RCNN counterpart, it still has an advantage of being real time to be viable for use in day-to-day problems. Here is a representation of architecture of YOLO The squeezeNet architecture is one more powerful architecture which is extremely useful in low bandwidth scenarios like mobile platforms. This architecture has occupies only 4.9MB of space, on the other hand, inception occupies ~100MB! This drastic change is brought up by a specialized structure called the fire module. Below image is a representation of fire module.The final architecture of squeezeNet is as follows: SegNet is a deep learning architecture applied to solve image segmentation problem. It consists of sequence of processing layers (encoders) followed by a corresponding set of decoders for a pixelwise classification . Below image summarizes the working of SegNet.One key feature of SegNet is that it retains high frequency details in segmented image as the pooling indices of encoder network is connected to pooling indices of decoder networks. In short, the information transfer is direct instead of convolving them. SegNet is one the the best model to use when dealing with image segmentation problems GAN is an entirely different breed of neural network architectures, in which a neural network is used to generate an entirely new image which is not present is the training dataset, but is realistic enough to be in the dataset. For example, below image is a breakdown of GANs are made of. I have covered how GANs work in this article. Go through it if you are curious. In this article, I have covered an overview of major deep learning architectures that you should get familiar with. If you have any questions on deep learning architectures, please feel free to share them with me through comments.",https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/
DataHack Summit 2017 – India’s largest conference for data science practitioners,Learn everything about Analytics|What is DataHack Summit?|Why do DataHack Summit?|What can you expect from DataHack Summit?|List of confirmed speakers / events|Early Bird Pricing,"Share this:|Like this:|Related Articles|10 Advanced Deep Learning Architectures Data Scientists Should Know!|30 Questions to test your understanding of Logistic Regression|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Analytics Vidhya is on a mission – to create next generation of data scientists in India.Over the last 4 years, we have helped millions of people learn data science. We have enabled career transitions and we have seen our community members go through a transformation in their data science journey. I am pretty sure this is just the start – a signal of things to come. I feel fortunate to be able to create a change at this scale!Today, I am excited to announce our largest initiative in this direction – DataHack Summit 2017. If you follow us, you would have already seen some details about DataHack Summit. But, let me present our perspective. DataHack Summit (DHS) is a conference aimed towards data science practitioners. It is a conference which celebrates the awesome work being done by data scientists across the globe and showcases it to the rest of the community. It is a conference where we talk data science, we breathe data science and we experience data science like never before.DataHack Summit is a festival for those who don’t see numbers as just numbers. It is a festival for those who see numbers as designs and patterns and trends, those who see and appreciate this art of dealing with data.DHS 2017 aims to show the bleeding edge, the horizon and the impact of data science to the professionals who perform it every day. It aims to inspire you with the new tools, techniques and applications by bringing the best in data science together. We at Analytics Vidhya feel passionately about enhancing data science culture in India. We want people to talk, showcase and learn data science.However, most of the events we have attended in past tend to leave you high and dry. People talk about benefits of data science from a high level and leave out the details. We haven’t seen people talk about the real challenges faced while solving data problems. For the last few years, we have never walked out of a conference in India feeling satisfied from the learning from the event.DataHack Summit aims to change this. We aim to create a conference where people take pride in talking about the intricacies in data science. We aim to create a conference which leaves you in company of several like minded passionate people. We aim to create a event where data science evolves in India.  You can check out the latest update here. Here is a screenshot of some of the confirmed speakers:Dr. Kirk Borne has graciously accepted to be a Keynote speaker for the event. Every speaker is a practicing Data Scientist and comes with tremendous experience in the domain. We will put all our experience in creating top quality content with these world class speakers to make these talks awesome.In addition to the talks and workshops, we will have a fun filled evening. More details to come out soon. The early bird tickets are available till 7th August 2017. Make sure you buy your tickets before the early bird goes away! You will only regret missing it later.Hope this preview gets you as excited as we are about DataHack Summit 2017. If you have any thoughts / suggestions / questions, happy to take them here.Looking forward to see you in Bangalore.",https://www.analyticsvidhya.com/blog/2017/08/datahack-summit-2017-india-data-science-practitioners-conference/
30 Questions to test your understanding of Logistic Regression,Learn everything about Analytics|Introduction|Overall Distribution| |Helpful Resources|Skill test Questions and Answers|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|DataHack Summit 2017 – India’s largest conference for data science practitioners|30 Questions to test a Data Scientist on Deep Learning (Solution – Skill test, July 2017)|
Ankit Gupta
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Logistic Regression is likely the most commonly used algorithm for solving all classification problems. It is also one of the first methods people get their hands dirty on.We saw the same spirit on the test we designed to assess people on Logistic Regression. More than 800 people took this test. This skill test is specially designed for you to test your knowledge on logistic regression and its nuances.If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how many could have answered correctly.Here is the leaderboard for the participants who took the test. Below is the distribution of the scores of the participants:You can access the scores here. More than 800 people participated in the skill test and the highest score obtained was 27. Here are some resources to get in depth knowledge in the subject. 1) True-False: Is Logistic regression a supervised machine learning algorithm?A) TRUE
B) FALSESolution: ATrue, Logistic regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variables (x) and an target variable (Y) when you train the model . 2) True-False: Is Logistic regression mainly used for Regression?A) TRUE
B) FALSESolution: BLogistic regression is a classification algorithm, don’t confuse with the name regression. 3) True-False: Is it possible to design a logistic regression algorithm using a Neural Network Algorithm?A) TRUE
B) FALSESolution: ATrue, Neural network is a is a universal approximator so it can implement linear regression algorithm. 4) True-False: Is it possible to apply a logistic regression algorithm on a 3-class Classification problem?A) TRUE
B) FALSESolution: AYes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression. 5) Which of the following methods do we use to best fit the data in Logistic Regression?A) Least Square Error
B) Maximum Likelihood
C) Jaccard distance
D) Both A and BSolution: BLogistic regression uses maximum likely hood estimate for training a logistic regression. 6) Which of the following evaluation metrics can not be applied in case of logistic regression output to compare with target?A) AUC-ROC
B) Accuracy
C) Logloss
D) Mean-Squared-ErrorSolution: DSince, Logistic Regression is a classification algorithm so it’s output can not be real time value so mean squared error can not use for evaluating it 7) One of the very good methods to analyze the performance of Logistic Regression is AIC, which is similar to R-Squared in Linear Regression. Which of the following is true about AIC?A) We prefer a model with minimum AIC value
B) We prefer a model with maximum AIC value
C) Both but depend on the situation
D) None of theseSolution: AWe select the best model in logistic regression which can least AIC. For more information refer this source: http://www4.ncsu.edu/~shu3/Presentation/AIC.pdf 8) [True-False] Standardisation of features is required before training a Logistic Regression.A) TRUE
B) FALSESolution: BStandardization isn’t required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization. 9) Which of the following algorithms do we use for Variable Selection?A) LASSO
B) Ridge
C) Both
D) None of theseSolution: AIn case of lasso we apply a absolute penality, after increasing the penality in lasso some of the coefficient of variables may become zero. Context: 10-11Consider a following model for logistic regression: P (y =1|x, w)= g(w0 + w1x)
where g(z) is the logistic function.In the above equation the P (y =1|x; w) , viewed as a function of x, that we can get by changing the parameters w.10) What would be the range of p in such case?
A) (0, inf)
B) (-inf, 0 )
C) (0, 1)
D) (-inf, inf)Solution: CFor values of x in the range of  real number from −∞ to +∞ Logistic function will give the output between (0,1) 11) In above question what do you think which function would make p between (0,1)?

A) logistic function
B) Log likelihood function
C) Mixture of both
D) None of themSolution: AExplanation is same as question number 10 Context: 12-13Suppose you train a logistic regression classifier and your hypothesis function H is  12) Which of the following figure will represent the decision boundary as given by above classifier?A)B)C)D) Solution: BOption B would be the right answer. Since our line will be represented by y = g(-6+x2) which is shown in the option A and option B. But option B is the right answer because when you put the value x2 = 6 in the equation then y = g(0) you will get that means y= 0.5 will be on the line, if you increase the value of x2 greater then 6 you will get negative values so output will be the region y =0. 13) If you replace coefficient of x1 with x2 what would be the output figure?A)
B)
C)D)Solution: DSame explanation as in previous question. 14) Suppose you have been given a fair coin and you want to find out the odds of getting heads. Which of the following option is true for such a case?A) odds will be 0
B) odds will be 0.5
C) odds will be 1
D) None of these
Solution: COdds are defined as the ratio of the probability of success and the probability of failure. So in case of fair coin probability of success is 1/2 and the probability of failure is 1/2 so odd would be 1 15) The logit function(given as l(x)) is the log of odds function. What could be the range of logit function in the domain x=[0,1]?A) (– ∞ , ∞)
B) (0,1)
C) (0, ∞)
D) (- ∞, 0)Solution: AFor our purposes, the odds function has the advantage of transforming the probability function, which has values from 0 to 1, into an equivalent function with values between 0 and ∞. When we take the natural log of the odds function, we get a range of values from -∞ to ∞. 16) Which of the following option is true?A) Linear Regression errors values has to be normally distributed but in case of Logistic Regression it is not the case
B) Logistic Regression errors values has to be normally distributed but in case of Linear Regression it is not the case
C) Both Linear Regression and Logistic Regression error values have to be normally distributed
D) Both Linear Regression and Logistic Regression error values have not to be normally distributedSolution:AOnly A is true. Refer this tutorial https://czep.net/stat/mlelr.pdf 17) Which of the following is true regarding the logistic function for any value “x”?Note:
Logistic(x): is a logistic function of any number “x”Logit(x): is a logit function of any number “x”Logit_inv(x): is a inverse logit function of any number “x”A) Logistic(x) = Logit(x)
B) Logistic(x) = Logit_inv(x)
C) Logit_inv(x) = Logit(x)
D) None of theseSolution: BRefer this link for the solution: https://en.wikipedia.org/wiki/Logit 18) How will the bias change on using high(infinite) regularisation?Suppose you have given the two scatter plot “a” and “b” for two classes( blue for positive and red for negative class). In scatter plot “a”, you correctly classified all data points using logistic regression ( black line is a decision boundary). A) Bias will be high
B) Bias will be low
C) Can’t say
D) None of theseSolution: AModel will become very simple so bias will be very high. 19) Suppose, You applied a Logistic Regression model on a given data and got a training accuracy X and testing accuracy Y. Now, you want to add a few new features in the same data. Select the option(s) which is/are correct in such a case.Note: Consider remaining parameters are same.A) Training accuracy increases
B) Training accuracy increases or remains the same
C) Testing accuracy decreases
D) Testing accuracy increases or remains the sameSolution: A and D
Adding more features to model will increase the training accuracy because model has to consider more data to fit the logistic regression. But testing accuracy increases if feature is found to be significant 20) Choose which of the following options is true regarding One-Vs-All method in Logistic Regression.A) We need to fit n models in n-class classification problem
B) We need to fit n-1 models to classify into n classes
C) We need to fit only 1 model to classify into n classes
D) None of theseSolution: AIf there are n classes, then n separate logistic regression has to fit, where the probability of each category is predicted over the rest of the categories combined. 21) Below are two different logistic models with different values for β0 and β1.Which of the following statement(s) is true about β0 and β1 values of two logistics models (Green, Black)?Note: consider Y = β0 + β1*X. Here, β0 is intercept and β1 is coefficient.A) β1 for Green is greater than Black
B) β1 for Green is lower than Black
C) β1 for both models is same
D) Can’t SaySolution: Bβ0 and β1: β0 = 0, β1 = 1 is in X1 color(black) and β0 = 0, β1 = −1 is in X4 color (green) Context 22-24Below are the three scatter plot(A,B,C left to right) and hand drawn decision boundaries for logistic regression.22) Which of the following above figure shows that the decision boundary is overfitting the training data?A) A
B) B
C) C
D)None of theseSolution: CSince in figure 3, Decision boundary is not smooth that means it will over-fitting the data. 23) What do you conclude after seeing this visualization?A) 1 and 3
B) 1 and 3
C) 1, 3 and 4
D) 5Solution: CThe trend in the graphs looks like a quadratic trend over independent variable X. A higher degree(Right graph) polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset. But if you see in left graph we will have training error maximum because it underfits the training data 24) Suppose, above decision boundaries were generated for the different value of regularization. Which of the above decision boundary shows the maximum regularization?A) A
B) B
C) C
D) All have equal regularizationSolution: ASince, more regularization means more penality means less complex decision boundry that shows in first figure A. 25) The below figure shows AUC-ROC curves for three logistic regression models. Different colors show curves for different hyper parameters values. Which of the following AUC-ROC will give best result?
A) Yellow
B) Pink
C) Black
D) All are sameSolution: AThe best classification is the largest area under the curve so yellow line has largest area under the curve. 26) What would do if you want to train logistic regression on same data that will take less time as well as give the comparatively similar accuracy(may not be same)?Suppose you are using a Logistic Regression model on a huge dataset. One of the problem you may face on such huge data is that Logistic regression will take very long time to train.A) Decrease the learning rate and decrease the number of iteration
B) Decrease the learning rate and increase the number of iteration
C) Increase the learning rate and increase the number of iteration
D) Increase the learning rate and decrease the number of iterationSolution: DIf you decrease the number of iteration while training it will take less time for surly but will not give the same accuracy for getting the similar accuracy but not exact you need to increase the learning rate. 27) Which of the following image is showing the cost function for y =1.Following is the loss function in logistic regression(Y-axis loss function and x axis log probability) for two class classification problem.Note: Y is the target classA) A
B) B
C) Both
D) None of theseSolution: AA is the true answer as loss function decreases as the log probability increases
 28) Suppose, Following graph is a cost function for logistic regression.
Now, How many local minimas are present in the graph?A) 1
B) 2
C) 3
D) 4Solution: CThere are three local minima present in the graph 29) Imagine, you have given the below graph of logistic regression  which is shows the relationships between cost function and number of iteration for 3 different learning rate values (different colors are showing different curves at different learning rates ).  Suppose, you save the graph for future reference but you forgot to save the value of different learning rates for this graph. Now, you want to find out the relation between the leaning rate values of these curve. Which of the following will be the true relation?Note:A) l1>l2>l3
B) l1 = l2 = l3
C) l1 < l2 < l3D) None of theseSolution: CIf you have low learning rate means your cost function will decrease slowly but in case of large learning rate cost function will decrease very fast. 30) Can a Logistic Regression classifier do a perfect classification on the below data?  Note: You can use only X1 and X2 variables where X1 and X2 can take only two binary values(0,1).A) TRUE
B) FALSE
C) Can’t say
D) None of theseSolution: BNo, logistic regression only forms linear decision surface, but the examples in the figure are not linearly separable.https://www.cs.cmu.edu/~tom/10701_sp11/midterm_sol.pdf I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test. For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/
"30 Questions to test a Data Scientist on Deep Learning (Solution – Skill test, July 2017)",Learn everything about Analytics|Introduction|Overall Distribution| |Helpful Resources|Skill test Questions and Answers|End Notes,"Learn, engage, compete, and get hired!|Share this:|Like this:|Related Articles|30 Questions to test your understanding of Logistic Regression|PyData Delhi 2017|
Dishashree Gupta
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Whether you are a novice at data science or a veteran, Deep learning is hard to ignore. And it deserves the attention, as deep learning is helping us achieve the AI dream of getting near human performance in every day tasks.Given the importance to learn Deep learning for a data scientist, we created a skill test to help people assess themselves on Deep Learning. A total of 644 people registered for this skill test.If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how many could have answered correctly.Here is the leaderboard for the participants who took the test. Below is the distribution of the scores of the participants:You can access the scores here. More than 200 people participated in the skill test and the highest score obtained was 26. Interestingly, the distribution of scores ended up being very similar to past 2 tests:Clearly, a lot of people start the test without understanding Deep Learning, which is not the case with other skill tests. This also means that these solutions would be useful to a lot of people. Here are some resources to get in depth knowledge in the subject. 1) Is the data linearly separable?Solution: BIf you can draw a line or plane between the data points, it is said to be linearly separable. 2) Which of the following are universal approximators?A) Kernel SVM
B) Neural Networks
C) Boosted Decision Trees
D) All of the aboveSolution: DAll of the above methods can approximate any function. 3) In which of the following applications can we use deep learning to solve the problem?A) Protein structure prediction
B) Prediction of chemical reactions
C) Detection of exotic particles
D) All of theseSolution: DWe can use neural network to approximate any function so it can theoretically be used to solve any problem. 4) Which of the following statements is true when you use 1×1 convolutions in a CNN?A) It can help in dimensionality reduction
B) It can be used for feature pooling
C) It suffers less overfitting due to small kernel size
D) All of the aboveSolution: D1×1 convolutions are called bottleneck structure in CNN. 5) Question Context:Statement 1: It is possible to train a network well by initializing all the weights as 0
Statement 2: It is possible to train a network well by initializing biases as 0Which of the statements given above is true?A) Statement 1 is true while Statement 2 is false
B) Statement 2 is true while statement 1 is false
C) Both statements are true
D) Both statements are falseSolution: BEven if all the biases are zero, there is a chance that neural network may learn. On the other hand, if all the weights are zero; the neural neural network may never learn to perform the task. 6) The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer areA) 50
B) Less than 50
C) More than 50
D) It is an arbitrary valueSolution: ASince MLP is a fully connected directed graph, the number of connections are a multiple of number of nodes in input layer and hidden layer. 7) The input image has been converted into a matrix of size 28 X 28 and a kernel/filter of size 7 X 7 with a stride of 1. What will be the size of the convoluted matrix?A) 22 X 22
B) 21 X 21
C) 28 X 28
D) 7 X 7Solution: AThe size of the convoluted matrix is given by C=((I-F+2P)/S)+1, where C is the size of the Convoluted matrix, I is the size of the input matrix, F the size of the filter matrix and P the padding applied to the input matrix. Here P=0, I=28, F=7 and S=1.  There the answer is 22. 8) In a simple MLP model with 8 neurons in the input layer, 5 neurons in the hidden layer and 1 neuron in the output layer. What is the size of the weight matrices between hidden output layer and input hidden layer?A) [1 X 5] , [5 X 8]
B) [8 X 5] , [ 1 X 5]
C) [8 X 5] , [5 X 1]
D) [5 x 1] , [8 X 5]
Solution: DThe size of weights between any layer 1 and layer 2 Is given by [nodes in layer 1 X nodes in layer 2]
 9) Given below is an input matrix named I, kernel F and Convoluted matrix named C. Which of the following is the correct option for matrix C with stride =2 ?Solution: C1 and 2 are automatically eliminated since they do not conform to the output size for a stride of 2. Upon calculation option 3 is the correct answer. 10) Given below is an input matrix of shape 7 X 7. What will be the output on applying a max pooling of size 3 X 3 with a stride of 2? Solution: AMax pooling takes a 3 X 3 matrix and takes the maximum of the matrix as the output. Slide it over the entire input matrix with a stride of 2 and you will get option (1) as the answer. 11) Which of the following functions can be used as an activation function in the output layer if we wish to predict the probabilities of n classes (p1, p2..pk) such that sum of p over all n equals to 1?A) Softmax
B) ReLu
C) Sigmoid
D) TanhSolution: ASoftmax function is of the form  in which the sum of probabilities over all k sum to 1. 12) Assume a simple MLP model with 3 neurons and inputs= 1,2,3. The weights to the input neurons are 4,5 and 6 respectively. Assume the activation function is a linear constant value of 3. What will be the output ?A) 32
B) 643
C) 96
D) 48Solution: CThe output will be calculated as 3(1*4+2*5+6*3) = 96 13) Which of following activation function can’t be used at output layer to classify an image ?A) sigmoid
B) Tanh
C) ReLU
D) If(x>5,1,0)
E) None of the above
Solution: CReLU gives continuous output in range 0 to infinity. But in output layer, we want a finite range of values. So option C is correct. 14) [True | False] In the neural network, every parameter can have their different learning rate.A) TRUE
B) FALSESolution: AYes, we can define the learning rate for each parameter and it can be different from other parameters. 15) Dropout can be applied at visible layer of Neural Network model?A) TRUE
B) FALSESolution: ALook at the below model architecture, we have added a new Dropout layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning one in 5 inputs will be randomly excluded from each update cycle.16) I am working with the fully connected architecture having one hidden layer with 3 neurons and one output neuron to solve a binary classification challenge. Below is the structure of input and output:Input dataset: [ [1,0,1,0] , [1,0,1,1] , [0,1,0,1] ]Output: [ [1] , [1] , [0] ]To train the model, I have initialized all weights for hidden and output layer with 1.What do you say model will able to learn the pattern in the data?A) Yes
B) NoSolution: BAs all the weights of the neural network model are same, so all the neurons will try to do the same thing and the model will never converge. 17) Which of the following neural network training challenge can be solved using batch normalization?A) Overfitting
B) Restrict activations to become too high or low
C) Training is too slow
D) Both B and C
E) All of the aboveSolution: DBatch normalization restricts the activations and indirectly improves training time. 18) Which of the following would have a constant input in each epoch of training a Deep Learning model?A) Weight between input and hidden layer
B) Weight between hidden and output layer
C) Biases of all hidden layer neurons
D) Activation function of output layer
E) None of the aboveSolution: AWeights between input and hidden layer are constant. 19) True/False: Changing Sigmoid activation to ReLu will help to get over the vanishing gradient issue? A) TRUE
B) FALSESolution: AReLU can help in solving vanishing gradient problem. 20) In CNN, having max pooling always decrease the parameters? A) TRUE
B) FALSESolution: BThis is not always true. If we have a max pooling layer of pooling size as 1, the parameters would remain the same. 21) [True or False] BackPropogation cannot be applied when using pooling layersA) TRUE
B) FALSESolution: BBackPropogation can be applied on pooling layers too. 22) What value would be in place of question mark?A) 3
B) 4
C) 5
D) 6Solution: BOption B is correct 23) For a binary classification problem, which of the following architecture would you choose?A) 1
B) 2
C) Any one of these
D) None of theseSolution: CWe can either use one neuron as output for binary classification problem or two separate neurons. 24) Suppose there is an issue while training a neural network. The training loss/validation loss remains constant. What could be the possible reason?A) Architecture is not defined correctly
B) Data given to the model is noisy
C) Both of theseSolution: CBoth architecture and data could be incorrect. Refer this article https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/ 25) The red curve above denotes training accuracy with respect to each epoch in a deep learning algorithm. Both the green and blue curves denote validation accuracy.Which of these indicate overfitting?A) Green Curve
B) Blue CurveSolution: BBlue curve shows overfitting, whereas green curve is generalized. 26) Which of the following statement is true regrading dropout?1: Dropout gives a way to approximate by combining many different architectures
2: Dropout demands high learning rates
3: Dropout can help preventing overfittingA) Both 1 and 2
B) Both 1 and 3
C) Both 2 and 3
D) All 1, 2 and 3Solution: BStatements 1 and 3 are correct, statement 2 is not always true. Even after applying dropout and with low learning rate, a neural network can learn. 27) Gated Recurrent units can help prevent vanishing gradient problem in RNN.A) True
B) FalseSolution: AOption A is correct. This is because it has implicit memory to remember past behavior. 28) Suppose you are using early stopping mechanism with patience as 2, at which point will the neural network model stop training? A) 2
B) 3
C) 4
D) 5Solution: CAs we have set patience as 2, the network will automatically stop training after  epoch 4. 29) [True or False] Sentiment analysis using Deep Learning is a many-to one prediction taskA) TRUE
B) FALSESolution: AOption A is correct. This is because from a sequence of words, you have to predict whether the sentiment was positive or negative. 30) What steps can we take to prevent overfitting in a Neural Network?A) Data Augmentation
B) Weight Sharing
C) Early Stopping
D) Dropout
E) All of the aboveSolution: EAll of the above mentioned methods can help in preventing overfitting problem. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test. For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/08/skilltest-deep-learning/
PyData Delhi 2017,Learn everything about Analytics|About PyData Delhi 2017,"Buy Tickets|Venue|Registration Fee|Share this:|Like this:|Related Articles|30 Questions to test a Data Scientist on Deep Learning (Solution – Skill test, July 2017)|Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience)|
Deepika
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found," PyData conferences bring together users and developers of data analysis tools to share ideas and learn from each other. The PyData community gathers to discuss how best to apply Python tools, as well as tools using R and Julia, to meet evolving challenges in data management, processing, analytics, and visualization.We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.For more information about the conference series, visit http://pydata.org/Website : http://bit.do/pydd17
Submit CFP : http://bit.do/pydd17-cfpPurchase here : http://bit.do/pydd17-ticketsIIIT Delhi, Okhla
Location : https://goo.gl/maps/7hpVhds7JRN2 INR 1000",https://www.analyticsvidhya.com/blog/2017/08/pydata-delhi-2017/
Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|PyData Delhi 2017|Data Scientist/Analyst- Chennai (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Experience in credit banking industry required – Hands on experience on SAS and Excel/VBA – 2-5 years of total experience of Data Analytics / Data Insights Generation – Proven experience in delivering Analytics Solutions – Good communication/problem solving/analytical bent of mind
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : 
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/08/senior-business-analyst-credit-risk-analytics-delhincrbangaloregurgaon-2-5-years-of-experience/
Data Scientist/Analyst- Chennai (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience)|Introduction to Genetic Algorithm & their application in data science|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Ability to code in SAS, R and Python is a must – Should have handled both structured and unstructured data – Exposure to CPG industry preferred – Ability to learn new techniques and technologies – Strong Academic/professional background is a must
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : 
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/08/data-scientistanalyst-chennai-2-5-years-of-experience/
Introduction to Genetic Algorithm & their application in data science,Learn everything about Analytics|Introduction|Table of Content|1. Intuition behind Genetic Algorithms|2. Biological Inspiration|3. What is a Genetic Algorithm?|4. Steps Involved in Genetic Algorithm|5. Application of Genetic Algorithm|6. Applications in Real World|7. End Notes,"4.1 Initialisation|4.2 Fitness Function |4.3 Selection |4.4 Crossover |4.5 Mutation |5.1 Feature Selection|5.2 Implementation using TPOT library||6.1 Engineering Design|6.2 Traffic and Shipment Routing (Travelling Salesman Problem)|6.3 Robotics|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Data Scientist/Analyst- Chennai (2-5 Years Of Experience)|Tableau for Beginners – Data Visualisation made easy|
Shubham Jain
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found," Few days back, I started working on a practice problem – Big Mart Sales. After applying some simple models and doing some feature engineering, I landed up on 219th position on the leader board.Not bad – but I needed something better.So, I started searching for optimization techniques which could improve my score. It was during this search that I was introduced to genetic algorithms. After applying Genetric algorithm to the practice problem, I ended up taking a considerable leap on the leaderboard.Yes, a jump from 219th to 15th position just on the basis on genetic algorithm. Isn’t that great? By end of this article, you will be comfortable applying genetic algorithms and can expect similar benefit on the problems you are working on.  Let’s start with the famous quote by Charles Darwin:It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line. Let us understand with a basic example:  Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this. Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them. I am sure you would remember:Cells are the basic building block of all living things.  Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.  Source : linkA chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color. I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm? Let’s get back to the example we discussed above and summarize what we did.This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent. So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results. The working of a genetic algorithm is also derived from biology, which is as shown in the image below.Source: linkSo, let us try to understand the steps one by one. Here, to make things easier, let us understand it by the famous Knapsack problem.If you haven’t come across this problem, let me introduce my version of this problem. Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of 30 kg. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points. Here is the table giving details about each item.  To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes. We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped. This set of chromosome is considered as our initial population. Let us calculate fitness points for our first two chromosomes.For A1 chromosome [100110], Similarly for A2 chromosome [001110],So, for this problem, our chromosome will be considered as more fit when it contains more survival points. Therefore chromosome 1 is more fit than chromosome 2. Now, we can select fit chromosomes from our population which can mate and create their off-springs.General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity. Therefore, we generally use Roulette Wheel Selection method. Don’t be afraid of name, just take a look at the image below.I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel. Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.  Based on these values, let us create our roulette wheel.So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated. Sometimes we mark two fixed point as shown in the figure below. So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method. So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below. This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs. If you take two crossover point, then it will called as multi point crossover which is as shown below. Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population. A simple method of mutation is shown in the image below. So the entire process is summarise as shown in the figure. Source : link The off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population. But the question is how we will get to know that we have reached our best possible solution? So basically there are different termination conditions, which are listed below:  Now, I suppose you have grasp the basic understanding of the genetic algorithm. So now let us look at some of the application of genetic algorithm in data science. Every time you participate in a data science competition, how do you select features that are important in prediction of the target variable? You always look at the feature importance of some model, and then manually decide the threshold, and select the features which have importance above that threshold.Is there any better way to deal with this kind of situations? Actually one of the most advanced algorithms for feature selection is genetic algorithm.The method here is completely same as the one we did with the knapsack problem.We will again start with the population of chromosome, where each chromosome will be binary string. 1 will denote “inclusion” of feature in model and 0 will denote “exclusion” of feature in the model.And another difference would be that the fitness function would be changed. The fitness function here will be our accuracy metric of the competition. The more accurate our set of chromosome in predicting value, the more fit it will be.I suppose, you would now be thinking is there any use of such tough tasks. I will not answer this question now, rather let us look at the implementation of it using TPOT library and then you decide this. So finally, here the comes the part for which you have been waiting from the beginning of this article.  First, let’s take a quick view on the TPOT (Tree-based Pipeline Optimisation Technique) which is build upon scikit-learn library.  A basic pipeline structure is shown in the image below.So the highlighted grey section in the image above is automated using TPOT. This automation is achieved using genetic algorithm.So, without going deep into this, let’s directly try to implement it.For using TPOT library, you first have to install some existing python libraries on which TPOT is build. So let us quickly install them.For the implementation part, here I have used Big Mart Sales dataset. So quickly download the train and test file.Now let’s look at its python code.  Once this code finishes running, tpot_exported_pipeline.py will contain the Python code for the optimised pipeline. We can see that ExtraTreeRegressor worked best for this problem.If you submit this csv, you will notice that what I promised in the start has not been fulfilled. Was I lying to make you study all of these?No, actually there is a simple rule of TPOT library, if you don’t run TPOT for very long, then it may not find the best possible pipeline for your problem.So, increase the number of generations, grab a cup of coffee and go out for a walk. TPOT will finish your work.You can also do classification problems with this library. For more, I would suggest you to once check out its documentation.Besides competitions, genetic algorithm also have many applications in the real world. Genetic algorithm has many applications in real world. Here I have listed some of the interesting application, but explaining each one of them will require me an extra article.Engineering design has relied heavily on computer modeling and simulation to make design cycle process fast and economical. Genetic algorithm has been used to optimize and provide a robust solution.Resources: link This is a famous problem and has been efficiently adopted by many sales-based companies as it is time saving and economical. This is also achieved using genetic algorithm. Source: link The use of genetic algorithm in the field of robotics is quite big. Actually, genetic algorithm is being used to create learning robots which will behave as a human and will do tasks like cooking our meal, do our laundry etc.Resources: linkNow after these I suppose, you must have developed enough curiosity to look out for some more other interesting applications of genetic algorithms. Also you can comment down if you want to share that with us. I hope that now you have gain enough understanding about what genetic algorithm is and also how to implement it using TPOT library. But this knowledge is not enough, if you don’t apply it somewhere.So try to implement it whether in any real world application or in a data science competition. If you face any difficulties, feel free to write on our discussion portal.Did you find this article helpful? Please share your opinions / thoughts in the comments section below. ",https://www.analyticsvidhya.com/blog/2017/07/introduction-to-genetic-algorithm/
Tableau for Beginners – Data Visualisation made easy,Learn everything about Analytics| Table of Contents :|1. Overview of Tableau|2. Getting Started|3. Other Functionalities|4. Dashboard|5. Story – Bringing it all together|6. End Notes,"|Sample Dashboard in Tableau|1.1 What is Tableau?|1.2 What do you need to know before using Tableau?|1.3 Installation :|2.1 Connect to the Data|2.2 Data Visualisations|2.3 Various Graphs and Charts|3.1 Filters|3.2 Drill Down and Drill Up|3.3 Trend Line|3.4 Forecasting|3.5 Clusters||Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Introduction to Genetic Algorithm & their application in data science|Web Scraping in Python using Scrapy (with multiple examples)|
Pavleen Kaur
|48 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The idea is to go from numbers to information to understanding – Hans RoslingHave a look at the visualisation below, which was created by a famous Swedish statistician, Hans Rosling. He compiled roughly 200 years of World Development Data and presented it in a very simple manner: This above is an excellent example of Data Visualisation, which rather than focussing on what the numbers are, focuses on telling their story. You can find the interactive version of this visual here.There are multiple Software that are available now at instant access which assist in such easy visualisations and one tool that we are going to cover in this article is Tableau.What can you make out from the picture below?This Dashboard, made on Tableau, represents the Sales and Profit Analysis of a Supermarket. At a glance, you can see:So, in this article, we will learn how to make such simple visualisations in Tableau to understand our data well.  Tableau is a Data Visualisation tool that is widely used for Business Intelligence but is not limited to it. It helps create interactive graphs and charts in the form of dashboards and worksheets to gain business insights. And all of this is made possible with gestures as simple as drag and drop!What Products does Tableau offer? You don’t need to know much to use Tableau, but still a basic awareness of all the types of graphs such as bar graph, line charts, histograms etc is preferred.Along with that it will be beneficial if you possessed some basic understanding of database management ( datatypes, joins, drill down, drill up etc ) too. Even if you don’t, not a reason to worry since I will be covering all such concepts in this and forthcoming articles. To work on Tableau, you need Tableau right?Out of the five above mentioned products, Tableau Desktop, Public and Online offer Data Visual Creation.Tableau Desktop  It is available in the following three formats :Tableau PublicTableau Public is purely free of all costs and does not require any licence. But it comes with a limitation that all of your data and workbooks are made public to all Tableau users. Tableau OnlineTableau Online is the best option for you, if you wish to make your Workbooks on the Cloud and be able to access them from anywhere. Now that you have the suitable product installed and set up, I am pretty sure that your hands must be tingling with anticipation to finally begin! Well let’s not keep you waiting then, go ahead and launch the tool. You should see a screen similar to the one above. This is where you import your data. As is visible, there are multiple formats that your data can be in. It can be in a flat file such as Excel, CSV or you can directly load it from data servers too.You can see that Tableau itself offers some Sample Workbooks, with pre-drawn charts and graphs. I would suggest going through these later for further exploration.The best way to learn is to get your hands dirty. Let us start with our Data, which can be found here. The data is that of a United States’ Superstore which is deliberating over its expansion. It wishes to know the prospective regions of the country where it could and hence requires your help.The first thing that you will obviously need to do is import the data onto Tableau. So quickly follow the below steps:Uh oh, the imported data looks a bit different for the first few rows. Don’t worry, the solution lies right ahead. Data Interpreter       3. You see the option of Use Data Interpreter?  Click on it to get the following clean view :All that messy data magically disappeared! If you open the Excel data file, you will see some metadata in it, i.e. information about data :Tableau imports the entire data file as is, but anticipating such discrepancies, explicitly provides a solution in the form of a Data Interpreter. If you wish to view the exact changes that it made, click on Review the results, and choose the Orders tab in the opened Excel sheet.As it will show, it simply removed the erroneous data. As soon as you had imported your dataset, next to the Data Source tab near the bottom of the screen, you immediately must have seen Go to Worksheet. A Worksheet is where you make all of your graphs, so click on that tab to reach the following screen :Don’t get overwhelmed by the various elements that you see here, we will cover them all one by one.Let’s start with Dimensions and Measures :Moving onto Shelves :Visualisation in Tableau is possible through dragging and dropping Measures and Dimensions onto these different Shelves.Rows and Columns : Represent the x and y – axis of your graphs / charts.
Filter : Filters help you view a strained version of your data. For example, instead of seeing the combined Sales of all the Categories, you can look at a specific one, such as just Furniture.
Pages : Pages work on the same principle as Filters, with the difference that you can actually see the changes as you shift between the Paged values. Remember that Rosling chart? You can easily make one of your own using Pages.
Marks : The Marks property is used to control the mark types of your data. You may choose to represent your data using different shapes, sizes or text.And finally there is Show Me, the brain of Tableau!When you drag and drop fields onto the visualisation area, Tableau makes default graphs for you, as we shall see soon, but you can change these by referring to the Show Me option. Note : Not every graph can be made with any combination of Dimensions or Measures. Each graph has its own conditions for the number and types of fields that can be used, which we shall discuss next.  So far we have pretty much covered the requisite theoretical knowledge. Lets finally begin with some visualisations now.I personally prefer to start from the shallow side of the pool, slowly swimming towards the deeper end. So I would suggest beginning by getting an overview of the Superstore Sales and Profit Statistics. That would include the Net Sales, the Net Profit and the growth of the two measures, to name a few. Here is a gist of what we will be making :From what can be observed, the net Sales are on the rise, but the Profit is creeping up slowly. We can also quite clearly see the peak Sales Months, which could be attributed to various reasons. We can only know more as we explore more.Before we start, there is one thing that I would like to recommend and that is you name your Worksheets as being done here. Since I will be referencing them back and forth throughout the article, it will be easier for you to follow.Let’s begin with the simplest visualisation, and that is displaying the Net Statistics numbers. Tableau, being as smart as it is, automatically computes such values under Measure Names and Measure Values. Follow these steps to make what is called a Text Table :  Note : Don’t get confused by the different colours of the fields that you see. Just remember one small trick : Blue means Discrete and Green, Continuous.So we have the net Sales and Profit values, let’s delve a little deeper by getting the Sales and Profit Values over the years. Lets make another, but a more detailed, Text Table :We have just covered the numeric part of the Dashboard, but that is not its selling point. It’s the Line Charts. Lets quickly learn how to make one :If you were to click on Show Me, you will see the different types of Line Charts that you can make, and if you were to hover over each of them, you will get to see their Dimension and Measure requirements too. In case you ever feel lost, I recommend referring to Show Me.With the previous visualisations, we had gotten a brief overview of the Superstore. Let’s dig a little deeper now. The next thing that I can think of exploring is the demographic of the Sales and Profit. What are the States that have the highest Sales Revenue, which ones are generating the maximum Profits:Before discussing the inferences, let’s first create the Pie Chart of Region Sales :From the visual it’s pretty evident that the two opposite ends, East and West are leading in the Sales game. Let’s dissect this a bit more.Note : Whenever you have some geographical data, it is always advisable to plot and see it on a Map to gain better insights.So, we are now going to make the Map Chart of State Sales Distribution :California and New York are the top most sellers from West and East region, but unfortunately there are other States such as Texas, Colorado which even after having good Sales, have negative Profits! This is certainly not good news for the Superstore. You can perceive a good analysis for the other States as well.And lastly, here are the steps for making the Scatter Plot of Sales and Profit Analysis :The findings from the Map chart become more prominent with the following Scatter plot inferences :One of the great things about Tableau is that it lets you interact with the visuals. Have a look at an example :When we clicked on the Central Region, it highlighted and showed the Central States of US, along with their respective Sales and Profit scatter. Here we used the chart as a Filter itself which is a feature of a Dashboard. We shall learn how to make one at a later stage.There is one pretty important analysis that we have yet to touch, and that is Product Statistics. High Sales could be easily attributed to the high cost of the products being sold. Also, when you are considering expansion, you will want to know the Sales distribution of the Products too:Here we have visualised not just the Sales but also the Profits.Its quite surprising to see Categories that have high Sales, generating negative profits, like Technology in November 2015, or Furniture in October 2016 and this is inferred from the first chart, which is also called a Highlight Table. As the name suggests, it highlights the relative proportion of the Measure Values of our data. So let’s learn how to make one :The Product Sub Category Sales is a Bar Chart, which is also quite easy to make :From the the above graph, we are getting a good idea of the Net Sales and Profit margins of the various products. Notice that even though Tables’ Sales are quite high on the scale, it’s the only product with the least profit.Now, just like before, consider an interaction with the visualisation :We are now able to view each Category’s Products’ Sales and Profits, at a low level granularity of Year and Month! Congratulations! You have now covered one of the important aspects of Tableau! But it’s not the end of your learning just yet. Tableau offers some advanced functionalities too, some of which we will cover next : Till now we have only made simple charts, that actually provide cumulative data, that is combined data over the lifetime of the Superstore. To look at Sales of a particular Year, a Month, for a certain Product, or to basically view the distinct aspects of the data, Filters are the way to go.Let’s head back to the first ever Chart that we had made, of Peak Sales and Profit Months :The visual here is an accumulation of all 4 years of data, for all Regions, States, Categories and Sub Categories.The steps of turning any Dimension into a Filter are the same. Let’s first experiment with the Order Date ( formatted to Year ) : By now you must have gotten some picture of the way our Data is built. We have Category as the main Field, divided into Sub – Category, which is further distinguished into the various Product IDs and their corresponding Product Names.This concept of breaking down our data to reach the absolute depth is called Drilling Down :Similarly you can drill down from Order Date to Order ID to Ship Date to Ship Mode. This is also referred to as making an Hierarchy of data.Let’s consider the ProductDrillDown first, which is really a Bar Graph :       4. To finally plot your data, drag the Product Hierarchy onto Rows and Sales onto Columns, and get:This was just a simple Bar Graph, but if you hover over the Category axis, you will see a small plus sign. Click on it to get a granulated version of your data. Do the same for the other generated axis as well to get to the absolute depth. The Tree Analysis of Product Sales is a Tree Map, which is a great way of representing Drilled Down data, and is quite easy to make :5. Following the drill down from Step 4, simply go to Show Me and select the Tree Map chart, to get the following :So far you have analysed the present scenario, but for expansion consideration, let’s try and analyse the future too.With the following Dashboard, you can not only see the Trends over the Sales Months, but also a Forecast over the Years too. And both of them tell a different story altogether :Although the Sales of the Superstore are increasing over the the months of a Year, the future in general looks a bit bleak. The sales seem to become constant for the next 3 years, but fortunately for the Superstore, the Profit is increasing steadily. Let’s get to making the above now. Traverse back to the Peak Sales and Profit Month Chart and follow these steps to make a Trend Line of your own :       2. To get the Trend Line, go to Analytics, and simply drag Trend Line over the chart, to get : For forecasting, we are going to deal with the Sales and Profit Growth chart. The construction is similar to that of Trend Lines, but with a small change. The steps are : Let’s head back to the Sales and Profit Analysis chart that we had made. Remember the detailed inference that we had generated from it? We are just going to make that a bit more prominent now, using Clusters. To make them : I am sure by now you must have gotten a pretty good idea of what a Dashboard is, having seen it plenty of times all throughout this article.If not, well then a Dashboard is simply a means of combining Worksheets together so that they convey some message. Without much further ado, let’s get right to it!  Consider the State Sales Distribution Map chart and Product Sub Categories.What if you wanted to know the various Sales margin of each Product within separate States? We had observed that Texas was one of the States with the lowest Profits. By looking at the following Dashboard, you will see that the reason is it’s not managing to generate Profits in majority of the Products :Now consider the state wise Sales distribution of a Sub – Category :The above beautifully shows the distribution of Appliances over the country, where California seems to be the major Profit contributor. Making such a Dashboard is actually quite easy. Let’s see how :Note : Even after the creation of the Dashboards, you can still edit your Worksheets, and the same changes shall be reflected here.If you were to click on the States or the Products after creating your first ever Dashboard, you won’t observe any change. Because for such visuals, we first have to convert the Charts themselves into filters.4. Simply click on the small Down Arrow on each chart you wish to turn into a Filter, and select Use as Filter:Note : While making Dashboards, it is preferred to use your charts as Filters, rather than cluttering up the view with custom ones. Just like Dashboards were a way to combine the Worksheets, a Story is where you combine all the dashboards, and if need be individual Sheets as well, to convey, as the name suggests – a Story. So let’s combine all those Dashboards that we had made into what could perhaps make a decent presentation for a beginner. Do ensure to Add a Caption to all of your Dashboards, to convey your message clearly :If you have ever come across Tableau Stories online, the ones which you could actually interact with, instead of just viewing, that is made possible by publishing your Workbooks onto the Tableau Server.If you have one set up, then all you need to do, after creating your Stories, is go to Server -> Publish Workbook and enter the Server Name : What we have covered so far is pretty much the basics of Tableau. It has various other features which I will be covering in my forthcoming articles. As it is said ‘With practice, comes perfection’, it is suggested that you experiment as much as you can with Tableau. Below is a sample Dashboard that I would encourage everyone of you to try and make. You will not only get to test the skills that you have learned so far, but also hopefully acquire more. The dataset used is the same as the one we had been working with so far :If there are ever any doubts, do leave them as comments 🙂All the best on your journey as a Data Explorer, and stay tuned for my next article on Tableau! ",https://www.analyticsvidhya.com/blog/2017/07/data-visualisation-made-easy/
Web Scraping in Python using Scrapy (with multiple examples),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Overview of Scrapy|2. Write your first Web Scraping code with Scrapy|3. Case studies using Scrapy|End Notes,"2.1 Set up your system|2.2 Scraping Reddit: Fast Experimenting with Scrapy Shell|2.3 Writing Custom Spiders|Scraping an E-Commerce site|Scraping Techcrunch: Creating your own RSS Feed Reader|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Tableau for Beginners – Data Visualisation made easy|Data Scientist – Ahmedabad (4-9 years Of Experience)|
Mohd Sanad Zaki Rizvi
|93 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Scrapy Shell|About Reddit |Extracting title of posts|Extracting Vote counts for each post|Dealing with relative time stamps: extracting time of post creation| Extracting Number of comments:|Creating a scrapy project|Creating a spider|Exporting scraped data as a csv |Extracting image URLs of the product|Extracting product name from <img> tags|How to download product images?|Overview of XPath and XML|Extracting title of post|Extracting author name: Dealing with namespaces in XML,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The explosion of the internet has been a boon for data science enthusiasts. The variety and quantity of data that is available today through the internet is like a treasure trove of secrets and mysteries waiting to be solved. For example, you are planning to travel – how about scraping a few travel recommendation sites, pull out comments about various do to things and see which property is getting a lot of positive responses from the users! The list of use cases is endless.Yet, there is no fixed methodology to extract such data and much of it is unstructured and full of noise.Such conditions make web scraping a necessary technique for a data scientist’s toolkit. As it is rightfully said,Any content that can be viewed on a webpage can be scraped. Period.With the same spirit, you will be building different kinds of web scraping systems using Python in this article and will learn some of the challenges and ways to tackle them.By end of this article, you would know a framework to scrape the web and would have scrapped multiple websites – let’s go!  Scrapy is a Python framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format.As diverse the internet is, there is no “one size fits all” approach in extracting data from websites. Many a time ad hoc approaches are taken and if you start writing code for every little task you perform, you will eventually end up creating your own scraping framework. Scrapy is that framework.With Scrapy you don’t need to reinvent the wheel.Note: There are no specific prerequisites of this article, a basic knowledge of HTML and CSS is preferred. If you still think you need a refresher, do a quick read of this article. We will first quickly take a look at how to setup your system for web scraping and then see how we can build a simple web scraping system for extracting data from Reddit website. Scrapy supports both versions of Python 2 and 3. If you’re using Anaconda, you can install the package from the conda-forge channel, which has up-to-date packages for Linux, Windows and OS X.To install Scrapy using conda, run:Alternatively, if you’re on Linux or Mac OSX, you can directly install scrapy by:Note: This article will follow Python 2 with Scrapy. Recently there was a season launch of a prominent TV series (GoTS7) and the social media was on fire, people all around were posting memes, theories, their reactions etc. I had just learnt scrapy and was wondering if it can be used to catch a glimpse of people’s reactions? I love the python shell, it helps me “try out” things before I can implement them in detail. Similarly, scrapy provides a shell of its own that you can use to experiment. To start the scrapy shell in your command line type:Woah! Scrapy wrote a bunch of stuff. For now, you don’t need to worry about it. In order to get information from Reddit (about GoT) you will have to first run a crawler on it. A crawler is a program that browses web sites and downloads content. Sometimes crawlers are also referred as spiders. Reddit is a discussion forum website. It allows users to create “subreddits”  for a single topic of discussion. It supports all the features that conventional discussion portals have like creating a post, voting, replying to post, including images and links etc. Reddit also ranks the post based on their votes using a ranking algorithm of its own.A crawler needs a starting point to start crawling(downloading) content from. Let’s see, on googling “game of thrones Reddit” I found that Reddit has a sub-reddit exclusively for game of thrones at https://www.reddit.com/r/gameofthrones/ this will be the crawler’s start URL.To run the crawler in the shell type:When you crawl something with scrapy it returns a “response” object that contains the downloaded information. Let’s see what the crawler has downloaded:This command will open the downloaded page in your default browser.Wow that looks exactly like the website, the crawler has successfully downloaded the entire web page.Let’s see how does the raw content looks like:That’s a lot of content but not all of it is relevant. Let’s create list of things that need to be extracted : Scrapy provides ways to extract information from HTML based on css selectors like class, id etc. Let’s find the css selector for title, right click on any post’s title and select “Inspect” or “Inspect Element”:This will open the the developer tools in your browser:As it can be seen,  the css class “title” is applied to all <p> tags that have titles. This will helpful in filtering out titles from rest of the content in the response object:Here response.css(..) is a function that helps extract content based on css selector passed to it. The ‘.’ is used with the title because it’s a css . Also you need to use ::text to tell your scraper to extract only text content of the matching elements. This is done because scrapy directly returns the matching element along with the HTML code. Look at the following two examples:Notice how “::text” helped us filter and extract only the text content. Now this one is tricky, on inspecting, you get three scores:The “score” class is applied to all the three so it can’t be used as a unique selector is required. On further inspection, it can be seen that the selector that uniquely matches the vote count that we need is the one that contains both “score” and “unvoted”.When more than two selectors are required to identify an element, we use them both. Also since both are CSS classes we have to use “.” with their names. Let’s try it out first by extracting the first element that matches:See that the number of votes of the first post is correctly displayed. Note that on Reddit, the votes score is dynamic based on the number of upvotes and downvotes, so it’ll be changing in real time. We will add “::text” to our selector so that we only get the vote value and not the complete vote element. To fetch all the votes:Note: Scrapy has two functions to extract the content extract() and extract_first(). On inspecting the post it is clear that the “time” element contains the time of the post.There is a catch here though, this is only the relative time(16 hours ago etc.) of the post. This doesn’t give any information about the date or time zone the time is in. In case we want to do some analytics, we won’t be able to know by which date do we have to calculate “16 hours ago”. Let’s inspect the time element a little more:The “title” attribute of time has both the date and the time in UTC. Let’s extract this instead:The .attr(attributename) is used to get the value of the specified attribute of the matching element. I leave this as a practice assignment for you. If you have any issues, you can post them here: https://discuss.analyticsvidhya.com/ and the community will help you out 🙂 .So far: Note: CSS selectors are a very important concept as far as web scraping is considered, you can read more about it here and how to use CSS selectors with scrapy. As mentioned above, a spider is a program that downloads content from web sites or a given URL. When extracting data on a larger scale, you would need to write custom spiders for different websites since there is no “one size fits all” approach in web scraping owing to diversity in website designs. You also would need to write code to convert the extracted data to a structured format and store it in a reusable format like CSV, JSON, excel etc. That’s a lot of code to write, luckily scrapy comes with most of these functionality built in. Let’s exit the scrapy shell first and create a new scrapy project:This will create a folder “ourfirstscraper” with the following structure:For now, the two most important files are: Let’s change directory into our first scraper and create a basic spider “redditbot” :This will create a new spider “redditbot.py” in your spiders/ folder with a basic template:Few things to note here:After every successful crawl the parse(..) method is called and so that’s where you write your extraction logic. Let’s add the earlier logic wrote earlier to extract titles, time, votes etc. in the parse function: Note: Here yield scraped_info does all the magic. This line returns the scraped info(the dictionary of votes, titles, etc.) to scrapy which in turn processes it and stores it.Save the file redditbot.py and head back to shell. Run the spider with the following command:Scrapy would print a lot of stuff on the command line. Let’s focus on the data.Notice that all the data is downloaded and extracted in a dictionary like object that meticulously has the votes, title, created_at and comments. Getting all the data on the command line is nice but as a data scientist, it is preferable to have data in certain formats like CSV, Excel, JSON etc. that can be imported into programs. Scrapy provides this nifty little functionality where you can export the downloaded content in various formats. Many of the popular formats are already supported.Open the settings.py file and add the following code to it:And run the spider :This will now export all scraped data in a file reddit.csv. Let’s see how the CSV looks:What happened here:There are a plethora of forms that scrapy support for exporting feed if you want to dig deeper you can check here and using css selectors in scrapy.Now that you have successfully created a system that crawls web content from a link, scrapes(extracts) selective data from it and saves it in an appropriate structured format let’s take the game a notch higher and learn more about web scraping. Let’s now look at a few case studies to get more experience of scrapy as a tool and its various functionalities. The advent of internet and smartphones has been an impetus to the e-commerce industry. With millions of customers and billions of dollars at stake, the market has started seeing the multitude of players. Which in turn has led to rise of e-commerce aggregator platforms which collect and show you the information regarding your products from across multiple portals? For example when planning to buy a smartphone and you would want to see the prices at different platforms at a single place. What does it take to build such an aggregator platform? Here’s my small take on building an e-commerce site scraper.As a test site, you will scrape ShopClues for 4G-SmartphonesLet’s first generate a basic spider:This is how the shop clues web page looks like:The following information needs to be extracted from the page: On careful inspection, it can be seen that the attribute “data-img” of the <img> tag can be used to extract image URLs: Notice that the “title” attribute of the <img> tag contains the product’s full name:Similarly, selectors for price(“.p_price”) and discount(“.prd_discount”). Scrapy provides reusable images pipelines for downloading files attached to a particular item (for example, when you scrape products and also want to download their images locally).The Images Pipeline has a few extra functions for processing images. It can:In order to use the images pipeline  to download images, it needs to be enabled in the settings.py file. Add the following lines to the file :you are basically telling scrapy to use the ‘Images Pipeline’ and the location for the images should be in the folder ‘tmp/images/. The final spider would now be:A few things to note here:On running the spider the output can be read from “tmp/shopclues.csv”:You also get the images downloaded. Check the folder “tmp/images/full” and you will see the images:Also, notice that scrapy automatically adds the download path of the image on your system in the csv:There you have your own little e-commerce aggregator 🙂If you want to dig in you can read more about scrapy’s Images Pipeline here Techcrunch is one of my favourite blogs that I follow to stay abreast with news about startups and latest technology products. Just like many blogs nowadays TechCrunch gives its own RSS feed here : https://techcrunch.com/feed/ . One of scrapy’s features is its ability to handle XML data with ease and in this part, you are going to extract data from Techcrunch’s RSS feed.Create a basic spider:Let’s have a look at the XML, the marked portion is data of interest:Here are some observations from the page: XPath is a syntax that is used to define XML documents. It can be used to traverse through an XML document. Note that XPath’s follows a hierarchy. Let’s extract the title of the first post. Similar to response.css(..) , the function response.xpath(..) in scrapy to deal with XPath. The following code should do it: Output :Wow! That’s a lot of content, but only the text content of the title is of interest. Let’s filter it out:Output :This is much better. Notice that text() here is equivalent of ::text from CSS selectors. Also look at the XPath //item/title/text() here you are basically saying find the element “item” and extract the “text” content of its sub element “title”.Similarly, the xpaths for link, pubDate as : Notice the <creator> tags:The tag itself has some text “dc:” because of which it can’t be extracted using XPath and the author name itself is crowded with “![CDATA..” irrelevant text. These are just XML namespaces and you don’t want to have anything to do with them so we’ll ask scrapy to remove the namespace:Now when you try extracting the author name , it will work :Output : u’Ophir Tanz,Cambron Carter’The complete spider for TechCrunch would be:Let’s run the spider:And there you have your own RSS reader :)! In this article, we have just scratched the surface of Scrapy’s potential as a web scraping tool. Nevertheless, if you have experience with any other tools for scraping it would have been evident by now that in efficiency and practical application, Scrapy wins hands down. All the code used in this article is available on github. Also, check out some of the interesting projects built with Scrapy:",https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
Data Scientist – Ahmedabad (4-9 years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Web Scraping in Python using Scrapy (with multiple examples)|Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 9 years
Requirements : 
Task Info : Job Description and Responsibilities:– Deploy machine learning methodologies and algorithms to build data products that streamline the process from data collection to insight generation. – Assemble data sets from disparate sources and analyze using appropriate quantitative methodologies, computational frameworks and systems – Disseminate findings to non-technical audiences through a variety of media, including interactive visualizations, reports and presentations– Work closely with customers and delivery teams to find applications and business use cases for advanced analytical capabilities. – Generating business insight using data analytics and information visualization methods to answer business problems in different domains. – Designing and developing analytics solutions to move towards data-driven innovation in alignment with business priorities and key targets. Desired Skills and Experience : – 5+ years working with data and relevant computational frameworks and systems – Have Expert knowledge with Big Data tools like Hadoop and with other Statistical tools like SAS, Matlab, R, SQL, and VBA – Hands-on experience with Machine Learning algorithms, natural language processing, data analytics, and information visualization – Be Fluent with at least one programming language like Java or Python – Have Strong commitment to quality and client service – Have excellent interpersonal skills– Have a PhD or a Masters Degree in a Quantitative discipline from a Tier 1 Institute like the IITs, ISI, IIMs, DSE etc.,
College Preference : tier1-any
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-scientist-ahmedabad-4-9-years-of-experience/
Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist – Ahmedabad (4-9 years Of Experience)|Senior Business Analyst – Bangalore (4-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 5 – 8 years
Requirements : 
Task Info : Job Description and Responsibilities:We are looking for a Data Analytics Manager to organize our analytics function and manage our team of data scientists. You will implement tools and strategies to translate raw data into valuable business insights. Key Responsibility:– Oversee all analytics operations to correct discrepancies and ensure quality – May collect and analyze external market data to provide benchmarks for comparison purposes. – Presents reports to management for use in decision making and strategic planning– Manages team of data scientists that use business data and statistical methods to provide insight into business performance and suggest area for and methods of improving operations – Implements analytical approaches and methodologies and assists in the interpretation of results.Skill Sets / Requirements: – Proven experience as a Data Analytics Expert – Very good knowledge of predictive modeling and statistical techniques – Knowledge of Excel, R and SQL; familiarity with business intelligence tools – Problem solving attitude – Ability to learn and adapt quickly and to correctly apply new tools and technology. – Strong leadership and mentoring skillsQualification: – B.tech/BE (Technology or Statistics related) – Master’s in Business or Analytics related (preferred)
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-analytics-manager-delhincr-5-8-years-of-experience/
Senior Business Analyst – Bangalore (4-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience)|Manager (Data Analyst)- Pune (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Work with engineering and research teams on designing, building and deploying data analysis systems for large data sets – Design, develop and implement R&D and pre-product prototype solutions and implementations using off the shelf tools (e.g. R, SAS,SPSS), and software (e.g. Python, Java, C/C++, .NET) – Create algorithms to extract information from large data sets. – Establish scalable, efficient, automated processes for model development, model validation, model implementation and large scale data analysis. – Develop metrics and prototypes that can be used to drive business decisions. Should Have:– Strong background in statistical concepts and calculations with 3.5+ yrs experience with real data – Innovative and strong analytical and algorithmic problem solvers. – Proficiency with statistical analysis tools (e.g. R, SAS,SPSS), software development technologies (e.g. Python, Java, C/C++, .Net) – Extensive experience solving analytical problems using quantitative approaches (e.g. Bayesian Analysis, Reduced Dimensional Data Representations, and Multi-scale Feature Identification). – Expert at data visualization and presentation. – Excellent critical thinking skills, combined with the ability to present your beliefs clearly and compellingly verbally and in written form.Qualification: Bachelor Degree in Engineering from top institute/college – IIT, NIT, BITS Pilani etc. Or MS / MSc in Statistics only 
College Preference : tier1-any
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/senior-business-analyst-bangalore-4-5-years-of-experience/
Manager (Data Analyst)- Pune (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst – Bangalore (4-5 Years Of Experience)|Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Strong Influencing skills to coordinate/ get things done cross functionally across the organization – Strong analytical skills & proficiency in MS Office to provide analytics & presentations enabling the Senior Management to take decisions. – Strong Project Management & Coordination Skills. – Strong problem solving, process & structured approach to work. – Ability to lead & effectively manage Direct & Indirect teams. – Emotionally balanced and mature to handle pressure & build relationships. – Willingness to stretch & work beyond role boundary. Educational Level & Work Experience :– Graduate Degree in Commerce/ Economics/Business Management/ Ideal would be a MBA Degree’ – Experience in Fraud/ Risk/ Compliance Management – High Proficiency in Data Analytics
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/manager-data-analyst-pune-2-5-years-of-experience/
Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager (Data Analyst)- Pune (2-5 Years Of Experience)|Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Programming and analytical skills in a major analytics software package (e.g. R, Python) – Superior analytical and quantitative skills, with experience working with large, complex data sets. – Strong troubleshooting, analytical and creative problem solving skills. Ability to collect, manipulate, cleanse and interpret data from multiple data sources, required. – Proven experience in analytically driven supply chain management techniques – Candidate should have excellent written and verbal communication – He/she should be highly motivated and self-directed, capable of multi-tasking and should be able to work with minimal supervision – Strong interpersonal skills, including ability to work in a cross functional environment and communicate with all levels of the organization. – Should have strong work ethic and positive team attitude – Candidate should be open for learning and should have the ability to adapt and learn different reporting tools, languages and applications – Experience with Supply Chain technologies and tools.– Bachelor’s degree with minimum work experience of 2 years with at least 2 years of experience in Analytics or Supply Chain in a corporate setting. – Candidate should have an advance knowledge of MS Excel,MS word, Power point 
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-analyst-delhincrgurugram-3-5-years-of-experience/
Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience)|Beginner’s guide to build data visualisations on the web with D3.js|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 8 years
Requirements : 
Task Info : Job Description and Responsibilities:– Create scalable, efficient, automated processes for large scale data analysis. – Perform comprehensive analysis to support ad-hoc reporting requests. – Develop and maintain predictive models for customer attrition and upsell. – Own demand generation analytics and reporting which includes proposing, driving creation of and managing metrics and analyses that monitor and improve marketing effectiveness. – Develop and execute standardized reporting and analysis to improve our understanding of customer behavior. Retrieve and analyze complex and large data sets to provide insights on customer life cycle and customer engagement. Analyze historical data to identify significant trends and drivers. – Develop A/B test scenarios and monitor results of customer engagement initiatives. Propose recommendations based on the analysis of those results. – Provide support for new functionalities and product launches within the Amazon Digital Music ecosystem. Assist in the definition and delivery of a marketing strategy for these new releases. – Have the obsession to drive a better customer experience through everything that we do at Amazon. Basic qualifications:– BA/BS degree in Mathematics, Statistics, Computer Science, or any other quantitative field; MBA preferred. – 5+ years professional experience in Analytics, Business Intelligence, or a Data Science role preferably in an e-commerce or retail company with large, complex data sources. – Advanced SQL writing skills and experience querying very large relational databases – Experience building and applying statistical models for forecasting, predicting outcomes, or understanding relationships in data – Must be passionate about data analytics and have a natural curiosity and ability to dive deep into large datasets uncovering insights and presenting relevant findings to business partners – Ability to quickly gain a deep understanding of our BI platforms, database schemas, ETL- s, and business logic with an ability to apply critical thinking skills towards gathering and analyzing data – Excellent communication skills, both verbal and written, are required as you will draft narratives outlining your findings, explaining variances against goals, and present these finding in a review forum – Strong ownership and bias for action; ability to internalize goals and work independently to crate appropriate action plans for those goals – Strong problem-solving and critical thinking skills; ability to analyze issues and create appropriate tactical plans – Meticulous attention to detail; ability to juggle many tasks in parallel without lowering quality bar – Ability to succeed in a fast-paced, innovative, and rapidly evolving industry and business organization
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/business-intelligence-analyst-mumbai-4-8-years-of-experience/
Beginner’s guide to build data visualisations on the web with D3.js,"Learn everything about Analytics|Introduction|The Web as a platform for visualisation|How D3.js fits in the picture?|Setup your machine|Technical Interlude – Brushing up basics of HTML, CSS, JavaScript and SVG|D3.js – Unpacking the name|Building with D3.js|End Notes","Storytelling through Data Visualisation|Create a simple web server in Python|Task 1. Create a basic HTML page with the text “Hello World” :|Task 2. Add formatting and styling to the text using CSS. Specifically, we want to increase size, bring the text to the center of the page, add colour and change its typeface:|Task 3. We want to add a behavior to our page. We want to show some text initially and change it when someone clicks on it. We also want to show an image on that click.|Task 4. Draw a group of two blue circles of radius 50px each. Fill the circle with blue colour and red boundary of some thickness using an SVG.|Including D3.js library|Adding an element|Adding CSS styling |Data binding|Where is our data?|Drawing SVG with D3|What is the “i” |Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience)|Debugging & Visualising training of Neural Network with TensorBoard|
Mohd Sanad Zaki Rizvi
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I am sure you have heard this many timesA picture is worth a thousand words.I think with the proliferation of data, this statement can easily be modified toA picture is worth thousand(s) of data points.If you are not convinced, look at the example below. Let’s look at the following statement:“In 2013, Gun Deaths Claimed 11,419 lives in the U.S.”What comes to your mind first?Without visualisation the above are just a set of statistics. Though this stat is useful, it still falls short in conveying the bigger picture. You don’t get the complete context – What kind of people were involved? How much was the nation’s loss? etc.Let’s see what happens when we visualise it:Periscope’s US Gun Deaths Visualisation (takes some time to load) – Periscope has made this visualisation that shows the number of people died due to the gun violence in the U.S. in 2013. The orange line denotes the age at which they died and the white line denotes the number of years those people could have lived had they died of natural causes. This visualisation shows what 11,419 lost lives looks like and just how many years of life was stolen from the victims. It is a good example of how we can express the data in a more sublime and impactful manner by using basic animations and interactivity.“When we present the data as a story, it gives a range of new perspectives and a much more holistic view of the situation which helps policy makers in making informed decisions.” The web is becoming more accessible day by day and with advancements in browser technology, it is now possible to render complex visualisations on the fly across a variety of devices. This combination of accessibility and complexity makes the web an apt platform to reach out to large audiences.A lot of organisations are already using web / mobile applications showing dashboards to your mobile as well at your laptop and computer. Enter D3.js, a powerful library that enables you to build customized visualisations for any kind of storytelling you could imagine for the web.Let’s look at another amazing visualisation :Hans Rosling’s 200 Countries, 200 Years, 4 Minutes – Hans tries to explain the change in health and income in the last 200 years across 200 countries of the world in just 4 minutes by utilising the power of effective visualisation.The properties of this graph are as follows:Notice how by using simple visual encodings like shape, size, colour, etc. such complex information(200 years of data) is conveyed across. We will talk about what is visual encoding and this visualisation in detail in the upcoming articles.What if I tell you  the above visualisation is created using D3.js? That’s the potential of D3.js! Hopefully, we will learn to create magic of our own too 🙂 We would be needing a few things before starting:Note: Since D3.js is designed to make visualisations on the web, it can fetch data over the internet from a web server to show visualisations. Hence we will also be working with a small server of our own. Step 1: Go to the directory you want to keep your D3.js project.Step 2: Python 2.7.x users can start the server by typing the code in the command line:Python 3.x users can do so by type the code in command line:You can check your python version by:Note: For windows users, please install anaconda and then open anaconda prompt.This should give an output similar to the following:Step 3: Here “0.0.0.0” is your server’s IP address and “8000” is the port number. Imagine you have ordered your favorite pizza and you need to give your house address to complete the order. You will first give the Street Name and then the House Number, here IP address is analogous to street name (i.e. It identifies your computer) and Port number is analogous to the house number that helps you reach a particular house (i.e. identifies the process on your computer) in a building.Step 4: To check if your server is working, head to your browser and type the link : http://0.0.0.0:8000/You will get an output like this:Congratulations, you are now on your way to build your first d3.js visualisation! Before diving into D3, we need to know some web development basics. The web is built on three main pillars: HTML, CSS and JavaScript. Also, for a clearer understanding of D3.js, you should know the basics of SVG. We will have a crash course on these topics with some task based questions.Note:  You can skip this section if you are familiar with HTML, CSS and JavaScript. In case you have some doubts or need help, each task is followed by a stepwise guide 🙂We have a list of four tasks to complete before we can get started with D3. Tasks 1 to 3 are based on HTML,CSS and JavaScript. Task 4 is based on SVG and related topics.Let’s start with our first task!For this task, you would need to :Try doing this task on your own before checking the solution 🙂 Solution:HTML or Hypertext Markup Language describes the structure of your web pages. HTML uses “tags”, a kind of syntax to form the skeletal structure of a web page. Specific text enclosed between angular brackets form a tag.Let’s create our first HTML !Congratulations! You just made your first web page. Let’s get into some details:For example : For this task you would need to :Try doing this task on your own before checking the solution 🙂 Solution:CSS or Cascading Style Sheets is used to add styling and formatting to web pages. In other words, it helps to set a variety of rules(like: position, color, formatting etc.) that make the HTML look pretty. Let’s make our own web page pretty!Note: The text between /**/ are comments and are ignored by the browser. They are for the programmer to keep notes of his / her code.Note: that we use the # symbol to call id in our CSS. This tells CSS to look for an element with the id =  “myText”.CSS Selectors –  Selectors are way our CSS is able to know which HTML elements does the rule applies to? We have already learned about the id selector.Now imagine, that if we have multiple paragraphs and we want to do the same styling to each? Surely we can repeat the ids for all elements but an id is supposed to be unique. We can use a class selector to group the elements that belong to the same class of styling.Note: We use “.” (dot) to tell our CSS we mean a class. For this task you would need to:Try doing this task on your own before checking the solution 🙂Solution:JavaScript is a programming language that adds immense functionality to a basic HTML web page. Almost all the “cool” things that you see in a website are because of the amazing power and flexibility JavaScript provides. It runs in the browser, unlike python and other programming languages you don’t need to install JavaScript, as long as you have a browser you can easily run it. Let’s add some special effects to impress our friend on his / her birthday!We will first show a generic message to our friend and when he /she clicks on the text, surprise! Happy Birthday! Let’s code it up :Isn’t it amazing what 5 lines of JavaScript can achieve? Let us dive into the code and understand what each line does:OrCongratulations! So far so good, you have successfully learnt the building blocks of web! We will be learning more about them as we go ahead with our D3 journey 🙂If you want to dig deeper into Web Technologies, you can look at W3Schools . For D3.js only basic understanding is required.In order to build with D3 we also need a basic knowledge of SVG. The following tasks are designed to give you a brief intro into the world of SVG: For this task you would need to :Try doing this task on your own before checking the solution 🙂 Solution:SVG or Scalable Vector Graphics is a format used to draw xml based graphics and animations. SVG graphics do NOT lose any quality if they are zoomed or resized.SVG provides some basic shapes like lines, rectangles, circle, ellipse, polygon  to work with. You can also create custom shapes by combining or tweaking these shapes. For extremely customisable graphics(like country maps, etc.) we use paths. A lot of these will refresh your high school geometry 🙂 Let’s go through each one: LineOne of the simplest graphic. A line is formed when we join two points separated by a distance. There can be many such lines joining two points but we are interested in a straight line. For example, you and your friend are standing some distance apart, if you walk straight to him the path you’ll follow will make a straight line. A Side Note – Coordinate space in SVG:Those of you who know a little about coordinate system in geometry would be finding something strange with the line. The line should be like this :This is because the origin(0,0) in SVG is at the “top-left” corner as opposed to conventional “bottom-left”.As we move towards the right the x value increases and on moving down y value increases.Enough of serious talk, let’s draw !CircleGroupsYou can read more about svg here, for using D3, only basic understanding of svg is required. D3.js is a javascript library written by Mike Bostock. It takes advantage of already established web technologies like canvas, svg to make out of the world visualisations.D3’s name comes from the fact that it is designed to act as a driver for web documents(web page) based on the available data (json, csv, tsv etc.). In other words,“D3’s magic helps you to create beautiful, interactive visualisations from seemingly boring data(json,csv,etc.)”We will get to know more about it as we go deeper into D3. You can also look at some of the awesome visualisations created using D3.js 🙂 Let’s get our hands dirty with D3!We will create a new html file , with name “barchart.html” in our D3 folder. Let us also put some basic html in our file : In order to use D3 we have to include it’s library, There are many ways to do so but we will stick to loading it from a url. D3 is completely written in javascript so we can include just like we include our javascript code with <script></script> tags like this:Note: This article uses version 3  of D3.js . You can find the documentation here.This line tells your browser to load the d3 file from the given URL. We will place this tag inside our <head> tag just below the <title> tags. We will also create another <script> tag inside our body for us to write javascript code. Our final HTML code should look like this : Our webpage looks empty, let’s add a paragraph here with D3. Write the below line between the <script> tags inside <body>:Save the page and head to your browser. This time you will have to use the url http://0.0.0.0:8000/barchart.html because your html file is named barchart.html. Let’s see what happened in our page:Wasn’t that easy? Let’s try to understand our D3 code:When we include D3’s library, it gives us a global object d3  that we can use to call various D3 functions. Here we are asking d3 to select <body> of our DOM and append a paragraph there with the text content “Our First Paragraph using D3!”.Let’s walk through what just happened. In sequence, we:All of those crazy dots are just part of D3’s chain syntax. The chain syntax is possible because every time you call a D3 function on an object it performs some operations on it and returns a reference to the new object, which in turn gets picked up by the next function in the chain.This is called method chaining. Note that the above task could have also achieved by calling each function separately like we do conventionally. Our text looks boring let us add some css styling to it and see what happens:Much better! We use .style(….) to add css styling in D3. Our chain has become too long to fit in a single line, let’s us arrange it :Notice how we first created a <p> element with some text and stored in a variable p and then later used the same variable to add css styling to it. We can now use this variable whenever we want to make changes or edits to the paragraph. What is data binding and why should I do it?Hence you need to bind your data to the DOM elements in order to represent it as a visualisation. D3.js provides powerful ways to bind data in different forms(csv,tsv, json etc.) to the DOM.Let’s create a data array :Note: In javascript an array is an equivalent of collections of R or lists of Python.Now that we have our data, let’s bind our data to the DOM. We want to create <p> elements per data value and display it. Our new code will be like:Let’s see what does our browser show:Whoa that was a lot of method chaining. We have got our text nine times on the page each as a paragraph. Let us break down the code:Since we don’t have any paragraphs it will return an empty selection. Think of this as the selection of paragraphs that will soon exist.This is how the entire process looked like:First select body →  from body select all the <p> elements that we are soon going to add → read the data values and count it, the following lines will be executed once for each value →  create placeholders for the elements going to be added and return references to them → append elements based on the references returned earlier → add text content to each element.We follow the similar process all throughout our code in D3. Hence it is important that you understand it thoroughly. Did you notice that even though we got nine paragraphs, but we don’t see any data? Where did it go?Remember the line where I said that every line following data(..) will be executed nine times, once for each value? D3 provides us a way to access each value while it iterates over them. Let’s check that value, edit the code to:This time instead of directly providing text content we create a function inside text(..). D3 executes this function every time it goes over our data. Each time this function is called, it is passed the value for which it is called. That means if we are on the first element of the array this function will be called with 5. We return the same value to our text(..) function so that it can add it to the text content of the paragraph. Here’s our data:Just like html elements, creating svg with D3 is quite easy(one line easy!). For example we want to create an svg with width 500px and height 500px :We append an <svg> like did for <p> element. The attr(..) function is used to set attributes of the svg. Here we want to set width and height of the svg.Let us draw a circle in this svg. We want to create a circle of radius 30 and centre at (50,50):Notice how we used append(..) to add a circle to our svg.Let us add fill to our circle and increase the radius:You can see that the same attributes we learnt in the svg work here. D3 provides an easy interface to create SVGs programmatically. Simple BarchartA bar chart is a graphic that presents grouped data with rectangular bars. Here lengths of bars are proportional to the values they represent. Drawing barsWe will be taking advantage of SVG rectangle to make rectangles for the bar chart. A rectangle has four properties : Height, Width, x start position, y start position. Let’s create a rectangle using D3:Notice how the (x,y) control the starting point of the rectangle. Let us add colour to our bar but before we will store our rectangle in a variable so that we can reuse it again. Let’s do that:Let us create bars based on our data values. We will use the same process that we used to bind data earlier. Our bars variable will change to:We have chosen a fixed width of 25 pixels for each bar and we are setting height based our data_values array. Let us see how the graphic looks:Weren’t we supposed to get nine rectangles? We have nine rectangles but since we have not given different x values they all lie on the same position. Let’s add different x values to our bars:We already know D3 passes the current data value while iterating through our data. Along with the current value it also passes the current value’s index. This index helps us keep track of the iteration count. What we are doing is we are giving i*30 as x value for our bars. This is because the i will denote the index of the current value and 30 is there because we know each bar’s width is 25 pixels so we give a padding of 5 pixels between each bar.Let’s refresh our page and we got our 9 data bars !:Isn’t our bars a little funky? Let’s scale our heights a little bit. Let’s multiply d by 5 in our height function:Much better, but why are the bars upside down?Remember that SVG Coordinate space is different as compared to conventional? Here Origin is at the top left corner so our y-axis starts from the top. Let’s adjust this so that we get a proper bar chart, we can change this by subtracting the height of each bar from the total height of the svg(400px).Let’s give a nice color to our bars:Let’s add some more values to our data_values array , it should now look like this:Do you think this visualisation is efficiently conveying the information? No, there are many things that can be improved so that we are able to convey inferences in as efficient manner as possible. One such improvement can be :“Highlighting min and max bars so that as soon as we put our gaze on the visualisation we get an idea of the two extremes, we won’t have to go through each bars and hence this will save our time.”Let’s do that!We first need to find the two extremes in our dataset. Lucky for us, D3 provides d3.max(..) and d3.min(..)  functions that we can use:We will have to now select those two bars two have max and min value respectively , for that we’ll use D3’s filter(..) method. What this will do is select only those bars that match the filter. In this case we will have filter for maximum value. We will add “green” colour fill to the bar:Similarly for the smallest bar, we will add “red” fill colour: There we have our simple barchart in less than 20 lines of code! Congratulations for coming this far, we have covered a lot of D3 basics these will help you pursue your own adventures someday! D3.js is a very powerful library and we have just scratched the surface. There is still a complex but immensely useful functionality it has to offer; that is making better charts, visualisations loaded with animations and interactivity.I have included all the code of this article is available on github. Also check out these small visualisations that I created for fun to get a glimpse of some cool stuff we can do by knowing just the basics of D3 :I urge you to try them out on your own. Good luck!You can check out my next article on D3.js here",https://www.analyticsvidhya.com/blog/2017/07/beginner-guide-build-data-visualisations-web-d3-js/
Debugging & Visualising training of Neural Network with TensorBoard,Learn everything about Analytics|Introduction|Table of Contents|Training a neural network: a boon or a curse?|Solving Age Detection Problem with Neural Networks|Intermission: an overview of Tensorboard|Back to solving the problem|Conclusion|Useful Resources|End Notes,"Testing the neural network architecture|Testing the data|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Beginner’s guide to build data visualisations on the web with D3.js|Text Classification & Word Representations using FastText (An NLP library by Facebook)|
Faizan Shaikh
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
","Step 1: Check the architecture|Step 2: Check the hyper-parameters of neural network|Step 3: Check the Complexity of network|Step 4: Check the Structure of Input data
|Step 5: Check the Distribution of data",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I started my deep learning journey a few years back. I have learnt a lot in this period. But, even after all these efforts, every Neural network I train provides me with a new experience.If you have tried to train a neural network, you must know my plight!But, through all this time, I have now made a workflow, which I will share with you today.I am sharing my learning / experience about building Neural Network with all of you.I cannot guarantee it will work all the time, but at least it may guide you as to how would you approach to solve the problem.  I will also share with you a tool which I find is a useful addition to the deep learning toolbox – TensorBoard. P.S.: This article assumes that you know the basics of building a neural network and have a fair degree of knowledge about keras. If not, I would recommend going through these articles first and then come back to this article.  Training a neural network is usually a high risk – high reward strategy. If you have tuned it right, you could potentially have a state-of-the-art working model for the task.Just to give an example, image generation is a recent breakthrough of neural networks. With just a few strokes of artificial brush, you can create an authentic looking image of natural scenes.SourceBut on the other hand, it is extremely difficult to train a generative neural network. You would have to go through a rigorous trial and error phase to get things right.I had to go through a similar experience of training a neural network few days back when trying to build a working model for Age detection problem. When I made my first model for the problem, it absolutely refused to train.  In this article, I will share my approach of how I debugged the neural network. Stay tuned, because there’s a happy ending! Before jumping on to solve the problem, let me give an overview of what the problem was.Age Detection of Indian Actors is a recently released practice problem which deals with extracting facial features for age analysis. As this is a image processing problem, I naturally jumped on to picking up neural networks to solve the problem. But as I said earlier, the network would not train. There was no change in the accuracy rates whatsoever, even when I let it train for more than an hour for 200 iterations. Here’s a proof!Clearly this was not working!Now, my years of experience came in handy! 🙂 The steps which I mention below are those which I usually follow when I’m stuck with this kind of problem. If you carefully consider, there may be two major reasons by which your neural network may not work correctly –Let’s go through these reasons one by one and eliminate the improbable. The first thing you should check when building a neural network is whether you have defined the architecture properly. Here we have a three class problem with varying sizes of images in the dataset. To simplify things, I converted all the images to size 32×32.So according to this, I had defined the architecture as followsI don’t see any issue in the architecture of neural network. According to me, this is the most important step when dealing with neural network. Its because there are so many parameters to tune that it may sometimes be frustrating to try them all. (P.S.: I did a survey of hyper-parameters  and how to tune them in this article)Fortunately, I had used a very simple neural network with only one hidden layer trained with classical gradient descent algorithm (SGD).Here is a thing to look out for; it is said that when you train a neural network with SGD, it may train slowly. To overcome this, we can use adaptive gradient descent to train the network faster.But the thing was, even on changing the training algorithm from SGD to Adam (aka adaptive gradient descent), the network did not train. This simply meant that something was fundamentally wrong with the network. I had to pull Thor’s hammer to break through this problem! Throughout my journey of understanding the neural network, I have gone through a number of tools for building and visualizing a neural network. Of all of them, I have found tensorboard to be an important asset. It can give you useful insight when training a neural network.Along with this, it gives a nice dashboard like view of the findings, which is very important when explaining your findings to the stakeholders 🙂 The image you saw above of a proof was a dashboard of tensorboard itself.I will mention the steps for how to install tensorboard in your system. I suggest you to try it out for yourself.You can install Tensorboard using pip the python package managerAfter installation you can open tensorboard by going to the terminal and typingHere’s a view of how it would look on the browser The “logs/” folder mentioned above should have the history of how the neural network was trained. You can simply get this by including a tensorboard callback in kerasIn this example, I have passed all the arguments so that everything gets saved. The meaning of the arguments are as follows: Coming back from our short excursion, the next thing I tried was to check if the network I had built was enough to learn the distributions of the problem or not. For this, instead of a simple neural network I changed the architecture to a convolutional neural network. The effect was that the accuracy increased drastically from the beginning (from 33% to 54%). But still it remained constant even after training.It seemed that our little experiment failed 🙁 After thoroughly checking the network architecture, it was time to check if we have the proper dataset itself.A few things to check are:As discussed in step 1, we have already ensured that the images are of same size before sending it to the network, so that’s out of question.The dataset is not that imbalanced, as we still have ample amount of images per classHere we should check if we have a properly processed input. For example in an image processing problem, if we processed the image and the resultant input has irregular aspect ratios, the neural network would obviously be flummoxed by it.Here as it was a simple network, we haven’t really done any preprocessing steps. So that’s out of question too. Having exhausted most of the problems that we might encounter, I was getting a bit frustrated as to what might be the real problem. Luckily my journey came to an end, as I found a weird bug which I should have caught on much earlier.The problem was that the input data I was sending to the network had a range of 0 to 255. Ideally this range should be between 0 and 1. The distribution of input data looked as below:Let me explain to you why normalizing (setting the range from 0 to 1) is important (as I have personally gone through the trouble of finding what is it so 😛 )SourceYou can see that if your data is does not have a simple distribution, the neural network might find it hard to learn this distribution. It will surely try to converge, but it won’t guarantee complete convergence. An extended version of this concept is batch normalization, which ensures the data is normalized after a layer of neural network. This paper covers in-depth analysis of how batch normalization helps train a neural network better.Here’s the code I used to read images and normalize them.Voila! After I added a simple step of normalization, I saw that the neural network started to train. I was so happy!! Just to summarize, these are the steps you should look at when debugging a neural network.Step 1: Check the architecture
Step 2: Check the hyper-parameters of neural network
Step 3: Check the Complexity of network
Step 4: Check the Structure of Input data
Step 5: Check the Distribution of data Here are some resources that I would recommend you to go through when you are stuck like me In this article, I have given my personal experience of debugging a neural network and a brief overview of visualization and debugging tool called tensorboard. I hope this will guide you solve the problems you would face in your own incursions. Please feel free to try out tensorboard and share your experiences in the comment below.",https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/
Text Classification & Word Representations using FastText (An NLP library by Facebook),Learn everything about Analytics|Introduction|Table of contents|What is FastText?|Installation|Implementation|Pros and Cons of FastText|Projects|End Notes,"Learning Word Representations|Text Classification|Computing Sentence Vectors (Supervised)|Pros|Cons|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Debugging & Visualising training of Neural Network with TensorBoard|Chief Risk Officer- Mumbai (5-7 years of experience)|
NSS
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"If you put a status update on Facebook about purchasing a car -don’t be surprised if Facebook serves you a car ad on your screen. This is not black magic! This is Facebook leveraging the text data to serve you better ads.The picture below takes a jibe at a challenge while dealing with text data.Well, it clearly failed in the above attempt to deliver the right ad. It is all the more important to capture the context in which the word has been used. This is a common problem in Natural Processing Language (NLP) tasks.A single word with the same spelling and pronunciation (homonyms) can be used in multiple contexts and a potential solution to the above problem is computing word representations.Now, imagine the challenge for Facebook. Facebook deals with enormous amount of text data on a daily basis in the form of status updates, comments etc. And it is all the more important for Facebook to utilise this text data to serve its users better. And using this text data generated by billions of users to compute word representations was a very time expensive task until Facebook developed their own library FastText, for Word Representations and Text Classification.In this article, we will see how we can calculate Word Representations and perform Text Classification, all in a matter of seconds in comparison to the existing methods which took days to achieve the same performance.  FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.This library has gained a lot of traction in the NLP community and is a possible substitution to the gensim package which provides the functionality of Word Vectors etc. If you are new to the Word Vectors and word representations in general then, I suggest you read this article first.But the question that we should be really asking is – How is FastText different from gensim Word Vectors?FastText differs in the sense that word vectors a.k.a word2vec treats every single word as the smallest unit whose vector representation is to be found but FastText assumes a word to be formed by a n-grams of character, for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]  etc, where n could range from 1 to the length of the word. This new representation of word by fastText provides the following benefits over word2vec or glove.We will now look at the steps to install the fastText library below. To make full use of the FastText library, please make sure you have the following requirements satisfied:If you do not have the above pre-requisites, I urge you to go ahead and install the above dependencies first.To install FastText, type the code below-You can check whether FastText has been properly installed by typing the below command inside the FastText folder.
./fasttextIf everything was installed correctly then, you should see the list of available commands for FastText as the output. As stated earlier, FastText was designed for two specific purposes- Word Representation Learning and Text Classification. We will see each of these steps in detail. Let us get started with learning word representations. Words in their natural form cannot be used for any Machine Learning task in general. One way to use the words is to transform these words into some representations that capture some attributes of the word. It is analogous to describing a person as – [‘height’:5.10 ,’weight’:75, ‘colour’:’dusky’, etc.] where height, weight etc are the attributes of the person. Similarly, word representations capture some abstract attributes of words in the manner that similar words tend to have similar word representations. There are primarily two methods used to develop word vectors – Skipgram and CBOW.We will see how we can implement both these methods to learn vector representations for a sample text file using fasttext.Learning word representations using Skipgram and CBOW modelsLet us see the parameters defined above in steps for easy understanding../fasttext – It is used to invoke the FastText library.
skipgram/cbow – It is where you specify whether skipgram or cbow is to be used to create the word representations.
-input – This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is.
data.txt – a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have.
-output – This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is.
model – This is the name of the model created.Running the above command will create two files named model.bin and model.vec. model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line.Now since we have created our own word vectors let’s see if we can do some common tasks like print word vectors for a word, find similar words, analogies etc. using these word vectors. Print word vectors of a wordIn order to get the word vectors for a word or set of words, save them in a text file. For example, here is a sample text file named queries.txt that contains some random words. We will get the vector representation of these words using the model we trained above../fasttext print-word-vectors model.bin < queries.txtTo check word vectors for a single word without saving into a file, you can doecho ""word"" | ./fasttext print-word-vectors model.bin Finding similar wordsYou can also find the words most similar to a given word. This functionality is provided by the nn parameter. Let’s see how we can find the most similar words to “happy”../fasttext nn model.binAfter typing the above command, the terminal will ask you to input a query word.happyby 0.183204
be 0.0822266
training 0.0522333
the 0.0404951
similar 0.036328
and 0.0248938
The 0.0229364
word 0.00767293
that 0.00138793
syntactic -0.00251774The above is the result returned for the most similar words to happy. Interestingly, this feature could be used to correct spellings too. For example, when you enter a wrong spelling, it shows the correct spelling of the word if it occurred in the training file.wrdword 0.481091
words. 0.389373
words 0.370469
word2vec 0.354458
more 0.345805
and 0.333076
with 0.325603
in 0.268813
Word2vec 0.26591
or 0.263104 AnalogiesFastText word vectors can also be used on analogies task of the kind, what is to C, what B is to A. Here, A, B and C are the words.The analogies functionality is provided by the parameter analogies. Let’s see this with the help of an example../fasttext analogies model.binThe above command will ask to input the words in the form A-B+C, but we just need to give three words separated by space.happy sad angryof 0.199229
the 0.187058
context 0.158968
a 0.151884
as 0.142561
The 0.136407
or 0.119725
on 0.117082
and 0.113304
be 0.0996916Training on a very large corpus will produce better results. As suggested by the name, text classification is tagging each document in the text with a particular class. Sentiment analysis and email classification are classic examples of text classification. In this era of technology, millions of digital documents are being generated each day. It would cost a huge amount of time as well as human efforts to categorise them in reasonable categories like spam and non-spam, important and unimportant and so on. Text classification techniques of NLP come here to our rescue. Let’s see how by doing hands-on practice based on a sentiment analysis problem. I have taken the data for this analysis from kaggle.Before we jump upon the execution, there is a word of caution about the training file. The default format of text file on which we want to train our model should be    _ _ label _ _ <X>  <Text>Where _ _label_ _ is a prefix to the class and <X> is the class assigned to the document. Also, there should not be quotes around the document and everything in one document should be on one line.
In fact, the reason why I have selected this data for this article is that the data is already available exactly in the required default format.If you are completely new to FastText and implementing text classification for very first time in FastText, I would strongly recommend using the data mentioned above.In case your data has some other formats of the label, don’t be bothered. FastText will take care of it once you pass a suitable argument. We will see how to do it in a moment. Just stick to the article.After this briefing about text classification, let’s move ahead and land on the implementation part. We will be using the train.ft text file to train the model and test.ft file to predict.#training the classifier
./fasttext supervised -input train.ft.txt -output model_kaggle -label  __label__Here, the parameters are same as the one mentioned while creating word representations. The only additional parameter is -label. This argument takes care of the format of the label specified. The file that you downloaded contains labels with the prefix __label__.If you do not wish to use default parameters for training the model, then they can be specified during the training time. For example, if you explicitly want to specify the learning rate of the training process then you can use the argument -lr to specify the learning rate../fasttext supervised -input train.ft.txt -output model_kaggle -label  __label__ -lr 0.5The other available parameters that can be tuned are –The values in the square brackets [] represent the default values of the parameters passed.# Testing the result
./fasttext test model_kaggle.bin test.ft.txtN 400000
[email protected] 0.916
[email protected] 0.916Number of examples: 400000
[email protected] is the precision
[email protected] is the recall# Predicting on the test dataset
./fasttext predict model_kaggle.bin test.ft.txt# Predicting the top 3 labels
./fasttext predict model_kaggle.bin test.ft.txt 3 This model can also be used for computing the sentence vectors. Let us see how we can compute the sentence vectors by using the following commands.echo ""this is a sample sentence"" | ./fasttext print-sentence-vectors model_kaggle.bin
0.008204 0.016523 -0.028591 -0.0019852 -0.0043028 0.044917 -0.055856 -0.057333 0.16713 0.079895 0.0034849 0.052638 -0.073566 0.10069 0.0098551 -0.016581 -0.023504 -0.027494 -0.070747 -0.028199 0.068043 0.082783 -0.033781 0.051088 -0.024244 -0.031605 0.091783 -0.029228 -0.017851 0.047316 0.013819 0.072576 -0.004047 -0.10553 -0.12998 0.021245 0.0019761 -0.0068286 0.021346 0.012595 0.0016618 0.02793 0.0088362 0.031308 0.035874 -0.0078695 0.019297 0.032703 0.015868 0.025272 -0.035632 0.031488 -0.027837 0.020735 -0.01791 -0.021394 0.0055139 0.009132 -0.0042779 0.008727 -0.034485 0.027236 0.091251 0.018552 -0.019416 0.0094632 -0.0040765 0.012285 0.0039224 -0.0024119 -0.0023406 0.0025112 -0.0022772 0.0010826 0.0006142 0.0009227 0.016582 0.011488 0.019017 -0.0043627 0.00014679 -0.003167 0.0016855 -0.002838 0.0050221 -0.00078066 0.0015846 -0.0018429 0.0016942 -0.04923 0.056873 0.019886 0.043118 -0.002863 -0.0087295 -0.033149 -0.0030569 0.0063657 0.0016887 -0.0022234 Like every library in development, it has its pros and cons. Let us state them explicitly. Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems: This article was aimed at making you aware of the FastText library as an alternative to the word2vec model and also letting you make your first vector representation and text classification model.For people who want to go in greater depth of the difference in performance of fastText and gensim, you can visit this link, where a researcher has carried out the comparison using a jupyter notebook and some standard text datasets.Please feel free to try out this library and share your experiences in the comment below.",https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/
Chief Risk Officer- Mumbai (5-7 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Text Classification & Word Representations using FastText (An NLP library by Facebook)|Machine Learning Developer- Bangalore (2- 4 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 5 – 7 years
Requirements : 
Task Info : Company Profile:We are a young and vibrant P2P lending marketplace and at the very start of our entrepreneurial journey. We have a vision to reinvent the way banking is transacted in India. We are highly focused on improving the borrower experience and improve Turn -around time and (TAT), encourage out of the box thinking and innovation.  We would be present in small ticket granular lending.Job Description:Chief Risk officer   Key Responsibilities & Job Functions:As part of core committee,  ideate and construct credit underwriting process and system. The ideal candidate in this venture would be open to think of innovative ways to do small ticket retail credit underwriting . We want to focus on new ideas to increase usage of low cost tech in banking, do credit differently and provide a seamless experience to all the stakeholders .Qualification:Post graduate with atleast 5-7 years of experience in banking operations. Candidate should have worked in small ticket retail credit underwriting/building scorecard /alternative forms of credit evaluation.
College Preference : no-bar
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/chief-risk-officer-mumbai-5-7-years-of-experience/
Machine Learning Developer- Bangalore (2- 4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Chief Risk Officer- Mumbai (5-7 years of experience)|Qlikview Consultant- Gurgaon (1-2 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 4 years
Requirements : 
Task Info : 
College Preference : no-bar
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/machine-learning-developer-bangalore-2-4-years-of-experience/
Qlikview Consultant- Gurgaon (1-2 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Developer- Bangalore (2- 4 years of experience)|Covariate Shift – Unearthing hidden problems in Real World Data Science|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 2 years
Requirements : Self-motivated, ability to work independently with minimal direction and be team oriented with ability to communicate to a wide variety of audiences
Task Info : Key responsibilities:​
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/qlikview-consultant-gurgaon-1-2-years-of-experience/
Covariate Shift – Unearthing hidden problems in Real World Data Science,Learn everything about Analytics|Introduction|Table of Contents|1. What is Dataset Shift?|2. What causes Dataset Shift?|3. Types of Dataset Shift|4. Covariate Shift|5. Identification|6. Treatment|7. End Notes,"|Steps to identify drift||6.1 Dropping|6.2 Importance weight using Density Ratio Estimation|Share this:|Like this:|Related Articles|Qlikview Consultant- Gurgaon (1-2 years of experience)|Tutorial on Automated Machine Learning using MLBox|
Shubham Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"You may have heard from various people that data science competitions are a good way to learn data science, but they are not as useful in solving real world data science problems. Why do you think this is the case?One of the differences lies in the quality of data that has been provided. In Data Science Competitions, the datasets are carefully curated. Usually, a single large dataset is split into train and test file. So, most of the times the train and test have been generated from the same distribution.But this is not the case when dealing with real world problems, especially when the data has been collected over a long period of time. In such cases, there may be multiple variables / environment changes might have happened during that period. If proper care is not taken then, the training dataset cannot be used to predict anything about the test dataset in a usable manner.In this article, we will see the different types of problems or Dataset Shift that we might encounter in the real world. Specifically, we will be talking in detail about one particular kind of shift in the Dataset (Covariate shift), the existing methods to deal with this kind of shift and an in depth demonstration of a particular method to correct this shift.  Every time you participate in a competition, your journey will look quite similar to the one shown in the figure below.Let me explain this with the help of a scenario depicted in the picture below. You are given a train and a test file in a competition. You complete the preprocessing, the feature engineering and the cross validation part on the model created but you do not get the same result as the one you get on the cross-validation. No matter what validation strategy you try, it seems like you are bound to get different results in comparison to the cross validation. Image source: linkWhat can be a possible reason for this failure? So, if you carefully notice the first picture, you will find that you did all the manipulation by just looking at the train file. Therefore, you completely ignored the information contained in the test file.Now take a look back on the second picture, you will notice that the training file contains information about male and females of fairly younger age while the test file contains information about people of older age. Therefore it means that the distribution of data contained in the train and test file is significantly different.So, if you build your model based on the data set containing information about people having lower age and predict on a data set containing higher values of age, that will definitely give you a low score. The reason is that there will a wide gap in the interest and the activities between these two groups. So your model will fail in these conditions.This change in the distribution of data contained in train and test file is called dataset shift (or drifting). Try to think some of the examples, where you can encounter the problem of dataset shift.Basically, in the real world, dataset shift mainly occurs because of the change of environments (popularly called as non-stationary environment), where the environment can be referred as location, time, etc.Let us consider an example. We collected the sales of various item during the period of July-September. Now your job is to predict the sales during the period of Diwali. The visual representation of sales in the train (blue line) and test (black line) file would be similar to the image shown below.Image source: linkClearly, the sales during the time of Diwali would be much higher as compared to routine days. Therefore we can say that it is the situation of dataset shift, which occurred due to change of time period between our train and test file.But our machine learning algorithms work by ignoring these changes. They presume that the train and test environments match and even if they don’t, it assumes that it makes no difference if the environment changes.Now take a look back at both of the examples that we discussed above. Is there any difference between them?Yes, in the first scenario, there was a shift in the age (independent variable or predictor) of the population due to which we were getting wrong predictions. While in the latter one, there was a shift in the sales (target variable) of the items. This brings the next topic to the table – Different types of Dataset shifts. Dataset shift could be divided into three types:In this article, we will discuss only covariate shift in this article since the other two topics are still an active research area and there has not been any substantial work to mitigate these problems.We will also see the methods to identify Covariate shift and the proper measures that can be taken in order to improve the predictions. Covariate shift refers to the change in the distribution of the input variables present in the training and the test data. It is the most common type of shift and it is now gaining more attention as nearly every real-world dataset suffers from this problem.First, let us try to understand how does the change in distribution creates a problem for us. Take a look at the image shown below.Image source: linkIf you carefully notice the image given above, our learning function tries to fit the training data. But here, we can see that the distribution of training and test is different, so predicting using this learned function will definitely give us wrong predictions.So our first step should be to identify this shift in the distribution. Let’s try and understand it. Here, I have used a quick and dirty machine learning technique to check whether there is a shift between the training data and the test data.For this purpose, I will use Sberbank Russian Housing Market dataset from Kaggle.The basic idea to identify shift – If there exists a shift in the dataset, then on mixing the train and test file, you should still be able to classify an instance of the mixed dataset as train or test with reasonable accuracy. Why?Because, if the features in both the dataset belong to different distributions then, they should be able to separate the dataset into train and test file significantly.Let’s try to make it simple. Take a look at the distribution of the feature ‘id’ in both the dataset.By looking at their distribution, we can clearly see that after a certain value (=30,473), all the instances will belong to test dataset.So if we create a dataset which is a mixture of training and test instances, where we have labelled each instance of training data as ‘training’ and test as ‘test’ before mixing.In this new dataset, if we just look at the feature ‘id’, we can clearly classify any instance that whether it belongs to training data or test data. Therefore, we can conclude that ‘id’ is a drifting feature for this dataset.So this was fairly easy. But we can’t visualise every variable and check whether it is drifting or not. For that purpose, let us try to code this in Python as a simple classification problem and identify the drifting features.The basic steps that we will follow are:Note that we generally take 0.80 as the threshold value, but the value can be altered based on the situation.So that is enough of theory, now let’s code this and find which of the features are drifting in this problem.## importing libraries
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import os
import matplotlib.pyplot as plt
get_ipython().magic('matplotlib inline')
os.chdir('/media/shubham/3AA25FBFA25F7DF7/Kaggle/russian housing market')
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.preprocessing import LabelEncoder## reading files
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')#### preprocessing ###### missing values
for i in train.columns:
    if train[i].dtype == 'object':
      train[i] = train[i].fillna(train[i].mode().iloc[0])
    if (train[i].dtype == 'int' or train[i].dtype == 'float'):
      train[i] = train[i].fillna(np.mean(train[i]))
for i in test.columns:
    if test[i].dtype == 'object':
      test[i] = test[i].fillna(test[i].mode().iloc[0])
    if (test[i].dtype == 'int' or test[i].dtype == 'float'):
      test[i] = test[i].fillna(np.mean(test[i]))## label encoding
number = LabelEncoder()
for i in train.columns:
    if (train[i].dtype == 'object'):
      train[i] = number.fit_transform(train[i].astype('str'))
      train[i] = train[i].astype('object')for i in test.columns:
    if (test[i].dtype == 'object'):
      test[i] = number.fit_transform(test[i].astype('str'))
      test[i] = test[i].astype('object')## creating a new feature origin
train['origin'] = 0
test['origin'] = 1
training = train.drop('price_doc',axis=1) #droping target variable## taking sample from training and test data
training = training.sample(7662, random_state=12)
testing = test.sample(7000, random_state=11)## combining random samples
combi = training.append(testing)
y = combi['origin']
combi.drop('origin',axis=1,inplace=True)## modelling
model = RandomForestClassifier(n_estimators = 50, max_depth = 5,min_samples_leaf = 5)
drop_list = []
for i in combi.columns:
score = cross_val_score(model,pd.DataFrame(combi[i]),y,cv=2,scoring='roc_auc')
 if (np.mean(score) > 0.8):
 drop_list.append(i)
 print(i,np.mean(score))
# Drifting features : {id, life_sq, kitch_sq, hospital_beds_raion, cafe_sum_500_min_price_avg, cafe_sum_500_max_price_avg, cafe_avg_price_500 }Here we have classified seven features as drifting. You can also manually check their difference in distribution through some visualisation or by using 1-way ANOVA test.So, now the important question is how to treat them effectively such that we can improve our predictions. There are different techniques by which we can treat these features in order to improve our model. Let us discuss some of them.So let’s try to understand them.This method is quite simple, as in this, we basically drop the features which are being classified as drifting. But just give it a thought, that simply dropping features might result in some loss of information.To deal with this, we have defined a simple rule.Features having a drift value greater than 0.8 and are not important in our model, we drop them.So, let’s try this in our problem.Here, I have used a basic random forest model just to check which features are important.# using a basic model with all the features
training = train.drop('origin',axis=1)
testing = test.drop('origin',axis=1)rf = RandomForestRegressor(n_estimators=200, max_depth=6,max_features=10)
rf.fit(training.drop('price_doc',axis=1),training['price_doc'])
pred = rf.predict(testing)
columns = ['price_doc']
sub = pd.DataFrame(data=pred,columns=columns)
sub['id'] = test['id']
sub = sub[['id','price_doc']]
sub.to_csv('with_drifting.csv', index=False)On submitting this file on Kaggle, we are getting a rmse score of 0.40116 on private leaderboard.So, let’s check first 20 important features for this model.### plotting importances
features = training.drop('price_doc',axis=1).columns.values
imp = rf.feature_importances_
indices = np.argsort(imp)[::-1][:20]#plot
plt.figure(figsize=(8,5))
plt.bar(range(len(indices)), imp[indices], color = 'b', align='center')
plt.xticks(range(len(indices)), features[indices], rotation='vertical')
plt.xlim([-1,len(indices)])
plt.show()Now, if we compare our drop list and feature importance, we will find that the features ‘life_sq’ and ‘kitch_sq’ are common.So, we will keep these two features in our model, while dropping the rest of the drifting features.NOTE: Before dropping any feature, just make sure you if there any possibility to create a new feature from it.Let’s try this and check whether it improves our prediction or not.## dropping drifting features which are not important.
drift_train = training.drop(['id','hospital_beds_raion','cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg','cafe_avg_price_500'], axis=1)
drift_test = testing.drop(['id','hospital_beds_raion','cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg','cafe_avg_price_500'], axis=1)rf = RandomForestRegressor(n_estimators=200, max_depth=6,max_features=10)
rf.fit(drift_train.drop('price_doc',axis=1),training['price_doc'])
pred = rf.predict(drift_test)
columns = ['price_doc']
sub = pd.DataFrame(data=pred,columns=columns)
sub['id'] = test['id']
sub = sub[['id','price_doc']]
sub.to_csv('without_drifting.csv', index=False)On submission of this file on Kaggle, we got a rmse score of 0.39759 on the private leaderboard.Congratulations, we have successfully improved our performance using this technique. In this method, the approach to importance estimation would be to first estimate the training and test densities separately and then estimate the importance by taking the ratio of the estimated densities of test and train.Then these densities act as weights for each instance in the training data.But giving weights to each instance based on the density ratio could be a rigorous task in higher dimensional data sets. I tried this method on an i7 processor with 128 GB RAM and it took around 3 minutes to calculate the ratio density for a single feature. Also, I could not find any improvement in the score on applying the weights to the training data.Also scaling this feature for 200 features would be a very time-consuming task.Therefore, this method is only good up to research papers but the application of this in the real world is still questionable. Also, this is an active area of research. I hope that now you have a better understanding about drift, how you can identify it and treat it effectively. It has now become a common problem in real world dataset. So you should develop a habit to check this every time while solving problems, and surely it will give you positive results.Did you find this article helpful? Please share your opinions/thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/
Tutorial on Automated Machine Learning using MLBox,Learn everything about Analytics|Introduction|Table of Contents|1. What is MLBox?|2. MLBox in comparison to the other Machine Learning Libraries|3. Installing MLBox|4. Layout / Pipeline of MLBox|5. Building a Machine Learning Regressor using MLBox|6. Basic Understanding of Drift|7. Basic Understanding of Entity Embedding|8. Pros and Cons of MLBox|9. End Notes,"Pre-Processing|Optimisation|Prediction|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Covariate Shift – Unearthing hidden problems in Real World Data Science|30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest – NLP]|
NSS
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Reading and cleaning a file|Removing the Drifting Variables|Most of the images have been taken from the documentation of MLBox itself.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Recently, one of my friends and I were solving a practice problem. After 8 hours of hard work & coding, my friend Shubham got a score of 1153 (position 219). Here is his position on leaderboard:On the other hand, I was able to achieve this by writing only 8 lines of code:How did I get there?What if I tell you there exists a library called MLBox , which does most of the heavy lifting in machine learning for you in minimal lines of code? From missing value imputation to feature engineering using state-of-the-art Entity Embeddings for categorical features, MLBox has it all.In these 8 lines of code using MLBox, I have also performed hyperparameter optimisation and tested around 50 models with blazing speed – isn’t that awesome? You will be able to use this library by end of this article.  According to the developer of MLBox,“MLBox is a powerful Automated Machine Learning Python library. It provides the following features: MLBox focuses on the below three points in particular in comparison to the other libraries:We will be studying about these below in some detail to have an idea about what they do. MLBox is currently available for Linux only. MLBox was primarily developed using Python 2 and last night it was extended to Python 3. We will be installing the latest 3.0 dev version of MLBox. Follow the below steps to install MLBox into your Linux System.Note – This library is currently under very active development and therefore there may be the cases that something that works now may break the next day. For example, this library worked pretty well till 2 days ago for Python 2.7 and didn’t work so good for Python 3.6. But at the time of writing, I am experiencing some issues with the 2.7 version and the 3 version is working fine for now. Also please feel free to open issues on the github repository and asking for help in the comments below. The entire pipeline of MLBox looks like below- The entire pipeline of MLBox has been divided into 3 sections/sub-packages.We will be studying about these 3 sub-packages in detail below. All the functionalities inside this sub-package can be used via the command-
from mlbox.preprocessing import *This sub-package provides functionalities related to two major functions.This package supports reading a wide variety of file formats like csv, Excel, hdf5, JSON etc. but in this article, we will be primarily seeing the most common “.csv” file format. Follow the below steps to read a csv file.Step1: Create an object of the Reader class with the separator as a parameter. “,” is the separator in the case of a csv file.
s="",""
r=Reader(s)   #initialising the object of Reader ClassStep2: Make a list of the train and test file paths and also identify the target variable name.
path=[""path of the train csv file"",""path of the test csv file ""]
target_name=""name of the target variable in the train file""

Step3: Performing the cleaning operation and creating a cleaned train and test file.
data=r.train_test_split(path,target_name)
The cleaning steps performed in the above step are-
-deleting unnamed columns
-removing duplicates
-extracting month, year and day of the week from a Date columnThe drifting Variables are explained in the later section. To remove the drifting variables, follow the below steps.Step1: Create an object of class Drift_thresholder
dft=Drift_thresholder()Step2: Use the fit_transform method of the created object to remove the drift variables.
data=dft.fit_transform(data) All the functionalities inside this sub-package can be used via the command-
from mlbox.optimisation import *This is the section where this library scores the maximum points. This hyper-parameter optimisation method in this library uses the hyperopt library which is very fast and you can almost optimise anything in this library from choosing the right missing value imputation method to the depth of an XGBOOST model. This library creates a high-dimensional space of the parameters to be optimised and chooses the best combination of the parameters that lowers the validation score.Below is the table of the four broad optimisations that are done in the MLBox library with terms to the right of hyphen that can be optimised for different values.Missing Values Encoder(ne) – numerical_strategy (when the column to be imputed is a continuous column eg- mean, median etc), categorical_strategy(when the column to be imputed is a categorical column e.g.- NaN values etc)Categorical Values Encoder(ce)– strategy (method of encoding categorical variables e.g.- label_encoding, dummification, random_projection, entity_embedding)Feature Selector(fs)– strategy (different methods for feature selection e.g. l1, variance, rf_feature_importance), threshold (the percentage of features to be discarded)Estimator(est)–strategy (different algorithms that can be used as estimators eg- LightGBM, xgboost etc.), **params(parameters specific to the algorithm being used eg- max_depth, n_estimators etc.)Let us take an example and create a hyperparameter space to be optimised. Let us state all the parameters that I want to optimise:Algorithm to be used- LightGBM
LightGBM max_depth-[3,5,7,9]
LightGBM n_estimators-[250,500,700,1000]
Feature selection-[variance, l1, random forest feature importance]
Missing values imputation – numerical(mean,median),categorical(NAN values)
categorical values encoder- label encoding, entity embedding and random projectionLet us now create our hyper-parameter space. Before that, remember, hyper-parameter is a dictionary of key and value pairs where value is also a dictionary given by the syntax
{“search”:strategy,”space”:list}, where strategy can be either “choice” or “uniform” and list is the list of values.space={'ne__numerical_strategy':{""search"":""choice"",""space"":['mean','median']},
'ne__categorical_strategy':{""search"":""choice"",""space"":[np.NaN]},
'ce__strategy':{""search"":""choice"",""space"":['label_encoding','entity_embedding','random_projection']},
'fs__strategy':{""search"":""choice"",""space"":['l1','variance','rf_feature_importance']},
'fs__threshold':{""search"":""uniform"",""space"":[0.01, 0.3]},
'est__max_depth':{""search"":""choice"",""space"":[3,5,7,9]},
'est__n_estimators':{""search"":""choice"",""space"":[250,500,700,1000]}}Now we will see the steps to choose the best combination from the above space using the following steps:Step1: Create an object of class Optimiser which has the parameters as ‘scoring’ and ‘n_folds’. Scoring is the metric against which we want to optimise our hyper-parameter space and n_folds is the number of folds of cross-validation
Scoring values for Classification- ""accuracy"", ""roc_auc"", ""f1"", ""log_loss"", ""precision"", ""recall""
Scoring values for Regression- ""mean_absolute_error"", ""mean_squarred_error"", ""median_absolute_error"", ""r2""
opt=Optimiser(scoring=""accuracy"",n_folds=5)Step2: Use the optimise function of the object created above which takes the hyper-parameter space, dictionary created by the train_test_split and number of iterations as the parameters. This function returns the best hyper-paramters from the hyper-parameter space.
best=opt.optimise(space,data,40) All the functions in this sub-package can be installed using the command below.
from mlbox.prediction import *This sub-package predicts on the test dataset using the best hyper-parameters calculated using the optimisation sub-package. To predict on the test dataset, go through the following steps.Step1: Create an object of class Predictor
pred=Predictor()Step2: Use the fit_predict method of the object created above which takes a set of hyperparameters and dictionary created through train_test_split as the parameter.
pred.fit_predict(best,data)The above method saves the feature importance, drift variables coefficients and the final predictions into a separate folder named ‘save’. We are now going to build a Machine Learning Classifier in just 7 lines of code with hyperparameter optimisation. We are going to solve the Big Marts sales problem. Download the train and test file and keep them in a single folder. Using the MLBox library, we are going to submit our first prediction without even having to look at the data. You can find the code below to make the prediction for the problem.# coding: utf-8# importing the required libraries
from mlbox.preprocessing import *
from mlbox.optimisation import *
from mlbox.prediction import *# reading and cleaning the train and test files
df=Reader(sep="","").train_test_split(['/home/nss/Downloads/mlbox_blog/train.csv', '/home/nss/Downloads/mlbox_blog/test.csv'],'Item_Outlet_Sales')# removing the drift variables
df=Drift_thresholder().fit_transform(df)# setting the hyperparameter space
space={'ne__numerical_strategy':{""search"":""choice"",""space"":['mean','median']},
'ne__categorical_strategy':{""search"":""choice"",""space"":[np.NaN]},
'ce__strategy':{""search"":""choice"",""space"":['label_encoding','entity_embedding','random_projection']},
'fs__strategy':{""search"":""choice"",""space"":['l1','variance','rf_feature_importance']},
'fs__threshold':{""search"":""uniform"",""space"":[0.01, 0.3]},
'est__max_depth':{""search"":""choice"",""space"":[3,5,7,9]},
'est__n_estimators':{""search"":""choice"",""space"":[250,500,700,1000]}}# calculating the best hyper-parameter
best=Optimiser(scoring=""mean_squared_error"",n_folds=5).optimise(space,df,40)# predicting on the test dataset
Predictor().fit_predict(best,df)The above code ranked 108(top 1%) on the Public Leaderboard without having to even open the train and test file. I think this is pretty awesome.Below is the image of feature importance as calculated by LightGBM. Drift is not a common topic but a very important one and it deserves an article of its own. But I will try to explain the functionality of Drift_Thresholder in brief.In general, we assume that train and test dataset are created through the same generative algorithm or process but this assumption is quite strong and we do not see this behaviour in the real world. In the real world, the data generator or the process may change. For example, in a sales prediction model, the customer behaviour changes over time and hence the data generated will be different than the data that was used to create the model. This is called drift.Another point to note is that in a dataset, both the independent features and the dependent feature may drift. When the independent features changes, it is called the covariate shift and when the relationship between the independent and dependent features change, it is called the concept shift. MLBox deals with the covariate shift. The general algorithm for detection of drift is as follows-Entity Embeddings owe their existence to the word2vec embeddings in the sense that they function the same way as word vectors do. For example, we know that in word vector representation, we can do things like below. In the similar sense, categorical variables could be encoded to create new informative features. Their effect was evident to the world in Kaggle’s Rossmann Sales Problem where a team used Entity Embeddings along with Neural Network and came third without performing any significant feature engineering. The entire code and the research paper on Entity Embeddings that resulted from the competition could be found here. The Entity Embeddings were able to capture the relationship between the German states as shown below.I don’t want to bog you down with the explanation of Entity Embeddings here. It deserves its own article. In MLBox, you can use Entity Embedding as a black box for encoding categorical variables.This library has its own sets of pros and cons.The pros are –The cons are-So, I suggest you weigh the pros and cons before making this your mainstream library for Machine Learning.I was really excited to try this library as soon as I read about its release on Github. I spent the next couple of days studying the library and simplifying it for you to use it on the go. I must say that I am really impressed with the library and am going to explore even more. With just 8 lines of code, I was able to break into top 1% and without having to spend time explicitly on handling data and hyperparameter optimisation, I could dedicate more time to feature engineering and check them on the fly. Please feel free to comment for any help or ideas below.",https://www.analyticsvidhya.com/blog/2017/07/mlbox-library-automated-machine-learning/
30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest – NLP],Learn everything about Analytics|Introduction||Skill Test Questions and Answers|End Notes,"Overall Distribution|Helpful Resources|Share this:|Like this:|Related Articles|Tutorial on Automated Machine Learning using MLBox|30 Questions to test a data scientist on Linear Regression [Solution: Skilltest – Linear Regression]|
Shivam Bansal
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Humans are social animals and language is our primary tool to communicate with the society. But, what if machines could understand our language and then act accordingly? Natural Language Processing (NLP) is the science of teaching machines how to understand the language we humans speak and write.We recently launched an NLP skill test on which a total of 817 people registered. This skill test was designed to test your knowledge of Natural Language Processing. If you are one of those who missed out on this skill test, here are the questions and solutions.Here are the leaderboard ranking for all the participants.Below are the distribution scores, they will help you evaluate your performance.You can access the scores here. More than 250 people participated in the skill test and the highest score obtained was 24.Here are some resources to get in-depth knowledge of the subject.Ultimate Guide to Understand & Implement Natural Language Processing (with codes in Python) Q1 Which of the following techniques can be used for the purpose of keyword normalization, the process of converting a keyword into its base form?
A) 1 and 2
B) 2 and 4
C) 1 and 3
D) 1, 2 and 3
E) 2, 3 and 4
F) 1, 2, 3 and 4Solution: (C)Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.
2) N-grams are defined as the combination of N keywords together. How many bi-grams can be generated from given sentence: “Analytics Vidhya is a great source to learn data science”A) 7
B) 8
C) 9
D) 10
E) 11Solution: (C)Bigrams: Analytics Vidhya, Vidhya is, is a, a great, great source, source to, To learn, learn data, data science 3) How many trigrams phrases can be generated from the following sentence, after performing following text cleaning steps:“#Analytics-vidhya is a great source to learn @data_science.”A) 3
B) 4
C) 5
D) 6
E) 7Solution: (C)After performing stopword removal and punctuation replacement the text becomes: “Analytics vidhya great source learn data science”Trigrams – Analytics vidhya great, vidhya great source, great source learn, source learn data, learn data science 4) Which of the following regular expression can be used to identify date(s) present in the text object: “The next meetup on data science will be held on 2017-09-21, previously it happened on 31/03, 2016”A) \d{4}-\d{2}-\d{2}
B) (19|20)\d{2}-(0[1-9]|1[0-2])-[0-2][1-9]
C) (19|20)\d{2}-(0[1-9]|1[0-2])-([0-2][1-9]|3[0-1])
D) None of the aboveSolution: (D)None if these expressions would be able to identify the dates in this text object. Question Context 5-6:You have collected a data of about 10,000 rows of tweet text and no other information. You want to create a tweet classification model that categorizes each of the tweets in three buckets – positive, negative and neutral.5) Which of the following models can perform tweet classification with regards to context mentioned above? A) Naive Bayes
B) SVM
C) None of the aboveSolution: (C)Since, you are given only the data of tweets and no other information, which means there is no target variable present. One cannot train a supervised learning model, both svm and naive bayes are supervised learning techniques. 6) You have created a document term matrix of the data, treating every tweet as one document. Which of the following is correct, in regards to document term matrix?A) Only 1
B) Only 2
C) Only 3
D) 1 and 2
E) 2 and 3
F) 1, 2 and 3Solution: (D)Choices A and B are correct because stopword removal will decrease the number of features in the matrix, normalization of words will also reduce redundant features, and, converting all words to lowercase will also decrease the dimensionality. 7) Which of the following features can be used for accuracy improvement of a classification model?A) Frequency count of terms
B) Vector Notation of sentence
C) Part of Speech Tag
D) Dependency Grammar
E) All of theseSolution: (E)All of the techniques can be used for the purpose of engineering features in a model. 8) What percentage of the total statements are correct with regards to Topic Modeling?A) 0
B) 25
C) 50
D) 75
E) 100Solution: (A)LDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant analysis. Selection of the number of topics is directly proportional to the size of the data, while number of topic terms is not directly proportional to the size of the data. Hence none of the statements are correct. 9) In Latent Dirichlet Allocation model for text classification purposes, what does alpha and beta hyperparameter represent-A) Alpha: number of topics within documents, beta: number of terms within topics False
B) Alpha: density of terms generated within topics, beta: density of topics generated within terms False
C) Alpha: number of topics within documents, beta: number of terms within topics False
D) Alpha: density of topics generated within documents, beta: density of terms generated within topics TrueSolution: (D)Option D is correct 10) Solve the equation according to the sentence “I am planning to visit New Delhi to attend Analytics Vidhya Delhi Hackathon”.A = (# of words with Noun as the part of speech tag)
B = (# of words with Verb as the part of speech tag)
C = (# of words with frequency count greater than one)What are the correct values of A, B, and C?A) 5, 5, 2
B) 5, 5, 0
C) 7, 5, 1
D) 7, 4, 2
E) 6, 4, 3Solution: (D)Nouns: I, New, Delhi, Analytics, Vidhya, Delhi, Hackathon (7)Verbs: am, planning, visit, attend (4)Words with frequency counts > 1: to, Delhi (2)Hence option D is correct. 11) In a corpus of N documents, one document is randomly picked. The document contains a total of T terms and the term “data” appears K times. What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term “data” appears in approximately one-third of the total documents?A) KT * Log(3)
B) K * Log(3) / T
C) T * Log(3) / K
D) Log(3) / KTSolution: (B)formula for TF is K/Tformula for IDF is log(total docs / no of docs containing “data”)= log(1 / (⅓))= log (3)Hence correct choice is Klog(3)/T Question Context 12 to 14:Refer the following document term matrix
12) Which of the following documents contains the same number of terms and the number of terms in the one of the document is not equal to least number of terms in any document in the entire corpus. A) d1 and d4
B) d6 and d7
C) d2 and d4
D) d5 and d6Solution: (C)Both of the documents d2 and d4 contains 4 terms and does not contain the least number of terms which is 3. 13) Which are the most common and the rarest term of the corpus?A) t4, t6
B) t3, t5
C) t5, t1
D) t5, t6Solution: (A)T5 is most common terms across 5 out of 7 documents, T6 is rare term only appears in d3 and d4 14) What is the term frequency of a term which is used a maximum number of times in that document? A) t6 – 2/5
B) t3 – 3/6
C) t4 – 2/6
D) t1 – 2/6Solution: (B)t3 is used max times in entire corpus = 3, tf for t3 is 3/6 15) Which of the following technique is not a part of flexible text matching?A) Soundex
B) Metaphone
C) Edit Distance
D) Keyword HashingSolution: (D)Except Keyword Hashing all other are the techniques used in flexible string matchingFeel like improving your skillset? Click Here16) True or False: Word2Vec model is a machine learning model used to create vector notations of text objects. Word2vec contains multiple deep neural networksA) TRUE
B) FALSESolution: (B)Word2vec also contains preprocessing model which is not a deep neural network 17) Which of the following statement is(are) true for Word2Vec model?A) The architecture of word2vec consists of only two layers – continuous bag of words and skip-gram model
B) Continuous bag of word (CBOW) is a Recurrent Neural Network model
C) Both CBOW and Skip-gram are shallow neural network models
D) All of the aboveSolution: (C)Word2vec contains the Continuous bag of words and skip-gram models, which are deep neural nets. 18) With respect to this context-free dependency graphs, how many sub-trees exists in the sentence?

A) 3
B) 4
C) 5
D) 6Solution: (D)Subtrees in the dependency graph can be viewed as nodes having an outward link, for example:Media, networking, play, role, billions, and lives are the roots of subtrees 19) What is the right order for a text classification model components
A) 12345
B) 13425
C) 12534
D) 13452Solution: (C)A right text classification model contains – cleaning of text to remove noise, annotation to create more features, converting text-based features into predictors, learning a model using gradient descent and finally tuning a model. 20) Polysemy is defined as the coexistence of multiple meanings for a word or phrase in a text object. Which of the following models is likely the best choice to correct this problem?A) Random Forest Classifier
B) Convolutional Neural Networks
C) Gradient Boosting
D) All of theseSolution: (B)CNNs are popular choice for text classification problems because they take into consideration left and right contexts of the words as features which can solve the problem of polysemy 21) Which of the following models can be used for the purpose of document similarity?A) Training a word 2 vector model on the corpus that learns context present in the document
B) Training a bag of words model that learns occurrence of words in the document
C) Creating a document-term matrix and using cosine similarity for each document
D) All of the aboveSolution: (D)word2vec model can be used for measuring document similarity based on context. Bag Of Words and document term matrix can be used for measuring similarity based on terms. 22) What are the possible features of a text corpus A) 1
B) 12
C) 123
D) 1234
E) 12345
F) 123456Solution: (E)Except for entire document as the feature, rest all can be used as features of text classification learning model. 23) While creating a machine learning model on text data, you created a document term matrix of the input data of 100K documents. Which of the following remedies can be used to reduce the dimensions of data – A) only 1
B) 2, 3
C) 1, 3
D) 1, 2, 3Solution: (D)All of the techniques can be used to reduce the dimensions of the data. 24) Google Search’s feature – “Did you mean”, is a mixture of different techniques. Which of the following techniques are likely to be ingredients?A) 1
B) 2
C) 1, 2
D) 1, 2, 3Solution: (C)Collaborative filtering can be used to check what are the patterns used by people, Levenshtein is used to measure the distance among dictionary terms. 25) While working with text data obtained from news sentences, which are structured in nature, which of the grammar-based text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection and object detection.A) Part of speech tagging
B) Dependency Parsing and Constituency Parsing
C) Skip Gram and N-Gram extraction
D) Continuous Bag of WordsSolution: (B)Dependency and constituent parsing extract these relations from the text 26) Social Media platforms are the most intuitive form of text data. You are given a corpus of complete social media data of tweets. How can you create a model that suggests the hashtags? A) Perform Topic Models to obtain most significant words of the corpus
B) Train a Bag of Ngrams model to capture top n-grams – words and their combinations
C) Train a word2vector model to learn repeating contexts in the sentences
D) All of theseSolution: (D)All of the techniques can be used to extract most significant terms of a corpus. 27) While working with context extraction from a text data, you encountered two different sentences: The tank is full of soldiers. The tank is full of nitrogen. Which of the following measures can be used to remove the problem of word sense disambiguation in the sentences? A) Compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood
B) Co-reference resolution in which one resolute the meaning of ambiguous word with the proper noun present in the previous sentence
C) Use dependency parsing of sentence to understand the meaningsSolution: (A)Option 1 is called Lesk algorithm, used for word sense disambiguation, rest others cannot be used. 28) Collaborative Filtering and Content Based Models are the two popular recommendation engines, what role does NLP play in building such algorithms.A) Feature Extraction from text
B) Measuring Feature Similarity
C) Engineering Features for vector space learning model
D) All of theseSolution: (D)NLP can be used anywhere where text data is involved – feature extraction, measuring feature similarity, create vector features of the text. 29) Retrieval based models and Generative models are the two popular techniques used for building chatbots. Which of the following is an example of retrieval model and generative model respectively.A) Dictionary based learning and Word 2 vector model
B) Rule-based learning and Sequence to Sequence model
C) Word 2 vector and Sentence to Vector model
D) Recurrent neural network and convolutional neural networkSolution: (B)choice 2 best explains examples of retrieval based models and generative models 30) What is the major difference between CRF (Conditional Random Field) and HMM (Hidden Markov Model)?A) CRF is Generative whereas HMM is Discriminative model
B) CRF is Discriminative whereas HMM is Generative model
C) Both CRF and HMM are Generative model
D) Both CRF and HMM are Discriminative modelSolution: (B)Option B is correct I tried my best to make the solutions as comprehensive as possible but if you have any questions/doubts please drop in your comments below. And I would love to hear the feedback about the skill test. Feel free to share them in comments below. For latest and upcoming skill test please refer to the DataHack platform of Analytics Vidhya.If you want to learn more about Natural Language Processing and how it is implemented in Python, then check out our video course on NLP using Python.Happy Learning!",https://www.analyticsvidhya.com/blog/2017/07/30-questions-test-data-scientist-natural-language-processing-solution-skilltest-nlp/
30 Questions to test a data scientist on Linear Regression [Solution: Skilltest – Linear Regression],Learn everything about Analytics|Introduction|Overall Distribution||Helpful Resources|Skill test Questions and Answers|End Notes,"Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest – NLP]|Architecture of Convolutional Neural Networks (CNNs) demystified|
Ankit Gupta
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Linear Regression is still the most prominently used statistical technique in data science industry and in academia to explain relationships between features.A total of 1,355 people registered for this skill test. It was specially designed for you to test your knowledge on linear regression techniques. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how many could have answered correctly.Here is the leaderboard for the participants who took the test. Below is the distribution of the scores of the participants:You can access the scores here. More than 800 people participated in the skill test and the highest score obtained was 28. Here are some resources to get in depth knowledge in the subject.5 Questions which can teach you Multiple Regression (with R and Python)Going Deeper into Regression Analysis with Assumptions, Plots & Solutions7 Types of Regression Techniques you should know! 1) True-False: Linear Regression is a supervised machine learning algorithm.A) TRUE
B) FALSESolution: (A)Yes, Linear regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variable (x) and an output variable (Y) for each example. 2) True-False: Linear Regression is mainly used for Regression.A) TRUE
B) FALSESolution: (A)Linear Regression has dependent variables that have continuous values. 3) True-False: It is possible to design a Linear regression algorithm using a neural network?A) TRUE
B) FALSESolution: (A)True. A Neural network can be used as a universal approximator, so it can definitely implement a linear regression algorithm. 4) Which of the following methods do we use to find the best fit line for data in Linear Regression?A) Least Square Error
B) Maximum Likelihood
C) Logarithmic Loss
D) Both A and BSolution: (A)In linear regression, we try to minimize the least square errors of the model to identify the line of best fit.5) Which of the following evaluation metrics can be used to evaluate a model while modeling a continuous output variable?A) AUC-ROC
B) Accuracy
C) Logloss
D) Mean-Squared-ErrorSolution: (D)Since linear regression gives output as continuous values, so in such case we use mean squared error metric to evaluate the model performance. Remaining options are use in case of a classification problem. 6) True-False: Lasso Regularization can be used for variable selection in Linear Regression.A) TRUE
B) FALSESolution: (A)True, In case of lasso regression we apply absolute penalty which makes some of the coefficients zero.7) Which of the following is true about Residuals ?A) Lower is better
B) Higher is better
C) A or B depend on the situation
D) None of theseSolution: (A)Residuals refer to the error values of the model. Therefore lower residuals are desired.8) Suppose that we have N independent variables (X1,X2… Xn) and dependent variable is Y. Now Imagine that you are applying linear regression by fitting the best fit line using least square error on this data.You found that correlation coefficient for one of it’s variable(Say X1) with Y is -0.95.Which of the following is true for X1?A) Relation between the X1 and Y is weak
B) Relation between the X1 and Y is strong
C) Relation between the X1 and Y is neutral
D) Correlation can’t judge the relationshipSolution: (B)The absolute value of the correlation coefficient denotes the strength of the relationship. Since  absolute correlation is very high it means that the relationship is strong between X1 and Y. 9) Looking at above two characteristics, which of the following option is the correct for Pearson correlation between V1 and V2?If you are given the two variables V1 and V2 and they are following below two characteristics.1. If V1 increases then V2 also increases2. If V1 decreases then V2 behavior is unknownA) Pearson correlation will be close to 1
B) Pearson correlation will be close to -1
C) Pearson correlation will be close to 0
D) None of these
Solution: (D)We cannot comment on the correlation coefficient by using only statement 1.  We need to consider the both of these two statements. Consider V1 as x and V2 as |x|. The correlation coefficient would not be close to 1 in such a case.10) Suppose Pearson correlation between V1 and V2 is zero. In such case, is it right to conclude that V1 and V2 do not have any relation between them?A) TRUE
B) FALSESolution: (B)Pearson correlation coefficient between 2 variables might be zero even when they have a relationship between them. If the correlation coefficient is zero, it just means that that they don’t move together. We can take examples like y=|x| or y=x^2.11) Which of the following offsets, do we use in linear regression’s least square line fit? Suppose horizontal axis is independent variable and vertical axis is dependent variable. A) Vertical offset
B) Perpendicular offset
C) Both, depending on the situation
D) None of aboveSolution: (A)We always consider residuals as vertical offsets. We calculate the direct differences between actual value and the Y labels. Perpendicular offset are useful in case of PCA.12) True- False: Overfitting is more likely when you have huge amount of data to train?A) TRUE
B) FALSESolution: (B)With a small training dataset, it’s easier to find a hypothesis to fit the training data exactly i.e. overfitting. 13) We can also compute the coefficient of linear regression with the help of an analytical method called “Normal Equation”. Which of the following is/are true about Normal Equation? A) 1 and 2
B) 1 and 3
C) 2 and 3
D) 1,2 and 3Solution: (D)Instead of gradient descent, Normal Equation can also be used to find coefficients. Refer this article for read more about normal equation. 14) Which of the following statement is true about sum of residuals of A and B?Below graphs show two fitted regression lines (A & B) on randomly generated data. Now, I want to find the sum of residuals in both cases A and B.Note:A) A has higher sum of residuals than B
B) A has lower sum of residual than B
C) Both have same sum of residuals
D) None of theseSolution: (C)Sum of residuals will always be zero, therefore both have same sum of residuals Question Context 15-17:Suppose you have fitted a complex regression model on a dataset. Now, you are using Ridge regression with penality x.15) Choose the option which describes bias in best manner.
A) In case of very large x; bias is low
B) In case of very large x; bias is high
C) We can’t say about bias
D) None of theseSolution: (B)If the penalty is very large it means model is less complex, therefore the bias would be high. 16) What will happen when you apply very large penalty?A) Some of the coefficient will become absolute zero
B) Some of the coefficient will approach zero but not absolute zero
C) Both A and B depending on the situation
D) None of theseSolution: (B)In lasso some of the coefficient value become zero, but in case of Ridge, the coefficients become close to zero but not zero. 17) What will happen when you apply very large penalty in case of Lasso?
A) Some of the coefficient will become zero
B) Some of the coefficient will be approaching to zero but not absolute zero
C) Both A and B depending on the situation
D) None of theseSolution: (A)As already discussed, lasso applies absolute penalty, so some of the coefficients will become zero. 18) Which of the following statement is true about outliers in Linear regression?A) Linear regression is sensitive to outliers
B) Linear regression is not sensitive to outliers
C) Can’t say
D) None of theseSolution: (A)The slope of the regression line will change due to outliers in most of the cases. So Linear Regression is sensitive to outliers. 19) Suppose you plotted a scatter plot between the residuals and predicted values in linear regression and you found that there is a relationship between them. Which of the following conclusion do you make about this situation? A) Since the there is a relationship means our model is not good
B) Since the there is a relationship means our model is good
C) Can’t say
D) None of theseSolution: (A)There should not be any relationship between predicted values and residuals. If there exists any relationship between them,it means that the model has not perfectly captured the information in the data. Question Context 20-22:Suppose that you have a dataset D1 and you design a linear regression model of degree 3 polynomial and you found that the training and testing error is “0” or in another terms it perfectly fits the data.20) What will happen when you fit degree 4 polynomial in linear regression?
A) There are high chances that degree 4 polynomial will over fit the data
B) There are high chances that degree 4 polynomial will under fit the data
C) Can’t say
D) None of theseSolution: (A)Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero. 21) What will happen when you fit degree 2 polynomial in linear regression?
A) It is high chances that degree 2 polynomial will over fit the data
B) It is high chances that degree 2 polynomial will under fit the data
C) Can’t say
D) None of theseSolution: (B)If a degree 3 polynomial fits the data perfectly, it’s highly likely that a simpler model(degree 2 polynomial) might under fit the data. 22) In terms of bias and variance. Which of the following is true when you fit degree 2 polynomial?
A) Bias will be high, variance will be high
B) Bias will be low, variance will be high
C) Bias will be high, variance will be low
D) Bias will be low, variance will be lowSolution: (C)Since a degree 2 polynomial will be less complex as compared to degree 3, the bias will be high and variance will be low. Question Context 23:Which of the following is true about below graphs(A,B, C left to right) between the cost function and Number of iterations?23) Suppose l1, l2 and l3 are the three learning rates for A,B,C respectively. Which of the following is true about l1,l2 and l3? A) l2 < l1 < l3B) l1 > l2 > l3
C) l1 = l2 = l3
D) None of theseSolution: (A)In case of high learning rate, step will be high, the objective function will decrease quickly initially, but it will not find the global minima and objective function starts increasing after a few iterations.In case of low learning rate, the step will be small. So the objective function will decrease slowly Question Context 24-25:We have been given a dataset with n records in which we have input attribute as x and output attribute as y. Suppose we use a linear regression method to model this data. To test our linear regressor, we split the data in training set and test set randomly.24) Now we increase the training set size gradually. As the training set size increases, what do you expect will happen with the mean training error? A) Increase
B) Decrease
C) Remain constant
D) Can’t SaySolution: (D)Training error may increase or decrease depending on the values that are used to fit the model. If the values used to train contain more outliers gradually, then the error might just increase. 25) What do you expect will happen with bias and variance as you increase the size of training data? A) Bias increases and Variance increases
B) Bias decreases and Variance increases
C) Bias decreases and Variance decreases
D) Bias increases and Variance decreases
E) Can’t Say FalseSolution: (D)As we increase the size of the training data, the bias would increase while the variance would decrease. Question Context 26:Consider the following data where one input(X) and one output(Y) is given.26) What would be the root mean square training error for this data if you run a Linear Regression model of the form (Y = A0+A1X)? A) Less than 0
B) Greater than zero
C) Equal to 0
D) None of theseSolution: (C)We can perfectly fit the line on the following data so mean error will be zero. Question Context 27-28:Suppose you have been given the following scenario for training and validation error for Linear Regression. 27) Which of the following scenario would give you the right hyper parameter?A) 1
B) 2
C) 3
D) 4Solution: (B)Option B would be the better option because it leads to less training as well as validation error.28) Suppose you got the tuned hyper parameters from the previous question. Now, Imagine you want to add a variable in variable space such that this added feature is important. Which of the following thing would you observe in such case?A) Training Error will decrease and Validation error will increaseB) Training Error will increase and Validation error will increase
C) Training Error will increase and Validation error will decrease
D) Training Error will decrease and Validation error will decrease
E) None of the aboveSolution: (D)If the added feature is important, the training and validation error would decrease.Question Context 29-30:Suppose, you got a situation where you find that your linear regression model is under fitting the data.29) In such situation which of the following options would you consider?A) 1 and 2
B) 2 and 3
C) 1 and 3
D) 1, 2 and 3Solution: (A)In case of under fitting, you need to induce more variables in variable space or you can add some polynomial degree variables to make the model more complex to be able to fir the data better. 30) Now situation is same as written in previous question(under fitting).Which of following regularization algorithm would you prefer? A) L1
B) L2
C) Any
D) None of theseSolution: (D)I won’t use any regularization methods because regularization is used in case of overfitting. I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skilltest. For more such skilltests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/
Architecture of Convolutional Neural Networks (CNNs) demystified,Learn everything about Analytics|Introduction|Table of Contents:|1. How does a machine look at an image?|2. How do we help a neural network to identify images  ?|So what did we do ?|3. Defining a Convolutional Neural Network|3. Putting it all together – How does the entire network look like ?|4. Using CNN to classify images in KERAS|Projects||End Notes," Case 1:|Case 2:|Case 3:|Case 4:|Case 5:|2.1 The Convolution Layer|The concept of stride and padding|Multiple filters and the activation map|2.2 The Pooling Layer|Output dimensions|2.3 The Output layer|Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Linear Regression [Solution: Skilltest – Linear Regression]|Big Data Architect- Mumbai (7+ Years of Experience)|
Dishashree Gupta
|59 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I will start with a confession – there was a time when I didn’t really understand deep learning. I would look at the research papers and articles on the topic and feel like it is a very complex topic. I tried understanding Neural networks and their various types, but it still looked difficult.Then one day, I decided to take one step at a time. I decided to start with basics and build on them. I decided that I will break down the steps applied in these techniques and do the steps (and calculations) manually, until I understand how they work. It was time taking and intense effort – but the results were phenomenal.Now, I can not only understand the spectrum of deep learning, I can visualize things and come up with better ways because my fundamentals are clear. It is one thing to apply neural networks mindlessly and it is other to understand what is going on and how are things happening at the back.Today, I am going to share this secret recipe with you. I will show you how I took the Convolutional Neural Networks and worked on them till I understood them. I will walk you through the journey so that you develop a deep understanding of how CNNs work.In this article I am going to discuss the architecture behind Convolutional Neural Networks, which are designed to address image recognition and classification problems.I am assuming that you have a basic understanding of how a neural network works. If you’re not sure of your understanding I would request you to go through this article before you read on.  Human brain is a very powerful machine. We see (capture) multiple images every second and process them without realizing how the processing is done. But, that is not the case with machines. The first step in image processing is to understand, how to represent an image so that the machine can read it?In simple terms, every image is an arrangement of dots (a pixel) arranged in a special order. If you change the order or color of a pixel, the image would change as well. Let us take an example. Let us say, you wanted to store and read an image with a number 4 written on it.The machine will basically break this image into a matrix of pixels and store the color code for each pixel at the representative location. In the representation below – number 1 is white and 256 is the darkest shade of green color (I have constrained the example to have only one color for simplicity).Once you have stored the images in this format, the next challenge is to have our neural network understand the arrangement and the pattern. A number is formed by having pixels arranged in a certain fashion.Let’s say we try to use a fully connected network to identify it? What does it do ?A fully connected network would take this image as an array by flattening it and considering pixel values as features to predict the number in image. Definitely it’s tough for the network to understand what’s happening underneath.It’s impossible even for a human to identify that this is a representation of number 4. We have lost the spatial arrangement of pixels completely.What can we possibly do? Let’s try to to extract features from the original image such that the spatial arrangement is preserved. Here we have used a weight to multiply the initial pixel values.It does get easier for the naked eye to identify that this is a 4. But again to send this image to a fully connected network, we would have to flatten it. We are unable to preserve the spatial arrangement of the image. Now we can see that flattening the image destroys its arrangement completely. we need to devise a way to send images to a network without flattening them and retaining its spatial arrangement. We need to send 2D/3D arrangement of pixel values.Let’s try taking two pixel values of the image at a time rather than taking just one. This would give the network a very good insight as to how does the adjacent pixel look like. Now that we’re taking two pixels at a time, we shall take two weight values too.I hope you noted that the image now became a 3 column arrangement from a 4 column arrangement initially. The image got smaller since we’re now moving two pixels at a time (pixels are getting shared in each movement). We made the image smaller and we can still understand that it’s a 4 to quite a great extent. Also, an important fact to realise is that we we’re taking two consecutive horizontal pixels, therefore only horizontal arrangement is considered here.This is one way to extract features from an image. We’re able to see the left and middle part well, however the right side is not so clear. This is because of the following two problems-Now we have two problems, we shall have two solutions to solve them as well. The problem encountered is that the left and right corners of the image is getting passed by the weight just once. What we need to do is we need the network to consider the corners also like other pixels.We have a simple solution to solve this. Put zeros along the sides of the weight movement.You can see that by adding the zeroes the information from the corners is retained. The size of the image is higher too. This can be used in cases where we don’t want the image size to reduce. The problem we’re trying to address here is that a smaller weight value in the right side corner is reducing the pixel value thereby making it tough for us to recognize. What we can do is, we take multiple weight values in a single turn and put them together.A weight value of (1,0.3) gave us an output of the formwhile a weight value of the form (0.1,5) would give us an output of the formA combined version of these two images would give us a very clear picture. Therefore what we did was simply use multiple weights rather than just one to retain more information about the image. The final output would be a combined version of the above two images. Till now we have used the weights which were trying to take horizontal pixels together. But in most cases we need to preserve the spatial arrangement in both horizontal and vertical direction. We can take the weight as a 2D matrix which takes pixels together in both horizontal and vertical direction. Also, keep in mind that since we have taken both horizontal and vertical movement of weights, the output is one pixel lower in both horizontal and vertical direction.Special thanks to Jeremy Howard for the inspiring me to create these visuals.What we did above was that we were trying to extract features from an image by using the spatial arrangement of the images. To understand an image its extremely important for a network to understand how the pixels are arranged. What we did above is what exactly a convolutional neural network does. We can take the input image, define a weight matrix and the input is convolved to extract specific features from the image without losing the information about its spatial arrangement.Another great benefit this approach has is that it reduces the number of parameters from the image. As you saw above the convolved images had lesser pixels as compared to the original image. This dramatically reduces the number of parameters we need to train for the network.We need three basic components to define a basic convolutional network.Let’s see each of these in a little more detail In this layer, what happens is exactly what we saw in case 5 above. Suppose we have an image of size 6*6. We define a weight matrix which extracts certain features from the imagesWe have initialized the weight as a 3*3 matrix. This weight shall now run across the image such that all the pixels are covered at least once, to give a convolved output. The value 429 above, is obtained by the adding the values obtained by element wise multiplication of the weight matrix and the highlighted 3*3 part of the input image.The 6*6 image is now converted into a 4*4 image.  Think of weight matrix like a paint brush painting a wall. The brush first paints the wall horizontally and then comes down and paints the next row horizontally. Pixel values are used again when the weight matrix moves along the image. This basically enables parameter sharing in a convolutional neural network.Let’s see how this looks like in a real image. The weight matrix behaves like a filter in an image extracting particular information from the original image matrix. A weight combination might be extracting edges, while another one might a particular color, while another one might just blur the unwanted noise.The weights are learnt such that the loss function is minimized similar to an MLP. Therefore weights are learnt to extract features from the original image which help the network in correct prediction. When we have multiple convolutional layers, the initial layer extract more generic features, while as the network gets deeper, the features extracted by the weight matrices are more and more complex and more suited to the problem at hand. As we saw above, the filter or the weight matrix, was moving across the entire image moving one pixel at a time. We can define it like a hyperparameter, as to how we would want the weight matrix to move across the image. If the weight matrix moves 1 pixel at a time, we call it as a stride of 1. Let’s see how a stride of 2 would look like.As you can see the size of image keeps on reducing as we increase the stride value. Padding the input image with zeros across it solves this problem for us. We can also add more than one layer of zeros around the image in case of higher stride values.We can see how the initial shape of the image is retained after we padded the image with a zero. This is known as same padding since the output image has the same size as the input. This is known as same padding (which means that we considered only the valid pixels of the input image). The middle 4*4 pixels would be the same. Here we have retained more information from the borders and have also preserved the size of the image. One thing to keep in mind is that the depth dimension of the weight would be same as the depth dimension of the input image. The weight extends to the entire depth of the input image. Therefore, convolution with a single weight matrix would result into a convolved output with a single depth dimension. In most cases instead of a single filter(weight matrix), we have multiple filters of the same dimensions applied together.The output from the each filter is stacked together forming the depth dimension of the convolved image. Suppose we have an input image of size 32*32*3. And we apply 10 filters of size 5*5*3 with valid padding. The output would have the dimensions as 28*28*10.You can visualize it as –This activation map is the output of the convolution layer. Sometimes when the images are too large, we would need to reduce the number of trainable parameters. It is then desired to periodically introduce pooling layers between subsequent convolution layers. Pooling is done for the sole purpose of reducing the spatial size of the image. Pooling is done independently on each depth dimension, therefore the depth of the image remains unchanged. The most common form of pooling layer generally applied is the max pooling.Here we have taken stride as 2, while pooling size also as 2. The max operation is applied to each depth dimension of the convolved output. As you can see, the 4*4 convolved output has become 2*2 after the max pooling operation.Let’s see how max pooling looks on a real image.As you can see I have taken convoluted image and have applied max pooling on it. The max pooled image still retains the information that it’s a car on a street. If you look carefully, the dimensions if the image have been halved. This helps to reduce the parameters to a great extent.Similarly other forms of pooling can also be applied like average pooling or the L2 norm pooling. It might be getting a little confusing for you to understand the input and output dimensions at the end of each convolution layer. I decided to take these few lines to make you capable of identifying the output dimensions. Three hyperparameter would control the size of output volume.We can apply a simple formula to calculate the output dimensions. The spatial size of the output image can be calculated as( [W-F+2P]/S)+1. Here, W is the input volume size, F is the size of the filter, P is the number of padding applied and S is the number of strides. Suppose we have an input image of size 32*32*3, we apply 10 filters of size 3*3*3, with single stride and no zero padding.Here W=32, F=3, P=0 and S=1. The output depth will be equal to the number of filters applied i.e. 10.The size of the output volume will be ([32-3+0]/1)+1 = 30. Therefore the output volume will be 30*30*10. After multiple layers of convolution and padding, we would need the output in the form of a class. The convolution and pooling layers would only be able to extract features and reduce the number of parameters from the  original images. However, to generate the final output we need to apply a fully connected layer to generate an output equal to the number of classes we need. It becomes tough to reach that number with just the convolution layers. Convolution layers generate 3D activation maps while we just need the output as whether or not an image belongs to a particular class. The output layer has a loss function like categorical cross-entropy, to compute the error in prediction. Once the forward pass is complete the backpropagation begins to update the weight and biases for error and loss reduction. CNN as you can now see is composed of various convolutional and pooling layers. Let’s see how the network looks like. Let’s try taking an example where we input several images of cats and dogs and we try to classify these images into their respective animal category. This is a classic problem of image recognition and classification. What the machine needs to do is it needs to see the image and understand by the various features as to whether its a cat or a dog.The features can be like extracting the edges, or extracting the whiskers of a cat etc. The convolutional layer would extract these features. Let’s take a hand on the data set.These are the examples of some of the images in the dataset.   we would first need to resize these images to get them all in the same shape. This is something we would generally need to do while handling images, since while capturing images, it would be impossible to capture all images of the same size.For simplicity of your understanding I have just used a single convolution layer and a single pooling layer, which generally doesn’t happen when we’re trying to make predictions. Dataset used can be downloaded from here.#import various packagesimport os
import numpy as np
import pandas as pd
import scipy
import sklearn
import keras
from keras.models import Sequential
import cv2
from skimage import io
%matplotlib inline#Defining the File Pathcat=os.listdir(""/mnt/hdd/datasets/dogs_cats/train/cat"")
dog=os.listdir(""/mnt/hdd/datasets/dogs_cats/train/dog"")
filepath=""/mnt/hdd/datasets/dogs_cats/train/cat/""
filepath2=""/mnt/hdd/datasets/dogs_cats/train/dog/""#Loading the Imagesimages=[]
label = []
for i in cat:
    image = scipy.misc.imread(filepath+i)
    images.append(image)
    label.append(0) #for cat imagesfor i in dog:
    image = scipy.misc.imread(filepath2+i)
    images.append(image)
    label.append(1) #for dog images#resizing all the imagesfor i in range(0,23000):
    images[i]=cv2.resize(images[i],(300,300))#converting images to arraysimages=np.array(images)
label=np.array(label)# Defining the hyperparametersfilters=10
filtersize=(5,5)epochs =5
batchsize=128input_shape=(300,300,3)#Converting the target variable to the required sizefrom keras.utils.np_utils import to_categorical
label = to_categorical(label)#Defining the modelmodel = Sequential()model.add(keras.layers.InputLayer(input_shape=input_shape))model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(1, 1), padding='valid', data_format=""channels_last"", activation='relu'))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Flatten())model.add(keras.layers.Dense(units=2, input_dim=50,activation='softmax'))model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(images, label, epochs=epochs, batch_size=batchsize,validation_split=0.3)model.summary()In this model, I have only used a single convolution and Pooling layer and the trainable parameters are 219,801. Wonder how many would I have had if i had used an MLP in this case. You can reduce the number of parameters by further by adding more convolution and pooling layers. The more convolution layers we add the features extracted would be more specific and intricate. Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems: I hope through this article I was able to provide you an intuition into convolutional neural networks. I did not go into the complex mathematics of CNN. In case you’re fond of understanding the same – stay tuned, there’s much more lined up for you. Try building your own CNN network to understand how it operates and makes predictions on images. Let me know your findings and approach using the comments section.",https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/
Big Data Architect- Mumbai (7+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Architecture of Convolutional Neural Networks (CNNs) demystified|Hands on with Deep Learning – Solution for Age Detection Practice Problem|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 7 – 15 years
Requirements : 
Task Info : Work on state-of-the art technologies and build things at a scale never seen before in India.This profile is a mix of:– implementing complex data processing flows to crunch data and produce insights– leading and supporting data scientists to create world class products– Bringing about significant innovation and solving complex problems in projects based on analytics-Evaluating impact of software performance, and recommending changes to software design team.Desired qualifications– MTech/MS in Computer Science.– Typically 7 or more years of experience executing on projects as a lead.– In depth knowledge of data science principles and best practices– Programming experience in Scala, Java or Python– Experience with Apache Spark platform.– Experience with big data technologies and databases like Redshift ,Elasticsearch, and Hadoop– Hands-on with Data Handling – data acquisition, data transformation, and data cleaning.– Expertise in Big Data with experience to handle and work with terabytes of data– Familiarity with modern machine learning methods for regression and classification.
College Preference : tier1-entire
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/big-data-architect-mumbai-7-years-of-experience/
Hands on with Deep Learning – Solution for Age Detection Practice Problem,Learn everything about Analytics|Introduction||Table of Contents|Why Participate in a Practice Problem (Deep Learning)?|What is the problem again?|Let’s solve the problem!|Intermission: Our first submission!|Let’s solve the problem! Part 2: Building better models||Intermission: Visual Inspection of our predictions|What’s Next|End Notes:,"Share this:|Like this:|Related Articles|Big Data Architect- Mumbai (7+ Years of Experience)|SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience)|
Faizan Shaikh
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"It is one thing to learn data science by reading or watching a video / MOOC and other to apply it on problems. You need to do both the things to learn the subject effectively. Today’s article is meant to help you apply deep learning on an interesting problem.If you are questioning, why learn or apply deep learning – you have most likely come out of a cave just now. Deep learning in already powering face detection in cameras, voice recognition on mobile devices to deep learning cars. Today, we will solve age detection problem using deep learning.If you are new to deep learning, I would recommend you to refer the articles below before going through this tutorial and making a submission. If you have learnt or read about Deep learning in last few days / months, and are looking for applying your new skill – practice problems are the right place to start. I say so because they provide you an experience of solving problems from scratch without making them too competitive.Here are the reasons why you should pick up a few practice problems: The first step to do when participating in a hackathon is to understand the problem. In this article, we will look at a recently published practice problem:  Age Detection of Indian Actors. You can view the problem statement on the hackathon page, but I will briefly mention it here.The task is to predict the age of a person from his or her facial attributes. For simplicity, the problem has been converted to a multi-class problem with classes as Young, Middle and Old.Seems easy at a first glance right?If you actually see the data, it seems hard even for a human! Lets check for ourselves! Here are some random good examples from our data.Middle Aged:Old:Young:But can you guess these?Apparently both are middle aged actors!To solve these problems, you need to have a streamlined approach. We will see this in the next section. Now that you know the problem, let us get started. I assume you have numpy, scipy, pandas, scikit-learn and keras installed. If you don’t, please install them. The articles above should help you.First things first; let us download the data and load it into our jupyter notebooks! If you haven’t already been introduced; here is the link to the practice problem (https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/).Before building a model, I urge you to solve this simple exercise:Can you write a script that will randomly load an image into jupyter notebook and print it? (PS: Don’t look at the answer below!). Post your code in this discuss thread.Here’s my approach to the exercise; As always, I first imported all the necessary modules,Then I loaded the csv files, so that it would be easier to locate the filesThen I wrote a script to randomly choose an image and printed it Here’s what I got:Age:  YOUNGThe motive of this exercise was that you can randomly view the dataset and check what problems you could possibly face when building the model.Here are a few of my hypotheses of what problems we could faceside viewfront viewFor now, let us focus on only one problem, viz how to handle variations in shape?We can do this by simply resizing the image. Let us load all the images and resize them into a single numpy arrayAnd similarly for test imagesWe can do one more thing that could help us build a better model; i.e. we can normalize our images. Normalizing the images will make our train faster.Now let’s take a look at our target variable. I have another exercise for you; What is the distribution of classes in our data? Could you say it is a highly imbalanced problem?Here’s my try for the exercise; On the basis of distribution of our data, we can create a simple submission. We see that most of the actors are middle aged. So we can say that all the actors in our test dataset are middle aged!Upload this file on the submission page to see the result! Before creating something substantial, let us bring our target variable in shape. We will convert our target into dummy columns so that it will be easier for our model to ingest it. Here’s how I would do it.Now comes the main part, building a model! As the problem is related to image processing, it is wiser to use neural networks to solve the problem. We will too build a simple feedforward neural network for this problem.First we should specify all the parameters we will be using in our networkThen we will import the necessary keras modulesAfter that, we will define our networkTo see how our model looks like; lets print itNow lets compile our network and let it train for a whileSeems like its training! But we still haven’t validated it. Validation is necessary if we want to ensure that our model will perform well on both the data it is training on and on a new testing dataLet’s tweak the code a little bit to cross validate it.The model seems to perform good for a first model. Lets submit the result.Here’s another simple exercise for you; Print the image along with the predictions of your trained model. Do this preferably on your training dataset so that you can check your predictions along with the real targetHere’s my take on the exercise, We have built a benchmark solution with a simple model at hand. What more can we do?Here are some tips I can suggest In this article, I have explained a simple benchmark solution for Age Detection Practice Problem. There’s many things you could do which even I haven’t mentioned in the article. You can suggest them in the comments below!",https://www.analyticsvidhya.com/blog/2017/06/hands-on-with-deep-learning-solution-for-age-detection-practice-problem/
SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hands on with Deep Learning – Solution for Age Detection Practice Problem|Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 7 years
Requirements : 
Task Info : Job Description and Responsibilities:– Very strong SAS, SQL, data modelling and transformation skills – with ability to manipulate large data sets, automate processes and debug code with minimal steer. -Strong passion for data exploration, manipulation, data quality and bench marking utilizing SAS or any other analytical tools -Become data and process experts across different clusters feeding information into the DBM models. -Excellent knowledge of data warehouse and MI environments – Knowledge discovery and presenting findings for range of stakeholders. This involves importing, cleaning, transforming, validating and bench marking or modelling data with the purpose of understanding or making conclusions from the data for decision making purposes – End-to-end project planning and delivery. Ability to work dynamically on multiple projects independently and strong stakeholder management while putting customers in the heart of everything we do – Validate, track, and monitor delivered projects. – Upkeep of BAU processes by ensuring regular review of DBM data preparation processes to identify areas of improvement, focusing on accuracy, robustness and controls – Produce robust documentation to ensure reliability of results Essential Skills : – A high level of analytical work experience in financial services strongly preferred. – Extensive knowledge of SAS, Rational Databases (Oracle, Teradata etc.), UNIX, Tivoli and other analytic toolsets. – Experience in consumer credit risk and/or finance analytics across customer lifecycle – Knowledge of financial services portfolios. Awareness of the economy, market and customer trends affecting the business. – Familiarity with analytical techniques and their value in business. – Strong communication and interpersonal skills. Preferred Skills : – Experience in working on Hadoop platform and tools such as Impala, Hive, Spark, Pig, etc. is a bonus – knowledge of Collections and Recoveries in customer life cycle – Experience with other analytical tools such as R, WPS and SPSS Knowledge of SAP Business Objects is desirable but not essential – Familiar modelling (PD, LGD and EaD) and Regulatory reporting – University degree in quantitative discipline is an advantage (e.g. Computer Science, Statistics, Operations Research, Economics and Engineering)
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/sasdata-manager-bfsi-delhincrgurugram-3-7-years-of-experience/
Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience)|Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 6 years
Requirements : 
Task Info : Job Description and Responsibilities:– Require professionals with retail analytics experience (2-6 years) preferably in Retail Domain would be ideal. – 2 to 3 years of banking experience. – Has worked in a model development in retail environment within acquisitions/existing customer management. – The role will support model development team for retail portfolio – Mathematical / statistical background. – Strong SAS analytics and programming skills – MS Office skills: Excel (Advance, including pivot tables, and macro), Word and PowerPoint (intermediate). – MI report development and assisting with other ad hoc reporting requirements – High-level attention to detail. – Ability to work independently with minimal supervision. Skill Matrix : – Proficiency in MS Excel – Proficiency in SAS and UNIX – Strong QA acumen – Excellent communication skills
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-retail-analytics-bangalore-2-7-years-of-experience/
Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience)|Course Review – PG Diploma in Data Analytics by UpGrad & IIIT-B|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 4 years
Requirements : 
Task Info : Job Description and Responsibilities:– Gather and document business requirements – Analyze and interpret data to identify trends, patterns and opportunities for the business and clients – Communicate analysis and interpretation to appropriate audience – Produce, publish and distribute scheduled and ad-hoc client and operational reports relating to the development and performance of products, processes and technologies Required Qualifications : Strong 4+ years U.S. healthcare domain knowledge. 3+ years SQL / PL SQL knowledge. Good knowledge of ETL concepts. Ability to identify sets and subsets of information across multiple joins or unions of tables Ability to effectively participate in multiple, concurrent projects Experience analyzing and drawing valuable conclusions from the data profiling results Preferred Qualifications : Experience with more than one relational database Knowledge of, or experience working with SAS Knowledge of trend analytics concepts Knowledge of data modeling concepts Required Qualifications : Bachelors degree in Business, Finance, Health Administration, related field or equivalent work experience 1+ years of experience in business/finance including analysis experience with a solid understanding of data storage structures 1+ years of experience with project methodology (requirements, design, development, test and implementation) 
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-healthcare-hyderabad-1-4-years-of-experience/
Course Review – PG Diploma in Data Analytics by UpGrad & IIIT-B,Learn everything about Analytics|Table of Contents|1. The Review process|2. My first impressions about the course|3. Partnership with IIIT-Bangalore|4. Application|5 The curriculum|6. Interaction with Stakeholders|My view – who is this course for?|7. Conclusion,"4.1 The application/selection process:|4.2 Applicant profile|5.1 Pre-requisites:|5.2 Course Outline:|5.3 Content of the course|5.4 Career Guidance sessions|6.1 My interaction with Faculty|6.2 Interaction with Industry experts|6.3 My interaction with students||Other details|Share this:|Like this:|Related Articles|Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience)|Machine Learning Lead- Bangalore (3 to 7 years of experience)|
Kunal Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The analytics training landscape in India is changing very rapidly and for the right reasons. There is a huge talent gap to solve for. The U.S. alone is expected to have a shortfall of 150,000 data scientists by 2018. The number for India would also be similar.People are inventing and discovering new ways to try and bridge this skill gap in the industry. As a result, not surprisingly, many new online and offline courses are emerging for people to upgrade their skills to match the industry demand.In the last few months, we received several queries regarding the PG Diploma in Data Analytics Program being offered by UpGrad and IIIT-Bangalore. The course has been running for more than a year now and I thought it would be good to review their material and publish my review to help people make the right career decision.  Like other reviews which we have done in the past, we interacted with various stakeholders across the chain to review the course. This includes Management, Faculty, past students and current students of the program. This time too, I reached out to the UpGrad team and asked them for access to the course material and they happily obliged.While the material is provided by UpGrad – all views expressed in the article are my own. When I first came across the UpGrad & IIIT-B offering, it looked like just another offering from a top-tier education institute in India, aimed at working professionals. I thought that, like most of the other competing programs out there – they would take 2 – 3 years to learn about the industry before they get the program right. Thankfully, I was wrong.When I interacted with the UpGrad team, a couple of things stood out compared to other institutes / courses I have reviewed in past: Since the program is run online, I wasn’t sure how the partnership with IIIT-Bangalore works. Hence, I spent some time understanding the model. IIIT-B and UpGrad collaborate on every aspect of the program including curriculum creation, program development, program delivery, career support etc. The learners receive a PG Diploma in Data Analytics from IIIT-B post successful completion.This is a unique programs launched by IIIT-B. The other programs that IIITB offers are full time, offline programs providing degrees such as M. Tech and PhD. This program, despite being online, has tried to keep most of the features of the university’s offline experience, such as a robust credit structure in place. I was happy that UpGrad was applying this selection criteria instead of accepting most of the applications they get. Hopefully, that should improve the experience of the people enrolled in the course. The PGDDA has two ongoing batches of 300 and 500 learners. The program boasts a healthy mix of individuals drawn from a variety of backgrounds and bringing in a wealth of prior experience. All the qualified learners are expected to go through a preparatory program which covers some mathematical – such as matrix, linear algebra – and technical prerequisites such as R, SQL, Tableau, Excel, Python. The 11 month PG Diploma program is a 36 credit program which comprises 7 courses (please check this on course website as it might change over time). The first 6 courses are:Course OutlineAll of these courses carry 4 credits each. The 7th course is a 3 month long capstone project, which carries 12 credits, and has been developed in collaboration with various companies/Industry professionals.This definitely looks comprehensive for an 11 month program – but let us look at the content a bit more closely. The interface of the UpGrad portal (LMS) looks good and intuitive to use. You can see a summary of your progress on the course screen. Additionally, there is a discussion portal (the participation in which is counted towards evaluation) and a browser based coding window. You can access past lectures and industry interactions here too.Discussion portalThe videos were of high quality and contained interesting story lines. The participant needs to watch the entire video before unlocking the exercises and moving ahead. I personally am not a big fan of this feature as I like to see the content selectively. But, I can understand why UpGrad decided to keep it this way to enforce the participation. There is a sharp focus on preparing the learners for jobs in the industry. The support provided to learners is on the following front: My interaction with Prof. Tricha from IIIT Bangalore gave me an inside view about the efforts which have gone in creating the course and some of the challenges which the team is working on. We also discussed how the assessment was different for this course compared to other courses. Interestingly, the assessment includes grading activities on the discussion portal. Here is the weightage of various activities in the course assessment: UpGrad has partnered with companies and senior individual contributors from the industry to create and deliver this program. They have experts from Uber, Fractal Analytics, Genpact, Viacom, Tata iQ etc. who work closely with them to develop the case studies. These experts are also involved in delivering lectures and providing industry perspective to the learners.Overall the industry engagement during the program would be around 40%. This can be experienced by learners in the form of recorded lectures, live sessions and even mentorship sessions. This link contains the list of faculty who are part of this program.During my interaction with industry faculty members, they told me that they are very excited about what UpGrad is creating and have high confidence in the team. They said that the willingness to try new things, really stands out. The students were overall happy about their learning and also the effort that was put in by the UpGrad team. They said that the team was highly approachable. The students mentioned that the course is fairly intensive and their schedule ends up being hectic – all for a good cause though.The students appreciated the connect with industry experts and the effort from UpGrad team. They also suggested that having a full time instructor with industry experience might further improve their learning.Manshul was working as a consultant at TCS when he decided to take a plunge into analytics after 9+ years of experience.Initially I was quite apprehensive about the program quality and given my 9-year experience, how would I keep pace with young people in my batch. But my experience has only been rewarding. From program content to networking to career support, this program offers an end-to-end solution. I got placed in Opera Solutions as Sr. Data Engineer through UpGrad IIIT-B career support.-Manshul Goel, Sr. Data Engineer, Opera Solutions. Sajal Roychowdhury was working with Amazon as a risk analyst and then transitioned to Shopclues as a Business Analyst. He enjoyed the peer to peer interaction during the program.It is brilliant to collaborate and learn with others even though it is an online program. IIIT-B is a good brand to have and the curriculum of the program is solid.-Sajal Roychowdhary, Business Analyst, Shopclues This course is the right fit for people with experience who want to understand / experience analytics, but cannot / do not want to take time out for attending an offline course. The UpGrad platform has been created after a lot of careful consideration, research and testing. The team is dedicated to improving the platform regularly.The content is of high quality and is delivered by experienced faculty. I can say very confidently that among all the courses coming up in India, the UpGrad & IIITB course has evolved the fastest and will continue to do so.While all the efforts are in the right direction, it will become clear in the next 6 months how the industry accepts those who have graduated from this course. I enjoyed talking to the UpGrad team and would thank them for providing me unrestricted access to their faculty / students. I loved their focus on the platform, the attitude of the team to get things done, quality of their content and the innovation they are bringing in.What would I want to see more? I would want them to bring on-board a full time specialist for the course. Also, the statistics about the placements would only be available in next 6 – 12 months. Disclosure: This sponsored post has been written by Analytics Vidhya on behalf of, and with inputs from UpGrad. All opinions expressed in this post are entirely those of Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2017/06/course-review-pg-diploma-in-data-analytics-by-upgrad-iiit-b/
Machine Learning Lead- Bangalore (3 to 7 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Course Review – PG Diploma in Data Analytics by UpGrad & IIIT-B|Big Data Engineer- Bangalore (2 to 3 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 7 years
Requirements : Target Companies: Google, Quest Global, Tata Elxsi, GE Digital, Robert Bosh Engineering Services, Fractal.
Task Info : 1. Min: 7 year of total experience with recent & relevant experience of 3 year on Machine Learning.2. Strong hands on experience on Python, Pyspark and R programming language3. Hands on experience in identifying the right algorithm / technique to solve a particular business problem4. Experience in building and scaling models for sensor/time series data5. Experience in building products6. Experience in building predictive maintenance algorithms7. Hackers mind-set / Quick learner8. Should be able to guide a team of developers
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, Pyspark, python, r, time series
Location : Bangalore
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/machine-learning-lead-bangalore-3-to-7-years-of-experience/
Big Data Engineer- Bangalore (2 to 3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Lead- Bangalore (3 to 7 years of experience)|Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 3 years
Requirements : Should have prior customer facing experience.
Ability to lead all requirement gathering sessions with the Customer
Strong co-ordination and interpersonal skills to handle complex projects.
Task Info : Relevant Experience (In Years) :1. Minimum 2 years of strong experience on Core Java, Hadoop ecosystem and any NoSQL Database.2. Minimum 1.5 or 2 Years of strong experience on Spark/Storm/Cassandr/Kafka/Scala.Technical/Functional Skills :1. Core Java, Multi-Threading, OOPS, Writing Parsers2. Hadoop/Hive/Pig/MapReduce3. Spark/Storm/Kafka/Scala/Cassandra4. SQL5. Cloud Computing(AWS/Azure etc)Roles & Responsibilities:1. Strong on Core Java, Multi-Threading, OOPS Concept, writing parsers in Core Java2. Should have strong knowledge on Hadoop ecosystem such as Hive/Pig/MapReduce3. Strong in SQL, NoSQL, RDBMS and Data warehousing concepts4. Writing complex MapReduce programs5. Should have strong experience on pipeline building such Spark or Storm or Cassandra or Scala.6. Designing efficient and robust ETL workflows7. Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).8. Tuning Hadoop solutions to improve performance and end-user experience;9. Processing unstructured data into a form suitable for analysis – and then do the analysis.10. Creating Big Data reference architecture deliverable11. Performance optimization in a Big Data environmentGeneric Leadership Skills:1. Should have prior customer facing experience.2. Ability to lead all requirement gathering sessions with the Customer3. Strong co-ordination and interpersonal skills to handle complex projects.
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, Cloud Computing, Data Warehouse, etl, hadoop, hive, java, mapreduce, pig, RDBMS, Scala, spark, sql, storm
Location : Bangalore
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/big-data-engineer-bangalore-2-to-3-years-of-experience/
Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Engineer- Bangalore (2 to 3 years of experience)|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 4 years
Requirements : 
Task Info : Job Description and Responsibilities:A very strong statistical modeller with expertise in building statistical models from scratch, expertise in R, Python, Matlab, C++ and comfortable working with large data sets. Manage clients expectations and manage advanced analytics delivery. Work with client to understand the business problem and convert the same into analytical problem Hands on involvement in extracting, collating, performing data integrity checks, manipulating and analysing data Hands on involvement in suggesting and building an appropriate statistical model relevant to the industry vertical Develop analytical strategies for business using model results of predictive modelling, time series forecasting, clustering and segmentation of customers Documenting and presenting work to the client. Proactively recommending and influencing changes in business decision-making. Excellent verbal and written communication and form liason between business and technical architects and developers, Strong inclination towards consulting engagements involving machine learning and artificial intelligence. To work in agile methodology for project delivery Good foundation in data structures, software design, algorithms and natural language processing Required skills: B.Tech., Dual Degree M.Tech., or M.Sc. (Integrated) in Computer Science, Mathematics, Statistics, or similar quantitative disciplines from the Indian Institutes of Technology (IIT) or Indian Statistical Institute (ISI). Demonstrated ability to conduct independent research utilizing large datasets At least 1 year of experience in programming, quantitative analysis or as a data scientist Strong programming skills in one of the following: Python, C++, C# or Java
College Preference : tier1-any
Min Qualification : ug
Skills : c++, matlab, predictive modeling, python, r, statistical modeling
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-iitisi-mumbai-1-4-years-of-experience/
Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience)|Senior Manager/AVP – Analytics – (BFSI)- Chennai (9-14 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 7 years
Requirements : 
Task Info : Job Description and Responsibilities:– 3-7 yrs. exp. in solving optimization problems using statistical, operations research, design of experiments approach. – Adept with applying Machine Learning and Data Analytics concepts for mostly structured data. – Expertise in building regression models, linear programming, mixed models, logistics models. – Programming exp. in R/ Python/ JAVA. – Individual Contributor role.Education: PhD in engg/statistics/economics/mathematics or any quantitative field. 
College Preference : no-bar
Min Qualification : pg
Skills : java, linear regression, logistic, machine learning, python, r, regression, statistical techniques, structured thinking
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-optimization-bangalore-3-7-years-of-experience/
Senior Manager/AVP – Analytics – (BFSI)- Chennai (9-14 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 9 – 14 years
Requirements : 
Task Info : Job Description and Responsibilities:– To understand current critical MI reports & their requirements; develop and deliver such reports in Tableau BI tool – To ensure project milestones are met as per agreed book of work. – Identify and drive process enhancements to ensure efficient and seamless delivery. – Ensure that the standards of the process consistently meet or exceed the requirements set under the Service Level Agreements (SLA) agreed with the in-country team, identifying and implementing innovative opportunities to add value to the deliverables and improving the quality of deliverables. – Upholding the Values of the Group and Company at all times – Compliance with all applicable Rules/ Regulations and Company and Group Policies; and – Periodic review key controls and ensure compliance with operational risk policy framework QUALIFICATION & SKILLS: – Highly motivated individuals with a strong track record of achievement, especially in Finance area. – A team player and enjoys interacting with people of all levels in a multicultural environment. – Able to creatively apply analytical solutions to issues and pain areas. – Ability to manage pressure of tight deadlines and deliver high quality output. – Ability to challenge status quo. Key Skills / Knowledge – Good knowledge about the Tableau BI tool is mandatory. – Experience in working with reporting tools such as Pipeline, Hyperion Planning, Cube, RMI or PMI would be useful – Knowledge about the reporting process would be useful – Any experience with Tableau application is an added advantage Education : MBA / CA
College Preference : no-bar
Min Qualification : ug
Skills : analytics, banking, bfsi, business intelligence, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-manageravp-analytics-bfsi-chennai-9-14-years-of-experience-2/
Senior Manager/AVP – Analytics – (BFSI)- Chennai (9-14 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 9 – 14 years
Requirements : 
Task Info : Job Description and Responsibilities:– To understand current critical MI reports & their requirements; develop and deliver such reports in Tableau BI tool – To ensure project milestones are met as per agreed book of work. – Identify and drive process enhancements to ensure efficient and seamless delivery. – Ensure that the standards of the process consistently meet or exceed the requirements set under the Service Level Agreements (SLA) agreed with the in-country team, identifying and implementing innovative opportunities to add value to the deliverables and improving the quality of deliverables. – Upholding the Values of the Group and Company at all times – Compliance with all applicable Rules/ Regulations and Company and Group Policies; and – Periodic review key controls and ensure compliance with operational risk policy framework QUALIFICATION & SKILLS: – Highly motivated individuals with a strong track record of achievement, especially in Finance area. – A team player and enjoys interacting with people of all levels in a multicultural environment. – Able to creatively apply analytical solutions to issues and pain areas. – Ability to manage pressure of tight deadlines and deliver high quality output. – Ability to challenge status quo. Key Skills / Knowledge – Good knowledge about the Tableau BI tool is mandatory. – Experience in working with reporting tools such as Pipeline, Hyperion Planning, Cube, RMI or PMI would be useful – Knowledge about the reporting process would be useful – Any experience with Tableau application is an added advantage Education : MBA / CA
College Preference : no-bar
Min Qualification : ug
Skills : analytics, banking, bfsi, business intelligence, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-manageravp-analytics-bfsi-chennai-9-14-years-of-experience/
"A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R",Learn everything about Analytics|Introduction|A small exercise to get your mind racing|Table of Contents|1. Simple models for Prediction|2. Linear Regression|3. The Line of Best Fit|4. Gradient Descent|5. Using Linear Regression for Prediction|6. Evaluating your Model – R square and adjusted R- square|7. Using all the features for prediction|8. Polynomial Regression|9. Bias and Variance in regression models|10. Regularization|11. Ridge Regression|12. Lasso regression|13. Elastic Net Regression|Implementation in R|||14. Types of Regularization Techniques [Optional]|End Notes,"Model 1 – Mean sales:||Model 2 – Average Sales by Location:|Model 3 – Enter Linear Regression:||Model 4 – Linear regression with more variables|Adjusted R-square|Data pre-processing steps for regression model|Building the model|Selecting the right features for your model|Interpretation of Regression Plots|Important Points:|Important Points:|Share this:|Like this:|Related Articles|Senior Manager/AVP – Analytics – (BFSI)- Chennai (9-14 Years Of Experience)|How to create animated GIF images for data visualization using gganimate (in R)?|
Shubham Jain
|68 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I was talking to one of my friends who happen to be an operations manager at one of the Supermarket chains in India. Over our discussion, we started talking about the amount of preparation the store chain needs to do before the Indian festive season (Diwali) kicks in.He told me how critical it is for them to estimate/predict which product will sell like hotcakes and which would not prior to the purchase. A bad decision can leave your customers to look for offers and products in the competitor stores. The challenge does not finish there – you need to estimate the sales of products across a range of different categories for stores in varied locations and with consumers having different consumption techniques.While my friend was describing the challenge, the data scientist in me started smiling! Why? I just figured out a potential topic for my next article. In today’s article, I will tell you everything you need to know about regression models and how they can be used to solve prediction problems like the one mentioned above. Take a moment to list down all those factors you can think, on which the sales of a store will be dependent on. For each factor create an hypothesis about why and how that factor would influence the sales of various products. For example – I expect the sales of products to depend on the location of the store, because the local residents in each area would have different lifestyle. The amount of bread a store will sell in Ahmedabad would be a fraction of similar store in Mumbai.Similarly list down all possible factors you can think of.Location of your shop, availability of the products, size of the shop, offers on the product, advertising done by a product, placement in the store could be some features on which your sales would depend on.How many factors were you able to think of? If it is less than 15, give it more time and think again! A seasoned data scientist working on this problem would possibly think of tens and hundreds of such factors.With that thought in mind, I am providing you with one such data set – The Big Mart Sales. In the data set, we have product wise Sales for Multiple outlets of a chain.Let’s us take a snapshot of the dataset:In the dataset, we can see characteristics of the sold item (fat content, visibility, type, price) and some characteristics of the outlet (year of establishment, size, location, type) and the number of the items sold for that particular item. Let’s see if we can predict sales using these features.  Let us start with making predictions using a few simple ways to start with. If I were to ask you, what could be the simplest way to predict the sales of an item, what would you say? Even without any knowledge of machine learning, you can say that if you have to predict sales for an item – it would be the average over last few days . / months / weeks.It is a good thought to start, but it also raises a question – how good is that model?Turns out that there are various ways in which we can evaluate how good is our model. The most common way is Mean Squared Error. Let us understand how to measure it. Prediction errorTo evaluate how good is a model, let us understand the impact of wrong predictions. If we predict sales to be higher than what they might be, the store will spend a lot of money making unnecessary arrangement which would lead to excess inventory. On the other side if I predict it too low, I will lose out on sales opportunity.So, the simplest way of calculating error will be, to calculate the difference in the predicted and actual values. However, if we simply add them, they might cancel out, so we square these errors before adding. We also divide them by the number of data points to calculate a mean error since it should not be dependent on number of data points.[each error squared and divided by number of data points]
This is known as the mean squared error.Here e1, e2 …. , en are the difference between the actual and the predicted values. So, in our first model what would be the mean squared error? On predicting the mean for all the data points, we get a mean squared error = 29,11,799. Looks like huge error. May be its not so cool to simply predict the average value.Let’s see if we can think of something to reduce the error.We know that location plays a vital role in the sales of an item. For example, let us say, sales of car would be much higher in Delhi than its sales in Varanasi. Therefore let us use the data of the column ‘Outlet_Location_Type’.So basically, let us calculate the average sales for each location type and predict accordingly.On predicting the same, we get mse = 28,75,386, which is less than our previous case. So we can notice that by using a characteristic[location], we have reduced the error.Now, what if there are multiple features on which the sales would depend on. How would we predict sales using this information? Linear regression comes to our rescue. Linear regression is the simplest and most widely used statistical technique for predictive modeling. It basically gives us an equation, where we have our features as independent variables, on which our target variable [sales in our case] is dependent upon.So what does the equation look like? Linear regression equation looks like this:Here, we have Y as our dependent variable (Sales), X’s are the independent variables and all thetas are the coefficients. Coefficients are basically the weights assigned to the features, based on their importance. For example, if we believe that sales of an item would have higher dependency upon the type of location as compared to size of store, it means that sales in a tier 1 city would be more even if it is a smaller outlet than a tier 3 city in a bigger outlet. Therefore, coefficient of location type would be more than that of store size.So, firstly let us try to understand linear regression with only one feature, i.e., only one independent variable. Therefore our equation becomes,This equation is called a simple linear regression equation, which represents a straight line, where ‘Θ0’ is the intercept, ‘Θ1’ is the slope of the line. Take a look at the plot below between sales and MRP.Surprisingly, we can see that sales of a product increases with increase in its MRP. Therefore the dotted red line represents our regression line or the line of best fit. But one question that arises is how you would find out this line? As you can see below there can be so many lines which can be used to estimate Sales according to their MRP. So how would you choose the best fit line or the regression line?The main purpose of the best fit line is that our predicted values should be closer to our actual or the observed values, because there is no point in predicting values which are far away from the real values. In other words, we tend to minimize the difference between the values predicted by us and the observed values, and which is actually termed as error. Graphical representation of error is as shown below. These errors are also called as residuals. The residuals are indicated by the vertical lines showing the difference between the predicted and actual value.Okay, now we know that our main objective is to find out the error and minimize it. But before that, let’s think of how to deal with the first part, that is, to calculate the error. We already know that error is the difference between the value predicted by us and the observed value. Let’s just consider three ways through which we can calculate error:Therefore, sum of squares of these residuals is denoted by:where, h(x) is the value predicted by us,  h(x) =Θ1*x +Θ0 , y is the actual values and m is the number of rows in the training set. The cost FunctionSo let’s say, you increased the size of a particular shop, where you predicted that the sales would be higher. But despite increasing the size, the sales in that shop did not increase that much. So the cost applied in increasing the size of the shop, gave you negative results.So, we need to minimize these costs. Therefore we introduce a cost function, which is basically used to define and measure the error of the model.If you look at this equation carefully, it is just similar to sum of squared errors, with just a factor of 1/2m is multiplied in order to ease mathematics.So in order to improve our prediction, we need to minimize the cost function. For this purpose we use the gradient descent algorithm. So let us understand how it works. Let us consider an example, we need to find the minimum value of this equation,Y= 5x + 4x^2. In mathematics, we simple take the derivative of this equation with respect to x, simply equate it to zero. This gives us the point where this equation is minimum. Therefore substituting that value can give us the minimum value of that equation.Gradient descent works in a similar manner. It iteratively updates Θ, to find a point where the cost function would be minimum. If you wish to study gradient descent in depth, I would highly recommend going through this article. Now let us consider using Linear Regression to predict Sales for our big mart sales problem.From the previous case, we know that by using the right features would improve our accuracy. So now let us use two features, MRP and the store establishment year to estimate sales.Now, let us built a linear regression model in python considering only these two features.# importing basic librariesimport numpy as np import pandas as pdfrom pandas import Series, DataFramefrom sklearn.model_selection import train_test_splitimport test and train filetrain = pd.read_csv('Train.csv')test = pd.read_csv('test.csv')# importing linear regressionfrom sklearnfrom sklearn.linear_model import LinearRegressionlreg = LinearRegression()splitting into training and cv for cross validationX = train.loc[:,['Outlet_Establishment_Year','Item_MRP']]x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales)training the modellreg.fit(x_train,y_train)predicting on cvpred = lreg.predict(x_cv)calculating msemse = np.mean((pred - y_cv)**2)In this case, we got mse = 19,10,586.53, which is much smaller than our model 2. Therefore predicting with the help of two features is much more accurate.Let us take a look at the coefficients of this linear regression model.# calculating coefficients coeff = DataFrame(x_train.columns) coeff['Coefficient Estimate'] = Series(lreg.coef_)coeffTherefore, we can see that MRP has a high coefficient, meaning items having higher prices have better sales. How accurate do you think the model is? Do we have any evaluation metric, so that we can check this? Actually we have a quantity, known as R-Square.R-Square: It determines how much of the total variation in Y (dependent variable) is explained by the variation in X (independent variable). Mathematically, it can be written as:The value of R-square is always between 0 and 1, where 0 means that the model does not model explain any variability in the target variable (Y) and 1 meaning it explains full variability in the target variable.Now let us check the r-square for the above model.lreg.score(x_cv,y_cv) 0.3287In this case, R² is 32%, meaning, only 32% of variance in sales is explained by year of establishment and MRP. In other words, if you know year of establishment and the MRP, you’ll have 32% information to make an accurate prediction about its sales.Now what would happen if I introduce one more feature in my model, will my model predict values more closely to its actual value? Will the value of R-Square increase?Let us consider another case.We learnt, by using two variables rather than one, we improved the ability to make accurate predictions about the item sales.So, let us introduce another feature ‘weight’ in case 3. Now let’s build a regression model with these three features.X = train.loc[:,['Outlet_Establishment_Year','Item_MRP','Item_Weight']]splitting into training and cv for cross validationx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales) ## training the modellreg.fit(x_train,y_train)ValueError: Input contains NaN, infinity or a value too large for dtype(‘float64’).It produces an error, because item weights column have some missing values. So let us impute it with the mean of other non-null entries.train['Item_Weight'].fillna((train['Item_Weight'].mean()), inplace=True)Let us try to run the model again.training the model lreg.fit(x_train,y_train) ## splitting into training and cv for cross validationx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales) ## training the model lreg.fit(x_train,y_train)predicting on cv pred = lreg.predict(x_cv)calculating msemse = np.mean((pred - y_cv)**2)mse1853431.59 ## calculating coefficientscoeff = DataFrame(x_train.columns)coeff['Coefficient Estimate'] = Series(lreg.coef_)calculating r-squarelreg.score(x_cv,y_cv) 0.32942Therefore we can see that the mse is further reduced. There is an increase in the value R-square, does it mean that the addition of item weight is useful for our model? The only drawback of R2 is that if new predictors (X) are added to our model, R2 only increases or remains constant but it never decreases. We can not judge that by increasing complexity of our model, are we making it more accurate?That is why, we use “Adjusted R-Square”.The Adjusted R-Square is the modified form of R-Square that has been adjusted for the number of predictors in the model. It incorporates model’s degree of freedom. The adjusted R-Square only increases if the new term improves the model accuracy.whereR2 = Sample R squarep = Number of predictorsN = total sample sizeNow let us built a model containing all the features. While building the regression models, I have only used continuous features. This is because we need to treat categorical variables differently before they can used in linear regression model. There are different techniques to treat them, here I have used one hot encoding(convert each class of a categorical variable as a feature). Other than that I have also imputed the missing values for outlet size.# imputing missing valuestrain['Item_Visibility'] = train['Item_Visibility'].replace(0,np.mean(train['Item_Visibility']))train['Outlet_Establishment_Year'] = 2013 - train['Outlet_Establishment_Year']train['Outlet_Size'].fillna('Small',inplace=True) # creating dummy variables to convert categorical into numeric valuesmylist = list(train1.select_dtypes(include=['object']).columns)dummies = pd.get_dummies(train[mylist], prefix= mylist)train.drop(mylist, axis=1, inplace = True) X = pd.concat([train,dummies], axis =1 )import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib.pyplot as plt%matplotlib inlinetrain = pd.read_csv('training.csv')test = pd.read_csv('testing.csv')# importing linear regression from sklearn from sklearn.linear_model import LinearRegressionlreg = LinearRegression()# for cross validationfrom sklearn.model_selection import train_test_splitX = train.drop('Item_Outlet_Sales',1)x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales, test_size =0.3)# training a linear regression model on trainlreg.fit(x_train,y_train) # predicting on cvpred_cv = lreg.predict(x_cv)# calculating msemse = np.mean((pred_cv - y_cv)**2)mse1348171.96# evaluation using r-squarelreg.score(x_cv,y_cv)0.54831541460870059Clearly, we can see that there is a great improvement in both mse and R-square, which means that our model now is able to predict much closer values to the actual values.When we have a high dimensional data set, it would be highly inefficient to use all the variables since some of them might be imparting redundant information. We would need to select the right set of variables which give us an accurate model as well as are able to explain the dependent variable well. There are multiple ways to select the right set of variables for the model. First among them would be the business understanding and domain knowledge. For instance while predicting sales we know that marketing efforts should impact positively towards sales and is an important feature in your model. We should also take care that the variables we’re selecting should not be correlated among themselves.Instead of manually selecting the variables, we can automate this process by using forward or backward selection. Forward selection starts with most significant predictor in the model and adds variable for each step. Backward elimination starts with all predictors in the model and removes the least significant variable for each step. Selecting criteria can be set to any statistical measure like R-square, t-stat etc.Take a look at the residual vs fitted values plot.residual plotx_plot = plt.scatter(pred_cv, (pred_cv - y_cv), c='b')plt.hlines(y=0, xmin= -1000, xmax=5000)plt.title('Residual plot')We can see a funnel like shape in the plot. This shape indicates Heteroskedasticity. The presence of non-constant variance in the error terms results in heteroskedasticity. We can clearly see that the variance of error terms(residuals) is not constant. Generally, non-constant variance arises in presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.We can easily check this by looking at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern as shown above. This indicates signs of non linearity in the data which has not been captured by the model. I would highly recommend going through this article for a detailed understanding of assumptions and interpretation of regression plots.In order to capture this non-linear effects, we have another type of regression known as polynomial regression. So let us now understand it. Polynomial regression is another form of regression in which the maximum power of the independent variable is more than 1. In this regression technique, the best fit line is not a straight line instead it is in the form of a curve.Quadratic regression, or regression with second order polynomial, is given by the following equation:Y =Θ1 +Θ2*x +Θ3*x2Now take a look at the plot given below. Clearly the quadratic equation fits the data better than simple linear equation. In this case, what do you think will the R-square value of quadratic regression greater than simple linear regression? Definitely yes, because quadratic regression fits the data better than linear regression. While quadratic and cubic polynomials are common, but you can also add higher degree polynomials.Below figure shows the behavior of a polynomial equation of degree 6.So do you think it’s always better to use higher order polynomials to fit the data set. Sadly, no. Basically, we have created a model that fits our training data well but fails to estimate the real relationship among variables beyond the training set. Therefore our model performs poorly on the test data. This problem is called as over-fitting. We also say that the model has high variance and low bias.Similarly, we have another problem called underfitting, it occurs when our model neither fits the training data nor generalizes on the new data.Our model is underfit when we have high bias and low variance. What does that bias and variance actually mean? Let us understand this by an example of archery targets.Let’s say we have model which is very accurate, therefore the error of our model will be low, meaning a low bias and low variance as shown in first figure. All the data points fit within the bulls-eye. Similarly we can say that if the variance increases, the spread of our data point increases which results in less accurate prediction. And as the bias increases the error between our predicted value and the observed values increases.Now how this bias and variance is balanced to have a perfect model? Take a look at the image below and try to understand.As we add more and more parameters to our model, its complexity increases, which results in increasing variance and decreasing bias, i.e., overfitting. So we need to find out one optimum point in our model where the decrease in bias is equal to increase in variance. In practice, there is no analytical way to find this point. So how to deal with high variance or high bias?To overcome underfitting or high bias, we can basically add new parameters to our model so that the model complexity increases, and thus reducing high bias.Now, how can we overcome Overfitting for a regression model?Basically there are two methods to overcome overfitting,Here we would be discussing about Regularization in detail and how to use it to make your model more generalized. You have your model ready, you have predicted your output. So why do you need to study regularization? Is it necessary?Suppose you have taken part in a competition, and in that problem you need to predict a continuous variable. So you applied linear regression and predicted your output. Voila! You are on the leaderboard. But wait what you see is still there are many people above you on the leaderboard. But you did everything right then how is it possible?“Everything should be made simple as possible, but not simpler – Albert Einstein”What we did was simpler, everybody else did that, now let us look at making it simple. That is why, we will try to optimize our code with the help of regularization.In regularization, what we do is normally we keep the same number of features, but reduce the magnitude of the coefficients j. How does reducing the coefficients will help us?Let us take a look at the coefficients of feature in our above regression model.checking the magnitude of coefficientspredictors = x_train.columns coef = Series(lreg.coef_,predictors).sort_values()coef.plot(kind='bar', title='Modal Coefficients') We can see that coefficients of Outlet_Identifier_OUT027 and Outlet_Type_Supermarket_Type3(last 2) is much higher as compared to rest of the coefficients. Therefore the total sales of an item would be more driven by these two features.How can we reduce the magnitude of coefficients in our model? For this purpose, we have different types of regression techniques which uses regularization to overcome this problem. So let us discuss them. Let us first implement it on our above problem and check our results that whether it performs better than our linear regression model.from sklearn.linear_model import Ridge ## training the modelridgeReg = Ridge(alpha=0.05, normalize=True)ridgeReg.fit(x_train,y_train)pred = ridgeReg.predict(x_cv)calculating msemse = np.mean((pred_cv - y_cv)**2)mse 1348171.96 ## calculating score ridgeReg.score(x_cv,y_cv) 0.5691So, we can see that there is a slight improvement in our model because the value of the R-Square has been increased. Note that value of alpha, which is hyperparameter of Ridge, which means that they are not automatically learned by the model instead they have to be set manually.Here we have consider alpha = 0.05. But let us consider different values of alpha and plot the coefficients for each case.   You can see that, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero.But if you calculate R-square for each alpha, we will see that the value of R-square will be maximum at alpha=0.05. So we have to choose it wisely by iterating it through a range of values and using the one which gives us lowest error.So, now you have an idea how to implement it but let us take a look at the mathematics side also. Till now our idea was to basically minimize the cost function, such that values predicted are much closer to the desired result.Now take a look back again at the cost function for ridge regression.Here if you notice, we come across an extra term, which is known as the penalty term. λ given here, is actually denoted by alpha parameter in the ridge function. So by changing the values of alpha, we are basically controlling the penalty term. Higher the values of alpha, bigger is the penalty and therefore the magnitude of coefficients are reduced.Now let us consider another type of regression technique which also makes use of regularization. LASSO (Least Absolute Shrinkage Selector Operator), is quite similar to ridge, but lets understand the difference them by implementing it in our big mart problem.from sklearn.linear_model import LassolassoReg = Lasso(alpha=0.3, normalize=True)lassoReg.fit(x_train,y_train)pred = lassoReg.predict(x_cv) # calculating msemse = np.mean((pred_cv - y_cv)**2) mse1346205.82lassoReg.score(x_cv,y_cv)0.5720As we can see that, both the mse and the value of R-square for our model has been increased. Therefore, lasso model is predicting better than both linear and ridge.Again lets change the value of alpha and see how does it affect the coefficients.So, we can see that even at small values of alpha, the magnitude of coefficients have reduced a lot. By looking at the plots, can you figure a difference between ridge and lasso?We can see that as we increased the value of alpha, coefficients were approaching towards zero, but if you see in case of lasso, even at smaller alpha’s, our coefficients are reducing to absolute zeroes. Therefore, lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge.Mathematics behind lasso regression is quiet similar to that of ridge only difference being instead of adding squares of theta, we will add absolute value of Θ.Here too, λ is the hypermeter, whose value is equal to the alpha in the Lasso function.  Now that you have a basic understanding of ridge and lasso regression, let’s think of an example where we have a large dataset, lets say it has 10,000 features. And we know that some of the independent features are correlated with other independent features. Then think, which regression would you use, Rigde or Lasso?Let’s discuss it one by one. If we apply ridge regression to it, it will retain all of the features but will shrink the coefficients. But the problem is that model will still remain complex as there are 10,000 features, thus may lead to poor model performance.Instead of ridge what if we apply lasso regression to this problem. The main problem with lasso regression is when we have correlated variables, it retains only one variable and sets other correlated variables to zero. That will possibly lead to some loss of information resulting in lower accuracy in our model.Then what is the solution for this problem? Actually we have another type of regression, known as elastic net regression, which is basically a hybrid of ridge and lasso regression. So let’s try to understand it.  Before going into the theory part, let us implement this too in big mart sales problem. Will it perform better than ridge and lasso? Let’s check!from sklearn.linear_model import ElasticNetENreg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)ENreg.fit(x_train,y_train)pred_cv = ENreg.predict(x_cv) #calculating msemse = np.mean((pred_cv - y_cv)**2)mse 1773750.73ENreg.score(x_cv,y_cv)0.4504So we get the value of R-Square, which is very less than both ridge and lasso. Can you think why? The reason behind this downfall is basically we didn’t have a large set of features. Elastic regression generally works well when we have a big dataset.Note, here we had two parameters alpha and l1_ratio. First let’s discuss, what happens in elastic net, and how it is different from ridge and lasso.Elastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters. So it uses both L1 and L2 penality term, therefore its equation look like as follows:So how do we adjust the lambdas in order to control the L1 and L2 penalty term? Let us understand by an example. You are trying to catch a fish from a pond. And you only have a net, then what would you do? Will you randomly throw your net? No, you will actually wait until you see one fish swimming around, then you would throw the net in that direction to basically collect the entire group of fishes. Therefore even if they are correlated, we still want to look at their entire group.Elastic regression works in a similar way. Let’ say, we have a bunch of correlated independent variables in a dataset, then elastic net will simply form a group consisting of these correlated variables. Now if any one of the variable of this group is a strong predictor (meaning having a strong relationship with dependent variable), then we will include the entire group in the model building, because omitting other variables (like what we did in lasso) might result in losing some information in terms of interpretation ability, leading to a poor model performance.So, if you look at the code above, we need to define alpha and l1_ratio while defining the model. Alpha and l1_ratio are the parameters which you can set accordingly if you wish to control the L1 and L2 penalty separately. Actually, we haveAlpha = a + b           and     l1_ratio =  a / (a+b)where, a and b weights assigned to L1 and L2 term respectively. So when we change the values of alpha and l1_ratio, a and b are set aaccordingly such that they control trade off between L1 and L2 as:a * (L1 term) + b* (L2 term)Let alpha (or a+b) = 1, and now consider the following cases:So let us adjust alpha and l1_ratio, and try to understand from the plots of coefficient given below.  Now, you have basic understanding about ridge, lasso and elasticnet regression. But during this, we came across two terms L1 and L2, which are basically two types of regularization. To sum up basically lasso and ridge are the direct application of L1 and L2 regularization respectively.But if you still want to know, below I have explained the concept behind them, which is OPTIONAL but before that let us see the same implementation of above codes in R.  Step 1: Linear regression with two variables “Item MRP” and “Item Establishment Year”. OutputAlso, the value of r square is 0.3354391 and the MSE is 20,28,538. Step 2: Linear regression with three variables “Item MRP”, “Item Establishment Year”, “Item Weight”. OutputAlso, the value of r square is 0.3354657 and the MSE is 20,28,692. Step 3: Linear regression with all variables. OutputAlso, the value of r square is 0.3354657 and the MSE is 14,38,692. Step 4: Implementation of Ridge regression OutputStep 5: Implementation of lasso regression OutputFor better understanding and more clarity on all the three types of regression, you can refer to this Free Course: Big Mart Sales In R.Let’s recall, both in ridge and lasso we added a penalty term, but the term was different in both cases. In ridge, we used the squares of theta while in lasso we used absolute value of theta. So why these two only, can’t there be other possibilities?Actually, there are different possible choices of regularization with different choices of order of the parameter in the regularization term, which is denoted by . This is more generally known as Lp regularizer.Let us try to visualize some by plotting them. For making visualization easy, let us plot them in 2D space. For that we suppose that we just have two parameters. Now, let’s say if p=1, we have term as  . Can’t we plot this equation of line? Similarly plot for different values of p are given below.In the above plots, axis denote the parameters(Θ1 and Θ2). Let us examine them one by one.For p=0.5, we can only get large values of one parameter only if other parameter is too small. For p=1, we get sum of absolute values where the increase in one parameter Θ is exactly offset by the decrease in other. For p =2, we get a circle and for larger p values, it approaches a round square shape.The two most commonly used regularization are in which we have p=1 and p=2, more commonly known as L1 and L2 regularization.Look at the figure given below carefully. The blue shape refers the regularization term and other shape present refers to our least square error (or data term).The first figure is for L1 and the second one is for L2 regularization. The black point denotes that the least square error is minimized at that point and as we can see that it increases quadratically as we move from it and the regularization term is minimized at the origin where all the parameters are zero .Now the question is that at what point will our cost function be minimum? The answer will be, since they are quadratically increasing, the sum of both the terms will be minimized at the point where they first intersect.Take a look at the L2 regularization curve. Since the shape formed by L2 regularizer is a circle, it increases quadratically as we move away from it. The L2 optimum(which is basically the intersection point) can fall on the axis lines only when the minimum MSE (mean square error or the black point in the figure) is also exactly on the axis. But in case of L1, the L1 optimum can be on the axis line because its contour is sharp and therefore there are high chances of interaction point to fall on axis. Therefore it is possible to intersect on the axis line, even when minimum MSE is not on the axis. If the intersection point falls on the axes it is known as sparse.Therefore L1 offers some level of sparsity which makes our model more efficient to store and compute and it can also help in checking importance of feature, since the features that are not important can be exactly set to zero. I hope now you understand the science behind the linear regression and how to implement it and optimize it further to improve your model.“Knowledge is the treasure and practice is the key to it”Therefore, get your hands dirty by solving some problems. You can also start with the Big mart sales problem and try to improve your model with some feature engineering.  If you face any difficulties while implementing it, feel free to write on our discussion portal.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
How to create animated GIF images for data visualization using gganimate (in R)?|,"Learn everything about Analytics|Introduction|Example – GDP vs. Life expectancy over time|Pre-requisites|Get the Data|Data Manipulation|R Codes|Split the Date into year, month and date|Speed up projection in .gif using animation package|Conclusion","Earthquake magnitude of 7 points on Richter Scale from 1965-2016|Share this:|Like this:|Related Articles|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|Director/ Head – Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience)|
Guest Blog
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Data visualization is probably the most important and typically the least talked about area of data science.I say that because how you create data stories and visualization has a huge impact on how your customers look at your work. Ultimately, data science is not only about how complicated and sophisticated your models are. It is about solving problems using data based insights. And in order to implement these solutions, your stakeholders need to understand what you are proposing.One of the challenges in creating effective visualizations is to create images which speak for themselves. This article will tell one of the ways to do so using animated GIF images (Graphics Interchangeable format). This would be particularly helpful when you want to show time / flow based stories. Using animation in images, you can plot comparable data over time for specific set of parameters. In other words, it is easy to understand and see the growth of certain parameter over time.Let me show this with an example Let us say you want to show how GDP and life expectancy have changed for various continents / countries over time. What do you think is the best way to represent this relationship?You can think of multiple options like:Now, let us look at this using an animated plot using .gif file:The recent development of gganimate package had made this possible and easier. By the end of this article, you will be able to make your own .gif file and create your own customised frame to compare different parameters on global or local scale. Please install the following packages:In addition to the above libraries in R, you will also need Image Magick Software in your system. You may download and install the same from Image Magick This article is an attempt to make .gif file on earthquake data from 1965-2016. It is better to plot year wise global seismic activity rather than a static look of all the values on the map. The data set for earthquake is available on Kaggle.
The data set contains data for global seismic activity from 1965 to 2016. Please visit the above link and scroll down to get the .csv file. The dataset had been modified and only seismic value of 7 points on richter scale has been considered for the study. From the .csv file we have only selected few parameters for the sake of simplicity.We are all set to start coding in R. I have used RStudio environment. You are free to use any environment you prefer. This is done in order to get the frame which is important for the plot. In other words, The core of the approach is to treat frame
(as in, the time point within an animation) as another dimension, just like x, y, size, color, or so on. Thus, a variable in your data can be mapped to frame just as others are mapped to x or y.Earthquake As we can see that plot has too many years from 1965 to 2016. Thus, in order to speed up the visualization, we can use the animation package to fast forward using ani.option()Earthquake – 1.5x speed This article was an introductory tutorial to the world of animated map. Readers can try this and apply the same in other projects. Some of the example are,Hope you found the article useful. If you have any questions, please feel free to ask in comments below.Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning.This post was received as part of our blogging competition – The Mightiest Pen. Check out other competitions here.",https://www.analyticsvidhya.com/blog/2017/06/a-study-on-global-seismic-activity-between-1965-and-2016/
Director/ Head – Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|How to create animated GIF images for data visualization using gganimate (in R)?|Director/ VP (Analytics) – Gurugram (8-14 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 7 – 10 years
Requirements : 
Task Info : About the Company:We are the leading A+/A1+ rated non-banking finance company providing the crucial link between debt capital markets and high quality originators who reach the emerging consumer andbusiness owner. Using its deep experience, unrivalled data, proven and proprietary risk management processes and innovative structured finance techniques we continues to deliver superior risk-adjusted returns to a growing client base of Indian and international investors keen to tap into a growing market opportunity.We have employed talented capital market professionals with extensive experience to help it deliver this proposition. By allowing lenders in remote areas of India to increase the volume and lower the cost of borrowing for low-income and financially excluded families and businesses, our activities are now benefiting some 15 million individuals. The company also helps its clients build better operating and oversight systems and to implement customer protection principles, thus improving the quality of products and services that end borrowers receive.Till date, we have completed over 300 rated capital market transactions and raised over USD 3 billion in financing for its clients with a zero delinquency track record. About the positionWe are looking to hire an experienced, innovative, passionate and knowledgeable candidates “Head – Risk Analytics and Modelling” to lead a high performance team and deliver data-driven business-decision oriented risk analytics and modelling. The team uses borrower and loanperformance data from retail and wholesale plain vanilla and structured finance portfolios across multiple asset classes – including but not limited to- microfinance, small business loan, housing finance, vehicle finance and agriculture-allied finance as well as corporate finance sector. Such analytics and modelling is done at a transaction/portfolio level as well as for our’s portfolio at an aggregate level.Key Result Areas– Managing and further building a high performance risk analytics and modelling team focused on innovation to deliver relevant business-decision oriented risk analytics and reporting– Enhancing the existing framework and evolving the estimation of economic capital and value-at risk for taking into account various risks and managing this proactively as the portfolio and business evolves– Evolving a robust stress testing framework and continuously developing tools and perform stress testing at pool, transaction, and portfolio level to estimate the worst case losses and assess possible risk mitigation (such as credit enhancements)– Further build the capability of the team in portfolio analytics and to analyse the granular loan level performance data to get insights into the borrower credit behaviour and share findings in the form of reports, views and special maps with internal and external teams– Manage portfolio analytics and other analytics advisory engagements and ensure timely execution and high quality deliverable for our partners, regulators and other entities to create a differentiated market position for us– Design and drive research on questions related to risk management and financial access for low income households and enterprises based on performance data of own portfolio, data subscribed from credit bureaus and other sources to consolidate our position thoughtleadership in these sectorsKey ResponsibilitiesPortfolio Risk ManagementTransaction Management– Oversee the team:Data AnalyticsRisk Mitigation – Product DevelopmentEssential Skills and ExperienceThe successful candidate1.  Must be passionate about managing, mentoring and developing a high performance risk analytics and modelling team, while possessing the personal qualities and skills to foster an innovative, forward looking, collaborative and cohesive team culture2. Will hold a Doctoral or Master’s degree in a quantitative field such as statistics, econometrics, economics, mathematical finance, finance, or applied science such as physics
College Preference : no-bar
Min Qualification : phd
Skills : banking, bfsi, machine learning, matlab, python, r, Risk Analytics, sql, statistics
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/director-head-risk-analytics-modelling-chennai-7-to-9-years-of-experience/
Director/ VP (Analytics) – Gurugram (8-14 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Director/ Head – Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience)|Introductory guide to Generative Adversarial Networks (GANs) and their promise!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 8 – 14 years
Requirements : 
Task Info : We are looking to hire a person who can lead our analytics strategy. This is a key role as the person would be defining the analytics road map for the organization. This would include:
College Preference : tier1-entire
Min Qualification : pg
Skills : bfsi, machine learning, predictive modeling, Risk Analytics
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/director-vp-analytics-gurugram-8-14-years-of-experience/
Introductory guide to Generative Adversarial Networks (GANs) and their promise!,"Learn everything about Analytics|Introduction|Excuse me, but what is a GAN?|How do GANs work?|Challenges with GANs|Implementing a Toy GAN|Applications of GAN|Resources|End Notes","|Another analogy from real life||Parts of training GAN||Steps to train a GAN|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Director/ VP (Analytics) – Gurugram (8-14 Years of Experience)|Data Scientist – Bangalore (4-8 Years Of Experience)|
Faizan Shaikh
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Neural Networks have made great progress. They now recognize images and voice at levels comparable to humans. They are also able to understand natural language with a good accuracy.But, even then, the talk of automating human tasks with machines looks a bit far fetched. After all, we do much more than just recognizing image / voice or understanding what people around us are saying – don’t we?Let us see a few examples where we need human creativity (at least as of now):Do you think, these tasks can be accomplished by machines? Well – the answer might surprise you 🙂These are definitely difficult to automate tasks, but Generative Adversarial Networks (GANs) have started making some of these tasks possible.If you feel intimidated by the name GAN – don’t worry! You will feel comfortable with them by end of this article.In this article, I will introduce you to the concept of GANs and explain how they work along with the challenges. I will also let you know of some cool things people have done using GAN and give you links to some of the important resources for getting deeper into these techniques. Yann LeCun, a prominent figure in Deep Learning Domain said in his Quora session that“(GANs), and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.”Surely he has a point. When I saw the implicationsGenerative Adversarial Networks (GANs) can have if they were executed to their fullest extent, I was impressed too.But what is a GAN?Let us take an analogy to explain the concept:If you want to get better at something, say chess; what would you do? You would compete with an opponent better than you. Then you would analyze what you did wrong, what he / she did right, and think on what could you do to beat him / her in the next game.You would repeat this step until you defeat the opponent. This concept can be incorporated to build better models. So simply, for getting a powerful hero (viz generator), we need a more powerful opponent (viz discriminator)!A slightly more real analogy can be considered as a relation between forger and an investigator.The task of a forger is to create fraudulent imitations of original paintings by famous artists. If this created piece can pass as the original one, the forger gets a lot of money in exchange of the piece.On the other hand, an art investigator’s task is to catch these forgers who create the fraudulent pieces. How does he do it? He knows what are the properties which sets the original artist apart and what kind of painting he should have created. He evaluates this knowledge with the piece in hand to check if it is real or not.This contest of forger vs investigator goes on, which ultimately makes world class investigators (and unfortunately world class forger); a battle between good and evil. We got a high level overview of GANs. Now, we will go on to understand their nitty-gritty of these things.As we saw, there are two main components of a GAN – Generator Neural Network and Discriminator Neural Network.The Generator Network takes an random input and tries to generate a sample of data. In the above image, we can see that generator G(z) takes a input z from p(z), where  z is a sample from probability distribution p(z). It then generates a data which is then fed into a discriminator network D(x). The task of Discriminator Network is to take input either from the real data or from the generator and try to predict whether the input is real or generated. It takes an input x from pdata(x) where pdata(x) is our real data distribution. D(x) then solves a binary classification problem using sigmoid function giving output in the range 0 to 1.Let us define the notations we will be using to formalize our GAN,Pdata(x) -> the distribution of real data
X -> sample from pdata(x)
P(z) -> distribution of generator
Z -> sample from p(z)
G(z) -> Generator Network
D(x) -> Discriminator NetworkNow the training of GAN is done (as we saw above) as a fight between generator and discriminator. This can be represented mathematically asIn our function V(D, G) the first term is entropy that the data from real distribution (pdata(x)) passes through the discriminator (aka best case scenario). The discriminator tries to maximize this to 1. The second term is entropy that the data from random input (p(z)) passes through the generator, which then generates a fake sample which is then passed through the discriminator to identify the fakeness (aka worst case scenario). In this term, discriminator tries to maximize it to 0 (i.e. the log probability that the data from generated is fake is equal to 0). So overall, the discriminator is trying to maximize our function V.On the other hand, the task of generator is exactly opposite, i.e. it tries to minimize the function V so that the differentiation between real and fake data is bare minimum. This, in other words is a cat and mouse game between generator and discriminator!Note: This method of training a GAN is taken from game theory called the minimax game.So broadly a training phase has two main subparts and they are done sequentiallyStep 1: Define the problem. Do you want to generate fake images or fake text. Here you should completely define the problem and collect data for it.Step 2: Define architecture of GAN. Define how your GAN should look like. Should both your generator and discriminator be multi layer perceptrons, or convolutional neural networks? This step will depend on what problem you are trying to solve.Step 3: Train Discriminator on real data for n epochs. Get the data you want to generate fake on and train the discriminator to correctly predict them as real. Here value n can be any natural number between 1 and infinity.Step 4: Generate fake inputs for generator and train discriminator on fake data. Get generated data and let the discriminator correctly predict them as fake.Step 5: Train generator with the output of discriminator. Now when the discriminator is trained, you can get its predictions and use it as an objective for training the generator. Train the generator to fool the discriminator.Step 6: Repeat step 3 to step 5 for a few epochs.Step 7: Check if the fake data manually if it seems legit. If it seems appropriate, stop training, else go to step 3. This is a bit of a manual task, as hand evaluating the data is the best way to check the fakeness. When this step is over, you can evaluate whether the GAN is performing well enough.Now just take a breath and look at what kind of implications this technique could have. If hypothetically you had a fully functional generator, you can duplicate almost anything. To give you examples, you can generate fake news; create books and novels with unimaginable stories; on call support and much more. You can have artificial intelligence as close to reality; a true artificial  intelligence! That’s the dream!! You may ask, if we know what could these beautiful creatures (monsters?) do; why haven’t something happened? This is because we have barely scratched the surface. There’s so many roadblocks into building a “good enough” GAN and we haven’t cleared many of them yet. There’s a whole area of research out there just to find “how to train a GAN”The most important roadblock while training a GAN is stability. If you start to train a GAN, and the discriminator part is much powerful that its generator counterpart, the generator would fail to train effectively. This will in turn affect training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless.Another way to glance at stability of GAN is to look as a holistic convergence problem. Both generator and discriminator are fighting against each other to get one step ahead of the other. Also, they are dependent on each other for efficient training. If one of them fails, the whole system fails. So you have to make sure they don’t explode.This is kind of like the shadow in Prince of Persia game . You have to defend yourself from the shadow, which tries to kill you. If you kill the shadow you die, but if you don’t do anything, you will definitely die!There are other problems too, which I will list down here. (Reference: http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)Note: Below mentioned images are generated by a GAN trained on ImageNet dataset.  A substantial research is being done to take care of these problems. Newer types of models are proposed which give more accurate results than previous techniques, such as DCGAN, WassersteinGAN etc Lets see a toy implementation of GAN to strengthen our theory. We will try to generate digits by training a GAN on Identify the Digits dataset. A bit about the dataset; the dataset contains 28×28 images which are black and white. All the images are in “.png” format. For our task, we will only work on the training set. You can download the dataset from here.You also need to setup the libraries , namelyBefore starting with the code, let us understand the internal working thorugh pseudocode. A pseudocode of GAN training can be thought out as followsSource: http://papers.nips.cc/paper/5423-generative-adversarialNote: This is the first implementation of GAN that was published in the paper. Numerous improvements/updates in the pseudocode can be seen in the recent papers such as adding batch normalization in the generator and discrimination network, training generator k times etc.Now lets start with the code!Let us first import all the modulesTo have a deterministic randomness, we set a seed valueWe set the path of our data and working directoryLet us load our dataTo visualize what our data looks like, let us plot one of the imageDefine variables which we will be using later# define vars g_input_shape = 100 d_input_shape = (28, 28) hidden_1_num_units = 500 hidden_2_num_units = 500 g_output_num_units = 784 d_output_num_units = 1 epochs = 25 batch_size = 128Now define our generator and discriminator networksHere is the architecture of our networksWe will then define our GAN, for that we will first import a few important modulesLet us compile our GAN and start the trainingHere’s how our GAN would look like,We get a graph like after training for 10 epochs.After training for 100 epochs, I got the following generated images   And voila! You have built your first generative model! We saw an overview of how these things work and got to know the challenges of training them. We will now see the cutting edge research that has been done using GANs     Here are some resources which you might find helpful to get more in-depth on GAN Phew! I hope you are now as excited about the future as I was when I first read about GANs. They are set to change what machines can do for us. Think of it – from preparing new recipes of food to creating drawings. The possibilities are endless.In this article, I tried to cover a general overview of GAN and its applications. GAN is very exciting area and that’s why researchers are so excited about building generative models and you can see that new papers on GANs are coming out more frequently.If you have any questions on GANs, please feel free to share them with me through comments. ",https://www.analyticsvidhya.com/blog/2017/06/introductory-generative-adversarial-networks-gans/
Data Scientist – Bangalore (4-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Introductory guide to Generative Adversarial Networks (GANs) and their promise!|Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 8 years
Requirements : 
Task Info : Job Description and Responsibilities:– Excellent coding skills. – Curious and detailed oriented. – Great analytical skills and problem solving capabilities. – Advanced R or Python knowledge. – Experience working with relational databases and SQL. – Familiarity with GIT. – Basic statistics knowledge. Desirable skills: – Experience with Java or C++. – Working experience with distributed processing. Experience with one of Hadoop, Spark, Netezza, Hive, SparkSQL, SparkR. – Experience in database development with strong skills in schema design, development and performance tuning. – Experience developing software in collaboration with others and maintaining a coding library. – Familiarity with investment theory and data. – Experience working in continuous integration using Jenkins. – Working knowledge of Linux. – Experience with visualization tools such as Tableau. – Knowledge of Tensorflow and/or Theano. Education : Advanced degree (MS/PhD) in physics, mathematics, statistics, engineering or computer science. Undergraduate degree with strong experiences can be considered as well.
College Preference : no-bar
Min Qualification : pg
Skills : c++, hadoop, hive, java, linux, python, r, spark, sql, statistics, tableau, tensor flow, Theano
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-bangalore-4-8-years-of-experience/
Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist – Bangalore (4-8 Years Of Experience)|Assistant Manager (Data Analytics) – Kolkata (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 4 years
Requirements : 
Task Info : Job Description and Responsibilities:– Strong background in statistical concepts and calculations with 2.5+ yrs experience with real data – Innovative and strong analytical and algorithmic problem solvers. – Proficiency with statistical analysis tools (e.g. R, SAS,SPSS), software development technologies (e.g. Python, Java, C/C++, .Net) – Extensive experience solving analytical problems using quantitative approaches (e.g. Bayesian Analysis, Reduced Dimensional Data Representations, and Multi-scale Feature Identification). – Expert at data visualization and presentation. – Excellent critical thinking skills, combined with the ability to present your beliefs clearly and compellingly verbally and in written form.Key Role & Responsibilities: – Work with engineering and research teams on designing, building and deploying data analysis systems for large data sets – Design, develop and implement R&D and pre-product prototype solutions and implementations using off the shelf tools (e.g. R, SAS,SPSS), and software (e.g. Python, Java, C/C++, .NET) – Create algorithms to extract information from large data sets. – Establish scalable, efficient, automated processes for model development, model validation, model implementation and large scale data analysis. – Develop metrics and prototypes that can be used to drive business decisions. Qualification: Bachelor Degree in Engineering from top institute/college – IIT, NIT, BITS Pilani etc. Or MS / MSc in Statistics
College Preference : tier1-any
Min Qualification : ug
Skills : c++, data visualization, java, problem solving, python, r, sas, spss
Location : Bengaluru, Delhi, Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-delhincrbangalorehyderabad-3-4-years-of-experience/
Assistant Manager (Data Analytics) – Kolkata (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience)|Which algorithm takes the crown: Light GBM vs XGBOOST?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 7 years
Requirements : 
Task Info : Job Description and Responsibilities:– Data integration and building a data warehouse for automating the Business Intelligence system – Running and maintaining the reporting system, presenting insights at a weekly meeting – Building templates, dashboards in Excel or on other third-party analytics tool for operational and management reporting – Data extraction as per business request for Ad hoc analysis – Business analysis and understanding – Evaluating metrics to be tracked as per business goals, exploring other available metrics for deeper understanding of business performance – Statistical and Analytical Models and methods for data analysis – Interacting with cross-functional teams to schedule campaigns and activities and track performance Preferred Qualification & Skills : – BE / B. Tech from premier schools – Proficient in R and Data Modelling – 3+ years of experience in working on reporting / business intelligence systems – Strong database concepts and experience in SQL – can convert any business requirement into a SQL statement – Familiar with Google Analytics, can build custom reports and dashboards for any business query – Quick learner and ability to work in dynamic work environment – Team player and comfortable interacting with people from multiple disciplines
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, excel, google analytics, modeling, r, sql
Location : Kolkata
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-data-analytics-kolkata-3-7-years-of-experience/
Which algorithm takes the crown: Light GBM vs XGBOOST?,Learn everything about Analytics|Introduction|Table of Contents|1. What is Light GBM?|2. Advantages of Light GBM|3. Installing Light GBM|4. Important Parameters of light GBM|5. LightGBM vs XGBoost|6. Tuning Parameters of Light GBM|7. End Notes,"For Windows|For Linux
|For OSX|#Applying xgboost|# Light GBM|For best fit|For faster speed|For better accuracy|Share this:|Like this:|Related Articles|Assistant Manager (Data Analytics) – Kolkata (3-7 Years Of Experience)|Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience)|
Pranjal Khandelwal
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"If you are an active member of the Machine Learning community, you must be aware of Boosting Machines and their capabilities. The development of Boosting Machines started from AdaBoost to today’s favorite XGBOOST. XGBOOST has become a de-facto algorithm for winning competitions at Analytics Vidhya and Kaggle, simply because it is extremely powerful. But given lots and lots of data, even XGBOOST takes a long time to train.Enter…. Light GBM.Many of you might not be familiar with the Light Gradient Boosting, but you will be after reading this article. The most natural question that will come to your mind is – Why another boosting machine algorithm? Is it superior to XGBOOST?Well, you very well must have guessed the answer otherwise why would a topic deserve its own article :p P.S. This article assumes knowledge about GBMs and XGBoost. If you don’t know them, you should first look at these articles.  Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’.Before is a diagrammatic representation by the makers of the Light GBM to explain the difference clearly.Level-wise tree growth in XGBOOST.Leaf wise tree growth in Light GBM. Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur.Below, we will see the steps to install Light GBM and run a model using it. We will be comparing the results with XGBOOST results to prove that you should take Light GBM in a ‘LIGHT MANNER’.Let us look at some of the advantages of Light GBM. I guess you must have got excited about the advantages of Light GBM. Let us now proceed to install the library into our system. Using Visual Studio (Or MSBuild)
-Install git for windows, cmake and MS Build (Not need the MSbuild if you already install Visual Studio).
-Run following command:The exe and dll will be in LightGBM/Release folder. Using MinGW64
-Install git for windows, cmake and MinGW64.
-Run following command:The exe and dll will be in LightGBM/ folder.Light GBM uses cmake to build. Run following: LightGBM depends on OpenMP for compiling, which isn’t supported by Apple Clang.Please use gcc/g++ instead.
-Run following:Now before we dive head first into building our first Light GBM model, let us look into some of the parameters of Light GBM to have an understanding of its underlying procedures.  Also, go through this article explaining parameter tuning in XGBOOST in detail. So now let’s compare LightGBM with XGBoost by applying both the algorithms to a dataset and then comparing the performance.Here we are using dataset that contains the information about individuals from various countries. Our target is to predict whether a person makes <=50k or >50k annually on basis of the other information available. Dataset consists of 32561 observations and 14 features describing individuals.Here is the link to the dataset: http://archive.ics.uci.edu/ml/datasets/Adult.Go through the dataset to have a proper intuition about predictor variables and so that you could understand the code below properly.Before we get to the code for this dataset, did you know that you can now code your own model in this very window? That’s right! Here’s a live coding window to play around with the code and see the results in real-time: Performance comparison There has been only a slight increase in accuracy and auc score by applying Light GBM over XGBOOST but there is a significant difference in the execution time for the training procedure. Light GBM is almost 7 times faster than XGBOOST and is a much better approach when dealing with large datasets.This turns out to be a huge advantage when you are working on large datasets in limited time competitions. Light GBM uses leaf wise splitting over depth wise splitting which enables it to converge much faster but also leads to overfitting. So here is a quick guide to tune the parameters in Light GBM.   In this blog, I’ve tried to give an intuitive idea of Light GBM. One of the disadvantages of using this algorithm currently is its narrow user base – but that is changing fast. This algorithm apart from being more accurate and time-saving than XGBOOST has been limited in usage due to less documentation available.However, this algorithm has shown far better results and has outperformed existing boosting algorithms. I’ll strongly recommend you to implement Light GBM over the other boosting algorithms and see the difference yourself.It might be still early days to crown LightGBM – but it has clearly challenged XGBoost. A word of caution – like all other ML algorithms,  make sure you properly tune the parameters before training the model!Do let us know your thoughts and opinions in the comment section below.",https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/
Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Which algorithm takes the crown: Light GBM vs XGBOOST?|Business Analyst – Bangalore (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 4 years
Requirements : 
Task Info : We are focused on BFSI, Retail & Healthcare space use cases for companies that need technology and professional services for their functional and operational analytics projects. We offer a range of solutions that bring immediate business benefits to our global customers leveraging big data, statistical and mathematical modeling techniques, social analytics, and mobile descriptive analytics for new business insights. Role Description:The role will participate in requirement gathering, system design, model implementation, code reviews, testing, and maintenance of the platform. You will be part of a highly focused development team that includes data scientists, data engineers and business analysts to help build products and specialized services on offer to clients across multi-platform environment. The role offers a high degree of challenge and provides opportunity to experiment offerings that speaks of innovation with a high velocity and quality. Skills required:– 2+ years of Analytics experience in developing applications using Python, predictive modeling and analysis. – Experienced in writing a good, clean, testable, Python code. – Experience with MySQL, Django and git. – Knowledge of Image processing libraries such as OpenCV, PIL, and pytesseract would be an added advantage. – Basic knowledge of Agile development practices. – Good understanding of numpy, scipy, pandas and scikit-learn libraries. – Strong analytical and problem solving skills. – Familiarity with the fast-paced startup environment and culture. – A team player with excellent written and verbal communication skills. 
College Preference : no-bar
Min Qualification : pg
Skills : numpy + scipy, predictive modeling, problem solving, python, scikit learn, statistics
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-python-expert-hyderabad-2-4-years-of-experience/
Business Analyst – Bangalore (3-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience)|Data Scientist – Delhi/NCR (0-2 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 5 years
Requirements : 
Task Info : Job Description:– First and foremost, problem solving ability you will be dealing with a very ambiguous set of problems – Very action oriented (we will have a bunch of things that keep coming up- taking them to closure is a critical skill) – Strong project management skills and communication / influencing skills. – Tech orientation is a must, past background in tech is not mandatory but preferred. Responsibilities : – Define business problems and define key metrics and indicators for measurement – Conduct analysis for assigned business problem by collecting data from all internal and external sources and perform analysis to get answers to business questions. – Conduct Market research for an assigned topic to dig out relevant data and analytics – Deeper analysis of consumer behaviors & trends, by reviewing the internal & external data Must have : – Strong communication skills (verbal & written) – Strong Analytical Skills and Process Orientation – The ability to thrive in a fast-paced, start-up environment Education : IIT/IIM Fresher
College Preference : tier1-any
Min Qualification : ug
Skills : analytics, business analysis, data management, problem solving, python, r, sas, sql, statistics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-bangalore-3-5-years-of-experience/
Data Scientist – Delhi/NCR (0-2 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst – Bangalore (3-5 Years Of Experience)|Analytics Specialist- Mumbai (2-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 0 – 2 years
Requirements : 
Task Info : Job Description and Responsibilities:Working in a small team led by the Chief Risk Officer and the co-founder of company, the data scientist will work on improving the data science capabilities. We currently have 2 key models, admit predictor model and the employability assessment model, and the selected candidate will work on improvising the same. The admit predictor model helps students shortlist the colleges they should target for higher studies. The employability assessment model helps lending partners make better offers to students who want to pursue higher studies. The ideal candidate can think big picture, and is comfortable with getting his/her hands dirty with data. The candidate should be resourceful in getting the data needed to support one’s hypothesis. Prior experience through internships / job experience / Kaggle contests would be highly valued. The candidate should understand that in the real world data is never clean and data collection / data gathering is a significant part of the daily job requirements. Do you appreciate the difference between causation and correlation- If you answered yes, can see through the hype of Big Data, and know when to use an OLS and when to use GBM, we want you here! Qualification: Prior experience working on data science projects Experience in NLP will be preferred 
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, nlp, predictive modeling, python, r, statistical modeling
Location : Delhi
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-delhincr-0-2-years-of-experience/
Analytics Specialist- Mumbai (2-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist – Delhi/NCR (0-2 Years Of Experience)|Assistant Manager – (Strategic Research)- Mumbai (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 4 years
Requirements : 
Task Info : Job Description:Analyze & interpret data and communicate results to clients, often with the aid of mathematical/statistical techniques and software. This role requires: – Data exploration, mathematical/statistical modelling, data analysis and hypothesis testing. – Design, development and deployment of Predictive models and frameworks. – Complex statistical concepts are explained in a way that clients can understand and advice on strategy. Responsibilities:– Well-versed in quantitative analysis, research, data mining, trend analysis, customer profiling, clustering, segmentation and predictive modelling. – Executing the data-driven planning process by building models and frameworks that connect business unit drivers to company financials and forecast to take the correct decision as per the business need. – Design and build dashboards and other visualizations in BI tools. – Should be able to handle assigned tasks in the capacity of individual contributor. – Demonstrate excellent teamwork. – Collaborate with other data analysts to provide development coverage, support, and knowledge sharing and mentoring of junior team members. Education & Skills Summary:– Bachelor’s in Computer Science/ Engineering, Statistics, Math or related quantitative degree or equivalent work experience. – Experience with modelling techniques such as clustering, linear & logistic regression, random forest etc. – Must have a passion for data, structured or unstructured. – 2-4 years of hands-on experience with R (is a must) – SAS, SQL and python would be a plus. – Should have sound experience in data mining and data analysis. – Excellent critical thinking, verbal and written communications skills. – Ability and desire to work in a proactive, highly engaging, high-pressure, client service environment. – Good presentation skills.
College Preference : no-bar
Min Qualification : ug
Skills : clustering, data analysis, data mining, linear regression, logistic regression, python, r, random forest, sas, sql, statistics
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/analytics-specialist-mumbai-2-4-years-of-experience/
Assistant Manager – (Strategic Research)- Mumbai (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Specialist- Mumbai (2-4 Years Of Experience)|APM/PM – (Predictive Analytics) – Mumbai/Pune (2-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities:– Strong quantitative and analytical skills with ability to translate data into meaningful insights is a big plus – Demonstrated accuracy, efficiency and attention to detail – Advanced skills in MS Office and various databases – Intermediate to advanced skills in MS-Excel will be an added advantage General Skills – Exceptional verbal and written communication and interpersonal skills – A high-minded team player – Strong intellect with proficient commercial and entrepreneurial instinct – Ability to work within tight deadlines – Ability to challenge upwards – Ability to adapt own work in a fast changing environment – Ability to operate in an ambiguous environment, break clich’s and introduce a dynamic approachPrior Experience – Between 2 to 6 years of experience in industry/country/business/market research with in-depth knowledge of global marketplace, key trends, economic and geo-political developments, global supply chain, and key business and financial risks – Experience of working with a UK/US based insurance/re-insurance/insurance broking/consulting firm of global repute will be looked at preferably
College Preference : no-bar
Min Qualification : ug
Skills : analytics, database, excel
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-strategic-research-mumbai-2-5-years-of-experience/
APM/PM – (Predictive Analytics) – Mumbai/Pune (2-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager – (Strategic Research)- Mumbai (2-5 Years Of Experience)|Data Scientist/Business Analyst – Mumbai (5- 10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 8 years
Requirements : 
Task Info : Job Description and Responsibilities: 2-5 years of experience in advanced analytics/ predictive modeling. Candidate should have worked hands on in developing predictive models in some of these areas – cross-sell / up-sell strategies, market segmentation, demand forecasting, price optimization, time to event, survival analysis etc Mandatory Skills : – Must be well versed with atleast one (1) of following tools – R, SAS, SPSS – Must have good knowledge / hands-on experience of statistical concepts like hypothesis testing, distributions etc – Must have good knowledge / hands-on experience of atleast some of the applied statistical techniques including multivariate regression, logistic regression, ARMA/ ARIMA, clustering, survival etc – Excellent presentation and communication skills (written and verbal) and comfortable presenting at senior levels – Conceptualize, design and deliver high-quality solutions and insightful analysis on a variety of projects ranging in both complexity and scope – Qualification: Masters in statistics/mathematics/economics or MBA from a reputed institute Preferred Skills : – Knowledge / hand-on experience of at least two (2) of the following tools – VBA, SQL, MS Excel, Tableau – Ability to present results of statistical models in business language – Experience for building predictive models for varied industries – Retail, hospitality, technology hardware, software etc – Experience in handling large datasets – Exposure to multiple statistical tools – Excellent interpersonal skills – ability to network and earn confidence of diverse Client personnel plus interaction with management of eClerx India based operations team
College Preference : no-bar
Min Qualification : ug
Skills : arima, excel, logistic regression, multivariate analysis, predictive modeling, r, Retail Analytics, sas, spss, sql, statistical techniques, tableau, VBA
Location : Mumbai, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/apmpm-predictive-analytics-mumbaipune-2-8-years-of-experience/
Data Scientist/Business Analyst – Mumbai (5- 10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|APM/PM – (Predictive Analytics) – Mumbai/Pune (2-8 Years Of Experience)|Operational Risk Manager – Hyderabad (10-12 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 5 – 10 years
Requirements : 
Task Info : Job Description and Responsibilities:– Help Leadership to use data to come up with intelligent and usable recommendations for business problems – Develop Metrics and Dashboards using MS SQL Access, Excel, Execute high priority (i.e. cross functional, high impact) projects to improve operations performance. – Create performance metrics and present them to the team with recommendations on areas to improve. – Getting report requirements from business teams and building and maintaining complex SQL queries to gather business data from data warehouse and financial reporting systems. Academic Qualification & Skill Set : – Graduate with 1st Class from a recognized institute – Post qualification relevant work experience as Analyst with a reputed organization. – Knowledge of RDBMS, MS SQL ( including SQL server), database design & development – Ability to identify suitable processes and automate MSSQL, MS Excel, Tasks. – Strong analytical business skills is a must.
College Preference : tier1-any
Min Qualification : ug
Skills : database, excel, RDBMS, sql server
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientistbusiness-analyst-mumbai-5-10-years-of-experience/
Operational Risk Manager – Hyderabad (10-12 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist/Business Analyst – Mumbai (5- 10 Years Of Experience)|Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 10 – 12 years
Requirements : 
Task Info : Job Description and Responsibilities– Responsible for planning, development, implementation, and maintenance of EDA Risk metrics and KRIs (Key Risk Indicators) for the EDA group – Provide thought leadership and apply areas of expertise to data domain to develop a risk metrics program that is effective and efficient in discovering and measuring the inherent risks across the line of business – Development and implementation of the deliverables for Risk Metrics work stream, a work stream within the EDA transformation program – Development and execution of Risk Metrics reporting – Preparation of documentation for deliverables within the Risk Metrics work stream – Build strong relationships and partner with business partners throughout the EDA group related to Business Processes and the related KRIs – Participate in strategic team and enterprise initiatives for risk metrics and KRIs – Ensure appropriate prioritization, status reporting, escalation and closure of outstanding Risk Metrics issues – Provide risk metrics support, expertise and consulting to identify, assess and manage risk in support of EDA for projects and initiatives – Review metrics and decipher their usability from the risk and /or the performance angle – Synthesize risk related data for business, customers, and/or products/services/portfolios to ‘form a story’ – Provide administrative oversight for EDA risk management offshore resources at EGS (please note that these resources will get work direction from their functional manager in US) Work with senior managers in India to provide required reporting re vacations/time taken off/time planning for offshore resources and facilitate performance management process on the ground Desired Qualifications : – Experience with enterprise database technologies (data storage, data movement, and data access etc.) and business intelligence tools – Experience utilizing Excel, PowerPoint and visualization skills (such as tableau) – Experience utilizing software such as SAS, SQL, Teradata, Oracle, Informatica, Ab Initio, Cognos etc. – Exposure to our data including BMG, MDSS, STS, Hogan, and/or Hemisphere databases – Exposure to or experience working with products in the financial industry – Professional certifications such as Certified IT Risk Professional (CRISC), Certified Information Systems Security Professional (CISSP), Certified in the Governance of Enterprise IT (CGEIT) or Certified Information Systems Auditor (CISA) 
College Preference : no-bar
Min Qualification : pg
Skills : Abinitio, business intelligence, excel, modeling, oracle, Power point, risk, sas, sql, tableau, teradata
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/operational-risk-manager-hyderabad-10-12-years-of-experience/
Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Operational Risk Manager – Hyderabad (10-12 Years Of Experience)|Getting started with Deep Learning using Keras and TensorFlow in R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 9 years
Requirements : 
Task Info : Job Description and Responsibilities:– Developing advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective manner – Implementing statistical and data mining techniques e.g. hypothesis testing, machine learning, and retrieval processes on a large amount of data to identify trends, figures and other relevant information– Collaborating with clients and other our stakeholders to effectively integrate and communicate analysis findings– Providing guidance and project management support to the Associates on the teamResearch and Firm Contribution :– Evaluating emerging datasets and technologies that may contribute to our analytical platform– Owning the development of select assets/accelerators for efficient scaling of capability– Contributing to the thought leadership of the firm by helping in researching the evolving topics and publishing them.Qualifications :– BE / BTech / BS (stats) from Tier-1 institute would be required.– PhD in Computer Science (OR Statistics) / MTech / MS from a premier institute would be highly preferred– Substantial experience in Machine Learning.– Knowledge of big data / advanced analytics concepts and algorithms text mining, social listening, recommender systems, predictive modeling, etc.– Knowledge of programming Java/Python/R– Exposure to tools/platforms Hadoop eco system and DB systems– Agile project planning and project management skills– Excellent communication skills– Domain knowledge / expertise (Pharma / Healthcare /Travel / Hi-tech/Insurance) is preferred though not mandatory.
College Preference : tier1-any
Min Qualification : ug
Skills : algorithms, bigdata, Hadoop Ecosystem, java, machine learning, python, r, statistics, text mining
Location : Delhi, Gurugram, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-data-scientist-delhincrpunegurgaon-4-9-years-of-experience/
Getting started with Deep Learning using Keras and TensorFlow in R,Learn everything about Analytics|Introduction|Table of contents|1. Installation of Keras with tensorflow at the backend.|2. Different types of models that can be built in R using keras|3. Classifying MNIST handwritten digits using an MLP in R|4. MLP using keras – R vs Python|5. End Notes,"Share this:|Like this:|Related Articles|Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience)|My journey from being an IT engineer to Head of Analytics|
NSS
|56 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"It has always been a debatable topic to choose between R and Python. The Machine Learning world has been divided over the preference of one language over the other. But with the explosion of Deep Learning, the balance shifted towards Python as it had an enormous list of Deep Learning libraries and frameworks which R lacked (till now).I personally switched to Python from R simply because I wanted to dive into the Deep Learning space but with an R, it was almost impossible. But not anymore!With the launch of Keras in R, this fight is back at the center. Python was slowly becoming the de-facto language for Deep Learning models. But with the release of Keras library in R with tensorflow (CPU and GPU compatibility)  at the backend as of now, it is likely that R will again fight Python for the podium even in the Deep Learning space.Below we will see how to install Keras with Tensorflow in R and build our first Neural Network model on the classic MNIST dataset in the RStudio.  The steps to install Keras in RStudio is very simple. Just follow the below steps and you would be good to make your first Neural Network Model in R.install.packages(""devtools"")devtools::install_github(""rstudio/keras"")The above step will load the keras library from the GitHub repository. Now it is time to load keras into R and install tensorflow.library(keras)By default RStudio loads the CPU version of tensorflow. Use the below command to download the CPU version of tensorflow.install_tensorflow()To install the tensorflow version with GPU support for a single user/desktop system, use the below command.install_tensorflow(gpu=TRUE)For multi-user installation, refer this installation guide.Now that we have keras and tensorflow installed inside RStudio, let us start and build our first neural network in R to solve the MNIST dataset. Below is the list of models that can be built in R using Keras.Let us start with building a very simple MLP model using just a single hidden layer to try and classify handwritten digits. #loading keras library
library(keras)#loading the keras inbuilt mnist dataset
data<-dataset_mnist()#separating train and test file
train_x<-data$train$x
train_y<-data$train$y
test_x<-data$test$x
test_y<-data$test$yrm(data)# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix
train_x <- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255
test_x <- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255#converting the target variable to once hot encoded vectors using keras inbuilt function
train_y<-to_categorical(train_y,10)
test_y<-to_categorical(test_y,10)#defining a keras sequential model
model <- keras_model_sequential()#defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]
#i.e number of digits from 0 to 9model %>% 
 layer_dense(units = 784, input_shape = 784) %>% 
 layer_dropout(rate=0.4)%>%
 layer_activation(activation = 'relu') %>% 
 layer_dense(units = 10) %>% 
 layer_activation(activation = 'softmax')#compiling the defined model with metric = accuracy and optimiser as adam.
model %>% compile(
 loss = 'categorical_crossentropy',
 optimizer = 'adam',
 metrics = c('accuracy')
)#fitting the model on the training dataset
model %>% fit(train_x, train_y, epochs = 100, batch_size = 128)#Evaluating model on the cross validation dataset
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128) The above code had a training accuracy of 99.14 and validation accuracy of 96.89. The code ran on my i5 processor and took around 13.5s for a single epoch whereas, on a TITANx GPU, the validation accuracy was 98.44 with an average epoch taking 2s. For the sake of comparison, I implemented the above MNIST problem in Python too. There should not be any difference since keras in R creates a conda instance and runs keras in it. But still, you can find the equivalent python code below.#importing the required libraries for the MLP model
import keras
from keras.models import Sequential
import numpy as np#loading the MNIST dataset from keras
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()#reshaping the x_train, y_train, x_test and y_test to conform to MLP input and output dimensions
x_train=np.reshape(x_train,(x_train.shape[0],-1))/255
x_test=np.reshape(x_test,(x_test.shape[0],-1))/255import pandas as pd
y_train=pd.get_dummies(y_train)
y_test=pd.get_dummies(y_test)#performing one-hot encoding on target variables for train and test
y_train=np.array(y_train)
y_test=np.array(y_test)#defining model with one input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer [10 #neurons]
model=Sequential()from keras.layers import Densemodel.add(Dense(784, input_dim=784, activation='relu'))
keras.layers.core.Dropout(rate=0.4)
model.add(Dense(10,input_dim=784,activation='softmax'))# compiling model using adam optimiser and accuracy as metric
model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])
# fitting model and performing validationmodel.fit(x_train,y_train,epochs=50,batch_size=128,validation_data=(x_test,y_test)) The above model achieved a validation accuracy of 98.42 on the same GPU. So, as we guessed initially, the results are the same. If this was your first Deep Learning model in R, I hope you enjoyed it. With a very simple code, you were able to classify hand written digits with 98% accuracy. This should be motivation enough to get you started with Deep Learning.If you have already worked on keras deep learning library in Python, then you will find the syntax and structure of the keras library in R to be very similar to that in Python. In fact, the keras package in R creates a conda environment and installs everything required to run keras in that environment. But, I am more excited to now see data scientists building real life deep learning models in R. As it is said – The competition should never stop. I would also like to hear your views on this new development for R. Feel free to comment.",https://www.analyticsvidhya.com/blog/2017/06/getting-started-with-deep-learning-using-keras-in-r/
My journey from being an IT engineer to Head of Analytics,Learn everything about Analytics,"Background – when sufficient is not enough|The Analytics Adventure|The Challenges|[email protected]|My first break|Learning through different roles and Executive program|Head of [email protected]|Reflections on the journey|End Notes|Share this:|Like this:|Related Articles|Getting started with Deep Learning using Keras and TensorFlow in R|Consultant – SAS/SQL/ Tableau – Bangalore (1-3 Years Of Experience)|
Guest Blog
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"I was an engineer working with an MNC in a pretty cushy job. It would have been a pretty happy life for people, but I had some other dreams. I wanted to do an MBA from outside India. Unfortunately, the plan didn’t work out – there were issues on the financial and personal fronts – and eventually I figured that maybe my ambitions were somewhat unrealistic. Although my MBA plan had fizzled out, I realized that I needed to chart out a new route. I researched the internet and discovered a new field called ‘Analytics’. I had no idea about Business Analytics at the time and neither did I know much about the programs available for formal training. A deeper study convinced me that Analytics was an interesting domain but it was complex as well. It was evident to me that in order to make analytics as my career I had to devote substantial time to learning it.I therefore sought a full-time classroom program and discovered Praxis (courtesy a banner on Analytics Vidhya) – a comparatively emerging business school but a kind of pioneer in formal, full-time teaching of Business Analytics. Praxis had a pretty comprehensive curriculum, an impressive set of professors and a growing bunch of alumni.  Convinced that this was the career I wanted, I applied to Praxis and fortunately got selected. I was thus far used to taking the tried and tested route to a career option. This was the first time I was trying to do something ‘different’. Obviously, it was not going to be easy.The first change was adapting to a different kind of academic environment – one that encouraged debate, discussion and critical thinking;The second challenge was to grapple with  a variety of subjects that were all new for me. Though I had pursued an engineering degree, I was never a numbers or tech geek. When I joined the program, the scope and range of the course, with subjects as varied and complex as advance statistics, data mining, machine learning, econometrics, visualization etc. along with tools like SAS and R was quite intimidating at first.However, the presence of the professors and my peers made sure that the learning, though rigorous, was effective and enjoyable. The program at Praxis was intense to say the least. The program design combines classroom lectures with case-studies, labs and projects; there is continual assessment in the form of quizzes, assignments, exams and project presentations. Some of the courses were entirely taught by industry experts.We all spent multiple sleepless nights solving assignments with midnight deadlines, doing projects and preparing for tests that tested and rewarded thinking. I got a new sense of confidence by solving various problems and completing projects on new subjects.Knowing that this was all in preparation for the professional world of my choice and liking made the exertions substantially sweeter. The year spent in studying analytics set me up for a career in this domain. My first job (that I landed through the campus placement program) was 3i Infotech. They were building an analytics team at that time and we were some of the initial members. We used to work mainly on Excel and R to create project demos for clients. The 3i journey was interesting, but it ended abruptly when the Head of analytics decided to leave the firm and work on his start-up. My next assignment was with Nabler Bangalore and they promptly sent me to the USA to work on building a product recommendation engine for a Fortune 30 company called ‘Lowes’. This was a golden opportunity and thanks to some wonderful colleagues, I managed to work hard and learnt a lot. The work culture and practices at Lowes helped me grow as a professional and I was ready to lead a team.During this year, I also did a 1 year Executive program in Business Analytics and Intelligence from IIM Bangalore to help me transition smoothly in to the senior management roles that I was aspiring to get in to.CarDekho was building their analytics team and I joined them as Senior Manager – Analytics. The role was quite different  – plus you had to be intrapreneurial – you had to dig down and conceptualise a project, take it to the CEO, get it approved and implement it – kind of on your own. Also, I was actively involved in hiring and managing people. Eventually, I moved to Yatra.com as Head of analytics. The stakes were high and so were the expectations. It took me some time to settle down and understand the system, the product, the people and of course the data. We have a dedicated dashboard team which builds executive dashboards for C level executives and business heads.We have a data warehousing team which collates the data from different sources i.e. booking data, CRM data and web data and stores it in a usable format. Then we have a predictive analytics team which is involved into long term projects such as customer churn analytics, cross-sell and recommendation engine based on Machine leaning concepts. My role involves end to end delivery of the projects, from the conceptualization to implementation. A good chunk of my time also goes into project and people management. From the time I graduated with a qualification in Analytics in 2013 to my taking up a leading position as Head of Analytics at Yatra.com in 2016, it has been a remarkable journey. I attribute this success to the following factors: Life comes back a full circle and I am sharing this journey at the same portal where it all started! I am sharing this story so that people like me – who want to do something ‘different’ and out of box can see what it takes and how to go about planning their career.If you have any question, feel free to ask questions in the comments below.Ritesh is an adroit professional more than 8 years of work experience in the area of Analytics and Product management (Statistical analysis, Social Media/ Digital Marketing), Strategy, Product development and Marketing and served various domain such as BFSI, Technology, Education, IT & FMCG.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhya’s view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/06/journey-from-it-engineer-head-of-analytics/
Consultant – SAS/SQL/ Tableau – Bangalore (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|My journey from being an IT engineer to Head of Analytics|SQL Expert – Delhi/NCR/Gurgaon (2-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 3 years
Requirements : 
Task Info : Job Description and Responsibilities 2. Strong communication skills  3. Tableau experience  4. Good amount of SAS statistical knowledge  5. Strong SAS, Strong Excel and PPT making skills  6. Willingness to work from Banglore  Good to have :   1. Quick learner of new technologies and business functions  2. Demonstrated maturity to support client in different time zone  3. Masters degree in Management /Finance /Mathematics /Statistics /Operations or Engineering graduate  4. Knowledge of basic statistical techniques such as clustering, segmentation, ranking, correlation or regression etc.  5. Highly motivated, self-starter and team player
College Preference : no-bar
Min Qualification : pg
Skills : sas, sql, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/consultant-sassql-tableau-bangalore-1-3-years-of-experience/
SQL Expert – Delhi/NCR/Gurgaon (2-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant – SAS/SQL/ Tableau – Bangalore (1-3 Years Of Experience)|Senior Data Analyst-Chennai (2-4 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 6 years
Requirements : 
Task Info : Job Description and Responsibilities:– Understand business requirements of customer and convert them into technical – Complex manipulation and treatment of data in SQL, excel, R/Python/SAS (whichever applicable in particular case) – Integration of data from different sources – Application of data analytics and machine learning techniques on datasets to obtain the desired output – Do deep data dives to find out insights – Interact and collaborate with internal teams for project requirement and delivery – Create front-ends or dashboards or slide decks to deliver the result to the end user – Present outputs to end users Skills: – MySQL/ Oracle database – R/Python. – SAS is good to have, but not mandatory – Statistical / Machine Learning techniques – Tableau / Qlikview is good to have – MS Excel / MS Access Experience & Education: – B.Tech/ M.Tech / MBA/ MS in Statistics or Machine Learning – 2-5 years of experience 
College Preference : no-bar
Min Qualification : ug
Skills : excel, machine learning, MS Acces, oracle, python, qlikview, r, sas, statistics, tableau
Location : Delhi, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/sql-expert-delhincrgurgaon-2-6-years-of-experience/
Senior Data Analyst-Chennai (2-4 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SQL Expert – Delhi/NCR/Gurgaon (2-6 Years Of Experience)|An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 4 years
Requirements : 
Task Info : Job Description and Responsibilities: – Understand business problems of various industries – Design analytical frameworks to address them – Apply leading-edge statistical modeling techniques and machine learning techniques – Work with state-of-the-art technologies – Work with complex data / big data – Be part of a growing team that is excited about the work we do – we would love to talk to you. The position offers a unique opportunity to be part of a small, fast-paced, challenging, and entrepreneurial environment, with a high degree of individual responsibility. Significant opportunities for professional development exist, as we continue to grow. Compensation packages among the best in the industry. Desired Skills and Experience: – 2+ years of programming experience which should include hands-on programming in Java/C/C++ – Knowledge of one of statistical/general purpose scripting languages software such as R, SAS, Python, SPSS etc is mandatory. – Excellent written and verbal communication skills
College Preference : no-bar
Min Qualification : ug
Skills : c++, C, java, python, r, sas, spss
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-data-analyst-chennai-2-4-years-of-experience/
An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec,Learn everything about Analytics|Introduction|Project to apply Word Embeddings for Text Classification|Problem Statement|Table of Contents|1. What are Word Embeddings?|2. Different types of Word Embeddings|2.2 Prediction based Vector|2.2.2 Skip – Gram model|3. Word Embeddings use case scenarios|4. Using pre-trained word vectors|5. Training your own word vectors|Projects|6. End Notes,"2.1 Frequency based Embedding|2.2.1 CBOW (Continuous Bag of words)|Advantages of Skip-Gram Model|Share this:|Like this:|Related Articles|Senior Data Analyst-Chennai (2-4 Years of Experience)|Data Science Evangelist- Gurgaon (2 to 3 years of experience)|
NSS
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",2.1.1 Count Vector|2.1.2 TF-IDF vectorization|2.1.3 Co-Occurrence Matrix with a fixed context window,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Before we start, have a look at the below examples. So what do the above examples have in common?You possible guessed it right – TEXT processing. All the above three scenarios deal with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation in the third.Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing the above the three tasks. It is neither scalable nor effective.So, how do we make computers of today perform clustering, classification etc on a text data since we know that they are generally inefficient at handling and processing strings or texts for any fruitful outputs?Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that “Apple” in “Apple is a tasty fruit” is a fruit that can be eaten and not a company?The answer to the above questions lie in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in.And all of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them.Below, we will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results. The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.Formally, given a training sample of tweets and labels, where label ‘1’ denotes the tweet is racist/sexist and label ‘0’ denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.Practice Now  In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked – Why do we need Word Embeddings?As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications. Some real world applications of text applications are – sentiment analysis of reviews by Amazon etc., document or news classification or clustering by Google etc.Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.Take a look at this example – sentence=” Word Embeddings are Word converted into numbers ”A word in this sentence may be “Embeddings” or “numbers ” etc.A dictionary may be the list of all unique words in the sentence. So, a dictionary may look like – [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]
A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of “numbers” in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].This is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest. The different types of word embeddings can be broadly classified into two categories-Let us try to understand each of these methods in detail. There are generally three types of vectors that we encounter under this category.Let us look into each of these vectorization methods in detail. Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).Let us understand this using a simple example.D1: He is a lazy boy. She is also lazy.D2: Neeraj is a lazy person.The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]
Here, D=2, N=6The count matrix M of size 2 X 6 will be represented as –Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for ‘lazy’ in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as – D2 contains ‘lazy’: once, ‘Neeraj’: once and ‘person’ once.Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-Below is a representational image of the matrix M for easy understanding.  This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word “Messi” in comparison to other documents. But common words like “the” etc. are also going to be present in higher frequency in almost every document.Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.So, how exactly does TF-IDF work?Consider the below sample table which gives the count of terms(tokens/words) in two documents.Now, let us define a few terms related to TF-IDF. TF = (Number of times term t appears in a document)/(Number of terms in the document)So, TF(This,Document1) = 1/8TF(This, Document2)=1/5It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word ‘Messi’ in large number.IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.where N is the number of documents and n is the number of documents a term t has appeared in.So, IDF(This) = log(2/2) = 0.So, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.Let us compute IDF for the word ‘Messi’.IDF(Messi) = log(2/1) = 0.301.Now, let us compare the TF-IDF for a common word ‘This’ and a word ‘Messi’ which seems to be of relevance to Document 1.TF-IDF(This,Document1) = (1/8) * (0) = 0TF-IDF(This, Document2) = (1/5) * (0) = 0TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15As, you can see for Document1 , TF-IDF method heavily penalises the word ‘This’ but assigns greater weight to ‘Messi’. So, this may be understood as ‘Messi’ is an important word for Document1 from the context of the entire corpus. The big idea – Similar words tend to occur together and will have similar context for example – Apple is a fruit. Mango is a fruit.
Apple and mango tend to have a similar context i.e fruit.Before I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified – Co-Occurrence and Context Window.Co-occurrence – For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.Context Window – Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below, The green words are a 2 (around) context window for the word ‘Fox’ and for calculating the co-occurrence only these words will be counted. Let us see context window for the word ‘Over’.  Now, let us take an example corpus to calculate a co-occurrence matrix.Corpus = He is not lazy. He is intelligent. He is smart. Let us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.Red box- It is the number of times ‘He’ and ‘is’ have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.while the word ‘lazy’ has never appeared with ‘intelligent’ in the context window and therefore has been assigned 0 in the blue box. Variations of Co-occurrence MatrixLet’s say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-But, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.Let me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.And, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.So, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation. Advantages of Co-occurrence Matrix Disadvantages of Co-Occurrence Matrix Pre-requisite: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through this awesome article by Sunil to gain a very good understanding of how NN works.So far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working. The way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.Suppose, we have a corpus C = “Hey, this is sample corpus using only one context word.” and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.The target for a single datapoint say Datapoint 4 is shown as below This matrix shown in the above image is sent into a shallow neural network with three layers: an input layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.Let us first see a diagrammatic representation of the CBOW model.The matrix representation of the above image for a single data point is below.The flow is as follows:We saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.Below is a matrix representation of the above architecture for an easy understanding.The image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.So, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.The steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.In both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article – Understanding and Coding Neural Networks from scratch.The differences between MLP and CBOW are  mentioned below for clarification:wo : output word
wi: context words2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has  sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP. Advantages of CBOW: Disadvantages of CBOW: Skip – gram follows the same topology as of CBOW. It just flips CBOW’s architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=”Hey, this is sample corpus using only one context word.” Let us construct the training data.The input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be “two” one hot encoded target variables and “two” corresponding outputs as can be seen by the blue section in the image.Two separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.The weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.The skip-gram architecture is shown below. For a better understanding, matrix style structure with calculation has been shown below. Let us break down the above image.Input layer  size – [1 X V], Input hidden weight matrix size – [V X N], Number of neurons in hidden layer – N, Hidden-Output weight matrix size – [N X V], Output layer size – C [1 X V]
In the above example, C is the number of context words=2, V= 10, N=4 This is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.Since word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-Below is one interesting visualisation of word2vec.The above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.5.  It can be used to perform Machine Translation.
The above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other. We are going to use google’s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.from gensim.models import Word2Vec
#loading the downloaded model
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)#the model is loaded. It can be used to perform all of the tasks mentioned above.# getting word vectors of a word
dog = model['dog']#performing king queen magic
print(model.most_similar(positive=['woman', 'king'], negative=['man']))#picking odd one out
print(model.doesnt_match(""breakfast cereal dinner lunch"".split()))#printing similarity index
print(model.similarity('woman', 'man')) We will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.word2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I won’t be covering the pre-preprocessing part here. So let’s take an example list of list to train our word2vec model.sentence=[[‘Neeraj’,’Boy’],[‘Sarwan’,’is’],[‘good’,’boy’]]
#training word2vec on 3 sentences
model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)Let us try to understand the parameters of this model.sentence – list of list of our corpus
min_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.
size=300 – the number of dimensions in which we wish to represent our word. This is the size of the word vector.
workers=4 – used for parallelization#using the model
#The new trained model can be used similar to the pre-trained ones.#printing similarity index
print(model.similarity('woman', 'man')) Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems: Word Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.Note: We also have a video course on Natural Language Processing covering many NLP topics including word embeddings. Do check it out!",https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
Data Science Evangelist- Gurgaon (2 to 3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|Transfer learning and the art of using Pre-trained Models in Deep Learning|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,What should you expect?|Who can fill in the shoes?|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 3 years
Requirements : 
Task Info : Do you feel passionately about solving problems through data? Have you spent a few years solving business problems through data?Do you aspire to take data science to millions of people out there? Can the leader in you make people follow data science out of sheer passion? Would you enjoy helping people solve problems with out expecting any thing in return?If the answer to all the questions is yes – look no more. Analytics Vidhya is looking for evangelists who can carry and deliver their baton to the world.A team of best data scientists and thought leaders from industryWhat is the role? Being a startup, the role would evolve over time. But, here are a few things you can expect:
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, data science, deep learning, machine learning, python, r
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/
Transfer learning and the art of using Pre-trained Models in Deep Learning,Learn everything about Analytics|Introduction|Table of Contents|What is transfer learning?|What is a Pre-trained Model?|Why would we use Pre-trained Models?|How can I use Pre-trained Models?|Ways to Fine tune the model|Use the pre-trained models to identify handwritten digits|Projects|End Notes,"Share this:|Like this:|Related Articles|Data Science Evangelist- Gurgaon (2 to 3 years of experience)|Data Scientist -Mumbai (1-4 Years Of Experience)|
Dishashree Gupta
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Neural networks are a different breed of models compared to the supervised machine learning algorithms. Why do I say so? There are multiple reasons for that, but the most prominent is the cost of running algorithms on the hardware.In today’s world, RAM on a machine is cheap and is available in plenty. You need hundreds of GBs of RAM to run a super complex supervised machine learning problem – it can be yours for a little investment / rent. On the other hand, access to GPUs is not that cheap. You need access to hundred GB VRAM on GPUs – it won’t be straight forward and would involve significant costs.Now, that may change in future. But for now, it means that we have to be smarter about the way we use our resources in solving Deep Learning problems. Especially so, when we try to solve complex real life problems on areas like image and voice recognition. Once you have a few hidden layers in your model, adding another layer of hidden layer would need immense resources.Thankfully, there is something called “Transfer Learning” which enables us to use pre-trained models from other people by making small changes. In this article, I am going to tell how we can use pre-trained models to accelerate our solutions.You can check out our list of the top pretrained models for computer vision and NLP here:Note – This article assumes basic familiarity with Neural networks and deep learning. If you are new to deep learning, I would strongly recommend that you read the following articles first:What is deep learning and why is it getting so much attention?Deep Learning vs. Machine Learning – the essential differences you need to know!25 Must Know Terms & concepts for Beginners in Deep LearningWhy are GPUs necessary for training Deep Learning models?  Let us start with developing an intuition for transfer learning. Let us understand from a simple teacher – student analogy.A teacher has years of experience in the particular topic he/she teaches. With all this accumulated information, the lectures that students get is a concise and brief overview of the topic. So it can be seen as a “transfer” of information from the learned to a novice.Keeping in mind this analogy, we compare this to neural network. A neural network is trained on a data. This network gains knowledge from this data, which is compiled as “weights” of the network. These weights can be extracted and then transferred to any other neural network. Instead of training the other neural network from scratch, we “transfer” the learned features.Now, let us reflect on the importance of transfer learning by relating to our evolution. And what better way than to use transfer learning for this! So I am picking on a concept touched on by Tim Urban from one of his recent articles on waitbutwhy.comTim explains that before language was invented, every generation of humans had to re-invent the knowledge for themselves and this is how knowledge growth was happening from one generation to other:Then, we invented language! A way to transfer learning from one generation to another and this is what happened over same time frame:Isn’t it phenomenal and super empowering? So, transfer learning by passing on weights is equivalent of language used to disseminate knowledge over generations in human evolution. Simply put, a pre-trained model is a model created by some one else to solve a similar problem. Instead of building a model from scratch to solve a similar problem, you use the model trained on other problem as a starting point.For example, if you want to build a self learning car. You can spend years to build a decent image recognition algorithm from scratch or you can take inception model (a pre-trained model) from Google which was built on ImageNet data to identify images in those pictures.A pre-trained model may not be 100% accurate in your application, but it saves huge efforts required to re-invent the wheel. Let me show this to you with a recent example. I spent my last week working on a problem at CrowdAnalytix platform – Identifying themes from mobile case images. This was an image classification problem where we were given 4591 images in the training dataset and 1200 images in the test dataset. The objective was to classify the images into one of the 16 categories. After the basic pre-processing steps, I started off with a simple MLP model with the following architecture-To simplify the above architecture after flattening the input image [224 X 224 X 3] into [150528], I used three hidden layers with 500, 500 and 500 neurons respectively. The output layer had 16 neurons which correspond to the number of categories in which we need to classify the input image.I barely managed a training accuracy of 6.8 % which turned out to be very bad. Even experimenting with hidden layers, number of neurons in hidden layers and drop out rates. I could not manage to substantially increase my training accuracy. Increasing the hidden layers and the number of neurons, caused 20 seconds to run a single epoch on my Titan X GPU with 12 GB VRAM.Below is an output of the training using the MLP model with the above architecture.Epoch 10/1050/50 [==============================] – 21s – loss: 15.0100 – acc: 0.0688As, you can see MLP was not going to give me any better results without exponentially increasing my training time. So I switched to Convolutional Neural Network to see how they perform on this dataset and whether I would be able to increase my training accuracy.The CNN had the below architecture –I used 3 convolutional blocks with each block following the below architecture-The result obtained after the final convolutional block was flattened into a size [256] and passed into a single hidden layer of with 64 neurons. The output of the hidden layer was passed onto the output layer after a drop out rate of 0.5.The result obtained with the above architecture is summarized below-Epoch 10/1050/50 [==============================] – 21s – loss: 13.5733 – acc: 0.1575 Though my accuracy increased in comparison to the MLP output, it also increased the time taken to run a single epoch – 21 seconds.But the major point to note was that the majority class in the dataset was around 17.6%. So, even if we had predicted the class of every image in the train dataset to be the majority class, we would have performed better than MLP and CNN respectively. Addition of more convolutional blocks substantially increased my training time. This led me to switch onto using pre-trained models where I would not have to train my entire architecture but only a few layers.So, I used VGG16 model which is pre-trained on the ImageNet dataset and provided in the keras library for use. Below is the architecture of the VGG16 model which I used.The only change that I made to the VGG16 existing architecture is changing the softmax layer with 1000 outputs to 16 categories suitable for our problem and re-training the dense layer.This architecture gave me an accuracy of 70% much better than MLP and CNN. Also, the biggest benefit of using the VGG16 pre-trained model was almost negligible time to train the dense layer with greater accuracy.So, I moved forward with this approach of using a pre-trained model and the next step was to fine tune my VGG16 model to suit this problem. What is our objective when we train a neural network? We wish to identify the correct weights for the network by multiple forward and backward iterations. By using pre-trained models which have been previously trained on large datasets, we can directly use the weights and architecture obtained and apply the learning on our problem statement. This is known as transfer learning. We “transfer the learning” of the pre-trained model to our specific problem statement.You should be very careful while choosing what pre-trained model you should use in your case. If the problem statement we have at hand is very different from the one on which the pre-trained model was trained – the prediction we would get would be very inaccurate. For example, a model previously trained for speech recognition would work horribly if we try to use it to identify objects using it.We are lucky that many pre-trained architectures are directly available for us in the Keras library. Imagenet data set has been widely used to build various architectures since it is large enough (1.2M images) to create a generalized model. The problem statement is to train a model that can correctly classify the images into 1,000 separate object categories. These 1,000 image categories represent object classes that we come across in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types etc.These pre-trained networks demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning. We make modifications in the pre-existing model by fine-tuning the model. Since we assume that the pre-trained network has been trained quite well, we would not want to modify the weights too soon and too much. While modifying we generally use a learning rate smaller than the one used for initially training the model. The below diagram should help you decide on how to proceed on using the pre trained model in your case –Scenario 1 – Size of the Data set is small while the Data similarity is very high – In this case, since the data similarity is very high, we do not need to retrain the model. All we need to do is to customize and modify the output layers according to our problem statement. We use the pretrained model as a feature extractor. Suppose we decide to use models trained on Imagenet to identify if the new set of images have cats or dogs. Here the images we need to identify would be similar to imagenet, however we just need two categories as my output – cats or dogs. In this case all we do is just modify the dense layers and the final softmax layer to output 2 categories instead of a 1000.Scenario 2 – Size of the data is small as well as data similarity is very low – In this case we can freeze the initial (let’s say k) layers of the pretrained model and train just the remaining(n-k) layers again. The top layers would then be customized to the new data set. Since the new data set has low similarity it is significant to retrain and customize the higher layers according to the new dataset.  The small size of the data set is compensated by the fact that the initial layers are kept pretrained(which have been trained on a large dataset previously) and the weights for those layers are frozen.Scenario 3 – Size of the data set is large however the Data similarity is very low – In this case, since we have a large dataset, our neural network training would be effective. However, since the data we have is very different as compared to the data used for training our pretrained models. The predictions made using pretrained models would not be effective. Hence, its best to train the neural network from scratch according to your data.Scenario 4 – Size of the data is large as well as there is high data similarity – This is the ideal situation. In this case the pretrained model should be most effective. The best way to use the model is to retain the architecture of the model and the initial weights of the model. Then we can retrain this model using the weights as initialized in the pre-trained model. Let’s now try to use a pretrained model for a simple problem. There are various architectures that have been trained on the imageNet data set. You can go through various architectures here. I have used vgg16 as pretrained model architecture and have tried to identify handwritten digits using it. Let’s see in which of the above scenarios would this problem fall into. We have around 60,000 training images of handwritten digits. This data set is definitely small. So the situation would either fall into scenario 1 or scenario 2. We shall try to solve the problem using both these scenarios. The data set can be downloaded from here.# importing required librariesfrom keras.models import Sequential
from scipy.misc import imread
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pdfrom keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
train=pd.read_csv(""R/Data/Train/train.csv"")
test=pd.read_csv(""R/Data/test.csv"")
train_path=""R/Data/Train/Images/train/""
test_path=""R/Data/Train/Images/test/""from scipy.misc import imresize
# preparing the train datasettrain_img=[]
for i in range(len(train)):    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))    temp_img=image.img_to_array(temp_img)    train_img.append(temp_img)#converting train images to array and applying mean subtraction processingtrain_img=np.array(train_img) 
train_img=preprocess_input(train_img)
# applying the same procedure with the test datasettest_img=[]
for i in range(len(test)):    temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))    temp_img=image.img_to_array(temp_img)    test_img.append(temp_img)test_img=np.array(test_img) 
test_img=preprocess_input(test_img)# loading VGG16 model weights
model = VGG16(weights='imagenet', include_top=False)
# Extracting features from the train dataset using the VGG16 pre-trained modelfeatures_train=model.predict(train_img)
# Extracting features from the train dataset using the VGG16 pre-trained modelfeatures_test=model.predict(test_img)# flattening the layers to conform to MLP inputtrain_x=features_train.reshape(49000,25088)
# converting target variable to arraytrain_y=np.asarray(train['label'])
# performing one-hot encoding for the target variabletrain_y=pd.get_dummies(train_y)
train_y=np.array(train_y)
# creating training and validation setfrom sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42) # creating a mlp model
from keras.layers import Dense, Activation
model=Sequential()model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))
keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)model.add(Dense(500,input_dim=1000,activation='sigmoid'))
keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)model.add(Dense(150,input_dim=500,activation='sigmoid'))
keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)model.add(Dense(units=10))
model.add(Activation('softmax'))model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])# fitting the model model.fit(X_train, Y_train, epochs=20, batch_size=128,validation_data=(X_valid,Y_valid)) 2. Freeze the weights of first few layers – Here what we do is we freeze the weights of the first 8 layers of the vgg16 network, while we retrain the subsequent layers. This is because the first few layers capture universal features like curves and edges that are also relevant to our new problem. We want to keep those weights intact and we will get the network to focus on learning dataset-specific features in the subsequent layers.Code for freezing the weights of first few layers.from keras.models import Sequential
from scipy.misc import imread
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pdfrom keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
from keras.utils.np_utils import to_categoricalfrom sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.optimizers import SGD
from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activationfrom sklearn.metrics import log_loss
train=pd.read_csv(""R/Data/Train/train.csv"")
test=pd.read_csv(""R/Data/test.csv"")
train_path=""R/Data/Train/Images/train/""
test_path=""R/Data/Train/Images/test/""from scipy.misc import imresizetrain_img=[]
for i in range(len(train)):    temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224))    temp_img=image.img_to_array(temp_img)    train_img.append(temp_img)train_img=np.array(train_img) 
train_img=preprocess_input(train_img)test_img=[]
for i in range(len(test)):temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224))    temp_img=image.img_to_array(temp_img)    test_img.append(temp_img)test_img=np.array(test_img) 
test_img=preprocess_input(test_img)from keras.models import Modeldef vgg16_model(img_rows, img_cols, channel=1, num_classes=None):    model = VGG16(weights='imagenet', include_top=True)    model.layers.pop()    model.outputs = [model.layers[-1].output]    model.layers[-1].outbound_nodes = []          x=Dense(num_classes, activation='softmax')(model.output)    model=Model(model.input,x)#To set the first 8 layers to non-trainable (weights will not be updated)          for layer in model.layers[:8]:       layer.trainable = False# Learning rate is changed to 0.001
    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])    return modeltrain_y=np.asarray(train['label'])le = LabelEncoder()train_y = le.fit_transform(train_y)train_y=to_categorical(train_y)train_y=np.array(train_y)from sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)# Example to fine-tune on 3000 samples from Cifar10img_rows, img_cols = 224, 224 # Resolution of inputs
channel = 3
num_classes = 10 
batch_size = 16 
nb_epoch = 10# Load our model
model = vgg16_model(img_rows, img_cols, channel, num_classes)model.summary()
# Start Fine-tuning
model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))# Make predictions
predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)# Cross-entropy loss score
score = log_loss(Y_valid, predictions_valid) Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:I hope that you would now be able to apply pre-trained models to your problem statements. Be sure that the pre-trained model you have selected has been trained on a similar data set as the one that you wish to use it on. There are various architectures people have tried on different types of data sets and I strongly encourage you to go through these architectures and apply them on your own problem statements. Please feel free to discuss your doubts and concerns in the comments section.",https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
Data Scientist -Mumbai (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Transfer learning and the art of using Pre-trained Models in Deep Learning|Strategic Analytics – Bangalore/Mumbai (1-6 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 4 years
Requirements : 
Task Info : Job Description and ResponsibilitiesExcellent verbal and written communication and form liason between business and technical architects and developers, Strong inclination towards consulting engagements involving machine learning and artificial intelligence.Manage clients expectations and manage advanced analytics delivery.Hands on involvement in extracting, collating, performing data integrity checks, manipulating and analysing data Hands on involvement in suggesting and building an appropriate statistical model relevant to the industry vertical Develop analytical strategies for business using model results of predictive modelling, time series forecasting, clustering and segmentation of customers Documenting and presenting work to the client. Proactively recommending and influencing changes in business decision-making. Required skills: At least 1 year of experience in programming, quantitative analysis or as a data scientist
College Preference : tier1-any
Min Qualification : ug
Skills : artificial intelligence, c++, C, java, machine learning, predictive modeling, python, statistics
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-mumbai-1-4-years-of-experience/
Strategic Analytics – Bangalore/Mumbai (1-6 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist -Mumbai (1-4 Years Of Experience)|Business Analyst – Mumbai- (1-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 6 years
Requirements : 
Task Info : Job Description and Responsibilities– Identify and understand trends and make recommendations. Create clear written communications and routinely present results to senior leaders including the ability to influence toward a particular decision– Work with Operations partners and leaders to ensure any new developments will be deliverable and executable in an Operations environment – Perform analysis as it pertains to eligibility and performance of portfolio management strategies. Utilize SAS, SQL, Oracle, Knowledge Seeker and MS Office analysis tools to complete analysis requirements – Partner with technical groups to develop requirements for changes to rules engines and automated strategies – Forecast performance changes that tie to segmentation or treatment changesQualifications – A minimum of 1 years of relevant analytics/modeling experience. Previous experience in credit card risk is strongly preferred – Bachelors or MS degree(preferred) in Statistics, Econometrics, Mathematics or Finance (or equivalent quantitative field) is required – Knowledge of credit card P&L is preferred – Knowledge and experience in gathering requirements, building and presenting strategies, data mining and presentation of findings is a must – Experienced programming knowledge in both SAS and SQL, Operating systems experience with UNIX and relational database knowledge such as ORACLE/TERADATA – Strong analytical, interpretive and problem solving skills with the ability to interpret large amounts of data and its impact in both operational and financial areas – Demonstrated excellent written and oral communication skills to clearly present analytical findings and make business recommendations via the use of Microsoft Word, Excel, and PowerPoint
College Preference : no-bar
Min Qualification : ug
Skills : credit risk, excel, oracle, Power point, sas, sql, statistics, teradata, unix
Location : Bengaluru, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/strategic-analytics-bangaloremumbai-1-6-years-of-experience/
Business Analyst – Mumbai- (1-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Strategic Analytics – Bangalore/Mumbai (1-6 Years Of experience)|Building Trust in Machine Learning Models (using LIME in Python)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 1 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities– Identify & analyze business requirements through discussion with users and direct research – Prepare relevant test scenarios to ensure that testing results correspond to the business expectations – Participate in and manage in systems validation and business-level testing and quality assurance.– Devise, implement and extend, data-modelling solutions in Excel/VBA, Python, R, and associated technologies– Develop portfolio performance reporting, simulations and optimization – Implementation of new functionalities in products – Assist in and author presentation materials – Conducting client trainings and demos, participate and present in client meetings – Understand business process flows for investment management, and other financial markets businesses Required Experience and Skills: – 1-to-5 years of relevant work experience, – Bachelor-level education in streams such as Business, Engineering, Finance, Economics, Statistics, Computer Systems, or equivalent, – Industry credentials such as CA, CFA, FRM or similar, – Experience in financial data analysis and modelling in Excel/VBA, R, Python, C++, and/or similar – Quantitative, analytical and critical thinking approach to solution finding – Demonstrated understanding of business process modeling, business systems development and analysis – Authored research reports and delivered presentations – Worked in/delivered to international team/clients – Excellent presentation, and communication skills 
College Preference : no-bar
Min Qualification : ug
Skills : c++, excel, modeling, python, r, VBA
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-mumbai-1-5-years-of-experience/
Building Trust in Machine Learning Models (using LIME in Python),Learn everything about Analytics|Introduction|Table of Contents|1. Motivation|2. The Problem|3. End Notes,"2.1 Steps for Model Building|2.2 Steps for using Lime to make your model interpretable|Share this:|Like this:|Related Articles|Business Analyst – Mumbai- (1-5 Years Of Experience)|Assistant Manager – Credit Risk – Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience)|
Guest Blog
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The value is not in software, the value is in data, and this is really important for every single company, that they understand what data they’ve got.-John Straw More and more companies are now aware of the power of data. Machine Learning models are increasing in popularity and are now being used to solve a wide variety of business problems using data. Having said that, it is also true that there is always a trade-off between accuracy of models & its interpretability.In general, if accuracy has to be improved, data scientists have to resort to using complicated algorithms like Bagging, Boosting, Random Forests etc. which are “Blackbox” methods. It is no wonder that many of the winning entries in Kaggle or Analytics Vidhya competitions tend to use algorithms like XGBoost, where there is no requirement to explain the process of generating the predictions to a business user.  On the other hand, in a business setting, simpler models that are more interpretable like Linear Regression, Logistic Regression, Decision Trees, etc. are used even if the predictions are less accurate.This situation has got to change – the trade-off between accuracy & interpretability is not acceptable. We need to find ways to use powerful black-box algorithms even in a business setting and still be able to explain the logic behind the predictions intuitively to a business user. With increased trust in predictions, organisations will deploy machine learning models more extensively within the enterprise. The question is – “How do we build trust in Machine Learning Models”?  It is in this context, I find the paper titled “Why Should I Trust You?”- Explaining the Predictions of Any Classifier [1] intriguing & interesting. In this paper [1] , the authors explain a framework called LIME (Locally Interpretable Model-Agnostic Explanations), which is an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.  In the paper, there are many examples of problems where the predictions from black-box algorithms (even as extreme as Deep Learning) can be formulated in an interpretable fashion. I am not going to explain the paper in this blog but rather show how it can be implemented in our own classification problems. Sigma Cab’s Surge Pricing Type ClassificationIn February this year, Analytics Vidhya ran a machine learning competition in which the objective was to predict the “Surge Pricing Type” for Sigma Cab – a taxi aggregation service. This was a multiclass classification problem. In this blog, we will see how to make predictions for this dataset and use LIME to make the predictions interpretable. The intent here is not to build the best possible model but rather the focus is on the aspect of interpretability. # Step 1 – Import all libraries# Step 2 – Define Functions, variables and dictionaries# Step 3 – Load Training dataset# Step 4 – Understand the data (Descriptive Statistics, Visualization)# Step 5 – Data Pre-processing (Handle Missing data & outliers, Feature Engineering, Feature Transformation etc.)# Step 6 – Feature Selection# Step 7 – Create the validation set# Step 8 – Compare Algorithms to find candidate algorithms?# Step 9 – Algorithm(s) Tuning# Step 10 – Finalize Model(s)In this case, we are fitting 3 models on the training data so that we can compare the explanations provided. The 3 models are a) Logistic Regression, b) Random Forests, c) XGBoost. LIME Step 1 – After installing LIME (On ANACONDA distribution – pip install LIME), import the relevant libraries as shown below:LIME Step 2 – Create a lambda function for each classifier that will return the predicted probability for the target variable (surge pricing type) given the set of featuresLIME Step 3 – Create a concatenated list of all feature names which will be utilised by the LIME explainer in subsequent stepsLIME Step 4 – This is the ‘magical’ step that creates the explainerThe parameters used by the function are:LIME Step 5 – Obtain the explanations from LIME for particular values in the validation datasetPick particular observations in the validation dataset to get their probability values for each class. LIME will provide an explanation as to the reason for assigning the probability. Compare the probability values to the actual class of the target variable for that prediction. Output is shown for 2 observations:(Note: The visualisation is not powerful enough to show the feature weights for all classes in a multi-class scenario but the same process is applicable to differentiate between class 1 & 3 also)2.  Id = 45 in validation set: In this case, only Random Forest is able to assign a higher probability to type 1 which is the actual value. Both Logistic Regression & XGBoost predicts that type 2 has a higher probability. Also, when you look at the NOT 2 | 2 table, you can see the weights assigned by different algorithms to each feature. For example, Trip Distance > 0.35 is assigned a weight of 0.01 in the case of Logistic Regression, a weight of 0.02 in the case of Random Forest and 0.01 in the case of Xgboost. Each feature is then color-coded to indicate whether it is contributing to the prediction of 2 (Orange) or NOT 2 (Grey) in the feature-value-table. The Feature-Value table by itself shows the actual values of the features for that particular record (in this case Id = 45)(Note: The visualisation is not powerful enough to show the feature weights for all classes in a multi-class scenario but the same process is applicable to differentiate between class 1 & 3 also) The probability values for each class is different for each algorithm as the feature weights computed by each algorithm are different. Depending on the actual value of the features for a particular record and the weights assigned to those features, the algorithm computes the class probability and then predicts the class having the highest probability. These results can be interpreted by a subject matter expert to see which algorithm is picking up the right signals / features to make the prediction. Essentially, the black box algorithms have become white box in the sense that now we know what drives the algorithms to make its predictions.Here’s the whole code for your reference I hope you are as excited as me after looking at these results. The output of LIME provides an intuition into the inner workings of machine learning algorithms as to the features that are being used to arrive at a prediction. If LIME or similar algorithms can help in providing interpretable output for any type of blackbox algorithm, it will go a long way in getting the buy-in from business users to trust the output of machine learning algorithms. By building such trust, powerful methods can be deployed in a business context achieving the twin benefits of higher accuracy and interpretability. Please do check out the LIME paper for the math behind this fascinating development.References Karthikeyan Sankaran is currently a Director at LatentView Analytics which provides solutions at the intersection of Business, Technology & Math to business problems across a wide range of industries. Karthik has close to two decades of experience in the Information Technology industry having worked in multiple roles across the space of Data Management, Business Intelligence & Analytics.This story was received as part of “The Mightiest Pen” contest on Analytics Vidhya. Karthikeyan’s entry was one of the winning entries in the competition.",https://www.analyticsvidhya.com/blog/2017/06/building-trust-in-machine-learning-models/
Assistant Manager – Credit Risk – Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Building Trust in Machine Learning Models (using LIME in Python)|Business Analyst – Gurgaon-(2-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

 Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities– 3-5 years of total experience of Data Analytics / Data Insights Generation – Hands on experience on SAS and Excel/VBA– Proven experience in delivering Analytics Solutions – Experience in banking industry required – Good communication/problem solving/analytical bent of mind
College Preference : no-bar
Min Qualification : ug
Skills : Data analytics, excel, problem solving, sas, VBA
Location : Bengaluru, Delhi, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/assistant-manager-credit-risk-delhincrbangaloregurgaon-3-6-years-of-experience/
Business Analyst – Gurgaon-(2-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager – Credit Risk – Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience)|Director – Analytics – CPG/Retail Domain- Bangalore- (12-15 Years Of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 7 years
Requirements : 
Task Info : Job Description and Responsibilities :– Team  responsible for developing and executing the most effective collections placement, segmentation, and pricing strategies, as well as driving analytics to improve the operational efficiency of fraud and in-house collections. – Candidate will be responsible for conducting analysis and generating insights into developing the optimal OA placement and pricing structure. – She/He will also work closely with Global Collections team to drive best placement / pricing practices across all markets, and the infrastructure team to ensure flawless execution. This role may be subject to additional background verification checks. – This role may be subject to additional background verification checks. Qualifications : – Strong analytical and problem solving skills. – Experienced in SAS preferred. – Good communication skills, Good relationship building and influencing skills, Motivated, results oriented, and effective at handling multiple projects at the same time. – Advanced degree in business, economics, statistics, operations research, or other quantitative fields preferred. – Thorough understanding of Global Risk Systems (GRMS, WCC, and RMS) or IDN will be a plus for internal candidates 
College Preference : no-bar
Min Qualification : ug
Skills : business analysis, sas, strategy
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analyst-gurgaon-2-7-years-of-experience/
Director – Analytics – CPG/Retail Domain- Bangalore- (12-15 Years Of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst – Gurgaon-(2-7 Years Of Experience)|Understanding and coding Neural Networks From Scratch in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 12 – 15 years
Requirements : 
Task Info : Job Description and Responsibilities :– Contributing to the thought capital through the creation of executive presentations, architecture documents, and IT position papers – Developing and maintaining strong client relations with senior and C-level executives – Leading and mentoring other IT consultants within the account and across analytics practice – Maintain high profitability while addressing the risks and issues of the account – Supporting business development and ensuring high levels of client satisfaction during delivery – Developing new insights into the client’s business model and pain points, and delivering actionable, high-impact results Mandatory Skills : – Should be able to Mentor the team members to groom them for the future roles – Ability to produce high quality outputs under pressure and within deadlines– In depth knowledge of any of CPG/Retail is mandatory and more is a plus. – Knowledge of analytical tool kits like SAS, SPSS, R – Knowledge of data analysisExperience Required :– 12 to 15 years of demonstrated hands-on experience in the area of Business Analytics – At least 5 Full cycle Business Analytics engagements with at-least two engagements with role of Analytics Manager Education :– Bachelor’s degree in Computer Science or a related field preferred (BE/B.Tech, BSc Stats) – Master’s degree in a related field preferred (MBA, MSc Stats) 
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, r, Retail Analytics, sas, spss
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/director-analytics-cpgretail-domain-bangalore-12-15-years-of-experience/
Understanding and coding Neural Networks From Scratch in Python and R,Learn everything about Analytics|Overview||Introduction|Table of Contents:|Simple intuition behind neural networks||Multi Layer Perceptron and its basics|Steps involved in Neural Network methodology|Visualization of steps for Neural Network methodology|Implementing NN using Numpy (Python)|Implementing NN in R|[Optional] Mathematical Perspective of Back Propagation Algorithm|End Notes:,"What is an activation function?||Forward Propagation, Back Propagation and Epochs||Multi-layer perceptron|Full Batch Gradient Descent and Stochastic Gradient Descent|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Director – Analytics – CPG/Retail Domain- Bangalore- (12-15 Years Of Experience )|Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience)|
Sunil Ray
|65 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"You can learn and practice a concept in two ways:I prefer Option 2 and take that approach to learning any new topic. I might not be able to tell you the entire math behind an algorithm, but I can tell you the intuition. I can tell you the best scenarios to apply an algorithm based on my experiments and understanding.In my interactions with people, I find that people don’t take time to develop this intuition and hence they struggle to apply things in the right manner.In this article, I will discuss the building block of a neural network from scratch and focus more on developing this intuition to apply Neural networks. We will code in both “Python” and “R”. By end of this article, you will understand how Neural networks work, how do we initialize weigths and how do we update them using back-propagation.Let’s start.  If you have been a developer or seen one work – you know how it is to search for bugs in a code. You would fire various test cases by varying the inputs or circumstances and look for the output. The change in output provides you a hint on where to look for the bug – which module to check, which lines to read. Once you find it, you make the changes and the exercise continues until you have the right code / application.Neural networks work in very similar manner. It takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as “Forward Propagation“.Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. Each of these neurons are contributing some error to final output. How do you reduce the error?We try to minimize the value/ weight of neurons those are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as “Backward Propagation“.In order to reduce these number of iterations to minimize the error, the neural networks use a common algorithm known as “Gradient Descent”, which helps to optimize the task quickly and efficiently.That’s it – this is how Neural network works! I know this is a very simple representation, but it would help you understand things in a simple manner.Just like atoms form the basics of any material on earth – the basic forming unit of a neural network is a perceptron. So, what is a perceptron?A perceptron can be understood as anything that takes multiple inputs and produces one output. For example, look at the image below.PerceptronThe above structure takes three inputs and produces one output. The next logical question is what is the relationship between input and output? Let us start with basic ways and build on to find more complex ways.Below, I have discussed three ways of creating input output relationships:But, all of this is still linear which is what perceptrons used to be. But that was not as much fun. So, people thought of evolving a perceptron to what is now called as artificial neuron. A neuron applies non-linear transformations (activation function) to the inputs and biases. Activation Function takes the sum of weighted input (w1*x1 + w2*x2 + w3*x3 + 1*b) as an argument and return the output of the neuron. In above equation, we have represented 1 as x0 and b as w0.The activation function is mostly used to make a non-linear transformation which allows us to fit nonlinear hypotheses or to estimate the complex functions. There are multiple activation functions, like: “Sigmoid”, “Tanh”, ReLu and many other.Till now, we have computed the output and this process is known as “Forward Propagation“. But what if the estimated output is far away from the actual output (high error). In the neural network what we do, we update the biases and weights based on the error. This weight and bias updating process is known as “Back Propagation“.Back-propagation (BP) algorithms work by determining the loss (or error) at the output and then propagating it back into the network. The weights are updated to minimize the error resulting from each neuron. The first step in minimizing the error is to determine the gradient (Derivatives) of each node w.r.t. the final output. To get a mathematical perspective of the Backward propagation, refer below section.This one round of forward and back propagation iteration is known as one training iteration aka “Epoch“.Now, let’s move on to next part of Multi-Layer Perceptron. So far, we have seen just a single layer consisting of 3 input nodes i.e x1, x2 and x3 and an output layer consisting of a single neuron. But, for practical purposes, the single-layer network can do only so much. An MLP consists of multiple layers called Hidden Layers stacked in between the Input Layer and the Output Layer as shown below.The image above shows just a single hidden layer in green but in practice can contain multiple hidden layers. Another point to remember in case of an MLP is that all the layers are fully connected i.e every node in a layer(except the input and the output layer) is connected to every node in the previous layer and the following layer.Let’s move on to the next topic which is training algorithm for a neural network (to minimize the error). Here, we will look at most common training algorithm known as Gradient descent. Both variants of Gradient Descent perform the same work of updating the weights of the MLP by using the same updating algorithm but the difference lies in the number of training samples used to update the weights and biases.Full Batch Gradient Descent Algorithm as the name implies uses all the training data points to update each of the weights once whereas Stochastic Gradient uses 1 or more(sample) but never the entire training data to update the weights once.Let us understand this with a simple example of a dataset of 10 data points with two weights w1 and w2.Full Batch: You use 10 data points (entire training data) and calculate the change in w1 (Δw1) and change in w2(Δw2) and update w1 and w2.SGD: You use 1st data point and calculate the change in w1 (Δw1) and change in w2(Δw2) and update w1 and w2. Next, when you use 2nd data point, you will work on the updated weightsFor a more in-depth explanation of both the methods, you can have a look at this article. Let’s look at the step by step building methodology of Neural Network (MLP with one hidden layer, similar to above-shown architecture). At the output layer, we have only one neuron as we are solving a binary classification problem (predict 0 or 1). We could also have two neurons for predicting each of both classes.First look at the broad steps:0.) We take input and output1.) We initialize weights and biases with random values (This is one time initiation. In the next iteration, we will use updated weights, and biases). Let us define:2.) We take matrix dot product of input and weights assigned to edges between the input and hidden layer then add biases of the hidden layer neurons to respective inputs, this is known as linear transformation:hidden_layer_input= matrix_dot_product(X,wh) + bh3) Perform non-linear transformation using an activation function (Sigmoid). Sigmoid will return the output as 1/(1 + exp(-x)).hiddenlayer_activations = sigmoid(hidden_layer_input)4.) Perform a linear transformation on hidden layer activation (take matrix dot product with weights and add a bias of the output layer neuron) then apply an activation function (again used sigmoid, but you can use any other activation function depending upon your task) to predict the outputoutput_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
output = sigmoid(output_layer_input)
All above steps are known as “Forward Propagation“5.) Compare prediction with actual output and calculate the gradient of error (Actual – Predicted). Error is the mean square loss = ((Y-t)^2)/2E = y – output6.) Compute the slope/ gradient of hidden and output layer neurons ( To compute the slope, we calculate the derivatives of non-linear activations x at each layer for each neuron). Gradient of sigmoid can be returned as x * (1 – x).slope_output_layer = derivatives_sigmoid(output)
slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)7.) Compute change factor(delta) at output layer, dependent on the gradient of error multiplied by the slope of output layer activationd_output = E * slope_output_layer8.) At this step, the error will propagate back into the network which means error at hidden layer. For this, we will take the dot product of output layer delta with weight parameters of edges between the hidden and output layer (wout.T).Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)9.) Compute change factor(delta) at hidden layer, multiply the error at hidden layer with slope of hidden layer activationd_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer10.) Update weights at the output and hidden layer: The weights in the network can be updated from the errors calculated for training example(s).wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate
wh =  wh + matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_ratelearning_rate: The amount that weights are updated is controlled by a configuration parameter called the learning rate)11.) Update biases at the output and hidden layer: The biases in the network can be updated from the aggregated errors at that neuron.bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
bout = bout + sum(d_output, axis=0)*learning_rateSteps from 5 to 11 are known as “Backward Propagation“One forward and backward propagation iteration is considered as one training cycle. As I mentioned earlier, When do we train second time then update weights and biases are used for forward propagation.Above, we have updated the weight and biases for hidden and output layer and we have used full batch gradient descent algorithm. We will repeat the above steps and visualize the input, weights, biases, output, error matrix to understand working methodology of Neural Network (MLP).Note:Step 0: Read input and outputStep 1: Initialize weights and biases with random values (There are methods to initialize weights and biases but for now initialize with random values)Step 2: Calculate hidden layer input:
hidden_layer_input= matrix_dot_product(X,wh) + bhStep 3: Perform non-linear transformation on hidden linear input
hiddenlayer_activations = sigmoid(hidden_layer_input)Step 4: Perform linear and non-linear transformation of hidden layer activation at output layeroutput_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
output = sigmoid(output_layer_input)Step 5: Calculate gradient of Error(E) at output layer
E = y-outputStep 6: Compute slope at output and hidden layer
Slope_output_layer= derivatives_sigmoid(output)
Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)Step 7: Compute delta at output layerd_output = E * slope_output_layer*lrStep 8: Calculate Error at hidden layerError_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)Step 9: Compute delta at hidden layerd_hiddenlayer = Error_at_hidden_layer * slope_hidden_layerStep 10: Update weight at both output and hidden layerwout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate
wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rateStep 11: Update biases at both output and hidden layerbh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
bout = bout + sum(d_output, axis=0)*learning_rateAbove, you can see that there is still a good error not close to actual target value because we have completed only one training iteration. If we will train model multiple times then it will be a very close actual outcome. I have completed thousands iteration and my result is close to actual target values ([[ 0.98032096] [ 0.96845624] [ 0.04532167]]).  # input matrix
X=matrix(c(1,0,1,0,1,0,1,1,0,1,0,1),nrow = 3, ncol=4,byrow = TRUE)# output matrix
Y=matrix(c(1,1,0),byrow=FALSE)#sigmoid function
sigmoid<-function(x){
1/(1+exp(-x))
}# derivative of sigmoid function
derivatives_sigmoid<-function(x){
x*(1-x)
}# variable initialization
epoch=5000
lr=0.1
inputlayer_neurons=ncol(X)
hiddenlayer_neurons=3
output_neurons=1#weight and bias initialization
wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=0,sd=1), inputlayer_neurons, hiddenlayer_neurons)
bias_in=runif(hiddenlayer_neurons)
bias_in_temp=rep(bias_in, nrow(X))
bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE)
wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=0,sd=1), hiddenlayer_neurons, output_neurons)bias_out=runif(output_neurons)
bias_out_temp=rep(bias_out,nrow(X))
bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE)
# forward propagation
for(i in 1:epoch){hidden_layer_input1= X%*%wh
hidden_layer_input=hidden_layer_input1+bh
hidden_layer_activations=sigmoid(hidden_layer_input)
output_layer_input1=hidden_layer_activations%*%wout
output_layer_input=output_layer_input1+bout
output= sigmoid(output_layer_input)# Back PropagationE=Y-output
slope_output_layer=derivatives_sigmoid(output)
slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)
d_output=E*slope_output_layer
Error_at_hidden_layer=d_output%*%t(wout)
d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer
wout= wout + (t(hidden_layer_activations)%*%d_output)*lr
bout= bout+rowSums(d_output)*lr
wh = wh +(t(X)%*%d_hiddenlayer)*lr
bh = bh + rowSums(d_hiddenlayer)*lr}
output Let Wi be the weights between the input layer and the hidden layer. Wh be the weights between the hidden layer and the output layer.Now, h=σ (u)= σ (WiX), i.e h is a function of u and u is a function of Wi and X. here we represent our function as σY= σ (u’)= σ (Whh), i.e Y is a function of u’ and u’ is a function of Wh and h.We will be constantly referencing the above equations to calculate partial derivatives.We are primarily interested in finding two terms, ∂E/∂Wi and ∂E/∂Wh i.e change in Error on changing the weights between the input and the hidden layer and change in error on changing the weights between the hidden layer and the output layer.But to calculate both these partial derivatives, we will need to use the chain rule of partial differentiation since E is a function of Y and Y is a function of u’ and u’ is a function of Wi.Let’s put this property to good use and calculate the gradients.∂E/∂Wh = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂Wh), ……..(1)We know E is of the form E=(Y-t)2/2.So, (∂E/∂Y)= (Y-t)Now, σ is a sigmoid function and has an interesting differentiation of the form σ(1- σ). I urge the readers to work this out on their side for verification.So, (∂Y/∂u’)= ∂( σ(u’)/ ∂u’= σ(u’)(1- σ(u’)).But, σ(u’)=Y, So,(∂Y/∂u’)=Y(1-Y)Now, ( ∂u’/∂Wh)= ∂( Whh)/ ∂Wh = hReplacing the values in equation (1) we get,∂E/∂Wh = (Y-t). Y(1-Y).hSo, now we have computed the gradient between the hidden layer and the ouput layer. It is time we calculate the gradient between the input layer and the hidden layer.∂E/∂Wi =(∂ E/∂ h). (∂h/∂u).( ∂u/∂Wi)But, (∂ E/∂ h) = (∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h). Replacing this value in the above equation we get,∂E/∂Wi =[(∂E/∂Y).( ∂Y/∂u’).( ∂u’/∂h)]. (∂h/∂u).( ∂u/∂Wi)……………(2)So, What was the benefit of first calculating the gradient between the hidden layer and the output layer?As you can see in equation (2) we have already computed ∂E/∂Y and ∂Y/∂u’ saving us space and computation time. We will come to know in a while why is this algorithm called the back propagation algorithm.Let us compute the unknown derivatives in equation (2).∂u’/∂h = ∂(Whh)/ ∂h = Wh∂h/∂u = ∂( σ(u)/ ∂u= σ(u)(1- σ(u))But, σ(u)=h, So,(∂Y/∂u)=h(1-h)Now, ∂u/∂Wi = ∂(WiX)/ ∂Wi = XReplacing all these values in equation (2) we get,∂E/∂Wi = [(Y-t). Y(1-Y).Wh].h(1-h).XSo, now since we have calculated both the gradients, the weights can be updated asWh = Wh + η . ∂E/∂WhWi = Wi + η . ∂E/∂WiWhere η is the learning rate.So coming back to the question: Why is this algorithm called Back Propagation Algorithm?The reason is: If you notice the final form of ∂E/∂Wh and ∂E/∂Wi , you will see the term (Y-t) i.e the output error, which is what we started with and then propagated this back to the input layer for weight updation.So, where does this mathematics fit into the code?hiddenlayer_activations=hE= Y-tSlope_output_layer = Y(1-Y)lr = ηslope_hidden_layer = h(1-h)wout = WhNow, you can easily relate the code to the mathematics. This article is focused on the building a Neural Network from scratch and understanding its basic concepts. I hope now you understand the working of a neural network like how does forward and backward propagation work, optimization algorithms (Full Batch and Stochastic gradient descent),  how to update weights and biases, visualization of each step in Excel and on top of that code in python and R.Therefore, in my upcoming article, I’ll explain the applications of using Neural Network in Python and solving real-life challenges related to:I enjoyed writing this article and would love to learn from your feedback. Did you find this article useful? I would appreciate your suggestions/feedback. Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Understanding and coding Neural Networks From Scratch in Python and R|Technical Lead – Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,"1. Use your knowledge of machine learning and classification algorithms to develop code that accurately predicts the output of complex engineering models.2. Apply pattern recognition technologies to recognize features in large GIS databases.
||1. 1-3.5 years of relevant work experience |2. B.Tech / B.E., MSc / M.Tech / ME.|3. Familiar with Java / J2EE.|4. Experience deploying / building a fast, distributed, and fault-tolerant AI infrastructure.","Experience : 1 – 3.5 years
Requirements : 
Task Info : Job Description and ResponsibilitiesDesired Candidate Profil

College Preference : no-bar
Min Qualification : ug
Skills : artificial intelligence, J2EE, java
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/artificial-intelligence-developer-ahmedabad-1-3-years-of-experience/
"Technical Lead – Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience)|Sr. Consultant – Data Scientist-Bengaluru (9-14 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 6 years
Requirements : 
Task Info : Job Responsibilities : – To help designing, innovating and building our next generation ML architecture – Working with a group (Data Science & Machine Learning Group) of ML engineers, Data Scientists and Product Analysts – Define and drive API oriented solutions for data and machine learning services – 6 years of full time programming experience within an operations or technical department. – 3+ years of direct experience with multiple Agile teams. – Be able to distill business objectives into technical solutions through effective system design and architecture – Be able to work independently on a project-by project basis and also work in a collaborative and fast-paced team environment – Be able to provide technical and analytical solutions to evaluate the merits and challenges of a product idea – Create applications on both the server-side and on the web interface – Perform high complexity integration testing and validate all services integrate according to specifications – Responsible for prevention and early detection of defects through verification and validation activities ensuring the integrity and quality of all work products Industry : IT-Software / Software Services Functional Area : IT Software – DBA, Datawarehousing Role Category : Programming & Design Role : Team Lead/Technical Lead Keyskills : Programming, Machine Learning, Artificial Intelligence, Data Science, product engineering, Elastic Search, NLP, DB NoSql – MongoDB, Neo4J, MySql. Desired Candidate Profile Technology Competencies Must Have : – Java, Play, Scala. – DB/NoSql – MongoDB, Neo4J, MySql. Good to have : – Elastic Search, NLP background and Machine Learning Platforms – Strong End User focus, understanding the needs of both technical and non-technical end users. Non-Technical Competencies : – Think big & scale – Thought leadership in respective Technology domains – Passionate & Obsessive about problem solving – Self-starter, Complete Ownership and absolute pro-activeness – Fast Learning curve with eagerness to keep in pace with the tech trends Work Experience : More than 5 years of hands on product engineering experience
College Preference : no-bar
Min Qualification : ug
Skills : artificial intelligence, java, machine learning, MongoDB, nlp, Scala
Location : Bengaluru, Delhi, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/technical-lead-machine-learningartificial-intelligence-mumbai-bengaluru-delhi-2-6-years-of-experience/
Sr. Consultant – Data Scientist-Bengaluru (9-14 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Technical Lead – Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)|Launching Analytics Industry Report 2017 – Trends and Salaries in India|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 9 – 14 years
Requirements : 
Task Info : Job Description:Expertise in AI, ML, Deep Learning, Text MiningExperience of creating a big data analytics using either AWS, Azure ML or Google ML Hands on expert level experience in Spark or StormExperience in image or video analytics Must have right attitude and right level of flexibility to in our kind of environment. (in contrast to LAB environment). Should be a good team PlayerShould have reasonable good communication skills and strong consulting mindset to engage with Global stakeholdersGood to have:Experience of working on Scala is useful but not essential Experience/exposure to Tensorflow will be an advantage Good to have experience in PySpark or something similar Academic Proficiency:Ph.D preferred Or, Masters in Mathematics/Statistics Or, ME/M.Tech in Computer Science Work Experience: Minimum 10 years 
College Preference : no-bar
Min Qualification : pg
Skills : artificial intelligence, aws, deep learning, machine learning, Scala, spark, storm, tensor flow, text mining
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/sr-consultant-data-scientist-bengaluru-9-14-years-of-experience/
Launching Analytics Industry Report 2017 – Trends and Salaries in India,Learn everything about Analytics|Introduction|Analytics Industry Report 2017 – Trends and Salaries in India|Key Insights in the report|A few trends from the report|Download Complete Report,"Share this:|Like this:|Related Articles|Sr. Consultant – Data Scientist-Bengaluru (9-14 Years of Experience)|Data scientist|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Let me start with laying out a recent experience:I was in middle of a large audience up-skilling themselves in analytics. Average experience in room was more than 5 years and people came from diverse background. My first question to the group:Kunal – “How many of you are looking for a transition in Analytics / Data Science?“As expected, a large percentage of the group raised their hands. It felt good that people are increasingly becoming aware about importance of analytics.The group has invested close to INR 4,00,000 (~$6,000) as part of the program they were undergoing. Given the maturity of the audience and the investment they had made, I was hoping that people would have done some real ground work before making the investment. So I asked:Kunal – “How many jobs are there in India for people with advanced analytics / predictive modeling skills?“To my surprise – there was no informed answer in the entire group! I thought may be I need to ask the question in a different manner – so I rephrased itKunal – “Given that you all have made huge investment in upskilling yourself, you would have done some research about expected increase in your salary after this course. So, how much increase do you expect after you have undergone the course.“Surprisingly and sadly – no informed answer again. There were expectations – but little research. Even more surprisingly, this was not a one off occurrence – this happens with most of the people I have been interacting in past few years.It was scenarios like these, which made us think what is the best best way to bring knowledge about Indian Analytics Industry to everyone. I am excited to announce launch of Analytics Industry Report 2017 – Trends and Salaries in India . I am pleased to announce the industry report 2017 for our followers and analytics industry in India. This is the most extensive effort I know of in Indian analytics industry. We, along with Jigsaw Academy have used all our experience / intelligence and perspective to come up with this report. The aim is to provide the ground realities to people about what is happening in industry.So, if you had questions like:This Industry report should help you answer these questions. We had launched a Analytics and Big Data Salary Report 2016 last year. This year, we decided that we will provide the best of insights and trends to make this a complete industry report. Hope you enjoy it.Click Here to go to Download Reports Most in demand tools in industry Average Salary by toolsAverage Salary by location – Indian cities
Salary by cities and experiencePlease enter your Email ID
Are you :

Recruiter
Data Science Professional

Your City

Mumbai
Delhi-NCR
Bengaluru
Chennai
Hyderabad
Kolkata
Pune
Ahmedabad
Indore
Others



* By filling this form you agree to receive jobs alerts and industry newsletter from Analytics Vidhya. Furthermore your account with Analytics Vidhya will also be created, the details of which shall be mailed to you.
Dont worry we will not spam you. ",https://www.analyticsvidhya.com/blog/2017/05/launching-analytics-industry-report-2017-trends-and-salaries-in-india/
Data scientist,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Launching Analytics Industry Report 2017 – Trends and Salaries in India|Senior MSBI Developer- Ahmedabad (4-8 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 4 years
Requirements : 
Task Info : Job Description and Responsibilities
College Preference : tier1-entire
Min Qualification : ug
Skills : 
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-scientist-2/
Senior MSBI Developer- Ahmedabad (4-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data scientist|ETL Developer- Pune (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 9 years
Requirements : 
Task Info : Job Description and ResponsibilitiesThis fulltime in house position is of Senior MSBI Developer (SSIS, SSAS, SSRS) with a software consulting company. We are looking for someone who can contribute to the full development lifecycle of reporting, database, data integration, cube and data warehousing solutions. Working with Microsoft SQL Server and the full Microsoft BI Stack (SSIS, SSAS, SSRS), you will be working on large scale BI projects.
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, sql server, sql server integration service (SSIS), SQL Server Reporting Services (SSRS)
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-msbi-developer-ahmedabad-4-8-years-of-experience/
ETL Developer- Pune (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior MSBI Developer- Ahmedabad (4-8 Years of Experience)|Business Intelligence Analyst- Bangalore (3-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 7 years
Requirements : 
Task Info : Job Description and Responsibilities– Assist in the ongoing development of technical best practices for data movement, data quality, data cleansing and other ETL-related activities.– Provide technical knowledge of Extract/Transform/Load (ETL) solutions for Business Intelligence projects Excellent written and verbal communication skills and be able to lead meetings with technical peers regarding the solution designs.– Ability to communicate with the Business Analysts, Data Modelers and Solution Architects when it comes to converting the ETL design into specific development activities Ability to do ETL development and to supervise and guide ETL development activities of other developers Work closely with project Business Analyst, Data Modeler and BI Lead to ensure that the end to end designs meet the business and data requirements Work closely with Project Manager to develop and update the task plan for ETL work and to keep the manager aware of any critical task issues and dependencies on other teams Supervise, lead and assist the creation of the data layer Perform checkpoints/reviews on presentation layer designs and code being created by the ETL developers Ensure that developers are applying Schumacher Group standards in all deliverables Lead Unit and Integration Testing activities and assist in User Acceptance Testing as neededRequirements:– Bachelor’s Degree in Computer Science, Information Systems, or other related field or equivalent work experience At least 5 years overall experience in IT applications development or consulting related to IT applications development At least 5 years in technical development and leadership roles on BI projects – with some experience along the way doing these types of activities:– Designing and developing with ETL Tools– Designing and developing with Relational Reporting Tools Managing or coordinating the time and activities of other people and other groups Demonstrated experience working on strategy projects and evaluating software in the Business Intelligence, Data Warehouse and/or Data Management space
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, etl
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/etl-developer-pune-3-7-years-of-experience/
Business Intelligence Analyst- Bangalore (3-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|ETL Developer- Pune (3-7 Years Of Experience)|A comprehensive beginners guide to Linear Algebra for Data Scientists|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 6 years
Requirements : 
Task Info : Job Description and Responsibilities﻿
College Preference : tier1-entire
Min Qualification : open
Skills : business intelligence, oracle, Power point
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-intelligence-analyst-bangalore-3-6-years-of-experience/
A comprehensive beginners guide to Linear Algebra for Data Scientists,Learn everything about Analytics|Introduction|Table of contents|1. Motivation – Why learn Linear Algebra?|2. Representation of problems in Linear Algebra|3. Matrix|4. Solving the Problem||5. Eigenvalues and Eigenvectors|6. Singular Value Decomposition|7. End notes,"Scenario 1:|Scenario 2:|Scenario 3:|Scenario 4:|2.1 Visualise the problem|2.2 Let’s complicate the problem|2.3 Planes|3.1 Terms related to Matrix|3.2 Basic operations on matrix|3.3 Representing equations in matrix form|4.1 Row Echelon form|4.2 Inverse of a Matrix|5.1 How to find Eigenvectors of a matrix?|5.2 Use of Eigenvectors in Data Science|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Business Intelligence Analyst- Bangalore (3-6 Years of Experience)|Business Analyst/senior Business Analyst – Data Analytics – BFSI Gurgaon (2-5 Years Of Experience)|
Vikas kumar Yadav
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",4.2.1 Finding Inverse of a matrix|4.2.2 Power of matrices|4.2.3 Application of inverse in Data Science,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"One of the most common questions we get on Analytics Vidhya is,How much maths do I need to learn to be a data scientist?Even though the question sounds simple, there is no simple answer to the the question. Usually, we say that you need to know basic descriptive and inferential statistics to start. That is good to start.But, once you have covered the basic concepts in machine learning, you will need to learn some more math. You need it to understand how these algorithms work. What are their limitations and in case they make any underlying assumptions. Now, there could be a lot of areas to study including algebra, calculus, statistics, 3-D geometry etc.If you get confused (like I did) and ask experts what should you learn at this stage, most of them would suggest / agree that you go ahead with Linear Algebra. But, the problem does not stop there. The next challenge is to figure out how to learn Linear Algebra. You can get lost in the detailed mathematics and derivation and learning them would not help as much! I went through that journey myself and hence decided to write this comprehensive guide.If you have faced this question about how to learn & what to learn in Linear Algebra – you are at the right place. Just follow this guide.And if you’re looking to understand where linear algebra fits into the overall data science scheme, here’s the perfect article:  I would like to present 4 scenarios to showcase why learning Linear Algebra is important, if you are learning Data Science and Machine Learning.What do you see when you look at the image above? You most likely said flower, leaves -not too difficult. But, if I ask you to write that logic so that a computer can do the same for you – it will be a very difficult task (to say the least).You were able to identify the flower because the human brain has gone through million years of evolution. We do not understand what goes in the background to be able to tell whether the colour in the picture is red or black. We have somehow trained our brains to automatically perform this task.But making a computer do the same task is not an easy task, and is an active area of research in Machine Learning and Computer Science in general. But before we work on identifying attributes in an image, let us ponder over a particular question- How does a machine stores this image?You probably know that computers of today are designed to process only 0 and 1. So how can an image such as above with multiple attributes like colour be stored in a computer? This is achieved by storing the pixel intensities in a construct called Matrix. Then, this matrix can be processed to identify colours etc.So any operation which you want to perform on this image would likely use Linear Algebra and matrices at the back end. If you are somewhat familiar with the Data Science domain, you might have heard about the world “XGBOOST” – an algorithm employed most frequently by winners of Data Science Competitions. It stores the numeric data in the form of Matrix to give predictions. It enables XGBOOST to process data faster and provide more accurate results. Moreover, not just XGBOOST but various other algorithms use Matrices to store and process data. Deep Learning- the new buzz word in town employs Matrices to store inputs such as image or speech or text to give a state-of-the-art solution to these problems. Weights learned by a Neural Network are also stored in Matrices. Below is a graphical representation of weights stored in a Matrix. Another active area of research in Machine Learning is dealing with text and the most common techniques employed are Bag of Words, Term Document Matrix etc. All these techniques in a very similar manner store counts(or something similar) of words in documents and store this frequency count in a Matrix form to perform tasks like Semantic analysis, Language translation, Language generation etc. So, now you would understand the importance of Linear Algebra in machine learning. We have seen image, text or any data, in general, employing matrices to store and process data. This should be motivation enough to go through the material below to get you started on Linear Algebra. This is a relatively long guide, but it builds Linear Algebra from the ground up. Let’s start with a simple problem. Suppose that price of 1 ball & 2 bat or 2 ball and 1 bat is 100 units. We need to find price of a ball and a bat.Suppose the price of a bat is Rs ‘x’ and the price of a ball is Rs ‘y’. Values of ‘x’ and ‘y’ can be anything depending on the situation i.e. ‘x’ and ‘y’ are variables.Let’s translate this in mathematical form –2x + y = 100 ...........(1)Similarly, for the second condition-x + 2y  =  100 ..............(2)Now, to find the prices of bat and ball, we need the values of ‘x’ and ‘y’ such that it satisfies both the equations. The basic problem of linear algebra is to find these values of ‘x’ and ‘y’ i.e. the solution of a set of linear equations.Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors.The number of variables as well as the number of equations may vary depending upon the condition, but the representation is in form of matrices and vectors. It is usually helpful to visualize data problems. Let us see if that helps in this case.Linear equations represent flat objects. We will start with the simplest one to understand i.e. line. A line corresponding to an equation is the set of all the points which satisfy the given equation. For example,Points (50,0) , (0,100), (100/3,100/3) and (30,40) satisfy our  equation (1) . So these points should lie on the line corresponding to our equation (1). Similarly, (0,50),(100,0),(100/3,100/3) are some of the points that satisfy equation (2).Now in this situation, we want both of the conditions to be satisfied i.e. the point which lies on both the lines.  Intuitively, we want to find the intersection point of both the lines as shown in the figure below.Let’s solve the problem by elementary algebraic operations like addition, subtraction and substitution.2x + y = 100 .............(1)x + 2y = 100 ..........(2)from equation (1)-y = (100- x)/2put value of y in equation (2)-x + 2*(100-x)/2 = 100......(3) Now, since the equation (3) is an equation in single variable x, it can be solved for x and subsequently y.That looks simple – let’s go one step further and explore. Now, suppose you are given a set of three conditions with three variables each as given below and asked to find the values of all the variables. Let’s solve the problem and see what happens.x+y+z=1.......(4)2x+y=1......(5)5x+3y+2z=4.......(6)From equation (4) we get,z=1-x-y....(7)Substituting value of z in equation (6), we get –5x+3y+2(1-x-y)=43x+y=2.....(8)Now, we can solve equations (8) and (5) as a case of two variables to find the values of ‘x’ and ‘y’ in the problem of bat and ball above. Once we know‘x’ and ‘y’, we can use (7)  to find the value of ‘z’.As you might see, adding an extra variable has tremendously increased our efforts for finding the solution of the problem. Now imagine having 10 variables and 10 equations. Solving 10 equations simultaneously can prove to be tedious and time consuming. Now dive into data science. We have millions of data points. How do you solve those problems?We have millions of data points in a real data set. It is going to be a nightmare to reach to solutions using the approach mentioned above. And imagine if we have to do it again and again and again. It’s going to take ages before we can solve this problem. And now if I tell you that it’s just one part of the battle, what would you think? So, what should we do? Should we quit and let it go? Definitely NO. Then?Matrix is used to solve a large set of linear equations. But before we go further and take a look at matrices, let’s visualise the physical meaning of our problem. Give a little bit of thought to the next topic. It directly relates to the usage of Matrices. A linear equation in 3 variables represents the set of all points whose coordinates satisfy the equations. Can you figure out the physical object represented by such an equation? Try to think of 2 variables at a time in any equation and then add the third one. You should figure out that it represents a three-dimensional analogue of line.Basically, a linear equation in three variables represents a plane. More technically, a plane is a flat geometric object which extends up to infinity.As in the case of a line, finding solutions to 3 variables linear equation means we want to find the intersection of those planes. Now can you imagine, in how many ways a set of three planes can intersect? Let me help you out. There are 4 possible cases –Can you imagine the number of solutions in each case? Try doing this. Here is an aid picked from Wikipedia to help you visualise.So, what was the point of having you to visualise all graphs above?Normal humans like us and most of the super mathematicians can only visualise things in 3-Dimensions, and having to visualise things in 4 (or 10000) dimensions is difficult impossible for mortals. So, how do mathematicians deal with higher dimensional data so efficiently? They have tricks up their sleeves and Matrices is one such trick employed by mathematicians to deal with higher dimensional data.Now let’s proceed with our main focus i.e. Matrix. Matrix is a way of writing similar things together to handle and manipulate them as per our requirements easily. In Data Science, it is generally used to store information like weights in an Artificial Neural Network while training various algorithms. You will be able to understand my point by the end of this article.Technically, a matrix is a 2-D array of numbers (as far as Data Science is concerned). For example look at the matrix A below.Generally, rows are denoted by ‘i’ and column are denoted by ‘j’.  The elements are indexed by ‘i’th row and ‘j’th column.We denote the matrix by some alphabet e.g.  A and its elements by A(ij).In above matrixA12 =  2To reach to the result, go along first row and reach to second column. Order of matrix – If a matrix has 3 rows and 4 columns, order of the matrix is 3*4 i.e. row*column.Square matrix – The matrix in which the number of rows is equal to the number of columns.Diagonal matrix – A matrix with all the non-diagonal elements equal to 0 is called a diagonal matrix.Upper triangular matrix – Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix – Square matrix with all the elements above the diagonal equal to 0.Scalar matrix – Square matrix with all the diagonal elements equal to some constant k.Identity matrix – Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix –  The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix –  A matrix consisting only of row.Trace – It is the sum of all the diagonal elements of a square matrix. Let’s play with matrices and realise the capabilities of matrix operations.Addition – Addition of matrices is almost similar to basic arithmetic addition. All you need is the order of all the matrices being added should be same. This point will become obvious once you will do matrix addition by yourself.Suppose we have 2 matrices ‘A’ and ‘B’ and the resultant matrix after the addition is ‘C’. ThenCij  =   Aij + BijFor example, let’s take two matrices and solve them.A      =B    =Then,C        =Observe that to get the elements of C matrix, I have added A and B element-wise i.e. 1 to 4, 3 to 5 and so on.Scalar Multiplication –  Multiplication of a matrix with a scalar constant is called scalar multiplication. All we have to do in a scalar multiplication is to multiply each element of the matrix with the given constant.  Suppose we have a constant scalar ‘c’ and a matrix ‘A’.  Then multiplying ‘c’ with ‘A’  gives-c[Aij] =  [c*Aij]
Transposition – Transposition simply means interchanging the row and column index. For example-AijT= AjiTranspose is used in vectorized implementation of linear and logistic regression.Code in pythonCode in ROutput Matrix multiplicationMatrix multiplication is one of the most frequently used operations in linear algebra. We will learn to multiply two matrices as well as go through its important properties.Before landing to algorithms, there are a few points to be kept in mind.Don’t worry if you can’t get these points. You will be able to understand by the end of this section.Suppose, we are given two matrices A and B to multiply. I will write the final expression first and then will explain the steps.I have picked this image from Wikipedia for your better understanding.In the first illustration, we know that the order of the resulting matrix should be 3*3. So first of all, create a matrix of order 3*3. To determine (AB)ij , multiply each element of ‘i’th row of A with ‘j’th column of B one at a time and add all the terms. To help you understand element-wise multiplication, take a look at the code below.import numpy as npA=np.arange(21,30).reshape(3,3)
B=np.arange(31,40).reshape(3,3)A.dot(B)So, how did we get 2250 as first element of AB matrix?  2250=21*31+22*34+23*37. Similarly, for other elements.Code in RNotice the difference between AB and BA.Properties of matrix multiplicationABC =  (AB)C = A(BC)import numpy as np
A=np.arange(21,30).reshape(3,3)
B=np.arange(31,40).reshape(3,3)
C=np.arange(41,50).reshape(3,3)temp1=(A.dot(B)).dot(C)temp2=A.dot((B.dot(C)))2. Matrix multiplication is not commutative i.e. AB and  BA are not equal. We have verified this result above.Matrix multiplication is used in linear and logistic regression when we calculate the value of output variable by parameterized vector method. As we have learned the basics of matrices, it’s time to apply them. Let me do something exciting for you.  Take help of pen and paper and try to find the value of the matrix multiplication shown below It can be verified very easily that the expression contains our three equations. We will name our matrices as ‘A’, ‘X’ and ‘Z’.It explicitly verifies that we can write our equations together in one place asAX   = ZNext step has to be solution methods.We will go through two methods to find the solution. Now, we will look in detail the two methods to solve matrix equations.Now you have visualised what an equation in 3 variables represents and had a warm up on matrix operations. Let’s find the solution of the set of equations given to us to understand our first method of interest and explore it later in detail.I have already illustrated that solving the equations by substitution method can prove to be tedious and time taking. Our first method introduces you with a neater and more systematic method to accomplish the job in which, we manipulate our original equations systematically to find the solution.  But what are those valid manipulations? Are there any qualifying criteria they have to fulfil? Well, yes. There are two conditions which have to be fulfilled by any manipulation to be valid.So, what are those manipulations?These points will become more clear once you go through the algorithm and practice it. The basic idea is to clear variables in successive equations and form an upper triangular matrix. Equipped with prerequisites, let’s get started. But before that, it is strongly recommended to go through this link for better understanding. I will solve our original problem as an illustration. Let’s do it in steps.What I have done is I have just concatenated the two matrices. The augmented matrix simply tells that the elements in a row are coefficients of ‘x’, ‘y’ and ‘z’ and last element in the row is right-hand side of the equation.Remember to make each leading coefficient, also called pivot equal to 1, by suitable manipulations; in this case multiplying row 2 with -1. Also, if a row consists of 0 only, it should be below each row which consists of a non-zero entry. The resulting form of Matrix is called Row Echelon form. Notice that the planes corresponding to new equations formed by manipulation are not equivalent. Doing these operations, we are just conserving the solution of equations and trying to reach to it.0*x+0*y+1*z=1
z=1Now retrieve equation (2) and put the value of ‘z’ in it to find ‘y’. Do the same for equation (1).Isn’t it pretty simple and clean?Let’s ponder over another point. Will we always be able to make an upper triangular matrix which gives a unique solution? Are there different cases possible? Recall that planes can intersect in multiple ways. Take your time to figure it out and then proceed further.Different possible cases-Note that in last equation, 0=0 which is always true but it seems like we have got only 2 equations. One of the equations is redundant. In many cases, it’s also possible that the number of redundant equations is more than one. In this case, the number of solutions is infinite.Let’s retrieve the last equation.0*x+0*y+0*z=40=4Is it possible? Very clear cut intuition is NO. But, does this signify something? It’s analogous to saying that it is impossible to find a solution and indeed, it is true. We can’t find a solution for such a set of equations. Can you think what is happening actually in terms of planes? Go back to the section where we saw planes intersecting and find it out.Note that this method is efficient for a set of 5-6 equations. Although the method is quite simple, if equation set gets larger, the number of times you have to manipulate the equations becomes enormously high and the method becomes inefficient.Rank of a matrix – Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.A set of vectors is linearly dependent if we can express at least one of the vectors as a linear combination of remaining vectors in the set. For solving a large number of equations in one go, the inverse is used. Don’t panic if you are not familiar with the inverse. We will do a good amount of work on all the required concepts. Let’s start with a few terms and operations.Determinant of a Matrix – The concept of determinant is applicable to square matrices only. I will lead you to the generalised expression of determinant in steps. To start with, let’s take a 2*2 matrix  A.For now, just focus on 2*2 matrix. The expression of determinant of the matrix A will be:det(A) =a*d-b*cNote that det(A) is a standard notation for determinant. Notice that all you have to do to find determinant in this case is to multiply diagonal elements together and put a positive or negative sign before them. For determining the sign, sum the indices of a particular element. If the sum is an even number, put a positive sign before the multiplication and if the sum is odd, put a negative sign.  For example, the sum of indices of element ‘a11’ is 2. Similarly the sum of indices of element ‘d’ is 4. So we put a positive sign before the first term in the expression.  Do the same thing for the second term yourself.Now take a 3*3 matrix ‘B’ and find its determinant.I am writing the expression first and then will explain the procedure step by step.Each term consists of two parts basically i.e. a submatrix and a coefficient. First of all, pick a constant. Observe that coefficients are picked from the first row only. To start with, I have picked the first element of the first row. You can start wherever you want. Once you have picked the coefficient, just delete all the elements in the row and column corresponding to the chosen coefficient. Next, make a matrix of the remaining elements; each one in its original position after deleting the row and column and find the determinant of this submatrix . Repeat the same procedure for each element in the first row. Now, for determining the sign of the terms, just add the indices of the coefficient element. If it is even, put a positive sign and if odd, put a negative sign. Finally, add all the terms to find the determinant. Now, let’s take a higher order matrix ‘C’ and generalise the concept. Try to relate the expression to what we have done already and figure out the final expression.Code in pythonimport numpy as np
 #create a 4*4 matrix
 arr = np.arange(100,116).reshape(4,4)#find the determinant
 np.linalg.det(arr)Code in R  Minor of a matrixLet’s take a square matrix A. then minor corresponding to an element A(ij)  is the determinant of the submatrix formed by deleting the ‘i’th  row and ‘j’th column of the matrix. Hope you can relate with what I have explained already in the determinant section. Let’s take an example.To find the minor corresponding to element A11, delete first row and first column to find the submatrix.Now find the determinant of this matrix as explained already. If you calculate the determinant of this matrix, you should get 4. If we denote minor by M11, thenM11 = 4Similarly, you can do for other elements.Cofactor of a matrixIn the above discussion of minors, if we consider signs of minor terms, the resultant we get is called cofactor of a matrix. To assign the sign, just sum the indices of the corresponding element. If it turns out to be even, assign positive sign. Else assign negative. Let’s take above illustration as an example. If we add the indices i.e. 1+1=2, so we should put a positive sign. Let’s say it C11. ThenC11 = 4You should find cofactors corresponding to other elements by yourself for a good amount of practice.Cofactor matrixFind the cofactor corresponding to each element. Now in the original matrix, replace the original element by the corresponding cofactor. The matrix thus found is called the cofactor matrix corresponding to the original matrix.For example, let’s take our matrix A. if you have found out the cofactors corresponding to each element, just put them in a matrix according to rule stated above. If you have done it right, you should get cofactor matrixAdjoint of a matrix – In our journey to find inverse, we are almost at the end. Just keep hold of the article for a couple of minutes and we will be there. So, next we will find the adjoint of a matrix.Suppose we have to find the adjoint of a matrix A. we will do it in two steps.In step 1, find the cofactor matrix of A.In step 2, just transpose the cofactor matrix.The resulting matrix is the adjoint of the original matrix. For illustration, lets find the adjoint of our matrix A. we already have cofactor matrix C. Transpose of cofactor matrix should beFinally, in the next section, we will find the inverse. Do you remember the concept of the inverse of a number in elementary algebra? Well, if there exist two numbers such that upon their multiplication gives 1 then those two numbers are called inverse of each other. Similarly in linear algebra, if there exist two matrices such that their multiplication yields an identity matrix then the matrices are called inverse of each other. If you can not get what I explained, just go with the article. It will come intuitively to you. The best way to learning is learning by doing. So, let’s jump straight to the algorithm for finding the inverse of a matrix A. Again, we will do it in two steps.Step 1: Find out the adjoint of the matrix A by the procedure explained in previous sections.Step2: Multiply the adjoint matrix by the inverse of determinant of the matrix A. The resulting matrix is the inverse of A.For example, let’s take our matrix A and find it’s inverse. We already have the adjoint matrix. Determinant of matrix A comes to be -2. So, its inverse will beNow suppose that the determinant comes out to be 0. What happens when we invert the determinant i.e. 0?  Does it make any sense?  It indicates clearly that we can’t find the inverse of such a matrix. Hence, this matrix is non-invertible. More technically, this type of matrix is called a singular matrix.Keep in mind that the resultant of multiplication of a matrix and its inverse is an identity matrix. This property is going to be used extensively in equation solving.Inverse is used in finding parameter vector corresponding to minimum cost function in linear regression. What happens when we multiply a number by 1? Obviously it remains the same. The same is applicable for an identity matrix i.e. if we multiply a matrix with an identity matrix of the same order, it remains same.Lets solve our original problem with the help of matrices. Our original problem represented in matrix was as shown belowAX = Z i.e.What happens when we pre multiply both the sides with inverse of coefficient matrix i.e. A. Lets find out by doing.A-1 A X =A-1 ZWe can manipulate it as,(A-1 A) X = A -1ZBut we know multiply a matrix with its inverse gives an Identity Matrix. So,IX =  A -1ZWhere I is the identity matrix of the corresponding order.If you observe keenly, we have already reached to the solution. Multiplying identity matrix to X does not change it. So the equation becomesX = A -1ZFor solving the equation, we have to just find the inverse. It can be very easily done by executing a few lines of codes. Isn’t it a really powerful method?Code for inverse in pythonimport numpy as np
#create an array arr1
arr1 = np.arange(5,21).reshape(4,4)#find the inverse
np.linalg.inv(arr1) Inverse is used to calculate parameter vector by normal equation in linear equation. Here is an illustration. Suppose we are given a data set as shown below- It describes the different variables of different baseball teams to predict whether it makes to playoffs or not. But for right now to make it a regression problem, suppose we are interested in predicting OOBP from the rest of the variables. So, ‘OOBP’ is our target variable. To solve this problem using linear regression, we have to find parameter vector. If you are familiar with Normal equation method, you should have the idea that to do it, we need to make use of Matrices. Lets proceed further and denote our Independent variables below as matrix ‘X’.This data is a part of a data set taken from analytics edge. Here is the link for the data set.so,  X= To find the final parameter vector(θ) assuming our initial function is parameterised by θ and X , all you have to do is to find the inverse of (XT X) which can be accomplished very easily by using code as shown below.First of all, let me make the Linear Regression formulation easier for you to comprehend.f θ (X)= θT X, where θ is the parameter we wish to calculate and X is the column vector of features or independent variables.import pandas as pd
import numpy as np#you don’t need to bother about the following. It just #transforms the data from original source into matrixDf = pd.read_csv( ""../baseball.csv”)
Df1 = df.head(14)# We are just taking 6 features to calculate θ.
X = Df1[[‘RS’, ‘RA’, ‘W’, ‘OBP’,'SLG','BA']]
Y=Df1['OOBP']#Converting X to matrix
X = np.asmatrix(X)#taking transpose of X and assigning it to x
x= np.transpose(X)#finding multiplication
T= x.dot(X)#inverse of T - provided it is invertible otherwise we use pseudoinverse
inv=np.linalg.inv(T)#calculating θ
theta=(inv.dot(X.T)).dot(Y)Imagine if you had to solve this set of equations without using linear algebra. Let me remind you that this data set is less than even 1% of original date set. Now imagine if you had to find parameter vector without using linear algebra. It would have taken a lots of time and effort and could be even impossible to solve sometimes.One major drawback of normal equation method when the number of features is large is that it is computationally very costly. The reason is that if there are ‘n’ features, the matrix (XT X) comes to be the order n*n and its solution costs time of order O( n*n*n). Generally, normal equation method is applied when a number of features is of the order of 1000 or 10,000. Data sets with a larger number of features are handled with the help another method called Gradient Descent.Next, we will go through another advanced concept of linear algebra called Eigenvectors. Eigenvectors find a lot of applications in different domains like computer vision, physics and machine learning. If you have studied machine learning and are familiar with Principal component analysis algorithm, you must know how important the algorithm is when handling a large data set. Have you ever wondered what is going on behind that algorithm? Actually, the concept of Eigenvectors is the backbone of this algorithm. Let us explore Eigen vectors and Eigen values for a better understanding of it.Let’s multiply a 2-dimensional vector with a 2*2 matrix and see what happens.This operation on a vector is called linear transformation.  Notice that the directions of input and output vectors are different. Note that the column matrix denotes a vector here.I will illustrate my point with the help of a picture as shown below. In the above picture, there are two types of vectors coloured in red and yellow and the picture is showing the change in vectors after a linear transformation. Note that on applying a linear transformation to yellow coloured vector, its direction changes but the direction of the red coloured vector doesn’t change even after applying the linear transformation. The vector coloured in red is an example of Eigenvector.Precisely, for a particular matrix; vectors whose direction remains unchanged even after applying linear transformation with the matrix are called Eigenvectors for that particular matrix. Remember that the concept of Eigen values and vectors is applicable to square matrices only. Another thing to know is that I have taken a case of two-dimensional vectors but the concept of Eigenvectors is applicable to a space of any number of dimensions. Suppose we have a matrix A and an Eigenvector ‘x’ corresponding to the matrix. As explained already, after multiplication with matrix the direction of ‘x’ doesn’t change. Only change in magnitude is permitted. Let us write it as an equation-Ax = cx(A-c)x = 0  …….(1)Please note that in the term (A-c), ‘c’ denotes an identity matrix of the order equal to ‘A’ multiplied by a scalar ‘c’We have two unknowns ‘c’ and ‘x’ and only one equation. Can you think of a trick to solve this equation?In equation (1), if we put the vector ‘x’ as zero vector, it makes no sense. Hence, the only choice is that (A-c) is a singular matrix. And singular matrix has a property that its determinant equals to 0. We will use this property to find the value of ‘c’.Det(A-c) = 0Once you find the determinant of the matrix (A-c) and equate to 0, you will get an equation in ‘c’ of the order depending upon the given matrix A. all you have to do is to find the solution of the equation. Suppose that we find solutions as ‘c1’ , ‘c2’ and so on. Put ‘c1’ in equation (1) and find the vector ‘x1’ corresponding to ‘c1’. The vector ‘x1’ that you just found is an Eigenvector of A. Now, repeat the same procedure with ‘c2’, ‘c3’ and so on.Code for finding EigenVectors in pythonimport  numpy as np#create an array
arr = np.arange(1,10).reshape(3,3)#finding the Eigenvalue and Eigenvectors of arr
np.linalg.eig(arr)Code in R for finding Eigenvalues and Eigenvectors:OutputThe concept of Eigenvectors is applied in a machine learning algorithm Principal Component Analysis. Suppose you have a data with a large number of features i.e. it has a very high dimensionality. It is possible that there are redundant features in that data. Apart from this, a large number of features will cause reduced efficiency and more disk space. What PCA does is that it craps some of lesser important features. But how to determine those features? Here, Eigenvectors come to our rescue.Let’s go through the algorithm of PCA. Suppose we have an ‘n’ dimensional data and we want to reduce it to ‘k’ dimensions. We will do it in steps.Step 1: Data is mean normalised and feature scaled.Step 2: We find out the covariance matrix of our data set.Now we want to reduce the number of features i.e. dimensions. But cutting off features means loss of information. We want to minimise the loss of information i.e. we want to keep the maximum variance. So, we want to find out the directions in which variance is maximum. We will find these directions in the next step.Step 3: We find out the Eigenvectors of the covariance matrix. You don’t need to bother much about covariance matrix. It’s an advanced concept of statistics.  As we have data in ‘n’ dimensions, we will find ‘n’ Eigenvectors corresponding to ‘n’ Eigenvalues.Step 4: We will select ‘k’ Eigenvectors corresponding to the ‘k’ largest Eigenvalues and will form a matrix in which each Eigenvector will constitute a column. We will call this matrix as U.Now it’s the time to find the reduced data points. Suppose you want to reduce a data point ‘a’ in the data set to ‘k’ dimensions.  To do so, you have to just transpose the matrix U and multiply it with the vector ‘a’. You will get the required vector in ‘k’ dimensions.Once we are done with Eigenvectors, let’s talk about another advanced and highly useful concept in Linear algebra called Singular value decomposition, popularly called as SVD. Its complete understanding needs  a rigorous study of linear algebra.  In fact, SVD is a complete blog in itself. We will come up with another blog completely devoted to SVD. Stay tuned for a better experience. For now, I will just give you a glimpse of how SVD helps in data science. Suppose you are given a feature matrix A. As suggested by name, what we do is we decompose our matrix A in three constituent matrices for a special purpose.  Sometimes, it is also said that svd is some sort of generalisation of Eigen value decomposition.  I will not go into its mathematics for the reason already explained and will stick to our plan i.e. use of svd in data science.Svd is used to remove the redundant features in a data set. Suppose you have a data set which comprises of 1000 features. Definitely, any real data set with such a large number of features is bound to contain redundant features. if you have run ML, you should be familiar with the fact that Redundant features cause a lots of problems in running machine learning algorithms. Also, running an algorithm on the original data set will be time inefficient and will require a lot of memory. So, what should you to do handle such a problem? Do we have a choice?  Can we omit some features? Will it lead to significant amount of information loss? Will we be able to get an efficient enough algorithm even after omitting the rows? I will answer these questions with the help of an illustration.Look at the pictures shown below taken from this linkWe can convert this tiger into black and white and can think of it as a matrix whose elements represent the pixel intensity as relevant location. In simpler words, the matrix contains information about the intensity of pixels of the image in the form of rows and columns. But, is it necessary to have all the columns in the intensity matrix? Will we be able to represent the tiger with a lesser amount of information? The next picture will clarify my point. In this picture, different images are shown corresponding to different ranks with different resolution. For now, just assume that higher rank implies the larger amount of information about pixel intensity. The image is taken from this linkIt is clear that we can reach to a pretty well image with 20 or 30 ranks instead of 100 or 200 ranks and that’s what we want to do in a case of highly redundant data. What I want to convey is that to get a reasonable hypothesis, we don’t have to retain all the information present in the original dataset. Even, some of the features cause a problem in reaching a solution to the best algorithm. For the example, presence of redundant features causes multi co-linearity in linear regression. Also, some features are not significant for our model. Omitting these features helps to find a better fit of algorithm along with time efficiency and lesser disk space. Singular value decomposition is used to get rid of the redundant features present in our data. If you have made this far – give yourself a pat at the back. We have covered different aspects of Linear algebra in this article. I have tried to give sufficient amount of information as well as keep the flow such that everybody can understand the concepts and be able to do necessary calculations. Still, if you get stuck somewhere, feel free to comment below or post on discussion portal. ",https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
Business Analyst/senior Business Analyst – Data Analytics – BFSI Gurgaon (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|A comprehensive beginners guide to Linear Algebra for Data Scientists|Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : Job Description and Responsibilities– 2-5 years of total experience of Data Analytics / Data Insights Generation – Proven experience in delivering Analytics Solutions – Experience in credit card / banking industry required – Hands on experience on SAS and Excel/VBA – Good communication/ problem solving/ analytical bent of mind.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, bfsi, Data analytics, excel, sas, VBA
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analystsenior-business-analyst-data-analytics-bfsi-gurgaon-2-5-years-of-experience/
Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst/senior Business Analyst – Data Analytics – BFSI Gurgaon (2-5 Years Of Experience)|Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 8 – 12 years
Requirements : 
Task Info : Key Roles & Responsibilities:Key Requirements – Experience & Skills
College Preference : no-bar
Min Qualification : ug
Skills : analytics, bfsi, business analysis, business analytics, insurance, risk
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-manager-business-analyst-insurance-domain-gurgaon-8-12-years-of-experience/
"Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience)|25 Must Know Terms & concepts for Beginners in Deep Learning|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 5 years
Requirements : 
Task Info : Responsibilities:Responsible for understanding Worldwide Alliance &Channel(WWA&C) business objectives and delivering the reporting needs to the business.Responsible for analyzing partner programs and sales data to measure key performance indicator.Responsible for creating and maintaining sales pipeline and forecasting reports for WWA&C.Manage organizational relationships and partner across different groups to drive reporting solutions in different areas within WWA&CResponsible for process documentation for all of BI systems with WWA&C.Support testing and report updates to management.Collaborate with subject matter experts from different LOBs to standardize reporting within OracleDeveloping and presenting proposals of project plan and getting agreement from appropriate stakeholders.Lead contributor individually and as a team member, providing direction and mentoring to others.Skills / Competencies Required:Good working knowledge of Oracle systems, Oracle Sales Cloud(OSC) , Oracle EBusiness SuiteMust have strong analytical skillsMust have experience with analytical reporting on OBIEEGood SQL skills to be able to query databases for data analysisBasic understanding of data model schemas, ETL programming using PL/SQL.Experience with Data Visualization tools (Oracle Data Visualization , Tableau etc..) would be a plus.Strong knowledge of MS Office, in particular the ability to use Excel for data analysis and data manipulationShould be able to do functional and technical presentations to the team.Good communication skills required: must be responsive, clear, and conciseGood problem-solving and analytical skillsStrong levels of enthusiasm, commitment, and energy requiredStrong team ethic: promote team spirit and motivate / lead as and when necessary to ensure good project outcomesStrong planning and coordinating skillsAbility to work under pressure and meet deadlines in a fast-paced environmentPrior experience working in an international environment is an advantageProcess and workflow improvement experience is an advantageBasic Qualifications:Bachelors in Computer Science or equivalent with 4 – 5 years of related experienceEducation:UG -B.Tech/B.E. – Any Specialization, Computers, B.Sc – Computers, BCA – Computers
College Preference : no-bar
Min Qualification : ug
Skills : business analytics, data visualization, etl, oracle, PLSQL, sql, tableau
Location : Bengaluru, Gurugram, Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analyst-noida-bengaluru-gurgaon-4-5-years-of-experience/
25 Must Know Terms & concepts for Beginners in Deep Learning,Learn everything about Analytics|Who should read this article?|Terms related to topics:|Basics of Neural Networks|Convolutional Neural Networks|Recurrent Neural Network|End Notes,"Commonly applied Activation Functions|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)|Senior Analyst / Manager – Qlikview Developer- Mumbai (3-8 Years of Experience)|
Dishashree Gupta
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,Source: Wikipedia|source: cs231n|Source: cs231n|Source: Original paper|Source: cs231n|Source: cs231n|Source: cs231n,"Artificial Intelligence, deep learning, machine learning — whatever you’re doing if you don’t understand it — learn it. Because otherwise you’re going to be a dinosaur within 3 years.-Mark CubanThis statement from Mark Cuban might sound drastic – but its message is spot on! We are in the middle of a revolution – a revolution caused by Big Huge data and a ton of computational power.For a minute, think how a person would feel in early 20th century if he / she did not understand electricity. You would have been used to doing things in a particular manner for ages and all of a sudden things around you started changing. Things which required many people can now be done with one person and electricity. We are going through a similar journey with machine learning & deep learning today.So, if you haven’t explored or understood the power of deep learning – you should start it today. I have written this article to help you understand common terms used in deep learning. If you are some one who wants to learn or understand deep learning, this article is meant for you. In this article, I will explain various terms used commonly in deep learning.If you are wondering why I am writing this article – I am writing it because I want you to start your deep learning journey without hassle or without getting intimidated. When I first began reading about deep learning, there were several terms I had heard about, but it was intimidating when I tried to understand them. There are several words which are recurring when we start reading about any deep learning application.In this article, I have created something like a deep learning dictionary for you which you can refer whenever you need the basic definition of the most common terms used. I hope after this article these terms wouldn’t haunt you anymore. To help you understand various terms, I have broken them in 3 different groups. If you are looking for a specific term, you can skip to that section. If you are new to the domain, I would recommend that you go through them in the order I have written them. 1) Neuron- Just like a neuron forms the basic element of our brain, a neuron forms the basic structure of a neural network. Just think of what we do when we get new information. When we get the information, we process it and then we generate an output. Similarly, in case of a neural network, a neuron receives an input, processes it and generates an output which is either sent to other neurons for further processing or it is the final output. 2) Weights – When input enters the neuron, it is multiplied by a weight. For example, if a neuron has two inputs, then each input will have has an associated weight assigned to it. We initialize the weights randomly and these weights are updated during the model training process. The neural network after training assigns a higher weight to the input it considers more important as compared to the ones which are considered less important. A weight of zero denotes that the particular feature is insignificant.Let’s assume the input to be a, and the weight associated to be W1. Then after passing through the node the input becomes a*W1 3) Bias – In addition to the weights, another linear component is applied to the input, called as the bias. It is added to the result of weight multiplication to the input. The bias is basically added to change the range of the weight multiplied input. After adding the bias, the result would look like a*W1+bias. This is the final linear component of the input transformation. 4) Activation Function – Once the linear component is applied to the input, a non-linear function is applied to it. This is done by applying the activation function to the linear combination.The activation function translates the input signals to output signals. The output after application of the activation function would look something like f(a*W1+b) where f() is the activation function.In the below diagram we have “n” inputs given as X1 to Xn and corresponding weights Wk1 to Wkn. We have a bias given as bk. The weights are first multiplied to its corresponding input and are then added together along with the bias. Let this be called as u.u=∑w*x+bThe activation function is applied to u i.e. f(u) and we receive the final output from the neuron as yk = f(u)  The most commonly applied activation functions are – Sigmoid, ReLU and softmaxa) Sigmoid – One of the most common activation functions used is Sigmoid. It is defined as:The sigmoid transformation generates a more smooth range of values between 0 and 1. We might need to observe the changes in the output with slight changes in the input values. Smooth curves allow us to do that and are hence preferred over step functions. b) ReLU(Rectified Linear Units) – Instead of sigmoids, the recent networks prefer using ReLu activation functions for the hidden layers. The function is defined as:The output of the function is X when X>0 and 0 for X<=0. The function looks like this:The major benefit of using ReLU is that it has a constant derivative value for all inputs greater than 0. The constant derivative value helps the network to train faster. c) Softmax – Softmax activation functions are normally used in the output layer for classification problems. It is similar to the sigmoid function, with the only difference being that the outputs are normalized to sum up to 1. The sigmoid function would work in case we have a binary output, however in case we have a multiclass classification problem, softmax makes it really easy to assign values to each class which can be easily interpreted as probabilities.It’s very easy to see it this way – Suppose you’re trying to identify a 6 which might also look a bit like 8. The function would assign values to each number as below. We can easily see that the highest probability is assigned to 6, with the next highest assigned to 8 and so on… 5) Neural Network – Neural Networks form the backbone of deep learning.The goal of a neural network is to find an approximation of an unknown function. It is formed by interconnected neurons. These neurons have weights, and bias which is updated during the network training depending upon the error. The activation function puts a nonlinear transformation to the linear combination which then generates the output. The combinations of the activated neurons give the output.A neural network is best defined by “Liping Yang” as –“Neural networks are made up of numerous interconnected conceptualized artificial neurons, which pass data between themselves, and which have associated weights which are tuned based upon the network’s “experience.” Neurons have activation thresholds which, if met by a combination of their associated weights and data passed to them, are fired; combinations of fired neurons result in “learning”. 6) Input / Output / Hidden Layer – Simply as the name suggests the input layer is the one which receives the input and is essentially the first layer of the network. The output layer is the one which generates the output or is the final layer of the network. The processing layers are the hidden layers within the network. These hidden layers are the ones which perform specific tasks on the incoming data and pass on the output generated by them to the next layer. The input and output layers are the ones visible to us, while are the intermediate layers are hidden. 7) MLP (Multi Layer perceptron) – A single neuron would not be able to perform highly complex tasks. Therefore, we use stacks of neurons to generate the desired outputs. In the simplest network we would have an input layer, a hidden layer and an output layer. Each layer has multiple neurons and all the neurons in each layer are connected to all the neurons in the next layer. These networks can also be called as fully connected networks.  8) Forward Propagation – Forward Propagation refers to the movement of the input through the hidden layers to the output layers. In forward propagation, the information travels in a single direction FORWARD. The input layer supplies the input to the hidden layers and then the output is generated. There is no backward movement. 9) Cost Function – When we build a network, the network tries to predict the output as close as possible to the actual value. We measure this accuracy of the network using the cost/loss function. The cost or loss function tries to penalize the network when it makes errors.Our objective while running the network is to increase our prediction accuracy and to reduce the error, hence minimizing the cost function. The most optimized output is the one with least value of the cost or loss function.If I define the cost function to be the mean squared error, it can be written as –C= 1/m ∑(y – a)2 where m is the number of training inputs, a is the predicted value and y is the actual value of that particular example.The learning process revolves around minimizing the cost. 10) Gradient Descent – Gradient descent is an optimization algorithm for minimizing the cost. To think of it intuitively, while climbing down a hill you should take small steps and walk down instead of just jumping down at once. Therefore, what we do is, if we start from a point x, we move down a little i.e. delta h, and update our position to x-delta h and we keep doing the same till we reach the bottom. Consider bottom to be the minimum cost point. SourceMathematically, to find the local minimum of a function one takes steps proportional to the negative of the gradient of the function.You can go through this article for a detailed understanding of gradient descent. 11) Learning Rate – The learning rate is defined as the amount of minimization in the cost function in each iteration. In simple terms, the rate at which we descend towards the minima of the cost function is the learning rate. We should choose the learning rate very carefully since it should neither be very large that the optimal solution is missed and nor should be very low that it takes forever for the network to converge.Source12) Backpropagation – When we define a neural network, we assign random weights and bias values to our nodes. Once we have received the output for a single iteration, we can calculate the error of the network. This error is then fed back to the network along with the gradient of the cost function to update the weights of the network. These weights are then updated so that the errors in the subsequent iterations is reduced. This updating of weights using the gradient of the cost function is known as back-propagation.In back-propagation the movement of the network is backwards, the error along with the gradient flows back from the out layer through the hidden layers and the weights are updated. 13) Batches – While training a neural network, instead of sending the entire input in one go, we divide in input into several chunks of equal size randomly. Training the data on batches makes the model more generalized as compared to the model built when the entire data set is fed to the network in one go. 14) Epochs – An epoch is defined as a single training iteration of all batches in both forward and back propagation. This means 1 epoch is a single forward and backward pass of the entire input data.The number of epochs you would use to train your network can be chosen by you. It’s highly likely that more number of epochs would show higher accuracy of the network, however, it would also take longer for the network to converge. Also you must take care that if the number of epochs are too high, the network might be over-fit. 15) Dropout – Dropout is a regularization technique which prevents over-fitting of the network. As the name suggests, during training a certain number of neurons in the hidden layer is randomly dropped. This means that the training happens on several architectures of the neural network on different combinations of the neurons. You can think of drop out as an ensemble technique, where the output of multiple networks is then used to produce the final output. 16) Batch Normalization – As a concept, batch normalization can be considered as a dam we have set as specific checkpoints in a river. This is done to ensure that distribution of data is the same as the next layer hoped to get. When we are training the neural network, the weights are changed after each step of gradient descent. This changes the how the shape of data is sent to the next layer.But the next layer was expecting the distribution similar to what it had previously seen. So we explicitly normalize the data before sending it to the next layer. 17) Filters – A filter in a CNN is like a weight matrix with which we multiply a part of the input image to generate a convoluted output. Let’s assume we have an image of size 28*28. We randomly assign a filter of size 3*3, which is then multiplied with different 3*3 sections of the image to form what is known as a convoluted output. The filter size is generally smaller than the original image size. The filter values are updated like weight values during backpropagation for cost minimization.Consider the below image. Here filter is a 3*3 matrix    which is multiplied with each 3*3 section of the image to form the convolved feature. 18) CNN (Convolutional neural network) – Convolutional neural networks are basically applied on image data. Suppose we have an input of size (28*28*3), If we use a normal neural network, there would be 2352(28*28*3) parameters. And as the size of the image increases the number of parameters becomes very large. We “convolve” the images to reduce the number of parameters (as shown above in filter definition). As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the output of that filter at every position. We will stack these activation maps along the depth dimension and produce the output volume.You can see the below diagram for a clearer picture. 19) Pooling – It is common to periodically introduce pooling layers in between the convolution layers. This is basically done to reduce a number of parameters and prevent over-fitting. The most common type of pooling is a pooling layer of filter size(2,2) using the MAX operation. What it would do is, it would take the maximum of each 4*4 matrix of the original image.You can also pool using other operations like Average pooling, but max pooling has shown to work better in practice. 20) Padding – Padding refers to adding extra layer of zeros across the images so that the output image has the same size as the input. This is known as same padding.After the application of filters  the convolved layer in the case of same padding has the size equal to the actual image.Valid padding refers to keeping the image as such an having all the pixels of the image which are actual or “valid”. In this case after the application of filters the size of the length and the width of the output keeps getting reduced at each convolutional layer. 21) Data Augmentation – Data Augmentation refers to the addition of new data derived from the given data, which might prove to be beneficial for prediction. For example, it might be easier to view the cat in a dark image if you brighten it, or for instance, a 9 in the digit recognition might be slightly tilted or rotated. In this case, rotation would solve the problem and increase the accuracy of our model. By rotating or brightening we’re improving the quality of our data. This is known as Data augmentation.22) Recurrent Neuron – A recurrent neuron is one in which the output of the neuron is sent back to it for t time stamps. If you look at the diagram the output is sent back as input t times. The unrolled neuron looks like t different neurons connected together. The basic advantage of this neuron is that it gives a more generalized output. 23) RNN(Recurrent Neural Network) – Recurrent neural networks are used especially for sequential data where the previous output is used to predict the next one. In this case the networks have loops within them. The loops within the hidden neuron gives them the capability to store information about the previous words for some time to be able to predict the output. The output of the hidden layer is sent again to the hidden layer for t time stamps. The unfolded neuron looks like the above diagram. The output of the recurrent neuron goes to the next layer only after completing all the time stamps. The output sent is more generalized and the previous information is retained for a longer period.The error is then back propagated according to the unfolded network to update the weights. This is known as backpropagation through time(BPTT). 24) Vanishing Gradient Problem – Vanishing gradient problem arises in cases where the gradient of the activation function is very small. During back propagation when the weights are multiplied with these low gradients, they tend to become very small and “vanish” as they go further deep in the network. This makes the neural network to forget the long range dependency. This generally becomes a problem in cases of recurrent neural networks where long term dependencies are very important for the network to remember.This can be solved by using activation functions like ReLu which do not have small gradients. 25) Exploding Gradient Problem – This is the exact opposite of the vanishing gradient problem, where the gradient of the activation function is too large. During back propagation, it makes the weight of a particular node very high with respect to the others rendering them insignificant. This can be easily solved by clipping the gradient so that it doesn’t exceed a certain value. I hope you enjoyed going through the article. I have given a high level overview of the basic deep learning terms. I hope you now have a basic understanding of these terms. I have tried to explain everything in a language as easy as possible, however in case of any doubts/clarifications, please feel free to drop in your comments.",https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/
Senior Analyst / Manager – Qlikview Developer- Mumbai (3-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|25 Must Know Terms & concepts for Beginners in Deep Learning|Machine Learning Engineer- Gurugram (4+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 8 years
Requirements : 
Task Info : Job Description and Responsibilities:Involved in Requirement gathering & designing QlikView Applications.Creating and Managing QlikView Reports & Dashboards.Implemented Security & involved in Deployment of QlikView Applications.experience in VBAWriting SQL Scripts to load the relevant data in QlikView Applications.Designing Data Modeling and Loading data from Multiple Data sources.Performance tuning by analyzing and comparing the turnaround times between SQL and QlikView.Designed and developed extensive QlikView reports using combination of charts and tables.Publishing and Deploying the Dashboards based on business requirements.Developing Set Analysis to provide the custom functionality in QlikView Applications.Testing Applications, reports using review checklists, Data quality for quality assurance before delivering to the end users.Involved in complex mappings like Slowly Changing Dimensions.
College Preference : no-bar
Min Qualification : ug
Skills : data modeling, qlikview, sql, VBA
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-analyst-manager-qlikview-developer-mumbai-3-8-years-of-experience/
Machine Learning Engineer- Gurugram (4+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst / Manager – Qlikview Developer- Mumbai (3-8 Years of Experience)|Data Scientist- Mumbai (5 Yrs of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – years
Requirements : 
Task Info : Company Description:We are a software solutions agency that combines skills of consulting, design, development and management to deliver high-quality mobile and web applications. We offers services for offshore development and application support, coupled with attention to the rapid and clear communications and project management.Job Description– Creating Scalable Machine Learning systems that are highly performant – Identifying patterns in data streams and generating actionable insights – Customizing Machine Learning (ML) algorithms for in-house use – Writing production quality code and libraries that can be packaged, installed and deployed – Maintain and improve existing in-house tools and code. Optimize for speed and efficiency Qualifications– Experience of handling various data types and structures: structured and unstructured data, extensive prior experience in integrating data, profiling, validating – Algorithms Geek and researcher in field of data science with a strong experience in statistical & analytics packages/tools (R, Alteryx etc.) – Expertise on Machine Learning/Information Retrieval/Artificial Intelligence/Text Mining problems. – Deep understanding & familiarity with Predictive modeling concepts, Recommendation & optimizing algorithms, clustering & classification techniques – High level of proficiency in statistical tools, relational databases & expertise in programming languages like Python/SQL is desired – Strong analytical, troubleshooting and problem-solving skills – Ability to work independently as well as collaboratively within a team – Candidate should open to learn new open source technologies and languages – Must be able to thrive in a fast paced, customer-focused, collaborative environment. Must be able to work hands-on with minimal supervision or assistance. – Masters- degree in operations research, applied statistics, data mining, machine learning & related quantitative discipline (Preferably M.Sc /M. Tech. in Statistics/Mathematics) OR MBA from Tier 1 with relevant experience & expertise
College Preference : tier1-any
Min Qualification : pg
Skills : artificial intelligence, data mining, machine learning, predictive modeling, python, r, sql, statistical modeling, text mining
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/machine-learning-engineer-gurugram-4-years-of-experience/
Data Scientist- Mumbai (5 Yrs of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Engineer- Gurugram (4+ Years of Experience)|Android Developer- Gurgaon (2 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,Role & Responsibilities:|Work Experience & Domain Knowledge: :,"Experience : 5 – years
Requirements : 
Task Info : ·         Looking for Dynamic Individual who wants to help develop the next generation of technologies that find, organize, analyze, and personalize the unending flow of information faced by users.·         Data Scientists work on solving problems in various areas of computing including automated information classification, efficient algorithms for “Text mining”, “Focused Crawler” “Machine learning” and “Natural Language Processing” to locate information on the web, scalability issues related to filtering and managing large amounts of data, converting unstructured data into structured data to name just a few.·         Quickly model, prototype, architect and engineer systems requiring machine learning to match business needs Development of predictive models to classify text into multiple categories and extract meaningful information from text to build knowledge base·         Manage the scale & high volume of TBs of realtime data streams·         5+ years of professional experience with core skill sets of Text Mining, Natural Language Processing, Entity Extraction, Relationship Extraction and Machine Learning·         Experience with text classification algorithms like Naïve-Bayes, SVM etc·         Experience in using tools GATE, UIMA with java or ntlk with python etc·         Experience with Open source NLP libraries e.g. Corenlp, Opennlp, mallet, etc.·         Good Knowledge of Elastic Search/Apache Solr, Lucene will be plus·         Must have strong combination of programming experience in building end to end applications in Java/Python·         Understanding of statistics is good to have·         Tweaking algorithms & data-structures for performance fine tunings·         Exposure to big data & hadoop eco-system is a plusQualifications:B.E/B. tech, MCA, M. Sc (Computer Science/IT)Skills & Attitude:·         Excellent analytical skills·         Enthusiasm for working in a team to solve interesting problems
College Preference : no-bar
Min Qualification : ug
Skills : apache spark, bigdata, hadoop, java, machine learning, naive bayes, nlp, python, svm, text mining
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-scientist-mumbai-5-yrs-of-experience/
Android Developer- Gurgaon (2 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Mumbai (5 Yrs of Experience)|Why are GPUs necessary for training Deep Learning models?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 5 years
Requirements : 
Task Info : As an Android Developer, you will be responsible for developing habit forming mobile app for Analytics Vidhya’s audience. This app would help hundreds of thousands of people across the globe to learn and practice data science on a daily basis.You will be bringing in new ideas to the company and its products and will be responsible to deliver world-class mobile experience to the users. You will work with our server-side engineers to deliver world class mobile experience.Responsibilities: Lead and drive entire development cycle of android app development on your own You will build and deploy android apps from scratch You will work closely with our backend engineering to interface with API services and contribute to the APIs when needed You will have to collaborate with cross-functional teams to develop and ship new features with short development cyclesKnowledge, Skills & Experience: 2-5 years of experience with building android applications Experience with Java, Android SDK, platform tools and optimisation techniques Experience interfacing with REST APIs and Git Should have developed at least one app from scratch on your own that has 10,000+ installs on play store B.E/B.Tech in Computer Science or related field, or equivalent experience Experience of backend development with Python would be a plus point
College Preference : no-bar
Min Qualification : ug
Skills : Android, GIT, java, REST
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/android-developer-gurgaon-2-to-5-years-of-experience/
Why are GPUs necessary for training Deep Learning models?,Learn everything about Analytics|Introduction|Table of Contents|Fact #101: Deep Learning requires a lot of hardware|Training a deep learning model|How to train your neural net faster?|Difference between CPU and GPU|Brief History of GPUs – how did we reach here|Which GPU to use today?|The future looks exciting|End Notes,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Android Developer- Gurgaon (2 to 5 years of experience)|Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python)|
Faizan Shaikh
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Most of you would have heard exciting stuff happening using deep learning. You would have also heard that Deep Learning requires a lot of hardware. I have seen people training a simple deep learning model for days on their laptops (typically without GPUs) which leads to an impression that Deep Learning requires big systems to run execute.However, this is only partly true and this creates a myth around deep learning which creates a roadblock for beginners. Numerous people have asked me as to what kind of hardware would be better for doing deep learning. With this article, I hope to answer them.Note: I assume that you have a fundamental knowledge of deep learning concepts. If not, you should go through this article.  When I first got introduced with deep learning, I thought that deep learning necessarily needs large Datacenter to run on, and “deep learning experts” would sit in their control rooms to operate these systems.This is because every book that I referred or every talk that I heard, the author or speaker always say that deep learning requires a lot of computational power to run on. But when I built my first deep learning model on my meager machine, I felt relieved! I don’t have to take over Google to be a deep learning expert 😀This is a common misconception that every beginner faces when diving into deep learning. Although, it is true that deep learning needs considerable hardware to run efficiently, you don’t need it to be infinite to do your task. You can even run deep learning models on your laptop!Just a small disclaimer; the smaller your system, more is the time you will need to get a trained model which performs good enough. You may basically look like this: Let’s just ask ourselves a simple question; why do we need more hardware for deep learning?The answer is simple, deep learning is an algorithm – a software construct. We define an artificial neural network in our favorite programming language which would then be converted into a set of commands that run on the computer.If you would have to guess which components of neural network do you think would require intense hardware resource, what would be your answer?A few candidates from top of my mind are:Among all these, training the deep learning model is the most intensive task. Lets see in detail why is this so. When you train a deep learning model, two main operations are performed:In forward pass, input is passed through the neural network and after processing the input, an output is generated. Whereas in backward pass, we update the weights of neural network on the basis of error we get in forward pass.Both of these operations are essentially matrix multiplications. A simple matrix multiplication can be represented by the image belowHere, we can see that each element in one row of first array is multiplied with one column of second array. So in a neural network, we can consider first array as input to the neural network, and the second array can be considered as weights of the network.This seems to be a simple task. Now just to give you a sense of what kind of scale deep learning – VGG16 (a convolutional neural network of 16 hidden layers which is frequently used in deep learning applications) has ~140 million parameters; aka weights and biases. Now think of all the matrix multiplications you would have to do to pass just one input to this network! It would take years to train this kind of systems if we take traditional approaches. We saw that the computationally intensive part of neural network is made up of multiple matrix multiplications. So how can we make it faster?We can simply do this by doing all the operations at the same time instead of doing it one after the other. This is in a nutshell why we use GPU (graphics processing units) instead of a CPU (central processing unit) for training a neural network.To give you a bit of an intuition, we go back to history when we proved GPUs were better than CPUs for the task.Before the boom of Deep learning, Google had a extremely powerful system to do their processing, which they had specially built for training huge nets. This system was monstrous and was of $5 billion total cost, with multiple clusters of CPUs.Now researchers at Stanford built the same system in terms of computation to train their deep nets using GPU. And guess what; they reduced the costs to just $33K ! This system was built using GPUs, and it gave the same processing power as Google’s system. Pretty impressive right? We can see that GPUs rule. But what exactly is the difference between a CPU and a GPU? To understand the difference, we take a classic analogy which explains the difference intuitively.Suppose you have to transfer goods from one place to the other. You have an option to choose between a Ferrari and a freight truck.Ferrari would be extremely fast and would help you transfer a batch of goods in no time. But the amount of goods you can carry is small, and usage of fuel would be very high.A freight truck would be slow and would take a lot of time to transfer goods. But the amount of goods it can carry is larger in comparison to Ferrari. Also, it is more fuel efficient so usage is lower.So which would you chose for your work?Obviously, you would first see what the task is; if you have to pick up your girlfriend urgently, you would definitely choose a Ferrari over a freight truck. But if you are moving your home, you would use a freight truck to transfer the furniture.Here’s how you would technically differentiate the two:SourceHere’s another video which would make your concept even clearer.Note: GPU is mostly used for gaming and doing complex simulations. These tasks and mainly graphics computations, and so GPU is graphics processing unit. If GPU is used for non-graphical processing, they are termed as GPGPUs – general purpose graphics processing unit Now, you might be asking this question that why are GPUs so much rage right now. Let us travel through a brief history of development of GPUsBasically a GPGPU is a parallel programming setup involving GPUs & CPUs which can process & analyze data in a similar way to image or other graphic form. GPGPUs were created for better and more general graphic processing, but were later found to fit scientific computing well. This is because most of the graphic processing involves applying operations on large matrices.The use of GPGPUs for scientific computing started some time back in 2001 with implementation of Matrix multiplication. One of the first common algorithm to be implemented on GPU in faster manner was LU factorization in 2005. But, at this time researchers had to code every algorithm on a GPU and had to understand low level graphic processing.In 2006, Nvidia came out with a high level language CUDA, which helps you write programs from graphic processors in a high level language. This was probably one of the most significant change in they way researchers interacted with GPUs Here I will quickly give a few know-hows before you go on to buy a GPU for deep learning.Scenario 1:The first thing you should determine is what kind of resource does your tasks require. If your tasks are going to be small or can fit in complex sequential processing, you don’t need a big system to work on. You could even skip the use of GPUs altogether. So, if you are planning to work mainly on “other” ML areas / algorithms, you don’t necessarily need a GPU. Scenario 2:If your task is a bit intensive, and has a handle-able data, a reasonable GPU would be a better choice for you. I generally use my laptop to work on toy problems, which has a slightly out of date GPU (a 2GB Nvidia GT 740M). Having a laptop with GPU helps me run things wherever I go. There are a few high end (and expectedly heavy) laptops with Nvidia GTX 1080 (a 8 GB VRAM) which you can check out at the extreme. Scenario 3:If you are regularly working on complex problems or are a company which leverages deep learning, you would probably be better off building a deep learning system or use a cloud service like AWS or FloydHub. We at Analytics Vidhya built a deep learning system for ourselves, for which we shared our specifications. Here’s the article. Scenario 4:If you are Google, you probably need another datacenter to sustain! Jokes aside, if your task is of a bigger scale than usual, and you have enough pocket money to cover up the costs; you can opt for a GPU cluster and do multi-GPU computing. There are also some options which may be available in the near future – like TPUs and faster FPGAs, which would make your life easier. As mentioned above, there is a lot of research and active work happening to think of ways to accelerate computing. Google is expected to come out with Tensorflow Processing Units (TPUs) later this year, which promises an acceleration over and above current GPUs.Similarly Intel is working on creating faster FPGAs, which may provide higher flexibility in coming days. In addition, the offerings from Cloud service providers (e.g. AWS) is also increasing. We will see each of them emerge in coming months. In this article, we covered the motivations of using a GPU for deep learning applications and saw how to choose them for your task. I hope this article was helpful to you. If you have any specific questions regarding the topic, feel free to comment below or ask them on discussion portal.",https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/
Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python),Learn everything about Analytics|Introduction|Table of Contents|1. Elastic Search (ES)|2. Kibana|3. Creating Dashboards,"Installation of Elastic Search|Installation|3.1 Indexing data|3.2 Linking Kibana|3.3 Create Visualizations|Finally the dashboard with all the visualizations created would look like this!|4. Search bar|Share this:|Like this:|Related Articles|Why are GPUs necessary for training Deep Learning models?|22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)|
Guest Blog
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Reading data|Example 1|Example 2|Example 3|Example,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Exploratory Data Analysis (EDA) helps us to uncover the underlying structure of data and its dynamics through which we can maximize the insights. EDA is also critical to extract important variables and detect outliers and anomalies. Even though there are many algorithms in Machine Learning, EDA is considered to be one of the most critical part to understand and drive the business.There are several ways to perform EDA on various platforms like Python (matplotlib, seaborn), R (ggplot2) and there are a lot of good resources on the web such as “Exploratory Data Analysis” by John W. Tukey, “Exploratory Data Analysis with R” by Roger D. Peng and so on..In this article, I am going to talk about performing EDA using Kibana and Elastic Search. Elastic Search is an open source, RESTful distributed and scalable search engine. Elastic search is extremely fast in fetching results for simple or complex queries on large amounts of data (Petabytes) because of it’s simple design and distributed nature. It is also much easier to work with than a conventional database constrained by schemas, tables.Elastic Search provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Installation and initialization is quite simple and it is as follows:Elasticsearch instance should be running at http://localhost:9200 in your browser if you run with default configuration.Keep the terminal open where elastic search is running to be able to keep the instance running. you could also use nohup mode to run the instance in the background. Kibana is an open source data exploration and visualization tool built on Elastic Search to help you understand data better. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data. Installation and initialization is similar to that of Elasticsearch:Kibana instance should be running at http://localhost:5601 in your browser if you run with default configuration.Keep the terminal open where Kibana was run to be able to keep the instance running. you could also use nohup mode to run the instance in the background. There are mainly three steps to create dashboards using ES and Kibana. I will be using Loan prediction practice problem data to create a dashboard. Please register for the problem to be able to download the data. Please check the data dictionary for more information.Note: In this article I will be using python to read data and insert data into Elasticsearch for creating visualizations through Kibana.  Elastic Search indexes data into its internal data format and stores them in a basic data structure similar to a JSON object. Please find the below python code to insert data into ES. Please install pyelasticsearch library as shown below for indexing through python.Note: Please note that the code assumes that the elastic search is run with default configuration. Repeat the above 4 steps for loan_prediction_test. Now kibana is linked with train and test data present in elastic searchVoila!! Dashboard created.Similarly for Gender distribution. This time we will use pie chart.Beautiful! isn’t it?Now I leave you here to explore more of elastic search and kibana and create various kind of visualizations Search bar allows you to explore data by string search, which helps us in understanding the changes in data with changes in one particular attribute which is not easy to do with visualizations.BeforeAfterInsight: Most of the clients that had credit history 0 did not receive Loan (Loan status is N = 92.1%)That’s all!! This article was contributed by Supreeth Manyam (@ziron) as part of The Mightiest Pen, DataFest 2017. Supreeth won the competition and also finished second in overall leaderboard of DataFest 2017. Supreeth is a passionate Data Scientist who is keen on bringing insights to business and help it get better by analyzing relevant data using Machine Learning and Artificial Intelligence.",https://www.analyticsvidhya.com/blog/2017/05/beginners-guide-to-data-exploration-using-elastic-search-and-kibana/
"22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)",Learn everything about Analytics|Introduction|Deep Learning talks|Big Data|Data Science|Natural Language Processing|End Notes,"1) Title : Deep Learning at Booking.com|2) Title : Using deep learning in natural language processing|3) Title: Creativity and AI: Deep Neural Nets “Going Wild”|4) Title : Neural Networks for Recommender Systems|5) Title : Training a TensorFlow model to detect lung nodules on CT scans|6) Title : Siamese LSTM in Keras: Learning Character-Based Phrase|7) Title : Deep learning for time series made easy|8) Title : Deep Reinforcement Learning: theory, intuition, code|9) Title: Different Strategies of Scaling H2O Machine Learning on Apache Spark|10) Title: A billion stars in the Jupyter Notebook|11) Title: Finding Needles in a Growing Haystack|12) Title: Survival analysis for conversion rates|13) Title : Risk Analysis|14) Title: Python vs Orangutan|15) Title : Diagnosing Machine Learning Models|16) Title : Data Science in Internet of Things using Python and Spark|17) Title : Bayesian optimization with Scikit-Optimize|18) Title : Applied Data Science|19) Title : Successfully applying Bayesian statistics to A/B testing in your business|20) Title: Deploying Python Models to Production|21) Title: Pythonic Metal|22) Title: Simulate your language|Share this:|Like this:|Related Articles|Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python)|Data Analyst- Hyderabad ( 3 to 8 years of experience)|
Sunil Ray
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Python is increasingly gaining popularity among machine learning and data science communities across the world – and for the right reasons. It probably has the most developed ecosystem for deep learning, a collection of awesome libraries like pandas and scikit learn and an awesome community.PyData is a community for developers and users for open source data tools. They also conduct several conferences and I came across amazing talks from PyData Amsterdam 2017 recently. Even though I wanted to be part of the conference, it was difficult for me to travel. Thankfully, PyData released all the videos on their YouTube channel.The spread of the talks is amazing. Be it a novice, intermediate or an expert python user, PyData had something for everyone. To help the community, I have summarized the best talks from data science perspective in this article. For your convenience, I’ve also added a short summary of each video. We have the videos segregated in 4 categories – Deep Learning, Big Data, Data Science and Natural Language Processing.Consume as you want, learn, like and share! Speaker : Emrah Tasli, Stas GirkinDuration : 00:32:38 hrsThis talk intrigued me as soon as I read the title. I have always been a booking.com user. To see how they use deep learning to enhance user experience was a treat.Watch this video to get a practical overview of how deep learning is used in the industry. It focuses mainly on the applications of deep learning at booking.com . This covers applications like analyzing image content, analyzing text, understanding speech and building recommendation systems.The speakers then discuss how these techniques are applied at scale, and the tools used by booking.com to handle this scale. Speaker : Rob RomijndersDuration : 00:25:42 hrsUnderstanding language nuances is a difficult problem to solve – but deep learning holds our hope. This video is a must watch for people who want to use deep learning in natural language processing. It explains the motivation for using deep learning for NLP applications such as machine translation. It further explains how RNN works and how they are implemented.Lastly, Rob presents tips for increasing performance of these systems. Speaker: Roelof PietersDuration: 00:33:45 hrsRoelof talks about basics of deep learning with the explosion of research and experiments that deal with creativity and artificial intelligence.He also talks about the wonderful trippy world of neural nets “going wild” and shows some of the exciting possibilities new technologies have to offer to make us all more creative. Like, dancing moves, freestyle raps, impressionist paintings and showed some of the exciting possibilities new technologies offer for creative use and explorations of human-machine interaction where the main theorem is “augmentation, not automation”.He particularly focuses on “generative” models, and shows the python fanatics how to make your move with a particular form of Deep Neural Nets, to then finish with an “experiment”. Speaker : Maciej kulaDuration : 00:32:55 hrsNeural Networks are constantly replacing every other machine learning algorithm in real life systems and recommendation systems are no exception.In this tutorial, the speaker starts from the advantages of neural networks in recommender systems and goes through various machine learning models used in recommender systems including Factorization models, Bilinear Neural Networks and sampled loss functions. If you are aspiring to make an efficient recommender system, this  video is worth watching. Speaker : Mark Jan Harte, Gerben van VeenendaalDuration : 00:25:53 hrsIf you’re a philanthropist, this video is a must watch for you. It shows one of the numerous breakthrough applications of deep learning – to automate the detection of abnormality in medical imaging.The speakers describe the pipeline devised for automating the process. They explain in detail what are the challenges they faced while approaching the problem, what kind of hardware they utilize and then technically define their pipeline end-to-end. Its inspiring to see what kind of advancements deep learning can achieve. Speaker : Carsten van Weelden, Beata NyariDuration : 00:29:42 hrsIn this talk, the speakers explains how they solved the problem of classifying job titles into a job ontology with more than 5000 different classes. They do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library. Speaker : Dafne van KuppeveltDuration : 00:22:47 hrsDeep learning is a state of the art method for many tasks, such as image classification and object detection. For researchers that have time series data, but are not an expert on deep learning, the barrier can be high to start using deep learning.In this talk, the speaker explores how machine learning novices can use deep learning for time series classification. The speaker then explains mcfly, an open source python library, to help machine learning novices explore the value of deep learning for time series data. Speaker : Maxim LapanDuration : 00:28:27 hrsIn this talk the speaker gives a practical introduction into deep reinforcement learning methods, used to solve complex applications like control problems in robotics, play Atari games, self-driving car control and lots more. Deep Reinforcement Learning is a very hot topic, successfully applied in lots of areas which require planning of actions in complex, noisy and partially-observed environments. Concrete examples vary from playing arcade games, navigating websites, helicopter, quadrocopter and car control, protein folding and lots of others. Speaker: Jakub HavaDuration: 00:32:12 hrsH2O is becoming increasingly popular when handling big data. In this video, Jakub has discussed about basic overview of machine learning on top of H2O and Spark. He explains different ways to scale your tasks on top of these technologies like data munging in spark and model building in H2O or using a mix of both for data munging and model building.Sparkling Water integrates H2O with the capabilities of Apache Spark. It also allows us to leverage H2O’s machine learning algorithms with Apache Spark applications via Scala, Python, R or H2O’s Flow GUI which makes Sparkling Water a great enterprise solution.This video introduces the basic architecture of Sparkling Water, going over different scaling strategies and explains the pros and cons of each solution. It finishes with a live demo demonstrating the approaches and should give you a real-life experience of configuring and running Sparkling Water for your use case(s). Speaker: Maarten BreddelsDuration: 00:30:58 hrsEver tried to visualise high dimensional data and didn’t get good results? Well, this is the right place for you. In this video, Maarten talks about two Python packages: “Vaex” and “ipyvolume”.“Vaex” enables calculating statistics for a billion samples per second and “ipyvolume” enables to interactively visualise and explore these billion sample tables for high dimensional spaces. He shows the methods to visualize and explore large datasets (>1 billion) instead of using cluttered scatter plots. “ipyvolume” helps us to visualize higher dimensional data in the notebook interactively which can render 3d volumes and up to a million glyphs (scatter plots and quiver) in the (Jupyter) notebook as a widget.“Vaex” and “ipyvolume” can be used together to explore and visualize any large tabular data set, or separately to calculate statistics, and render 3d plots in the notebook and outside. Speaker: Stephen HelmsDuration: 00:31:02 hrsIn this video, Stephen Helms discusses about the architectural designs for big data. As the machines get more and more advanced, we’ll collect more and more data. With high amounts of data, it becomes a challenge to efficiently summarise the data and present relevant data to the users.Stephan addresses this challenge and tries to discuss the architectural designs and implementations which can be scaled to large amounts of data. He uses Bayesian statistics to build the automated reporting system. If you’re interested to know more about scaling your analysis to production, you would find this video very interesting. Speaker: Tristan BoudreaultDuration: 00:22:01 hrsDo you buy a product after the free trial ends ? As a product manager, your job might be on the line depending on how many users subscribe to your product after their free trial ends?In this video, Tristan Boudreault tries to estimate as to how many customers would be ready to pay after the trail expires. In business context, he tries to analyse how successful a website is, in converting its trail users into paid ones. When we actually look at the data we realise that people are not as impulsive as we think they are. They spend money after being comfortable with the product.He also discusses that sometimes it might be really tough to actually estimate the conversion by just looking at the numbers especially in cases when the company is growing exponentially. He has taken really interesting examples and it’s a great video if you’re looking for applying analytics to your offering on the web. Speaker :  Rogier van der GeerDuration – 00:31:20 hrsEver thought that data science can be used to win a game? Well here is a video illustrating how to play risk using python. In this video Rogier van der Geer explained how python based simulation is used to train genetic algorithm to play the game.The video also focusses on designing and implementation of these algorithms in a simplified way that can be optimised for winning the game. A must watch for Data Science enthusiast as it shows how Data Science can be used to win a game! Speaker: Dirk GorissenDuration: 00:35:35 hrsThis is probably the most interesting talk and a Keynote session by Dirk Gorissen. He addresses the problem of locating the orang-utans in the jungle. So, orang-utans are one of the rare forms of apes which need to be located and protected in the jungles. To locate them they have used radio waves and identify the orang-utans when the result is unique/anomalous.This video discusses this problem using a drone based tracking system. He shows beautifully how we can solve this problem analysing the data we receive from each signal. Speaker : Lucas Javier BernardiDuration – 00:39:00 hrsA Machine Learning model is never perfect.  If it completely fails, it must be fixed. If it performs well, we want to improve it. In this talk Lucas Javier Bernardi discuss about various techniques and tools needed to diagnose machine learning algorithms and models.The video explains how simple techniques and statistics can be used to improve a model and is a must watch for an aspiring data scientist. Speaker : Rafael Schultze KraftDuration : 00:32:01 hrsTime series forecasting is one of the most interesting application of Data Analysis. In this video Rafael Schultze Kraft discussed about predicting time series forecast using Python and Spark .The videos explains how to build machine learning models using AWS and python on data from sensor after suitable preprocessing which can be further used to predict significant information regarding time series data. Speaker : Gilles LouppeDuration : 00:28:53 hrsOptimization has always been an integral part of problem solving. Bayesian Optimization is a principled approach to optimize an expensive function. In this tutorial, Gilles Louppe demonstrates the use of Bayesian optimization algorithm using a newly built package Scikit-optimize which provides an easy-to-use set of tools to serve the purpose. Here you’ll understand the steps involved in Bayesian optimization and how to implement it in python, with an interesting analogy with brewing good quality coffee. Speaker : Giovanni LanzaniDuration : 00:35:13 hrsWith the data science and machine learning industry growing at a fast pace and all the companies incorporating these self-learning tools in their businesses, we always strive for developing the best models with the highest achievable accuracy. But this is not always in the best interest of the business, where a combination of practicality with accuracy will deliver a more acceptable end product. In this talk, Giovanni Lanzani discusses about the same while phrasing real life examples from big companies like Amazon and Netflix. Being a data science aspirant one could consider these important details to better optimize the delivered product. Speaker : Ruben MakDuration : 00:38:51 hrsA/B testing in business is a very good way to test which of your variants of product is performing the best and in turn improve the business outcome. In this tutorial, Ruben Mak discusses about applying Bayesian Statistics to improve A/B testing in your business. Shortly discussing the frequentist calculations of an A/B test and common problems in it, he uses this to explain Bayesian Statistics and more specifically hierarchical Bayes to further reduce the probability of making errors in multiple comparisons. The video also focuses on one of the most important aspects from a business perspective: when to stop an insignificant test. Speaker : Niels ZeilemakerDuration : 00:31:45 hrsDeveloping a model is actually half of the battle and you still need to put it in the production. This tutorial is all about doing so. Starting from Gitlab, the speaker covers the tools necessary for deployment of a machine learning model such as Jenkins, Docker, Kuebernetes, json logger and DTAP and goes through why and how of every tool along with codes wherever needed. I would suggest you to take your time and go through every slide of the talk to be a better data science practitioner. Speaker: Iain BarrDuration : 00:26:55 hrsBasics of NLP are always a challenge to conquer. This tutorial discusses the basic concepts of Natural Language Processing like vectorization of words, bag of words, word count as binomial frequency and deriving intelligence from it with the help of an example data set of 200,000 songs. Go ahead and take a look on it if you aspire to learn Natural Language Processing. Keep in mind that this video is a bit demanding, and you should have prior knowledge of basics of data science. Speaker: John PatonDuration: 00:27:36 hrsI was living in another state for almost 6 years and didn’t know the native language of the place. I always used to wonder if they hear my words similar to what I think of theirs. John Paten has answered my question here. He tries to demonstrate how our language looks to people who actually don’t speak it. He makes simple Markov Models for simulating any language in python. He shows various visualisations to understand the similarity and differences between various languages. There are very simple yet interesting insights about different languages regarding the most commonly used letters or whether a language uses long words or shorter ones to express the feelings. After this video you shall be able to understand the working of Markov models and would be able to understand and analyse languages using your models. Just watching these videos wouldn’t make you a better analyst. You need to practice too. For best results, you can take notes from the video. This will help you to quickly refer the topic at a later point in time.While watching these videos, there were several moments when I felt, there are lot many things in Python which I am yet to explore. Once again I would like to thank python community for being so generous, helpful and always being helpful in time of need. If you would like to see more such videos from Pydata, you can check out their Youtube channel.Did you find this list of tutorials helpful? Which tutorial or talk did you like the most? Share your experience/ suggestion in the comments below.",https://www.analyticsvidhya.com/blog/2017/05/pydata-amsterdam-2017-machine-learning-deep-learning-data-science/
Data Analyst- Hyderabad ( 3 to 8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)|Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 0 – years
Requirements : 
Task Info : About the company:We are the information technology service provider that specializes in design, delivery and implementing technology driven business solutions. Through offices in the United States, UK and India, we provides complete range of services leveraging it domain expertise. Clients gain immediate and measurable value from our offerings that span business and technology consulting, application services, systems integration, product development, custom software development, maintenance, re-engineering, testing and validation services, IT infrastructure services and business process outsourcing.Job briefWe are looking for Data Analyst/Scientist to dig insights from various data sources.Data Analyst Job DutiesSource, clean, analyse and report insights using good software life cycle quality controls.Responsibilities·        Interpret data, analyze results using statistical techniques and provide ongoing reports·        Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality·        Acquire data from primary or secondary data sources and maintain databases/data systems·        Identify, analyse, and interpret trends or patterns in complex data sets·        Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems·        Work with management to prioritize business and information needs·        Locate and define new process improvement opportunitiesRequirements·        Strong Python or R programming skills.·        Prior experience working with Time Series data modelling, especially in Financial and Capital Markets segment.·        BS in Mathematics, Economics, Computer Science, Information Management or Statistics.
College Preference : tier1-entire
Min Qualification : open
Skills : bfsi, python, r, time series
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-analyst-hyderabad-3-to-8-years-of-experience/
"Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- Hyderabad ( 3 to 8 years of experience)|Inspiring story of Deepak Vadithala – from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 10 years
Requirements : 
Task Info : Key Responsibilities: We aims to launch a 9-month online Diploma Program in Big Data (Developers) in partnership with a top Indian University. We envision building this credential into a gold standard for Big Data professionals, akin to a CFA in the Finance world. As a Subject Matter Expert (SME) – Big Data, we would seek your leadership in the following areas to help deliver a rigorous, industry-relevant program: Ø  Curriculum development (10% time): Build a rigorous program by informing decisions on curriculum structure, describing various Big Data Processing concepts in innovative, simple ways, and time allocation across concepts   Ø  Industry-relevant cases and projects (40% time): Identify industry-relevant projects/cases and develop them (e.g. data sets, code solutions, evaluation criteria) with industry partners to offer students compelling project experience from a recruiter standpoint.  Ø  Academic quality assurance and guest faculty (40% time): Help create learning material with an in-house team of instructional designers and review its technical quality. Deliver 1-2 modules to supplement the existing faculty. Ø  Thought-leadership (10% time): Leverage our platform to publish original content(e.g. video as guest faculty, articles on blog) to position yourself, and by association  we as a thought leader in the Big Data industry Ø  Student experience (post-program launch): Lead a team of program associates to assist students with their academic doubts on our platform  We are engaging with various partners like IIIT-B, Citi, Flipkart, Fractal, etc. which will give us the opportunity to build brand equity alongside industry thought leaders through our proprietary events, video content database, blog, among others.  Join us on the journey of building a world-class program in Big Data!  Desired Profile: 8-10 years in Big Data education and industry Hands-on, workplace experience in applying concepts in Hadoop, Spark, Hadoop Ecosystem Tools, NoSQL, Visualisations, etc. Intermediate to advanced proficiency in Java, SQL, and Python  Experience in multiple verticals will be a plus  
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, datavisualization, hadoop, Hadoop Ecosystem, java, nosql, python, spark
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/subject-matter-expert-sme-big-data-mumbai-3-to-10-years-of-experience-full-time-part-time/
Inspiring story of Deepak Vadithala – from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary,Learn everything about Analytics|Introduction|My encounter with Deepak|About Deepak Vadithala,"Share this:|Like this:|Related Articles|Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)|Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017|
Kunal Jain
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"We love heroes. We use them to get our dose of daily inspiration. We look forward to see / hear what they are doing, how they are converting challenges into opportunities and what they are thinking. We pick our heroes based on our experience, preferences, challenges and likings. I am sure you have your own heroes.Today, we bring you a story of one such hero – Deepak Vadithala. Deepak started his career as a paper boy. He has been a booth guy, a nightwatchman at a computer institute to learn Computers and is a Qlik Luminary today. The first time I came across Deepak was when he was running a QlikView course for charity. He asked us if Analytics Vidhya would be able to spread word for his course. Next time I can across him – he was using his Data Science skills for charity work and involving a group of 30 data scientists to solve social problems for underprivileged children.My curiosity about him grew and I looked up what and why is he up to. And I came across an inspirational story – a story of sheer hard work & perseverance. A story which puts all excuses I hear from people to shame and could inspire an entire generation of data scientists. I am going to share this story with our community today. Deepak is currently working as a Lead Data Engineer / Data Scientist. He was recognised as a Qlik Luminary for the year 2014 and 2017. He co-founded Qlik Dev Group and is an author of www.QlikShare.com. He was also rewarded as the top 20 Global QlikView Community contributor. He is also a certified, Scrum Master (PSM) and is currently, pursuing Masters in Data Science from the University of London.  Kunal: Hi Deepak, thanks for finding out time for this interview. Tell us briefly about yourself.Deepak: I am born and breed in Hyderabad, India. Currently, I live and work in London. I work as a Lead Data Engineer for Exterion on a TFL project. Also, I am pursuing Masters in Data Science from University of London (Part-time). I am passionate about solving data problems using programming and data analysis. I got awarded as a Qlik Luminary in 2014 and 2017. I have 13+ years’ experience in application development, database management and building analytics applications. I am working on two charity projects to raise money for the underprivileged children for their education.Thank you for interviewing me, and I am excited to share my story with the AV community. You guys are doing a great job with AV. Kunal: Thanks Deepak! Tell us how did your journey on analytics / data science / visualization start?Deepak: As Steve Jobs quoted:“You cannot connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something – your gut, destiny, life, karma, whatever”. I had no idea back then that I will become a developer or work with the data. However, every job I did in the past has either direct or indirect contribution to my journey.Before, I talk about my journey into analytics world. I had done 13 different non-tech jobs before becoming a data analyst at Dell. It had been a bumpy ride, to say the least, but in every job, I managed to learn some skills which helped me some way or the other. Perseverance is the key factor and helped me all the way. I had intuition when I was doing odd jobs that things will work out at some point. I made sure that I had learnt something new on every job.  This approach has never let me down, and it has made all the difference to my life.My Journey so farCurrently I am working in a Lead Data Engineer in London. I started working when I was 15 years old, and I’m 35 years old now. At times, I used to do two jobs, so there is overlap between years. Jobs in chronological orderKunal: Wow! That’s inspiring and it would have been damn tough. How did the transition from a Call Centre Agent to a Data Analyst happen?Deepak: My exposure to analytics started started when I was working in Dell call centre, I met a guy called Ram, and he was working on Excel and SQL Server. I observed that all the managers go to him to get their reports/data. I liked the attention he got and thought he was indispensable. That was it! I had decided to become a Data Analyst, so I can avoid taking calls, and at the same time, I could work on something which is a transferable skill set when I move organisations.So, I started learning Excel during weekends in the office as I did not have the computer at home back then. I would spend weekends in office practising various Excel formulas, pivot tables and charts. I showed off my skills to my manager, and he encouraged me to prepare internal team reports in Excel. I did that for about six months when my manager informed me there would be an opportunity in Gurgaon Dell office for Data Analyst role. I was excited and worked hard to prepare for the interview. I attended the interview and showed what I had been doing and some sample reports. I was offered the role, and I moved away from Call Centre to Dell IT team.I did not stop there, I went on acquiring new skills continuously. Learning new things and practising them has become a habit and I had started enjoying the process. Kunal: You had no background in analytics and data science. Neither were you from a strong education background. What kind of challenges did you face in reaching here? And what are key decisions in shaping your career?Deepak: Currently, I am pursuing Masters in Data Science from the University of London as part-time. I study from 6 pm – 9 pm while working full time and I have a one-year-old baby to take care of.However, I do not have strong academic and engineering background. I did my bachelor’s in Journalism and another one in commerce. I had studied while working. So, I had to pursue all my education through distance learning. It was not easy at all as I was doing odd jobs initially. I accepted that learning is a continuous process and it takes the time to master a subject.Here are a few stories about my journey till now:The story of Job Number # 7: Joining as a Night Watchmen / Security Guard for computer institute.During my initial journey I wanted to learn computers, but I couldn’t afford to pay for the training institutes neither I had a computer at home. I thought I should find a way to learn computers. I directly approached computer institutes in Hyderabad with my resume asking for any roles. A lot of them said no, but one of them said – they need someone to sleep in the institute in the night times as security guard and also to open the doors early in the mornings for students.I was embarrassed and felt uncomfortable being a security guard. But I thought who cares, no one is watching me, and at the same time, I will get access to the computers all night. And they kindly agreed for me to sit in whatever training classes I was interested in learning. I saw this as a tremendous opportunity while it is uncomfortable then. I decided to try it anyway, and I’m so glad I did that.I tried learning C language but it didn’t make sense at that time as I didn’t have math or CS foundation and I couldn’t follow the lectures. I decided to take design classes – Photoshop, Illustrator and Coreldraw. As they were interesting and I always loved the design. So, I took the classes and practised almost every night for 2–4 hrs. Within a couple of months, I did pick up decent skills.After six months as a night watchmen or security guard. I got a job as a DTP operator paying me more than bartender job. And I continued to improve my skills on other design packages then.Looking back, this was a good decision as I moved into computer based jobs instead of odd jobs. Story of Job Number # 13: Coming to the UK with Dell Exit CompensationWhile I was working as a Data Analyst role in Delhi. Dell made structural changes to operations team (I was part of Operations team as Data Analyst). And management asked me to take a manager role in the call centre business in Hyderabad or Bangalore (then). My gut feeling was I need to have independent skills and not Dell specific skills. So, I wasn’t too interested in the managerial position. Most of my colleagues took that opportunity except two of us. We were asked to leave Dell as they don’t have any Data Analysis roles in India. I was offered compensation of 7 months’ salary as worked for 3.5 years.It was a hard and emotional decision as I enjoyed my time working at Dell and made loads of friends. I took a holiday for two weeks and went looking for a job to assess the market outside. There were no phone calls nor interviews and my two weeks ended quickly. But a friend suggested me to try HSMP (UK Visa). I thought I don’t stand a chance as I didn’t see myself Highly Skilled then. But I thought I should try or else I will end up regretting not trying. So I took the compensation and applied for HSMP. And thankfully all worked in my favour. I got the visa, and I was left with £700 to come to the UK. I was told there were no jobs in the UK. And it was 2008 beginning and recession started. I decided to come anyway thinking I can work in supermarkets as I had done all sorts of jobs in the past. Again, I got interview call within a week, and Survey Solutions hired me within two weeks of landing in London. Story: Almost redundant In 2009, I was working as a developer however my knowledge was very limited. My company then made a decision to make me redundant because of recession. They kindly gave a month’s notice and asked me to search for a job. I thought I had experience and skills to survive. However, the market was not great, and I had applied for at least 300 jobs. I had gathered hundreds of email address and sent blanket emails to potential employers. Most of them never replied, and some responded saying they will get back to me in the future. Finally, I found a job in a small village in Cambridge, and they offered me less than my actual salary then. Luckily, on the last day of the notice period my employer changed his mind and asked me to stay with them as they realised my work and how it would affect them. I decided to stick with the same employer and continued working for them for another 18 months.That situation hit me hard and made me realised that I could not be complacent and I need to acquire new skills continuously. I started going to the library over the weekend and spent almost all my weekend studying and preparing for certifications. I did this for two years, and this made a big difference regarding skills. Since then, I chose the job I wanted rather than I was forced to take up a job. Kunal: I see you are also a Qlik Luminary – tell us about that journey.Deepak: I accidentally discovered QlikView. I was asked to attend an interview for a Database developer and DBA role. Moreover, the employer wanted someone with QlikView skills too. I had to clue what was QlikView!I started by installing the desktop edition and instantly loved the associative engine. I decided to learn and spend time preparing for the interview. I was honest and told my prospective employer that I am super keen to learn. They believed in me offered a role to work on both databases and QlikView.Sounds a bit weird but I read most of the manual as there were no books or courses on QlikView apart from Qlik’s official training course, which was too expensive. Also, I started answering questions on Qlik community to exposure to a variety of problems. It was like eat-sleep-Qlik-repeat for 18 months. At one point, I was amongst top 30 contributors to Qlik community. Then I thought I should share my knowledge with other developer and started www.QlikShare.com blog. I had recorded almost 100 video tutorials and 250+ quiz questions.Jason and Matt (co-founders) of QlikDevGroup asked me to join them to start the QlikDevGroup. I liked the idea, and we have begun the QlikDevGroup (www.QlikDevGroup.com).Thankfully, Qlik recognised all my work and awarded me Qlik Luminary in 2014 and 2017. Kunal: Coming to the fascinating work you are doing – how did it start – Teaching / Doing Data Science for charity?Deepak: I wanted to support underprivileged children and empower them with education. There are many poor children who cannot afford education. I thought I should pay back somehow and provide support. I realised that running a marathon may not work in my case (I do not have that kind of fitness levels!).I got an idea – Learn while empowering someone to learn. Project # 1:I thought I could create a course which can help students learn QlikView and other things while their money will be contributed to the charity. I am thankful to my friend – Shilpan Patel who also kindly agreed to collaborate with me to create QlikView Server course. Together – we had created the course and published on Udemy. There are 503 students so far, and the course is rated 4.5 out of 5. This course is the most comprehensive QlikView Server and Publisher course available. Project # 2:I am currently working on building a pricing app for the charities to increase their revenue. This project is open sourced. I see an opportunity where Charities could generate more revenue. Charities sell most of the donated goods for less because they have pricing constraints because of time, skills and accessibility of the data and they settle down to sell stuff for less, and their pricing is not on par with eBay.We could solve this problem using data. So, we are building an analytical app which helps charities price better. The project is called  DtP – “Discover the Potential”. Again, I am thankful to all the contributors who are helping me to build this platform. This project is self-funded and open source project, and we are taking data feeds from eBay. We process them on AWS. Enrich the data and process the data to provide search functionality. We are planning to use MongoDB, Python and Jhipster for the web app.Contact me if you are interested to know more about these projects. Kunal: What is the force which drives you? What would you want to achieve and change in next few years?Deepak: Honestly, I feel like a celebrity when you ask that question (laughs!). But I’m definitely not.I enjoy the learning process, and I do not feel stressed when I am studying or learning new skills. I like the process. Also, the ability to deal with uncertainties and staying composed helped me a lot. I am sure I will continue to learn as long as I can. And I have accepted the fact that nothing can be attained overnight. It will be a slow process, but surely every job helped me to learn something new. I am thankful to have wife and family who support and encourage me to learn.I want to focus on Data Science applications in investment banking/ finance domain. After masters, I am planning to pursue FRM Part 1. Also, I am working on statistical skills. Also, I want to participate in Kaggle competitions. At the same time, I am interested in open source projects to help charities. I am sure there are loads of problems we could solve using data more effectively. Kunal: This kind of impact would feel very empowering – tell us some stories of people/life you have changed?Deepak: I do not think I have changed anyone’s life yet. However, I am sure I had encouraged all my friends and colleagues to learn and acquire new skills. I would be happy if I can become a good mentor and help change someone’s life in the future. Kunal: Anything else you want to say to AV community?Deepak: Are you looking to learn QlikView? Please check my courses. I am giving 100 discount coupons exclusively for AV readers. And I would love to hear your suggestions, thoughts or feedback. You can connect me on LinkedIn and Twitter. I’m sharing my blog and course links with you below.BlogCourse Links:QlikView Server and PublisherQlikView Mastering Set Analysis Kunal: Thanks Deepak, your story is truly inspiring & motivating. We are glad to interview you and I’m sure many people out there will take away tremendous learning. All the best with your charity work and do let us know, if we can be of any help.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhya’s view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/05/exclusive-interview-with-deepak-vadithala-lead-data-engineer-qlikview-luminary/
"Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017",Learn everything about Analytics|Introduction|Problem Statement|Winners|End Notes,"Piyush Jaiswal, Rank 3|Rohan Rao, Rank 2|Mark Landry, Rank 1|Check out all the upcoming competitions here|Share this:|Like this:|Related Articles|Inspiring story of Deepak Vadithala – from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary|41 questions on Statistics for data scientists & analysts|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"The best way to learn data science is to work on data problems and solve them. What is even better is to solve these problems with thousands of data scientists around solving the same problem in a competition. And if you get to know what the winners did to solve the problem – you know you are participating in an Analytics Vidhya Hackathon!We conducted 3 machine learning hackathons as part of AV DataFest 2017. The QuickSolver was conducted on 30 April 2017. More than 1800 data scientists from across the globe competed to grab the top spots.If you missed this hackathon, you did miss an amazing opportunity. Well, you can still learn from the winners & their strategies. To hear the experiences and strategies of the data scientists who won, join us for the DataFest Closing Ceremony on 10 May 2017. The problem statement revolved around a digital publication house. They publish articles on varied categories & topics like general knowledge, practical well being, sports & health. These articles are written by distinguished authors in this field. To keep the reader engaged on the website, the portal recommends articles to its readers randomly.They want to enhance their customer experience, and understand the interest of their customers in detail. Instead of recommending articles to its reader randomly they would like to recommend articles based on their interest & are more likely to read. Currently, the portal has an option to rate articles based after reading.The data scientists had to predict how much would the reader like the article based on the given data. The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1: Mark LandryRank 2: Rohan RaoRank 3: Piyush JaiswalHere are the final rankings of all the participants at the leaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go ahead Piyush Jaiswal Piyush is a Data Science Analyst at 64 Squares based in Pune. He is a machine learning enthusiast and has participated in several competitions on Analytics Vidhya. Here’s what Piyush shared with us.Piyush says ” The model was built around 4 sets of features primarily.1. Meta data on User : Age buckets and variable V12. Meta data on Article : Time since article was published, Number of articles by the same author & Number of articles in the same category3. Article Preferences4. Choice of model was Xgboost. Tried an ensemble between 2 xgb models but it gave little boost (from 1.7899 to 1.7895)Solution: Link to Code Rohan RaoRohan is a Senior Data Scientist at Paytm & a Kaggle Grandmaster. Rohan holds multiple accolades on his name. He has won several competitions on Analytics Vidhya and has been actively participating in machine learning competitions. His approach always has interesting insights. Here’s what Rohan shared with us.2. Used raw features + count features initially with XGBoost.3. On plotting feature importance, I found the user-id to be the most important variable. So, split the train data into two halves, and used the average rating of users from one half of the data as a feature in the second half, and built my model on only the second half of the training data. This gave a major boost and the score reached 1.804. I ensembled few XGBoosts with different seeds and to finally get below 1.795. Some final tweaks like directly populating the common IDs between train & test and clipping the predictions between 0 and 6, gave minor improvements as well.Tuning parameters and using linear models didn’t really work. I even tried building a multi-class classification model, but that performed much worse than regression, which is natural considering the metric is RMSE.Solution: Link to Code Mark LandryMark is a Competitive Data Scientist & Product manager at H2O.ai. Mark is also an active participant in machine learning competition on Analytics Vidhya & Kaggle. Mark has won several hackathons and ranked highly for his machine learning skills. He has several other accomplishments on his name.Here’s what Mark shared with us.Mark says,In short, features were created using data.table in R  and the modeling was done with three H2O models: random forest and two GBMs.The progression of the models is actually represented in the way the features are laid out. The initial submission scored 1.94 on the public leaderboard (which scored very close to private) and was quickly tuned to about 1.82. This spanned the first six target encodings, no IDs, only the three Article features directly. Over time, I kept adding more (including a pair that surprisingly reduced model quality) all the way to the end. Light experimentation on the modeling hyperparameters, which are likely underfit. Random forest wound up my strongest model, most likely due to lack of internal CV of the GBMs that led to me staying fairly careful.I kept the User and Article IDs out of the modeling from the start. At one point I used the most frequent User_ID values, but this did not help – the models were already picking up enough of the user qualities. The main feature style is target encoding, so in the code you will see sets of three lines that calculate the sum of response, count of records, and then performs an average that removes the impact of the record to which it is applying the “average”. You’ll see the same style in my Xtreme ML Hack solution (I started from that code, in fact) and also Knocktober 2016.The short duration and unlimited submissions for this competition kept me moving quickly, but at a disadvantage for model tuning. No doubt these parameters are not ideal, but that was just a choice I made to keep the leaderboard as my improvement focus and iterations extremely short, rather than internal validation. Had either the train/test split or public/private split not appeared random, I would have modeled things differently. But this was a fairly simple situation for getting away with such tactics.A 550-tree default Random Forest was my best individual model.Here is the feature utilization.I used a pair of GBMs with slightly different parameters. These features are for the model with 200 trees, a 0.025 learning rate, depth of 5, row sampling of 60% and column sampling of 60%. Thanks to Analytics Vidhya for hosting this competition and all of AV DataFest 2017, as well as all participants who worked on the problem and pushed the leaderboard. This was my first six-hour competition and it was as fun as the half-week competitions.Solution: Link to Code It was great fun interacting with these winners and to know their approach during the competition. Hopefully, you will be able to evaluate where you missed out.",https://www.analyticsvidhya.com/blog/2017/05/winners-solutions-approach-the-quicksolver-minihack-datafest-2017/
41 questions on Statistics for data scientists & analysts,Learn everything about Analytics|Introduction|Overall Scores,"Questions & Solution|End Notes|Learn, compete, hack and get hired!|Share this:|Related Articles|Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017|42 Questions on SQL for all aspiring Data Scientists|
Dishashree Gupta
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Statistics forms the back bone of data science or any analysis for that matter. Sound knowledge of statistics can help an analyst to make sound business decisions.On one hand, descriptive statistics helps us to understand the data and its properties by use of central tendency and variability. On the other hand, inferential statistics helps us to infer properties of the population from a given sample of data. Knowledge of both descriptive and inferential statistics is essential for an aspiring data scientist or analyst.To help you improve your knowledge in statistics we conducted this practice test. The test covered both descriptive and inferential statistics in brief. I am providing the answers with explanation in case you got stuck on particular questions.In case you missed the test, try solving the questions before reading the solutions. Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than  450 people took this test and the highest score obtained was 37. Here are a few statistics about the distribution.Mean Score: 20.40Median Score: 23Mode Score: 25 1) Which of these measures are used to analyze the central tendency of data? A) Mean and Normal DistributionB) Mean, Median and ModeC) Mode, Alpha & RangeD) Standard Deviation, Range and MeanE) Median, Range and Normal DistributionSolution: (B)The mean, median, mode are the three statistical measures which help us to analyze the central tendency of data. We use these measures to find the central value of the data to summarize the entire data set. 2) Five numbers are given: (5, 10, 15, 5, 15). Now, what would be the sum of deviations of individual data points from their mean?A) 10B)25C) 50D) 0E) None of the aboveSolution: (D)The sum of deviations of the individual will always be 0. 3) A test is administered annually. The test has a mean score of 150 and a standard deviation of 20. If Ravi’s z-score is 1.50, what was his score on the test?A) 180
B) 130
C) 30
D) 150
E) None of the aboveSolution: (A)X= μ+Zσ where μ is the mean,  σ is the standard deviation and X is the score we’re calculating. Therefore X = 150+20*1.5 = 180 4) Which of the following measures of central tendency will always change if a single value in the data changes? A) MeanB) MedianC) ModeD) All of theseSolution: (A)The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset. 5) Below, we have represented six data points on a scale where vertical lines on scale represent unit. Which of the following line represents the mean of the given data points, where the scale is divided into same units?A) A
B) B
C) C
D) DSolution: (C)It’s a little tricky to visualize this one by just looking at the data points. We can simply substitute values to understand the mean. Let A be 1, B be 2, C be 3 and so on. The data values as shown will become {1,1,1,4,5,6} which will have mean to be 18/6 = 3 i.e. C. 6) If a positively skewed distribution has a median of 50, which of the following statement is true? A) Mean is greater than 50
B) Mean is less than 50
C) Mode is less than 50
D) Mode is greater than 50
E) Both A and C
F) Both B and DSolution: (E)Below are the distributions for Negatively, Positively and no skewed curves.As we can see for a positively skewed curve, Mode<Median<Mean. So if median is 50, mean would be more than 50 and mode will be less than 50.7) Which of the following is a possible value for the median of the below distribution?A) 32
B) 26
C) 17
D) 40Solution: (B)To answer this one we need to go to the basic definition of a median. Median is the value which has roughly half the values before it and half the values after. The number of values less than 25 are (36+54+69 = 159) and the number of values greater than 30 are (55+43+25+22+17= 162). So the median should lie somewhere between 25 and 30. Hence 26 is a possible value of the median. 8) Which of the following statements are true about Bessels Correction while calculating a sample standard deviation?A)  Only 2B) Only 3C) Both 2 and 3D) Both 1 and 3Solution: (C)Contrary to the popular belief Bessel’s correction should not be always done. It’s basically done when we’re trying to estimate the population standard deviation using the sample standard deviation. The bias is definitely reduced as the standard deviation will now(after correction) be depicting the dispersion of the population more than that of the sample. 9) If the variance of a dataset is correctly computed with the formula using (n – 1) in the denominator, which of the following option is true?A) Dataset is a sample
B) Dataset is a population
C) Dataset could be either a sample or a population
D) Dataset is from a census
E) None of the aboveSolution: (A)If the variance has n-1 in the formula, it means that the set is a sample. We try to estimate the population variance by dividing the sum of squared difference with the mean with n-1.When we have the actual population data we can directly divide the sum of squared differences with n instead of n-1. 10) [True or False] Standard deviation can be negative. A) TRUEB) FALSESolution: (B)Below is the formula for standard deviationSince the differences are squared, added and then rooted, negative standard deviations are not possible. 11) Standard deviation is robust to outliers?A) TrueB) FalseSolution: (B)If you look at the formula for standard deviation above, a very high or a very low value would increase standard deviation as it would be very different from the mean. Hence outliers will effect standard deviation. 12) For the below normal distribution, which of the following option holds true ?σ1, σ2 and σ3 represent the standard deviations for curves 1, 2 and 3 respectively.
A) σ1> σ2> σ3B) σ1< σ2< σ3C) σ1= σ2= σ3D) NoneSolution: (B)From the definition of normal distribution, we know that the area under the curve is 1 for all the 3 shapes. The curve 3 is more spread and hence more dispersed (most of values being within 40-160). Therefore it will have the highest standard deviation. Similarly, Curve 1 has a very low range and all the values are in a small range of 80-120. Hence, curve 1 has the least standard deviation. 13) What would be the critical values of Z for 98% confidence interval for a two-tailed test ?A) +/- 2.33
B) +/- 1.96
C) +/- 1.64
D) +/- 2.55Solution: (A)We need to look at the z table for answering this. For a 2 tailed test, and a 98% confidence interval, we should check the area before the z value as 0.99 since 1% will be on the left side of the mean and 1% on the right side. Hence we should check for the z value for area>0.99. The value will be +/- 2.33 14) [True or False] The standard normal curve is symmetric about 0 and the total area under it is 1.A)TRUEB) FALSESolution: (A)By the definition of the normal curve, the area under it is 1 and is symmetric about zero. The mean, median and mode are all equal and 0. The area to the left of mean is equal to the area on the right of mean. Hence it is symmetric. Context for Questions 15-17Studies show that listening to music while studying can improve your memory. To demonstrate this, a researcher obtains a sample of 36 college students and gives them a standard memory test while they listen to some background music. Under normal circumstances (without music), the mean score obtained was 25 and standard deviation is 6. The mean score for the sample after the experiment (i.e With music) is 28.15) What is the null hypothesis in this case?A) Listening to music while studying will not impact memory.
B) Listening to music while studying may worsen memory.
C) Listening to music while studying may improve memory.
D) Listening to music while studying will not improve memory but can make it worse.Solution: (D)The null hypothesis is generally assumed statement, that there is no relationship in the measured phenomena. Here the null hypothesis would be that there is no relationship between listening to music and improvement in memory. 16) What would be the Type I error?A) Concluding that listening to music while studying improves memory, and it’s right.
B) Concluding that listening to music while studying improves memory when it actually doesn’t.
C) Concluding that listening to music while studying does not improve memory but it does.Solution: (B)Type 1 error means that we reject the null hypothesis when its actually true. Here the null hypothesis is that music does not improve memory. Type 1 error would be that we reject it and say that music does improve memory when it actually doesn’t. 17) After performing the Z-test, what can we conclude ____ ?A) Listening to music does not improve memory.B)Listening to music significantly improves memory at pC) The information is insufficient for any conclusion.D) None of the aboveSolution: (B)Let’s perform the Z test on the given case. We know that the null hypothesis is that listening to music does not improve memory.Alternate hypothesis is that listening to music does improve memory.In this case the standard error i.e. The Z score for a sample mean of 28 from this population isZ critical value for α = 0.05 (one tailed) would be 1.65 as seen from the z table.Therefore since the Z value observed is greater than the Z critical value, we can reject the null hypothesis and say that listening to music does improve the memory with 95% confidence. 18) A researcher concludes from his analysis that a placebo cures AIDS. What type of error is he making?A) Type 1 errorB) Type 2 errorC) None of these. The researcher is not making an error.D) Cannot be determinedSolution: (D)By definition, type 1 error is rejecting the null hypothesis when its actually true and type 2 error is accepting the null hypothesis when its actually false. In this case to define the error, we need to first define the null and alternate hypothesis. 19) What happens to the confidence interval when we introduce some outliers to the data?A) Confidence interval is robust to outliersB) Confidence interval will increase with the introduction of outliers.C) Confidence interval will decrease with the introduction of outliers.D) We cannot determine the confidence interval in this case.Solution: (B)We know that confidence interval depends on the standard deviation of the data. If we introduce outliers into the data, the standard deviation increases, and hence the confidence interval also increases. Context for questions 20- 22A medical doctor wants to reduce blood sugar level of all his patients by altering their diet. He finds that the mean sugar level of all patients is 180 with a standard deviation of 18. Nine of his patients start dieting and the mean of the sample is observed to 175. Now, he is considering to recommend all his patients to go on a diet.Note: He calculates 99% confidence interval.20) What is the standard error of the mean?A) 9
B) 6
C) 7.5
D) 18Solution: (B)The standard error of the mean is the standard deviation by the square root of the number of values. i.e.Standard error =   = 6 21) What is the probability of getting a mean of 175 or less after all the patients start dieting?A) 20%
B) 25%
C) 15%
D) 12%Solution: (A)This actually wants us to calculate the probability of population mean being 175 after the intervention. We can calculate the Z value for the given mean.If we look at the z table, the corresponding value for z = -0.833 ~ 0.2033.Therefore there is around 20% probability that if everyone starts dieting, the population mean would be 175. 22) Which of the following statement is correct?A) The doctor has a valid evidence that dieting reduces blood sugar level.B) The doctor does not have enough evidence that dieting reduces blood sugar level.C) If the doctor makes all future patients diet in a similar way, the mean blood pressure will fall below 160.Solution: (B) We need to check if we have sufficient evidence to reject the null. The null hypothesis is that dieting has no effect on blood sugar. This is a two tailed test. The z critical value for a 2 tailed test would be ±2.58.The z value as we have calculated is -0.833.Since Z value < Z critical value, we do not have enough evidence that dieting reduces blood sugar.Question Context 23-25A researcher is trying to examine the effects of two different teaching methods. He divides 20 students into two groups of 10 each. For group 1, the teaching method is using fun examples. Where as for group 2 the teaching method is using software to help students learn. After a 20 minutes lecture of both groups, a test is conducted for all the students.We want to calculate if there is a significant difference in the scores of both the groups.It is given that:23) What is the value of t-statistic?A) 3.191
B) 3.395
C) Cannot be determined.
D) None of the aboveSolution: (A)The t statistic of the given group is nothing but the difference between the group means by the standard error.=(10-7)/0.94 = 3.191 24) Is there a significant difference in the scores of the two groups?A) Yes
B) NoSolution: (A)The null hypothesis in this case would be that there is no difference between the groups, while the alternate hypothesis would be that the groups are significantly different.The t critical value for a 2 tailed test at α = 0.05 is ±2.101. The t statistic obtained is 3.191. Since the t statistic is more than the critical value of t, we can reject the null hypothesis and say that the two groups are significantly different with 95% confidence. 25) What percentage of variability in scores is explained by the method of teaching?A) 36.13
B) 45.21
C) 40.33
D) 32.97Solution: (A)The % variability in scores is given by the R2 value. The formula for R2 given by  R2 =  The degrees of freedom in this case would be 10+10 -2 since there are two groups with size 10 each. The degree of freedom is 18.R2 =     = 36.13 26) [True or False] F statistic cannot be negative. A) TRUEB) FALSESolution: (A)F statistic is the value we receive when we run an ANOVA test on different groups to understand the differences between them. The F statistic is given by the ratio of between group variability to within group variabilityBelow is the formula for f Statistic.Since both the numerator and denominator possess square terms, F statistic cannot be negative.27) Which of the graph below has very strong positive correlation?
A)
B)
C)
D)Solution: (B)A strong positive correlation would occur when the following condition is met. If x increases, y should also increase, if x decreases, y should also decrease. The slope of the line would be positive in this case and the data points will show a clear linear relationship. Option B shows a strong positive relationship. 28) Correlation between two variables (Var1 and Var2) is 0.65. Now, after adding numeric 2 to all the values of Var1, the correlation co-efficient will_______ ?A) Increase
B) Decrease
C) None of the aboveSolution: (C)If a constant value is added or subtracted to either variable, the correlation coefficient would be unchanged. It is easy to understand if we look at the formula for calculating the correlation.If we add a constant value to all the values of x, the xi and  will change by the same number, and the differences will remain the same. Hence, there is no change in the correlation coefficient. 29) It is observed that there is a very high correlation between math test scores and amount of physical exercise done by a student on the test day. What can you infer from this? A) Only 1
B) 1 and 3
C) 2 and 3
D) All the statements are trueSolution: (C)Though sometimes causation might be intuitive from a high correlation but actually correlation does not imply any causal inference. It just tells us the strength of the relationship between the two variables. If both the variables move together, there is a high correlation among them. 30) If the correlation coefficient (r) between scores in a math test and amount of physical exercise by a student is 0.86, what percentage of variability in math test is explained by the amount of exercise?A) 86%
B) 74%
C) 14%
D) 26%Solution: (B)The % variability is given by r2, the square of the correlation coefficient. This value represents the fraction of the variation in one variable that may be explained by the other variable. Therefore % variability explained would be 0.862.31) Which of the following is true about below given histogram?A) Above histogram is unimodalB) Above histogram is bimodalC) Given above is not a histogramD) None of the aboveSolution: (B)The above histogram is bimodal. As we can see there are two values for which we can see peaks in the histograms indicating high frequencies for those values. Therefore the histogram is bimodal. 32) Consider a regression line y=ax+b, where a is the slope and b is the intercept. If we know the value of the slope then by using which option can we always find the value of the intercept?A) Put the value (0,0) in the regression line TrueB) Put any value from the points used to fit the regression line and compute the value of b FalseC) Put the mean values of x & y in the equation along with the value a to get b FalseD) None of the above can be used FalseSolution: (C)In case of ordinary least squares regression, the line would always pass through the mean values of x and y. If we know one point on the line and the value of slope, we can easily find the intercept. 33) What happens when we introduce more variables to a linear regression model?A) The r squared value may increase or remain constant, the adjusted r squared may increase or decrease.B) The r squared may increase or decrease while the adjusted r squared always increases.C) Both r square and adjusted r square always increase on the introduction of new variables in the model.D) Both might increase or decrease depending on the variables introduced.Solution: (A)The R square always increases or at least remains constant because in case of ordinary least squares the sum of square error never increases by adding more variables to the model. Hence the R squared does not decrease. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. 34) In a scatter diagram, the vertical distance of a point above or below regression line is known as ____ ?A) Residual
B) Prediction Error
C) Prediction
D) Both A and B
E) None of the aboveSolution: (D)The lines as we see in the above plot are the vertical distance of points from the regression line. These are known as the residuals or the prediction error. 35) In univariate linear least squares regression, relationship between correlation coefficient and coefficient of determination is ______ ?A) Both are unrelated FalseB) The coefficient of determination is the coefficient of correlation squared TrueC) The coefficient of determination is the square root of the coefficient of correlation FalseD) Both are same FSolution: (B)The coefficient of determination is the R squared value and it tells us the amount of variability of the dependent variable explained by the independent variable. This is nothing but correlation coefficient squared. In case of multivariate regression the r squared value represents the ratio of the sum of explained variance to the sum of total variance. 36) What is the relationship between significance level and confidence level?A) Significance level = Confidence level
B) Significance level = 1- Confidence level
C) Significance level = 1/Confidence level
D) Significance level = sqrt (1 – Confidence level)Solution: (B)Significance level is 1-confidence interval. If the significance level is 0.05, the corresponding confidence interval is 95% or 0.95. The significance level is the probability of obtaining a result as extreme as, or more extreme than, the result actually obtained when the null hypothesis is true. The confidence interval is the range of likely values for a population parameter, such as the population mean. For example, if you compute a 95% confidence interval for the average price of an ice cream, then you can be 95% confident that the interval contains the true average cost of all ice creams.The significance level and confidence level are the complementary portions in the normal distribution. 37) [True or False] Suppose you have been given a variable V, along with its mean and median. Based on these values, you can find whether the variable “V” is left skewed or right skewed for the conditionA) True
B) FalseSolution: (B)Since, its no where mentioned about the type distribution of the variable V, we cannot say whether it is left skewed or right skewed for sure. 38) The line described by the linear regression equation (OLS) attempts to ____ ?A) Pass through as many points as possible.B)  Pass through as few points as possibleC) Minimize the number of points it touchesD) Minimize the squared distance from the pointsSolution: (D)The regression line attempts to minimize the squared distance between the points and the regression line. By definition the ordinary least squares regression tries to have the minimum sum of squared errors. This means that the sum of squared residuals should be minimized. This may or may not be achieved by passing through the maximum points in the data. The most common case of not passing through all points and reducing the error is when the data has a lot of outliers or is not very strongly linear. 39) We have a linear regression equation ( Y = 5X +40) for the below table. Which of the following is a MAE (Mean Absolute Error) for this linear model?A) 8.4
B) 10.29
C) 42.5
D) None of the aboveSolution: (A)To calculate the mean absolute error for this case, we should first calculate the values of y with the given equation and then calculate the absolute error with respect to the actual values of y. Then the average value of this absolute error would be the mean absolute error. The below table summarises these values. 40) A regression analysis between weight (y) and height (x) resulted in the following least squares line: y = 120 + 5x. This implies that if the height is increased by 1 inch, the weight is expected toA) increase by 1 pound
B) increase by 5 pound
C) increase by 125 pound
D) None of the aboveSolution:  (B)Looking at the equation given y=120+5x. If the height is increased by 1 unit, the weight will increase by 5 pounds. Since 120 will be the same in both cases and will go off in the difference. 41) [True or False] Pearson captures how linearly dependent two variables are whereas Spearman captures the monotonic behaviour of the relation between the variables.A)TRUEB) FALSESolution: (A)The statement is true. Pearson correlation evaluated the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable.The spearman evaluates a monotonic relationship. A monotonic relationship is one where the variables change together but not necessarily at a constant rate. I hope you had fun solving the questions and they did make you scratch your head sometime. Please share your thoughts on the above topics and also your feedback.We shall be happy to incorporate your ideas in further articles and tests. Also, one question might have multiple approaches and the solution above might show just one. I have tried to be descriptive with the solutions but feel free to investigate further in case of doubts using the comments below.",https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/
42 Questions on SQL for all aspiring Data Scientists,Learn everything about Analytics|Introduction|Questions & Solution||End Notes,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|41 questions on Statistics for data scientists & analysts|40 Questions to test your skill in Python for Data Science|
Ankit Gupta
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"SQL is a universal tool in data science. Irrespective of which language you use as your main tool – you need to know SQL. There are no 2 ways about it. For most of the organisations, SQL is the way to store and retrieve structured data form underlying systems. So, if you are an aspiring data scientist or already a data science professional, having . expertise in SQL is a big boon.To help our community test themselves on SQL, we designed a SQL skill test as part of DataFest 2017. More than 1500 people registered in the skill test and more than 500 people took the test. Below is the distribution of scores:You can access the final scores here. Here are a few statistics about the distribution:Mean Score: 18.41Median Score: 20Mode Score: 28 1) Which of the following option(s) is/are correct?A) 1B) 2C) Both 1 and 2D) None of theseSolution: (A)SQL is a querying language and it is not case sensitive. 2) What is true for a Null value in SQL?A) 1 and 3B) 2 and 4C) 1 and 4D) 2 and 3Solution: (A)NULL represents a unknown value so adding anything to null value will give a null value in result. 3) Which of the following is not an aggregrate function in SQL?A) MIN()B) MAX()C) DISTINCT()D) COUNT()Solution: (C)All of the functions except  DISTINCT function given in the question is an example of  aggregate function. 4) Which of the following option is true for the below queries?A) Both queries will give different outputs.B) Both queries will give same outputs.C) Can’t saySolution: (B)Both query will return the same output. 5) Which of the following cannot be a superkey in a relational schema with attributes A,B,C,D,E and primary key AD?A) A B C DB) A C D EC) A B C ED) A B D ESolution: (C)The attributes “A”, “D” should be present in the super key. Option C doen’t have “D” attribute so C would be the right answer. Questions Context 6-7You run the following Queries in the following order:Create a table “Me” using the below SQL query.Next, you create a view based on “Me” table by using the following query. Finally, you run the following query: 6) Which of the following statements are true?A) 1 and 3B) 1 and 4C) 2 and 3D) 2 and 4Solution: (B)Query 2 is used for creating the view on table “Me” so it would run fine but if you run the Query3 it will generate the below error.ERROR:  cannot drop table me because other objects depend on itDETAIL:  view me_view depends on table meHINT:  Use DROP … CASCADE to drop the dependent objects too. 7) Now, you have changed the ‘Query3’ as below.And, you also want to run below query on the same table.Which of the following statements are true for such cases?A) 1 and 3B) 1and 4C) 2 and 3D) 2 and 4Solution: (C)If you drop the base table using cascade it will drop the base table as well as view table also so Query 3 will run fine but Query 4 will give an error. 8) Imagine, you have a column ‘A’ in table1 which is a primary key and it refers to column ‘B’ in table2.Further, column ‘A’ has only three values (1,2,3). Which of the following options is / are correct?A) 1 and 2B) 2 and 3C) 1 and 2D) 3 and 4Solution: (B)You can insert any value except the duplicate values in column A in table 1 but you cannot insert the values other then 1,2 and 3 in columns B in table2 due to foreign key integrity because it is referenced by the column A.9) Consider a table T1 which contains ‘Column A’ in it. Column A is declared as a primary key. Which of the following is true for column “A” in T1?A) 1 and 4B) 2 and 4C) 1 and 3D) 2 and 3Solution: (B)A primary key column cannot contain duplicate and null values. 10) Imagine you have a table “T1” which has three columns “A”, “B” and “C” where A is a “primary key”.Which of the following query will return number of rows present in the table T1A) 1 and 2B) 2 and 3C) 1 and 3D) 1, 2 and 3Solution: (A)Query1 and Query2 will return the same output. 11) Which of the following statement describes the capabilities of UPDATE command most appropriately?A) It can only modify one value of a single columnB) It can update multiple values of a single columnC) It can update one value of multiple columnsD) It can update multiple values of multiple columnsSolution: (D) 12) What is true about indexing in a database?A) Search will be faster after you have indexed the databaseB) Search will be slower after using indexingC) Indexing has nothing to do with searchD) None of theseSolution: (A)Option A is correct. Read more here. 13) Consider three tables T1, T2 and T3. Table T1, T2 and T3 have 10, 20, 30 number of records respectively. Further, there are some records which are common in all three tables.You want to apply a cartesian product on these three tables. How many rows will be available in cartesian product of these tables?A) 6000B) More than 6000C) Less than 6000D) None of theseSolution: (A) 14) Tables A, B have three columns (namely: ‘id’, ‘age’, ‘name’) each. These tables have no null values and there are 100 records in each of the table.Here are two queries based on these two tables A and B.Which of the following statement is correct for the output of each query?A) The number of tuples in the output of Query 1 will be more than or equal to the output of Query 2B) The number of tuples in the output of Query 1 will be equal to the output of Query 2C) The number of tuples in the output Query 1 will be less than or equal to the output of Query 2D) Can’t saySolution: (C)Answer C is correct because natural join always give either same or less number of rows if you compare it with cartesian product.  To know more read from this tutorial. 15) What will be the output of the following query in PostgreSQL?A)B)C)D)
Solution: (C)It will give the year differece in output so answer C is correct. 16) Imagine you are given the following table named “AV”. And you want to run the following queries Q1, Q2 and Q3 given below. Which sequence for the three queries will not result in an error?A) Q1 -> Q2 -> Q3B) Q2 -> Q1 -> Q3C) Q3 -> Q1 -> Q2D) Q2 -> Q3 -> Q1Solution: (D)“DROP  TABLE” will drop the table as well as it’s reference. So, you can’t access the table once you have dropped it. But in case of “DELETE TABLE” reference will not be droped so you can still access the table if you use “DELETE TABLE” command. 17) Imagine you are given the following table named “AV”. You apply the following query Q1 on AV, which is given below:What will be the output for query Q1?A)B)C)D)Solution: (A)The boundary salaries (200 and 500) will also be in the out so A is the right answer. 18) Imagine you are given the following table named “AV”. What would be the output for the following query?A)B)C)Solution: (B) Question Context 19-21 Assume you are given the two tables AV1 and AV2 which represent two different departments of AV.
AV1 TABLEAV2 TABLE19) Now, you want the names of all people who work in both the departments. Which of the following SQL query would you write?A) SELECT NAME FROM AV1 INTERSECT SELECT NAME FROM AV2;B) SELECT NAME FROM AV1 UNION SELECT NAME FROM AV2;C) SELECT NAME FROM AV1 DIFFERENCE SELECT NAME FROM AV2;D) None of theseSolution: (A)INTERSECT would be used for such output. 20) What is the output for the below query?A)B)C) ERRORD) None of theseSolution: (A)This query will give the names in AV1 which are not present in AV2. 21) What will be the output of below query?A)B)C) None of theseSolution: (B) Question Context 22-24Suppose you are given the below table called A_V. 22) What is the output for the below query?A)B)C) ERROR D) None of theseSolution: (B)  23) What is the output for the below query?A)
B)C)D) None of theseSolution: (B)First replace null value will be replaced to 2 using COALESCE  then 100 will be added. 24) What is the output for the below query? A)B) Empty outputC)ErrorD)None of theseSolution: (B)SQL is not case sensitive but when you search for something in a string column it becomes case sensitive. So output will have zero rows because ‘Ankit’ != ‘ANKIT’ and ‘Faizan’ != ‘FAIZAN’.25) You are given a string ” AnalyticsVidhya “. The string contains two unnecessary spaces – one at the start and another at the end. You find out the length of this string by applying the below queries. If op1, op2, op3, op4 are the output of the Query 1, 2, 3 and 4 respectively, what will be the correct relation between these four queries?A) 1 or 2B) 2C) 3D) 1 and 4Solution: (D)Option D is correct. For more information read from this tutorial. Questions Context 26-27Below you are given a table “split”. 26) Now, you want to apply a query on this.What is the output for the above query?A)B)C)D)E) ErrorSolution:(E)The query will give the below error.ERROR:  field position must be greater than zero27) In the above table “split”, you want to replace some characters using “translate” command. Which of the following will be the output of the following query?A)B)C) ErrorD)None of theseSolution: (A)In the above query character “A” will replace to “1”, “B” to 2 and “C” to 3.  28) Which of the following query will list all station names which contain their respective city names. For example, station “Mountain View Caltrain Station” is for city “Mountain View”.Refer to the table below this question.A) select * from station where station_name like ‘%’  || city || ‘%’;B) select *  from station where city like ‘%’  || station_name || ‘%’ ;C) ErrorD) None of these Solution: (A) 29) Consider the following legal instance of a relational schema S with attributes ABC.Which of following functional dependencies is/are not possible?A) 1 and 2B) 2 and 3C) 1 and 3D) None of aboveSolution: (D)Read from this tutorial.
30) Suppose you have a table called “Student” and this table has a column named “marks”. Now, you apply Query1 on “Student” table.After this, you create an index on column “marks” and then you re-run Query 2 (same as Query 1).If Query 1 is taking time T1 and Query 2 is taking time T2.Which of the following is true for the query time?A) T1 > T2B) T2 > T1C) T1 ~ T2D) None of theseSolution: (C)To search fast you need to create the index on marks*100 but in the question we have created the index on marks.31) Suppose you have 1000 records in a table called “Customers”. You want to select top 100 records from it. Which of the below commands can you use?A) 1B) 2C) 1 and 2D) None of themSolution: (C) Both query can be used to get the desired output. 32) Which of the following is the outcome of the following query?A) Faizan and Ankit are close friendsB) Ankit and Ankit are close friendsC) Faizan and Faizan are close friends
D) Ankit and Faizan are close friendsSolution: (B)“Faizan” will be replaced by “Ankit”. 33) Which one of the following queries always gives the same answer as the nested “Query” shown below. A) select R.* from R, S where R.a=S.aB) select distinct R.* from R,S where R.a=S.aC) select R.* from R,(select distinct a from S) as S1 where R.a=S1.aD) None of aboveSolution: (C)Option C is correct. Question Context 34-35Consider the following table “avian” (id, name, sal).34) Which of the following options will be required at the end of the following SQL query?So that the appended query finds out the name of the employee who has the maximum salary?A) WHERE P1.sal >= Any (select P2.sal from avian P2)B) WHERE P1.sal <= All(select max(P2.sal) from avian P2)C) WHERE P1.sal > Any (select max(P2.sal) from avian P2)D) WHERE P1.sal >= Any (select max(P2.sal) from avian P2)Solution: (D)B – Returns the addresses of all theaters.
C – Returns null set. max() returns a single value and there won’t be any value > max.
D – Returns null set. Same reason as C. All and ANY works the same here as max returns a single value. 35) Which of the following options can be used to find the name of the person with second highest salary?A) select max(sal) from avian where sal < (select max(sal) from avian)B) BothC) None of theseSolution: (B)Query in the option B”(select max(sal) from avian)”first return the highest salary(say H) then the query“(select max(sal) from avian where sal < H )”will search for highest salary which is less then H. Question Context 36-39Suppose you are given a database of bike sharing which has three tables: Station, Trip and Weather.Station Table Trip Table Weather Table 36) Imagine, you run following query on above schema.Which of the following option is correct for this query? A) This query will print city name and number of stations sorted by number of stations in increasing magnitude. If number of stations are same, it will print by decreasing order of city name.B) This query will print city name and number of stations sorted by city name in increasing magnitude. For cities with same name, it will print by decreasing order of number of stations.C) None of theseSolution: (A)A is correct answer. 37) Which of the following query will find the percentage (round to 5 decimal places) of self-loop trips (which start and end at the same station) among all trips? A)B)C)D) None of theseSolution: (A)Query in option A will give the desired result 38 Which of the following statements is / are true for the below query?Note: All the zip_code are present in table weather also present in station tableA) 1 and 2B) 1 and 3C) 1D) 1,2 and 3Solution: (A) 39) What will be the output of the following query?C) ErrorD) None of theseSolution: (B)This query will find a cumulative traveling durations of bike 301. Question Context 40-42Suppose you are given 4 tables: Team, Player, Game and GameStats. Below are the SQL statements which create these tables. 40) Which of the following query will return distinct names of the players who play at “Guard” Position and their name contains “Jo”. (ORDER BY A) A) SELECT name FROM player WHERE position=’Guard’ AND name LIKE ‘%jo%’ ORDER BY nameB) SELECT name FROM player WHERE position=’Guard’ AND name LIKE ‘%Jo%’ ORDER BY nameC) Both of themD) None of themSolution: (B)This query Finds any values that have “Jo” in any position using ‘%jo%’ expression in command. Notice that ‘Jo’ is different then ‘jo’ because expression in like operator is case sensitive. 41) What will be the output for the below query?A) Return the number of games where ‘Saurabh’ scored more points than ‘Faizan’B) Return the number of games where ‘Saurabh’ scored less points than ‘Faizan’C) ErrorD) None of theseSolution: (A) 42) What is the expected output of the following query?A) List all players’ playerIDs and their average points in all home games that they played in (ORDER BY Players’ playerID)B) List all players’ playerIDs and their average points in all games that they played in (ORDER BY Players’ playerID)C) ErrorD) None of theseSolution: (A)I hope you enjoyed the questions and were able to test your knowledge about SQL. Irrespective of what role you are in data science, you need to know SQL. If you haven’t done already, take time out to undergo the test and reflect on where you went wrong.If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/questions-sql-for-all-aspiring-data-scientists/
40 Questions to test your skill in Python for Data Science,Learn everything about Analytics|Questions & Answers,"End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|42 Questions on SQL for all aspiring Data Scientists|40 questions to test your skill on R for Data Science|
Faizan Shaikh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Python is increasingly becoming popular among data science enthusiasts, and for right reasons. It brings the entire ecosystem of a general programming language. So you can not only transform and manipulate data, but you can also create strong pipelines and machine learning workflows in a single ecosystem.At Analytics Vidhya, we love Python. Most of us use Python as our preferred tool for machine learning. Not only this, if you want to learn Deep Learning, Python clearly has the most mature ecosystem among all other languages.If you are learning Python for Data Science, this test was created to help you assess your skill in Python. This test was conducted as part of DataFest 2017. Close to 1,300 people participated in the test with more than 300 people taking this test.Below are the distribution scores of the people who took the test:You can access the final scores here. Here are a few statistics about the distribution.Mean Score: 14.16Median Score: 15Mode Score: 0 Question Context 1You must have seen the show “How I met your mother”. Do you remember the game where they played, in which each person drinks a shot whenever someone says “but, um”. I thought of adding a twist to the game. What if you could use your technical skills to play this game?
To identify how many shots a person is having in the entire game, you are supposed to write a code.
Below is the subtitle sample script.Note: Python regular expression library has been imported as re.1) Which of the following codes would be appropriate for this task?A) len(re.findall(‘But, um’, txt))B) re.search(‘But, um’, txt).count()C) len(re.findall(‘[B,b]ut, um’, txt))D) re.search(‘[B,b]ut, um’, txt)).count()Solution: (C)You have to find both capital and small versions of “but” So option C is correct. Question Context 2Suppose you are given the below stringstr = “””Email_Address,Nickname,Group_Status,Join_Year
[email protected],aa,Owner,2014
[email protected],bb,Member,2015
[email protected],cc,Member,2017
[email protected],dd,Member,2016
[email protected],ee,Member,2020
“””In order to extract only the domain names from the email addresses from the above string (for eg. “aaa”, “bbb”..) you write the following code:2) What number should be mentioned instead of “__” to index only the domains?Note: Python regular expression library has been imported as re. A) 0B) 1C) 2D) 3Solution: (C)Read syntax of regular expression re.
 Question Context 3Your friend has a hypothesis – “All those people who have names ending with the sound of “y” (Eg: Hollie) are intelligent people.” Please note: The name should end with the sound of ‘y’ but not end with alphabet ‘y’.Now you being a data freak, challenge the hypothesis by scraping data from your college’s website. Here’s data you have collected. 
You want to make a list of all people who fall in this category. You write following code do to the same:3) What should be the value of “pattern” in regular expression?Note: Python regular expression library has been imported as re.A) pattern = ‘(i|ie)(,)’B) pattern = ‘(i$|ie$)(,)’C) pattern = ‘([a-zA-Z]+i|[a-zA-Z]+ie)(,)’D) None of theseSolution: (B)You have to find the pattern the end in either “i” or “ie”. So option B is correct. Question Context 4Assume, you are given two lists:a = [1,2,3,4,5] b = [6,7,8,9]The task is to create a list which has all the elements of a and b in one dimension.Output:a = [1,2,3,4,5,6,7,8,9]4) Which of the following option would you choose?A) a.append(b)B) a.extend(b)C) Any of the aboveD) None of theseSolution: (B)Option B is correct 5) You have built a machine learning model which you wish to freeze now and use later. Which of the following command can perform this task for you?Note: Pickle library has been imported as pkl.
A) push(model, “file”)B) save(model, “file”)C) dump(model, “file”)D) freeze(model, “file”)Solution: (C)Option C is correct Question Context 6We want to convert the below string in date-time value:6) To convert the above string, what should be written in place of date_format?A) “%d/%m/%y”B) “%D/%M/%Y”C) “%d/%M/%y”D) “%d/%m/%Y”Solution: (D)Option D is correct Question Context 7I have built a simple neural network for an image recognition problem. Now, I want to test if I have assigned the weights & biases for the hidden layer correctly. To perform this action, I am giving an identity matrix as input. Below is my identity matrix:A =  [ 1, 0, 0
0, 1, 0
0, 0, 1]7) How would you create this identity matrix in python?Note: Library numpy has been imported as np.A) np.eye(3)B) identity(3)C) np.array([1, 0, 0], [0, 1, 0], [0, 0, 1])D) All of theseSolution: (A)Option B does not exist (it should be np.identity()). And option C is wrong, because the syntax is incorrect. So the answer is option A 8) To check whether the two arrays occupy same space, what would you do?I have two numpy arrays “e” and “f”.You get the following output when you print “e” & “f”When you change the values of the first array, the values for the second array also changes. This creates a problem while processing the data.For example, if you set the first 5 values of e as 0; i.e.the final values of e and f areYou surmise that the two arrays must have the same space allocated.A) Check memory of both arrays, if they match that means the arrays are same.B) Do “np.array_equal(e, f)” and if the output is “True” then they both are sameC) Print flags of both arrays by e.flags and f.flags; check the flag “OWNDATA”. If one of them is False, then both the arrays have same space allocated.D) None of theseSolution: (C)Option C is correct Question Context 9Suppose you want to join train and test dataset (both are two numpy arrays train_set and test_set) into a resulting array (resulting_set) to do data processing on it simultaneously. This is as follows:9) How would you join the two arrays?Note: Numpy library has been imported as npA) resulting_set = train_set.append(test_set)B) resulting_set = np.concatenate([train_set, test_set])C) resulting_set = np.vstack([train_set, test_set])D) None of theseSolution: (C)Both option A and B would do horizontal stacking, but we would like to have vertical stacking. So option C is correct Question Context 10Suppose you are tuning hyperparameters of a random forest classifier for the Iris dataset.10) What would be the best value for “random_state (Seed value)”?A) np.random.seed(1)B) np.random.seed(40)C) np.random.seed(32)D) Can’t saySolution: (D)There is no best value for seed. It depends on the data. Question 11While reading a csv file with numpy, you want to automatically fill missing values of column “Date_Of_Joining” with date “01/01/2010”. 11) Which command will be appropriate to fill missing value while reading the file with numpy? Note: numpy has been imported as npA) filling_values = (“-“, 0, 01/01/2010, 0)
temp = np.genfromtxt(filename, filling_values=filling_values)B) filling_values = (“-“, 0, 01/01/2010, 0)
temp = np.loadtxt(filename, filling_values=filling_values)C) filling_values = (“-“, 0, 01/01/2010, 0)
temp = np.gentxt(filename, filling_values=filling_values)D) None of theseSolution: (A)Option A is correct 12) How would you import a decision tree classifier in sklearn?A) from sklearn.decision_tree import DecisionTreeClassifierB) from sklearn.ensemble import DecisionTreeClassifierC) from sklearn.tree import DecisionTreeClassifierD) None of theseSolution: (C)Option C is correct 13) You have uploaded the dataset in csv format on google spreadsheet and shared it publicly. You want to access it in python, how can you do this?Note: Library StringIO has been imported as StringIO.C) D) None of theseSolution: (A)Option A is correct Question Context 14Imagine, you have a dataframe train file with 2 columns & 3 rows, which is loaded in pandas.import pandas as pdNow you want to apply a lambda function on “features” column: 14) What will be the output of following print command?A)0    A B C
1    A D E
2    C D F B)0    AB1    ADE2    CDFC) Error
D) None of theseSolution: (A)Option A is correct Question Context 15We have a multi-class classification problem for predicting quality of wine on the basis of its attributes. The data is loaded in a dataframe “df”The quality column currently has values 1 to 10, but we want to substitute this by a binary classification problem. You want to keep the threshold for classification to 5, such that if the class is greater than 5, the output should be 1, else output should be 0.15) Which of the following codes would help you perform this task?Note: Numpy has been imported as np and dataframe is set as df.A)B)C)D)None of theseSolution: (A)Option A is correct Question Context 16Suppose we make a dataframe as16) What is the difference between the two data series given below?Note: Pandas has been imported as pdA) 1 is view of original dataframe and 2 is a copy of original dataframe.B) 2 is view of original dataframe and 1 is a copy of original dataframe.C) Both are copies of original dataframe.D) Both are views of original dataframeSolution: (B)Option B is correct. Refer the official docs of pandas library. Question Context 17Consider a function “fun” which is defined below:Now you define a list which has three numbers in it.g = [10,11,12]17) Which of the following will be the output of the given print statement:A) [5, 11, 12] [5, 11, 12]
B) [5, 11, 12] [10, 11, 12]
C) [10, 11, 12] [10, 11, 12]
D) [10, 11, 12] [5, 11, 12]
Solution: (A)Option A is correct Question Context 18Sigmoid function is usually used for creating a neural network activation function. A sigmoid function is denoted as18) It is necessary to know how to find the derivatives of sigmoid, as it would be essential for backpropagation. Select the option for finding derivative?A) B)C)D) None of theseSolution: (C)Option C is correct Question Context 19Suppose you are given a monthly data and you have to convert it to daily data.For example,

For this, first you have to expand the data for every month (considering that every month has 30 days)19) Which of the following code would do this?Note: Numpy has been imported as np and dataframe is set as df.A) new_df = pd.concat([df]*30, index = False)B) new_df = pd.concat([df]*30, ignore_index=True)C) new_df = pd.concat([df]*30, ignore_index=False)D) None of theseSolution: (B)Option B is correct Context: 20-22Suppose you are given a dataframe df.20) Now you want to change the name of the column ‘Count’ in df to ‘Click_Count’. So, for performing that action you have written the following code.What will be the output of print statement below?Note: Pandas library has been imported as pd.A) [‘Click_Id’, ‘Click_Count’]
B) [‘Click_Id’, ‘Count’]
C) ErrorD) None of theseSolution: (B)Option B is correct Context: 20-22Suppose you are given a data frame df.21) In many data science projects, you are required to convert a dataframe into a dictionary. Suppose you want to convert “df” into a dictionary such that ‘Click_Id’ will be the key and ‘Count’ will be the value for each key. Which of the following options will give you the desired result?Note: Pandas library has been imported as pdA) set_index(‘Click_Id’)[‘Count’].to_dict()B) set_index(‘Count’)[‘Click_Id’].to_dict()C) We cannot perform this task since dataframe and dictionary are different data structuresD) None of these Solution: (A)Option A is correct 22) In above dataframe df. Suppose you want to assign a df to df1, so that you can recover original content of df in future using df1 as below.Now you want to change some values of “Count” column in df.Which of the following will be the right output for the below print statement?Note: Pandas library has been imported as pd.A) [200 200 300 400 250] [200 200 300 400 250]
B) [100 200 300 400 250] [100 200 300 400 250]
C) [200 200 300 400 250] [100 200 300 400 250]
D) None of theseSolution: (A)Option A is correct 23) You write a code for preprocessing data, and you notice it is taking a lot of time. To amend this, you put a bookmark in the code so that you come to know how much time is spent on each code line. To perform this task, which of the following actions you would take?A) 1 & 2B) 1,2 & 3C) 1,2 & 4D) All of the above Solution: (C)Option C is correct 24) How would you read data from the file using pandas by skipping the first three lines?Note: pandas library has been imported as pd In the given file (email.csv), the first three records are empty.A) read_csv(‘email.csv’, skip_rows=3)B) read_csv(‘email.csv’, skiprows=3)C) read_csv(‘email.csv’, skip=3)D) None of theseSolution: (B)Option B is correct 25) What should be written in-place of “method” to produce the desired outcome?Given below is dataframe “df”:Now, you want to know whether BMI and Gender would influence the sales.For this, you want to plot a bar graph as shown below:The code for this is:A) stacked=TrueB) stacked=FalseC) stack=FalseD) None of theseSolution: (A)It’s a stacked bar chart. 26) Suppose, you are given 2 list – City_A and City_B.City_A = [‘1′,’2′,’3′,’4’]City_B = [‘2′,’3′,’4′,’5’]In both cities, some values are common. Which of the following code will find the name of all cities which are present in “City_A” but not in “City_B”.A) [i for i in City_A if i not in City_B]
B) [i for i in City_B if i not in City_A]
C) [i for i in City_A if i in City_B]
D) None of theseSolution: (A)Option A is correct Question Context 27Suppose you are trying to read a file “temp.csv” using pandas and you get the following error.27) Which of the following would likely correct this error?Note: pandas has been imported as pdA) pd.read_csv(“temp.csv”, compression=’gzip’)B) pd.read_csv(“temp.csv”, dialect=’str’)C) pd.read_csv(“temp.csv”, encoding=’utf-8′)D) None of theseSolution: (C)Option C is correct, because encoding should be ‘utf-8’ 28) Suppose you are defining a tuple given below:tup = (1, 2, 3, 4, 5 )Now, you want to update the value of this tuple at 2nd index to 10. Which of the following option will you choose?A) tup(2) = 10B) tup[2] = 10C) tup{2} = 10D) None of theseSolution: (D)A tuple cannot be updated. 29) You want to read a website which has url as “www.abcd.org”. Which of the following options will perform this task?A) urllib2.urlopen(www.abcd.org)B) requests.get(www.abcd.org)C) Both A and BD) None of theseSolution: (C) Option C is correct Question Context 30Suppose you are given the below web page30) To read the title of the webpage you are using BeautifulSoup. What is the code for this? Hint: You have to extract text in title tagSolution: (B)Option B is correct
Question Context 31Imagine, you are given a list of items in a DataFrame as below.D = [‘A’,’B’,’C’,’D’,’E’,’AA’,’AB’]Now, you want to apply label encoding on this list for importing and transforming, using LabelEncoder.31) What will be the output of the print statement below ?Solution: (D)Option D is correct 32) Which of the following will be the output of the below print statement?Assume, you have defined a data frame which has 2 columns.A) 0   False
1    False
2   False
3    FalseB)  0    False
1    False
2    True
3    FalseC) 0 True
1    True
2    True
3    TrueD) None of theseSolution: (A)Option A is correct 33) Suppose the data is stored in HDFS format and you want to find how the data is structured. For this, which of the following command would help you find out the names of HDFS keys?Note: HDFS file has been loaded by h5py as hf.A) hf.key()B) hf.keyC) hf.keys()D) None of theseSolution: (C)Option C is correct Question Context 34You are given reviews for movies below:reviews = [‘movie is unwatchable no matter how decent the first half is  . ‘, ‘somewhat funny and well  paced action thriller that has jamie foxx as a hapless  fast  talking hoodlum who is chosen by an overly demanding’, ‘morse is okay as the agent who comes up with the ingenious plan to get whoever did it at all cost .’]Your task is to find sentiments from the review above. For this, you first write a code to find count of individual words in all the sentences.34)What value should we split on to get individual words?Solution: (A)Option A is correct 35) How to set a line width in the plot given below?For the above graph, the code for producing the plot wasSolution: (C)Option C is correct 36) How would you reset the index of a dataframe to a given list? The new index is given as:new_index=[‘Safari’,’Iceweasel’,’Comodo Dragon’,’IE10′,’Chrome’]Note: df is a pandas dataframeA) df.reset_index(new_index,)B) df.reindex(new_index,)C) df.reindex_like(new_index,)D) None of theseSolution: (A)Option A is correct 
37) Determine the proportion of passengers survived based on their passenger class. Solution: (A)Option A is correct 38) You want to write a generic code to calculate n-gram of the text. The 2-gram of this sentence would be [[“this, “is”], [“is”, “a”], [“a, “sample”], [“sample”, “text”]]
Which of the following code would be correct?For a given a sentence:
‘this is a sample text’.Solution: (B)Option B is correct 39) Which of the following code will export dataframe (df) in CSV file, encoded in UTF-8 after hiding index & header labels.Solution: (C)Option C is correct 40) Which of the following is a correct implementation of mean squared error (MSE) metric?Note: numpy library has been imported as np.Solution: (B)Option B is correct If you are learning Python, make sure you go through the test above. It will not only help you assess your skill. You can also see where you stand among other people in the community. If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/questions-python-for-data-science/
40 questions to test your skill on R for Data Science,Learn everything about Analytics|Questions & Answers,"End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|40 Questions to test your skill in Python for Data Science|40 Questions to test a data scientist on Machine Learning [Solution: SkillPower – Machine Learning, DataFest 2017]|
NSS
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"R is one of the most popular language among the data science community. If you are serious about data science, chances are that you either already know R or are learning it. R also has a thriving ecosystem of various statistics and data science libraries. In order to help our community test themselves on their knowledge of R, we created these skill tests as part of DataFest 2017.More than 1500 people registered for this skill test and close to 500 people took this test. Below is the distribution of scores from various participants:You can access the final scores here. Here are a few statistics about the distribution:Mean Score: 16.69Median Score: 19Mode Score: 0We are releasing the solutions to the skill tests, so that you can evaluate where you went wrong. If you missed the test, you can still look at the Questions and Answers to see where you stand.Happy Learning! Question Context 1Consider the following function.1)If we execute following commands (written below), what will be the output?z <- 10f(4)A) 12B) 7C) 4D) 16Solution: (A)Scoping rule of R will cause z<-4 to take precedence over z<-10. Hence, g(x) will return a value of 8. Therefore, option A is the correct answer. Question context 2The iris dataset has different species of flowers such as Setosa, Versicolor and Virginica with their sepal length. Now, we want to understand the distribution of sepal length across all the species of flowers. One way to do this is to visualise this relation through the graph shown below.2) Which function can be used to produce the graph shown above? A) xyplot()
B) stripplot()
C) barchart()
D) bwplot()Solution: (B)The plot above is of type strip whereas the options a, c and d will produce a scatter, bar and box whisker plot respectively. Therefore, option B is the correct solution. Question Context 3File Name – Dataframe.csv3) Which of the following commands will correctly read the above csv file with 5 rows in a dataframe? A) csv(‘Dataframe.csv’)B) csv(‘Dataframe.csv’,header=TRUE)C) dataframe(‘Dataframe.csv’)D) csv2(‘Dataframe.csv’,header=FALSE,sep=’,’)Solution: (D)Options 1 and 2 will read the first row of the above dataframe as header. Option 3 doesn’t exist. Therefore, option D is the correct solution. Question Context 4Excel file format is one of the most common formats used to store datasets. It is important to know how to import an excel file into R. Below is an excel file in which data has been entered in the third sheet.File Name – Dataframe.xlsx4) Which of the following codes will read the above data in the third sheet into a dataframe in R?A) Openxlsx::read.xlsx(“Dataframe.xlsx”,sheet=3,colNames=FALSE)B) Xlsx::read.xlsx(“Dataframe.xlsx”,sheetIndex=3,header=FALSE)C)XLConnect::readWorksheetFromFile(“Dataframe.xlsx”,sheet=3,header=FALSE)D)All of the aboveSolution: (D) All of the above options are true, as they give out different methods to read an excel file into R and reads the above file correctly. Therefore, option D is the correct solution. Question Context 5File Name – Dataframe.csv5) Missing values in this csv file has been represented by an exclamation mark (“!”) and a question mark (“?”). Which of the codes below will read the above csv file correctly into R?A) csv(‘Dataframe.csv’)B) csv(‘Dataframe.csv’,header=FALSE, sep=’,’,na.strings=c(‘?’))C) csv2(‘Dataframe.csv’,header=FALSE,sep=’,’,na.strings=c(‘?’,’!’))D) dataframe(‘Dataframe.csv’)Solution: (C) Option A will not be able to read “?” and “!” as NA in R. option B will be able to read only “?” as NA but not “!”. Option 4 doesn’t exist. Therefore, option C is the correct solution. Question Context  6-7File Name – Dataframe.csv6) The above csv file has row names as well as column names. Which of the following code will read the above csv file properly into R?A) delim(‘Train.csv’,header=T,sep=’,’,row.names=TRUE)B) csv2(‘Train.csv’,header=TRUE, row.names=TRUE)C) dataframe(‘Train.csv’,header=TRUE,sep=’,’)D) csv(‘Train.csv’,,header=TRUE,sep=’,’)Solution: (D)row.names argument in options A and B takes only the vector containing the actual row names or a single number giving the column of the table which contains the row names and not a logical value. Option C doesn’t exist. Therefore, option D is the correct solution.  Question Context 6-7File Name – Dataframe.csv7) Which of the following codes will read only the first two rows of the csv file?A) csv(‘Dataframe.csv’,header=TRUE,row.names=1,sep=’,’,nrows=2B) csv2(‘Dataframe.csv’,row.names=1,nrows=2)C) delim2(‘Dataframe.csv’,header=T,row.names=1,sep=’,’,nrows=2)D) dataframe(‘Dataframe.csv’,header=TRUE,row.names=1,sep=’,’,skip.last=2)Solution: (A)Option B will not be able to read the csv file correctly since the default separator in csv2 function is “;” whereas csv files are of type “,”. Option C has wrong header argument value. Option D doesn’t exist. Therefore, Option  A is the correct answer.Question Context 88) There are two dataframes stored Dataframe1 and Dataframe2 shown above. Which of the following codes will produce the output shown below?A) merge(dataframe[,1:3],dataframe2)B) merge(dataframe1,dataframe2)[,1:3]
C) merge(dataframe1,dataframe2,all=TRUE)D) Both 1 and 2E) All of the aboveSolution: (D)Option C will result in feature 4 being included in the merged dataframe which is what we do not want. Therefore, Option D is the correct solution.Question Context 9dataframe9) A data set has been read in R and stored in a variable “dataframe”. Which of the below codes will produce a summary (mean, mode, median) of the entire dataset in a single line of code?A) summary(dataframe)B) stats(dataframe)C) summarize(dataframe)D) summarise(dataframe)E) None of the aboveSolution: (E)Option A will give only the mean and the median but not the mode. Option B, C and D will also fail to provide the required statistics. Therefore, Option E is the correct solution.Question Context 10A dataset has been read in R and stored in a variable “dataframe”. Missing values have been read as NA.dataframe10) Which of the following codes will not give the number of missing values in each column?A) colSums(is.na(dataframe))B) apply(is.na(dataframe),2,sum)C) sapply(dataframe,function(x) sum(is.na(x))D) table(is.na(dataframe))Solution: (D)Option D will give the overall count of the missing values but not column wise. Therefore, Option D is the correct solution. Question context 11One of the important phase in a Data Analytics pipeline is univariate analysis of the features which includes checking for the missing values and the distribution, etc. Below is a dataset and we wish to plot histogram for “Value” variable.dataframed11) Which of the following commands will help us perform that task ? A) hist(dataframed$Value)B) ggplot2::qplot(dataframed$Value,geom=”Histogram”)C)ggplot2::ggplot(data=dataframed,aes(dataframe$Value))+geom_histogram()D) All of the aboveSolution: (D)All of the given options will plot a histogram and that can be used to see the skewness of the desired data. Question Context 12Certain Algorithms like XGBOOST work only with numerical data. In that case, categorical variables present in dataset are first converted to DUMMY variables which represent the presence or absence of a level of a categorical variable in the dataset. For example After creating the Dummy Variable for the feature “Parameter”, the dataset looks like below.12) Which of the following commands will help us to achieve this?A) dummies:: dummy.data.frame(dataframe,names=c(‘Parameter’))B) dataframe$Parameter_Alpha=0dataframe$Gende_Beta=0dataframe$Parameter_Alpha[which(dataframe$Parameter==’Alpha’)]=1dataframe$Parameter_Beta[which(dataframe$Parameter==’Alpha’)]=0dataframe$Parameter_Alpha[which(dataframe$Parameter==’Beta’]=0dataframe$Parameter_Beta[which(dataframe$Parameter==’Beta’]=1C) contrasts(dataframe$Parameter)D) Both 1 and 2Solution: (D)Option C will encode the Parameter column will 2 levels but will not perform one hot encoding. Therefore, option D is the correct solution. Question context 13dataframe13) We wish to calculate the correlation between “Column2” and “Column3” of a “dataframe”. Which of the below codes will achieve the purpose?A) corr(dataframe$column2,dataframe$column3)B) (cov(dataframe$column2,dataframe$column3))/(var(dataframe$column2)*sd(dataframe$column3))C)(sum(dataframe$Column2*dataframe$Column3)-                 (sum(dataframe$Column2)*sum(dataframe$Column3)/nrow(dataframe)))/(sqrt((sum(dataframe$Column2*dataframe$Column2)-(sum(dataframe$Column2)^3)/nrow(dataframe))*     (sum(dataframe$Column3*dataframe$Column3)-(sum(dataframe$Column3)^2)/nrow(dataframe))))D) None of the AboveSolution: (D)In option A, corr is the wrong function name. Actual function name to calculate correlation is cor. In option B, it is the standard deviation which should be the denominator and not variance. Similarly, the formula in Option C is wrong. Therefore, Option D is the correct solution. Question Context 14dataframe14) The above dataset has been loaded for you in R in a variable named “dataframe” with first row representing the column name. Which of the following code will select only the rows for which parameter is Alpha?A) subset(dataframe, Parameter=’Alpha’)B) subset(dataframe, Parameter==’Alpha’)C) filter(dataframe,Parameter==’Alpha’)D) Both 2 and 3E) All of the aboveSolution: (D)In option A, there should be an equality operator instead of the assignment operator. Therefore, option D is the correct solution.15) Which of the following function is used to view the dataset in spreadsheet like format?A) disp()B) View()C) seq()D) All of the AboveSolution : (B)Option B is the only option that will show the dataset in the spreadsheet format. Therefore, option B is the correct solution. Question Context 16The below dataframe is stored in a variable named data.data16) Suppose B is a categorical variable and we wish to draw a boxplot for every level of the categorical level. Which of the below commands will help us achieve that?A) boxplot(A,B,data=data)B) boxplot(A~B,data=data)C) boxplot(A|B,data=data)D) None of the aboveSolution: (B)Boxplot function in R requires a formula input to draw different boxplots by levels of a factor variable. Therefore, Option B is the correct solution. 17) Which of the following commands will split the plotting window into 4 X 3 windows and where the plots enter the window column wise.A) par(split=c(4,3))B) par(mfcol=c(4,3))C) par(mfrow=c(4,3))D) par(col=c(4,3))Solution: (B)mfcol argument will ensure that the plots enter the plotting window column wise. Therefore, Option B is the correct solution. Question Context 18A Dataframe “df” has the following data:Dates2017-02-282017-02-272017-02-262017-02-252017-02-242017-02-232017-02-222017-02-21After reading above data, we want the following output:Dates28 Tuesday Feb 1727 Monday Feb 1726 Sunday Feb 1725 Saturday Feb 1724 Friday Feb 1723 Thursday Feb 1722 Wednesday Feb 1721 Tuesday Feb 17 18) Which of the following commands will produce the desired output?A) format(df,”%d %A %b %y”)B) format(df,”%D %A %b %y”)C) format(df,”%D %a %B %Y”)D) None of aboveSolution: (D)None of the above options will produce the desired output. Therefore, Option D is the correct solution. 19) Which of the following command will help us to rename the second column in a dataframe named “table” from alpha to beta?A) colnames(table)[2]=’beta’B) colnames(table)[which(colnames==’alpha’)]=’beta’C) setnames(table,’alpha’,’beta’)D) All of the aboveSolution: (D)All of the above options are different methods to rename the column names of a dataframe.Therefore, option D is the correct solution. Question Context: 20A majority of work in R uses systems internal memory and with large datasets, situations may arise when the R workspace cannot hold all the R objects in memory. So removing the unused objects is one of the solution.20) Which of the following command will remove an R object / variable named “santa” from the workspace?A) remove(santa)
B) rm(santa)
C) Both
D) NoneSolution : (C)remove and rm , both can be used to clear the workspace. Therefore, option C is the correct solution. 21) “dplyr” is one of the most popular package used in R for manipulating data and it contains 5 core functions to handle data. Which of the following is not one of the core functions of dplyr package?A) select()B) filter()C) arrange()D) summary()Solution: (D)summary is a function in the R base package and not dplyr. Context – Question 22During Feature Selection using the following dataframe (named table), “Column1” and “Column2” proved to be non-significant. Hence, we would not like to take these two features into our predictive model.table22) Which of the following commands will select all the rows from column 3 to column 6 for the below dataframe named table?A) dplyr::select(table,Column3:Column6)B) table[,3:6]
C) subset(table,select=c(‘Column3’,’Column4’,’Column5’,’Column6’))D) All of the aboveSolution: (D)Option A, B and C are different column sub setting methods in R. Therefore, option D is the correct solution. Context Question 23-24table23) Which of the following commands will select the rows having “Alpha” values in “Column1” and value less than 50 in “Column4”? The dataframe is stored in a variable named table.A) dplyr::filter(table,Column1==’Alpha’, Column4<50)B) dplyr::filter(table,Column1==’Alpha’ & Column4<50)C) Both of the aboveD) None of the aboveSolution: (C) Question Context  23-24table24) Which of the following code will sort the dataframe based on “Column2” in ascending order and “Column3” in descending order?A) dplyr::arrange(table,desc(Column3),Column2)B) table[order(-Column3,Column2),]
C) Both of the aboveD) None of the aboveSolution: (C)Both order and arrange functions can be used to order the columns in R. Therefore, Option C is the correct solution. 25) Dealing with strings is an important part of text analytics and splitting a string is often one of the common task performed while creating tokens, etc. What will be the output of following commands?B←paste(“phi”,”theta”,”zeta”,sep=””)parts←strsplit(c(A,B),split=” ”)A) alphaB) betaC) gammaD) phiE) thetaF) zetaSolution : (B)c(A.B) would concatenate A=”alpha beta gamma” and B=”phithetazeta” separated by a white space. Upon using strsplit, the two strings will be separated at the white space between A and B into two lists. Parts[[1]][2] tells us to print the second sub element of the first element of the list which is “beta”. Therefore, option B is the correct solution. 26) What will be the output of the following commandA) [FALSE TRUE TRUE FALSE TRUE]
B) [FALSE TRUE TRUE FALSE FALSE]
C) [FALSE FALSE TRUE FALSE FALSE]
D) None of the aboveSolution: (C)The above command will go for the exact match of the passed argument and therefore Option C is the correct solution. Question Context 27Sometimes as a Data Scientist working on textual data we come across instances where we find multiple occurrences of a word which is unwanted. Below is one such string.A) gsub(“because”,”since”,A)B) sub(“because”,”since”,AC) regexec(“because”,”since”,A)D) None of the aboveSolution: (A)sub command will replace only the first occurrence in a string whereas regexec will return a list of positions of the match or -1 if no match occurs. Therefore, Option A is the correct solution. 28) Imagine a dataframe created through the following code.Which of the following command will help us remove the duplicate rows based on both the columns?A) df[!duplicated(df),]
B) unique(df)C) dplyr::distinct(df)D) All of the aboveSolution: (D)All the above methods are different ways of removing the duplicate rows based on both the columns. Therefore, Option D is the correct solution. Question Context 29Grouping is an important activity in Data Analytics and it helps us discover some interesting trends which may not be visible easily in the raw data.Suppose you have a dataset created by the following lines of code.29) Which of the following command will help us to calculate the mean bar value grouped by foo variable?A) aggregate(bar~foo,table,mean)B) table::df[,mean(bar),by=foo]
C) dplyr::table%>%group_by(foo)%>%summarize(mean=mean(bar))D) All of the aboveSolution: (D)All the above methods are used to calculate the grouped statistic of a column. Therefore, Option D is the correct solution.30) If I have two vectors x<- c(1,3, 5) and y<-c(3, 2), what is produced by the expression cbind(x, y)?A) a matrix with 2 columns and 3 rowsB) a matrix with 3 columns and 2 rowsC) a data frame with 2 columns and 3 rowsD) a data frame with 3 columns and 2 rowsSolution: (D)All of the above options define messy data and hence Option D is the correct solution. 31) Which of the following commands will convert the following dataframe named maverick into the one shown at the bottom?Input Dataframe – “maverick”Output dataframeA) tidyr::Gather(maverick, Sex,Count,-Grade)B) tidyr::spread(maverick, Sex,Count,-GradeC) tidyr::collect(maverick, Sex,Count,-Grade)D) None of the aboveSolution: (A)Spread command converts rows into columns whereas there is no collect command in tidyr or base package.Therefore, Option A is the correct solution. 32) Which of the following command will help us to replace every instance of Delhi with Delhi_NCR in the following character vector?A) gsub(“Delhi”,”Delhi_NCR”,C)B) sub(“Delhi”,”Delhi_NCR”,C)C) Both of the aboveD) None of the aboveSolution: (C)Though sub command only replaces the first occurrence of a pattern. In this case, strings have just a single appearance of Delhi. Hence, both gsub and sub command will work in this situation. Therefore, Option C is the correct solution.Question Context 33Sometimes creating a feature which represents whether another variable has missing values or not can prove to be very useful for a predictive model.Below is a dataframe which has missing values in one of its columns.
33) Which of the following commands will create a column named “missing” with value 1 where variable “Feature2” has missing values?A)dataframe$missing<-0dataframe$Missing[is.na(dataframe$Feature2)]<-1B)dataframe$missing<-0dataframe$Missing[which(is.na(dataframe$Feature2))]<-1C) Both of the aboveD) None of the aboveSolution: (C)Option C is the correct answer. 34) Suppose there are 2 dataframes “A” and “B”. A has 34 rows and B has 46 rows. What will be the number of rows in the resultant dataframe after running the following command?A) 46B) 12C) 34D) 80Solution: (C)all.x forces the merging to take place on the basis of A and hence will contain the same number of rows as of A. Therefore, Option C is the correct solution. Question context 35The very first thing that a Data Scientist generally does after loading dataset is find out the number of rows and columns the dataset has. In technical terms, it is called knowing the dimensions of the dataset. This is done to get an idea about the scale of data that he is dealing with and subsequently choosing the right techniques and tools.35) Which of the following command will not help us to view the dimensions of our dataset?A) dim()B) str()C) View()D) None of the aboveSolution: (C)View command will print the dataset to the console in a spreadsheet like format but will not help us to view the dimensions. Therefore, option C is the correct solution. Question context 36Sometimes, we face a situation where we have two columns of a dataset and we wish to know which elements of the column are not present in another column. This is easily achieved in R using the setdiff command.dataframe36) What will be the output of the following command?A) TRUEB)FALSEC) Can’t SaySolution: (B)The order of arguments matter in setdiff function. Therefore, option B is the correct solution. Question Context 37The below dataset is stored in a variable called “frame”. 37) Which of the following commands will create a bar plot for the above dataset. Use the values from Column B to represent the height of the bar plot.A) ggplot(frame,aes(A,B))+geom_bar(stat=”identity”)B) ggplot(frame,aes(A,B))+geom_bar(stat=”bin”)C) ggplot(frame,aes(A,B))+geom_bar()D) None of the aboveSolution: (A)stat=”identity” will ensure the values in column B become the height of the bar. Therefore, Option A is the correct solution. Question Context 3838) We wish to create a stacked bar chart for cyl variable with stacking criteria Being vs Variable. Which of the following commands will help us perform this action?A)qplot(factor(cyl),data=mtcars,geom=”bar”,fill=factor(vs)B) ggplot(mtcars,aes(factor(cyl),fill=factor(vs)))+geom_bar()C) All of the aboveD) None of the aboveSolution: (C)Both options A and B will create a stacked bar chart guided by the “fill” parameter. Therefore, option C is the correct solution. 39) What is the output of the command – paste(1:3,c(“x”,”y”,”z”),sep=””) ?A) [1 2 3x y z]
B) [1:3x y z]
C) [1x 2y 3z]
D) None of the aboveSolution: (C) Question Context 40R has a rich library reserve for drawing some of the very high end graphs and plots and many a times you want to save the graphs for presenting your findings to someone else. Saving your plots to a PDF file is one such option.40) If you want to save a plot to a PDF file, which of the following is a correct way of doing that?A) Construct the plot on the screen device and then copy it to a PDF file with dev.copy2pdf().B) Construct the plot on the PNG device with png(), then copy it to a PDF with dev.copy2pdf().C) Open the PostScript device with postscript(), construct the plot, then close the device with dev.off().D) Open the screen device with quartz(), construct the plot, and then close the device with dev.off().Solution: (A)The plots are first created on the screen device and then can be copied easily to a pdf file. Therefore, option A is the correct solution. If you are learning R, you should use the test above to check your skills in R. If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/40-questions-r-for-data-science/
"40 Questions to test a data scientist on Machine Learning [Solution: SkillPower – Machine Learning, DataFest 2017]",Learn everything about Analytics|Introduction|Overall Scores|Useful Resources|Questions & Solutions,"End Notes|Share this:|Related Articles|40 questions to test your skill on R for Data Science|5 AI applications in Banking to look out for in next 5 years|
Ankit Gupta
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found," Machine Learning is one of the most sought after skills these days. If you are a data scientist, then you need to be good at Machine Learning – no two ways about it. As part of DataFest 2017, we organized various skill tests so that data scientists can assess themselves on these critical skills. These tests included Machine Learning, Deep Learning, Time Series problems and Probability. This article will lay out the solutions to the machine learning skill test. If you missed out on any of the above skill tests, you can still check out the questions and answers through the articles linked above.In Machine Learning skill test, more than 1350 people registered for the test. The test was designed to test your conceptual knowledge in machine learning and make you industry ready. If you missed on the real time test, you can still read this article to find out how you could have answered correctly.Here are the leaderboard rankings for all the participants.These questions, along with hundreds of others, are part of our ‘Ace Data Science Interviews‘ course. It’s a comprehensive guide, with tons of resources, to crack data science interviews and land your dream role! And if you’re just starting your data science journey, then check out our most popular course – ‘Introduction to Data Science‘! Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 210 people participated in the skill test and the highest score obtained was 36. Here are a few statistics about the distribution.Mean Score: 19.36Median Score: 21Mode Score: 27 Machine Learning basics for a newbieEssentials of Machine Learning Algorithms (with Python and R Codes)Deep Learning vs. Machine Learning – the essential differences you need to know!Introduction to Data Science CourseAce Data Science Interviews Course Question ContextA feature F1 can take certain value: A, B, C, D, E, & F and represents grade of students from a college.1) Which of the following statement is true in following case?A) Feature F1 is an example of nominal variable.
B) Feature F1 is an example of ordinal variable.
C) It doesn’t belong to any of the above category.
D) Both of theseSolution: (B)Ordinal variables are the variables which has some order in their categories. For example, grade A should be consider as high grade than grade B. 2) Which of the following is an example of a deterministic algorithm?A) PCAB) K-MeansC) None of the aboveSolution: (A)A deterministic algorithm is that in which output does not change on different runs. PCA would give the same result if we run again, but not k-means. 3) [True or False] A Pearson correlation between two variables is zero but, still their values can still be related to each other.A) TRUEB) FALSESolution: (A)Y=X2. Note that, they are not only associated, but one is a function of the other and Pearson correlation between them is 0. 4) Which of the following statement(s) is / are true for Gradient Decent (GD) and Stochastic Gradient Decent (SGD)?A) Only 1B) Only 2C) Only 3D) 1 and 2E) 2 and 3F) 1,2 and 3Solution: (A)In SGD for each iteration you choose the batch which is generally contain the random sample of data But in case of GD each iteration contain the all of the training observations. 5) Which of the following hyper parameter(s), when increased may cause random forest to over fit the data? A) Only 1B) Only 2C) Only 3D) 1 and 2E) 2 and 3F) 1,2 and 3Solution: (B)Usually, if we increase the depth of tree it will cause overfitting. Learning rate is not an hyperparameter in random forest. Increase in the number of tree will cause under fitting. 6) Imagine, you are working with “Analytics Vidhya” and you want to develop a machine learning algorithm which predicts the number of views on the articles. Your analysis is based on features like author name, number of articles written by the same author on Analytics Vidhya in past and a few other features. Which of the following evaluation metric would you choose in that case?A) Only 1B) Only 2C) Only 3D) 1 and 3E) 2 and 3F) 1 and 2Solution:(A)You can think that the number of views of articles is the continuous target variable which fall under the regression problem. So, mean squared error will be used as an evaluation metrics. 7) Given below are three images (1,2,3). Which of the following option is correct for these images?A)B)C) 
A) 1 is tanh, 2 is ReLU and 3 is SIGMOID activation functions.B) 1 is SIGMOID, 2 is ReLU and 3 is tanh activation functions.C) 1 is ReLU, 2 is tanh and 3 is SIGMOID activation functions.D) 1 is tanh, 2 is SIGMOID and 3 is ReLU activation functions.Solution: (D)The range of SIGMOID function is [0,1].The range of the tanh function is [-1,1].The range of the RELU function is [0, infinity].So Option D is the right answer. 8) Below are the 8 actual values of target variable in the train file.[0,0,0,1,1,1,1,1]What is the entropy of the target variable? A) -(5/8 log(5/8) + 3/8 log(3/8))B) 5/8 log(5/8) + 3/8 log(3/8)C) 3/8 log(5/8) + 5/8 log(3/8)D) 5/8 log(3/8) – 3/8 log(5/8)Solution: (A)The formula for entropy is  So the answer is A. 9) Let’s say, you are working with categorical feature(s) and you have not looked at the distribution of the categorical variable in the test data.You want to apply one hot encoding (OHE) on the categorical feature(s). What challenges you may face if you have applied OHE on a categorical variable of train dataset? A) All categories of categorical variable are not present in the test dataset.B) Frequency distribution of categories is different in train as compared to the test dataset.C) Train and Test always have same distribution.D) Both A and BE) None of theseSolution: (D)Both are true, The OHE will fail to encode the categories which is present in test but not in train so it could be one of the main challenges while applying OHE. The challenge given in option B is also true you need to more careful while applying OHE if frequency distribution doesn’t same in train and test. 10) Skip gram model is one of the best models used in Word2vec algorithm for words embedding. Which one of the following models depict the skip gram model?A) AB) BC) Both A and BD) None of theseSolution: (B)Both models (model1 and model2) are used in Word2vec algorithm. The model1 represent a CBOW model where as Model2 represent the Skip gram model. 11) Let’s say, you are using activation function X in hidden layers of neural network. At a particular neuron for any given input, you get the output as “-0.0001”. Which of the following activation function could X represent? A) ReLUB) tanhC) SIGMOIDD) None of theseSolution: (B)The function is a tanh because the this function output range is between (-1,-1). 12) [True or False] LogLoss evaluation metric can have negative values.A) TRUE
B) FALSESolution: (B)Log loss cannot have negative values. 13) Which of the following statements is/are true about “Type-1” and “Type-2” errors?A) Only 1B) Only 2C) Only 3D) 1 and 2E) 1 and 3F) 2 and 3Solution: (E)In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a “false positive”), while a type II error is incorrectly retaining a false null hypothesis (a “false negative”). 14) Which of the following is/are one of the important step(s) to pre-process the text in NLP based projects?A) 1 and 2B) 1 and 3C) 2 and 3D) 1,2 and 3Solution: (D)Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.Stop words are those words which will have not relevant to the context of the data for example is/am/are.Object Standardization is also one of the good way to pre-process the text. 15) Suppose you want to project high dimensional data into lower dimensions. The two most famous dimensionality reduction algorithms used here are PCA and t-SNE. Let’s say you have applied both algorithms respectively on data “X” and you got the datasets “X_projected_PCA” , “X_projected_tSNE”.Which of the following statements is true for “X_projected_PCA” & “X_projected_tSNE” ?A) X_projected_PCA will have interpretation in the nearest neighbour space.B) X_projected_tSNE will have interpretation in the nearest neighbour space.C) Both will have interpretation in the nearest neighbour space.D) None of them will have interpretation in the nearest neighbour space.Solution: (B)t-SNE algorithm consider nearest neighbour points to reduce the dimensionality of the data. So, after using t-SNE we can think that reduced dimensions will also have interpretation in nearest neighbour space. But in case of PCA it is not the case.Context: 16-17Given below are three scatter plots for two features (Image 1, 2 & 3 from left to right).16) In the above images, which of the following is/are example of multi-collinear features?A) Features in Image 1B) Features in Image 2C) Features in Image 3D) Features in Image 1 & 2E) Features in Image 2 & 3F) Features in Image 3 & 1Solution: (D)In Image 1, features have high positive correlation where as in Image 2 has high negative correlation between the features so in both images pair of features are the example of multicollinear features. 17) In previous question, suppose you have identified multi-collinear features. Which of the following action(s) would you perform next? A) Only 1B)Only 2C) Only 3D) Either 1 or 3E) Either 2 or 3Solution: (E)You cannot remove the both features because after removing the both features  you will lose all of the information so you should either remove the only 1 feature or you can use the regularization algorithm like L1 and L2. 18) Adding a non-important feature to a linear regression model may result in.A) Only 1 is correctB) Only 2 is correctC) Either 1 or 2D) None of theseSolution: (A)After adding a feature in feature space, whether that feature is important or unimportant features the R-squared always increase. 19) Suppose, you are given three variables X, Y and Z. The Pearson correlation coefficients for (X, Y), (Y, Z) and (X, Z) are C1, C2 & C3 respectively.Now, you have added 2 in all values of X (i.enew values become X+2), subtracted 2 from all values of Y (i.e. new values are Y-2) and Z remains the same. The new coefficients for (X,Y), (Y,Z) and (X,Z) are given by D1, D2 & D3 respectively. How do the values of D1, D2 & D3 relate to C1, C2 & C3? A) D1= C1, D2 < C2, D3 > C3B) D1 = C1, D2 > C2, D3 > C3C) D1 = C1, D2 > C2, D3 < C3D) D1 = C1, D2 < C2, D3 < C3E) D1 = C1, D2 = C2, D3 = C3F) Cannot be determinedSolution: (E)Correlation between the features won’t change if you add or subtract a value in the features. 20) Imagine, you are solving a classification problems with highly imbalanced class. The majority class is observed 99% of times in the training data.Your model has 99% accuracy after taking the predictions on test data. Which of the following is true in such a case?A) 1 and 3B) 1 and 4C) 2 and 3D) 2 and 4Solution: (A)Refer the question number 4 from in this article. 21) In ensemble learning, you aggregate the predictions for weak learners, so that an ensemble of these models will give a better prediction than prediction of individual models.Which of the following statements is / are true for weak learners used in ensemble model?A) 1 and 2B) 1 and 3C) 2 and 3D) Only 1E) Only 2F) None of the aboveSolution: (A)Weak learners are sure about particular part of a problem. So, they usually don’t overfit which means that weak learners have low variance and high bias. 22) Which of the following options is/are true for K-fold cross-validation? A) 1 and 2B) 2 and 3C) 1 and 3D) 1,2 and 3Solution: (D)Larger k value means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) and higher running time (as you are getting closer to the limit case: Leave-One-Out CV). We also need to consider the variance between the k folds accuracy while selecting the k. Question Context 23-24Cross-validation is an important step in machine learning for hyper parameter tuning. Let’s say you are tuning a hyper-parameter “max_depth” for GBM by selecting it from 10 different depth values (values are greater than 2) for tree based model using 5-fold cross validation.Time taken by an algorithm for training (on a model with max_depth 2) 4-fold is 10 seconds and for the prediction on remaining 1-fold is 2 seconds.Note: Ignore hardware dependencies from the equation.23) Which of the following option is true for overall execution time for 5-fold cross validation with 10 different values of “max_depth”? A) Less than 100 secondsB) 100 – 300 secondsC) 300 – 600 secondsD) More than or equal to 600 secondsC) None of the aboveD) Can’t estimateSolution: (D)Each iteration for depth “2” in 5-fold cross validation will take 10 secs for training and 2 second for testing. So, 5 folds will take 12*5 = 60 seconds. Since we are searching over the 10 depth values so the algorithm would take 60*10 = 600 seconds. But training and testing a model on depth greater than 2 will take more time than depth “2” so overall timing would be greater than 600. 24) In previous question, if you train the same algorithm for tuning 2 hyper parameters say “max_depth” and “learning_rate”.You want to select the right value against “max_depth” (from given 10 depth values) and learning rate (from given 5 different learning rates). In such cases, which of the following will represent the overall time?A) 1000-1500 secondB) 1500-3000 SecondC) More than or equal to 3000 SecondD) None of theseSolution: (D)Same as question number 23. 25) Given below is a scenario for training error TE and Validation error VE for a machine learning algorithm M1. You want to choose a hyperparameter (H) based on TE and VE. Which value of H will you choose based on the above table?A) 1B) 2C) 3D) 4E) 5Solution: (D)Looking at the table, option D seems the best 26) What would you do in PCA to get the same projection as SVD?A) Transform data to zero meanB) Transform data to zero medianC) Not possibleD) None of theseSolution: (A)When the data has a zero mean vector PCA will have same projections as SVD, otherwise you have to centre the data first before taking SVD. Question Context 27-28Assume there is a black box algorithm, which takes training data with multiple observations (t1, t2, t3,…….. tn) and a new observation (q1). The black box outputs the nearest neighbor of q1 (say ti) and its corresponding class label ci. You can also think that this black box algorithm is same as 1-NN (1-nearest neighbor).27) It is possible to construct a k-NN classification algorithm based on this black box alone.Note: Where n (number of training observations) is very large compared to k.A) TRUEB) FALSESolution: (A)In first step, you pass an observation (q1) in the black box algorithm so this algorithm would return a nearest observation and its class.In second step, you through it out nearest observation from train data and again input the observation (q1). The black box algorithm will again return the a nearest observation and it’s class.You need to repeat this procedure k times 28) Instead of using 1-NN black box we want to use the j-NN (j>1) algorithm as black box. Which of the following option is correct for finding k-NN using j-NN?A)  1B) 2C) 3Solution: (A)Same as question number 27 29) Suppose you are given 7 Scatter plots 1-7 (left to right) and you want to compare Pearson correlation coefficients between variables of each scatterplot.Which of the following is in the right order?A) 1 and 3B) 2 and 3C) 1 and 4D) 2 and 4Solution: (B)from image 1to 4 correlation is decreasing (absolute value). But from image 4 to 7 correlation is increasing but values are negative (for example, 0, -0.3, -0.7, -0.99).30) You can evaluate the performance of a binary class classification problem using different metrics such as accuracy, log-loss, F-Score. Let’s say, you are using the log-loss function as evaluation metric.Which of the following option is / are true for interpretation of log-loss as an evaluation metric?A) 1 and 3B) 2 and 3C) 1 and 2D) 1,2 and 3Solution: (D)Options are self-explanatory. Question 31-32Below are five samples given in the dataset.Note: Visual distance between the points in the image represents the actual distance. 31) Which of the following is leave-one-out cross-validation accuracy for 3-NN (3-nearest neighbor)?A) 0D) 0.4C) 0.8D) 1Solution: (C)In Leave-One-Out cross validation, we will select (n-1) observations for training and 1 observation of validation. Consider each point as a cross validation point and then find the 3 nearest point to this point. So if you repeat this procedure for all points you will get the correct classification for all positive class given in the above figure but negative class will be misclassified. Hence you will get 80% accuracy. 32) Which of the following value of K will have least leave-one-out cross validation accuracy?A) 1NNB) 3NNC) 4NND) All have same leave one out errorSolution: (A)Each point which will always be misclassified in 1-NN which means that you will get the 0% accuracy. 33) Suppose you are given the below data and you want to apply a logistic regression model for classifying it in two given classes.You are using logistic regression with L1 regularization. Where C is the regularization parameter and w1 & w2 are the coefficients of x1 and x2.Which of the following option is correct when you increase the value of C from zero to a very large value?A) First w2 becomes zero and then w1 becomes zeroB) First w1 becomes zero and then w2 becomes zeroC) Both becomes zero at the same timeD) Both cannot be zero even after very large value of CSolution: (B)By looking at the image, we see that even on just using x2, we can efficiently perform classification. So at first w1 will become 0. As regularization parameter increases more, w2 will come more and more closer to 0.34) Suppose we have a dataset which can be trained with 100% accuracy with help of a decision tree of depth 6. Now consider the points below and choose the option based on these points.Note: All other hyper parameters are same and other factors are not affected. A) Only 1B) Only 2C) Both 1 and 2D) None of the aboveSolution: (A)If you fit decision tree of depth 4 in such data means it will more likely to underfit the data. So, in case of underfitting you will have high bias and low variance. 35) Which of the following options can be used to get global minima in k-Means Algorithm?A) 2 and 3B) 1 and 3C) 1 and 2D) All of aboveSolution: (D)All of the option can be tuned to find the global minima. 36) Imagine you are working on a project which is a binary classification problem. You trained a model on training dataset and get the below confusion matrix on validation dataset.Based on the above confusion matrix, choose which option(s) below will give you correct predictions?A) 1 and 3B) 2 and 4C) 1 and 4D) 2 and 3Solution: (C)The Accuracy (correct classification) is (50+100)/165 which is nearly equal to 0.91.The true Positive Rate is how many times you are predicting positive class correctly so true positive rate would be 100/105 = 0.95 also known as “Sensitivity” or “Recall”37) For which of the following hyperparameters, higher value is better for decision tree algorithm?A)1 and 2B) 2 and 3C) 1 and 3D) 1, 2 and 3E) Can’t saySolution: (E)For all three options A, B and C, it is not necessary that if you increase the value of parameter the performance may increase. For example, if we have a very high value of depth of tree, the resulting tree may overfit the data, and would not generalize well. On the other hand, if we have a very low value, the tree may underfit the data. So, we can’t say for sure that “higher is better”. Context 38-39Imagine, you have a 28 * 28 image and you run a 3 * 3 convolution neural network on it with the input depth of 3 and output depth of 8.Note: Stride is 1 and you are using same padding.38) What is the dimension of output feature map when you are using the given parameters.A) 28 width, 28 height and 8 depthB) 13 width, 13 height and 8 depthC) 28 width, 13 height and 8 depthD) 13 width, 28 height and 8 depthSolution: (A)The formula for calculating output size isoutput size = (N – F)/S + 1where, N is input size, F is filter size and S is stride.Read this article to get a better understanding. 39)  What is the dimensions of output feature map when you are using following parameters.A)  28 width, 28 height and 8 depthB) 13 width, 13 height and 8 depthC) 28 width, 13 height and 8 depthD) 13 width, 28 height and 8 depthSolution: (B)Same as above 40) Suppose, we were plotting the visualization for different values of C (Penalty parameter) in SVM algorithm. Due to some reason, we forgot to tag the C values with visualizations. In that case, which of the following option best explains the C values for the images below (1,2,3 left to right, so C values are C1 for image1, C2 for image2 and C3 for image3 ) in case of rbf kernel.A) C1 = C2 = C3B) C1 > C2 > C3C) C1 < C2 < C3D) None of theseSolution: (C)Penalty parameter C of the error term. It also controls the trade-off between smooth decision boundary and classifying the training points correctly. For large values of C, the optimization will choose a smaller-margin hyperplane. Read more here. I hope you enjoyed the questions and were able to test your knowledge about machine learning. If you have any questions or doubts, feel free to post them below.Check out all the upcoming events here.",https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-machine-learning-solution-skillpower-machine-learning-datafest-2017/
5 AI applications in Banking to look out for in next 5 years,Learn everything about Analytics|Introduction|Top 10 companies using AI|AI & its relevance to Banking|How small banks can make the most of AI?,"About the Author|Share this:|Like this:|Related Articles|40 Questions to test a data scientist on Machine Learning [Solution: SkillPower – Machine Learning, DataFest 2017]|Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"“Machine intelligence is the last invention that humanity will ever need to make.”Nick BostromArtificial intelligence is a reality today and it is impacting our lives faster than we can imagine. It is already present everywhere, from Siri in your phone to the Netflix recommendations that you receive on your smart TV. The revolution brought by Artificial intelligence has been the biggest in some time. There is no denying that it has already become a crucial and integral part of our life.Artificial intelligence is the blend of three advanced technologies – machine learning, natural language processing and cognitive computing. The concept of Artificial Intelligence is to simulate the intelligence of humans into artificial machines with the help of sophisticated machine learning and natural language processing algorithms. The prime motive for the idea of transferring the intelligence from humans to machines is to overcome the very barrier of human intelligence: scalability. There’s always a limit to the speed with which humans can perform the given tasks. Artificial intelligence looks to overcome this very challenge with human intelligence by transferring the human intelligence to cognitive machines with supreme computational capabilities.Let’s take two examples to better understand the concept of artificial intelligence:These are scenarios where artificial intelligence is focusing on, to simulate the mapping of inputs to outputs as it happens in a human brain which makes very difficult tasks for computers like image recognition, sarcasm detection, voice recognition, etc. seamlessly easy for even an 8-year old kid. There are many use cases for AI in a variety of industries.  Bizofit, a platform that intelligently connects enterprises with appropriate service providers, has compiled the following list of top 10 AI companies.One of the leading artificial intelligence companies, AIBrain builds AI solutions for smartphones devices primarily. Their key area of expertise is robotics and digital personal assistant.Anki is another company in AI domain which has received funding from over $157.5 million from the likes of J.P. Morgan and other Ventures. The flagship robot of Anki – Cozmo – is one of the most emotionally intelligent robot while dealing with customers.Banjo has raised over $100 million worth of funding till now.They use the strong social media analytics from multiple social media platforms to identify the events taking place around the globe.iCarbonX is an artificial intelligence based startup in health care domain. They provide individualized health analysis and prediction of health index through the use of advanced data mining and machine analysis technologies. iCarbonX is valued at more than $1 billion USD.Jibo is the first robot in the world made to help families with their daily tasks. Also, it learns about the behavior and personality of family as it interacts with them.Next IT applies AI in healthcare and finance industries with focus mainly on natural language processing, chatbots and machine learning.Being one of the most popular iOS app, Prisma brought a revolution in mobile app industry with the use of deep learning algorithms to recreate images as if they were painted.ReSnap using AI and deep learning to take a large number of images from the user and create beautiful photo books out of these images. AI helps in selecting images and chooses best for the photobook.ViSenze is revolutionizing the e-commerce market by recommending visually similar products out of the several millions products. They use deep learning and computer vision. They recently raised $10.5 million for developing their AI technologies.X.ai’s virtual assistant is helping busy people schedule meetings without any human intervention. As soon as you copy a mail to Amy, it makes sure that with the use of natural language processing and machine learning, it identifies the most suitable time and place for your meeting. In recent years, if Artificial Intelligence has impacted one industry more than any other, it’s the Banking industry. For organizations working in the banking industry, it has become increasingly crucial to keep up with competition, and increase their standing as an innovative company. The following graphic shows reasons for its widespread adoption in Banking & Financial Services.Source: financialbrand.comArtificial intelligence has several applications in the banking industry.Here are five key applications of artificial intelligence in the Banking industry that will revolutionize the industry in the next 5 years.Anti-money laundering (AML) refers to a set of procedures, laws or regulations designed to stop the practice of generating income through illegal actions. In most cases, money launderers hide their actions through a series of steps that make it look like money that came from illegal or unethical sources are earned legitimately.Most of the major banks across the globe are shifting from rule based software systems to artificial intelligence based systems which are more robust and intelligent to the anti-money laundering patterns. Over the coming years, these systems are only set to become more and more accurate and fast with the continuous innovations and improvements in the field of artificial intelligence. Chat bots are artificial intelligence based automated chat systems which simulate human chats without any human interventions. They work by identifying the context and emotions in the text chat by the human end user and respond to them with the most appropriate reply. With time, these chat bots collect massive amount of data for the behaviour and habits of the user and learns the behaviour of user which helps to adapts to the needs and moods of the end user.Chat bots are already being extensively used in the banking industry to revolutionize the customer relationship management at personal level. Bank of America plans to provide customers with a virtual assistant named “Erica” who will use artificial intelligence to make suggestions over mobile phones for improving their financial affairs. Allo, released by Google is another generic realization of chat bots. Plenty of Hedge funds across the globe are using high end systems to deploy artificial intelligence models which learn by taking input from several sources of variation in financial markets and sentiments about the entity to make investment decisions on the fly. Reports claim that more than 70% of the trading today is actually carried out by automated artificial intelligence systems. Most of these hedge funds follow different strategies for making high frequency trades (HFTs) as soon as they identify a trading opportunity based on the inputs.A few hedge funds active in AI space are: Two Sigma, PDT Partners, DE Shaw, Winton Capital Management, Ketchum Trading, LLC, Citadel, Voleon, Vatic Labs, Cubist, Point72, Man AHL. Fraud detection is one of the fields which has received massive boost in providing accurate and superior results with the intervention of artificial intelligence. It’s one of the key areas in banking sector where artificial intelligence systems have excelled the most. Starting from the early example of successful implementation of data analysis techniques in the banking industry is the FICO Falcon fraud assessment system, which is based on a neural network shell to deployment of sophisticated deep learning based artificial intelligence systems today, fraud detection has come a long way and is expected to further grow in coming years. Recommendation engines are a key contribution of artificial intelligence in banking sector. It is based on using the data from the past about users and/ or various offerings from a bank like credit card plans, investment strategies, funds, etc. to make the most appropriate recommendation to the user based on their preferences and the users’ history. Recommendation engines have been very successful and a key component in revenue growth accomplished by major banks in recent times.With Big Data and faster computations, machines coupled with accurate artificial intelligence algorithms are set to play a major role in how recommendations are made in banking sector. For further reading on recommendation engines, you can refer to the complete guide of how recommendation engines work. In several of our conversations with executives of smaller banks like Community banks in the US, it became very apparent that they were seeking a differentiator in their intense competition with the larger banks. Big banks are using cutting edge artificial intelligence techniques by using in-house teams of Data Scientists and Quants for risk assessment, financial analysis, portfolio management, credit approval process, KYC & anti-money laundering systems. On the other hand, small banks can use AI for achieving operational efficiency and better customer interactions.Some of the several applications of AI that smaller banks can benefit from are:In conclusion, it is evident that AI is here to stay, and is impacting a large number of industries, Banking is an early adopter of this trend.  This trend is likely to grow exponentially in the future. Companies that embrace this trend are likely to be winners over the next 10 years. Devendra Mangani, Sr. Consultant, BizofitHaving 12+ years of experience in strategy, business planning, B2B IT sales and raising capital for startups and companies in IT sector. Experience in managing multiple stakeholders and worked with global teams in previous companies including investment bank. He is guest faculty at management colleges and does workshops on design thinking. Devendra brings in strong understanding of research reports and consulting for building research capabilities of Bizofit. He is an IIT Bombay graduate and MBA from Queen’s University in Canada.",https://www.analyticsvidhya.com/blog/2017/04/5-ai-applications-in-banking-to-look-out-for-in-next-5-years/
"Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017","Learn everything about Analytics|Introduction|About the Competition|Problem Statement|Winners|Approach & code of all the winners|Sonny Laskar, Rank 2|Rohan Rao & Mark Landry, Rank 1|End Notes","Aanish Singla, Rank 3||Check out all the upcoming competitions here|Share this:|Like this:|Related Articles|5 AI applications in Banking to look out for in next 5 years|Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Key findings from analysis were,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Analytics Vidhya just completed 4 years and we had to make sure we mark this event in style! We did that by creating our most unique hackathon – Xtreme ML Hack as part of DataFest 2017. We were looking for an exciting real life problem and would want to thanks our sponsor – Aigues de Barcelona for providing us with an opportunity to host this wonderful competition.Xtreme ML Hack started on 20 April 2017 and went on for 4 days. We saw more than 2400 participants from across the globe. There were multiple things which were unique for this competition. This was the first time we were working for a Spainish company. This was also an unique hackathon because the participants were allowed to use any available open data. Also, this was the fist time we were working with an utility company.I think this was probably the most challenging hackathon we have released till now – this is as close as you can get to a real life project. Like always, the winners of the competition have generously shared their detailed approach and the codes they used in the competition.If you missed out the fun, make sure you participate in the upcoming MiniHack – The QuickSolver. The problem statement revolved around public-private company Aigües de Barcelona. It manages the integral cycle of water, from the uptake to the drinking process, transport and distribution, besides the sanitation and the purification of waste water for its return to the natural environment or its re-use. It offers service to about 3 million people in the municipalities of the metropolitan area of Barcelona. It also manages a customer service to ensure excellence in their services.Customer Service is one of the top priorities of the company. They want to redesign the customer service using machine learning.  To respond to their customers efficiently they want to predict the volume and typologies of contacts to the call center. They also want to forecast the number of contacts on daily basis. The task was to forecast the number of contacts & resolutions the company should open on daily basis. The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1: Rohan Rao & Mark LandryRank 2: Sonny LaskarRank 3: Aanish SinglaHere are the final rankings of all the participants at the leaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go ahead. Here’s what Aanish shared with us.Aanish says “I started out by building an understanding of how autonomous communities, provinces, municipalities and districts are related to each other in Spain. Then I looked at the data dictionary and what all was provided and what was expected, followed by exploratory data analysis.”   For Contacts: For ResolutionsI also figured out that there was a close correlation between new contracts, ended contracts and number of contacts. I had 2 choices.I used the second approach. Predicting contactsI used top down approach as the there was only 1 level involved. I predicted the contacts at day level using Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components (tbats) using weekly, quarterly and yearly seasonality. To accommodate the trends at type level, I found the average contribution by type for 2016 and for all years (some types had different trends in recent years). Using separate contribution percentages by types and for weekdays and weekends, I split the overall forecast into detailed forecast. Predicting ResolutionResolutions had 2 levels (category and subject), hence top down approach was not suitable. Also, breaking the high-level forecast into 2 levels would be quite error prone so I selected hierarchical time series modeling for this. I used tbats() for each subject to account for multiple seasonality on the data. Most of the effort was in preparing data in the format accepted by hierarchical time series modeling function, hts(). I used only last 3 years of data (optimized after first submission) as that was having lesser missing values and avoided the spiked 2013 data. Combining of low level forecast with top level was done using LU decomposition algorithm.Prediction from this resulted in some negative values, especially for the weekend. I replaced them with respective mean from 2016 weekend data.On submission, I had a score of 104.6 but decided not to change the model too much as it was very generic.Solution: Link to Code Here’s what Sonny shared with us.He says “This was one of the best competitions till now on Analytics Vidhya. Moreover, the introduction of submission limits made it even more interesting. This competition was definitely not a good place to start from if you had just started learning ML. My first thought on looking at the problem made me think that it had to be solved as a time-series problem which later seemed to be wrong.” Modeling ApproachIt was a no-brainer to find that the only data in the Test Set were “future” dates. Did that look odd? Come to reality! The variable to predict was # of contacts/resolutions. After some initial analysis, I decided that this has to be treated as two different problems.For the modeling purpose, I created day wise-medium wise-department wise aggregated # of contacts and resolutions from 2010 onwards (since we had data on contacts/resolutions only after 2010). For cross-validation, I decided to use the last four months. So my first model was built on few “Date” features and it scored 101.X which was a very good score on Day1.The next thing that struck me was that holidays should have an impact on these contacts and resolutions. Hence I created a list of holidays in Spain since 2010. Just adding holidays to the above list improved the score to 93.X. I was feeling good but it didn’t last longer since I saw Mark jumping to 78.X.Later, I decided to add “lag” features of # of contacts/resolutions from the past. After a few iterations, I decided to keep lag features of 75, 90 and 120 days. With this, the score improved to 64.X (ahead of Mark J). Rohan was around 114.X so I knew that he was either asleep or solving Soduku. This was on Saturday morning and I was unable to find any additional features that could help me move ahead. So I decided to take a break.  By evening, I noticed that Rohan had woken up and was also at around 78.X (That’s when I guess Mark and Rohan decided to team up).On Sunday, I added a feature on number of days (percentile) elapsed since last holiday and that added few points. My final 10-bags XGboost ensemble scored 61.47081 on the public leaderboard which I selected for “final submission”. I had around 8 submissions left which I wish I could donate to few folks who were not aware that they could not mark their final submissions if they didn’t upload code L.Huh!It might sound that the journey of adding features and improving scores was a very smooth line something like this.But it was not. I tried many things which didn’t work.Below are few approaches that I tried but didn’t seem to add value:Overall, this was a different problem and really enjoyed solving it. Thanks to Mark and Rohan for a tough fight.  Here’s what Mark & Rohan shared with us.Mark LandryI kept the feature space fairly small. Calendar Features:
day of the week, week of the year, the day of the year & year.Holiday Features (all binary):national, local, observance, common local (per https://www.timeanddate.com/holidays/spain/)Average Features (*):average Contacts per day of week and Contact typeaverage Resolution per day of week, category, and subjectOther modeling: used all the dataused all the dataall H2O gbmseparate models for Contacts and Resolutionvalidation: started with Leave-One-Year-Out, finished measuring January – March 2016 (prior year from test predictions)I started modeling with simple averages, as shown above, where the entire data set was used, and those results were applied to the entire dataset. But soon I moved to a more sound version of the calculation where the impact of each record is removed from the calculation so that it is not leaked. And toward the end, I used an entire validation set to where no records of that set were used in the average calculations (similar to the actual prediction environment).Rohan RaoI started exploring the data to get a sense of basic movement of values across the years. I decided to go with granularity and build a model for each type / category in Contacts and Resolution. I ultimately ended up plotting over 2000 graphs and summaries, which really helped identify various patterns.After trying a bunch of features and models, I found historic averages out-performing all ML-based models. So, I decided to focus and tune the average values so as to capture seasonality and trend in the right way. In some ways, this was like a manually-customized time series model.Main features1. There were two highly seasonal components. Weekly and yearly. The following lag features: same week year and weekday in the previous year, previous/next week year and same weekday in the previous year, helped capture seasonality.Besides these core features, other things that worked were:1. Using 21st-Jan, 2016 to 15th-Mar, 2016 as the validation data. This helped in preventing overfitting to public LB.2. Using estimates for holiday dates. Most holidays had 0 volume and highly noisy values. Aligning these correctly across the years helped.3. My non-ML historic averages approach blended extremely well with Mark’s ML-model which didn’t use lag-features. Our simple ensemble gave a lift of almost 10 on RMSE.4. Throw away all data prior to 2015, so as to focus only on the most recent trends.Solution: Link to Code It was great fun interacting with these winners and to know their approach during the competition. Hopefully, you will be able to evaluate where you missed out.",https://www.analyticsvidhya.com/blog/2017/04/winners-solution-codes-xtreme-mlhack-datafest-2017/
Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?,"Learn everything about Analytics|Brain Teaser|My recent experience with BlackJack||Pre-requisites for the article|What to expect in this article?|Let’s get the ball rolling|Deep dive into betting strategy|Enough of this monologue, let’s try some hands-on exercise|End Notes","Category 1 – Gaming Strategy related questions|Category 2 – Betting Strategy|Category 3 – A few Fundamental Questions|Simulation 1|Simulation 2|Simulation 5|Simulation 6|Simulation 7|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017|Data Scientist- Bangalore (3+ years of experience)|
Tavish Srivastava
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",Simulation 3|Simulation 4,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Can we create a flawless winning strategy in a Casino using Data Science?Of course not! Otherwise, all the data scientists out there would be sitting on piles of cash and the casinos would shut us out!But, in this article we will learn how to evaluate if a game in Casino is biased or fair. We will understand the biases working in a casino and create strategies to become profitable. We will also learn how can we control the probability of going bankrupt in Casinos.To make the article interactive, I have added few puzzles in the end to use these strategies. If you can crack them there is no strategy that can make you hedge against loosing in a Casino.Read on!! Before we begin, let me ask you a couple of questions:If your answer for second question is more than half of question one, then you fall in same basket as most of the players going to a Casino (and you make them profitable!).Now here’s an interesting fact, for most of the games you play at a Casino, you will win the amount in line with your odds of winning. For example: If the probability of winning in a roulette table is 1/37, you will get 37X of the amount you had bet.  Hence, the expected losses of a trade in Casino is almost equal to  zero.Expected gain from roulette = 1/37 * $X * 37 (amount you get if you win) – $X (amount you bet) = 0So, where are we going wrong?Why do our chances of gaining 100% or more are less than 50% but our chances of losing 100% is a lot more than 50%. Let’s start with my recent visit to a Casino. Last week, I went to Atlantic City – the casino hub of US east coast. BlackJack has always been my favorite game because of a lot of misconceptions.For the starters, let me take you through how BlackJack is played.There are few important things to note about BlackJack. If you know the game, you can skip the points below:Ace card is counted as 1 or 11 points, numbered cards (2 to 9) are counted at face value. 10 numbered card and face cards are counted as 10 points.The value of a hand is the sum of the point values of the individual cards. Except, a “blackjack” is the highest hand, consisting of an ace and any 10-point card, and it outranks all other 21-point hands.If the sum exceeds 21, it is called a “burst” and whoever has a burst looses right away.The dealer will ask the players for any further bet, and a player can choose to double the bet based on his 2 cards and dealer’s 1 card. Then the dealer will ask the player if he/she needs more cards. Player tries to maximize his score without being burst.After the final bet, the dealer will open more card for him/her. Dealer will keep opening his/her cards, until the value reaches 17 (Dealer might have to open another card if he/she has an Ace card counted as 11, and total sum as 17 – this is called soft 17).There are a few more complicated concepts like insurance and split, which is beyond the scope of this article. So, we will keep things simple.I always thought that given the dealer has a constraint of opening cards till he/she reaches 17/18, they cannot stop taking more cards. Hence, a player has better odds of winning than the dealer as he/she has no such constraints. I was excited about all the winning I was about to get!!As soon as I entered the Casino, I separated $X in my purse as “Casino money”, Got my cash converted into chips, and with in the next hour, I was bankrupt. I was not done yet – In hopes of recovering the money I lost, I donated another $2X in two rounds and I was finally convinced that the game was not as simple as I thought and I needed to pull out my data science hat to win it.Following are a few good to have skills to enjoy the article more: Here are the questions, I will try to answer in this article. They can be broadly classified in three heads:   Here is a situation, you see that dealer has a “4” open and following are your cards.Your total score is “14”. What would you do?By now, you will know that your cards are really poor but do you take another card and expose yourself to the risk of getting burst OR you will take the chance to stay and let the dealer get burst. A lot of people will recommend you to stay till dealer’s cards are as bad as yours – let us check out through simulations. Let us try to calculate the probability of the dealer getting burst. I will first define a few basic functions on R and then simulate dealer’s hand.So what did I find?Here is the probability distribution given for the first card of the dealer.The probability of the dealer getting burst is 39.6%, which will be player’s probability of winning.  This means you will loose 60% of times – Is that a good strategy? We can’t answer that question until unless I know what is the probability of winning if I take one more card and either increase or burst my score.Insight 1 – Probability of the dealer getting burst given his/her first card (say is 4) can be found from the table above (39.6% in this case).  Now we need to bring in player’s cards as well and find what is the probability of winning if the player has a “14” in hand. With this additional information, we can make refinement to the probability of winning given our 2 cards and dealers 1 card.The reason the row 21 has a lot of 100% is that with just 2 cards, 21 is a black jack. And if the dealer does not have the same, the Player is definite to win. The probability of winning for the player sum 12-16 should ideally be equal to the probability of dealer going burst. As this is the only way Player is going to win if he/she chooses to stay. Dealer will have to open a new card if it has a sum between 12-16. This is actually the case which validates that our two simulations are consistent. To decide whether it is worth opening another card, calls into question what will be the probability to win if player decides to take another card.Insight 2 – If your sum is more than 17 and dealer gets a card 2-6, odds of winning is in your favor. This is even without including Ties. To make this analysis simple, let’s say favorable probability is Chances to Win + 50% chances to Tie.Favorable probability table if you choose to draw a card is as follows.So what did you learn from here. Is it beneficial to draw a card at 8 + 6 or stay?Favorable probability without drawing a card at 8 + 6 and dealer has 4 ~ 40%Favorable probability with drawing a card at 8 + 6 and dealer has 4 ~ 43.5%Clearly you are better off drawing a new card, which was so counter intuitive.Here is the difference of %Favorable events for each of the combination that can help you design a strategy.Cells highlighted in green are where you need to pick a new card. Cells highlighted in pink are all stays. Cells not highlighted are where player can make a random choice, difference in probabilities is indifferent.Insight 2 – Follow the below grid to take a decision whether to stay(0’s) or “hit me” (1’s). Win Rate = 41.4%Tie Rate = 9.5%Loss Rate = 49.1%SHOCKER!!! Our win rate is far lower than the loss rate of the game. It would have been much better if we just tossed a coin. The biggest difference  is that the dealer wins if both the player and the dealer gets burst. If you remove that single condition, here are the win/loss rate.Win Rate = 41.4%Tie Rate = 17.1%Loss Rate = 41.5%As you can see,  both dealer and player burst in about 8% of the games. By just changing this small thing, Casino’s make sure that we loose much more frequently than the house do.Insight 3 – Even with the best strategy, a player wins 41% times as against dealer who wins 49% times. The difference is driven by the tie breaker when both player and dealer goes burst.This is consistent with our burst table, which shows that probability of the dealer getting burst is 28.4%. Hence, both the player and the dealer getting burst will be 28.4% * 28.4% ~ 8%. Now we know what is the right gaming strategy, however, even the best gaming strategy can lead you to about 41% wins and 9% ties, leaving you to a big proportion of losses. Is there a betting strategy that can come to rescue us from this puzzle?The probability of winning in blackjack is known now. To find the best betting strategy, let’s simplify the problem. We know that the strategy that works in a coin toss event will also work in black jack. However, coin toss event is significantly less computationally intensive. Avg. number of games won = 50Avg. money you walk out of Casino =  $99.74 ~ $100 Max money won = $780%times person becomes bankrupt = 63.1%Did any of the above 4 metrics shock you? What got me to thinking was that even though the average value of anyone leaving the casino is same as what one starts with, the percentage times someone becomes bankrupt is much higher than 50%. Also, if you increase the number of games, the percentage times someone becomes bankrupt increases. Why is that?The reason is that we have a lower bound at $0 which is bankruptcy, but we don’t have an upper bound. On your lucky days, you can win as much as you can possibly win, and Casino will never stop you saying that Casino is now bankrupt. So in this biased game between you and Casino, for a non-rigged game, both you and Casino has the expected value of no gain no loss. But you have a lower bound and Casino has no lower bound. Simply put, let’s assume that you start with $100. If you win you can reach as high as $1000 or even $10k. But the expected value of your final amount is still $100 as the game was even. So, to pull the expected value down, a high number of people like you have to become bankrupt. Let us validate this theory through a simuation using the previously defined functions. In Mathematical style – Hence Proved!Clearly the bankruptcy rate and maximum earning seem correlation. What it means is that the more games you play, your probability of becoming bankrupt and becoming a millionaire both increases simultaneously. So, if it is not your super duper lucky day, you will end up loosing everything. Imagine 10 people P1, P2, P3, P4 ….P10.P10 is most lucky, P9 is second in line….P1 is the most unlucky.If all of them start with $100, the first one becoming bankrupt will be P1, and his $100 is divided among the other 9. Next in line of bankruptcy is P2 and so on. So, you might think P3 is on his lucky day, but if he/she plays enough number of games after P4 is gone, he is now fueling the earning of P2 and P1. In no time, P1 and P2 would rob P3. Obviously P2 is next and finally P1 will leave the casino with $1000 in his/her pocket. Casino is just a medium to redistribute wealth if the games are fair and not rigged, which we have already concluded is not the case. If all P1-P10 are playing black jack, I won’t be surprised if all of them loose bankrupt as dealer has higher odds of winning a game.Insight 4 – The more games you play, the chances of your bankruptcy and maximum amount you can win, both increases for a fair game (which itself is a myth). Is there a way to control for this bankruptcy in a non-bias game? Fortunately YES!What if we make the game fair. The lowest you can reach is a loss of $100. Let us now fix the highest profit you reach is also $100, and then you stop no matter what. Let’s try to simulate this. Avg. number of games won = 50Avg. money you walk out of Casino =  $99.34 ~ $100Max money won = $200%times person becomes bankrupt = 47.5%BINGO! % times reaching $0 is 50% and other 50% reach $200.  Now this looks fair! Let us run the same simulation we ran with the earlier strategy.Again mathematician style – Hence Proved! The Bankruptcy rate clearly fluctuates around 50%.  You can decrease it even further if you cap your earning at a lower % than 100%.  But sadly, no one can cap their winning when they are in Casino. And not stopping at 100% makes them more likely to become bankrupt later.Insight 5 – The only way to win in a Casino is to decide the limit of winning. On your lucky day, you will actually win that limit. If you do otherwise, you will be bankrupt even in your most lucky day. Here are a few exercise you can try solving and reply back in the comment section.Exercise 1 (Level : Low) – If you set your higher limit of earning as 50% instead of 100%, at what % will your bankruptcy rate reach a stagnation? Exercise 2 (Level : High) – Martingale is a famous betting strategy. The rule is simple, whenever you loose, you make the bet twice of the last bet. Once you win, you come back to the original minimum bet. For instance, your start with $1. You win 3 games and then you loose 3 games and finally you win 1 game. So your series of wins will beBasically what happens is that if you break any loosing streak, you recover all the money you have invested with profit same a minimum bet. For such a betting strategy, find:a. If  the expected value of winning changes?b. Does probability of winning changes at the end of a series of game? You can give the results as a function of number of games/minimum bet/total initial money in your pocket.c. Is this strategy any better than our constant value strategy (without any upper bound)? Talk about bankruptcy rate, expect value at the end of series, probability to win more games, highest earning potential.d. Does the strategy makes sense if you play a high number of matches or low number of matches for say – Minimum bet of $1 and Total initial money in pocket $100. High number of matches can be as high as 500, low number of matches can be as low as 10. Exercise 3 (Level – Medium) – For the Martingale strategy, does it make sense to put a cap on earning at 100% to decrease the chances of bankruptcy? Is this strategy any better than our constant value strategy (with 100% upper bound with constant betting)? Talk about bankruptcy rate, expect value at the end of series, probability to win more games, highest earning potential. Casinos are the best place to apply concepts of mathematics and the worst place to test these concepts. As most of the games are rigged, you will only have fair chances to win while playing against other players, in games like Poker. If there was one thing you want to take away from this article before entering a Casino, that will be always fix the upper bound to %earning. You might think that this is against your winning streak, however, this is the only way to play a level game with Casino.I hope you enjoyed reading this articl. If you use these strategies next time you visit a Casino I bet you will find them extremely helpful. If you have any doubts feel free to post them below.Now, I am sure you are excited enough to solve the three examples referred in this article. Make sure you share your answers with us in the comment section.
You can also read this article on Analytics Vidhya's Android APP ",https://www.analyticsvidhya.com/blog/2017/04/flawless-winning-strategy-casino-blackjack-data-science/
Data Scientist- Bangalore (3+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?|Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 3 – 8 years
Requirements : 
Task Info : About the company:We provides an AI-augmented on-demand workforce to enterprises for doing human tasks. We believe that in this day & age enterprises shouldn’t hire large teams to do operational work.By building world class enterprise software that combines the best of human & machine intelligence, we’re on our way to displace the incumbents of the BPO industry.We believe that special companies are built only when extremely smart and highly motivated individuals work towards a common goal. We started from India, went through the Y Combinator & Google Launchpad accelerator programs, and now have an international presence. But it’s still day one, there’s lots to be done. Job SummaryAs our first data scientist, you’ll be responsible for building our data science products, driving ML innovation across the stack and establishing thought leadership amongst industry experts.Responsibilities and DutiesPositive Background Correlations :Benefits and Perks
College Preference : no-bar
Min Qualification : ug
Skills : Computer Vision, deep learning, machine learning, python, r, statistical techniques, tensor flow
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-scientist-bangalore-3-years-of-experience/
Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Bangalore (3+ years of experience)|Business Analyst- Chennai (2 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 8 years
Requirements : •	Strong track record of delivering on high value / critical projects. Ability to communicate with senior stakeholders crisply and confidently on findings, status, bottlenecks and escalations
•	Ability to work independently, structure analyses and handle multiple priories
•	Proven track record of delivering in a start-up environment would be a plus
Task Info : About CompanyWe are India`s largest financial marketplace for lending and investment products. Our aim is to make personal finance decisions easy, transparent and convenient for India. Here, you can compare loans, credit cards and investment products, from India`s top banks and NBFCs, from anywhere and at any time.We offer a wide suite of products that include Personal Loan, Credit Card, Home Loan, Loan Against Property, Home Loan Balance Transfer, Car Loan, Education Loan, Savings Accounts, Mutual Funds, Fixed Deposits, Mobile Wallet and Gold Loan.About DepartmentThe Analytics function supports all business lines from Credit Strategy perspective, with responsibility for driving companywide credit initiatives. Also it drives the other analytics projects of organization importance. The key objectives of the function are:
•       Building Predictive models (Application Score Card, Product, Pricing & Portfolio Analytics, Loss Forecasting, Cross Sell Model, Developing Recommendation Algorithm)•       Creating Credit vision and Framework to ensure we are able to maximise on each visit on our portal•       Identifying the opportunity area/segments where we can buy the risk and fulfil the lead•       Business Intelligence: Develop and publish various new age dashboards for the Management & Senior leadership to track key business metrics also real time tracking of each action performed by Customer & Agent on our website and mobile app
The function works directly with CXO’s to identify areas of data analysis requirement, define problem statements and develop key insights.Purpose of your role and key accountabilitiesSelected candidate will lead Advance Analytics Team, will have the responsibility of working in a hands-on delivery role with primary focus on building Predictive Models, Credit Score Card, Strategic Analytics Initiatives.
Above responsibilities are just to outline primary focus, however it will not be limited to that. We are an emerging organisation, there is a high possibility that scope of the role will enhance/change time to time based on the needs and organisation priorities.Your skills and experienceAcademic Qualifications : B.E./ Master’s in Economics/ Business/ Marketing Science/ Econometrics / Master’s in business AnalyticsMandatory Experience: 4-8 year of experience in building Credit & Application Score Card, Predictive Modelling Role (Preferably in Fintech/BFSI/financial service industry)Tools: R, Python
College Preference : no-bar
Min Qualification : ug
Skills : bfsi, credit risk, predictive modeling, python, r
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/sr-manager-manager-data-science-gurgaon-4-to-8-years-of-experience/
Business Analyst- Chennai (2 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience)|Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 2 – 3 years
Requirements : The role needs someone who is good with numbers, has strong communication and interpersonal skills, superior problem solving abilities and analytical thinking with hands on experience in the Retail/ CPG/FMCG analytics domain.
Task Info : About the company:We are working on solving challenges at the point of sale, and in the process enabling retailers to better their business in the evolving world. We belong to the one of  the largest retail merchandising company in India. close to 5000 employees, 80 offices, operations across 600+ towns and 17 years of deep retail experience.Indian grocery retail is a melting pot of challenges. Some see these challenges as formidable and unsurmountable…we see them as opportunities, we are building solutions to monetise these opportunities. 
The role needs someone who is good with numbers, has strong communication and interpersonal skills, superior problem solving abilities and analytical thinking with hands on experience in the Retail/ CPG/FMCG analytics domain. Job DescriptionDesign and Build Statistical and Machine learning Models, Market Basket Analysis and Extraction systemsSkill SetEducation  Graduation in Engineering/mathematics/Science/ Economics/ Statistics/ MBA (preferred) /Masters Preferred Experience:  2-3 yrs. of total exp. in Analytics/ Research.  Experience in Retail or FMCG domain will be valuable.Job Location: Chennai
College Preference : no-bar
Min Qualification : pg
Skills : Market Basket analysis, predictive modeling, python, r, Retail Analytics, sas, sql, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/business-analyst-chennai-2-to-5-years-of-experience/
Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience),Learn everything about Analytics|,"Share this:|Like this:|Related Articles|Business Analyst- Chennai (2 to 5 years of experience)|DataHack Hour Revealed – the best way to learn data science through hands on problems!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 4 – 6 years
Requirements : •	Knowledge of at least a few analytical approaches required
•	Should have 1-2 years of experience in project & team management
•	Excellent problem solving skills and communication skills required
Task Info : About the company:We are the leading companies leverage big data, analytics & technology to drive smarter, faster & more accurate decisions in every aspect of their business. We serve as a strategic partner to our clients where we consult & deliver a wide range of analytics services for centralized analytics team or individual business units.Job description:Position ExpectationsQualification & ExperienceEducation: Preferably an MBA or an Engineer with relevant experience
College Preference : no-bar
Min Qualification : pg
Skills : marketing analytics, predictive modeling, project management, r, sas
Location : Bengaluru, Gurugram, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/senior-consultant-mumbai-bangalore-gurugram-4-to-5-years-of-experience/
DataHack Hour Revealed – the best way to learn data science through hands on problems!,Learn everything about Analytics|Introduction|Table of Contents|What is DataHack Hour?|Launch of DataHack Hour|Day 1 – Webinar|Day 2 – Jupyter Notebook and Python Scripts|Day 3 – Data Exploration and Visualization|Day 4 – Missing Values & Outliers|Day 5 – Building a Linear Regression model|Next Steps:|End Notes:,"Share this:|Like this:|Related Articles|Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience)|Head of Analytics-Mumbai (7 to 10 years of experience)|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"As part of DataFest 2017, we launched a new initiative – DataHack Hour. DataHack Hour was inspired by numerous queries we get related to learning Data Science. Questions like “How to learn analytics?” or “How to become a data scientist?” are asked to us multiple time every day.While we had written several articles on this subject on Analytics Vidhya – we needed something more definitive to answer these queries. In order to answer these queries, we decided to create an experience to show people how to learn Data Science. This version of DataHack Hour is our answer to the above questions or many more questions which come to us.DataHack Hour is completely free to consume for Analytics Vidhya community and is created with an aim to help more and more people learn data science. This article will tell you the journey participants of DataHack Hour are undergoing. If you are one of the people struggling to learn Data Science – join DataHack Hour today and become part of this awesome experience.  DataHack Hour is based on a very simple concept – “Daily small improvements or learning in small steps can make a huge difference over time.” It is the same principle which Jeff Olson describes in his book “The Slight Edge”. Let me explain this in a bit more detail.Most of the queries which we receive on Analytics Vidhya about challenges in learning can fall in one of the following categories:We believe that DataHack Hour is the solution to the first 3 problems mentioned here. We believe that by going over one chapter at a time daily, with help of volunteers and mentors from community to help is the most powerful way to learn Data Science. You learn by solving hands on problem, the content has been curated by Analytics Vidhya team and there are mentors to help you out on a daily basis. Honestly, I can’t think of a better way to learn! We launched DataHack Hour on 16th April 2017 as part of DataFest 2017. We got outstanding response from the community members and from people who want to really learn the subject.We came across various users like EspyM, who said he does not have access to such resources in his country and in 5 days we have seen him devoting time to build first model and submit a solution to DataHack platform! I am pretty sure that by end of this DataHack Hour, we will have multiple people like EspyM who would enable learning in their own communities later on.In order to raise awareness about DataHack Hour further, we are releasing the content of the first 5 days on our blog. The idea is to put the content out to a larger world and invite people who have missed out on 5 awesome days. You can still join today by learning the content below. You can register for DataHack here.If you registered on DataHack Hour and missed out a particular day, you can go through the content below and come back on track. We kicked off Datahack Hour with this awesome session by Tavish Srivastava. The agenda of the webinar was “How to convert a business problem to analytics problem? and Importance of hypothesis generation”. This is the best place to start your journey about learning analytics. It also touches about the point which gets ignored in a lot of tool focussed courses today.Here is the webinar recording from the session:Hopefully you are all geared up for the hands on exercises to come! From Day 2 onwards we started our 1 hour challenges. The agenda for day 2 included the following:Let us cover them one by one. You can download all Day 1 resources here after Logging in and Signing up. By end of the day you would have installed Anaconda, become comfortable with Jupyter notebook interface and would have written a few simple programs in Python and Pandas. We also cover different data structures in Python, iterative and conditional statement and ways to load and access the data.Our mentor for the day was none other than me 🙂 This session focussed on some of the practical challenges people face while doing exploratory analysis. Irrespective of how good is your data, you would come across missing values and Outliers. This session was aimed to help people deal with missing values and Outliers in the data. Again, you can access the content here after logging in and registering for DataHack Hour.Topics covered in Missing ValueOutlier detectionOn day 5, people will start build simple predictive models. The sessions starts with talking about what is a predictive model and enables you to build a simple and a multivariate regression model by end of this session. You can download the resources here.Here is the agenda for the coming days. If you think you got stuck in learning analytics and data science in past, come and join us in these DataHack Hour sessions. By end of this DataHack hour, you will be able to work on data science problems independently, would have 10+ mentors you wold have interacted with and a few hundreds of peers. All of this is available freely and can be absorbed as long as you are motivated!Day 6: Feature Engineering and Transformation, helps to improve model performanceDay 7: Validating and measuring model performanceDay 8: Building logistic regression modelDay 9: Building naive bayes modelDay 10: Building a decision tree modelDay 11: Building a k-NN modelDay 12: Ensemble, Methods to combine model outcomesDay 13: Apply your learnings on 6-hours hackathon",https://www.analyticsvidhya.com/blog/2017/04/datahack-hour-solutions-revealed/
Head of Analytics-Mumbai (7 to 10 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|DataHack Hour Revealed – the best way to learn data science through hands on problems!|Senior Machine Learning Scientist- Bangalore (3 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

Everything you Should Know about p-value from Scratch for Data Science 
|

Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor 
|

Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model 
|

Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills! 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :,No header found,"Experience : 7 – 10 years
Requirements : •	Strong communication and presentation skills
•	Attention to detail and ability to work in high pressure environment
Task Info : About the company:We believe buying insurance is not about avoiding risk, its about overcoming it. We hope to create the most simple to use & transparent platform to buy & manage insurance. We exist because buying insurance is currently confusing & painful. It should be easy too understand what is covered in your policy. It should be easy to compare features & pick the right policy. And it should be really easy to buy a policy online. We are here to make it happen, be really easy to buy a policy online. We are here to make it happen.As Head of analytics, you will design, develop and execute data driven solutions for multiple business units across the analytical spectrum for customer engagement and revenue growth.You will be working with the business functions & CXO team to identify business issues and strategic growth areas and enhance quality of decision in various business through analytics. Business Strategy:DataFinanceQualifications & Skills  
College Preference : tier1-any
Min Qualification : pg
Skills : clustering, data visualization, decision trees, linear regression, predictive modeling, python, r, sas, spss
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/head-of-analytics-mumbai-7-to-10-years-of-experience/
