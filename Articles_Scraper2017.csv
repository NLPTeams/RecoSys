Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
Key Highlights in Data Science / Deep Learning / Machine Learning 2017 and What can we Expect in 2018?,Learn everything about Analytics|Introduction|Interesting Snippets of Year 2017|Neuralink : A high bandwidth and safe Brain-Machine Interface|What more can we expect in 2018?|End Notes,"PowerBlox developed a scalable energy device capable of storing and distributing electricity from a variety of inputs|Face Recognition for payment transaction in KFC China|Release of Deeplearn.js: Harness Machine Learning in Your Browser |Release of CatBoost: A machine learning library to handle categorical data automatically|IBM Watson to aid in filing taxes|Shelf Engine: A startup developing AI to prevent food wastage|Body Labs  a start-up acquired by Amazon develops 3D models of individual human bodies from images|Data Science competition platform Kaggle joins Google Cloud|Capsule Networks  an improved deep learning architecture has been introduced|Canada bets big on artificial intelligence with AI institute|Baidu trained an AI agent to navigate the world like a parent teaches a baby|Machine learning creates living atlas of the planet|Disney understanding audience personality to better target them|Entrupy uses Deep learning to spot product authenticity|Detecting heart disease using Deep Learning|Facebook has decreased the training for visual recognition models |IBM Watson automatically generated highlight reels of Wimbledon|DeepMind created artificial agents that can imagine and plan ahead|Replika, a chatbot that creates a digital representation of you the more you interact with it|Using Twitter as a tool to forecast crime|HireVue is using AI to analyze word choice, tone, and facial movement of job applicants who do video interviews|E&Y using Email and Calendar data to understand how employees work|Carnegie Mellons Superhuman AI bests leading Texas Holdem poker professionals|Deep Learning to emulate cyber threats|Mozillas releases Speech Recognition model and Voice dataset|Course content for Fast.ais Cutting Edge Deep Learning for Coders  Part 2|China will allow self-driving cars to be tested on public roads|Emergence of Automated Machine Learning |Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|Introductory Guide  Factorization Machines & their application on huge datasets (with codes in Python)|11 most read Machine Learning articles from Analytics Vidhya in 2017|
Faizan Shaikh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"2017 has been a really exciting year for a data science professional. This is pretty evident from the new technologies that have been emerging day-by-day such as Face-ID which has revolutionized the way we secure information in our mobile phones. Self-driving cars had been a myth, but now they are very much a reality, the adoption of which can be seen by governments throughout the world.Data science is a field wherein ground-breaking research is happening at a much faster pace, in comparison to any other emergent technologies ever before. The time between contemplating a research idea and actually implementing it has come down significantly. . This is also fueled by the immense amount of resources freely available to everyone  which essentially enables even a normal person to contribute to research in their own way. For example, GitHub (a collaborative platform for software development) is now paving the way for research ideas to be shared in an implementation format. As Andrew Ng saidData is the new Oil
AI is the new ElectricityPersonalization and Automation is the talk of the day and more and more industries such as Financial Services, Healthcare, Pharmaceuticals and Automotive are adapting to the developments being brought upon by better Machine learning / Deep Learning models. This article specially focuses on the defining moments of Data Science in 2017. We have kept a few criterion in mind when we curated the list, namely:Also, we have shared our predictions in Data Science industry for the year 2018, which we believe would be really something to look forward to.Enjoy!Tags  Startup, Renewable EnergyA young company PowerBlox is using algorithms to give energy grids swarm intelligence. Power-Blox gives grids the ability to adapt automatically to fluctuating electrical loads and an ever-changing roster of power sources. This technology is extremely important as we move to grids sourced by wind, solar, and wave energy that fluctuate minute-by-minute, thus making renewable sources of energy more usable.
Tags  Startup, Innovation2017 probably saw its biggest announcement from Elon Musk who announced creation of a company called Neuralink that aims at building high bandwidth and safe Brain-Machine interfaces. It looks like Elon has finally decided to bring Matrix into reality where learning new skills for example flying a helicopter is just a matter of plugging a wire into your neocortex. All this may sound fiction and over exaggerated but Elon has been known to make things real, take Tesla and SpaceX for example.If it all comes true, humans may soon have the technology to study and map the brain in its entirety. This has far reaching implications to vastly improving healthcare to augmenting human capacity. And Elon seems serious too. The company recently received a $27 M funding and is looking to raise another $100M through its shares. For us mortal minds, open the link in the heading to understand how it all started and how the mission of Neuralink is grandeur enough to change the future of humanity if brought to reality.Tags  Innovation, Retail Industry, Computer VisionAlipay and KFC China are allowing customers to pay via facial recognition plus their phone
numbers. No cash, credit cards or smarts phones are necessary.This is the first retailer in the world to do so.Tags  Product release, machine learning, open source softwareDeeplearn.js an open source WebGL-accelerated JavaScript library for machine learning that runs entirely in a browser.Software engineers Nikhil Thorat and Daniel Smilkov noted, There are many reasons to bring machine learning into the browser. A client-side ML library can be a platform for interactive explanations, for rapid prototyping and visualization, and even for offline computation. And if nothing else, the browser is one of the worlds most popular programming platforms.Tags  machine learning, open source softwareYou have seen errors while dealing with categorical variables to build machine learning models using library sklearn, at least in the initial days ValueError: could not convert string to float. This error occurs when dealing with categorical (string) variables. In sklearn, you are required to convert these categories in the numerical format. In order to do this conversion, we use several pre-processing methods like label encoding, one hot encoding and others. CatBoost,a recently open-sourced library, developed and contributed by Yandex does this for you automatically.There are many such open source tools/ libraries which have been released in the last year. This article captures a few of the most popular ones.Tags  Company Collaboration, FinanceH & R Block, a tax preparation company is partnering with IBM Watson, to develop internal systems to help its employees to file customers taxes. The US tax code is 74,000 pages long which is difficult for any one person to know. IBM Watson will help tax professionals with prompts and questions to aid the tax interview process. At the end of this years tax process, this will also leave Watson with a massive library of tax data, which it can then analyze.Tags  Startup, Food IndustryShelf Engine is a startup developing robust models to help grocery stores enable their category managers to match orders of hundreds or thousands of products to demand. In the companys case study, it explains how many managers often make orders based on their current waste numbers  a flawed method, because that decision isnt based on a cumulation of waste and deliveries. Shelf Engine uses an order prediction engine and probability models that analyze historical order and sales data, gross margins, and shelf life information. The more a customer uses the system, the more accurate its recommendations become. The startup is backed by by Initialized Capital (Reddit co-founder Alexis Ohanian is a general partner), with participation from Founders Co-op, Liquid 2 Ventures (Joe Montana is a general partner), and others.Tags  Company Acquisition, Fashion RetailBody labs, a computer vision startup has developed applications that takes any input, whether thats 2D photos, 3D scans or actual body measurements and predicts full, 3D visual body shapes. The implications of this built-out technology are massive, spanning not only commercial opportunities in fashion and apparel, but fitness, gaming, health and manufacturing. This could actually solve the fitting issues with the customers especially in the ecommerce which is marred by huge return requests due to size issues. Body Labs was founded by Michael Black, William J. OFarrell, Eric Rachlin, and Alex Weiss who were connected at Brown University and Max Planck Institute for Intelligent Systems.Tags  Company AcquisitionGoogle acquired Kaggle, a competition platform for data scientists in March, 2017. Kaggle is known for hosting data science and machine learning competitions as well. Google is claimed to have acquired Kaggle in an attempt to enhance the AI and machine learning functionalities and to take advantage of the 600,000 data scientists at Kaggles community. With this acquisition, Kaggle has continued to provide services as before, but the product enhancements seen in Kaggle platform has increased multifold. For example, kernels which are online coding environments are a lot smoother and provide a lot more functionality than before  such as longer runtimes.Tags  Research, Deep LearningGeoffrey Hinton, one of the pioneers in deep learning, explains how capsule networks can be useful to improve over the traditional convolutional neural network architecture. If this technique is brought into application, it could easily beat the benchmarks of previous techniques until now.Actually, this technique was also discovered previously  but it has now been implemented in a stable way and can be seen to perform better.Tags  Industry collaborationUnder Geoffery Hinton, Canada Government and big companies like Google and Facebook has invest $150 milliion in Vector Institute to churn out 1000 graduates in AI every year. The Vector Institute intends propel Canada to the forefront of the global shift to artificial intelligence (AI) by promoting and maintaining Canadian excellence in deep learning and machine learningTags  Innovation, RoboticsBaidu taught an AI agent to navigate 2D space using only natural language, the same basic feedback mechanism a human parent uses with an infant. This is a progress towards building AI that can be taught in the same way as humans. Next target for the Chinese giant is to teach a physical robot navigate in a 3D space which is more realistic. This application based on reinforcement learning has huge implications for the robotics industry.Tags  Startup, Food ManagementDescartes Labs, a start-up in Mexico, uses satellite imagery and AI to predict food supplies and crisis-level food shortages months in advance. This leaves enough time to mount orderly humanitarian responses or optimize food supply networks. By processing these images and data via their advanced machine learning algorithm, Descartes Labs collect remarkably in-depth information such as being able to distinguish individual crop fields and determining the specific fields crop by analyzing how the suns light is reflecting off its surface. After the type of crop has been established, the machine learning program then monitors the fields production levels.Tags  Innovation, Behavioural AnalyticsDisney Researchs Maarten Bos explains about how his team of behavioral scientists conducted a series of studies on how people react to targeted marketing using images and discusses the ways this information might be used within Disney and beyond. If done intelligently and diligently, this could revolutionize the way we do brand marketing.Tags  Startup, Computer Vision, Deep LearningEntrupy is a start-up that uses computer vision algorithms to detect counterfeit products. They created a portable scanning device that instantly detects imitation designer bags by taking microscopic pictures that take into account details of the material, processing, workmanship, serial number, and wears/tear. It then employs the technique of deep learning to compare the images against a vast database that includes top luxury brands and if the bag is deemed authentic, users immediately get a Certificate of Authenticity.Tags  Startup, HealthcareCardiogram, in partnership with University of California San Francisco, modified an Apple watch that can detect atrial fibrillation  the most common heart arrhythmia  with higher accuracy than previously validated methods. They achieved this feat using deep learning techniques. As soon as the disease is detected, the device could send you a notification: We noticed an abnormality in your heartbeat. Want to chat with a cardiologist? which can potentially decrease the time between the onset of the disease, its detection, and its care.Tags  Innovation, Deep LearningEvery minute spent training a deep learning model is a minute not doing something else, and in todays fast-paced world of research, that minute is worth a lot. Facebook published a paper this morning detailing its personal approach to this problem. The company says it has managed to reduce the training time of a ResNet-50 deep learning model on ImageNet from 29 hours to one.Tags  Innovation, JournalismPreviously, the task of creating highlight packages and annotating photographs would be the responsibility of a human. But this year round, the job was placed in the hands of the Watson AI.Watson can generate highlight packages without any human input. It can watch a video feed and identify the most pertinent parts of a match. This can be seen by players shaking hands, gesticulating in celebration, or something as simple as the levels of volume from the audience.Tags  Innovation, Reinforcement learningDeepMind researchers created what theyre calling imagination-augmented agents, or I2As, that have a neural network trained to extract any information from its environment that could be useful in making decisions later on. These agents can create, evaluate and follow through on plans. To construct and evaluate future plans, the I2As imagine actions and outcomes in sequence before deciding which plan to execute. They can also choose how they want to imagine, options for which include trying out different possible actions separately or chaining actions together in a sequence.Tags  Innovation, Artificial IntelligenceReplika is a shadow-bot that tracks what youre up to on your computer and mimics your style, attitude, and tendencies in order to text like you would. To give an example, the inventor used it to mimic the presence of a dearly departed friend.Tags  Predictive Analysis, Twitter MiningUniversity of Virginia Assistant Professor Matthew Gerber is using Twitter data to predict crime in order to give police a digital spotlight on geographical crime hotspots. He demonstrated that using some old forecasting models and new tweets, he was able to predict 19 of 25 types of crime. This is another step detect sentiment of people on social media and taking emergency precautionary measures.Tags  Human Resources, Computer Vision, Natural Language ProcessingHireVue is using AI in the human resources space for making hiring decisions. This company uses video from job interviews to assess candidates facial expressions, body language, tone of voice, and keywords to predict which applicants are going to be the best employees. This technology will completely revolutionize the HR industryTags  Behavioural AnalyticsErnst and Young, one of the largest accounting firms in the US, is using a calendar and email data from its employees to identify patterns around who is engaging with whom, which parts of the organization are under stress, and which individuals are most active in reaching across company boundaries.Tags  Innovation, Game agentsCMU releases there a secret recipe of how they built a Superhuman AI that beats professional players at Poker. This is significant because no-limit Texas Holdem is whats called an imperfect-information game, which means that not all information about all elements in play is available to all players at all times. Thats in contrast to games like Go and Chess, both of which feature a board which contains all the pieces in play, plainly visible to both competitors.
Tags  Innovation, Cyber Security, Deep LearningScientists have harnessed the power of artificial intelligence (AI) to create a program that, combined with existing tools, figures out passwords. The work could help average users and companies measure the strength of passwords, says Thomas Ristenpart, a computer scientist who studies computer security at Cornell Tech in New York City but was not involved with the study. The new technique could also potentially be used to generate decoy passwords to help detect breaches.Tags  Product Release, Speech Recognition, Deep LearningTo speed up advancements in the audio domain, Mozilla has released the worlds second largest publicly available voice dataset, along with open sourcing the cutting edge technology of speech recognition. This release will certainly affect the advancements of speech recognition in general.
Tags  Deep LearningVideos and course content for Course Cutting Edge Deep Learning for Coders  Part 2 is now available for general public. For those who havent had a chance to see Part 1 of the course  the course introduces you to basics of Deep learning in a practical manner. Part 2 enables you to get into the details of Deep Learning and introducing you to the cutting edge research that is going on in the industry
Tags  Self-driving cars, TransportationChina is opening up its roads to self-driving cars. The Beijing Municipal Transport Commission released a statement saying that on certain roads and under certain conditions, companies registered in China will be able to test their autonomous vehicles.Tags  Automated Machine LearningAutomated Machine Learning is the new deal. It does most of the heavy lifting required to complete a data science lifecycle on its own. This is a very powerful idea; while we previously have had to worry about tuning parameters and hyperparameters, automated machine learning systems can learn the best way to tune these for optimal outcomes by a number of different possible methods.
There is so much happening in the industry that its difficult to keep up with the current trends. And it will continue to grow, as soon as the cutting-edge research is transformed into use for the ordinary man. To give an example, you can see the impact that deep learning research has done to computer vision, where we see applications like Face-ID, self-driving car just around the corner. In the future, you will certainly see a boom in the applications that can be driven by deep learning techniques.A few things that I am particularly looking forward to,As Andrej Karpathy, an eminent personality in data science industry explains,Neural networks are not just another classifier, they represent the beginning of a fundamental shift in how we write software. They are Software 2.0. Software 2.0 is not going to replace the software we know now, but it is going to take over increasingly large portions of what software is responsible for today.This article list down a few of the groundbreaking events that have happened in the year 2017 in data science industry, and the implications they have on a data science professional. It also shows a path that can be seen by these advancements in the near future.If you know a ground-breaking event and want to share it with the community, please do write your comment below.",https://www.analyticsvidhya.com/blog/2017/12/reminiscing-2017-defining-moments-and-future-of-data-science/
11 most read Machine Learning articles from Analytics Vidhya in 2017,Learn everything about Analytics|Introduction|Top 11 Machine Learning articles from Analytics Vidhya in 2017|End Notes,"Ultimate Guide to Understand & Implement Natural Language Processing (with codes in Python)|Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning|A comprehensive beginners guide for Linear, Ridge and Lasso Regression

|Natural Language Processing Made Easy  using SpaCy ( in Python)|How to build Ensemble Models in machine learning? (with code in R)|Which algorithm takes the crown: Light GBM vs XGBOOST?|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)||Comprehensive Tutorial to Learn Data Science with Julia from Scratch|CatBoost: A machine learning library to handle categorical (CAT) data automatically||Solving Multi-Label Classification problems (Case studies included)||Tutorial on Automated Machine Learning using MLBox||Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|Key Highlights in Data Science / Deep Learning / Machine Learning 2017 and What can we Expect in 2018?|11 most read Deep Learning Articles from Analytics Vidhya in 2017|
NSS
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

 9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The next post at the end of the year 2017 on our list of best-curated articles on  Machine Learning. These curated articles will be a one stop solution for people who are getting started with Machine Learning or who already have. This article contains all the best articles of 2017 which gathered the interest of the Machine Learning community.Similar to the previous article on -Best Deep Learning articles in 2017, I have added the used tool and the level of difficulty for each article to facilitate you with the choice. If you wish to include any other learning resource/article here, please mention them in the comments.A large amount of unstructured data present today is in the form of text, for example : Medical documents, legal agreements, tweets, blogs, newspapers, chat conversions etc. These text informations are the storehouse of new innovative products that can revolutionise the way we interact with the technology and live our lives. A few of the examples are:This is just the tip of the iceberg for what is possible if Natural Language is exploited.This article explains the basic concepts behind Natural Language Processing such as Text Processing, Feature Extraction from text etc. along with their codes in Python.This is a must read article for someone getting started into the field of Natural Language Processing.Tool: PythonLevel: BeginnerMachine Learning has been with us since a long time ago, but it picked up pace about a decade back, part in thanks to the advancements in the hardware and in part to the Algorithms.This article is about one such Algorithm which is extremely popular in the field of Machine Learning  Gradient Descent. This article explains in detail about how Gradient Descent works, the problems in the original Gradient Descent and the variants of Gradient Descent for overcoming the problem along with the implementation.Level: IntermediateAn operations manager working at a Supermarket chain in India knows about the amount of preparation the store chain needs to do before the Indian festive season (Diwali) kicks in. It is for them to estimate/predict which product will sell like hotcakes and which would not prior to the purchase. A bad decision can leave your customers to look for offers and products in the competitor stores. The challenge does not finish there  he also needs to estimate the sales of products across a range of different categories for stores in varied locations and with consumers having different consumption techniques. This article tells you everything you need to know about regression models and how they can be used to solve prediction problems like the one mentioned above.Tools: Python
Level: IntermediateThere are many libraries out in the industry which provides methods for exploiting the text data to make sense out of it. Some of the examples being like Stanford CoreNLP, NLTK etc. and Python has been the go-to choice for working with text data.But these libraries lacking in the sense that they are bulky and with too much overhead like NLTK which downloads thousands and thousands of files for performing any NLP task.This is where SpaCy comes in  an industrial grade superfast NLP library which can perform almost all the NLP tasks with the breeze. This article makes you aware of the syntax of SpaCy and teaches you to perform some very common NLP tasks like PoS tagging, NER etc with minimal lines of code. The article also introduces the concept of Word vectors which are currently the state-of-the-art in features extracted from the text.Tools: PythonLevel: IntermediateIf you are an active participant in the Data Science Competitions or have just started participating in the competitions and have gone through the solutions of the winners, you will notice that most of them use a blend of different models to extract that last drop of performance from the models.This blend of models is what is called  Ensemble Learning, where you combine the learnings of different models to create a better-learned model. In this article, you will learn about the different Ensembling techniques along with how you can code them up in R to ace your Data Science Competitions.Tools: RLevel: IntermediateFor active members of the Data Science Competitions, XGBOOST almost became the go-to algorithm for performance and winning the competitions. It has the best of both the boosting machines and regularised methods.But it suffers from one problem: Given a huge amount of data, it takes a very long time to train. This is where LightGBM comes in.This article explains about LightGBM and compares it with XGBOOST in terms of performance and speed. This article is a must for people looking to reduce their training time in the competition without losing on the performance of the model.Tool: PythonLevel: ExpertWe as data scientists and machine learning engineers spend a lot of time trying to come up with the best performing model for solving a problem and most of the time we do get successful. But all these investments of time and mind will become useless if do not put the model in the real life.For example, an algorithm that can detect cataract just by looking at a photo is useless if the end user or person with cataract cannot input the image into the model. After all, models are created to solve a problem. Running a model shouldnt be a problem for an end customer.This is where this article comes in. This article explains how you can deploy a machine learning model and use it to solve problems.Tools: PythonLevel: ExpertThere is a quote about Julia that says  Walks like python. Runs like C.The above line tells a lot about why creating ripples in the numerical computing space, even though it was in its early stages. Julia is a work straight out of MIT, a high-level language that has a syntax as friendly as Python and performance as competitive as C. This is not all, it provides a sophisticated compiler, distributed parallel execution, numerical accuracy, and an extensive mathematical function library.This article is about how can you utilize it in your workflow as a data scientist without going through hours of confusion which usually comes when we come across a new language.Tool: JuliaLevel: BeginnerYou have seen below error while building your machine learning models using sklearn  at least in the initial days.
This error occurs when dealing with categorical (string) variables. In sklearn, you are required to convert these categories in the numerical format.In order to do this conversion, we use several pre-processing methods like label encoding, one hot encoding and others.This article discusses a recently open-sourced library  CatBoost developed and contributed by Yandex. As said by Mikhail Bilenko, Yandexs head of machine intelligence and research, This is the first Russian machine learning technology thats an open source! Pretty interesting right?
Tool  PyhtonLevel  IntermediateIf we consider the image below  does this image contain a house? The option will be YES or NO.Consider another case, like what all things (or labels) are relevant to this picture?These types of problems, where we have a set of target variables, are known as multi-label classification problems. This article explains in detail what this problem entails and how to deal with it in the form of case studiesTool: Python
Level: ExpertAs soon as the library was released on GitHub, many data scientists were extremely excited to try it out. In this article, we have talked about an automated machine learning library MLBox.MLBox is a powerful Automated Machine Learning Python library.It provides the following features:The library automates the machine learning and feature engineering process itself. Just to give you an example, with just 8 lines of code  the creator of the library broke into top 1% of data science hackathon. This article gives you hands-on practice of the library MLBox.Tool: PythonLevel: ExpertI hope you found the resources useful. Machine Learning is already helpful in solving many problems in different fields. I hope that we have been helpful on your journey to learn this year and we promise to do so in the coming year as well.The Analytics Vidhya family wishes you Merry Christmas and very happy new year. May the new year bring the best of health, wealth and knowledge for you. In the meanwhile, if you have any suggestions / feedback, do share them with us. If you have any questions, feel free to drop your comments below.",https://www.analyticsvidhya.com/blog/2017/12/11-machine-learning-articles-analytics-vidhya-2017/
11 most read Deep Learning Articles from Analytics Vidhya in 2017,Learn everything about Analytics|Introduction|Top 11 Deep learning articles from Analytics Vidhya in 2017|End Notes,"1. Understanding and coding Neural Networks From Scratch in Python and R|2. Getting started with Deep Learning using Keras and TensorFlow in R|3. Architecture of Convolutional Neural Networks(CNN) demystified|4. Hands-on with Deep Learning  Solution for Age Detection Practice Problem|5.Debugging & Visualizing training of Neural Network with TensorBoard|6.Transfer learning & The art of using Pre-trained Models in Deep Learning|7.Fundamentals of Deep Learning  Introduction to Recurrent Neural Networks|8. An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|9.Getting Started with Audio Data Analysis using Deep Learning (with case study)|10.An introductory guide to Generative Adversarial Networks (GANs) and their promise!|11.6 Deep Learning Applications a beginner can build in minutes (using Python)|Learn,engage, hackandget hired!|Share this:|Like this:|Related Articles|11 most read Machine Learning articles from Analytics Vidhya in 2017|15 Trending Data Science GitHub Repositories you can not miss in 2017|
Dishashree Gupta
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This is that time of year, when you reflect on the year gone by.2017 has been a year of growth for us at Analytics Vidhya. By any means / metric  we have seen growth, be it web traffic, number of hackathons, number of discussions, team size, a journey of meetup to the large-scale summit.This year, we covered breadth as well as depth on various data science topics including machine learning, deep learning, and reinforcement learning. You will see lot more articles coming your way in 2018. It gives us immense satisfaction to be able to create something which is helping more and more people every day.Whether you are charting your learning plan for 2018 or reflecting back on your learning this year  curation of best resources can be immensely helpful. This is why, we present our first curation on best AV articles published on Deep Learning in 2017. If you missed these articles, you should go through them. If you are new to deep learning  this is going to be the best resources curated at a single place for you.To help you, we have also mentioned the level of article and the tools used in the article.So stay warm, keep your machines running and keep learning as the new year sets in. If you followed any other resources on deep learning this year, please feel free to mention them in the comments below.Neural networks are believed to be black boxes. People run far away from understanding their inner working. Reading this article would completely change that.This article simplifies the architecture of a neural network beginning from what a perceptron is going all the way to code them in numpy and in R.By end of this article, you will understand how Neural networks work, how do we initialize weights and how do we update them using back-propagation. Its a must read article for those who wish to completely understand the working of neural networks in and out.Tools Used: Python(numpy), RLevel: IntermediateIt has always been a debatable topic to choose between R and Python. The Machine Learning world has been divided over the preference of one language over the other. But with the explosion of Deep Learning, the balance shifted towards Python as it had an enormous list of Deep Learning libraries and frameworks which R lacked (till now).With launch of Keras in R, this fight is back at the center. Python was slowly becoming the de-facto language for Deep Learning models. But with the release of Keras library in R with tensorflow (CPU and GPU compatibility) at the backend as of now, it is likely that R will again fight Python for the podium even in the Deep Learning space.In this article, you will see how to install Keras with Tensorflow in R and build your first Neural Network model on the classic MNIST dataset in RStudio.Library used  keras in RLevel  IntermediateIn this article we discussed the architecture behind Convolutional Neural Networks, which are designed to address image recognition and classification problems. Every image is an arrangement of pixels arranged in a special order. If you change the order or value of a pixel, the image would change as well. To understand an image for a network its extremely important to understand how the pixels are arranged. Convolutional networks are designed specifically to conserve the arrangement of these pixels and obtain various features from the images. This article would help you understand the various layers in a convolutional neural network and would also enable you to implement an image classification task using CNNs.Library used: KerasLevel: IntermediateIt is one thing to learn data science by reading or watching a video / MOOC and other to apply it on problems. You need to do both the things to learn the subject effectively. If you are questioning, why learn or apply deep learning  you have most likely come out of a cave just now. Deep learning in already powering face detection in cameras, voice recognition on mobile devices to deep learning cars. This article encourages you to solve fun and interesting problem  to detect the age of a person using Deep Learning.Library Used: kerasLevel: AdvancedIf you have tried to train a neural network, you must know the plight of figuring out why does it not converge. It could range from a simple data transformation issue to a model creation issue. This article focuses on a workflow to debug a neural network. The purpose is guide to you as to how would you approach to solve the problem. The article also introduces a tool which is a useful addition to the deep learning toolbox  TensorBoard.Library used: Keras, TensorBoardLevel: IntermediateIn todays world, RAM on a machine is pretty cheap and is easily available with little investment. You need hundreds of GBs of RAM to run a super complex supervised machine learning problem. On the other hand, access to GPUs is not that cheap. If you need access to a hundred GB VRAM on GPUs  it wont be straight forward and would involve significant costs. When we try to solve complex real life problems on areas like image and voice recognition, once you have a few hidden layers in your model, adding another layer of hidden layer would need immense resources and time. There is something called Transfer Learning which enables us to use pre-trained models from other people for our own problems by some tweaking. In this article, I am going to tell how we can use pre-trained models to accelerate our solutions and use pre-trained models like VGG16 for image classification.Library used: KerasLevel: AdvancedThere are multiple such cases wherein the sequence of information defines the data itself. If we are trying to use such data for any reasonable output, we need a network which has access to some prior knowledge about the data to completely understand it. Recurrent neural networks thus come into play. In this article we introduce recurrent neural networks. To make things simple for you we have shown the working for a recurrent neuron in Excel. We also discuss the Backpropagation in time along with some shortcomings of the recurrent neural networks. To say in a few words its an introductory guide that shall enable you to understand and to use recurrent neural networks in a problem of your own. Library used: KerasLevel: IntermediateWhen we look at an example such as typing a sentence in google translate in English and getting an equivalent Chinese conversion, we see an application of text processing. Text processing deals with humongous amount of text to perform different range of tasks like clustering, classification and Machine Translation.Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing these tasks. Whereas for a computer, it is very hard to perform the tasks which humans can do quite effectively. Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? The answer to the question lies in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in.All of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them.In the article, you will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results.Library used  gensimLevel  AdvancedWhen you get started with data science, you start simple. You go through simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format. In other words, you are spoon-fed the hardest part in data science pipeline.The datasets in real life are much more complex. You first have to understand it, collect it from various sources and arrange it in a format which is ready for processing. This is even more difficult when the data is in an unstructured format such as image or audio. This is so because you would have to represent image/audio data in a standard way for it to be useful for analysis.Interestingly, unstructured data represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice.In this article, an overview of audio / voice processing with a case study is covered so that you would get a hands-on introduction to solving audio processing problems.Library used: KerasLevel: IntermediateNeural Networks have made great progress. They now recognize images and voice at levels comparable to humans. They are also able to understand natural language with a good accuracy. But even then, the talk of automating human tasks with machines looks a bit far fetched. After all, we do much more than just recognizing image / voice or understanding what people around us are saying  dont we? Let us see a few examples where we need human creativity (at least as of now):Do you think, these tasks can be accomplished by machines? Well  the answer might surprise you These are definitely difficult to automate tasks, but Generative Adversarial Networks (GANs) have started making some of these tasks possible.If you feel intimidated by the name GAN  dont worry! You will feel comfortable with them by end of this article.This article introduces you to the concept of GANs and explains how they work along with the challenges. It will also let you know of some cool things people have done using GANs and give you links to some of the important resources for getting deeper into these techniques.Library used: KerasLevel: AdvancedDeep Learning has been the most researched and talked about topic in data science recently. And it deserves the attention it gets, as some of the recent breakthroughs in data science are emanating from deep learning. Its predicted that many deep learning applications will affect your life in the near future. However, if you have been looking at deep learning from the outside, it might look difficult and intimidating. Terms like TensorFlow, Keras, GPU based computing might scare you. But its actually not that difficult! While cutting edge deep learning will take time and effort to follow, applying them in simple day to day problems is very easy.This article showcases 6 such applications  which might look difficult at the outset, but can be achieved using Deep Learning implementation in less than an hour, to give you a taste of how they work.Tools Used: Deep Learning APIs and Open source Deep learning softwareLevel: BeginnerI hope you found the resources useful. Deep learning is spreading its wings and this year has made us learn a lot in this domain. I hope that we have been helpful on your journey to learn this year and we promise to do so in the coming year as well.The Analytics Vidhya family wishes you Merry Christmas and very happy new year. May the new year bring the best of health, wealth and knowledge for you. In the meanwhile, if you have any suggestions / feedback, do share them with us. If you have any questions, feel free to drop your comments below.",https://www.analyticsvidhya.com/blog/2017/12/11-deep-learning-analytics-vidhya-2017/
15 Trending Data Science GitHub Repositories you can not miss in 2017,Learn everything about Analytics|Introduction|Table of Contents|1. Learning Resources|2. Open Source Softwares|End Notes,"1.1 Awesome Data Science|1.2 Machine Learning / Deep Learning Cheat Sheet|1.3 Oxford Deep Natural Language Processing Course Lectures|1.4 PyTorch  Tutorial|1.5 Resources of NIPS 2017|2.1 TensorFlow|2.2 TuriCreate  A Simplified Machine Learning Library|2.3 OpenPose|2.4 DeepSpeech|2.5 Mobile Deep Learning|2.6 Visdom||2.7 Deep Photo Style Transfer|2.8 CycleGAN|2.9 Seq2seq|2.10 Pix2code|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|11 most read Deep Learning Articles from Analytics Vidhya in 2017|Introduction to Computational Linguistics and Dependency Trees in data science|
Sunil Ray
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GitHub is much more than a software versioning tool, which it was originally meant to be. Now people from different backgrounds and not just software engineers are using it to share their tools / libraries they developed on their own, or even share resources that might be helpful for the community.Following the best repos on GitHub can be an immense learning experience. You not only see what are the best open contributions, but also see how their code was written and implemented.Being an avid data science enthusiast, I have curated a list of repositories that have been particularly famous in the year 2017. Enjoy and Keep learning!This GitHub repository is an ultimate resource guide to data science. It is built upon multiple contributions over the years with links to resources ranging from getting-started guides, infographics to people to follow on social networking sites like twitter, facebook, Instagram etc. There are plenty of resources waiting to be viewed, irrespective of whether you are a beginner or a veteran.A look at table of contents of the repo says it all about the depth of resources in the repository:Link to repository: https://github.com/bulutyazilim/awesome-datascienceThis repository consists of the commonly used tools and techniques compiled in the form of cheatsheets. The cheatsheets range from very simple tools like pandas to techniques like Deep Learning. After giving a star or forking the repository, you wont need to google the most commonly used tips and tricks.To give you a glimpse, the different types of cheatsheets are pandas, numpy, scikit learn, matplotlib, ggplot, dplyr, tidyr, pySpark and Neural Networks .Link to repository: https://github.com/kailashahirwar/cheatsheets-aiStanford NLP has always been a golden course for people wanting to venture out into the field of Natural Language Processing. But with the advent of Deep Learning, NLP has seen tremendous progress, all thanks to the capabilities of Deep Learning Architectures such as RNN and LSTMs.This repository based on Oxford NLP Lectures take the education of NLP to next level. A practical course, these lectures covers the techniques and terminologies to advance material such as using RNNs for Language Modeling, Speech Recognition, Text to Speech etc. This repository is a one stop shop for all the materials of the Oxford Lectures providing Lecture materials to Practical assignments.Link to repository: https://github.com/oxford-cs-deepnlp-2017/lecturesAs of now, PyTorch is the sole competitor to Tensorflow and it is doing a good job of maintaining its reputation. With the ease of Pythonic style coding, Dynamic Computations, and faster prototyping, PyTorch has garnered enough attention of the Deep Learning Community.This repository contains codes for Deep Learning tasks ranging from learning basic of creating a Neural Network in PyTorch to coding RNNs, GANs and Neural Style Transfers. Most of the models have been implemented with as few as 30 lines of code. This speaks volume about the abstraction provided by PyTorch so that researchers may focus on finding the right model quickly rather than getting entangled in the nitty gritty of programming language or tool choice.Link to repository: https://github.com/yunjey/pytorch-tutorialThis repository is a list of resources and slides of all invited talks, tutorials,and workshops in NIPS 2017 conference. For those who do not know what NIPS is, it is an annual conference specifically for Machine learning and Computational Neuroscience.Most of the breakthrough research that has happened in the data science industry in the last couple of years has been a result of the research that has been presented at this conference. If you want to stay ahead of the curve, this is the right resource to follow!Link to repository: https://github.com/hindupuravinash/nips2017It has been 2 years since the official release of TensorFlow, but it has maintained the status of being the top Machine Learning / Deep Learning library. Google Brain and the community behind the development of TensorFlow has been actively contributing and keeping it abreast with the latest developments especially in Deep Learning domain.TensorFlow was originally built as a library for numerical computation using data flow graphs. But looking at its current state, it can be pretty much said to be a complete library for building Deep Learning models. Although TensorFlow majorly supports Python, it also provides support for languages such as C, C++, Java and many more. And a cherry on the cake, it can also be run on a mobile platform!Link to Repository: https://github.com/tensorflow/tensorflowA recent open source contribution by Apple, TuriCreate is the talk of the day. It boasts of easy-to-use creation and deployment of machine learning models for complex tasks such as object detection, activity classification, and recommendation systems.Being a data science enthusiast for some time now, I remember that void that had been created when Turi (the company that created GraphLab Create  an amazing machine learning library) was acquired by Apple. Everyone in the data science industry had been waiting for this kind of explosion to happen!TuriCreate is developed specially for python. One of the best features that TuriCreate provides is its easy deployability of machine learning models to Core ML (another open source software by Apple) for use in iOS, macOS, watchOS, and tvOS appsLink to repository: https://github.com/apple/turicreateOpenPose is a multi-person keypoint detection library which helps you to detect positions of a person in an image or video at real-time speed. Developed by CMUs perceptual computing lab, OpenPose is a fine example of how open sourced research can be easily inculcated in the industry.One of the use cases that OpenPose helps to solve is activity detection. For example, an activity done by an actor can be captured in real time. Then these key points and their motions can be used to create animated films.OpenPose has a C++ API which can be used to access the library. But it also has a simple command line interface to process images or videos.Link to repository: https://github.com/CMU-Perceptual-Computing-Lab/openposeDeepSpeech library is an open source implementation of the state-of-the-art technique for Speech-to-Text synthesis by Baidu Research. It is based on TensorFlow and can be used specifically for Python, but it also has bindings for NodeJS and can be used on the command line too.Mozilla has been one of the main workforces for building DeepSpeech from scratch and open sourcing the library. There are only a few commercial quality speech recognition services available, dominated by a small number of large companies. This reduces user choice and available features for startups, researchers or even larger companies that want to speech-enable their products and services, Together with a community of like-minded developers, companies, and researchers, we have applied sophisticated machine learning techniques and a variety of innovations to build a speech-to-text engine  Sean White, vice president of technology strategy at Mozilla, wrote in a blog post.I believe that the library is worth checking out. Do let me know if you use it in the comments below.Link to repository: https://github.com/mozilla/DeepSpeechThis repository brings about the state-of-the-art technique in data science to the mobile platform. Developed by Baidu Research, the repository aims to deploy Deep Learning models on mobile devices such as Android and IOS with low complexity and high speed. A simple use case as explained in the repository itself is object detection. It can identify the exact location of an object such as mobile in an image. Pretty cool right?Link to repository: https://github.com/baidu/mobile-deep-learningVisdom is a library that supports broadcasting of plots, images, and text among collaborators. You can organize your visualization space programmatically or through the UI to create dashboards for live data, inspect results of experiments, or debug experimental code.The exact inputs into the plotting functions vary, although most of them take as input a tensor X than contains the data and an (optional) tensor Y that contains optional data variables (such as labels or timestamps). It supports all basic plot types to create visualizations that are powered by Plotly.Visdom supports Torch and Numpy within Python.Link to repository: https://github.com/facebookresearch/visdomThis repository is based on a research paper that introduces a deep learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. The approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including the transfer of the time of day, weather, season, and artistic edits. This code is based on torch.Link to Repository: https://github.com/luanfujun/deep-photo-styletransferCycleGAN is a fun but powerful library which shows the potential of the state-of-the-art technique. Just to give an example, the image below is a glimpse of what the library can do  adjusting the depth perception of the image. The catch here is that you havent told the algorithm which part of the image to focus upon. It does this on its own!The library is currently written in Lua, but it can be used in command line too.Link to repository: https://github.com/junyanz/CycleGANSeq2seq was initially built for Machine Translation, but have since been developed to be used for a variety of other tasks, including Summarization, Conversational Modeling, and Image Captioning. As long as a problem can be moulded as encoding input data in one format and decoding it into another format, this framework can be used. It is programmed using the all popular Tensorflow library for Python.Link to Repository: https://github.com/google/seq2seqThis one is a really exciting project using deep learning that attempts to automatically generate code for a given GUI. When building a website or a mobile interface, front-end engineers typically have to write repetitive code that is time consuming and non-productive. This essentially prevent developers from dedicating the majority of their time to implement the actual functionality and logic of the software they are building. Pix2code intends to remedy this by automating the process. It is based on a novel approach allowing the generation of computer tokens from a single GUI screenshot as input.Here is a video explaining the use case of pix2code.Pix2code is written in python and can be used to convert image captures of both mobile and web interfaces to code. The project can be accessed in the link below.Link to Repository: https://github.com/tonybeltramelli/pix2codeI hope you got to know a few of the new open source tools/technologies that have been released on GitHub in the year 2017. I have also listed down resources that have been trending on GitHub. If you have seen more such useful repositories in the past, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/12/15-data-science-repositories-github-2017/
Introduction to Computational Linguistics and Dependency Trees in data science,Learn everything about Analytics|Introduction||Computational linguistics|Applications of Dependency Trees|Dependency Trees using Spacy||End Notes,"|Example  Problem with Neural Networks|Named Entity Recognition|Coreference Resolution or Anaphora Resolution|Question Answering|Other Tasks that uses Computational Linguistics|Generating Dependency Trees using Stanford Core NLP|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|15 Trending Data Science GitHub Repositories you can not miss in 2017|Essentials of Deep Learning : Introduction to Long Short Term Memory|
Shivam Bansal
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In recent years, the amalgam of deep learning fundamentals with Natural Language Processing techniques has shown a great improvement in the information mining tasks on unstructured text data.The models are now able to recognize natural language and speech comparable to human levels. Despite such improvements, discrepancies in the results still exist as sometimes the information is coded very deep in the syntaxes and syntactic structures of the corpus.For example, a conversation system which is trained using recurrent neural network produces the following results in two scenarios:User: Hi, I took a horrible picture in a museum, can you tell where is it located?
Bot Reply 1: The museum is located at a horrible placeUser: Hi, I took a horrible picture in a museum, can you tell where is it located?
Bot Reply 2: The horrible museum is located at this placeThe two responses have virtually similar tokens but different structures which completely changes the context. In this article, I will discuss the interdisciplinary field of Computational Linguistics which deals with the structural aspects of the text that are used to solve common text related problems. Some examples are Named entity extraction, coreference resolution, and machine translation.Computational linguistics often overlaps with the field of Natural Language Processing as most of the tasks are common to both the fields. While Natural Language Processing focuses on the tokens/tags and uses them as predictors in machine learning models, Computational Linguistics digs further deeper into the relationships and links among them.Structural aspects of the text refer to the organization of tokens in a sentence and the how the contexts among them are interrelated. This organization is often depicted by the word-to-word grammar relationships which are also known as dependencies. Dependency is the notion that syntactic units (words) are connected to each other by directed links which describe the relationship possessed by the connected words.These dependencies map directly onto a directed graph representation, in which words in the sentence are nodes in the graph and grammatical relations are edge labels. This directed graph representation is also called as the dependency tree. For example, the dependency tree of the sentence is shown in the figure below:AnalyticsVidhya is the largest community of data scientists and provides best resources for understanding data and analytics.Another way to represent this tree is following:-> community-NN (root)
-> AnalyticsVidhya-NNP (nsubj)
-> is-VBZ (cop)
-> the-DT (det)
-> largest-JJS (amod)
-> scientists-NNS (pobj)
-> of-IN (prep)
-> data-NNS (case)
-> and-CC (cc)
-> provides-VBZ (conj)
-> resources-NNS (dobj)
-> best-JJS (amod)
-> understanding-VBG (pcomp)
-> for-IN (mark)
-> data-NNS (dobj)
-> and-CC (cc)
-> analytics-NNS (conj)These trees can be generated in python using libraries such as NLTK, Spacy or Stanford-CoreNLP and can be used to obtain subject-verb-object triplets, noun and verb phrases, grammar dependency relationships, and part of speech tags etc for example -> of-IN (prep)-> data-NNS (nn)POS: IN  NNS  NNSPhrase: of data scientist-> for-IN (prep)-> data-NNS (dobj)-> and-CC (cc)-> analytics-NNS (conj)POS: NNS  CC  NNSPhrase: data and analyticsGrammar: <prep> <pcomp> <prep> <dobj>POS: VBG  IN  NNSPhrase: for understanding data and analyticsNamed-entity recognition (NER) is the process of locating and classifying named entities in a textual data into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.To recognize the named entities, one needs to parse the dependency tree in a top to bottom manner and identify the noun phrases. Noun phrases are the subtrees which are connected by a relation from nouns family such as nsubj, nobj, dobj etc nmod etc. For example Donald Trump will be visiting New Delhi next summer for a conference at Google-> visiting-VBG (root)
-> Trump-NNP (nsubj)
-> Donald-NNP (compound)
-> will-MD (aux)
-> be-VB (aux)
-> Delhi-NNP (dobj)
-> New-NNP (compound)
-> summer-NN (tmod)
-> next-JJ (amod)
-> conference-NN (nmod)
-> for-IN (case)
-> a-DT (det)
-> Google-NNP (nmod)
-> at-IN (case)In the above trees, following noun phrases are are detected from the grammar relations of noun family such as:Trump <-> visiting <by> nsubj
Delhi <-> visiting <by> dobj
summer <-> Delhi <by> nmod
conference <-> visiting <by> nmod
Google <-> conference <by> nmodNamed entities can be obtained by identifying the NNP (proper noun) part of speech tag of the root node. Example  Trump, Delhi, and Google have the part of speech tag NNP. To generate the proper noun phrase linked with root node, one needs to parse the subtree linked with the root nodes. Using the grammar rules, following named entities are obtained:<compound> <nsubj> : Donald Trump
<compound> <dsubj> : New Delhi
<nmod> : GoogleOther tasks such as phrase chunking and entity wise sentiment analysis can be performed using similar processes. For example, one sentence may contain multiple sentiments, contexts and entities and dictionary based models may not perform well. In the following sentence, there are two contexts:Sentence  His acting was good but the script was poorContext 1: His acting was good
Context 2: the script was poorBoth of the contexts have different sentiments, one is negative while other is positive. In the dependency tree of this sentence, there are two sub-trees which correspond to different contexts and can be used to extract them. The dependency tree of the sentence isThe subtrees can be extracted and evaluated individually from this dependency tree to compute the dependency tree. This paper from Stanford and this paper from Singapore University describes the efficient approaches to perform NER using dependencies.Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.The coreference processJohn telephoned Bill. He lost his laptop.Np -> noun phrases nodes
Hw -> headwords
Rl -> grammar relations
Lv -> level in the tree
Pos -> part of speech tag
Gen -> gender of part of speech tagUsing Named Entity Recognizer, identified named entities are Bill, JohnUsing gender api such as this, gender of named entities are Male entities.Features of sentences:3.1 map the tokens with same gender of pronoun and named entity3.2 map the tokens with same singularity / plurality3.3 map the tokens with same grammar relationsThis paper from Soochow University describes the use of dependency trees in coreference resolution.Another important task in which computational linguistics can help to obtain results with high relevance is the Question Answering which is treated as one of the hardest tasks involved with text data. Question answering systems based on computational linguistics uses the syntactic structures of the query questions and matches them with the responses having similar syntactic structures. The similar syntactic structures contribute the answer set to a particular question. For exampleQuestion: What is the capital of India?Answer: New Delhi is the capital of India-> capital-NN (root)
-> what-WP (nsubj)
-> is-VBZ (cop)
-> the-DT (det)
-> of-IN (prep)
-> India-NNP (pobj)-> capital-NN (root)
-> Delhi-NNP (nsubj)
-> New-NNP (nn)
-> is-VBZ (cop)
-> the-DT (det)
-> of-IN (prep)
-> India-NNP (pobj)Both the question and answer dependency trees have similar patterns and can be used to generate the answer responses to specific queries. This paper1 and paper2 describe the approaches to perform question answering using dependency trees.Note:If you wish to explore this field further, then have a look at our detailed video course on NLP.In this article, I discussed the field of computational linguistics and how grammar relations among the sentences can be used in different tasks related to text data. If you feel, there are any other resources, tasks related to dependency trees and computational linguistics that I have missed, please feel free to comment with your suggestions and feedback.",https://www.analyticsvidhya.com/blog/2017/12/introduction-computational-linguistics-dependency-trees/
Essentials of Deep Learning : Introduction to Long Short Term Memory,"Learn everything about Analytics|Introduction|Table of Contents|1. Flashback: A look into Recurrent Neural Networks (RNN)
|2. Limitations of RNNs|3. Improvement over RNN: LSTM (Long Short-Term Memory) Networks|4. Architecture of LSTMs|5. Text generation using LSTMs|End Notes","4.1 Forget Gate|4.2 Input Gate|4.3 Output Gate|Importing dependencies |Loading text file and creating character to integer mappings|Preparing dataset|Reshaping of X|Defining the LSTM model|Fitting the model and generating characters|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Introduction to Computational Linguistics and Dependency Trees in data science|Fundamentals of Deep Learning  Introduction to Recurrent Neural Networks|
Pranjal Srivastava
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Sequence prediction problems have been around for a long time. They are considered as one of the hardest problems to solve in the data science industry. These include a wide range of problems; from predicting sales to finding patterns in stock markets data, from understanding movie plots to recognizing your way of speech, from language translations to predicting your next word on your iPhones keyboard. With the recent breakthroughs that have been happening in data science, it is found that for almost all of these sequence prediction problems, Long short Term Memory networks, a.k.a LSTMs have been observed as the most effective solution.
LSTMs have an edge over conventional feed-forward neural networks and RNN in many ways. This is because of their property of selectively remembering patterns for long durations of time. The purpose of this article is to explain LSTM and enable you to use it in real life problems. Lets have a look!Note: To go through the article, you must have basic knowledge of neural networks and how Keras (a deep learning library) works. You can refer the mentioned articles to understand these concepts:Take an example of sequential data, which can be the stock markets data for a particular stock. A simple machine learning model or an Artificial Neural Network may learn to predict the stock prices based on a number of features: the volume of the stock, the opening value etc. While the price of the stock depends on these features, it is also largely dependent on the stock values in the previous days. In fact for a trader, these values in the previous days (or the trend) is one major deciding factor for predictions.In the conventional feed-forward neural networks, all test cases are considered to be independent. That is when fitting the model for a particular day, there is no consideration for the stock prices on the previous days.This dependency on time is achieved via Recurrent Neural Networks. A typical RNN looks like:This may be intimidating at first sight, but once unfolded, it looks a lot simpler:Now it is easier for us to visualize how these networks are considering the trend of stock prices, before predicting the stock prices for today. Here every prediction at time t (h_t) is dependent on all previous predictions and the information learned from them. RNNs can solve our purpose of sequence handling to a great extent but not entirely. We want our computers to be good enough to write Shakespearean sonnets. Now RNNs are greatwhen it comes to short contexts, but in order to be able to build a story and remember it, we need our models to be able to understand and remember the context behind the sequences, just like a human brain. This is not possible with a simple RNN.Why? Lets have a look.Recurrent Neural Networks work just fine when we are dealing with short-term dependencies. That is when applied to problems like:RNNs turn out to be quite effective. This is because this problem has nothing to do with the context of the statement. The RNN need not remember what was said before this, or what was its meaning, all they need to know is that in most cases the sky is blue. Thus the prediction would be:However, vanilla RNNs fail to understand the context behind an input. Something that was said long before, cannot be recalled when making predictions in the present. Lets understand this as an example:Here, we can understand that since the author has worked in Spain for 20 years, it is very likely that he may possess a good command over Spanish. But, to make a proper prediction, the RNN needs to remember this context. The relevant information may be separated from the point where it is needed, by a huge load of irrelevant data. This is where a Recurrent Neural Network fails!The reason behind this is the problem of Vanishing Gradient. In order to understand this, youll need to have some knowledge about how a feed-forward neural network learns. We know that for a conventional feed-forward neural network, the weight updating that is applied on a particular layer is a multiple of the learning rate, the error term from the previous layer and the input to that layer. Thus, the error term for a particular layer is somewhere a product of all previous layers errors. When dealing with activation functions like the sigmoid function, the small values of its derivatives (occurring in the error function) gets multiplied multiple times as we move towards the starting layers. As a result of this, the gradient almost vanishes as we move towards the starting layers, and it becomes difficult to train these layers.A similar case is observed in Recurrent Neural Networks. RNN remembers things for just small durations of time, i.e. if we need the information after a small time it may be reproducible, but once a lot of words are fed in, this information gets lost somewhere. This issue can be resolved by applying a slightly tweaked version of RNNs  the Long Short-Term Memory Networks. When we arrange our calendar for the day, we prioritize our appointments right? If in case we need to make some space for anything important we know which meeting could be canceled to accommodate a possible meeting. Turns out that an RNN doesnt do so. In order to add a new information, it transforms the existing information completely by applying a function. Because of this, the entire information is modified, on the whole, i. e. there is no consideration for important information and not so important information. LSTMs on the other hand, make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. This way, LSTMs can selectively remember or forget things. The information at a particular cell state has three different dependencies. Well visualize this with an example. Lets take the example of predicting stock prices for a particular stock. The stock price of today will depend upon: These dependencies can be generalized to any problem as:Another important feature of LSTM is its analogy with conveyor belts! Thats right!Industries use them to move products around for different processes. LSTMs use this mechanism to move information around. We may have some addition, modification or removal of information as it flows through the different layers, just like a product may be molded, painted or packed while it is on a conveyor belt. The following diagram explains the close relationship of LSTMs and conveyor belts.Source
Although this diagram is not even close to the actual architecture of an LSTM, it solves our purpose for now.Just because of this property of LSTMs, where they do not manipulate the entire information but rather modify them slightly, they are able to forget and remember things selectively. How do they do so, is what we are going to learn in the next section?The functioning of LSTM can be visualized by understanding the functioning of a news channels team covering a murder story. Now, a news story is built around facts, evidence and statements of many people. Whenever a new event occurs you take either of the three steps.Lets say, we were assuming that the murder was done by poisoning the victim, but the autopsy report that just came in said that the cause of death was an impact on the head. Being a part of this news team what do you do? You immediately forget the previous cause of deathand all stories that were woven around this fact.What, if an entirely new suspect is introduced into the picture. A person who had grudges with the victim and could be the murderer? You input this information into your news feed, right?Now all these broken pieces of information cannot be served on mainstream media. So, after a certain time interval, you need to summarize this information and output the relevant things to your audience. Maybe in the form of XYZ turns out to be the prime suspect..Now lets get into the details of the architecture of LSTM network:SourceNow, this is nowhere close to the simplified version which we saw before, but let me walk you through it. A typical LSTM network is comprised of different memory blocks called cells
(the rectangles that we see in the image). There are two states that are being transferred to the next cell; the cell state and the hidden state. The memory blocks are responsible for remembering things and manipulations to this memory is done through three major mechanisms, called gates. Each of them is being discussed below.Taking the example of a text prediction problem. Lets assume an LSTM is fed in, the following sentence:As soon as the first full stop after person is encountered, the forget gate realizes that there may be a change of context in the next sentence. As a result of this, the subject of the sentence is forgotten and the place for the subject is vacated. And when we start speaking about Danthis position of the subject is allocated to Dan. This process of forgetting the subject is brought about by the forget gate.
A forget gate is responsible for removing information from the cell state. The information that is no longer required for the LSTM to understand things or the information that is of less importance is removed via multiplication of a filter. This is required for optimizing the performance of the LSTM network. This gate takes in two inputs; h_t-1 and x_t.h_t-1 is the hidden state from the previous cell or the output of the previous cell and x_t is the input at that particular time step. The given inputs are multiplied by the weight matrices and a bias is added. Following this, the sigmoid function is applied to this value. The sigmoid function outputs a vector, with values ranging from 0 to 1, corresponding to each number in the cell state. Basically, the sigmoid function is responsible for deciding which values to keep and which to discard. If a 0 is output for a particular value in the cell state, it means that the forget gate wants the cell state to forget that piece of information completely. Similarly, a 1 means that the forget gate wants to remember that entire piece of information. This vector output from the sigmoid function is multiplied to the cell state. Okay, lets take another example where the LSTM is analyzing a sentence:Now the important information here is that Bob knows swimming and that he has served the Navy for four years. This can be added to the cell state, however, the fact that he told all this over the phone is a less important fact and can be ignored. This process of adding some new information can be done via the input gate.Here is its structure:The input gate is responsible for the addition of information to the cell state. This addition of information is basically three-stepprocess as seen from the diagram above.Once this three-step process is done with, we ensure that only that information is added to the cell state that is important and is not redundant. Not all information that runs along the cell state, is fit for being output at a certain time. Well visualize this with an example:In this phrase, there could be a number of options for the empty space. But we know that the current input of brave, is an adjective that is used to describe a noun. Thus, whatever word follows, has a strong tendency of being a noun. And thus, Bob could be an apt output.This job of selecting useful information from the current cell state and showing it out as an output is done via the output gate. Here is its structure:The functioning of an output gate can again be broken down to three steps:The filter in the above example will make sure that it diminishes all other values but Bob. Thus the filter needs to be built on the input and hidden state values and be applied on the cell state vector.We have had enough of theoretical concepts and functioning of LSTMs. Now we would be trying to build a model that can predict some n number of characters after the original text of Macbeth. Most of the classical texts are no longer protected under copyright and can be found here. An updated version of the .txt file can be found here.We will use the library Keras, which is a high-level API for neural networks and works on top of TensorFlow or Theano. So make sure that before diving into this code you have Keras installed and functional. Okay, so lets generate some text!We import all the required dependencies and this is pretty much self-explanatory. The text file is open, and all characters are converted to lowercase letters. In order to facilitate the following steps, we would be mapping each character to a respective number. This is done to make the computation part of the LSTM easier.Data is prepared in a format such that if we want the LSTM to predict the O in HELLO we would feed in [H, E , L  , L ] as the input and [O] as the expected output. Similarly, here we fix the length of the sequence that we want (set to 50 in the example) and then save the encodings of the first 49 characters in X and the expected output i.e. the 50th character in Y.A LSTM network expects the input to be in the form [samples, time steps, features] where samples is the number of data points we have, time steps is the number of time-dependent steps that are there in a single data point, features refers to the number of variables we have for the corresponding true value in Y. We then scale the values in X_modified between 0 to 1 and one hot encode our true values in Y_modified.A sequential model which is a linear stack of layers is used. The first layer is an LSTM layer with 300 memory units and it returns sequences. This is done to ensure that the next LSTM layer receives sequences and not just randomly scattered data. A dropout layer is applied after each LSTM layer to avoid overfitting of the model. Finally, we have the last layer as a fully connected layer with a softmax activation and neurons equal to the number of unique characters, because we need to output one hot encoded result. The model is fit over 100 epochs, with a batch size of 30. We then fix a random seed (for easy reproducibility) and start generating characters. The prediction from the model gives out the character encoding of the predicted character, it is then decoded back to the character value and appended to the pattern. This is how the output of the network would look likeEventually, after enough training epochs, it will give better and better results over the time. This is how you would use LSTM to solve a sequence prediction task.LSTMs are a very promising solution to sequence and time series related problems. However, the one disadvantage that I find about them, is the difficulty in training them. A lot of time and system resources go into training even a simple model. But that is just a hardware constraint! I hope I was successful in giving you a basic understanding of these networks. For any problems or issues related to the blog, please feel free to comment below.",https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
Fundamentals of Deep Learning  Introduction to Recurrent Neural Networks,Learn everything about Analytics|Introduction|Table of Contents|Need for a Neural Network dealing with Sequences|What are Recurrent Neural Networks?|Understanding a Recurrent Neuron in Detail|Forward Propagation in a Recurrent Neuron in Excel|Back propagation in a Recurrent Neural Network(BPTT)|Implementation of Recurrent Neural Networks in Keras|Vanishing and Exploding Gradient Problem|Other RNN architectures|End Notes,"Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Essentials of Deep Learning : Introduction to Long Short Term Memory|Introduction to Altair  A Declarative Visualization Library in Python|
Dishashree Gupta
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let me open this article with a question  working love learning we on deep, did this make any sense to you? Not really  read this one  We love working on deep learning. Made perfect sense! A little jumble in the words made the sentence incoherent. Well, can we expect a neural network to make sense out of it? Not really! If the human brain was confused on what it meant I am sure a neural network is going to have a tough time deciphering such text.There are multiple such tasks in everyday life which get completely disrupted when their sequence is disturbed. For instance, language as we saw earlier- the sequence of words define their meaning, a time series data  where time defines the occurrence of events, the data of a genome sequence- where every sequence has a different meaning. There are multiple such cases wherein the sequence of information determines the event itself. If we are trying to use such data for any reasonable output, we need a network which has access to some prior knowledge about the data to completely understand it. Recurrent neural networks thus come into play.In this article I would assume that you have a basic understanding of neural networks, in case you need a refresher please go through this article before you proceed.Before we deep dive into the details of what a recurrent neural network is, lets ponder a bit on if we really need a network specially for dealing with sequences in information. Also what are kind of tasks that we can achieve using such networks.The beauty of recurrent neural networks lies in their diversity of application. When we are dealing with RNNs they have a great ability to deal with various input and output types.So RNNs can be used for mapping inputs to outputs of varying types, lengths and are fairly generalized in their application. Looking at their applications, lets see how the architecture of an RNN looks like.Lets say the task is to predict the next word in a sentence. Lets try accomplishing it using an MLP. So what happens in an MLP. In the simplest form, we have an input layer, a hidden layer and an output layer. The input layer receives the input, the hidden layer activations are applied and then we finally receive the output.Lets have a deeper network, where multiple hidden layers are present. So here, the input layer receives the input, the first hidden layer activations are applied and then these activations are sent to the next hidden layer, and successive activations through the layers to produce the output. Each hidden layer is characterized by its own weights and biases.Since each hidden layer has its own weights and activations, they behave independently. Now the objective is to identify the relationship between successive inputs. Can we supply the inputs to hidden layers? Yes we can!Here, the weights and bias of these hidden layers are different. And hence each of these layers behave independently and cannot be combined together. To combine these hidden layers together, we shall have the same weights and bias for these hidden layers.We can now combines these layers together, that the weights and bias of all the hidden layers is the same. All these hidden layers can be rolled in together in a single recurrent layer.So its like supplying the input to the hidden layer. At all the time steps weights of the recurrent neuron would be the same since its a single neuron now. So a recurrent neuron stores the state of a previous input and combines with the current input thereby preserving some relationship of the current input with the previous input.Lets take a simple task at first. Lets take a character level RNN where we have a word Hello. So we provide the first 4 letters i.e. h,e,l,l and ask the network to predict the last letter i.e.o. So here the vocabulary of the task is just 4 letters {h,e,l,o}. In real case scenarios involving natural language processing, the vocabularies include the words in entire wikipedia database, or all the words in a language. Here for simplicity we have taken a very small set of vocabulary.Lets see how the above structure be used to predict the fifth letter in the word hello. In the above structure, the blue RNN block, applies something called as a recurrence formula to the input vector and also its previous state. In this case, the letter h has nothing preceding it, lets take the letter e. So at the time the letter e is supplied to the network, a recurrence formula is applied to the letter e and the previous state which is the letter h. These are known as various time steps of the input. So if at time t, the input is e, at time t-1, the input was h. The recurrence formula is applied to e and h both. and we get a new state.The formula for the current state can be written as Here, Ht is the new state, ht-1 is the previous state while xt is the current input. We now have a state of the previous input instead of the input itself, because the input neuron would have applied the transformations on our previous input. So each successive input is called as a time step.In this case we have four inputs to be given to the network, during a recurrence formula, the same function and the same weights are applied to the network at each time step.Taking the simplest form of a recurrent neural network, lets say that the activation function is tanh, the weight at the recurrent neuron is Whh and the weight at the input neuron is Wxh, we can write the equation for the state at time t as The Recurrent neuron in this case is just taking the immediate previous state into consideration. For longer sequences the equation can involve multiple such states. Once the final state is calculated we can go on to produce the outputNow, once the current state is calculated we can calculate the output state as-Let me summarize the steps in a recurrent neuron for you-Lets take a look of how we can calculate these states in Excel and get the output.Lets take a look at the inputs first The inputs are one hot encoded. Our entire vocabulary is {h,e,l,o} and hence we can easily one hot encode the inputs.Now the input neuron would transform the input to the hidden state using the weight wxh. We have randomly initialized the weights as a 3*4 matrix Step 1:Now for the letter h, for the the hidden state we would need Wxh*Xt. By matrix multiplication, we get it as Step 2:Now moving to the recurrent neuron, we have Whh as the weight which is a 1*1 matrix asand the bias which is also a 1*1 matrix asFor the letter h, the previous state is [0,0,0] since there is no letter prior to it.So to calculate -> (whh*ht-1+bias)Step 3:Now we can get the current state as Since for h, there is no previous hidden state we apply the tanh function to this output and get the current state Step 4:Now we go on to the next state. e is now supplied to the network. The processed output of ht, now becomes ht-1, while the one hot encoded e, is xt. Lets now calculate the current state ht.Whh*ht-1 +bias will be Wxh*xt will be Step 5:Now calculating ht for the letter e,Now this would become ht-1 for the next state and the recurrent neuron would use this along with the new character to predict the next one.Step 6:At each state, the recurrent neural network would produce the output as well. Lets calculate yt for the letter e.Step 7:The probability for a particular letter from the vocabulary can be calculated by applying the softmax function. so we shall have softmax(yt)If we convert these probabilities to understand the prediction, we see that the model says that the letter after e should be h, since the highest probability is for the letter h. Does this mean we have done something wrong? No, so here we have hardly trained the network. We have just shown it two letters. So it pretty much hasnt learnt anything yet.Now the next BIG question that faces us is how does Back propagation work in case of a Recurrent Neural Network. How are the weights updated while there is a feedback loop?To imagine how weights would be updated in case of a recurrent neural network, might be a bit of a challenge. So to understand and visualize the back propagation, lets unroll the network at all the time steps. In an RNN we may or may not have outputs at each time step.In case of a forward propagation, the inputs enter and move forward at each time step. In case of a backward propagation in this case, we are figuratively going back in time to change the weights, hence we call it the Back propagation through time(BPTT).In case of an RNN, if yt is the predicted valuet is the actual value, the error is calculated as a cross entropy loss Et(t,yt) = t log(yt)E(,y) =  t log(yt)We typically treat the full sequence (word) as one training example, so the total error is just the sum of the errors at each time step (character). The weights as we can see are the same at each time step. Lets summarize the steps for backpropagationThe unrolled network looks much like a regular neural network. And the back propagation algorithm is similar to a regular neural network, just that we combine the gradients of the error for all time steps. Now what do you think might happen, if there are 100s of time steps. This would basically take really long for the network to converge since after unrolling the network becomes really huge.In case you do not wish to deep dive into the math of backpropagation, all you need to understand is that back propagation through time works similar as it does in a regular neural network once you unroll the recurrent neuron in your network. However, I shall be coming up with a detailed article on Recurrent Neural networks with scratch with would have the detailed mathematics of the backpropagation algorithm in a recurrent neural network.Lets use Recurrent Neural networks to predict the sentiment of various tweets. We would like to predict the tweets as positive or negative. You can download the dataset here.We have around 1600000 tweets to train our network. If youre not familiar with the basics of NLP, I would strongly urge you to go through this article. We also have another detailed article on word embedding which would also be helpful for you to understand word embeddings in detail.Lets now use RNNs to classify various tweets as positive or negative.If you would run this model, it may not provide you with the best results since this is an extremely simple architecture and quite a shallow network. I would strongly urge you to play with the architecture of the network to obtain better results. Also, there are multiple approaches to how to preprocess your data. Preprocessing shall completely depend on the task at hand.RNNs work upon the fact that the result of an information is dependent on its previous state or previous n time steps. Regular RNNs might have a difficulty in learning long range dependencies. For instance if we have a sentence like The man who ate my pizza has purple hair. In this case, the description purple hair is for the man and not the pizza. So this is a long dependency.If we backpropagate the error in this case, we would need to apply the chain rule. To calculate the error after the third time step with respect to the first one E/W =E/y3 *y3/h3 *h3/y2 *y2/h1 .. and there is a long dependency.Here we apply the chain rule and if any one of the gradients approached 0, all the gradients would rush to zero exponentially fast due to the multiplication. Such states would no longer help the network to learn anything. This is known as the vanishing gradient problem.Vanishing gradient problem is far more threatening as compared to the exploding gradient problem, where the gradients become very very large due to a single or multiple gradient values becoming very high.The reason why Vanishing gradient problem is more concerning is that an exploding gradient problem can be easily solved by clipping the gradients at a predefined threshold value. Fortunately there are ways to handle vanishing gradient problem as well. There are architectures like the LSTM(Long Short term memory) and the GRU(Gated Recurrent Units) which can be used to deal with the vanishing gradient problem.As we saw, RNNs suffer from vanishing gradient problems when we ask them to handle long term dependencies. They also become severely difficult to train as the number of parameters become extremely large. If we unroll the network, it becomes so huge that its convergence is a challenge.Long Short Term Memory networks  usually called LSTMs  are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber. They work tremendously well on a large variety of problems, and are now widely used.LSTMs also have this chain like structure, but the repeating module has a slightly different structure. Instead of having a single neural network layer, there are multiple layers, interacting in a very special way. They have an input gate, a forget gate and an output gate. We shall be coming up with detailed article on LSTMs soon.Another efficient RNN architecture is the Gated Recurrent Units i.e. the GRUs. They are a variant of LSTMs but are simpler in their structure and are easier to train.Their success is primarily due to the gating network signalsthat control how the present input and previous memory areused, to update the current activation and produce the currentstate. These gates have their own sets of weights that areadaptively updated in the learning phase. We have just two gates here, the reset an the update gate. Stay tuned for more detailed articles on GRUs.I hope that this article would have given you a head start with the Recurrent Neural Networks. In the upcoming articles we shall deep dive into the complex mathematics of Recurrent Neural Networks along with the detailed descriptions of LSTMs and GRUs.Try playing with the architecture of these RNNs and be amazed by their performance and applications. Do share your findings and approach in the comments section.",https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
Introduction to Altair  A Declarative Visualization Library in Python,Learn everything about Analytics|Introduction|Table of Contents|1. Overview of Altair|2. Exploring a Real World Problem|3. Pros and Cons|4. End Notes,"1.1 What is Altair?|1.2 Installation|2.1 Understanding the dataset|2.2 Histograms|2.3 Scatterplot|2.4 Stacked Bar Plot|2.5 Line Chart|2.6 Heatmap|Share this:|Like this:|Related Articles|Fundamentals of Deep Learning  Introduction to Recurrent Neural Networks|Heart Sound Segmentation using Deep Learning  A doctor in making?|
Shubham Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Visualization is one of the most exciting parts of data science. Plotting huge amounts of data to unveil underlying relationships has its own fun.Whether youre identifying relationships between features or simply understanding the working of a model, visualizations are usually the best way to go about it. Visualizations also help explain your work to your customers and stakeholders.Python provides a lot of libraries, specifically for plotting and visualization and I usually have a tough time picking out which one to use for my problem statement.I recently come across with Altair, a visualization library in Python and I was amazed by its capabilities. It is a very user-friendly library which actually performs a lot of things with the minimal amount of code.Please note that Altair is still in development phase, so things might change over time. We can still do a lot of exciting work on it and the future potential really excites me  hence this article!So, lets get started.Altair is a declarative statistical visualization library in Python, based on  Vega-lite  .I am sure many of you would be asking these two questions by now:By declarative, we mean that while plotting any chart, you only need to declare links between data columns to the encoding channels, such as x-axis, y-axis, colour, etc. and rest all of the plot details are handled automatically. Lets understand it by an example.Take a look at the plot below and above that is the code required for generating that plot in Altair.So, if you notice, we just mentioned x,y and color and rest of the things like legend, axis labels, range etc. all are automatically set.Now, take a look at the image below which is the matplotlib implementation of the same visualization we previously did with Altair.Here, we need to explicitly use groupby function, define axis names, legends etc. which becomes a lot of extra work while you are doing an exploratory analysis.Therefore being declarative makes Altair simple, friendly and consistent. It produces beautiful and effective visualizations with the minimal amount of code. Therefore you can spend more time understanding your data rather than spending your time on setting the legends, defining axes and so on.I suppose by now you must have got the answer to the second question also.Altair can be installed via conda as follows:Or we can also install it via pip with the following:Great, now you have Altair installed on your system. I always feel that the best way to learn a library is to practice it on a real-life problem. So, without any further delay, let us quickly begin!For this purpose,I am using The Big Mart Salesdataset. Download the training file and load it into your working environment.In the data set, we have product wise Sales for Multiple outlets of a chain. Lets take a look at the dataset.The total data frame consists of 12 columns, out of which 7 are categorical and rest are numerical. Here, we have Item_Outlet_Salesas the target feature.NOTE: I have only used the first 1000 rows, just to provide you with the sense of different plots using Altair.During the exploration journey, we generally start with the univariate analysis using histograms.So, lets start with theItem_Outlet_Sales feature, by plotting its histogram using Altair.To understand the code, lets understand some basic terminologies, to begin with.Here, we have shown bar mark in order to visualization our data points as a bar chart.Above defined are the common parameters which are used in every Altair visualization. Lets take a look at some specific attributes which are used in the above code.That was seriously very difficult to grasp after reading it for the first time. But, surely we will get hands-on it by the end of this article.Okay, now if you look at the above chart, the label for x-axis is not defined properly by the Altair, and we wish to change it. You can do this by defining another parameter called axis.It comes out to be a right-skewed distribution for the sales in our dataset. If I wish to do the same visualization using matplotlib, the implementation is shown below.You may consider the above code simple, but the visualization obtained from Altair is far more appealing than the one obtained from matplotlib.We can also change the color of the histogram in Altair by simply adding a color attribute.Now, lets try to do some bivariate analysis using Altair.Now, let us look at the relationship between MRP and Sales features.We can clearly see a linear relationship between MRP and sales in the above plot. Lets introduce some other encoding of Altair in order to draw more inferences from the visualizations.Here, we have defined another encoding called color,which is used to differentiate the data points and helps us to understand the relationship better.We can see that the sales for the grocery stores are pretty low as compared to sales in supermarkets, where supermarket type3 showing the highest sales.To draw more information from the above plot, lets add another encoding called row.So, we can now notice that items in supermarket type 1 have comparatively more weight than the grocery store and supermarket type 2.These were different variates of scatter plot that can be drawn using the Altair. Lets now look at some barplot charts to do some bivariate analysis.For this, lets look at the relationship between Outlet_location_type and size of the outlet.Above drawn is the stacked bar plot, which denotes that theoutlets in tier 2 cities only have outlets of small size, while only tier 3 cities have outlets which are high in size.Notice that here we have defined the count aggregator in the X encoding, in order to produce a horizontal stack chart.Now, let us look at the price of the item across the years. Generally, for the time series related visualizations, we prefer the use of line charts which is provided by Altair by using the line mark.Altair supports date and discretization of dates when setting the type of it as temporal (T).Notice: Here, we have separately defined our transformation using the aggregate attribute.Altair provides another type of chart known as heat map, which uses text mark attribute.Similarly, you can draw more patterns in data by this and you can also try some other data transformations and plots to find various trends and relationships between features.Note that, Altair is currently in the development phase and sooner it will include more and better visualizations in the coming time.Pros:Cons:I hope that you had a fun time learning this new library where you can do a lot of things with the minimal amount of code. So, try using it yourself on the dataset you are working on, and share your experience or doubts in the comment section below.I would also recommend you to once the visit the gallery page of the Altair documentation for getting more idea about the type of visualizations possible using Altair.Happy Exploring!",https://www.analyticsvidhya.com/blog/2017/12/introduction-to-altair-a-declarative-visualization-in-python/
Heart Sound Segmentation using Deep Learning  A doctor in making?,Learn everything about Analytics|Introduction|Table of Contents|What is a Segmentation problem (in general)?|Supervised Segmentation approach|Understanding our problem  What do you mean by Heart Sound?|Implementation of Heart Sound Segmentation|End Notes,"Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Introduction to Altair  A Declarative Visualization Library in Python|Introductory guide to Information Retrieval using kNN and KDTree|
Faizan Shaikh
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The idea of doing a project on heart sound segmentation came from a recent breakthrough I heard over the internet. One of the influencers I follow  Andrew Ng published a research paper a while back  which essentially is a state-of-the-art method for detecting heart disease.Heart disease? ML diagnoses from ECG better than cardiologist! @awnihannun @pranavrajpurkar @iRhythmTech @willknight https://t.co/bZAm8Y5Z09 Andrew Ng (@AndrewYNg) July 7, 2017It was an intriguing idea for me, so I went through all the materials published on the subject to understand what was the original idea behind.To keep the story short, the authors said that they used deep learning  a technique which has been in the news for a while now, for extracting the patterns that experts used to identify a diseased patient. This algorithm after training, became so good at the task that the authors claim to surpass even seasoned doctors. This idea influenced me that even I  albeit small  could have an impact on the substantial advancements that these researchers are having!This article focuses on audio segmentation problem in ECG signals and how we leverage deep learning to solve the task. I will first discuss a bit about segmentation problem in general and then show you the ways that can be used to solve the problem. I will also discuss what heart sound is and then show you an implementation of heart sound segmentation.So lets get on with it!Note: This article assumes that you have a basic knowledge of audio data analysis. If you want to brush up the concepts  you can go through the articleBefore we dive into heart sound segmentation, let us go back and understand what segmentation problem entails. Segmentation literally means dividing a particular object into parts (or segments) based on a defined set of characteristics. This object can be anything  ranging from concrete things like a frame of an image or an audio signal, to abstract objects like market or consumers.You may ask, why would you segment objects? The answer is simple  if you break down an object, it becomes an easier task extract information from it. For example in Customer Management, working with averages never reveals actionable insights until broken down in segments. As mentioned in the article, this is an example of customer segmentation of credit card usage on the basis of their age.Now that you know Segmentation as a problem, let us understand the approaches to solve a segmentation problem.Segmentation, specially for audio data analysis, is an important pre-processing step. This is because you can segment a noisy and lengthy audio signal into short homogeneous segments, which are handy short sequences of audio used for further processing. Now to solve a segmentation problem, you can either do it directly using unsupervised methods or convert it into a supervised problem and then group it according to its class.To explain this more intuitively, lets take an example of Image Segmentation task.Suppose you have an image of a cat in a field as we can see below. What you want is to divide the image into chunks  so that one individual object can be separately identified from the other. You can do this in two waysAlthough both the approaches has its pros and cons, the decision to start out with either of the approach will depend upon how hard it is to get training examples to go on with the supervised approach.Without wasting any time, let us jump on to what our actual problem is and try to solve it. Quoting the challenge page itself,According to the World Health Organisation, cardiovascular diseases (CVDs) are the number one cause of death globally: more people die annually from CVDs than from any other cause. An estimated 17.1 million people died from CVDs in 2004, representing 29% of all global deaths. Of these deaths, an estimated 7.2 million were due to coronary heart disease. Any method which can help to detect signs of heart disease could therefore have a significant impact on world health. This challenge is to produce methods to do exactly that.The task in the challenge is to find a method that can locate sounds particular to a heart (aka lub & dub, which are technically called S1 and S2) within audio data and then segment the audio files on the basis of these sounds. After segmenting the sounds, the challenge then asks us to produce a method that can classify heartbeat into normal and diseased categories. For the purpose of this article, we will take up only the first task of the challenge, i.e. to segment heart audio.
To give you a practical glimpse, this is how the heart sounds likeA normal heart sound has a clear lub dub, lub dub pattern, with the time from lub to dub shorter than the time from dub to the next lub (when the heart rate is less than 140 beats per minute). A temporal description of lub and dub locations over time in the following illustration:
The very basic step you need to do whenever you start up on a problem is to understand the data and go through it record by record. Let us start with this:Note: you can download the required dataset from this webpage. Only download Dataset A of challenge 1 (Atraining_normal.zip and Atraining_normal_seg.csv)We see that there are cycles of heartbeat, with a higher intensity sound followed by a lower intensity sound.from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPool1D
from keras.models import Sequentialmodel = Sequential()model.add(InputLayer(input_shape=data_x.shape[1:]))model.add(Conv1D(filters=50, kernel_size=10, activation=relu))
model.add(MaxPool1D(strides=8))
model.add(Conv1D(filters=50, kernel_size=10, activation=relu))
model.add(MaxPool1D(strides=8))
model.add(Flatten())
model.add(Dense(units=1, activation=softmax))model.compile(optimizer=adam, loss=binary_crossentropy, metrics=[accuracy])Our model will have this type of architectureWe are restricting the training for only 1 epoch here. But you can increase this to make your model perform better.And voila! You have a trained model which can be used to perform segmentation task. Now to get the durations where you should segment a heartbeat, just divide your raw test file into multiple parts and get the top prediction out of it. The model would give a prediction like thisI hope this article gave you a glimpse of how advancements in audio analysis can help us creating amazing technologies that can change our lives. The possibilities it opens up for humans can be huge.I have specially included an implementation of the technique so that you can use it to try it out locally. If you find the article helpful or have any suggestions, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/11/heart-sound-segmentation-deep-learning/
Introductory guide to Information Retrieval using kNN and KDTree,Learn everything about Analytics|Introduction|Table of Contents|What is Information Retrieval?|Where is Information Retrieval used?|KNN for Information Retrieval|Improvement over KNN: KD Trees for Information Retrieval||Intuitive Explanation of KD Trees|Implementation of KD Trees||Advantages and Disadvantages of KD trees|End Notes,"Use Case 1: Digital Library|Use Case 2: Search Engine|Use Case 3: Image retrieval|Advantages of KD Trees|Disadvantages of KD Tree|Share this:|Like this:|Related Articles|Heart Sound Segmentation using Deep Learning  A doctor in making?|FlashText  A library faster than Regular Expressions for NLP tasks|
Gurchetan Singh
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love cricket as much as I love data science. A few years back (on 16 November 2013 to be precise), my favorite cricketer  Sachin Tendulkar retired from International Cricket. I spent that entire day reading articles and blogs about him on the web.By the end of the day, I had read close to 50 articles about him. Interestingly, while I was reading these articles  none of the websites suggested me articles outside ofSachin or cricket. Was it a co-incidence? Surely not.I was being suggested the next article based on what I was currently reading.The technique behind this process is known as Information Retrieval.In this article, I would take you through the basics of Information Retrieval and two common algorithms used to implement it, KNN and KD Tree. By end of this article, you will be able to create your own information retrieval systems, which can be implemented in any digital library / search.Lets get going!In the past few decades, the availability of cheap and effective storage devices and information systems has prompted the rapid growth of graphical and textual databases. Information collection and storage efforts have become easier, but the effort required to retrieve relevant information has become significantly greater, especially in large-scale databases. This situation is criticalfor textual databases, with so much of textual information around us  for instancebusiness applications (e.g., manuals, newsletters, and electronic data interchanges), scientific applications (e.g., electronic community systems and scientific databases) etc.SourceTo aid the users to access these databases and extract the relevant knowledge or documents, Information Retrieval is used.Information Retrieval (IR) is the process by which a collection of data is represented, stored, and searched for the purpose of knowledge discovery as a response to a user request (query). This process involves various stages initiating with representing data and ending with returning relevant information to the user. The intermediate stage includes filtering, searching, matching and ranking operations. The main goal of information retrieval system (IRS) is to find the relevant information or a document that satisfies users information needsA digital library is a library in which collection of data are stored in digital formats and accessible by computers. The digital content may be stored locally, or accessed remotely via computer networks. A digital library is a type of information retrieval system.A search engine is one of the most the practical applications of information retrieval techniques to large scale text collections.An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images.Very famous example of image retrieval system is https://reverse.photos/ which uses image as the search query and returns similar images.SourceOne of the most common algorithms that most of the Data scientists use for retrieval of information is KNN. K Nearest Neighbour (KNN) is one of the simplest algorithms that calculates the distance between the query observation and each data point in the train dataset and finds the K closest observations.When we use Nearest neighbour search algorithm, it compares all the data points with the mentioned query point and finds the closest points.There are many ways in which we can find the distance between two data points. Most commonly used distance metrics are Euclidean distance and Hamming distance.This research paper focuses on them.Imagine a situation where you have thousands of queries, and every time the algorithm compares the query point with all the data points. Isnt it very computationally intensive?Also, greater the data points, higher will be the amount of computation needed. Obviously!SourceSo, KNN search has O(N) time complexity for each query where N= Number of data points. For KNN with K neighbor search, the time complexity will be O(log(K)*N) only if we maintain a priority queue to return the closest K observations. You can read more about KNN here.So, for a dataset with millions of rows and thousands of queries, KNN seems to be computationally very demanding. So is there any alternative to KNN which uses similar approach but can be time efficient also?KD Tree is one such algorithm which uses a mixture of Decision trees and KNN to calculate the nearest neighbour(s).KD-trees are a specific data structure for efficiently representing our data. In particular, KD-trees helps organize and partition the data points based on specific conditions.Lets say we have a data set with 2 input features. We can represent our data as-Now, were going to be making some axis aligned cuts, and maintaining lists of points that fall into each one of these different bins.  SourceAnd what this structure allows us to do as were going to show, is efficiently prune our search space so that we dont have to visit every single data point.Now the question arises of how to draw these cuts?Then a question is when do you stop?There are a couple of choices that we have.So again, this second criteria would ignore the actual data in the box whereas the first one uses facts about the data to drive the stocking criterion. We can use the same distance metrics(Euclidean distance and Hamming distance) that we used while implementing KNN.Suppose we have a data set with only two features.Lets split data into two groups.We do it by comparing x with mean of max and min value.Value = (Max + Min)/2= (0.96 + 0.11)/2= 0.53At each node we will save 3 things.Similarly dividing the structure into more parts on the basis of alternate dimensions until we get maximum 2 data points in a Node.So now we plotted the points and divided them into various groups.Lets say now we have a query point Q to which we have to find the nearest neighbor.Using the tree we made earlier, we traverse through it to find the correct Node.When using Node 3 to find the Nearest Neighbor.But we can easily see, that it is in fact not the Nearest neighbor to the Query point.We now traverse one level up, to Node 1.We do this because the nearest neighbor may not necessarily fall into the same node as the query point.Do we need to inspect all remaining data points in Node 1 ?We can check this by checking if the tightest box containing all the points of Node 4 is closer than the current near point or not.This time, the bounding box for Node 4 lies within the circle, indicating that Node 4 may contain a point thats closer to the query.When we calculate the distance of the points within the Node 4 and previous closest point with the query point, we find that point lying above the query point is actually the nearest neighbor within the given points.We now traverse one level up, to Root.Do we need to inspect all remaining data points in Node 2 ?We can check this by checking if the tightest box containing all the points of Node 2 is closer than the current Near point or not.We can see that the Tightest box is far from the current Nearest point. Hence, we can prune that part of the tree.Since weve traversed the whole tree, we are done: data point marked is indeed the true nearest neighbour of the query.For the implementation of KD Tree, we will use the most common form of IR ie Document Retrieval. Based on the current document, document retrieval returns the most similar document(s) to the user.We will use the dataset which consists of articles on famous personalities. We would implement KD Tree to help us retrieve articles most similar to that of the Barack Obama.You can download the dataset in the form of csv from here.sourceHence we can see that articles of Bill Clinton and Donald Flower who share the same field of politics as Barack Obama are similar.So now we look at the advantages of KD Tree :Well, KD-trees are really cool. Theyre a very intuitive way to think about storing data, and as we saw, they could lead to help us find relevant information way sooner.But there are few issues.SourceKD Tree can prove to be a better Retrieval algorithm on a specific Dataset that matches its condition. Though there are more models such as Locality Sensitive Hashing which can overcome its limitations. We shall explore them as well in the upcoming articles.Did you find the article useful? Do you plan to use KD Tree or LSH in near future in Python or R? If yes, share with us how you plan to go about it.",https://www.analyticsvidhya.com/blog/2017/11/information-retrieval-using-kdtree/
FlashText  A library faster than Regular Expressions for NLP tasks|Introduction,Learn everything about Analytics|Table of Contents|1. What is FlashText?|2. How is FlashText so fast?|3. When do you use FlashText?|4. Installing FlashText|5. Searching for words in a text document|6. Replacing words in a text document|7. End Notes,"Learn,engage,compete,andget hired!|Share this:|Like this:|Related Articles|Introductory guide to Information Retrieval using kNN and KDTree|A Step Towards Reproducible Data Science : Docker for Data Science Workflows|
NSS
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People like me working in the field of Natural Language Processing almost always come across the task of replacing words in a text. The reasons behind replacing the words may be different. Some of them are.Now, if the number of words to replace and the corpus of text is not huge i.e within thousands, then Regular Expressions have always been my solution. But as I started working on bigger and bigger datasets with tens of thousands of documents and sometimes millions, I noticed that performing the above tasks started taking days. In todays fast-moving world, this is not the amount of time one would want to invest in a very simple but important task. So earlier, it would come down to optimizing the number of words necessary to be changed and time required to replace these words.But in the early November, I found FlashText  a super blazingly fast library that reduces days of replacement computation time to minutes.FlashText is a Python library created specifically for the purpose of searching and replacing words in a document. Now, the way FlashText works is that it requires a word or a list of words and a string. The words which FlashText calls keywords are then searched or replaced in the string.Let us check out in detail about FlashText working. When keywords are passed to FlashText for searching or replacing, they are stored as a Trie Data Structure which is very efficient at Retrieval tasks. Below is an example of how a Trie Data Structure looks like.Above is a Trie Data Structure for the words (their, there, answer, any, and bye).Now, in case of searching the keywords, FlashText will return the keywords that are present in the string. In case of replacing, FlashText will create a new string with the keywords replaced. Both these operations happen over a single pass. Now it is important to understand the concept of the single pass.An example of single pass replacement looks like this-String = spamham sha
Replace spam with eggs and sha with md5Now lets see how does the String look like with and without a single pass.Single-pass
String = eggsham md5Without Single-pass
String = eggmd5m md5Above, you can see the difference between the single pass and without the single pass.Now, we have an overview of how FlashText works. It is imperative that we understand  What is that FlashText has which Regular Expressions dont? After all, Regular Expressions are mostly considered the one-stop solution for string manipulation in terms of both speed and variety of manipulations that can be done. This can be better understood with the help of an example by the author of the FlashText package himself.Suppose, there is a string called sample =  This is a sample sentence and a collection of words called keywords = {sample, sameer, pony, time}. Now, if we are to perform searching of the keywords in the sample, there are 2 ways of doing it.Method 1:for each word in keywords: Line 1
check if word exists in sample     Line 2Now the above method has a loop which will run n times, where n is the number of words in the keywords. Also, there will be significant time consumption in Line 2 which checks whether a particular word is present in a string or not.Method 2:for each word in sample: Line 1
check if word exists in the keywords dictionary Line 2Now, the loop in this method will run m times, where m is the number of words in the sample. The major advantage is the execution time of Line 2. Checking a key in a dictionary is a significantly faster process than checking for a word in a string.FlashText uses Method 2 for faster searching and replacing and is inspired by the Aho-Corasick Algorithm and Trie Data Structure. Lets have a look at the way it works:First, a Trie dictionary is created from the keywords which looks like below.Start and EOT represent the word boundaries such as space, new_line etc. The idea is that words which have word boundaries on both ends should only match. So, pam will not match to pamella.  Now lets see how we can perform searching,Since we have our keywords dictionary with us. We pass in the input string sample. Now, the comparison will look like below.Is <start>This<EOT>in the keywords dictionary? No
Is <start>is<EOT>in the keywords dictionary? No
Is <start>a<EOT>in the keywords dictionary? No
Is <start>sample<EOT>in the keywords dictionary? Yes
Is <start>sentence<EOT>in the keywords dictionary? NoVisually it looks like below.An important thing to keep in mind is that checking for a word in the dictionary happens at a character level. So for example, while checking for the word is, it willsearch for <start>i. Since i does not exist anywhere after <start> in the Keywords trie dictionary, therefore the entire word is skipped. This character level matching and skipping are what gives FlashText its speed.Pretty much every time since it is super fast. Below is the image of the benchmarking done by FlashText for searching and replacing against regular expressions.SearchingLooking at the image, it looks like when the number of keywords to be searched is below 500 then, regular expressions provide a little edge over FlashText. But as soon as number of keywords cross 500, FlashText surpasses regular expression performance by a wide margin.Conclusion: Use FlashText when you have to search for a large number of keywords, preferably more than 500.ReplacingWhile Regular Expressions presented stiff competition to FlashText in the sub 500 keyword domain for searching, when it comes to replacing FlashText beats Regular Expressions hands down. While there is a linear increase in time as the number of keywords increase, FlashText stays constant.Conclusion: FlashText is any day better than regular expressions for replacing keywords in a document.These are the codes for benchmarking of searching and replacing by the author.There are still a few caveats of using FlashText. As of now, FlashText does not support partial word matching or special characters matching. For that, Regular Expressions are the best.Installing FlashText is as easy as any other package. In command prompt/ terminal, type:pip install flashtextBelow is the code to find keywords in a document.# Replacing all occurences of word 'batman'(case insensitive) with Bruce Wayne
processor = KeywordProcessor(case_sensitive = False)
processor.add_keywords('batman','Bruce Wayne')
found = processor.replace_keywords(document)
print(found)output:Bruce Wayne is a fictional superhero appearing in American comic books published by DC Comics. The character was created by artist Bob Kane and writer Bill Finger,[4][5] and first appeared in Detective Comics #27 (1939). Originally named the Bat-Man, the character is also referred to by such epithets as the Caped Crusader, the Dark Knight, and the World\s Greatest Detective.[6]\n\nBruce Wayne\s secret identity is Bruce Wayne, a wealthy American playboy, philanthropist, and owner of Wayne Enterprises. After witnessing the murder of his parents Dr. Thomas Wayne and Martha Wayne as a child, he swore vengeance against criminals, an oath tempered by a sense of justice. Bruce Wayne trains himself physically and intellectually and crafts a bat-inspired persona to fight crime.[7]\n\nBruce Wayne operates in the fictional Gotham City with assistance from various supporting characters, including his butler Alfred, police commissioner Gordon, and vigilante allies such as Robin. Unlike most superheroes, Bruce Wayne does not possess any superpowers; rather, he relies on his genius intellect, physical prowess, martial arts abilities, detective skills, science and technology, vast wealth, intimidation, and indomitable will. A large assortment of villains make up Bruce Wayne\s rogues gallery, including his archenemy, the Joker.\n\nThe character became popular soon after his introduction in 1939 and gained his own comic book title, Bruce Wayne, the following year. As the decades went on, differing interpretations of the character emerged. The late 1960s Bruce Wayne television series used a camp aesthetic, which continued to be associated with the character for years after the show ended. Various creators worked to return the character to his dark roots, culminating in 1986 with The Dark Knight Returns by Frank Miller. The success of Warner Bros.\ live-action Bruce Wayne feature films have helped maintain the character\s prominence in mainstream culture.[8]\n\nAn American cultural icon, Bruce Wayne has garnered enormous popularity and is among the most identifiable comic book characters. Bruce Wayne has been licensed and adapted into a variety of media, from radio to television and film, and appears on various merchandise sold around the world, such as toys and video games. The character has also intrigued psychiatrists, with many trying to understand his psyche. In 2015, FanSided ranked Bruce Wayne as number one on their list of 50 Greatest Super Heroes In Comic Book History.[9] Kevin Conroy, Bruce Greenwood, Peter Weller, Anthony Ruivivar, Jason O\Mara, and Will Arnett, among others, have provided the character\s voice for animated adaptations. Bruce Wayne has been depicted in both film and television by Lewis Wilson, Robert Lowery, Adam West, Michael Keaton, Val Kilmer, George Clooney, Christian Bale, anSo this was all about FlashText  an efficient library for searching and replacing of keywords in millions of document. If you are into the NLP field and it is your day to day job of dealing with this kind of problem of text cleaning and modification then, I would really suggest you try the library once. The source for the library is present here for any reference.If you try out this amazing library, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/11/flashtext-a-library-faster-than-regular-expressions/
A Step Towards Reproducible Data Science : Docker for Data Science Workflows,Learn everything about Analytics|Introduction|Table of Contents|1. What is Docker?|2. Use Cases for Data Science|3. Docker Terminology|4. Docker: Hello-World|5. Data Science tools without installation:|6. Your first Docker Image|7. Docker Eco-system|End Notes,"Distribution and setup ofData Sciencetools and software:|Sharing reproducible analysis and code viaDocker Images:|Sharing Data Science applications directly without a dedicated DevOps team:|About the Author|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|FlashText  A library faster than Regular Expressions for NLP tasks|2 days to go for the DataHack Summit|
Guest Blog
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"My first encounter with Docker was not to solve a Data Science problem, but to install MySQL. Yes, to install MySQL! Quite an anti-climatic start, right? At times, you stumble upon jewels while going through StackOverflow and Docker was one of them. What started with a one-off use case, ended up becoming a useful tool in my daily workflow.I got a taste of docker when I tried to install TensorFlow in my system. Just to give you the context, TensorFlow is a deep learning library which requires a series of steps that you ought to do for system setup. Especially it is extremely complex to install Nvidia Graphics drivers. I literally had to reinstall my Operating system countless number of times. That loop stopped only when I shifted to docker, thankfully!Docker provides you with an easy way to share your working environments including libraries and drivers. This enables us to create reproducible data science workflows.This article aims to provide the perfect starting point to nudge you to use Docker for your Data Science workflows! I will cover both the useful aspects of Docker  namely, setting up your system without installing the tools and creating your own data science environment.Docker is a software technology providing containers, promoted by the company Docker, Inc. Docker provides an additional layer of abstraction and automation of operating-system-level virtualization on Windows and Linux.The underlying concept that Docker promotes is usage of containers, which are essentially boxes of self-contained softwares. Containers have been in existence before Docker & quite successful, but 2015 saw a huge adoption by the software community in terms of containerization to solve the day-to-day issues.When you walk into a cubicle of Data Science folks, they are either doing data processing or struggling to setup something on their work-stations/laptops. Okay, that might be an exaggeration but you get the sense of helplessness. To give a small example, for someone to setup aCaffeenvironment there are more than30 unique ways. And trust me, youll end up creating a new blogpost just for showing all the steps!Source: xkcd comicsYou get the idea.Anaconda distributionhas made virtual environments and replicating environments using a standardized method a realityyet things do get muddled and sometimes we miss the bullet points in theREADME file, carefully created to replicate those.To solve the above problem,bash scriptsandmakefilesare added which adds to more confusion. It becomes as simple asuntangling earphones, phew!Dockers learning curvemight be a bit steep, but it helps to solve:TheCaffeexample we discussed is one of the pain points that everyone experiences in their Data Science journey. Not onlyDockerhelps to set a consistent platform via which these tools can be shared, the time wasted in searching foroperating systemspecific installers/libraries is eliminated.Along with sharing the tools (docker images as installers), we can shareJupyter notebooksorscriptsalong with their results baked inside aDocker image. All the other person/colleague needs to do is run theDocker imageto find out whats there!Inmy last article, we looked at how wrapping ML model in an API helps to make it available to your consumers. This is just one part of it. With small teams with no independent DevOps team to take care of deployments,Docker and the eco-systemaround it docker-compose,docker-machinehelps to ease the problems at a small scale.Sales guys needs to present an RShiny application but dont want to run the code?Dockercan help you with that!Ive been going on about containers, containerization in the previous section. Lets understand theDocker terminologiesfirst.For starters,containerscan be thought of as mini-VMs that are light-weight, disposable. Technically though, they are just processes (threads if you might say) that are created when you fire Docker commands in your terminal via their Command Line Interface (CLI).Docker also provides:imagesthat are essentially snapshots of thecontainerswhose running state is saved using Docker CLI or generated usingDockerfile.ADockerfilecan be considered as an automated setup file. This small file helps to create/modify Docker images. All the talk makes no sense, until theres some proof. Lets dive in and fire up your terminals.Think of the process of creating aDocker imageas creating alayered cake.Dockerfilebeing yourrecipe,Docker imagecreated out of various layers.In the next few sections, well try to get a feel ofDockerand work with its command line commands. Also, well create our ownDocker Image.This is all we need to execute adocker image.hello-worldis simple, it has to be, but lets move on to better things. Those that will help more, next section is all about that:Data Science tools without installation, our first use case.You have a clean laptop and you need to installTensorFlowin your system, but you are lazy (yes we all are sometimes). You want to procrastinate and not install things on your laptop, but you haveDockerinstalled already as a standard company practice. Hmm, interesting times, you ponder!You go toDockerhuband search for the officialDockerimage forTensorFlow. All you need to run on your terminal is:docker pull tensorflow/tensorflowAs discussed above (inDocker Terminologysection), thetensorflowdocker image is also a layered object that forms images. Once all the intermediate layers are downloaded, run:docker imagesto check whether ourdocker pullwas successful.To run the image, run the command:docker run -it -p 8888:8888 tensorflow/tensorflowNow the abovedocker runcommand packs in a few more command line argurments. A few which you need to know better are as follows:Now since a docker container is created, you can visit:http://localhost:8889where you can try outtensorflow.Wasnt that easy? Now as a exercise, replace-itin thedocker runcommand by-d. See whether you can get the tensorflow jupyter environment again or not?You should get the following outputs as in the screenshot below:Exercise:Create more containers with different ports using thedocker runcommand and see how many get created.We as Data Science folks are picky about what tools we use for our analysis, some like to work withRwhile others preferPython. Personally, Id whine about the above TensorFlow image. I dont know whats there in it (unless I look at the source code i.e. theDockerfileakarecipe). Tensorflow isnt enough on its own, suppose you want to useOpenCVtoo and maybescikit-learn&matplotlib.Lets see how to create your own customTensorFlowimage!Docker provides a good support to build up from a prototype level scale to production levels. Purely from a deployments perspective:docker-machine,docker-compose&docker-swarmare components that help achieve that.Starting off with a new habit is a difficult task. But once the learning curve smoothens out, things start to work out and new ideas open up with the usage. It is the same with Docker, hoping that this primer makes you think about using it in your daily Data Science workflows. Comment down below, how do you plan to useDocker, starting today!Prathamesh Sarang works as a Data Scientist at Lemoxo Technologies. Data Engineering is his latest love, turned towards the *nix faction recently. Strong advocate of Markdown for everyone.",https://www.analyticsvidhya.com/blog/2017/11/reproducible-data-science-docker-for-data-science/
2 days to go for the DataHack Summit,Learn everything about Analytics|Introduction|What is the DataHack Summit?|What to expect @The DataHack Summit|Conference Talks|Hack Sessions|Workshops|Speed Networking|Focus Areas|Why DataHack Summit is the place to be this week !|End Notes,"Profiles|Participating Organisations|Share this:|Like this:|Related Articles|A Step Towards Reproducible Data Science : Docker for Data Science Workflows|Exclusive Interview  Six years of Analytics Education  what has changed, and what hasnt?|
Dishashree Gupta
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What do you think of when you hear the word Summit or Conference? You think, knowledgeable talks and theoretical buzz. Research and case studies. Reports and insights.A lot of conferences follow the same guidelines and eventually get redundant. As the times change, so should the way in which conferences are conducted.In a glorious attempt to redefine those two words and to eradicate redundancy; if Data Science & Machine Learning have always been your calling and you want to meet the biggest names of the industry who have redefined the data science-scape, we bring to you Data Hack Summit 2017 and it definitely is the place to be this week.Crucial to science education is hands-on involvement: showing, not just telling; real experiments and field trips and not just virtual reality.We at Analytics Vidhya feel passionately about enhancing data science culture in India. We want people to talk, showcase and learn data science. With the Datahack Summit, we aim to create a conference which leaves you in the company of several like minded passionate people.And not just meeting these people, we have 30+ talks, 10 Hack Sessions, 6 full day workshops and much more. We get you in touch with people with expertise in several domains involving Machine learning, Deep Learning, Artificial intelligence, Business Analytics, BioGenomics, Geo-spatial coding and various other topics. At the DataHack Summit, you dont just hear people talk, you also witness multiple live coding sessions. You have 10 hack sessions where the speaker gives a live demo for you to understand their approach to various problems.Lets go into bit more detail of what to expect in these 3 days!The Conference is not just aimed at having Data Scientists. We have people from various domains and Profiles, we have people from managers to professors, Analysts to CEOs. Lets take a look at the distribution of the profiles of people you shall meet at the DATAHACK SUMMIT.DATAHACK SUMMIT marks the presence of 200+ organisations under a single roof. We shall have 800+ people from organisations like American Express, Fidelity Investments, Accenture, Shell, HP, Intel and many other organisations. You can take a look at the distribution of all these participating organisations.The first step towards learning about a specific topic would be to hear what it is all about. Who can do this better than the best in the field?Hers a list of a few of our speakers. You can check the entire list here.     Our Keynote speaker, Dr. Kirk Borne is one of the most influential people in the field of Data Science. He worked 18 years supporting NASA projects in various roles, including Data Archive Project Scientist for the Hubble Space Telescope which was once informally dubbed as one of the greatest achievements of mankind. He has a PhD in Astronomy from Caltech and a BS in Physics from LSU.Another one of our famed speakers, Bharat Kaul, has been at Intel for 17+ years, a brand that most people associate very closely with.Bharats lab focuses on Application driven Architectural Performance leadership for Intel for multi-core/many architectures with special focus on Artificial Intelligence and High Performance Computing. The lab works in close collaboration with leading academic and industry partners with 30+ papers in last years in Tier-1 conferences.This is something unique we can up with. We dont just want to audience to hear experts talk, we would also want the participants to see experts coding. The DataHack Summit marks 10, one hour hack sessions where experts would be doing a live problem solving in front of the audienceSome of the Hack Sessions include You can see the entire list of the hack sessions here.Now that youve acquainted yourself with the latest advancements of the field and domain, how do you understand where and when to apply these?The 8 hour workshop sessions are specially curated for beginners and experts alike, where youll be taught about a certain topic and guided through hands on coding by experts as you get the most out of your learning. You will be taught by industry experts and Kaggle grandmasters to learn tips and tricks you never knew about!Workshops we would be having at the DataHack Summit:More than anything else the conference aims to summon all data scientists, CXOs, Directors, Business Consultants, Startup Founders and Architects under one roof and get them talking about whats going on new in the industry.At this mini event, we give you time to exchange your cards, get familiar with as many people as you can. We will pair up consultants with consultees, trainers with trainees and most importantly, job seekers with employers!Following will be the areas of focus of discussions at the DataHack Summit 2017:Data Hack Summit 2017 is Indias biggest conference on Data science , ML and AI. Visit www.analyticsvidhya.com/datahacksummit to know more about the conference and how you can be a part of the learning extravaganza. We are extremely excited to organize the first ever largest AI/ML conference in India. See you at the DataHack Summit!",https://www.analyticsvidhya.com/blog/2017/11/2-days-to-go-for-the-datahack-summit/
"Exclusive Interview  Six years of Analytics Education  what has changed, and what hasnt?",Learn everything about Analytics|,"Learn,engage,compete,andget hired!|Share this:|Like this:|Related Articles|2 days to go for the DataHack Summit|A Guide To Conduct Analysis Using Non-Parametric Statistical Tests|
Kunal Jain
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics and Data Science is a fast evolving landscape. What was relevant few years back has become obsolete today. Over the last few years,Data Science and Analytics have progressively become part of the vocabulary of professionals and students in the country.We thought its a good time to see how this domain has evolved over the past few years  from the point of view of an early player in Indias Analytics training landscape. Praxis Business School was one of thepioneer in recognizing the need to create a pipeline of trained resources for this science.We spoke to Charanpreet Singh, Co-founder and Director, Praxis Business School Foundation, and Dr. Prithwis Mukerjee, Director, Business Analytics Program, Praxis Business School to examine how different does the analytics landscape looks today compared to 2011, when Praxis started its full-time program in this field.Analytics Vidhya (AV): What triggered your starting a full-time post graduate program in business analytics in November 2011, and what was the response to this move? How has the program progressed in the last 6 years?Charanpreet: We were in Bangalore, meeting companies for recruitment for our business management students. In two of our meetings, we were told by two completely unconnected people to start a course in analytics  they believed that there would be a great demand for trained professionals and Praxis had the capability to deliver a program in this area. We did our own research and decided to go ahead with the idea  as it would help us differentiate ourselves among the several thousand B-schools!The response we got was overwhelmingly meager  we started with a batch of 8 students. As the awareness of this domain increased, not the least due to the efforts of communities like yours, and as successive batches saw real jobs at the end of the program, the demand picked up. Today we have 2 intakes a year  January and July batches running out of Kolkata and Bangalore  and also run the analytics program at the International Institute of Digital Technologies.AV: How has the analytics landscape changed over the last 6 years from a Praxis point of view?Prithwis: There are changes across all stakeholders: the most obvious change of course is the number of people seeking to skill themselves in analytics. There are two important reasons for this:On the program front  technology has the nasty habit of changing just when you thought that you have mastered it. This is no surprise to anyone who has been at the cutting edge  as is the case for many of us at Praxis. But instead of being resigned to this bleeding edge we thrive on it by keeping the Praxis curriculum in perpetual beta.When we began in 2011, SAS was central to our curriculum and this was supplemented with domain knowledge from partners like ICICI Bank. Within a year or two we realised that the world was moving into the open source tools like R for analysis and Hadoop for managing Big Data, and altered our curriculum accordingly. In another two years with the advent of Spark and TensorFlow, Python has become the dominant platform because of its native affinity for these tools and so our curriculum was modified again  in fact in the middle of the 2016-17 session!There has been a continual change in the way the subjects are taught as well  there are more used cases available, more hackathons and problems in the public domain that we can get our students to work on. So, both from content and pedagogy perspectives, the analytics program at Praxis has undergone huge changes to remain contemporary and effective.AV: What about the demand side of things  the industry?Charanpreet: Three trends are clearly visible:At present, our placements are witnessing the dual advantage of two factors  repeat purchase by loyal companies and a significant number of new companies joining the fray. I am sure this must be the case industry-wide.KJ: Now that we have a hold on the journey so far, what do you think lies ahead?Prithwis: There is no doubt that the future belongs to artificial intelligence  of the kind demonstrated in self driving cars and automatic face and voice recognition. All these are extensions of what the data scientist refers to as data mining or machine learning and there are two aspects to this change. Obviously, subjects like artificial neural networks, deep learning and cognitive learning will become increasingly important and we are introducing them into our curriculum.The other subtle change is that this same AI will be used to make data science simpler and easier to use. Managers will be able to use GUI driven tools to carry out data science tasks without having to learn about the algorithms and data-structures that support machine learning models. The challenge lies in being able to harmonise these two widely different scenarios in a manner that caters to the aspirations and expectations of all our future students.KJ: Finally, what would you say to an aspiring data scientist?Charanpreet: An aspiring data scientist has already made the right decision  so we encourage him whole-heartedly to join this exciting world. As the subject is sufficiently complex, my advice would be to invest adequate time in a process that strengthens your knowledge and skill levels and gives you a sound conceptual base from which you can chart your growth. Too many aspirants think they can juggle several things in life along with learning  our experience has been that even as a full-time pursuit fathoming this science is a challenge.My appeal to everyone who believes he/ she is a good problem solver, analytical and number friendly  analytics and data science is pretty much the best option you have for an interesting, remunerative and sustainable career  because data generation is poised to grow at unprecedented rates, and the requirement for people who can make sense out of this data is similarly poised for an explosion! With the fear of repeatable tasks in the IT workplace getting automated, you would likely be taking a big risk if you do not re-skill yourself.Thanks Charanpreet & Prithwis for spending your valuable time with us and sharing these thought with us. We wish you all the best for coming batches and hope to see you at the DataHack Summit 2017.",https://www.analyticsvidhya.com/blog/2017/11/six-years-of-analytics-education-what-changed/
A Guide To Conduct Analysis Using Non-Parametric Statistical Tests,Learn everything about Analytics|Introduction|Table of Contents|1. How are Non-Parametric tests different from Parametric tests?|2. When can I apply non-parametric tests?|3. Pros and Cons of using non-parametric test|4. Hypothesis testing with non-parametric tests|5. Non-Parametric Tests|End Notes,"Pros|Cons|1. The first step is to set up hypothesis and opt a level of significance|2. Set a test statistic|3. Set decision rule|4.Calculate test statistic|5. Compare the test statistic to the decision rule|Mann Whitney U test|Wilcoxon Sign-Rank Test|Sign Test|Kruskal-Wallis Test|Spearman Rank Correlation|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Exclusive Interview  Six years of Analytics Education  what has changed, and what hasnt?|A Comprehensive Tutorial to Learn Data Science with Julia from Scratch|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The average salary package of an economics honors graduate at Hansraj College during the end of the 1980s was around INR 1,000,000 p.a. The number is significantly higher than people graduating in early 80s or early 90s.What could be the reason for such a high average? Well, one of the highest paid Indian celebrity, Shahrukh Khan graduated from Hansraj College in 1988 where he was pursuing economics honors. This, and many such examples tell us that average is not a good indicator of the center of the data. It can be extremely influenced by Outliers. In such cases, looking at median is a better choice.It is a better indicator of the center of the data because half of the data lies below the median and the other half lies above it.So far, so good  I am sure you have seen people make this point earlier. The problem is no one tells youhow to perform the analysis like hypothesis testing taking median into consideration. Statistical tests are used for making decisions.To perform analysis using median, we need to use non-parametric tests. Non-parametric tests are distribution independent tests whereas parametric tests assume that the data is normally distributed. It would not be wrong to say parametric tests are more infamous than non-parametric tests but the former does not take median into account while the latter makes use of median to conduct the analysis.Without wasting any more time, lets dive into the world of non-parametric tests.Note: This article assumes that you have prerequisite knowledge of hypothesis testing, parametric tests, one-tailed & two-tailed tests.If you read our articles on probability distributionsand hypothesis testing, I am sure you know that there are several assumptions attached to each probability distribution. Parametric tests are used when the information about the population parameters is completely known whereas non-parametric tests are used when there is no or few information available about the population parameters. In simple words, parametric test assumes that the data is normally distributed. However, non-parametric tests make no assumptions about the distribution of data.But what are parameters? Parameters are nothing but characteristics of the population that cant be changed. Lets look at an example to understand this better.A teacher calculated average marks scored by the students of her class by using the formula shown below:
Look at the formula given above, the teacher has considered the marks of all the students while calculating total marks. Assuming that the marking of students is done accurately and there are no missing scores, can you change the total marks scored by the students? No. Therefore, average marks is called a parameter of the population since it cannot be changed. Lets look at some examples.1. A winner of the race is decided by the rank and rank is allotted on the basis of crossing the finish line. Now, the first person to cross the finish line is ranked 1, the second person to cross the finish line is ranked 2 and so on. We dont know by what distance the winner beat the other person so the difference is not known.2. A sample of 20 people followed a course of treatment and their symptoms were noted by conducting a survey. The patient was asked to choose among the 5 categories after following the course of treatment. The survey looked somewhat like thisNow, if you look carefully the values in the above survey arent scalable, it is based on the experience of the patient. Also, the ranks are allocated and not calculated. In such cases, parametric tests become invalid.For a nominal data, there does not exist any parametric test. 3. Limit of detection is the lowest quantity of a substance that can be detected with a given analytical method but not necessarily quantitated as an exact value. For instance, a viral load is the amount of HIV in your blood. A viral load can either be beyond the limit of detection or it can a higher value.4. In the example above of average salary package, Shahrukhs income would be an outlier. What is an outlier? The income of Shahrukh lies at an abnormal distance from the income of other economics graduates. So the income of Shahrukh here becomes an outlier because it lies at an abnormal distance from other values in the data.To summarize, non-parametric tests can be applied to situations when:The point to be noted here is that if there exists a parametric test for a problem then using nonparametric tests will yield highly inaccurate answers.In the above discussion, you may have noticed that I mentioned few points where using non-parametric tests could be beneficial or disadvantageous so now lets look at these points collectively. The pros of using non-parametric tests over parametric tests are 1. Non-parametric tests deliver accurate results even when the sample size is small.2. Non-parametric tests are more powerful than parametric tests when the assumptions of normality have been violated.3. They are suitable for all data types, such as nominal, ordinal, interval or the data which has outliers.1. If there exists any parametric test for a data then using non-parametric test could be a terrible blunder.2. The critical value tables for non-parametric tests are not included in many computer software packages so these tests require more manual calculations.Now you know that non-parametric tests are indifferent to the population parameters so it does not make any assumptions about the mean, standard deviation etc of the parent population. The null hypothesis here is as general as the two given populations are equal. Steps to follow while conducting non-parametric tests:Now, lets look at what these two areHypothesis: My prediction is that Rahul is going to win the race and the other possible outcome is that Rahul isnt going to win the race. These are our hypothesis. Our alternative hypothesis is Rahul will win the race because we set alternative hypothesis equal to what we want to prove. The null hypothesis is the opposite one, generally null hypothesis is the statement of no difference. For example,
Level of significance:It is the probability of making a wrong decision. In the above hypothesis statement, null hypothesis indicates no difference between sample and population mean. Say theres a 5% risk of rejecting the null hypothesis when there is no difference between the sample and population mean. This risk or probability of rejecting the null hypothesis when its true is called level of significance.In a non-parametric test, the test hypothesis can be one tailed or two tailed depending on the interest of research.To understand what a statistic is, lets look at an example. A teacher computed the average marks, say 36, scored by the students in section A, and she used the average marks scored by the students in section A to represent the average marks scored by the students in sections B, C and D. The point to be noted here is that the teacher did not make the use of total marks scored by the students in all the sections instead she used the average marks of section A. Here, the average marks is called a statistic since the teacher did not make use of the entire data. In a non-parametric test, the observed sample is converted into ranks and then ranks are treated as a test statistic.A decision rule is just a statement that tells when to reject the null hypothesis.In non-parametric tests, we use the ranks to compute the test statistic.
Here, youll be accepting or rejecting your null hypothesis on the basis of comparison. We will dig deeper into this section while discussing types of non-parametric tests. Also known as Mann Whitney Wilcoxon and Wilcoxon rank sum test and, is an alternative to independent sample t-test. Lets understand this with the help of an example.A pharmaceutical organization created a new drug to cure sleepwalking and observed the result on a group of 5 patients after a month. Another group of 5 has been taking the old drug for a month. The organization then asked the individuals to record the number of sleepwalking cases in the last month. The result was:If you look at the table, the number of sleepwalking cases recorded in a month while taking the new drug is lower as compared to the cases reported while taking the old drug.Look at the graphs given below. 
Now, here you see that the frequency of sleepwalking cases is lower when the person is taking new drugs. 

Understood the problem? Lets see how Mann Whitney U test works here. We are interested in knowing whether the two groups taking different drugs report the same number of sleepwalking cases or not. The hypothesis is given below:
I am selecting 5% level of significance for this test. The next step is to set a test statistic. 
For Mann Whitney U test, the test statistic is denoted by U which is the minimum of U1 and U2.Now, we will compute the ranks by combining the two groups. The question isHow to assign ranks?Ranks are a very important component of non-parametric tests and therefore learning how to assign ranks to a sample is considerably important. Lets learn how to assign ranks.1. We will combine the two samples and arrange them in ascending order. I am using OD and ND for Old Drug and New Drug respectively.The lowest value here is assigned the rank 1 and the second lowest value is assigned the rank 2 and so on.But notice that the numbers 1, 4 and 8 are appearing more than once in the combined sample. So the ranks assigned are wrong.How to assign ranks when there are ties in the sample?Ties are basically a number appearing more than once in a sample. Look at the position of number 1 in the sample after sorting the data. Here, the number 1 is appearing at 1st and 2nd position. In such a case, we take the mean of 1 and 2 (because the number 1 is appearing at 1st and 2nd position) and assign the mean to the number 1 as shown below. We follow the same steps for number 4 and 8. The number 4 here is appearing at position 5th and 6th and their mean is 5.5 so we assign rank 5.5 to the number 4. Calculate rank for number 8 along these lines. We assign the mean rank when there are ties in a sample to make sure that the sum of ranks in each sample of size n is same. Therefore, the sum of ranks will always be equal to2. The next step is to compute the sum of ranks for group 1 and group 2.R1 = 15.5
R2 = 39.53. Using the formula of U1 & U2, compute their values.U1 = 24.5
U2 = 0.5Now, U = min(U1, U2) = 0.5Note: For Mann Whitney U test, the value of U lies in the range(0, n1*n2) where 0 indicates that the two groups are completely different from each other and n1*n2 indicates some relation between the two groups. Also, U1 + U2 is always equal to n1*n2. Notice that the value of U is 0.5 here which is very close to 0. Now, we determine a critical value (denoted by p),using the table for critical values, which is a point derived from the level of significanceof the test and is used to reject or accept the null hypothesis. In Mann Whitney U test, the test criteria areU < critical value, therefore, we reject the null hypothesis and conclude that the theres no significant evidence to state that two groups report same number of sleepwalking cases.This test can be used in place of paired t-test whenever the sample violates the assumptions of a normal distribution. A teacher taught a new topic in the class and decided to take a surprise test on the next day. The marks out of 10 scored by 6 students were as follows:
Note: Assume that the following data violates the assumptions of normal distribution.Now, the teacher decided to take the test again after a week of self-practice. The scores wereLets check if the marks of the students have improved after a week of self-practice.In the table above, there are some cases where the students scored less than they scored before and in some cases, the improvement is relatively high (Student 4). This could be due to random effects. We will analyse if the difference is systematic or due to chance using this test.The next step is to assign ranks to the absolute value of differences. Note that this can only be done after arranging the data in ascending order.In Wilcoxon sign-rank test, we need signed ranks which basically is assigning the sign associated with the difference to the rank as shown below.                   Easy, right? Now, what is the hypothesis here?The hypothesis can be one-sided or two-sided and I am considering one-sided hypothesis and using 5% level of significance. Therefore,The test statistic for this test is W is the smaller of W1 and W2 defined below:W1 = 17.5
W2 = 3.5
W = min(W1, W2 ) = 3.5Here, if W1 is similar to W2 then we accept the null hypothesis. Otherwise, in this example, if the difference reflects greater improvement in the marks scored by the students, then we reject the null hypothesis. The critical value of W can be looked up in the table. The criteria to accept or reject null hypothesis areHere, W > critical value=2, therefore we accept the null hypothesis and conclude that theres no significant difference between the marks of two tests.This test is similar to Wilcoxon sign-rank test and this can also be used in place of paired t-test if the data violates the assumptions of normality. I am going to use the same example that I used in Wilcoxon sign-rank test, assuming that it does not follow the normal distribution, to explain sign test. Lets look at the data again.In sign test, we dont take magnitude into consideration thereby ignoring the ranks. The hypothesis is same as before.Here, if we see a similar number of positive and negative differences then the null hypothesis is true. Otherwise, if we see more of positive signs then the null hypothesis is false.Test Statistic: The test statistic here is smaller of the number of positive and negative signs. Determine thecritical value and the criteria for rejecting or accepting null hypothesis isHere, the smaller number of + &  signs = 2 < critical value = 6. Therefore, we reject the null hypothesis and conclude that theres no significant evidence to state that the median difference is zero.This test is extremely useful when you are dealing with more than 2 independent groups and it compares median among k populations. This test is an alternative to One way ANOVA when the data violates the assumptions of normal distribution and when the sample size is too small.

Note: The Kruskal-Wallis test can be used for both continuous and ordinal-level dependent variables.Lets look at an example to enhance our understanding of Kruskal-Wallis test. Patients suffering from Dengue were divided into 3 groups and three different types of treatment were given to them. The platelet count of the patients after following a 3-day course of treatment is given below.Notice that the sample size is different for the three treatments which can be tackled using Kruskal-Wallis test.Sample sizes for treatments 1, 2 and 3 are as follows:Treatment 1; n1 = 5
Treatment 2; n2 = 3
Treatment 3; n3 = 4
n = n1 + n2 + n3 = 5+3+4 = 12The hypothesis here is given below and I have selected 5% level of significance.Order these samples from smallest to largest and then assign ranks to the clubbed sample.Recall that the sum of ranks will always be equal to n(n+1)/2.Here, sum of ranks = 78
n(n+1)/2 = (12*13)/2 = 78We have to check if there is a difference between 3 population medians so we will summarize the sample information in a test statistic based on ranks. Here, the test statistic is denoted by H and given by the following formulaThe next step is to determine the critical value of H using the table of critical values and the test criteria is given by:H comes out to be 6.0778 and the critical value is 5.656. Therefore, we reject our null hypothesis and conclude that theres no significant evidence to state that the three population medians are same.Note: In a Kruskal-Wallis test, if there are 3 or more independent comparison groups with 5 or more observations in each group then the test statistic H approximates a chi-square distribution with k-1 degree of freedom. Therefore, in such a case, you can find the critical value of the test in the chi-square distribution table for critical values. 
I went to the market to buy a skirt and coincidently my friend bought the same skirt from the market near her place but she paid a higher price for it. The market near my friends place is more expensive as compared to mine. So does a region affect the price of a commodity? If it does then there is a link between the region and price of the commodity. We make use of Spearman rank correlation here because it establish if there is the correlation between two datasets. The prices of vegetables differ across areas. We can check if theres a relation between the price of a vegetable and area by using Spearman rank correlation. The hypothesis here is:
Here, the trend line suggests a positive correlation between the price of vegetable and area. However, Spearmans rank correlation method should be used to check the direction and strength of correlation.Spearman rank correlation is a non-parametric alternative to Pearson correlation coefficient and is denoted by . The value oflies in the range (-1,1) where 
-1 represents a negative correlation between ranks
0 represents no correlation between ranks
1 represents a positive correlation between ranks

After assigning ranks to the sample, use the following formula to calculate Spearman rank correlation coefficient.Lets understand the application of these formula using a sample data.The following table includes marks of students in math and science.
The null hypothesis states that there is no relationship between the marks and alternative hypothesis states that there is a relationship between the marks. I have selected 5% level of significance for this test.Now calculate rank and d which is the difference between ranks and n is the sample size = 10. This is done as follows:Now, use the formula to calculate Spearman rank correlation coefficient. Hence, the Spearman rank correlation comes out to be 0.67 which indicates a positive relation between the ranks students obtained in maths and science test which implies that the higher you ranked in maths, the higher you ranked in science and vice-versa.You can also check this by determining the critical values using the significance level and sample size. The criteria to reject or accept null hypothesis is given byNote: The degree of freedom here is n-2. 
The critical value is found to be 0.033 which is lower than 0.67. Hence, we reject our null hypothesis.Non-parametric tests are more powerful when the assumptions for parametric tests are violated and can be used for all data types such as nominal, ordinal, interval and also when data has outliers. If any of the parametric tests is valid for a problem then using non-parametric test will give highly inaccurate results. To summarize,Mann Whitney U test is used for testing the difference between two independent groups with ordinal or continuous dependent variable.Wilcoxon sign rank test is used for testing the difference between two related variables which takes into account the magnitude and direction of difference, however, Sign test ignores the magnitude and only considers the direction of the difference.Kruskal-Wallis test compares the outcome among more than 2 independent groups by making use of the medians.Spearman Rank Correlation technique is used to check if there is a relationship between the two data sets and it also tells about the type of relationship. I hope that you find this article useful and if you would like to see more articles on non-parametric or parametric tests then write down in the comment section below.",https://www.analyticsvidhya.com/blog/2017/11/a-guide-to-conduct-analysis-using-non-parametric-tests/
A Comprehensive Tutorial to Learn Data Science with Julia from Scratch,Learn everything about Analytics|Introduction|Table of Contents|Installation|Basics of Julia for Data Analysis|Exploratory Analysis using Julia (Analytics Vidhya Hackathon)|Data Munging in Julia|Building a predictive ML model|Calling R and Python libraries in Julia||End Notes,"Running your first Julia program|Few things to note|Julia Data Structures|Loops, Conditionals in Julia|Introduction to DataFrames.jl|Practice dataset: Loan Prediction Problem|Importing libraries and the data set|Quick Data Exploration|Visualisation in Julia using Plots.jl|Distribution analysis|Bonus: Interactive visualizations using Plotly|Data munging  recap of the need|Check missing values in the dataset|How to fill missing values?|Label Encoding categorical data|Logistic Regression|Decision Tree|Random Forest|Using pandas with Julia|Using ggplot2 in Julia|Learn,engage,compete,andget hired!|Share this:|Like this:|Related Articles|A Guide To Conduct Analysis Using Non-Parametric Statistical Tests|The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks)|
Mohd Sanad Zaki Rizvi
|27 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Installing Julia|Installing IJulia and Jupyter Notebook|Installing Julia Packages,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently, I came across a quote about Julia:Walks like python. Runs like C.The above line tells a lot about why I chose to write this article. I came across Julia a while ago even though it was in its early stages, it was still creating ripples in the numerical computing space. Julia is a work straight out of MIT, a high-level language that has a syntax as friendly as Python and performance as competitive as C. This is not all,It provides a sophisticated compiler,distributed parallel execution, numerical accuracy, and anextensive mathematical function library.But this article isnt about praising Julia, it is about how can you utilize it in your workflow as a data scientist without going through hours of confusion which usually comes when we come across a new language. Read more about Why Julia? here.Before we can start our journey into the world of Julia, we need to set up our environment with the necessary tools and libraries for data science. https://julialang.org/downloads/  https://julialang.org/downloads/platform.html Jupyter notebook has become an environment of choice for data science since it is really useful for both fast experimenting and documenting your steps. There are other environments too for Julia like  Juno IDE  but I recommend to stick with the notebook. Lets look at how we can setup the same for Julia.Go to the Julia prompt and type the following codeNotethat Pkg.add() command downloads files and package dependencies in the background and installs it for you. For this, you should have an active internet connection. If your internet is slow, you might have to wait for some time.After ijulia is successfully installed you can type the following code to run it,By default, the notebook dashboard opens in your home directory ( homedir() ), but you can open the dashboard in a different directory with notebook(dir=/some/path) .There you have your environment all set up. Lets install some important Julia libraries that wed be needing for this tutorial.A simple way of installing any package in Julia is using the command Pkg.add(..). Like Python or R, Julia too has a long list of packages for data science. I thought instead of installing all the packages together it would be better if we install them as and when needed, thatd give you a good sense of what each package does. So we will be following that process for this article.Julia is a language that derives a lot of syntax from other data analysis tools like R, Python, and MATLAB. If you are from one of these backgrounds, it would take you no time to get started with it. Lets learn some of the basic syntaxes. If you are in a hurry heres a cheat sheet comparing syntax of all the three languages:There, you created your first Julia notebook! Just like you use jupyter notebook for R or Python, you can write Julia code here, train your models, make plots and so much more all while being in the familiar environment of jupyter.Go ahead and play around a bit with the notebook to get familiar.The following are some of the most common data structures we end up using when performing data analysis on Julia:Note that in Julia the indexing starts from 1, so if you want to access the first element of an array youll do A[1].Notice that => operator is used to link key with their respective values. You access the values of the dictionary using its key.Like most languages, Julia also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax:Here Julia Iterable can be a vector, string or other advanced data structures which we will explore in later sections. Lets take a look at a simple example, determining the factorial of a number n.Julia also supports the while loop and various conditionals like if, if/else, for selecting a bunch of statements over another based on the outcome of the condition. Here is an example,The above code snippet performs a check on N and prints whether it is a positive or a negative number. Note that julia is not indentation sensitive like Python but it is a good practice to indent your code thats why youll find code samples in this article well indented. Here is a list of Julia conditional constructs compared to their counterparts in MATLAB and Python.You can learn more about Julia basics  here  .Now that we are familiar with Julia fundamentals, lets take a deep dive into problem-solving. Yes, I mean making a predictive model! In the process, we use some powerful libraries and also come across the next level of data structures. We will take you through the 3 key phases:The first step in any kind of data analysis is exploring the dataset at hand. There are two ways to do that, the first is exploring the data tables and applying statistical methods to find patterns in numbers and the second is plotting the data to find patterns visually.The former requires an advanced data structure that is capable of handling multiple operations and at the same time is fast and scalable. Like many other data analysis tools, Julia provides one such structure called DataFrame. You need to install the following package for using it:A dataframe is similar to Excel workbook  you have column names referring to columns and you have rows, which can be accessed with the use of row numbers. The essential difference is that column names and row numbers are known as column and row index, in case of dataframes . This is similar to pandas.DataFrame in Python or data.table in R.Lets work with a real problem. We are going to analyze an Analytics Vidhya Hackathon as a practice dataset.You can download the dataset from  here  . Here is the description of variables:In Julia we import a library by the following command:Lets first import our DataFrames.jl library and load the train.csv file of the data set:Once the data set is loaded, we do preliminary exploration on it. Such as finding the size(number of rows and columns) of the data set, the name of columns etc. The function size(train) is used to get the number of rows and columns of the data set and names(train) is used to get the names of columns(features).The data set is not that large(only 614 rows) knowing the size of data set sometimes affect the choice of our algorithm. There are 13 columns(features) we have that is also not much, in case of a large number of features we go for techniques like dimensionality reduction etc. Lets look at the first 10 rows to get a better feel of how our data looks like? The head(,n) function is used to read the first n rows of a dataset.A number of preliminary inferences can be drawn from the above table such as:Note that these inferences are just preliminary they will either get rejected or updated after further exploration.I am interested in analyzing the LoanAmount column, lets have a closer look at that.describe() function would provide the count(length), mean, median, minimum, quartiles and maximum in its output (Read  this article  to refresh basic statistics to understand population distribution).Please note that we can get an idea of a possible skew in the data by comparing the mean to the median, i.e. the 50% figure.For the non-numerical values (e.g. Property_Area, Credit_History etc.), we can look at frequency distribution to understand whether they make sense or not. The frequency table can be printed by the following command:Similarly, we can look at unique values of credit history. Note that dataframe_name[:column_name] is a basic indexing technique to access a particular column of the dataframe. A column can also be accessed by its index. For more information, refer to the documentation .Another effective way of exploring the data is by doing it visually using various kind of plots as it is rightly said, A picture is worth a thousand words .Julia doesnt provide a plotting library of its own but it lets you use any plotting library of your own choice in Julia programs. In order to use this functionality you need to install the following package:The package Plots.jl provides a single frontend(interface) for any plotting library(matplotlib, plotly, etc.) you want to use in Julia. StatPlots.jl is a supporting package used for Plots.jl. PyPlot.jl is used to work with matplotlib of Python in Julia.Now that we are familiar with basic data characteristics, let us study the distribution of various variables. Let us start with numeric variables  namely ApplicantIncome and LoanAmountLets start by plotting the histogram of ApplicantIncome using the following commands:Here we observe that there are few extreme values. This is also the reason why 50 bins are required to depict the distribution clearly.Next, we look at box plots to understand the distributions. Box plot for fare can be plotted by:This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:We can see that there is no substantial difference between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers.Now, Lets look at the histogram and boxplot of LoanAmount using the following command:Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging. LoanAmount has missing and well as extreme values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections.That was a lot of useful visualizations, to learn more about creating visualizations in Julia using Plots.jl Plots.jl DocumentationNows the time where awesomeness of Plots.jl comes into play. The visualizations we created till now were all good but while exploration it is useful if the plot is interactive. We can create interactive plots in Julia using Plotly as a backend. Type the following codeYou can do much more with Plots.jl and various backends it supports. Read Plots.jl Documentation For those, who have been following, here you must wear your shoes to start running.While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as Data Munging. Here are the problems, we are already aware of:In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Education and Dependents to see, if they contain any useful information.Let us look at missing values in all the variables because most of the models dont work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the datasetThough the missing values are not very high in number, many variables have them and each one of these should be estimated and added to the data.Note: Remember that missing values may not always be NaNs. For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and youre right. So we should check for values which are unpractical.There are multiple ways of fixing missing values in a dataset. Take LoanAmount for example, there are numerous ways to fill the missing values  the simplest being replacement by the mean.The other extreme would be to build a supervised learning model to predict loan amount on the basis of other variables and then use age along with other variables to predict survival.We would be taking the simpler approach to fix missing values in this article:I have basically replaced all missing values in numerical columns with their means and with the mode in categorical columns. Lets understand the code little closely,I hope this gives you a better understanding of the code part that is used to fix missing values.As discussed earlier, there are better ways to perform data imputation and I encourage you to learn as many as you can. Get a detailed view of different imputation techniques through  this article  .Now that we have fixed all missing values, we will be building a predictive machine learning model. We will also be cross-validating it and saving it to the disk for future use. The following packages are required for doing so:This package is an interface to Pythons scikit-learn package so python users are in for a treat. The interesting thing about using this package is you get to use the same models and functionality as you used in Python.Sklearn requires all data to be of numeric type so lets label encode our data,Those who have used sklearn before will find this code to be familiar, we are using LabelEncoder to encode the categories. I have used the index of columns with categorical data.Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores. Since this is an introductory article and julia code is very similar to python, I will not go into the details of coding. Please refer to  this article  for getting details of the algorithms with R and Python codes. Also, itll be good to get a refresher on cross-validation through  this article  , as it is a very important measure of power performance.Lets make our first Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting (dont worry if youre unaware of this terminology yet). In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well. Read more about  Logistic Regression  .We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for:So lets make our first model with Credit_History.Accuracy : 80.945% Cross-Validation Score : 80.957%Accuracy : 80.945% Cross-Validation Score : 80.957%Generally, we expect the accuracy to increase by adding variables. But this is a more challenging case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now:A decision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more aboutDecision Trees.Accuracy : 80.945% Cross-Validation Score : 76.656%Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Lets try a few numerical variables:Accuracy : 99.345% Cross-Validation Score : 72.009%Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Lets try an even more sophisticated algorithm and see if it helps:Random forest is another algorithm for solving the classification problem.Read more aboutRandom Forest.An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features.Accuracy : 100.000% Cross-Validation Score : 78.179%Here we see that the accuracy is 100% for the training set. This is the ultimate case of overfitting and can be resolved in two ways:The updated code would now beAccuracy : 82.410% Cross-Validation Score : 80.635%Notice that although accuracy reduced, the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark.You would have noticed that even after some basic parameter tuning on the random forest, we have reached a cross-validation accuracy only slightly better than the original logistic regression model. This exercise gives us some very interesting and unique learning:So are you ready to take on the challenge? Start your data science journey withLoan Prediction Problem.Julia is a powerful language with interesting libraries but it may so happen that you want to use library of your own from outside Julia. One such reason can be lack of functionality in existing Julia libraries(it is still very young). For situations like this, Julia provides ways to call libraries from R and Python. Lets see how can we do that?Install the following package:There is something interesting about using a Python library as smoothly in another language.Pandas is a very mature and performant library, it is certainly a bliss that we can use it wherever the native DataFrames.jl falls short.Install the following packages:I hope this tutorial will helpyou maximize your efficiency when starting with data science in Julia. I am sure this not only gave you an idea aboutbasic data analysis methods but it also showed you how to implement some of the more sophisticated techniques available today.Julia is really a great tool and is becoming an increasingly popular language among the data scientists. The reason being, its easy to learn, integrates well with other tools, gives C like speed and also allows using libraries of existing tools like R and Python.So, learn Julia to perform the full life-cycle of any data science project. It includes reading, analyzing, visualizing and finally making predictions.Also note, all the code used in this article is available on GitHub.If you come across any difficulty while practicing Julia, or you have any thoughts/suggestions/feedback on the post, please feel free to post them in comments below.",https://www.analyticsvidhya.com/blog/2017/10/comprehensive-tutorial-learn-data-science-julia-from-scratch/
The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks),Learn everything about Analytics|Introduction|Why did I create this Guide?|Table of Contents|1. Stemming|2. Lemmatisation|3. Word Embeddings|4. Part-Of-Speech Tagging|5. Named Entity Disambiguation|6. Named Entity Recognition|7. Sentiment Analysis|8. Semantic Text Similarity|9. Language Identification|10. Text Summarisation|End Notes,"Learn,engage,compete,andget hired!|Share this:|Like this:|Related Articles|A Comprehensive Tutorial to Learn Data Science with Julia from Scratch|Fundamentals of Deep Learning  Activation Functions and When to Use Them?|
NSS
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Organizations today deal with huge amount and wide variety of data  calls from customers, their emails, tweets, data from mobile applications and what not. It takes a lot of effort and time to make this data useful. One of the core skills in extracting information from text data is Natural Language Processing (NLP).Natural Language Processing (NLP) is the art and science which helps us extract information from text and use it in our computations and algorithms. Given then increase in content on internet and social media, it is one of the must have still for all data scientists out there.Whether you know NLP or not, this guide should help you as a ready reference for you. Through this guide, I have provided you with resources and codes to run the most common tasks in NLP.Once you have gone through this guide, feel free to have a look at our video courseon Natural Language Processing (NLP).After having been working on NLP problems for some time now, I have encountered various situations where I needed to consult hundred of different of sources to study about the latest developments in the form of research papers, blogs and competitions for some of the common NLP tasks.So, I decided to bring all these resources to one place and make it a One-Stop solution for the latest and the important resources for these common NLP tasks. Below is the list of tasks covered in this article along with their relevant resources. Lets get started.What is Stemming?:Stemming is the process of reducing the words(generally modified or derived) to their word stem or root form. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word. For example, in the English language-Paper:The original paper by Martin Porteron Porter Algorithm for stemming.Algorithm:Here is the Python implementation of Porter2 stemming algorithm.Implementation: Here is how you canstem a word using the Porter2 algorithm from thestemming library.What is Lemmatisation?:Lemmatisation is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence, the meaning of the word in the nearby sentences etc. before reducing the word to its lemma. For example, in the English Language-Paper 1: This paper discusses different methods for performing lemmatisation in great detail. A must read if you want to know hoe traditional lemmatisers work.Paper 2: This is an excellent paper which addresses the problem of lemmatisation for variation rich languages using Deep Learning.Dataset: This is the link for Treebank-3 dataset which you can use if you wish to create your own Lemmatiser.Implementation:Below is an implementation of an English Lemmatiser using spacy.#!pip install spacy
#python -m spacy download en
import spacy
nlp=spacy.load(""en"")
doc=""good better best""for token in nlp(doc):
  print(token,token.lemma_)What is Word Embeddings?:Word Embeddings is the name of the techniques which are used to represent Natural Language in vector form of real numbers. They are useful because of computers inability to process Natural Language. So these Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100.So for example-A word man might be represented in a 5-dimension vector aswhere each of these numbers is the magnitude of the word in a particular direction.Blog:Here is an article which explains Word Embeddings in great detail.Paper: A very good paper which explains Word Vectors in detail. A must-read for an in-depth understanding of Word Vectors.Tool: A browser based tool for visualising Word Vectors.Pre-trained Word Vectors:Here is an exhaustive list of pre-trained Word Vectors in 294 languages by facebook.Implementation: Here is how you can obtain pre-trained Word Vector of a word using the gensim package.Download the Google News pre-trained Word Vectors from here.#!pip install gensim
from gensim.models.keyedvectorsimport KeyedVectors
word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)
word_vectors['human']Implementation:Here is how you can train your own word vectors using gensimsentence=[['first','sentence'],['second','sentence']]
model=gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)What is Part-Of-Speech Tagging?: In Simplistic terms,Part-Of-Speech Tagging is the process of marking up of words in a sentence as nouns, verbs, adjectives, adverbs etc. For example, in the sentence-Ashok killed the snake with a stickThe Parts-Of-Speech are identified as Ashok PROPN
killed VERB
the DET
snake NOUN
with ADP
a DET
stick NOUN
. PUNCTPaper 1: This paper by choi aptly titled The Last Gist to the State-of-the-Art presents a novel method called Dynamic Feature Induction which achieves state-of-the-art on POS Tagging taskPaper 2:This paper presents performing unsupervised POS Tagging using Anchor Hidden Markov Models.Implementation: Here is how we can perform POS Tagging using spacy.#!pip install spacy
#!python -m spacy download en
nlp=spacy.load('en')
sentence=""Ashok killed the snake with a stick""
for token in nlp(sentence):
 print(token,token.pos_)What is Named Entity Disambiguation?:Named Entity Disambiguation is the process of identifying the mentions of entities in a sentence. For example, in the sentence-Apple earned a revenue of 200BillionUSD in 2016It is the task of Named Entity Disambiguation to infer that Apple in the sentence is the company Apple and not a fruit.Named Entity, in general, requires a knowledge base of entities which it can use to link entities in the sentence to the knowledge base.Paper 1: This paper by Huangmakes use of Deep Semantic Relatedness models based on Deep Neural Networks along with Knowledgebase to achieve a state-of-the-art result on Named Entity Disambiguation.Paper 2: This paper by Ganea and Hofmann make use of Local Neural Attention along with Word Embeddings and no manually crafted features.What is Named Entity Recognition?: Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example, a NER would take in a sentence like Ram of Apple Inc. travelled to Sydney on 5th October 2017and return something likeRam
of
Apple ORG
Inc. ORG
travelled
to
Sydney GPE
on
5th DATE
October DATE
2017 DATEHere, ORG stands for Organisation and GPE stands for location.The problem with current NERs is that even state-of-the-art NER tend to perform poorly when they are used on a domain of data which is different from the data, the NER was trained on.Paper: This excellent paper makes use of bi-directional LSTMs and combines Supervised and Unsupervised learning methods to achieve a state-of-the-art result in Named Entity Recognition in 4 languages.Implementation: Here is how you can perform Named Entity Recognition using spacy.import spacy
nlp=spacy.load('en')sentence=""Ram of Apple Inc. travelled to Sydney on 5th October 2017""
for token in nlp(sentence):
 print(token, token.ent_type_)What is Sentiment Analysis?: Sentiment Analysis is a broad range of subjective analysis which uses Natural Language processing techniques to perform tasks such as identifying the sentiment of a customer review, positive or negative feeling in a sentence, judging mood via voice analysis or written text analysis etc. For example-I did not like the chocolate ice-cream  is a negative experience of ice-cream.I did not hate the chocolate ice-cream  may be considered as a neutral experienceThere is a wide range of methods which are used to perform sentiment analysis starting from taking a count of negative and positive words in a sentence to using LSTMs with Word Embeddings.Blog 1: This article focuses on performing sentiment analysis on movie tweetsBlog 2: This article focuses on performing sentiment analysis of tweets during the Chennai flood.Paper 1: This paper takes the Supervised Learning method approach with Naive Bayes method to classify IMDB reviews.Paper 2: This paper makes use of Unsupervised Learning method with LDA to identify aspects and sentiments of user-generated reviews. This paper is outstanding in the sense that it addresses the problem of shortage of annotated reviews.Repository: This is an awesome repository of the research papers and implementation of sentiment analysis in various languages.Dataset 1: Multi-Domain sentiment dataset version 2.0Dataset 2: Twitter Sentiment analysis DatasetCompetition:A very good competition where you can check the performance of your models on the sentiment analysis task of movie reviews of rotten tomatoes.Perform Twitter Sentiment Analysis your self.What is Semantic Text Similarity?: Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.For example Car and Bus are similar but Car and fuel are related.Paper 1: This paper presents the different approaches to measuring text similarity in detail. A must read paper to know about the existing approaches at a single place.Paper 2: This paper introduces CNNs to rank a pair of two short textsPaper 3: This paper makes use of Tree-LSTMs which achieve a state-of-the-art result on Semantic Relatedness of texts and Semantic Classification.What is Language Identification?: Language identification is the task of identifying the language in which the content is in. It makes use of statistical as well as syntactical properties of the language to perform this task. It may also be considered as a special case of text classification.Blog:In this blog post by fastText, they introduce a new tool which can identify 170 languages under 1MB of memory usage.Paper 1:This paperdiscusses 7 methods of language identification of 285 languages.Paper 2: This paper describes how Deep Neural Networks can be used to achieve state-of-the-art results on Automatic Language Identification.What is Text Summarisation?:Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.Paper 1: This paper describes a Neural Attention Model based approach for Abstractive Sentence Summarization.Paper 2: This paper describes how sequence-to-sequence RNNs can be used to achieve state-of-the-art results on Text Summarisation.Repository: This repository by Google Brain team has the codes for using a sequence-to-sequence model customised for Text Summarisation. The model is trained on Gigaword dataset.Application: Reddits autotldr bot uses Text Summarisation to summarise articles into the comments of a post. This feature turned out to be very famous amongst the Reddit users.Implementation: Here is how you can quickly summarise your text using the gensim package.from gensim.summarization import summarize

sentence=""Automatic summarizationis the process of shortening a text document withsoftware, in order to create asummarywith the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style andsyntax.Automatic data summarization is part ofmachine learninganddata mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today.Search enginesare an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization:extractionandabstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.""

summarize(sentence)So this was all about the most common NLP tasks along with their relevant resources in the form of blogs, research papers, repositories and applications etc. If you feel, there is any great resource on any of these 10 tasks that I have missed or you want to suggest adding another task, then please feel free to comment with your suggestions and feedback.Happy Learning!",https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/
Fundamentals of Deep Learning  Activation Functions and When to Use Them?,Learn everything about Analytics|Introduction|Table of Contents|Brief overview of neural networks|What is an Activation Function?|Can we do without an activation function?|Popular types of activation functions and when to use them|Choosing the right Activation Function|Projects|End Notes,"Binary Step Function|Linear Function|Sigmoid|Tanh|ReLU|Leaky ReLU|Softmax|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|The Essential NLP Guide for data scientists (with codes for top 10 common NLP tasks)|The Art of Story Telling in Data Science and how to create data stories?|
Dishashree Gupta
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Internet provides access to plethora of information today. Whatever we need is just a Google (search) away. However, when we have so much of information, the challenge is to segregate between relevant and irrelevant information.When our brain is fed with a lot of information simultaneously, it tries hard to understand and classify the information between useful and not-so-useful information. We need a similar mechanism to classify incoming information as useful or less-useful in case of Neural Networks.This is a very important in the way a network learns because not all information is equally useful. Some of it is just noise. Well, activation functions help the network do this segregation. They help the network use the useful information and suppress the irrelevant data points.Let us go through these activation functions, how they work and figure out which activation functions fits well into what kind of problem statement.Before I delve into the details of activation functions, lets do a little review of what are neural networks and how they function. A neural network is a very powerful machine learning mechanism which basically mimics how a human brain learns. The brain receives the stimulus from the outside world, does the processing on the input, and then generates the output.As the task gets complicated multiple neurons form a complex network, passing information among themselves.Using a artificial neural network, we try to mimic a similar behavior. The network you see below is a neural network made of interconnected neurons.The black circles in the picture above are neurons.Each neuron is characterized by its weight, bias and activation function. The input is fed to the input layer. The neurons do a linear transformation on the input by the weights and biases. The non linear transformation is done by the activation function. The information moves from the input layer to the hidden layers. The hidden layers would do the processing and send the final output to the output layer. This is the forward movement of information known as the forward propagation. But what if the output generated is far away from the expected value? In a neural network, we would update the weights and biases of the neurons on the basis of the error. This process is known as back-propagation. Once the entire data has gone through this process, the final weights and biases are used for predictions.Activation functions are an extremely important feature of the artificial neural networks. They basically decide whether a neuron should be activated or not. Whether the information that the neuron is receiving is relevant for the given information or should it be ignored.The activation function is the non linear transformation that we do over the input signal. This transformed output is then sen to the next layer of neurons as input.Now the question which arises is that if the activation function increases the complexity so much, can we do without an activation function?When we do not have the activation function the weights and bias would simply do a linear transformation. A linear equation is simple to solve but is limited in its capacity to solve complex problems.A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. We would want our neural networks to work on complicated tasks like language translations and image classifications. Linear transformations would never be able to perform such tasks.Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. Without the differentiable non linear function, this would not be possible.The first thing that comes to our mind when we have an activation function would be a threshold based classifier i.e. whether or not the neuron should be activated. If the value Y is above a given threshold value then activate the neuron else leave it deactivated.It is defined as = 0, x<0The binary function is extremely simple. Itcan be used while creating a binary classifier. When we simply need to say yes or no for a single class, step function would be the best choice, as it would either activate the neuron or leave it to zero.The function is more theoretical than practical since in most cases we would be classifying the data into multiple classes than just a single class. The step function would not be able to do that.Moreover, the gradient of the step function is zero. This makes the step function not so useful since during back-propagation when the gradients of the activation functions are sent for error calculations to improve and optimize the results. The gradient of the step function reduces it all to zero and improvement of the models doesnt really happen.We saw the problem with the step function, the gradient being zero, it was impossible to update gradient during the backpropagation. Instead of a simple step function, we can try using a linear function. We can define the function as-f(x)=axWe have taken a as 4 in the figure above. Here the activation is proportional to the input. The input x, will be transformed to ax. This can be applied to various neurons and multiple neurons can be activated at the same time. Now, when we have multiple classes, we can choose the one which has the maximum value. But we still have an issue here. Lets look at the derivative of this function.The derivative of a linear function is constant i.e. it does not depend upon the input value x.This means that every time we do a back propagation, the gradient would be the same. And this is a big problem, we are not really improving the error since the gradient is pretty much the same. And not just that suppose we are trying to perform a complicated task for which we need multiple layers in our network. Now if each layer has a linear transformation, no matter how many layers we have the final output is nothing but a linear transformation of the input. Hence, linear function might be ideal for simple tasks where interpretabilityis highly desired.Sigmoid is a widely used activation function. It is of the form-Lets plot this function and take a look of it.This is a smooth function and is continuously differentiable. The biggest advantage that it has over step and linear function is that it is non-linear. This is an incredibly cool feature of the sigmoid function. This essentially means that when I have multiple neurons having sigmoid function as their activation function  the output is non linear as well. The function ranges from 0-1 having an S shape. Lets take a look at the shape of the curve. The gradient is very high between the values of -3 and 3 but gets much flatter in other regions. How is this of any use?This means that in this range small changes in x would also bring about large changes in the value of Y. So the function essentially tries to push the Y values towards the extremes. This is a very desirable quality when were trying to classify the values to a particular class.Lets take a look at the gradient of the sigmoid function as well.Its smooth and is dependent on x. This means that during backpropagation we can easily use this function. The error can be backpropagated and the weights can be accordingly updated.Sigmoids are widely used even today but we still have a problems that we need to address. As we saw previously  the function is pretty flat beyond the +3 and -3 region. This means that once the function falls in that region the gradients become very small. This means that the gradient is approaching to zero and the network is not really learning.Another problem that the sigmoid function suffers is that the values only range from 0 to 1. This means that the sigmoid function is not symmetric around the origin and the values received are all positive. So not all times would we desire the values going to the next neuron to be all of the same sign. This can be addressed by scaling the sigmoid function. Thats exactly what happens in the tanh function. lets read on.The tanh function is very similar to the sigmoid function. It is actually just a scaled version of the sigmoid function.It can be directly written as Tanh works similar to the sigmoid function but is symmetric over the origin. it ranges from -1 to 1.It basically solves our problem of the values all being of the same sign. All other properties are the same as that of the sigmoid function. It is continuous and differentiable at all points. The function as you can see is non linear so we can easily backpropagate the errors.Lets have a look at the gradient of the tan h function.The gradient of the tanh function is steeper as compared to the sigmoid function. Our choice of using sigmoid or tanh would basically depend on the requirement of gradient in the problem statement. But similar to the sigmoid function we still have the vanishing gradient problem. The graph of the tanh function is flat and the gradients are very low.The ReLU function is the Rectified linear unit. It is the most widely used activation function. It is defined as-It can be graphically represented as-ReLU is the most widely used activation function while designing networks today. First things first, the ReLU function is non linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time. What does this mean ? If you look at the ReLU function if the input is negative it will convert it to zero and the neuron does not get activated. This means that at a time only a few neurons are activated making the network sparse making it efficient and easy for computation.Lets look at the gradient of the ReLU function.But ReLU also falls a prey to the gradients moving towards zero.If you look at the negative side of the graph, the gradient is zero, which means for activations in that region, the gradient is zero and the weights are not updated during back propagation. This can create dead neurons which never get activated. When we have a problem, we can always engineer a solution.Leaky ReLU function is nothing but an improved version of the ReLU function. As we saw that for the ReLU function, the gradient is 0 for x<0, which made the neurons die for activations in that region. Leaky ReLU is defined to address this problem. Instead of defining the Relu function as 0 for x less than 0, we define it as a small linear component of x. It can be defined as-What we have done here is that we have simply replaced the horizontal line with a non-zero, non-horizontal line. Here a is a small value like 0.01 or so. It can be represented on the graph as-The main advantage of replacing the horizontal line is to remove the zero gradient. So in this case the gradient of the left side of the graph is non zero and so we would no longer encounter dead neurons in that region. The gradient of the graph would look like Similar to the Leaky ReLU function, we also have the Parameterised ReLU function. It is defined similar to the Leaky ReLU as However, in case of a parameterised ReLU function, a is also a trainable parameter. The network also learns the value of a for faster and more optimum convergence. The parametrised ReLU function is used when the leaky ReLU function still fails to solve the problem of dead neurons and the relevant information is not successfully passed to the next layer.The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems. The sigmoid function as we saw earlier was able to handle just two classes. What shall we do when we are trying to handle multiple classes. Just classifying yes or no for a single class would not help then. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs. This essentially gives the probability of the input being in a particular class. It can be defined as Lets say for example we have the outputs as-The softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input.Now that we have seen so many activation functions, we need some logic / heuristics to know which activation function should be used in which situation. Good or bad  there is no rule of thumb.However depending upon the properties of the problem we might be able to make a better choice for easy and quicker convergence of the network.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:In this article I have discussed the various types of activation functions and what are the types of problems one might encounter while using each of them.I would suggest to begin with a ReLU function and explore other functions as you move further. You can also design your own activation functions giving a non-linearity component to your network. If you have used your own activation function which worked really well, please share it with us and we shall be happy to incorporate it into the list.",https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
The Art of Story Telling in Data Science and how to create data stories?,Learn everything about Analytics|Introduction|Table of Contents|The need for storytelling|How to create stories?|Types of Data and Suitable Charts|Storytelling during the steps of predictive modeling|Best Practices for Story Telling|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Fundamentals of Deep Learning  Activation Functions and When to Use Them?|8 Essential Tips for People starting a Career in Data Science|
Analytics Vidhya Content Team
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The idea of storytelling is fascinating; to take an idea or an incident, and turn it into a story. It brings the idea to life and makes it more interesting. This happens in our day to day life. Whether we narrate a funny incident or our findings, stories have always been the go-to to draw interest from listeners and readers alike.For instance; when we talk of how one of our friends got scolded by a teacher, we tend to narrate the incident from the beginning so that a flow is maintained.Lets take an example of the most common driving distractions by gender. There are two ways to tell this.The first is that I give you some statistics as follows:Another way to recreate similar statistics is this visual from kids4kars.org.Which one do you think tells a better story?The art of storytelling is simple and complex at the same time. Stories provoke thought and bring out insights that could not have been understood or explained before. Its often overlooked in data driven operations as we believe its a trivial task.
What we fail to understand is that the best stories not presented well end up being useless!In several firms, the first step towards analyzing anything is story-boarding it. Questions like why do we have to analyze it? what decisions can we make out of it? Sometimes, data alone tells such visual and intricate stories that we dont need to run complex correlations to confirm it.The best example of needing stories and visuals to explain data is the Anscombes Quartet. The Anscombes Quartet is a set of four datasets with very similar statistical summaries, but completely different when you visualize them.These are the four datasets used during the depiction of the Anscombes Quartet. If we look at mere numbers, we find that their summary statistics are almost identical.Lets see how they appear when we visualize them.Did you ever think these four quartets would have such varying visuals?To create a story or a plot is the first step to selling your ideas with a strong foot forward. Most people fail to think their stories through and cannot differentiate themselves from mediocrity.Let me take an example and guide you through the steps of creating stories.We will be exploring a dataset that has news headlines and details of every stock price from the NASDAQ 100 tech companies. The columns selected are as follows.1. Begin with a pen-paper approachVisually engaging presentations will inspire your audience, but they definitely need more work to be put in. One of the best presentations have been created on rough pages and tissue papers.Scripting down your ideas and flow before you start structuring your story is very essential to your final product.The single most important thing you can do to dramatically improve your analytics is to have a story to tell. A flow that you can generate can have a lot of friction in your end result.Aristotles classic five point plan that helps deliver strong impacts is:The way I structured my report was by involving plots that would give me a better understanding of my data.The first idea that I had was, how can I make better business decisions of stocks by using the data that I have?Involving a line graph would help me analyse trend lines of specific stock prices.As I can see, February 2016 has been a drop for all stocks. This would help me scrape news articles only from that period to identify what caused the drop. Now, how do I select which news source to scrape from?By identifying which news source reported most about a particular stock, we would have reason to believe that this is a good source for the specific stock.2. Dig deeper to identify the sole purpose of your story3. Use powerful headings4. Design a Road-Map5. Conclude with brevityNow that you have put forward all points of your story, your conclusion should be short and powerful. In my report, I mentioned small 3-4 liner summaries to conclude why to buy a particular stock.Let us see the common types of data we encounter and how to tell stories from those, by selecting the best fit charts.Commonly encountered types of data:1. Textual Data When data is found in this form, its usually good to be finding how often a word has been used or what the sentiment of the text is. Stories can be told best using this form of data.One of the best suited visualizations for textual data is the WordCloud. The wordcloud brings the more frequent ones to the center and enlarges them, giving us a clear picture of what the general idea of the text depicts.For example, the wordcloud in this article displayed above gives a representation of twitter dataset. It shows that love is the most frequent positive term used in the tweets.2. Mixed DataWhen our data consists of numeric or any other variety of formats, we need to know which ones are important and give us better insights from our dataset.The preferred visual for this kind of data can vary; here I will show you how to use facet grids for the data. I will be using the Titanic Passenger Data.As this plot shows us, females and first-class passengers tend to have a higher survival chance than men who are a part of the crew or lower boarding classes.Isnt that what had really happened on the Titanic?Another way to visualize this kind of data is by trying a multivariate plot. The dataset in use for this plot is the Car Performance and Specifications dataset.Here we can see how Cars that have a heavier built are slower than the ones with lighter bodies. Makes sense, right?3. Numeric DataWhen we encounter this kind of data, were usually looking for trends or lines that depict numbers. The visual that would suit numeric data best would be a line or a step graph.Here, we can very clearly see the rise of prices at a local attraction for adults and children. See how easy it is to see the growth at each year interval?4. StocksOne of the datasets that we also encounter are related to stocks. Stock market data is primarily a time series data of numeric values, but as a trader or an investor, I would like to understand each date and drop carefully.The most visually captivating charts in this regard is the Candlestick chart.
Here, we take the example of Teslas stocks. The candlestick charts can be used to maneuver across each date and see the lows and highs of stocks individually. This could help us take better investment decisions based on current or past market trends.As the graph shows us, February 2016 was a drop for Teslas stocks. We could now use this information to understand other market conditions and economic situations to make decisions about their stock.5. Geographic DataWhen we have data pertaining to specific locations and areas, we use maps to add clarity and meaning to our analysis.In this example, we can see how countries fared at and after the 2002 World Cup. Germany has scored the maximum number of goals, being one of the most dominant teams in world football ever since.Often, we would be questioned about how our stories and visuals can work or help when its time to create mathematical models. During all stages of predictive modeling, storytelling could be a vital addition to your analysis.Let us understand the basic steps involved in creating models out of our data and go through telling stories within them.1. Data ExplorationThe first step of model building is understanding your data. Ill give you instances and show you how you can explore your data without computing complex statistics.Lets consider a dataset on Wine Quality. This is the structure of the dataset is as followsHere, we can see the associated summary statistics of the dataset in use.So, if we need to see whether there is any correlation between alcohol volumes and wine qualities, how do we do it?We could either compute Pearsons r. It would help us in building a model, but would not help us in analyzing much.This shows a very strong correlation between Alcohol content and wine quality. But does it tell you anything else?Ideally, it doesnt. So, what does?Lets see how we can visualize these and tell a lot more from them.First, well begin by seeing how Wine Quality relates to Alcohol content.Here, we can see that the higher alcohol volumes relate to better wine qualities and it helps us come to a better understanding of our data. We can also spot outliers better in this scenario.Next, would you wonder how acid contents in your wine affect its quality?This would be one way to visualise the effects of acid. As the Violin Plot expands horizontally, it shows that there are higher numbers of data points within those areas.2. Feature VisualizingAfter you generate features, how do you see how well one is predicting?Graphs tell us how far away our predicted points are from our fitted line.Another example where we might have to visualize newly created visuals is the Principal Component Analysis. If you want to get an in-depth understanding of PCA, you can go through this article.This is the Iris dataset found in RStudio.When we run the principal component analysis on this dataset, we find these statistics.Although when we plot this, we find that the resulting visual is much more informative than the statistics.3. Model Creation and ComparisonComing to the model creation phase, we usually find the need to understand how our data is being fitted.This is a model that predicts whether the car should go fast or slow, based on the grade of the road and bumpiness.As you can see, the decision boundary clearly classifies most of the data but an accuracy of 88.21% doesnt tell much of a story. Here we can even see how far the misclassified points are from the decision boundary.We can also compare certain algorithms and techniques by looking at their decision boundaries as we did above.Another example using the Iris dataset is shown below.Here, theres not much information to derive valuable insights about our model.To learn more about Support Vector Machines, you can go through this article.On the other hand, this plot shows us a clear classification boundary where the Species separate from each other.Now that you know the scenarios where we can use story telling to explain our point, I will give you a few practical tips when you take this up on your own.Storytelling is more than what it has been used for. It can uncover insights from your data that you might have missed before. Relations between features and data that numbers can never clearly depict, can be shown using stories and charts.In this piece, weve elaborated on how stories are used in almost all avenues to explain a detail better. Starting from how theyre used in the steps of model building, weve gradually gone on to which charts suit specific data types well.Hope you had a great time reading the article. Eager to hear your data stories!",https://www.analyticsvidhya.com/blog/2017/10/art-story-telling-data-science/
8 Essential Tips for People starting a Career in Data Science,Learn everything about Analytics|Introduction|End Notes,"1. Choose the right role|2. Take up a Course and Complete it|3. Choose a Tool / Language and stick to it|4. Join a peer group|5. Focus on practical applications and not just theory|6. Follow the right resources|7. Work on your Communication skills|8. Network, but dont waste too much time on it!|Learn, engage,compete,andget hired!|Related Articles|The Art of Story Telling in Data Science and how to create data stories?|Introductory guide to Linear Optimization in Python (with TED videos case study)|
Faizan Shaikh
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Learning data science can be intimidating. Specially so, when you are just starting your journey. Which tool to learn  R or Python? What techniques to focus on? How much statistics to learn? Do I need to learn to code? These are some of the many questions you need to answer as part of your journey.That is why I thought that I would create this guide, which could help people starting in Analytics or Data Science. The idea was to create a simple, not very long guide which can set your path to learn data science. This guide would set a framework which can help you learn data science through this difficult and intimidating period.Just follow through these tips, and enroll in the below courses to get a head start in your data science journey!So lets get started!There are a lot of varied roles in data science industry. A data visualization expert, a machine learning expert, a data scientist, data engineer etc are a few of the many roles that you could go into. Depending on your background and your work experience, getting into one role would be easier than another role. For example, if you a software developer, it would not be difficult for you to shift into data engineering. So, until and unless you are clear about what you want to become, you will stay confused about the path to take and skills to hone.What to do, if you are not clear about the differences or you are not sure what should you become? I few things which I would suggest are:Here is a descriptive comparison done by Analytics Vidhya a few months back on what is it like being a Data Scientist vs Data Engineer vs Statistician. Im sure it will help you reach your decision.A point to keep in mind when choosing a role: dont just hastily jump on to a role. You should first understand clearly what the field requires and prepare for it.Now that you have decided on a role, the next logical thing for you is to put in dedicated effort to understand the role. This means not just going through the requirements of the role. The demand for data scientists is big so thousands of courses and studies are out there to hold your hand, you can learn whatever you want to. Finding material to learn from isnt a hard call but learning it may become if you dont put efforts.What you can do is take up a MOOC which is freely available, or join an accreditation program which should take you through all the twists and turns the role entails. The choice of free vs paid is not the issue, the main objective should be whether the course clears your basics and brings you to a suitable level, from which you can push on further.When you take up a course, go through it actively. Follow the coursework, assignments and all the discussions happening around the course. For example, if you want to be a machine learning engineer, you can take up Machine learning by Andrew Ng. Now you have to diligently follow all the course material provided in the course. This also means the assignments in the course, which are as important as going through the videos. Only doing a course end to end will give you a clearer picture of the field.As I mentioned before, it is important for you to get an end-to-end experience of whichever topic you pursue. A difficult question which one faces in getting hands-on is which language/tool should you choose?This would probably be the most asked question by beginners. The most straight-forward answer would be to choose any of the mainstream tool/languages there is and start your data science journey. After all, tools are just means for implementation; but understanding the concept is more important.Still the question remains, which would be a better option to start with? There are various guides / discussions on the internet which address this particular query. The gist is that start with the simplest of language or the one with which you are most familiar with. if you are not as well versed with coding, you should prefer GUI based tools for now. Then as you get a grasp on the concepts, you can get your hands on with the coding part.You can learn Python for Data Science here.Now that you know that which role you want to opt for and are getting prepared for it, the next important thing for you to do would be to join a peer group. Why is this important? This is because a peer group keeps you motivated. Taking up a new field may seem a bit daunting when you do it alone, but when you have friends who are alongside you, the task seems a bit easier.The most preferable way to be in a peer group is to have a group of people you can physically interact with. Otherwise you can either have a bunch of people over the internet who share similar goals, such as joining a Massive online course and interacting with the batch mates.Even if you dont have this kind of peer group, you can still have a meaningful technical discussion over the internet. There are online forums which give you this kind of environment. I will list a few of them:While undergoing courses and training, you should focus on the practical applications of things you are learning. This would help you not only understand the concept but also give you a deeper sense on how it would be applied in reality.A few tips you should do when following a course:Create your first Time Series Forecast using Python here.To never stop learning, you have to engulf each and every source of knowledge you can find. The most useful source of this information is blogs run by most influential Data Scientists. These Data Scientists are really active and update the followers on their findings and frequently post about the recent advancement in this field.Read about data science every day and make it a habit to be updated with the recent happenings. But there may be many resources, influential data scientists to follow, and you have to be sure that you dont follow the incorrect practices. So it is very important to follow the right resources.Here is a list of Data Scientists that you can follow.People dont usually associate communication skills with rejection in data science roles. They expect that if they are technically profound, they will ace the interview. This is actually a myth. Ever been rejected within an interview, where the interviewer said thank you after listening to your introduction?Try this activity once; make your friend with good communication skills hear your intro and ask for honest feedback. He will definitely show you the mirror!Communication skills are even more important when you are working in the field. To share your ideas to a colleague or to prove your point in a meeting, you should know how to communicate efficiently.Initially, your entire focus should be on learning. Doing too many things at initial stage will eventually bring you up to a point where youll give up.Gradually, once you have got a hang of the field, you can go on to attend industry events and conferences, popular meetups in your area, participate in hackathons in your area  even if you know only a little. You never know who, when and where will help you out!Actually, a meetup is very advantageous when it comes down to making your mark in the data science community. You get to meet people in your area who work actively in the field, which provides you networking opportunities along with establishing a relationship with them will in turn help you advance your career heavily. A networking contact might:The demand of data science is huge and employers are investing significant time and money in Data Scientists. So taking the right steps will lead to an exponential growth. This guide provides tips that can get you started and help you to avoid some costly mistakes.If you went through a similar experience in the past and want to share this with the community, do comments below!",https://www.analyticsvidhya.com/blog/2017/10/tips-people-starting-career-data-science/
Introductory guide to Linear Optimization in Python (with TED videos case study),"Learn everything about Analytics|Introduction|Table of Contents||Introduction to Linear Optimization|The Problem  Creating the Watch List for TED videos
|Step 1  Import relevant packages|Step 2  Create a dataframe for TED talks|Step 3  Set up the Linear Optimization Problem|Step 4  Convert the Optimization results into an interpretable format|End Notes","Share this:|Like this:|Related Articles|8 Essential Tips for People starting a Career in Data Science|25 Questions to test a Data Scientist on Image Processing|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Science & Machine Learning are being used by organizations to solve a variety of business problems today. In order to create a real business impact, an important consideration is to bridge the gap between the data science pipeline and business decision making pipeline.The outcome of data science pipeline is uaully predictions, patterns and insights from data (typically without any notion of constraints) but that alone is insufficient for business stakeholders to take decisions. Data science output has to be fed into the business decision making pipeline which involves some sort of optimization involving constraints and decision variables which model key aspects of the business.For example, if you are running a Super Market chain  your data science pipeline would forecast the expected sales. You would then take those inputs and create an optimised inventory / sales strategy.In this article, we will show one such example of Linear optimization for selecting which TED videos to watch.Among optimization techniques, Linear Optimization using the Simplex Method is considered one of the most powerful ones and has been rated as one of the Top 10 algorithms of the 20th century. As data science practitioners, it is important to have hands-on knowledge in implementing Linear Optimization and this blog post is to illustrate its implementation using Pythons PuLP package.To make things interesting & simpler to understand, we will learn this optimization technique by applying it on a practical, day-to-day problem. Having said that, what we learn is applicable to a variety of business problems as well.Note: This article assumes you have a basics knowledge of linear programming. You can go through this article if you want to review the topic.TED is a nonprofit devoted to spreading ideas. TED began in 1984 as a conference where Technology, Entertainment and Design converged, and today covers almost all topics  from science to business to global issues  in more than 100 languages. TED talks are delivered by experts passionate about work in their chosen domains and have a wealth of information.Now, for the purpose of this blog post, imagine a situation where one is interested to create their watch list of the most popular TED talks given their constraints (time that can be allotted to viewing and the number of talks). We will see how to implement the Python program to help us create the watchlist in the optimal manner.The code of the article can be found here. Screenshots from my Jupyter notebook are shown below:PuLP is a free open source software written in Python. It is used to describe optimisation problems as mathematical models. PuLP can then call any of numerous external LP solvers (CBC, GLPK, CPLEX, Gurobi etc) to solve this model and then use python commands to manipulate and display the solution. By default, CoinMP solver is bundled with PuLP.Dataset having all the TED talks (2550) is downloaded from Kaggle and read into a dataframe. A subset of relevant columns is selected and the resulting dataset has the following details  Index of the talk, Name of the talk, TED Event Name, Talk duration (in minutes), Number of Views (Proxy for Popularity of the talk)Start with defining the LP Object. The prob variable is created to contain the problem formulationStep 3.1: Create the decision variablesIterate over each row of the data frame to create the decision variables, such that each talk becomes one decision variable. Since each talk can either be selected or not selected as part of the final watch list, the decision variable is binary in nature (1=Selected, 0=Not Selected)Step 3.2: Define the Objective FunctionThe objective function is the sum over all rows of the views for each talk. The views serve as a proxy for the popularity of the talk and so in essence we are trying to maximize the views (popularity) by selecting appropriate talks (decision variables)Step 3.3: Define the ConstraintsIn the problem, we have 2 constraints:a) We only have fixed amount of total time that can be allocated to view the talksb) We dont want to view more than a certain number of talks to avoid information overloadStep 3.4: The Final Format (for problem formulation)The final format of the problem formulated is written out into a .lp file. This will list the objective function, the decision variables and the constraints imposed on the problem.Step 3.5: The Actual OptimizationThe actual optimization is a single line of code that calls prob.solve. Assert statement is inserted to ascertain whether an optimal result was obtained for the problem.The optimization results which indicates the specific decision variables (talks) that were selected to maximize the outcome has to be converted into a format of a watch list, as shown below:This article provides an example of utilizing Linear Optimization techniques available in Python to solve the everyday problem of creating video watch list. The concepts learned are also applicable in more complex business situations involving thousands of decision variables and many different constraints.Every data science practitioner needs to add Optimization techniques to their body of knowledge so that they can use advanced analytics to solve real world business problems and this article is intended to help you take the first step in that direction.Karthikeyan Sankaran is currently a Director at LatentView Analytics which provides solutions at the intersection of Business, Technology & Math to business problems across a wide range of industries. Karthik has close to two decades of experience in the Information Technology industry having worked in multiple roles across the space of Data Management, Business Intelligence & Analytics.This story was received as part of Blogathon contest on Analytics Vidhya. Karthikeyans entry was one of the winning entries in the competition.",https://www.analyticsvidhya.com/blog/2017/10/linear-optimization-in-python/
25 Questions to test a Data Scientist on Image Processing,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Introductory guide to Linear Optimization in Python (with TED videos case study)|25 Questions to test a Data Scientist on Support Vector Machines|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Extracting useful information from unstructured data has always been a topic of huge interest in the research community. One such example of unstructured data is an image, and analysis of image data has applications in various aspects of business.This skilltest is specially designed for you to test your knowledge on the knowledge on how to handle image data, with an emphasis on image processing. More than 300 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboardfor the participants who took the test.Here are some resources to get in depth knowledge in the subject.1) Match the following image formats to their correct number of channelsNone
A) RGB -> I, GrayScale-> III
B) RGB -> IV, GrayScale-> II
C) RGB -> III, GrayScale -> I
D) RGB -> II, GrayScale -> ISolution: CGrayscale images have one number per pixel, and are stored as an m  n matrix, whereas Color images have 3 numbers per pixel  red, green, and blue brightness (RGB)2) Suppose you have to rotate an image. Image rotation is nothing but multiplication of image by a specific matrix to get a new transformed image.For simplicity, we consider one point in the image to rotate with co-ordinates as (1, 0) to a co-ordinate of (0, 1), which of the following matrix would we have to multiply with?A)
B)
C)
D)Solution: CThe calculation of would be like this;3) [True or False] To blur an image, you can use a linear filterA) TRUE
B) FALSESolution: BBlurring compares neighboring pixels in a filter and smooth them. For this, you cannot use a linear filter.4) Which of the following is a challenge when dealing with computer vision problems?A) Variations due to geometric changes (like pose, scale etc)
B) Variations due to photometric factors (like illumination, appearance etc)
C) Image occlusion
D) All of the aboveSolution: DAll the above mentioned options are challenges in computer vision5) Suppose we have an image given below.Our task is to segment the objects in the image. A simple way to do this is to represent the image in terms of the intensity of pixels and the cluster them according to the values. On doing this, we got this type of structure.A) 1
B) 2
C) 3
D) 4Solution: CThree clusters will be formed; points in the circle, points in the square and the points excluding both of these objects6)In this image, you can find an edge labelled in the red region. Which form of discontinuity creates this kind of edge?A) Depth Discontinuity
B) Surface color Discontinuity
C) Illumination discontinuity
D) None of the aboveSolution: AThe chair and wall are far from each other, causing an edge in the image.7) Finite difference filters in image processing are very susceptible to noise. To cope up with this, which of the following methods can you use so that there would be minimal distortions by noise?A) Downsample the image
B) Convert the image to grayscale from RGB
C) Smooth the image
D) None of the aboveSolution: CSmoothing helps in reducing noise by forcing pixels to be more like their neighbours8) Consider and image with width and height as 100100. Each pixel in the image can have a color from Grayscale, i.e. values. How much space would this image require for storing?Note: No compression is done.A) 2,56,00,000
B) 25,60,000
C) 2,56,000
D) 8,00,000
E) 80,000
F) 8,000Solution: EThe answer will be 8x100x100 because 8 bits will be required to represent a number from 0-2569) [True or False] Quantizing an image will reduce the amount of memory required for storage.A) TRUE
B) FALSESolution: AThe statement given is true.10) Suppose we have a grayscale image, with most of the values of pixels being same. What can we use to compress the size of image?A) Encode the pixels with same values in a dictionary
B) Encode the sequence of values of pixels
C) No compression can be doneSolution: AEncoding same values of pixels will greatly reduce the size for storage11) [True or False] JPEG is a lossy image compression techniqueA) TRUE
B) FALSESolution: AThe reason for JPEG being a lossy compression technique is because of the use of quantization.12) Given an image with only 2 pixels and 3 possible values for each pixel, what is the number of possible image histograms that can be formed?A) 3
B) 6
C) 9
D) 12Solution: CThe permutations possible of the histograms would be 9.13) Suppose we have a 1D image with values asNow we apply average filter on this image of size 3. What would be the value of the last second pixel?A) The value would remain the same
B) The value would increase by 2
C) The value would decrease by 2
D) None of the above
Solution: A(8+5+2)/3 will become 5. So there will be no change.14) fMRI (Functional magnetic resonance imaging) is a technology where volumetric scans of the brain are acquired while the subject is performing some cognitive tasks over time. What is the dimensionality of fMRI output signals?A) 1D
B) 2D
C) 3D
D) None of the aboveSolution: D
The question itself mentions volumetric scans over time, so it would be a series of 3D scans15) Which of the following methods is used as a model fitting method for edge detection?A) SIFT
B) Difference of Gaussian detector
C) RANSAC
D) None of the aboveSolution: CRANSAC is used to find the best fit line in edge detection16)Suppose we have an image which is noisy. This type of noise in the image is called salt-and-pepper noiseA) TRUE
B) FALSESolution: AMedian filter technique helps reduce noise to a good enough extent17) If we convolve an image with the matrix given below, what would be the relation between the original and modified image?Solution: AI would suggest you to try this yourself and see the result!18) Which of the following is a correct way to sharpen an image?A)B)C)D) None of the aboveSolution: BOption B gives a correct way to sharpen an image19) Below given images are two operations performed on a signal. Can you identify which is which?Solution: ACorrelation and convolution are two different methods with give different result. Convolution defines how much the signals overlap, whereas correlation tries to find the relation between the signals20) [True or False] By using template matching along with cross correlation, you can build a vision system for TV remote controlA) TRUE
B) FALSESolution: AThis is a excellent example of cross correlation in computer vision. Refer paper Computer Vision for Interactive Computer Graphics, W.Freeman et al, IEEE Computer Graphics and Applications21) Suppose you are creating a face detector in the wild. Which of the following features would you select for creating a robust facial detector?1. Location of iris, eyebrow and chin
2. Boolean feature: Is the person smiling or not
3. Angle of orientation of face
4. Is the person sitting or standingA) 1, 2
B) 1, 3
C) 1, 2, 3
D) 1, 2, 3, 4Solution: BOptions 1, 3 would be relevant features for the problem, but 2, 4 may not be22) Which of the following is example of low level feature in an image?A) HOG
B) SIFT
C) HAAR features
D) All of the aboveSolution: DAll the above are examples of low-level features23) In RGBA mode of color representation, what does A represent?A) Depth of an image
B) Intensity of colors
C) Opacity of an image
D) None of the aboveSolution: COpacity can be mentioned by introducing it as a fourth parameter in RGB24) In Otsu thresholding technique, you remove the noise by thresholding the points which are irrelevant and keeping those which do not represent noise.In the image given, at which point would you threshold on?A) A
B) B
C) C
D) DSolution: BLine B would catch most of the noise in the image.25) Which of the following data augmentation technique would you prefer for an object recognition problem?`
A) Horizontal flipping
B) Rescaling
C) Zooming in the image
D) All of the aboveSolution: DAll the mentioned techniques can be used for data augmentation.Below is the distribution of the scores of the participants:You can access the scores here. More than a hundered people participated in the skill test and the highest score obtained was a 22.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.",https://www.analyticsvidhya.com/blog/2017/10/image-skilltest/
25 Questions to test a Data Scientist on Support Vector Machines,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution||End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|25 Questions to test a Data Scientist on Image Processing|Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R)|
Ankit Gupta
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You can think of machine learning algorithms as an armory packed with axes, sword and blades. You have various tools, but you ought to learn to use them at the right time. As an analogy, think of Regression as a sword capable of slicing and dicing data efficiently, but incapable of dealing with highly complex data. On the contrary, Support Vector Machines is like a sharp knife  it works on smaller datasets, but on them, it can be much more stronger and powerful in building models.This skilltest was specially designed for you to test your knowledge on SVM techniques and its applications. More than 550 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboardfor the participants who took the test.Here are some resources to get in depth knowledge in the subject.Question Context: 1  2Suppose you are using a Linear SVM classifier with 2 class classification problem. Now you have been given the following data in which some points are circled red that are representing support vectors.1) If you remove the following any one red points from the data. Does the decision boundary will change? A) Yes
B) NoSolution: AThese three examples are positioned such that removing any one of them introduces slack in the constraints. So the decision boundary would completely change.2) [True or False] If you remove the non-red circled points from the data, the decision boundary will change?A) True
B) FalseSolution: BOn the other hand, rest of the points in the data wont affect the decision boundary much.3) What do you mean by generalization error in terms of the SVM?A) How far the hyperplane is from the support vectors
B) How accurately the SVM can predict outcomes for unseen data
C) The threshold amount of error in an SVMSolution: BGeneralisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.4) When the C parameter is set to infinite, which of the following holds true?A) The optimal hyperplane if exists, will be the one that completely separates the data
B) The soft-margin classifier will separate the data
C) None of the aboveSolution: AAt such a high level of misclassification penalty, soft margin will not hold existence as there will be no room for error.5) What do you mean by a hard margin?A) The SVM allows very low error in classification
B) The SVM allows high amount of error in classification
C) None of the aboveSolution: AA hard margin means that an SVM is very rigid in classification and tries to work extremely well in the training set, causing overfitting.6) The minimum time complexity for training an SVM is O(n2). According to this fact, what sizes of datasets are not best suited for SVMs?A) Large datasets
B) Small datasets
C) Medium sized datasets
D) Size does not matterSolution: ADatasets which have a clear classification boundary will function best with SVMs.7) The effectiveness of an SVM depends upon:A) Selection of Kernel
B) Kernel Parameters
C) Soft Margin Parameter C
D) All of the aboveSolution: DThe SVM effectiveness depends upon how you choose the basic 3 requirements mentioned above in such a way that it maximises your efficiency, reduces error and overfitting.8) Support vectors are the data points that lie closest to the decision surface.A) TRUE
B) FALSESolution: AThey are the points closest to the hyperplane and the hardest ones to classify. They also have a direct bearing on the location of the decision surface.9) The SVMs are less effective when:A) The data is linearly separable
B) The data is clean and ready to use
C) The data is noisy and contains overlapping pointsSolution: CWhen the data has noise and overlapping points, there is a problem in drawing a clear hyperplane without misclassifying.10) Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?A) The model would consider even far away points from hyperplane for modeling
B) The model would consider only the points close to the hyperplane for modeling
C) The model would not be affected by distance of points from hyperplane for modeling
D) None of the aboveSolution: BThe gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well.11) The cost parameter in the SVM means:A) The number of cross-validations to be made
B) The kernel to be used
C) The tradeoff between misclassification and simplicity of the model
D) None of the aboveSolution: CThe cost parameter decides how much an SVM should be allowed to bend with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. It is also simply referred to as the cost of misclassification.12)Suppose you are building a SVM model on data X. The data X can be error prone which means that you should not trust any specific data point too much. Now think that you want to build a SVM model which has quadratic kernel function of polynomial degree 2 that uses Slack variable C as one of its hyper parameter. Based upon that give the answer for following question.What would happen when you use very large value of C(C->infinity)?Note: For small C was also classifying all data points correctly
A) We can still classify data correctly for given setting of hyper parameter C
B) We can not classify data correctly for given setting of hyper parameter C
C) Cant Say
D) None of theseSolution: AFor large values of C, the penalty for misclassifying points is very high, so the decision boundary will perfectly separate the data if possible.13) What would happen when you use very small C (C~0)?A) Misclassification would happen
B) Data will be correctly classified
C) Cant say
D) None of theseSolution: AThe classifier can maximize the margin between most of the points, while misclassifying a few points, because the penalty is so low.14) If I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on validation set, what should I look out for?A) Underfitting
B) Nothing, the model is perfect
C) OverfittingSolution: CIf were achieving 100% training accuracy very easily, we need to check to verify if were overfitting our data.15) Which of the following are real world applications of the SVM?A) Text and Hypertext Categorization
B) Image Classification
C) Clustering of News Articles
D) All of the aboveSolution: DSVMs are highly versatile models that can be used for practically all real world problems ranging from regression to clustering and handwriting recognitions.Question Context: 16  18Suppose you have trained an SVM with linear decision boundary after training SVM, you correctly infer that your SVM model is under fitting.16) Which of the following option would you more likely to consider iterating SVM next time?A) You want to increase your data points
B) You want to decrease your data points
C) You will try to calculate more variables
D) You will try to reduce the featuresSolution: CThe best option here would be to create more features for the model.17) Suppose you gave the correct answer in previous question. What do you think that is actually happening?1. We are lowering the bias
2. We are lowering the variance
3. We are increasing the bias
4. We are increasing the variance
A) 1 and 2
B) 2 and 3
C) 1 and 4
D) 2 and 4Solution: C
Better model will lower the bias and increase the variance18) In above question suppose you want to change one of its(SVM) hyperparameter so that effect would be same as previous questions i.e model will not under fit? A) We will increase the parameter C
B) We will decrease the parameter C
C) Changing in C dont effect
D) None of theseSolution: AIncreasing C parameter would be the right thing to do here, as it will ensure regularized model19) We usually use feature normalization before using the Gaussian kernel in SVM. What is true about feature normalization?1. We do feature normalization so that new feature will dominate other
2. Some times, feature normalization is not feasible in case of categorical variables
3. Feature normalization always helps when we use Gaussian kernel in SVMA) 1
B) 1 and 2
C) 1 and 3
D) 2 and 3Solution: BStatements one and two are correct.Question Context: 20-22Suppose you are dealing with 4 class classification problem and you want to train a SVM model on the data for that you are using One-vs-all method. Now answer the below questions?20) How many times we need to train our SVM model in such case?A) 1
B) 2
C) 3
D) 4Solution: DFor a 4 class problem, you would have to train the SVM at least 4 times if you are using a one-vs-all method.21) Suppose you have same distribution of classes in the data. Now, say for training 1 time in one vs all setting the SVM is taking 10 second. How many seconds would it require to train one-vs-all method end to end?A) 20
B) 40
C) 60
D) 80Solution: B
It would take 104 = 40 seconds22) Suppose your problem has changed now. Now, data has only 2 classes. What would you think how many times we need to train SVM in such case?A) 1
B) 2
C) 3
D) 4Solution: ATraining the SVM only one time would give you appropriate resultsQuestion context: 23  24Suppose you are using SVM with linear kernel of polynomial degree 2, Now think that you have applied this on data and found that it perfectly fit the data that means, Training and testing accuracy is 100%.23) Now, think that you increase the complexity(or degree of polynomial of this kernel). What would you think will happen?A) Increasing the complexity will overfit the data
B) Increasing the complexity will underfit the data
C) Nothing will happen since your model was already 100% accurate
D) None of theseSolution: AIncreasing the complexity of the data would make the algorithm overfit the data.24) In the previous question after increasing the complexity you found that training accuracy was still 100%. According to you what is the reason behind that?1. Since data is fixed and we are fitting more polynomial term or parameters so the algorithm starts memorizing everything in the data
2. Since data is fixed and SVM doesnt need to search in big hypothesis spaceA) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the given statements are correct.25) What is/are true about kernel in SVM?1. Kernel function map low dimensional data to high dimensional space
2. Its a similarity functionA) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the given statements are correct.Below is the distribution of the scores of the participants:You can access the scores here. More than 350 people participated in the skill test and the highest score obtained was a 25.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.",https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/
Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R),Learn everything about Analytics|Introduction|Table of Contents|Setting up the System|Trading with Bollinger Bands|Analyzing the Volatility of Bank Stocks||Prediction of Stock Prices|End Notes,"What are Bollinger Bands?|Aspects of Bollinger Bands|Keep it Clean!|Identification of Patterns|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|25 Questions to test a Data Scientist on Support Vector Machines|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)|
Guest Blog
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","|Code Break up for Quandl and Quandl API key,|1. Signal: W  Bottoms|2. Signal: M-Tops",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Finding underlying patterns and taking decisions is very critical in Stock market. The same skill can be applied to many parallel domains. For example, I met some one who was doing the same thing with Cryptocurrency recently. Risk & Unemployment prediction in banks, customer churn in telecom and spend analysis are all examples of similar problems.That is why I decided to create this series of articles. By following this series, you will understand some of the techniques used in stock market. You can also apply them to the parallel domains I mentioned before.In the last article (Part I) , we started with descriptive analysis for comparison on stocks. In this post, we will emphasize on identifying patterns in order to know how a stock behaves. This behavior, as you will see later on, is very important for stock trading. In the latter part of the article, I will show how to predict stock prices using the conventional ARIMA (Auto-Regressive intensive Moving Average Method) methodology from Time Series Analysis and Regression Model.So lets get on with it!I will mention below the packages necessary to get on hands on in this article. Make sure you have them set up in your system before you continue.Perhaps the most important thing when you get into stock market trading is to know what Bollinger Bands are. In this section, I will mention what they are and how they were discovered.The Bollinger Band was introduce by John Bollinger in 1980s. These Bands depict the volatility of stock as it increases or decreases. The bands are placed above and below the moving average line of the stocks. The wider the gap between the bands, higher is the degree of volatility.On the other hand, as the width within the band decreases, lower is the degree of volatility of the stock. At times, the width within the band is constant over a period of time, which shows the constant behavior of a certain stock over that period of time.There are three lines in the Bollinger Band,Note: SMA is Simple Moving Average, Standard Deviation, K and N period is usually set at 20 days. The upper and lower band are placed 2 units above and below respectively.Below image is the typical example for Bollinger Band. This shows the volatility of Axis Bank stock for the period of 1 year from 1st September, 2016 to 1st September, 2017. The gap was higher in the months of September till December.In this section, we will discuss few aspects of Bollinger Band. This information can be used in different stock trading.The study will discuss the above points along with the identification of popular patterns like W  Bottoms & M  Tops in Bollinger band.We will keep the data clean with tidyverse. In this section we will first download the data with the help of Quandl package and then manipulate the dataframe with tidyverse to get our desired dataset,If you havent gone through this in my previous post onComparative Stock Analysis Vol-I , lets setup up the Quandl APIThere are patterns which are usualy seen in stock market data. These patterns (or signals) help us identify the behavior of stocks. Let us quickly understand the two most popular ones (W-Bottoms and M-Tops)A W-Bottom forms in a downtrend and involves two reaction lows. In particular, Bollinger looks for W-Bottoms where the second low is lower than the first but holds above the lower band. There are four steps to confirm Bollinger W  Bottoms,Below image is the W-Bottoms identification for BOB (Bank of Baroda). Both of the W-Bottoms are followed by strong northward move in February and May, 2017 respectively.An M-Top is similar to a double top. M-Tops are reversal signals from upward trend into a downward trend. The first high can be higher or lower than the second high. Initially there is a wave higher, which gets close to or move above the upper band. Then price will move downward to middle band and then continues northward journey, might or might not touch the upper band (at times it goes above the previous high) and then does not close above the upper band.Bollinger suggests looking for signs of non-confirmation when a security is making new highs.A non-confirmation occurs with three steps.Below is the image for MTOPS signal for SBI (State Bank Of India) stock in NSE over the period of one year starting from 1st sept, 2016 to 1st sept, 2017. Each of the MTops are followed by decline in prices in the months of Nov-Dec, May and August.If you want to get more information on the Bollinger Band and related identification patterns. Below are the links to the resources,Let us visualize the volatility (gap between the upper and lower band) and also try to identify the patterns / signals in our six selected bank stocks.In this section, we will predict the prices for two selected bank PNB and Axis Bank. In stock market, generally the prices are dynamic and depends on various factors like news, weather, public policy, interest rate. It is difficult to predict the stock price behavior as it depends on lots of factor. In order to get more accuracy in prediction, weve used two different approach to come to prediction.In the last post, we have seen that the stock prices is also dependent on the traded quantity, but direction can be either ways. In our analysis, we will take consideration of these movements. We will also analyze the random part of the stock price movement, so called white noise and will include in our prediction model.There is also available study on white noise on Analytics Vidhya by Tavish Srivastava.The following points are the steps to arrive at PredictionsNote: ggplot shows the prediction and actual prices. Predcition prices has the band for lower and upper limit.In this article, I have focused on Predictive Analysis of bank stocks. I have summarized a bit on Bollinger Bands, which probably is the most important topic in stock analysis. I have also walked you through the volatility of bank stocks and ways to see through this volatility.This ends our journey of comparative analysis of stock market data. I hope it will help you to make your mark in the world of stocks. Good Luck!Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning.",https://www.analyticsvidhya.com/blog/2017/10/comparative-stock-market-analysis-in-r-using-quandl-tidyverse-part-i/
Tutorial to deploy Machine Learning models in Production as APIs (using Flask),Learn everything about Analytics|Introduction|Table of Contents|1. Options to implement Machine Learning models|2. What are APIs?|3. Python Environment Setup & Flask Basics|4. Creating a Machine Learning Model|5. Saving Machine Learning Model : Serialization & Deserialization|6. Creating an API using Flask|End Notes,"About the Author|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Bollinger Bands and their use in Stock Market Analysis (using Quandl & tidyverse in R)|How to build your first Machine Learning model on iPhone (Intro to Apples CoreML)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I remember the initial days of my Machine Learning (ML) projects. I had put in a lot of efforts to build a really good model. I took expert advice on how to improve my model, I thought about feature engineering, I talked to domain experts to make sure their insights are captured. But, then I came across a problem!How do I implement this model in real life? I had no idea about this. All the literature I had studied till now focussed on improving the models. But I didnt know what was the next step.This is why, I have created this guide  so that you dont have to struggle with the question as I did. By end of this article, I will show you how to implement a machine learning model using Flask framework in Python.Most of the times,the real use of our Machine Learning model lies at the heart of a product  that maybe a small component of an automated mailer system or a chatbot. These are the times when the barriers seem unsurmountable.For example, majority of ML folks use R / Python for their experiments. But consumer of those ML models would be software engineers who use a completely different stack. There are two ways via which this problem can be solved:In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software.You can read this article to understand why APIs are a popular choice amongst developers:Majority of the Big Cloud providers and smaller Machine Learning focussed companies provide ready-to-use APIs. They cater to the needs of developers / businesses that dont have expertise in ML, who want to implement ML in their processes or product suites.One such example of Web APIs offered is the Google Vision APIAll you need is a simple REST call to the API via SDKs (Software Development Kits) provided by Google. Click here to get an idea of what can be done using Google Vision API.Sounds marvellous right! In this article, well understand how to create our own Machine Learning API using Flask, a web framework in Python.NOTE:Flask isnt the only web-framework available. There is Django, Falcon, Hug and many more. For R, we have a package called plumber.Viola! You wrote your first Flask application. As you have now experienced with a few simple steps, we were able to create web-endpoints that can be accessed locally.Using Flask, we can wrap our Machine Learning models and serve them as Web APIs easily. Also, if we want to create more complex web applications (that includes JavaScript *gasps*) we just need a few modifications.To follow the process on how we ended up with this estimator, referthis notebookWell create a pipeline to make sure that all the preprocessing steps that we do are just a single scikit-learn estimator.To search for the best hyper-parameters (degree for Polynomial Features & alpha for Ridge), well do a Grid Search:Our pipeline is looking pretty swell & fairly decent to go the most important step of the tutorial: Serialize the Machine Learning ModelIn computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment.In Python, pickling is a standard way to store objects and retrieve them as their original state. To give a simple example:When we load the pickle back:We can save the pickled object to a file as well and use it. This method is similar to creating .rda files for folks who are familiar with R Programming.NOTE: Some people also argue against using pickle for serialization(1). h5py could also be an alternative.We have a custom Class that we need to import while running our training, hence well be using dill module to packup the estimator Class with our grid object.It is advisable to create a separate training.py file that contains all the code for training the model (See here for example).So our model will be saved in the location above. Now that the model is pickled, creating a Flask wrapper around it would be the next step.Before that, to be sure that our pickled file works fine  lets load it back and do a prediction:Since, we already have the preprocessing steps required for the new incoming data present as a part of the pipeline, we just have to run predict(). While working with scikit-learn, it is always easy to work with pipelines.Estimators and pipelines save you time and headache, even if the initial implementation seems to be ridiculous. Stitch in time, saves nine!Well keep the folder structure as simple as possible:There are three important parts in constructing our wrapper function, apicall():HTTP messages are made of a header and a body. As a standard, majority of the body content sent across are in json format. Well be sending (POST url-endpoint/) the incoming data as batch to get predictions.(NOTE: You can send plain text, XML, csv or image directly but for the sake of interchangeability of the format, it is advisable to use json)Once done, run: gunicorn --bind 0.0.0.0:8000 server:appLets generate some prediction data and query the API running locally at https:0.0.0.0:8000/predictThere are a few things to keep in mind when adopting API-first approach:Next logical step would be creating a workflow to deploy such APIs out on a small VM. There are various ways to do it and well be looking into those in the next article.Code & Notebooks for this article: pratos/flask_apiSources & Links:Prathamesh Sarang works as a Data Scientist at Lemoxo Technologies. Data Engineering is his latest love, turned towards the *nix faction recently. Strong advocate of Markdown for everyone.",https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/
How to build your first Machine Learning model on iPhone (Intro to Apples CoreML),"Learn everything about Analytics|Introduction|Table of Contents|1. What is CoreML?|2. Setting up the System
|3. Case Study: Implementing a spam message classifier for iPhone|4. Pros and Cons of CoreML|End Notes","A little context for CoreML|Enter CoreML|What else does CoreML provide?|Converting your machine learning model into CoreML format|Integrating the model with our app|Pros|Cons|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Tutorial to deploy Machine Learning models in Production as APIs (using Flask)|Why we are so excited about DataHack Summit 2017?|
Mohd Sanad Zaki Rizvi
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",What is mlmodel?|About Spam Collection dataset |Build a basic model|What happened here?|Downloading the project|Adding a pre-trained model into your app|Compiling the model|Using the model in code|But why is tfidf() required?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The data scientist in me is living a dream  I can see top tech companies coming out with products close to the area I work on.If you saw the recent Apple iPhone X launch event, iPhone X comes withsome really cool features like FaceID, Animoji, Augmented Realityout of box, which use the power of machine learning. The hacker in me wanted to get my hands dirty and figure out what it takes to build a system like that? When probed further, the answer was CoreML which is Apples official machine learning kit for developers. It works with iPhone, Macbook, Apple TV, Apple watch, in short, every Apple device.Another interesting announcement was that Apple has designed a custom GPU and an advanced processing chip A11 Bionic with a neural engine optimised for Machine Learning in the latest iPhones. With a powerful computing engine at its core, the iPhone will now be open to new avenues of machine learning and the significance of CoreML will only rise in the coming days.By end of this article, you will see what Apple CoreML is and why it is gaining the momentum. We will also look into implementation details of CoreML by building a message spam classification app for iPhone. We will finish off the article by objectively looking at pros and cons of the same. Apple launched CoreML this year in their annual developer conference WWDC(which is equivalent of Google I/O conference) with a lot of hype. In order to better understand CoreMLs role, we have to know a bit of context.Interestingly this is not the first time that Apple has come out with a framework for machine learning on its devices. Last year it launched a bunch of libraries for the same:The point of difference was, one was optimized for CPU while the other was optimized for the GPU. The reason for this is sometimes during inference the CPU can be faster than the GPU. While during training almost every time GPU is faster.These multiple frameworks created a lot of confusion among developers and since they were quite close to the hardware(for high performance), they were difficult to program in.CoreML provides another layer of abstraction over the previous two libraries and gives an easy interface to achieve the same level of efficiency. Another benefit is that CoreML takes care of context switching between the CPU and GPU itself while your app is running. That is to say, for example, you have a memory heavy task that involves dealing with text (natural language processing), CoreML will automatically run it on the CPU whereas if you have compute heavy tasks like image classification, it will use the GPU. If you have both the functionalities in your app, it will also take care of that automatically so that you can get best of both the worlds.CoreML also comes with three libraries built on top of it :All of the above libraries, again are very easy to use and provide a simple interface to do a bunch of tasks. With the above libraries, the final structure of CoreML would look something like thisNotice that the above design gives a nice modular structure to your iOS application. You have different layers for different tasks and you can make use of them in a variety of ways (for example, using NLP with image classification in your app). You can read more about these libraries here : Vision, Foundation and GameplayKit. Well, that was enough theory for now, it is time to get our hands dirty!To make full use of the CoreML , you need the following requirements set up:Once you log in, you will have to verify your apple ID. You will receive the notification regarding the same on the device that is registered with your apple ID.Select Allow and type the given 6 digit passcode in the website Once you do this step, you will be shown a download option and you can download Xcode from there.Now that we have set up our system and all ready lets move on to the implementation part!We will be looking at two important ways to utilize the power of CoreML by building them. Lets start then!Now, one of the strengths of CoreML or rather should I say wise decision of its creators was to support conversion of trained machine learning models built in other popular frameworks like sklearn, caffe, xgboost etc. This didnt alienate the data science community from trying out CoreML because they can experiment, train their models in their favourite environment and then simply import it to use in their iOS/MacOS app. The following are the frameworks that CoreML supports right out of the box:In order to make the conversion process simple, Apple designed its own open format for representing cross framework machine learning models called mlmodel. This model file contains a description of the layers in your model, the inputs and outputs, the class labels, and any preprocessing that needs to be done on the data. It also contains all the learned parameters (the weights and biases).The conversion flow looks like this :For this example, we will be building a spam message classifier in sklearn and then port the same model to CoreML.The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-encoded messages, tagged according to being legitimate (ham) or spam. You can download the dataset from here.We will be building a basic model using LinearSVC in sklearn. Also, I have used TF-IDF on the text of the message as a feature for the model. TF-IDF is a technique used in natural language processing that helps classify documents based on words that uniquely identify a document over other. If you would like to learn more about NLP and tf-idf, you can read this article. This would be the code for that: That builds our model. Lets test it with a spam message,Interesting, our model works well! lets add some cross-validation too,Now that we have built our model, we need to port it to .mlmodel format in order to make it compatible with CoreML. We will use the coremltools package that we installed earlier for that. The following code would convert our model into .mlmodel formatWe first import the coremltools package in python. Then we use one of the converters to convert our model, in this case, we used converters.sklearn because we have to convert a model built in sklearn. We then pass the model object, input variable name, and the output variable name in .convert(). We then set the parameters of the model to add more info about the inputs, outputs and finally call .save() to save our model file.When you double click on the model file, it should open in a Xcode window. As you can see, the model file shows details about the type of model, its inputs, and outputs, their types etc. I have highlighted all this information in the above figure. You can match the description with the ones we provided while converting to .mlmodel. That is how easy it is to import your model into CoreML. Now your model is into Apple ecosystem, thats when the real fun starts!Note: The complete code file for this step can be found here. Read more about coremltools here and different types of converters available here.Now that we have trained our model and ported it to CoreML, let us use that model and build a spam classifier app for iPhone!We would be running our app on a simulator. A simulator is a software that shows how an app will look and work as if it was really running on the phone. This saves a lot of time because we can experiment with our code and fix all bugs before trying the app on an actual phone. Have a look at what the end product would be like:I have already built a basic UI for our app and it is available on GitHub. Use the following commands to get it up and running:This will open our project using Xcode. I have highlighted three main regions in the Xcode window :Lets first run our app and see what happens. Click on the play button on the top left that will run our app in the simulator. Try typing some text in the box and clicking on the Predict button. What happens?For now, our app doesnt do much it just prints whatever has been typed in the box. This bit is fairly easy, Here is the whole process for reference :Before we can start making inferences from our model, we need to tell Xcode to compile the model during the build stage. For that follow the following steps:Now every time we run our app, Xcode will compile our machine learning model so that it can be used for making predictions.Any application that is to be developed for an Apple device is programmed in swift. For following this tutorial you dont need to learn swift but if afterward it interests you and you want to go deeper, you can follow this tutorial.The above code checks whether the user has entered any message in the text box. If he has, it calculates the tfidf of the text by calling a function tfidf(). It then creates an object of our SpamMessageClassifier and then calls the .prediction() function. This is equivalent of .predict() in sklearn. It then displays an appropriate message based on the prediction.Remember that we trained our model based on tf-idf representation of text so our model expects the input in the same format. Once we get the message entered in the text box we are calling tfidf() function on it to do the same. Lets write code for it, copy the following code just below the predictSpam() function:The above code finds the tfidf representation of the message entered in the text box for that it reads the original dataset file SMSSpamCollection.txt and returns the same. Once you save the program and re-run the simulator, your app should be working fine now.Like every library in development, it has its pros and cons. Let us state them explicitly.In this article, we learned more about CoreML and its application in building a machine learning app for iPhone. CoreML is a relatively new library and hence has its own share of pros and cons. A very useful feature provided here is it runs on the device locally thus giving more speed and providing data privacy. At the same time, it cant be thought of as a full-fledged data scientist friendly library yet. We will have to wait and see how does it evolve in the coming releases.For those of you who got stuck at some step, all of the code for this article is available on GitHub.Also, if you want to explore CoreML in more depth these are some of the resources:",https://www.analyticsvidhya.com/blog/2017/09/build-machine-learning-iphone-apple-coreml/
Why we are so excited about DataHack Summit 2017?,Learn everything about Analytics|The DataHack Summit philosophy|Speakers @ DataHack Summit 2017|Talks @ DataHack Summit 2017|Areas of Focus|Buy your Tickets before they vanish!,"Share this:|Like this:|Related Articles|How to build your first Machine Learning model on iPhone (Intro to Apples CoreML)|Introduction to Pseudo-Labelling : A Semi-Supervised learning technique|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently I attended a Data Science conference  full with celebrated & experienced speakers, nice looking agenda having multiple tracks and attended by a large number of industry professionals.What I took away from the conference was 2 days of networking, bunch of exchanged visiting cards and may be one good session which was useful for the data scientist inside me to some degree. Most of the sessions talked about Analytics / Data Science from a very high level to be useful for me. The same use cases  Alexa using Deep Learning or Target being able to predict a pregnancy in advance or Google driverless cars being slapped in sessions after sessions!This is not the first time this happened to me  this actually happens almost every time I attend an event like this. I know there are many more practitioners who feel the same about Data Science events in India.This is exactly what we will change in DataHack Summit 2017 in Bengaluru November 9  11, 2017DataHack Summit (DHS) is a conference created for thedata science practitioner. It is a conference which celebrates the awesome work being done by data scientists across the globe and showcases it to the rest of the community. It is a conference where we talk data science, we breathe data science and we experience data science like never before.DataHack Summit is a festival for those who dont see numbers as just numbers. It is a festival for those who see numbers as designs and patterns and trends, those who see and appreciate this art of dealing with data.DHS 2017 aims to show the bleeding edge, the horizon and the impact of data science to the professionals who perform it every day. It aims to inspire you with the new tools, techniques and applications by bringing the best in data science together.Our philosophy reflect directly in our speakers. All the speakers are Data Science practitioners and leaders.These are the people who have built scalable data science solutions and have solved real life problems using cutting edge tools and techniques. The spread of speakers include Dr. Kirk D. Borne who brings 30 years of data science wisdom to young Axel De Romblay, talking about Automated Machine Learning and the open source project he created.Check out the latest speakers for yourself on our speakers page.Every session, every talk, every workshop in DataHack Summit will be created with the same diligence you see in our articles. Hands on, practical knowledge delivered in simple and exciting manner.We will not have a session unless Analytics Vidhya team sees value in that session for themselves.We will make sure that you walk away with practical knowledge and industry use case when you come out of DataHack Summit 2017.Following will be the areas of focus of discussions at DataHack Summit 2017:The detailed agenda would be out in a few days.We have to thank our community for such an awesome response. The workshop tickets are sold out already. The regular tickets will sell out shortly as well. Make sure you book your tickets before they vanish.If you have any questions / suggestions, feel free to reach out to us at [emailprotected]See you at the DataHack Summit 2017!",https://www.analyticsvidhya.com/blog/2017/09/excited-datahack-summit-2017/
Introduction to Pseudo-Labelling : A Semi-Supervised learning technique,"Learn everything about Analytics|Introduction|Table of Contents|1. What is Semi-Supervised Learning (SSL) ?|2. How Unlabelled data can help?|3. Introduction Pseudo Labeling|4. Implementation of SSL
|5. Dependence of Sampling Rate|6. Applications of SSL||End Notes","1. Multimodal semi-supervised learning for image classification|2.SSL for detecting human trafficking|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Why we are so excited about DataHack Summit 2017?|6 Common Probability Distributions every data science professional should know|
Shubham Jain
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have made huge progress in solving Supervised machine learning problems. That also means that we need a lot of data to build our image classifiers or sales forecasters. The algorithms search patterns through the data again and again.But, that is not how human mind learns. A human brain does not require millions of data for training with multiple iterations of going through the same image for understanding a topic. All it needs is a few guiding points to train itself on the underlying patterns. Clearly, we are missing something in current machine learning approaches.Thankfully, there is a line of research which specifically caters to this question. Can we build a system capable of requiring minimal amount of supervision which can learn majority of the tasks on its own. In this article, I would like to cover one such technique called pseudo-labelling. I will give an intuitive explanation of what pseudo-labelling is and then provide a hands-on implementation of it.Are you ready?Note: I assume you have clarity on the basic topics of machine learning. If not, I would suggest you to go througha basic machine learning article first and then come back to this one.Lets say, we have a simple image classification problem. So, our training data consists of two labelled images as shown below.So, we need to classify images of eclipse from the non-eclipse images. But, the problem is that we need to build our model on just a training set of just two images.Therefore, in order to apply any supervised learning algorithm we need more data to build a robust model. To solve this purpose, we find a simple solution that we download some images from the web to increase our training data.But, for the supervised approach we also need labels for these images. So, we manually classify each image into a category as shown below.After running supervised algorithm on this data, our model will definitely out-perform the model just containing two images in the training data.But this approach is only valid for small purposes because human annotation to a large dataset can be very hard and expensive.So, to solve these type of problems, we define a different type of learning known as semi-supervised learning, which is used both labelled data (supervised learning) and unlabelled data (unsupervised learning).Source: linkTherefore, let us understand how unlabelled data can help to improve our model.Consider a situation as shown below.
You have only two data points belonging to two different categories, and the line drawn is the decision boundary of any supervised model.Now, lets say we add some unlabelled data to this data as shown in the image below.Images source: linkIf we notice the difference between the above two images, you can say that after adding unlabelled data, the decision boundary of our model has become more accurate.So, the advantage of using unlabelled data are:Now, we have a basic understanding that what is semi-supervised learning. There are different techniques of applying SSL, in this article we will try to understand one such technique known as Pseudo Labeling.In this technique, instead of manually labeling the unlabelled data, we give approximate labels on the basis of the labelled data. Lets make it simpler by breaking into steps as shown in the image below.
Source: linkI suppose, you understood the steps mentioned in the above image. So, the final model trained in the third step is used for the final predictions on the test data.For better understanding, I always prefer understanding a concept by its implementation on a real world problem.Here, we will be using Big Mart Sales problem from AV data hack platform. So, lets get start by downloading the train and test file present in the data section.So, lets get start by importing the basic libraries.Now, lets read train and test file that we have downloaded and do some basic preprocessing in order to form modelling.Starting with different supervised learning algorithm, let us check which algorithm gives us the best results.We can see XGB gives us the best model performance. Note here, I have not tuned parameter of any algorithm for the simplicity of this article.Now, lets us implement Pseudo-labelling, for this purpose I will be using test data as the unlabelled data. This look quite complex, but you need not to worry about this as it is the same implementation of the method we learned above. So, copy the same code every time you need to perform pseudo labeling.So, now lets us now check the results of pseudo labeling on the dataset.In this case, we a get rmse value which comes out to lesser than any of the supervised learning algorithm.If you have notice sample_rate was one of the parameter, which denotes the percentage of unlabelled data to be used as the pseudo labelled for the modelling purpose.Therefore, lets us check the dependance of sample_rate on the performance of the pseudo labelling.In order to find out the dependence of sample_rate on the performance of the pseudo labelling, let us plot a graph between those two.Here, I am using only two algorithm to show you the dependence because of the time constraint, but you can try for other algorithms too.So, we can see that the rmse is minimum for a particular value of sample_rate, which is different for both the algorithm.Therefore, it is important to tune sample_rate in order to achieve better results while using pseudo labeling.In past, there are limited number of applications of semi-supervised learning, but currently there is lot of working going on in this field.Some applications which I found interesting are listed below.Generally, in image categorisation, the goal is to classify an image whether it belongs to the category or not. In this paper, not only images are used for modelling but the keywords associated with labelled and unlabelled images are also used to improve the classifier using semi-supervised learning.Source: linkHuman trafficking is one of the most atrocious crimes and among the challenging problems facing law enforcement which demands attention of global magnitude. Semi-supervised learning is to applied to use both labelled and unlabelled data in order to produce better results than the normal approaches.Source: linkI hope that now you have a understanding what semi-supervised learning is and how to implement it in any real world problem. Therefore, try to explore it further and learn other types of semi-supervised learning technique and share with the community in the comment section.You can find the full code of this article from my github repository.Also, did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/
6 Common Probability Distributions every data science professional should know,Learn everything about Analytics|Introduction|Table of Contents|Common Data Types|Types of Distributions|Relations between the Distributions|Test your knowledge|End Notes,"Bernoulli Distribution|Uniform Distribution|Binomial Distribution|Normal Distribution|Poisson Distribution|Exponential Distribution|Relation between Bernoulli and Binomial Distribution|Relation between Poisson and Binomial Distribution|Relation between Normal and Binomial Distribution & Normal and Poisson Distribution:|Relation between Exponential and Poisson Distribution:|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Introduction to Pseudo-Labelling : A Semi-Supervised learning technique|Comparative Stock Market Analysis in R using Quandl & tidyverse  Part I|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Welcome to the world of Probability in Data Science! Let me start things off with an intuitive example.Suppose you are a teacher at a university. After checking assignments for a week, you graded all the students. You gave these graded papers to a data entry guy in the university and tell him to create a spreadsheet containing the grades of all the students. But the guy only stores the grades and not the corresponding students.He made another blunder, he missed a couple of entries in a hurry and we have no idea whose grades are missing. Lets find a way to solve this.One way is that you visualize the grades and see if you can find a trend in the data.The graph that you have plot is called the frequency distribution of the data. You see that there is a smooth curve like structure that defines our data, but do you notice an anomaly? We have an abnormally low frequency at a particular score range. So the best guess would be to have missing values that remove the dent in the distribution.This is how you would try to solve a real-life problem using data analysis. For any Data Scientist, a student or a practitioner, distribution is a must know concept. It provides the basis for analytics and inferential statistics.While the concept of probability gives us the mathematical calculations, distributions help us actually visualize whats happening underneath.In this article, I have covered some important probability distributions which are explained in a lucid as well as comprehensive manner.Note: This article assumes you have a basic knowledge of probability. If not, you can refer this probability distributions.Before we jump on to the explanation of distributions, lets see what kind of data can we encounter. The data can be discrete or continuous.Discrete Data, as the name suggests, can take only specified values. For example, when you roll a die, the possible outcomes are 1, 2, 3, 4, 5 or 6 and not 1.5 or 2.45.Continuous Data can take any value within a given range. The range may be finite or infinite. For example, A girls weight or height, the length of the road. The weight of a girl can be any value from 54 kgs, or 54.5 kgs, or 54.5436kgs.Now let us start with the types of distributions.Lets start with the easiest distribution that is Bernoulli Distribution. It is actually easier to understand than it sounds!All you cricket junkies out there! At the beginning of any cricket match, how do you decide who is going to bat or ball? A toss! It all depends on whether you win or lose the toss, right? Lets say if the toss results in a head, you win. Else, you lose. Theres no midway.A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial. So the random variable X which has a Bernoulli distribution can take value 1 with the probability of success, say p, and the value 0 with the probability of failure, say q or 1-p.Here, the occurrence of a head denotes success, and the occurrence of a tail denotes failure.
Probability of getting a head = 0.5 = Probability of getting a tail since there are only two possible outcomes.The probability mass function is given by: px(1-p)1-x where x  (0, 1).
It can also be written asThe probabilities of success and failure need not be equally likely, like the result of a fight between me and Undertaker. He is pretty much certain to win. So in this case probability of my success is 0.15 while my failure is 0.85Here, the probability of success(p) is not same as the probability of failure. So, the chart below shows the Bernoulli Distribution of our fight.Here, the probability of success = 0.15 and probability of failure = 0.85. The expected value is exactly what it sounds. If I punch you, I may expect you to punch me back. Basically expected value of any distribution is the mean of the distribution. The expected value of a random variable X from a Bernoulli distribution is found as follows:E(X) = 1*p + 0*(1-p) = pThe variance of a random variable from a bernoulli distribution is:V(X) = E(X)  [E(X)] = p  p = p(1-p)There are many examples of Bernoulli distribution such as whether its going to rain tomorrow or not where rain denotes success and no rain denotes failure and Winning (success) or losing (failure) the game.When you roll a fair die, the outcomes are 1 to 6. The probabilities of getting these outcomes are equally likely and that is the basis of a uniform distribution. Unlike Bernoulli Distribution, all the n number of possible outcomes of a uniform distribution are equally likely.A variable X is said to be uniformly distributed if the density function is:The graph of a uniform distribution curve looks likeYou can see that the shape of the Uniform distribution curve is rectangular, the reason why Uniform distribution is called rectangular distribution.For a Uniform Distribution, a and b are the parameters.
The number of bouquets sold daily at a flower shop is uniformly distributed with a maximum of 40 and a minimum of 10.Lets try calculating the probability that the daily sales will fall between 15 and 30.The probability that daily sales will fall between 15 and 30 is (30-15)*(1/(40-10)) = 0.5Similarly, the probability that daily sales are greater than 20 is = 0.667The mean and variance of X following a uniform distribution is:Mean -> E(X) = (a+b)/2Variance -> V(X) = (b-a)/12The standard uniform density has parameters a = 0 and b = 1, so the PDF for standard uniform density is given by:Lets get back to cricket. Suppose that you won the toss today and this indicates a successful event. You toss again but you lost this time. If you win a toss today, this does not necessitate that you will win the toss tomorrow. Lets assign a random variable, say X, to the number of times you won the toss. What can be the possible value of X? It can be any number depending on the number of times you tossed a coin.There are only two possible outcomes. Head denoting success and tail denoting failure. Therefore, probability of getting a head = 0.5 and the probability of failure can be easily computed as: q = 1- p = 0.5.A distribution where only two outcomes are possible, such as success or failure, gain or loss, win or lose and where the probability of success and failure is same for all the trials is called a Binomial Distribution.The outcomes need not be equally likely. Remember the example of a fight between me and Undertaker? So, if the probability of success in an experiment is 0.2 then the probability of failure can be easily computed as q = 1  0.2 = 0.8.Each trial is independent since the outcome of the previous toss doesnt determine or affect the outcome of the current toss. An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial.On the basis of the above explanation, the properties of a Binomial Distribution areThe mathematical representation of binomial distribution is given by:A binomial distribution graph where the probability of success does not equal the probability of failure looks likeNow, when probability of success = probability of failure, in such a situation the graph of binomial distribution looks likeThe mean and variance of a binomial distribution are given by:Mean ->  = n*pVariance -> Var(X) = n*p*qNormal distribution represents the behavior of most of the situations in the universe (That is why its called a normal distribution. I guess!). The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application. Any distribution is known as Normal distribution if it has the following characteristics:A normal distribution is highly different from Binomial Distribution. However, if the number of trials approaches infinity then the shapes will be quite similar.The PDF of a random variable X following a normal distribution is given by:The mean and variance of a random variable X which is said to be normally distributed is given by:Mean -> E(X) = Variance -> Var(X) = ^2Here,  (mean) and  (standard deviation) are the parameters.
The graph of a random variable X ~ N (, ) is shown below.A standard normal distribution is defined as the distribution with mean 0 and standard deviation 1. For such a case, the PDF becomes:Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. Some more examples areYou can now think of many examples following the same course. Poisson Distribution is applicable in situations where events occur at random points of time and space wherein our interest lies only in the number of occurrences of the event.A distribution is called Poisson distribution when the following assumptions are valid:1. Any successful event should not influence the outcome of another successful event.
2. The probability of success over a short interval must equal the probability of success over a longer interval.
3. The probability of success in an interval approaches zero as the interval becomes smaller.Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are:Here, X is called a Poisson Random Variable and the probability distribution of X is called Poisson distribution.Let  denote the mean number of events in an interval of length t. Then,  = *t.The PMF of X following a Poisson distribution is given by:The mean  is the parameter of this distribution.  is also defined as the  times length of that interval. The graph of a Poisson distribution is shown below:The graph shown below illustrates the shift in the curve due to increase in mean.It is perceptible that as the mean increases, the curve shifts to the right.The mean and variance of X following a Poisson distribution:Mean -> E(X) = 
Variance -> Var(X) = Lets consider the call center example one more time.What about the interval of time between the calls ? Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls.Other examples are:1. Length of time beteeen metro arrivals,
2. Length of time between arrivals at a gas station
3. The life of an Air ConditionerExponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result.A random variable X is said to have an exponential distribution with PDF:f(x) = { e-x, x  0and parameter >0 which is also called the rate.For survival analysis,  is called the failure rate of a device at any time t, given that it has survived up to t.Mean and Variance of a random variable X following an exponential distribution:Mean -> E(X) = 1/Variance -> Var(X) = (1/)Also, the greater the rate, the faster the curve drops and the lower the rate, flatter the curve. This is explained better with the graph shown below.To ease the computation, there are some formulas given below.
P{Xx} = 1  e-x, corresponds to the area under the density curve to the left of x.P{X>x} = e-x, corresponds to the area under the density curve to the right of x.P{x1<X x2} = e-x1  e-x2, corresponds to the area under the density curve between x1 and x2.1. Bernoulli Distribution is a special case of Binomial Distribution with a single trial.2. There are only two possible outcomes of a Bernoulli and Binomial distribution, namely success and failure.3. Both Bernoulli and Binomial Distributions have independent trails.Poisson Distribution is a limiting case of binomial distribution under the following conditions:Normal distribution is another limiting form of binomial distribution under the following conditions:The normal distribution is also a limiting case of Poisson distribution with the parameter  .If the times between random events follow exponential distribution with rate , then the total number of events in a time period of length t follows the Poisson distribution with parameter t.You have come this far. Now, are you able to answer the following questions? Let me know in the comments below!1. The formula to calculate standard normal random variable is:a. (x+) / 
b. (x-) / 
c. (x-) / 2. In Bernoulli Distribution, the formula for calculating standard deviation is given by:a. p (1  p)
b. SQRT(p(p  1))
c. SQRT(p(1  p))3. For a normal distribution, an increase in the mean will:a. shift the curve to the left
b. shift the curve to the right
c. flatten the curve4. The lifetime of a battery is exponentially distributed with  = 0.05 per hour. The probability for a battery to last between 10 and 15 hours is:a.0.1341
b.0.1540
c.0.0079Probability Distributions are prevalent in many sectors, namely, insurance, physics, engineering, computer science and even social science wherein the students of psychology and medical are widely using probability distributions. It has an easy application and widespread use. This article highlighted six important distributions which are observed in day-to-day life and explained their application. Now you will be able to identify, relate and differentiate among these distributions.If you have any doubts and want to see more articles on distributions, please do write in the comment section below. For a more in-depth write up of these distributions, you can refer this resource.I hope this article helps you in your data science journey. Was it explanatory? Let me know in the comment section.",https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/
Comparative Stock Market Analysis in R using Quandl & tidyverse  Part I,Learn everything about Analytics|Introduction|Objective of this Tutorial|Table of Contents|Setting Up The System|Getting Started with Comparative Analysis|End Notes,"Creating the Dataset|Visualization of monthly prices|Discovering the Relation between Total Traded Quantity vs Close Price|Finding the Density Distribution of Deviation of High Price from Open Price|Observing the Autocorrelation lags|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|6 Common Probability Distributions every data science professional should know|Understanding Support Vector Machine algorithm from examples (along with code)|
Guest Blog
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What differentiates the best data scientists from others? It is their focus on application of data science. The best data scientists I know of, see data science and its application every where they look. They look at this world as an outcome of flow of data and information.On the other hand, most beginners often ask the question  how do we apply our learning on real life problems?In this post (and another one following this), I have picked up a real life dataset (Stock Markets in India) and showed how I would use this data to come out with useful insights.I hope that you will find this useful. The idea is show the vast opportunities present in data science in a simple yet powerful manner. If you can think of more examples like this  let me know in comments below!For the best results, I would strongly recommend to build the application yourself as you follow the tutorial.In this article, we will analyze stock market in banking segment based on the bank stocks which are listed in NSE India. Our objective is to find the trends (Seasonal or cyclic) in banking stocks.In our comparative analysis we will use several packages and the primary focus will be on tidy verse package. The emphasis will be given on grouping with the help of tibble dataframe from tidy verse package. This will help to perform similar operation on multiple groups at a time, hence reducing the code length and computational time.This article also focuses on API Key, database code search using quandl, and finally how to directly download the data from R Console.So lets get started!Note: The code that has been mentioned below is to be run on the R command line for best results.There are a few things you should take care of before you go on further. Below mentioned are the packages you need to install in the systemIf you dont have any of the packages, then use the below code to install the packages. Modify the packages variable if any of the above packages are already installed.You can then call the necessary packages using the code belowWe will be using Quandl is online repository for the core financial, macroeconomic statistics and forex. Quandl has a vast collection of free and open data collected from a variety of organizations: central banks, governments, multinational organizations and more. You can use it without payment and with few restrictions.Both Free and Premium data are available. Authenticated Free users have a limit of 300 calls per 10 seconds, 2,000 calls per 10 minutes and a limit of 50,000 calls per day. Premium data subscribers have a limit of 5,000 calls per 10 minutes and a limit of 720,000 calls per day.We will use this online repository to get our data using Quandl package directly from the R Console. Quandl package directly interacts with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search.For More information on Quandl Package, please visit this page.To get started with Quandl, create an account and get the quandl API key. Please click here to create an account. Then click on the Login button provided on the top right corner of the screen. Once the registration is complete, please click here to get the API Key.In our analysis, we have selected following banksWe have selected these banks as they are in the price band of Rs 200 to Rs 500. We will use the following codes to get the data into R console.The parameters we use are as follows:Now we will download the data, add a column Stock for the stock identifier, and then we paste the respective stock name in the downloaded dataset. We will then consolidate all stock data into one Master Data frame for analysisLet us look at Monthly and Daily price pattern for Stocks using ggplot package. For this we will need to group the master dataframe according by Stock.We have heavily manipulated the theme section of ggplot to get the desired plot. More information on plot is provided here.
Usually, traded quantity increases if the stock price increases or decreases too rapidly on a given day. This parameter is important for our model for prediction. So we should take some time out to identify the relation between them in our data.We have an idea of trend of the stock price, but not much is clear from the Monthly prices. Axis Bank share price improved in september and stayed at Rs750 for a month. whereas all other Banks were consistent and did not show much of volatility.Now we will see the density distribution of High Price from Open Price in order to get an understanding that how much price is deviating in either direction (North or South) on weekly basis. This gives us an idea of price range for any stock in intraday trading.We will use the transmute_tq() function from tidyquant package to compute the weekly prices. Please click here to get more information.For this add a new column with the difference of high and open price using mutate function. Add another new column with the difference of low and open price using mutate function. Calculate the weekly average of differences using tq_transmute() function from tidyverse package. Visualize both density plots with dot distribution on ggplotThe lag operator (also known as backshift operator) is a function that shifts (offsets) a time series such that the lagged values are aligned with the actual time series. The lags can be shifted any number of units, which simply controls the length of the backshift.Here, k is denoted as lag. We will see the lag of 180 days period and see how stocks behave.These are the steps for ComputationIts apparent from the ACF plot, that there is no weekly or monthly pattern.This article contains descriptive analysis of stocks in terms of Daily/Weekly Price fluctuations. It also includes analysis on deviation from High and Low Price. The focus is also given on the relationship between the daily traded quantity of shares & close price and to check for the relationship. In the later part, the main focus is on xts package for the computation of Auto-correaltion. In the article, the focus is provided on finding lag and acf plot using ggplot rather than using the conventional time series package. This includes the analysis on ACF using different lags and to check if there is any pattern in the series.You can read part 2 of this article here.Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning.",https://www.analyticsvidhya.com/blog/2017/09/comparative-stock-analysis/
Understanding Support Vector Machine algorithm from examples (along with code),Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Support Vector Machine?|How does it work?|How to implement SVM in Python and R?|Pros and Cons associated with SVM||Practice Problem|End Notes,"Problem Statement|How to tune Parameters of SVM?||If you like what you just read & want to continue your analytics learning,subscribe to our emails,follow us on twitteror like ourfacebookpage.|Share this:|Related Articles|Comparative Stock Market Analysis in R using Quandl & tidyverse  Part I|Python vs. R vs. SAS  which tool should I learn for Data Science?|
Sunil Ray
|93 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Note: This article was originally published on Oct 6th, 2015 and updated on Sept 13th, 2017Mastering machine learning algorithms isnt a myth at all. Most of the beginners start by learning regression. It is simple to learn and use,but does that solve our purpose? Of course not! Because, you can do so much more than just Regression!Think of machine learning algorithms as anarmory packed with axes, sword, blades, bow, daggeretc.You have various tools, but you ought to learn to use them at the right time. As an analogy,think of Regression as a sword capable of slicing and dicing dataefficiently, but incapable of dealing with highly complex data. On the contrary,Support Vector Machines is like a sharp knife  it works on smaller datasets, but on them, it can bemuch morestronger and powerful in building models.By now, I hope youve now mastered Random Forest,Naive Bayes Algorithmand Ensemble Modeling. If not, Id suggest you to take out few minutes and read about them as well. In this article, I shall guide you through the basics to advanced knowledge of a crucial machine learning algorithm, support vector machines.If youre a beginner looking to start your data science journey, youve come to the right place! Check out the below comprehensive courses, curated by industry experts, that we have created just for you:Understanding Support Vector Machine algorithm from examples (along with code)Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,it is mostly usedinclassification problems.In this algorithm, we ploteach data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, weperform classification by finding the hyper-plane thatdifferentiate the two classes very well (look at the below snapshot).Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes (hyper-plane/ line).You can look at support vector machines and a few examples of its working here.Above, we got accustomed to the process ofsegregating the two classes with a hyper-plane. Now the burning question is How can we identify the right hyper-plane?. Dont worry, its not as hard as you think!Letsunderstand:Here, maximizing the distances between nearest data point (either class) and hyper-plane willhelp us to decide the right hyper-plane. This distance is called as Margin. Lets look at the below snapshot:
Above, you can see that the margin for hyper-plane C is highas compared to both A and B. Hence, we name theright hyper-plane asC. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification. Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is thecatch, SVM selects the hyper-plane which classifies the classes accuratelyprior tomaximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, theright hyper-plane is A.In SVM, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is,should we need to add this feature manually to have a hyper-plane. No,SVM hasa technique called the kernel trick. These are functions which takes low dimensional input space and transformit to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem.Simply put, it does some extremely complex data transformations, then find out the processto separate thedata based on the labels or outputs youve defined.When we look at the hyper-plane in original input space it looks like a circle:
Now, lets look at the methods to apply SVM algorithm in a data science challenge.In Python, scikit-learn is a widely used library for implementing machine learning algorithms, SVM is also available in the scikit-learn library and follow the same structure (Import library, object creation, fitting model and prediction).Now, let us have a look at a real-life problem statement and dataset to understand how to apply SVM for classificationDream Housing Finance company deals in all home loans. They have a presence across all urban, semi-urban and rural areas. Customer first applies for home loan after that company validates the customer eligibility for a loanCompany wants to automate the loan eligibility process (real-time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.Use the coding window below to predict the loan eligibility on the test set.Try changing the hyperparameters for the linear SVM to improve the accuracy.The e1071 package in R is used to create Support Vector Machines with ease. It has helper functions as well as code for the Naive Bayes Classifier. The creation of a support vector machine in R and Python follow similar approaches, lets take a look now at the following code:Tuning parameters value for machine learning algorithms effectively improves the model performance. Lets look at the list of parameters available with SVM.I am going to discuss about some important parameters having higher impact on model performance, kernel, gamma and C.kernel:We have already discussed about it. Here, we have various options available with kernel like, linear, rbf,poly and others (default value is rbf). Here rbf and poly are useful for non-linear hyper-plane. Lets look at the example, where weve used linear kernel on two feature of iris data set to classify their class.Example:Have linear kernelExample:Have rbf kernelChange the kernel type to rbf in below line and look at the impact.I would suggest you to go for linear kernelif you have large number of features (>1000) because it is more likely that the data is linearly separable in high dimensional space. Also, you canRBF but do not forget to cross validate for its parameters as to avoid over-fitting.gamma: Kernel coefficient for rbf, poly and sigmoid. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem.Example: Lets difference if we have gamma different gamma values like 0, 10 or 100.C:Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.We should always look at the cross validation score to have effective combination of these parameters and avoid over-fitting.In R, SVMs can be tuned in a similar fashion as they are in Python. Mentioned below are the respective parameters for e1071 package:Find right additional feature to have a hyper-plane for segregating the classes in below snapshot:Answer the variable name in the comments section below. Ill shall then reveal the answer.In this article, we looked at the machine learning algorithm, Support Vector Machine in detail. I discussed its concept of working, process of implementation in python, the tricks to make the model efficient by tuning its parameters, Pros and Cons, and finally a problem to solve. I would suggest you to use SVMand analyse the power of this model by tuning the parameters. I also want to hear your experience with SVM, how have you tuned parameters to avoid over-fitting and reduce the training time?Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
Python vs. R vs. SAS  which tool should I learn for Data Science?,Learn everything about Analytics|Overview|Introduction|Hasnt a lot already been said on this topic?|Background|Attributes For Comparison|Conclusion,"|1. Availability / Cost|2. Ease of Learning||3. Data Handling Capabilities|4. Graphical Capabilities|5. Advancements in Tool|6. Job Scenario|7. Customer Service Support & Community|8. Deep Learning Support|Other Factors:|Learn, engage,compete,andget hired!|Share this:|Related Articles|Understanding Support Vector Machine algorithm from examples (along with code)|6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R|
Kunal Jain
|191 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Note: This article was originally published onMar 27th, 2014 and updated on Sept 12th, 2017We love comparisons!From Samsung vs. Apple vs. HTC in smartphones; iOS vs. Android vs. Windows in mobile OS to comparing candidates for upcoming elections or selecting captain for the world cup team, comparisons and discussions enrich us in our life. If you love discussions, all you need to do is pop up a relevant question in middle of a passionate community and then watch it explode! The beauty of the process is that everyone in the room walks away as a more knowledgeable person.I am sparking something similar here. SAS vs. R has probably been the biggest debate the data science industry might have witnessed. Python is one of the fastest growing languages now and has come a long way since its inception. The reason for me to start this discussion is not to watch it explode (that would be fun as well though). I know that we all will benefit from the discussion.This has also been one of the most commonly asked questions to me on this blog. So, I thought Ill discuss it with all my readers and visitors!Probably yes! But I still feel the need for discussion for following reasons:So, without any further delay,let the combat begin!Here is a brief description about the 3 ecosystems:Ill compare these languages on following attributes:I am comparing these from point of view of an analyst. So, if you are looking for purchasing a tool for your company, you may not get complete answer here. The information below will still be useful. For each attribute I give a score to each of these 3 languages (1  Low; 5  High).The weightage for these parameters will vary depending on what point of career you are in and your ambitions.SAS is a commercial software. It is expensive and still beyond reach for most of the professionals (in individual capacity). However, it holds the highest market share in Private Organizations. So, until and unless you are in an Organization which has invested in SAS, it might be difficult to access one. Although, SAS has brought in a University Edition that is free to access but it has some limitations. You can also use Jupyter Notebooks in there!R & Python, on the other hand are completely free. Here are my scores on this parameter:SAS  3R  5Python  5SAS is easy to learn and provides easy option (PROC SQL) for people who already know SQL. Even otherwise, it has a good stable GUI interface in its repository. In terms of resources, there are tutorials available on websites of various university and SAS has a comprehensive documentation. There are certifications from SAS training institutes, but they again come at a cost.R has the steepest learning curve among the 3 languages listed here. It requires you to learn and understand coding. R is a low level programming language and hence simple procedures can take longer codes.Python is known for its simplicity in programming world. This remains true for data analysis as well. While there are no widespread GUI interfaces as of now, I am hoping Python notebooks will become more and more mainstream. They provide awesome features for documentation and sharing.SAS  4.5R  2.5Python  3.5This used to be an advantage for SAS till some time back. R computes every thing in memory (RAM) and hence the computations were limited by the amount of RAM on 32 bit machines. This is no longer the case. All three languages have good data handling capabilities and options for parallel computations. This I feel is no longer a big differentiation. Theyve all also brought on Hadoop and Spark integrations, with them also supporting Cloudera and Apache Pig.SAS  4R  4Python  4SAS has decent functional graphical capabilities. However, it is just functional. Any customization on plots are difficult and requires you to understand intricacies of SAS Graph package.R has highly advanced graphical capabilities along with Python. There are numerous packages which provide you advanced graphical capabilities.With the introduction of Plotly in both the languages now and with Python having Seaborn, making custom plots has never been easier.SAS  3R  4.5Python  4.5All 3 ecosystems have all the basic and most needed functions available. This feature only matters if you are working on latest technologies and algorithms.Due to their open nature, R & Python get latest features quickly. SAS, on the other hand updates its capabilities in new version roll-outs. Since R has been used widely in academics in past, development of new techniques is fast.Having said this, SAS releases updates in controlled environment, hence they are well tested. R & Python on the other hand, have open contribution and there are chances of errors in latest developments.SAS  4R  4.5Python  4.5Globally, SAS is still the market leader in available corporate jobs. Most of the big organizations still work on SAS. R / Python, on the other hand are better options for start-ups and companies looking for cost efficiency. Also, number of jobs on R / Python have been reported to increase over last few years. Here is a trend widely published on internet, which shows the trend for R and SAS jobs. Python jobs for data analysis will have similar or higher trend as R jobs:The graph below shows R in Blue and SAS in Orange.This one on the other hand, now shows R in Blue and Python in Orange.Overall, the market based on languages can be pictured as such:SAS  4R  4.5Python  4.5R and Python have the biggest online communities but no customer service support. So if you have trouble, you are on your own. You will get a lot of help though.SAS on the other hand has dedicated customer service along with the community. So, if you have problems in installation or any other technical challenges, you can reach out to them.SAS  4R  3.5Python  3.5Deep Learning in SAS is still in its beginning phase and theres a lot to work on it.On the other hand, Python has had great advancements in the field and has numerous packages like Tensorflow and Keras.R has recently added support for those packages, along with some basic ones too. The kerasR and keras packages in R act as an interface to the original Python package, Keras.SAS  2Python  4.5R  3Following are some more pointsworthy to note:We see the market slightly bending towards Python in todays scenario. It will be pre-mature to place bets on what will prevail, given the dynamic nature of industry. Depending on your circumstances (career stage, financials etc.) you can add your own weights and come up with what might be suitable for you. Here are a few specific scenarios:Strategically, corporate setups that require more hands-on assistance and training choose SAS as an option.Researchers and statisticians choose R as an alternative because it helps in heavy calculations. As they say, R was meant to get the job done and not to ease your computer.Python has been the obvious choice for startups today due to its lightweight nature and growing community. It is the best choice for deep learning as well.Here is the final scorecard:These are my views on this comparison. Now, its your turn to share your views through the comments below.",https://www.analyticsvidhya.com/blog/2017/09/sas-vs-vs-python-tool-learn/
6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R,Learn everything about Analytics|Overview|Introduction|Project to apply Naive Bayes|Problem Statement|Table of Contents|What is Naive Bayes algorithm?|How Naive Bayes algorithm works?|What are the Pros and Consof Naive Bayes?|4 Applicationsof Naive Bayes Algorithms|How to build a basic model using Naive Bayes in Python and R?|Tips toimprove the power of Naive Bayes Model|End Notes,"Python Code:|R Code:|Learn, engage,compete,andget hired!|Share this:|Related Articles|Python vs. R vs. SAS  which tool should I learn for Data Science?|4 Essential Tools any Data Scientist can use to improve their productivity|
Sunil Ray
|41 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Note: This article was originally published on Sep 13th, 2015 and updated on Sept 11th, 2017Heres a situation youve got into in your data science project:You are working on a classification problem and have generated your set ofhypothesis, created features and discussed the importance of variables. Within an hour, stakeholders want to see the first cut of the model.What will you do? You have hundreds of thousands of data points and quite a few variables in your training data set. In such a situation, if I were in your place, I would have used Naive Bayes, which can be extremely fast relative to otherclassification algorithms. It works on Bayes theorem of probability to predict the class of unknown data sets.In this article, Ill explain the basics of this algorithm, so that next time when you come across large data sets, you can bring this algorithm to action. In addition, if you are a newbie inPython or R, you should not be overwhelmed by the presence of available codes in this article.HR analytics is revolutionizing the way human resources departments operate, leading to higher efficiency and better results overall. Human resources have been using analytics for years.However, the collection, processing, and analysis of data have been largely manual, and given the nature of human resources dynamics and HR KPIs, the approach has been constraining HR. Therefore, it is surprising that HR departments woke up to the utility of machine learning so late in the game. Here is an opportunity to try predictive analytics in identifying the employees most likely to get promoted.Practice NowIt is a classification technique based onBayes Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as Naive.Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known tooutperform even highlysophisticated classification methods.Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:Above,Lets understand it using an example. Below I have a training data set of weather and corresponding target variable Play (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather condition. Lets follow the below steps to perform it.Step 1: Convert the data set into a frequency tableStep 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.Step 3: Now, useNaive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.Problem: Players will play if weather is sunny. Is this statement is correct?We can solve it using above discussed method of posterior probability.P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.Pros:Cons:Again, scikit learn (python library) will help here to build a Naive Bayes model in Python. There are three types of Naive Bayes model under the scikit-learn library:Gaussian:It is used in classification and itassumes that features follow a normal distribution.Multinomial:It is used for discrete counts. For example, lets say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of word occurring in the document, we have count how often word occurs in the document, you can think of it as number of times outcome number x_i is observed over the n trials.Bernoulli:The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with bag of words model where the 1s & 0s are word occurs in the document and word does not occur in the document respectively.Try out the below code in the coding window and check your results on the fly!Above, we looked at the basic Naive Bayes model, you can improve the power of this basic model by tuning parameters and handle assumption intelligently. Lets look at the methods to improve the performance of Naive Bayes Model.Id recommend you to go through this document for more details on Text classification using Naive Bayes.Here are some tips for improving power of Naive Bayes Model:In this article, we looked at one of the supervisedmachine learning algorithm Naive Bayes mainly used for classification. Congrats, if youve thoroughly & understood this article, youve already taken you first step to master this algorithm. From here, all you need is practice.Further, I would suggest you to focus more ondata pre-processing and feature selection prior toapplying Naive Bayes algorithm.0 In future post, I will discuss about text and document classification using naive bayes in more detail.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
4 Essential Tools any Data Scientist can use to improve their productivity,Learn everything about Analytics|Introduction|Table of Contents|What does a data science stack look like?|Case Study of a Deep Learning problem: Getting started with Python Ecosystem|Overview of Jupyter: A Tool for Rapid Prototyping|Keeping Tab of Experiments: Version Control with GitHub|Running Multiple Experiments at the Same Time: Overview Tmux|Deploying the Solution: Using Docker to Minimize Dependencies|Summarization of tools|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|6 Easy Steps to Learn Naive Bayes Algorithm with codes in Python and R|Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems|
Faizan Shaikh
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Many people have asked me this question that whenever they get started with data science, they get stuck with the manifold of tools available to them.Although there are handful of guides available out there concerning the problem such as 19 Data Science Tools for people who arent so good at Programming or A Complete Tutorial to Learn Data Science with Python from Scratch, I would like to show what tools I generally prefer for my day-to-day data science needs.Read on if you are interested!Note: I usually work in Python and in this article I intend to cover the tools used in python ecosystem on Windows.I come from a software engineering background. There people are inquisitive about what kind of stack people currently working on? Their intention here is to find which tools are trending and what are the pros and cons while using it. Its because each and everyday new tools come out which try to eliminate the nagging problems people face everyday.In data science, I can probably say that there is more inclination on what kinds of techniques you use to solve a problem rather than the tools to use. Still, its wise to get to know what kind of tools are available to you. A survey was done keeping this in mind. Below image summarizes these findings.SourceInstead of blatantly saying which the tools to be used, I will give you a rundown of these tools with a practical example. We will do this exercise on Identify the Digits practice problem.Let us first get to know what the problem entails. The home page mentions Here, we need to identify the digit in given images. We have total 70,000 images, out of which 49,000 are part of train images with the label of digit and rest 21,000 images are unlabeled (known as test images). Now, we need to identify the digit for test images.This essentially means that the problem is an image recognition problem.The first step here would be to setup your system for the problem. I usually create a specific folder structure for each problem I start (yes Im a windows user  ) and start working from there.For this kind of problems, I have a tendency to use the kit mentioned below:Fortunately, most of the things above can be accessed using a single software called Anaconda. Im accustomed to using Anaconda because of its comprehensiveness of data science packages and ease of use.To setup anaconda in your system, you have to simply download the appropriate version for your platform. More specifically, I have the python 3.6 version of anaconda 4.4.0.Now to use the newly install python ecosystem, open the anaconda command prompt and type python As I said earlier,most of the things come pre-installed in anaconda. The only libraries left are tensorflow and keras. A smart thing to do here which anaconda provides a feature for is creating an environment. You do this because even if you do something wrong when setting up, it wont affect your original system. This is like creating a sandbox for all your experiments. To do this, go to the anaconda command prompt and typeNow not install , the remaining packages by typingNow you can start writing your codes in your favorite text editor and run the python scripts!The problem in working with a plain text editor is that each time you update something, you have to run the code from the start again. Suppose you have a small code for reading data and processing. The code for data reading is functioning correctly but takes an hour to run. Now if you try to change the code for processing, you have to wait for the code for data reading to run and then see if your update works. This is tiresome and it wastes your time a lot.To get over this issue, you can use jupyter notebooks. Jupyter notebooks essentially save your progress and let you continue from where you left off. Here you can write your code in a structured way so that you can resume the code and update it whenever you want to.In the section above, you had setup your system with anaconda software. As I mentioned, anaconda has jupyter preinstalled in it. To open jupyter notebook, open the anaconda prompt, go to the directory that you created and typeThis opens up jupyter notebook in you web browser (Mozilla Firefox for me). Now to create a notebook, click on New-> Python 3Now what i usually do is divide the code into small blocks of code, so that it would be easier for me to debug. I always keep these in mind when I write code:Here is a sample of code I wrote to solve the deep learning problem mentioned. You can follow along if you like!A data science project requires multiple iterations of experimentation and testing. It is very difficult to remember all the things you tried out, which one of those worked and which did not.One method to control this issue is to take notes of all the experiments that you did and summarize them. But this too is a bit tedious and manual work. A workaround for this (and my personal choice) is to use GitHub. Originally GitHub was used as a code sharing platform for software engineers, but now it is gradually being accepted by the data science community. What GitHub does is provides you with a framework for saving all the changes you did in the code and reverting back to it anytime you want. This gives you the flexibility that you need to do a data science project efficiently.I will give you an example of how to use GitHub. But first, let us install it in our system. Go to the download link and click on the version as per your system. Let it download and then install it.This will install git in your system, along with command prompt for it called git bash. The next step is to configure the system with your git account. Once you have signed up for GitHub, you can use this to setup your system. Open up you git bash and type the following commands. Now theres two tasks you have to do.To accomplish the tasks, follow on to the steps mentioned belowStep 1: Add a new repository on GitHubStep 2: Give proper descriptions to setup the repository.Step 3: Get link to the repositoryStep 4: Connect you local repository with the repository on GitHub with the commands below.Now if you want GitHub to ignore the changes in the files you can mention them in .gitignore file. Open it and add the file or folder you want to ignore.Now each time you want to save your progress, run these commands againThis will ensure that you can go back to where you left off!Jupyter notebooks are very helpful for our experimentations. But what if you wanted to multiple experiments at the same time? You would have to wait for the previous command to finish before you can run another one.I usual have many ideas that I want to test out. And there are times I want to scale up. I use the companys deep learning box we built a few months back. So I prefer to use tmux for it. tmux or Terminal Multiplexer lets you switch easily between several programs in the same terminal. Here is my command center right now We setup Ubuntu in the monster because it seemed like a good idea at that time. Setting up tmux in your system is pretty easy, you just have to install it using the command belowsudo apt-get install tmuxTo open tmux, type tmux in your command line.To make a new window, just press control + B and then shift + 5Now if you want to let this experiment continue and do something else, just typeand it will give you the terminal again. And if you want to come back to the experiment again, typeAs simple as that! Note that setting up tmux in windows is a bit different. You can refer this article to set it up in your system.Now after all the implementations are done, we still have to deploy the solutions so that the end user such as a developer can access it. But the issue we always face is that the system we have might not be the same as that of the user. There will always be installation and setting up issues in their system.This is a very big problem when it comes to deployment of products in market. So to curb this issue, you can rely on a tool called docker. Docker is works on the idea that you can package code along with its dependencies into a self-contained unit. This unit can then be distributed to the end user.I usually do toy problems on my local machine, but when it comes to final solutions, I rely on the monster. I had setup docker in that system using these commandsIntegrating GPU on docker has some additional steps:Now comes the main part, installing DL libraries on docker. We built the docker from scratch, following this excellent guide (https://github.com/saiprashanths/dl-docker/blob/master/README.md)To run your code on docker, open your dockers command prompt using the commandNow whenever I have a complete working model, I throw it in the docker system to try it out on scale. You can use the dockerfile in a different system to install the same softwares and libraries in their system. This ensures that there isnt any installation problem.To conclude these are the tools I use and their usagesHope these tools will help you in your experiments too!In this article, I have covered what I feel are the necessarily tools for doing a data science project. To summarize, I use Python environment in anaconda stack, Jupyter notebooks for experimentations, Github for saving the crucial experiments, Tmux for running multiple experiments at once, and docker for deployment.If you have any more tools to suggest or if you use a completely different stack, do let me know in the comments below!",https://www.analyticsvidhya.com/blog/2017/09/essential-tools-data-scientist-improve-productivity/
"Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems",Learn everything about Analytics,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|4 Essential Tools any Data Scientist can use to improve their productivity|Commonly used Machine Learning Algorithms (with Python and R Codes)|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Corporate by chance, Entrepreneur by choice Interview with Pankaj Kulshreshtha, CEO, Scienaptic SystemsIf you have to learn experience of other people, I can recommend 2 sure shot ways of doing it:Because of my role at Analytics Vidhya, I am lucky to have multiple opportunities of the second type. Today, I am sharing my interaction with one such leader  Pankaj Kulshreshtha, CEO, Scienaptic SystemsPankaj has 20+ years of experience in analytics space. He holds a PhD in Quantitative Methods from IIM-Bangalore. He has grown multiple businesses in past and is still a researcher at heart.Read on to know about his journey & story of setting up Scienaptic Systems.P.S. If you have not bought tickets for DataHack Summit 2017  this could be your opportunity to interact with thought leaders from across the globe.Kunal: Hi Pankaj, thanks for taking time out for this interview. One of the fears I had when I was in my job was that if I stay too long in a job / secure environment, I might not become an entrepreneur later on. When I look at you, you started your venture after spending close to 20 years in corporate. How did this journey start?Pankaj: Every day we read or hear stories about a group of people who wanted to be Entrepreneurs right from the beginning. But, that was not my journey  In fact, even my entry into the corporate world was a chance encounter.When I was finishing my doctoral work at IIM-B, GE advertised for a Decision Science center and this was quite unique in 1997-98. At that time, predictive modeling roles and working with credit bureau data were unheard of in India. Even though my interest lay in academics, I decided to experience the corporate world for a while, and began with a catchy designation: Assistant Manager  Modelling. The experience of working in analytics was great, timing was just right. I did hands on work that still keeps me somewhat sharp in geeky discussions! The more interesting aspect of growth though was on other dimensions. GE is legendary for developing leadership skills and I was fortunate to grow in a very conducive environment. After about 4-5 years, it was pretty clear that I wasnt going to academic world, at least not full time!I am a very passionate believer that use of data and analytics will dramatically change not just the corporations but also our societies. Building and growing the analytics business at Genpact was very fulfilling and enjoyable, but I started getting a serious inner push to start thinking about starting a new venture in the last couple of years. I chewed on it and discussed the ideas widely. Came to a conclusion that there is much friction among technologies, processes and people that slows down the adoption of predictive analytics. Once that clarity descended, I finally decided to start on the entrepreneurial journey and here I am today  a very unlikely entrepreneur at an unlikely age!Kunal: So does that mean your ideal clients are MNCs who have huge amount of data and they may not be having the right infrastructure. Basically, they want to accelerate or transform the process. Pankaj: Our target audience is big organizations, especially banks, financial services and insurance companies that have been investing large amount of resources, both in terms of monetary investments for buying new data infrastructure and also setting up machine learning teams.But what we have observed is that in spite of making these investments for 5-10 years now, these organizations still struggle to deliver intelligent customer interactions. This is what we want to change, by enabling organizations to use and enrich omnichannel data and leveraging ML at industrial scale.We have created a platform (Ether) that companies can use without having to spend tens of millions of dollars on long range projects but still deliver superior intelligence in their customer interactions. We work in such a way that we leverage the existing technology investments of our client along with our platform and accelerate the development of customer management strategies embedded with advanced ML algorithms. Everything we do is geared to produce significant business impact in short agile cycles of 2-3 months.Kunal: In my experience, I have also seen companies making huge investments in data infrastructure and data science. But, I havent seen a marked difference from that in customer interactions. What do you think are the flaws in their strategies or common challenges your clients face and how do you solve them?Pankaj: This is a matter of opinion to some extent. I have worked with global banks & financial companies in the past and I have seen a huge revolution in the way they use & manage data. I think there has been massive progress made over the years.There is a plethora of young, Fintech startups that have dramatically improved the customer experience in the past few years. This kind of innovation in customer experience is hard to achieve for larger financial organizations because of a few reasons:Larger organizations have multiple cross-functional transactions which makes it difficult for them to create a centralized & intelligent customer view at the speed of smaller startups. Hence, creating cross functional strategies to drive customer experience takes up a lot of time and effort in these organizations.The other big problem organizations deal with is around creating a unified customer experience. e.g. If I reach out to a call center of a bank, they should know who I am, what my past transactions have been, whether I have visited the bank branch to resolve my issue etc. All that I (the customer) want is a unified customer experience irrespective of the channel. However, even as banks are getting better at building applications, the data and technology is not yet there to create a unified customer viewThe third issue is the fact that most banks are still organize their P&Ls by products like mortgage, credit cards, loans, etc. Each of these products are separate business units, many times with their own data warehouses. But essentially, these separate business units are targeting the same customer / prospect pool. The challenge here, is to integrate the different business units across customer lifecycle to ensure appropriate product and price targeting so that customer feels an intelligent approach to how they are being offered and reached for new offers.Kunal: How did you get your first set of clients? Pankaj: Our very first client was an ex colleague who had gone on to become the CEO of a loans business. When he heard about our company, he wanted us to handle analytics and data science for his company. Then another client came in as a referral from one of my ex-colleagues.To be honest, I have been fortunate. The people I have worked with in the industry had a certain level of trust in our ability to get things done and chose to partner with us. I believe that hard work never remains unnoticed; building relationships along your journey always helps.Kunal: Can you tell us any use case which came in as a problem from clients end and how did you solve it?Pankaj: We worked with a large US cards company to help them manage their fraud. This client was experiencing surging fraud trends because of check-kiting and synthetic ID based credit bust-outs and the projection was incremental fraud losses of ~ $100MM. Historically they had used a set of rules to control the frauds, but these rules also had the flip side of killing a lot of good sales because of the high false positive rates. Within 6 weeks of our engagement, we designed a machine learning strategy for managing the losses and increasing the sales which were otherwise going on hold. We were able to deliver a clear impact of 20 million dollars in the fraud reduction and were able to reduce their False Positive Rates by 1/5th.Kunal: How did you build your initial team?Pankaj: I went about it the traditional way. I approached people from my network with whom I had worked in the past. I spoke to few of them and they decided to join us. Then the team approached their ex-colleagues & hired few more people.Even as we hire more folks to build bigger teams, we still find that looking through our networks is the most effective way to hire the initial team.Kunal: What was the most challenging time for you in the journey to build Scienaptic?Pankaj: I started Scienaptic with a couple of friends and when one of my co-founders decided to leave the company after 9 months because it was not the right thing for them, the world came crashing down on me! I had to evaluate my willingness to even run the company. More than the loss of the co-founder, it was the loneliness in that situation that affected me. At that time, I did ask myself whether I really wanted to go ahead with my startup.What kept me going was my belief in our idea. We were a team of 20 people at that time and I was totally convinced of the product as there was enough evidence that the market needed it. After that, some senior folks joined us, and since then, the company has been growing. Today, we have a great team and I feel energized by just being around themKunal: How do you define Scienaptic today, is it a product or services driven company and what is the culture like?Pankaj: Before we hired any analytical folks, we hired engineers to build the platform. It was clear from day one that we were building a platform based company. The premise of starting Scienaptic was that the friction among technologies, processes and humans has to be dramatically reduced to enable ML to get embedded in organizations at scale and that needed a platform based approach. Our vision is still the same and we continue to make huge investments in developing our platform capabilities. Based on the serious traction that we are seeing, we feel our strategy is working out!Our culture is hands-on and very tech savvy. Whole of our senior leadership is hands on and leads with personal example. We focus on building new things every day. We ensure that every piece of work that we do, delivers a significant impact to our clients. We strive to create a future-ready workplace for ourselves, where there is an immense opportunity and flexibility to learn and grow, both professionally and personally. We are building a work culture which the millennials look forward to!Kunal: Can you tell us more about Ether?Pankaj: Ether is a platform that has data management, visualization, machine learning and workflow capabilities that come together to solve very specific business problems. We have designed Ether with several deeply technical innovations. It is a natural language based platform where working with structured & unstructured data is seamless. There is a significant push to democratize machine learning. Ether lets users work on complicated machine learning problems and evaluate the impact rather than worrying about writing thousands of lines of codes to solve the problem.One of our core solution built on Ether is Credit Decisioning and Fraud Management. We understand that as technology evolves within Banks and Financial Institutions, there is an emerging need to also evolve the Credit Decisioning process and our solution is designed to do just that. Our advanced Machine Learning driven approach radically outperforms the traditional approach. This solution populates a rich multidimensional Customer Consciousness that can evolve to provide intelligent signals to other processes like customer service and collections.Kunal: What are your top priorities today as the CEO of Scienaptic?Pankaj: We intend to change the way organizations deploy and leverage machine learning in their customer engagements. As part of that vision, we are creating class-defining software that reduces friction among technologies, processes and humans. Another key element of our strategy is to make sure that everything we do delivers impact in very short agile cycles. Towards that we are continually innovating on our operating and engagement models.Kunal: What are the different skillsets you look at while hiring in the US and in India?Pankaj: We look for people who are hands-on and versatile with technology. Our engineering team has been based in Bangalore till now and our core developers are skilled in Java, Scala, Spark, Hadoop etc. As we deploy our platform to more US clients, we are building out an engineering team in New York as well.The other team common to both New York and Bangalore comprises of people who have significant understanding of our platform and Machine Learning techniques. These folks primarily work with clients to understand their problems and create appropriate solutions using technology.Kunal: This is kind of different than the regular practices in the industry. What was the thought process that went behind this?Pankaj: The delivery & execution of work actually happens in both New York and Bangalore in line with what makes sense for the client. We dont think of ourselves as an off-shoring company. We want to reduce the friction that can be caused in managing remote work. Our aim is to solve problems and transform the costs using technology and machine learning rather than off-shoring.Kunal: Apart from India & US, are there any other markets where you have your presence or looking for expansion. Pankaj: I am very conscious about geographical diversifications as we dont want to thin out the intensity. Right now, we are focused on the US and UK as our primary markets. Most of our customers and prospects are Fortune 500 corporations and they have presence on both sides of the Atlantic. And because we have presence in Bangalore, we are also working with a few progressive Indian companies.Kunal: You were one of the key members of the team which scaled up the analytics operations at Genpact. Tell us a bit more about that journey. Pankaj: As I mentioned, I started with GE when analytics wasnt known in India, about two decades ago. One good thing was that I started building models myself and that is the reason people still think I am technically solid. Over time the Analytics Center of Excellence at GE became popular across GE business units and I got to managing teams, started with 3-4 people team and by 2004 I was leading a team of about 250 people. At that time I decided to move into a functional role as Chief Risk Officer with GE Money, UK. People would often ask me if it made sense to move from leading a team of 250 to leading one with 8 people! But, I think that was the most powerful career move I made because I actually got to see analytics live in action! What I learnt in that time & the relationships I built, helped me a lot in my journey. And living in another country was very enriching, it opens up our mind to the variety and possibilities.In 2008, I came back to what had become Genpact. With hindsight, the timing was great as the meltdown of 2008 followed pretty quickly! Genpact was a tremendously rewarding and enjoyable experience. We grew 3 times while I was the business leader. I believe those six years taught me how to think big, scale organizations, and sell!Kunal: These days machine learning, big data & AI is getting a lot of attention. How do you think things will change in the next 5 years? Pankaj: I actually think all the hoopla around machine learning & AI is good. In my last few trips to New York, I have heard many conversations about Hadoop, Big data & AI on the streets in midtown. Unlike few years back big data analytics is being talked about a lot today. I think people still underestimate how much our lives will change as a result of big data and how insights will be generated and consumed.I believe that enterprise engagement with customers in an interactive way through multiple channels must become a reality in the coming future. I also believe that Machine Learning will not just be a hyped-up curiosity. Rather, people will actually be using ML in real cases to solve real world problems.The other thing that I think will change, is that more and more people will start getting comfortable with working with a set of technologies rather than just 1 or 2 primary enterprise platforms.Kunal: What would be your advice to people already in data science industry & the beginners entering the industry?Pankaj: There is a lot of talk around Big Data & Machine Learning in the industry. I believe that there is essentially only one problem that everyone is trying to solve  most organizations want to make sure their customers are happy and give them more business over time.Today we have vast data available because of the digitization & one has the ability to do large computations on that data using machine learning & AI. There are numerous resources available to learn techniques and practice hands-on. My advice to people is to focus on solving business problems using machine learning  there is a scarcity of skilled professional who do that well enough.I think there is no substitute for intensity & passion. Even if you have a niche skillset, you need to keep evolving your skills. We are living in times where ability to learn is lot more important than knowing stuff.Kunal: Thanks Pankaj for taking time out for this interview. We hope to see you around in our community interactions in future.",https://www.analyticsvidhya.com/blog/2017/09/interview-pankaj-kulshreshtha-ceo-scienaptic-systems/
Commonly used Machine Learning Algorithms (with Python and R Codes),"Learn everything about Analytics|Overview|Introduction|Who can benefit the most from this guide?|Broadly, there are 3 types of Machine Learning Algorithms||List of Common Machine Learning Algorithms|1. Linear Regression|2. Logistic Regression|3. Decision Tree|4. SVM (Support Vector Machine)|5. Naive Bayes|6. kNN (k- Nearest Neighbors)|7. K-Means|8. Random Forest|9. Dimensionality Reduction Algorithms|10. Gradient Boosting Algorithms|Projects||End Notes","1. Supervised Learning|2. Unsupervised Learning|3. Reinforcement Learning:||Furthermore..|10.1. GBM|10.2. XGBoost|10.3. LightGBM|10.4. Catboost|If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.|Share this:|Related Articles|Exclusive Interview with Pankaj Kulshreshtha, CEO, Scienaptic Systems|Building Machine Learning Model is fun using Orange|
Sunil Ray
|76 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","What I am giving out today is probably the most valuable guide, I have ever created.||Python Code|Python Code|Python Code|R Code|Python Code|R Code",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Note: This article was originally published on August 10, 2015 and updated on Sept 9th, 2017
Googles self-driving cars and robots get a lot of press, but the companys real future is in machine learning, the technology that enables computers to get smarter and more personal. Eric Schmidt (Google Chairman)We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to cloud. But what makes it defining is not what has happened, but what is coming our way in years to come.What makes this period exciting and enthralling for someone like me is the democratization of the various tools and techniques, which followed the boost in computing. Welcome to the world of data science!Today, as a data scientist, I can build data-crunching machines with complex algorithms for a few dollars per hour. But reaching here wasnt easy! I had my dark days and nights.Are you a beginner looking for a place to start your data science journey? Presenting two comprehensive courses, full of knowledge and data science learning, curated just for you to learn data science (using Python) from scratch:The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world. Through this guide, I will enable you to work on machine learning problems and gain from experience. I am providing a high-level understanding of various machine learning algorithms along with R & Python codes to run them. These should be sufficient to get your hands dirty.Essentials of machine learning algorithms with implementation in R and PythonI have deliberately skipped the statistics behind these techniques, as you dont need to understand them at the start. So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. But, if you are looking to equip yourself to start building machine learning project, you are in for a treat.How it works: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.How it works: In this algorithm, we do not have any target or outcome variable to predict / estimate. It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means.How it works: Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision ProcessHere is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.In this equation:These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.Look at the below example. Here we have identified the best fit line having linear equation y=0.2811x+13.9. Now using this equation, we can find the weight, knowing the height of a person.Linear Regression is mainly of two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding the best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.Heres a coding window to try out your hand and build your own linear regression model in Python:R CodeDont get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).Again, let us try and understand this through a simple example.Lets say your friend gives you a puzzle to solve. There are only 2 outcome scenarios  either you solve it or you dont. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this  if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).Now, you may ask, why take a log? For the sake of simplicity, lets just say that this is one of the best mathematical way to replicate a step function. I can go in more details, but that will beat the purpose of this article.Build your own logistic regression model in Python here and check the accuracy:R CodeThere are many different steps that could be tried in order to improve the model:This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: Decision Tree Simplified.source: statsexchangeIn the image above, you can see that population is classified into four different groups based on multiple attributes to identify if they will play or not. To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.The best way to understand how decision tree works, is to play Jezzball  a classic game from Microsoft (image below). Essentially, you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.So, every time you split the room with a wall, you are trying to create 2 different populations with in the same room. Decision trees work in very similar fashion by dividing a population in as different groups as possible.More: Simplified Version of Decision Tree AlgorithmsLets get our hands dirty and code our own decision tree in Python!R CodeIt is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.For example, if we only had two features like Height and Hair length of an individual, wed first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, thats what class we can classify the new data as.More: Simplified Version of Support Vector MachineThink of this algorithm as playing JezzBall in n-dimensional space. The tweaks in the game are:Try your hand and design an SVM model in Python through this coding window:R CodeIt is a classification technique based on Bayes theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:
Here,Example: Lets understand it using an example. Below I have a training data set of weather and corresponding target variable Play. Now, we need to classify whether players will play or not based on weather condition. Lets follow the below steps to perform it.Step 1: Convert the data set to frequency tableStep 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.Problem: Players will pay if weather is sunny, is this statement is correct?We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.Code a Naive Bayes classification model in Python:R CodeIt can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.More: Introduction to k-nearest neighbors : Simplified.KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!Things to consider before selecting kNN:R CodeIt is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!How K-means forms cluster:How to determine value of K:In K-means, we have clusters and each cluster has its own centroid. Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. Here, we can find the optimum number of cluster.R CodeRandom Forest is a trademark term for an ensemble of decision trees. In Random Forest, weve collection of decision trees (so known as Forest). To classify a new object based on attributes, each tree gives a classification and we say the tree votes for that class. The forest chooses the classification having the most votes (over all the trees in the forest).Each tree is planted & grown as follows:For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:Introduction to Random forest  SimplifiedComparing a CART model to Random Forest (Part 1)Comparing a Random Forest to a CART model (Part 2)Tuning the parameters of your Random Forest modelPython Code:R CodeIn the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. Howd you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.To know more about this algorithms, you can read Beginners Guide To Learn Dimension Reduction Techniques.GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.More: Know about Boosting algorithms in detailGradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about the difference between these two algorithms.Another classic gradient boosting algorithm thats known to be the decisive choice between winning and losing in some Kaggle competitions.The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.The support includes various objective functions, including regression, classification and ranking.One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process.To learn more about XGBoost and parameter tuning, visit https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/.Python Code:R Code:LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:The framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.Also, it is surprisingly very fast, hence the word Light.Refer to the article to know more about LightGBM: https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/Python Code:R Code:If youre familiar with the Caret package in R, this is another way of implementing the LightGBM.CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Googles TensorFlow and Apples Core ML.The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.Make sure you handle missing data well before you proceed with the implementation.Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.Learn more about Catboost from this article: https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/Python Code:R Code:Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your data science journey with the following Practice Problems:By now, I am sure, you would have an idea of commonly used machine learning algorithms. My sole intention behind writing this article and providing the codes in R and Python is to get you started right away. If you are keen to master machine learning, start right away. Take up problems, develop a physical understanding of the process, apply these codes and see the fun!Did you find this article useful ? Share your views and opinions in the comments section below.",https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
Building Machine Learning Model is fun using Orange,Learn everything about Analytics|Introduction|Table of Contents:|1. Why Orange?||2. Setting up your System|3. Creating Your First Workflow|4. Familiarising yourself with the basics|5. How do you clean your data?|6.Training your First Model|End Notes,"4.1 Problem|4.2 Importing the data files|4.3 Understanding our Data|Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Commonly used Machine Learning Algorithms (with Python and R Codes)|Creating & Visualizing Neural Network in R|
Analytics Vidhya Content Team
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"With growing need of data science managers, we need tools which take out difficulty from doing data science and make it fun. Not everyone is willing to learn coding, even though they would want to learn / apply data science. This is where GUI based tools can come in handy.Today, I will introduce you to another GUI based tool  Orange. This tool is great for beginners who wish to visualize patterns and understand their data without really knowing how to code.In my previous article, I presented you with another GUI based tool KNIME. If you do not want to learn to code but still apply data science, you can try out any of these tools.By the end of this tutorial, youll be able to predict which person out of a certain set of people is eligible for a loan with Orange!Orange is a platform built for mining and analysis on a GUI based workflow. This signifies that you do not have to know how to code to be able to work using Orange and mine data, crunch numbers and derive insights.You can perform tasks ranging from basic visuals to data manipulations, transformations, and data mining. It consolidates all the functions of the entire process into a single workflow.The best part and the differentiator about Orange is that it has some wonderful visuals. You can try silhouettes, heat-maps, geo-maps and all sorts of visualizations available.Orange comes built-in with the Anaconda tool if youve previously installed it. If not, follow these steps to download Orange.Step 1: Go to https://orange.biolab.si and click on Download.Step 2: Install the platform and set the working directory for Orange to store its files.This is what the start-up page of Orange looks like. You have options that allow you to create new projects, open recent ones or view examples and get started.Before we delve into how Orange works, lets define a few key terms to help us in our understanding:You can also go to Example Workflows on your start-up screen to check out more workflows once you have created your first one.For now, click on New and lets start building your first workflow.This is the first step towards building a solution to any problem. We need to first understand what steps we need to take in order to achieve our final goal. After you clicked on New in the above step, this is what you should have come up with.This is your blank Workflow on Orange. Now, youre ready to explore and solve any problem by dragging any widget from the widget menu to your workflow.Orange is a platform that can help us solve most problems in Data Science today. Topics that range from the most basic visualizations to training models. You can even evaluate and perform unsupervised learning on datasets:The problem were looking to solve in this tutorial is the practice problem Loan Prediction that can be accessed via this link on Datahack.We begin with the first and the necessary step to understand our data and make predictions: importing our dataStep 1: Click on the Data tab on the widget selector menu and drag the widget File to our blank workflow.Step 2: Double click the File widget and select the file you want to load into the workflow. In this article, as we will be learning how to solve the practice problem Loan Prediction, I will import the training dataset from the same.Step 3: Once you can see the structure of your dataset using the widget, go back by closing this menu.Step 4: Now since we have the raw .csv details, we need to convert it to a format we can use in our mining. Click on the dotted line encircling the File widget and drag, and then click anywhere in the blank space.Step 5: As we need a data table to better visualize our findings, we click on the Data Table widget.Step 6: Now double click the widget to visualize your table.Neat! Isnt it?Lets now visualize some columns to find interesting patterns in our data.4.3.1 Scatter Plot
Click on the semicircle in front of the File widget and drag it to an empty space in the workflow and select the Scatter Plot widget.Once you create a Scatter Plot widget, double click it and explore your data like this! You can select the X and Y axes, colors, shapes, sizes and a lot of other manipulations.The plot Ive explored is a Gender by Income plot, with the colors set to the education levels. As we can see in males, the higher income group naturally belongs to the Graduates!Although in females, we see that a lot of the graduate females are earning low or almost nothing at all. Any specific reason? Lets find out using the scatterplot.One possible reason I found was marriage. A huge number graduates who were married were found to be in lower income groups; this may be due to family responsibilities or added efforts. Makes perfect sense, right?4.3.2 DistributionAnother way to visualize our distributions would be the Distributions widget. Click on the semi-circle again, and drag to find the widget Distributions.Now double click on it and visualize!What we see is a very interesting distribution. We have in our dataset, more number of married males than females.4.3.3 Sieve diagramHow does income relate to the education levels? Do graduates get paid more than non-grads?Lets visualize using a sieve diagram.Click and drag from the File widget and search for Sieve Diagram.Once you place it, double click on it and select your axes!This plot divides the sections of distribution into 4 bins. The sections can be investigated by hovering the mouse over it.For example, graduates and non-graduates are divided 78% by 22%. Then subdivisions of 25% each are made by splitting the applicant incomes into 4 equal groups. Here the task for you, generate insight from these charts and share in the comment section.Lets now look at how to clean our data to start building our model.Here for cleaning purpose, we will impute missing values. Imputation is a very important step in understanding and making the best use of our data.Click on the File widget and drag to find the Impute widget.When you double click on the widget after placing it, you will see that there are a variety of imputation methods you can use. You can also use default methods or choose individual methods for each class separately.Here, I have selected the default method to be Average for numerical values and Most Frequent for text based values (categorical).You can select from a variety of imputations like:The other things you can include in your approach to training your model are Feature Extraction and Generation.For further understanding, follow this article on Data Exploration and Feature Engineering (https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)Beginning with the basics, we will first train a linear model encompassing all the features just to understand how to select and build models.Step 1: First, we need to set a target variable to apply Logistic Regression on it.Step 2:Go to the File widget and double click it.Step 3:Now, double click on the Loan_Status column and select it as the target variable. Click Apply.Step 4: Once we have set our target variable, find the clean data from the Impute widget as follows and place the Logistic Regression widget.Step 5: Double click the widget and select the type of regularization you want to perform.For a better understanding of these, please visit the link about Ridge and Lasso regressionshttps://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/I have chosen Ridge for my analysis, you are free to choose between the two.Step 6: Next, click on the Impute or the Logistic Regression widget and find the Test and Score widget. Make sure you connect both the data and the model to the testing widget.Step 7: Now, click on the Test and Score widget to see how well your model is doing.Step 8: To visualize the results better, drag and drop from the Test and Score widget to fin d Confusion Matrix.Step 9: Once youve placed it, click on it to visualize your findings!This way, you can test out different models and see how accurately they perform.Lets try to evaluate, how a Random Forest would do? Change the modeling method to RandomForest and look at the confusion matrix.Looks decent, but the Logistic Regression performed better.We can try again with a Support Vector Machine.Better than the Random Forest, but still not as good as the Logistic Regression model.Sometimes the simpler methods are the better ones, isnt it?This is how your final workflow would look after you are done with the complete process.For people who wish to work in groups, you can also export your workflows and send it to friends who can work alongside you!The resulting file is of the (.ows) extension and can be opened in any other Orange setup.Orange is a platform that can be used for almost any kind of analysis but most importantly, for beautiful and easy visuals. In this article, we explored how to visualize a dataset. Predictive modeling was undertaken as well, using a logistic regression predictor, SVM, and a random forest predictor to find loan statuses for each person accordingly.Hope this tutorial has helped you figure out aspects of the problem that you might not have understood or missed out on before. It is very important to understand the data science pipeline and the steps we take to train a model, and this should surely help you build better predictive models soon!",https://www.analyticsvidhya.com/blog/2017/09/building-machine-learning-model-fun-using-orange/
Creating & Visualizing Neural Network in R,Learn everything about Analytics|Introduction|Table of Contents|The Basics of Neural Network|Fitting Neural Network in R|Cross Validation of a Neural Network|End Notes,"Share this:|Like this:|Related Articles|Building Machine Learning Model is fun using Orange|30 Questions to test a data scientist on Tree Based Models|
Guest Blog
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural network is an information-processing machine and can be viewed as analogous to human nervous system. Just like human nervous system, which is made up of interconnected neurons, a neural network is made up of interconnected information processing units. The information processing units do not work in a linear manner. In fact, neural network draws its strength from parallel processing of information, which allows it to deal with non-linearity. Neural network becomes handy to infer meaning and detect patterns from complex data sets.Neural network is considered as one of the most useful technique in the world of data analytics. However, it is complex and is often regarded as a black box, i.e. users view the input and output of a neural network but remain clueless about the knowledge generating process. We hope that the article will help readers learn about the internal mechanism of a neural network and get hands-on experience to implement it in R.A neural network is a model characterized by an activation function, which is used by interconnected information processing units to transform input into output. A neural network has always been compared to human nervous system. Information in passed through interconnected units analogous to information passage through neurons in humans. The first layer of the neural network receives the raw input, processes it and passes the processed information to the hidden layers. The hidden layer passes the information to the last layer, which produces the output. The advantage of neural network is that it is adaptive in nature. It learns from the information provided, i.e. trains itself from the data, which has a known outcome and optimizes its weights for a better prediction in situations with unknown outcome.A perceptron, viz. single layer neural network, is the most basic form of a neural network. A perceptron receives multidimensional input and processes it using a weighted summation and an activation function. It is trained using a labeled data and learning algorithm that optimize the weights in the summation processor. A major limitation of perceptron model is its inability to deal with non-linearity. A multilayered neural network overcomes this limitation and helps solve non-linear problems. The input layer connects with hidden layer, which in turn connects to the output layer. The connections are weighted and weights are optimized using a learning rule.There are many learning rules that are used with neural network:a) least mean square;
b) gradient descent;
c) newtons rule;
d) conjugate gradient etc.The learning rules can be used in conjunction with backpropgation error method. The learning rule is used to calculate the error at the output unit. This error is backpropagated to all the units such that the error at each unit is proportional to the contribution of that unit towards total error at the output unit. The errors at each unit are then used to optimize the weight at each connection. Figure 1 displays the structure of a simple neural network model for better understanding.Figure 1 A simple neural network modelNow we will fit a neural network model in R. In this article, we use a subset of cereal dataset shared by Carnegie Mellon University (CMU). The details of the dataset are on the following link: http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. The objective is to predict rating of the cereals variables such as calories, proteins, fat etc. The R script is provided side by side and is commented for better understanding of the user. . The data is in .csv format and can be downloaded by clicking: cereals.Please set working directory in R using setwd( ) function, and keep cereal.csv in the working directory. We use rating as the dependent variable and calories, proteins, fat, sodium and fiber as the independent variables. We divide the data into training and test set. Training set is used to find the relationship between dependent and independent variables while the test set assesses the performance of the model. We use 60% of the dataset as training set. The assignment of the data to training and test set is done using random sampling. We perform random sampling on R using sample ( ) function. We have used set.seed( ) to generate same random sample everytime and maintain consistency. We will use the index variable while fitting neural network to create training and test data sets. The R script is as follows:Now we fit a neural network on our data. We use neuralnet library for the analysis. The first step is to scale the cereal dataset. The scaling of data is essential because otherwise a variable may have large impact on the prediction variable only because of its scale. Using unscaled may lead to meaningless results. The common techniques to scale data are: min-max normalization, Z-score normalization, median and MAD, and tan-h estimators. The min-max normalization transforms the data into a common range, thus removing the scaling effect from all the variables. Unlike Z-score normalization and median and MAD method, the min-max method retains the original distribution of the variables. We use min-max normalization to scale the data. The R script for scaling the data is as follows.The scaled data is used to fit the neural network. We visualize the neural network with weights for each of the variable. The R script is as follows.Figure 3 visualizes the computed neural network. Our model has 3 neurons in its hidden layer. The black lines show the connections with weights. The weights are calculated using the back propagation algorithm explained earlier. The blue line is the displays the bias term.Figure 2 Neural NetworkWe predict the rating using the neural network model. The reader must remember that the predicted rating will be scaled and it must me transformed in order to make a comparison with real rating. We also compare the predicted rating with real rating using visualization. The RMSE for neural network model is 6.05. The reader can learn more about RMSE in another article, which can be accessed by clicking here. The R script is as follows:Figure 3: Predicted rating vs. real rating using neural networkWe have evaluated our neural network method using RMSE, which is a residual method of evaluation. The major problem of residual evaluation methods is that it does not inform us about the behaviour of our model when new data is introduced. We tried to deal with the new data problem by splitting our data into training and test set, constructing the model on training set and evaluating the model by calculating RMSE for the test set. The training-test split was nothing but the simplest form of cross validation method known as holdout method. A limitation of the holdout method is the variance of performance evaluation metric, in our case RMSE, can be high based on the elements assigned to training and test set.The second commonly cross validation technique is k-fold cross validation. This method can be viewed as a recurring holdout method. The complete data is partitioned into k equal subsets and each time a subset is assigned as test set while others are used for training the model. Every data point gets a chance to be in test set and training set, thus this method reduces the dependence of performance on test-training split and reduces the variance of performance metrics. The extreme case of k-fold cross validation will occur when k is equal to number of data points. It would mean that the predictive model is trained over all the data points except one data point, which takes the role of a test set. This method of leaving one data point as test set is known as leave-one-out cross validation. Now we will perform k-fold cross-validation on the neural network model we built in the previous section. The number of elements in the training set, j, are varied from 10 to 65 and for each j, 100 samples are drawn form the dataset. The rest of the elements in each case are assigned to test set. The model is trained on each of the 5600 training datasets and then tested on the corresponding test sets. We compute RMSE of each of the test set. The RMSE values for each of the set is stored in a Matrix[100 X 56]. This method ensures that our results are free of any sample bias and checks for the robustness of our model. We employ nested for loop. The R script is as follows:The RMSE values can be accessed using the variable Matrix.RMSE. The size of the matrix is large; therefore we will try to make sense of the data through visualizations. First, we will prepare a boxplot for one of the columns in Matrix.RMSE, where training set has length equal to 65. One can prepare these box plots for each of the training set lengths (10 to 65). The R script is as follows.Figure 4 BoxplotThe boxplot in Fig. 4 shows that the median RMSE across 100 samples when length of training set is fixed to 65 is 5.70. In the next visualization we study the variation of RMSE with the length of training set. We calculate the median RMSE for each of the training set length and plot them using the following R script.Figure 5 Variation of RMSEFigure 5 shows that the median RMSE of our model decreases as the length of the training the set. This is an important result. The reader must remember that the model accuracy is dependent on the length of training set. The performance of neural network model is sensitive to training-test split.The article discusses the theoretical aspects of a neural network, its implementation in R and post training evaluation. Neural network is inspired from biological nervous system. Similar to nervous system the information is passed through layers of processors. The significance of variables is represented by weights of each connection. The article provides basic understanding of back propagation algorithm, which is used to assign these weights. In this article we also implement neural network on R. We use a publically available dataset shared by CMU. The aim is to predict the rating of cereals using information such as calories, fat, protein etc. After constructing the neural network we evaluate the model for accuracy and robustness. We compute RMSE and perform cross-validation analysis. In cross validation, we check the variation in model accuracy as the length of training set is changed. We consider training sets with length 10 to 65. For each length a 100 samples are random picked and median RMSE is calculated. We show that model accuracy increases when training set is large. Before using the model for prediction, it is important to check the robustness of performance through cross validation.The article provides a quick review neural network and is a useful reference for data enthusiasts. We have provided commented R code throughout the article to help readers with hands on experience of using neural networks.Bio: Chaitanya Sagar is the Founder and CEO of Perceptive Analytics. Perceptive Analytics is one of the top analytics companies in India. It works on Marketing Analytics for ecommerce, Retail and Pharma companies.",https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/
30 Questions to test a data scientist on Tree Based Models,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution||End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|Creating & Visualizing Neural Network in R|30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm|
Ankit Gupta
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Decision Trees are one of the most respected algorithm in machine learning and data science. They are transparent, easy to understand, robust in nature and widely applicable. You can actually see what the algorithm is doing and what steps does it perform to get to a solution. This trait is particularly important in business context when it comes to explaining a decision to stakeholders.This skill test was specially designed for you to test your knowledge on decision tree techniques. More than 750 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions.Here is the leaderboardfor the participants who took the test.Here are some resources to get in depth knowledge in the subject.1) Which of the following is/are true about bagging trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features and samples.2) Which of the following is/are true about boosting trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: BIn boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. Bagging and boosting both can be consider as improving the base learners results.3) Which of the following is/are true about Random Forest and Gradient Boosting ensemble methods?A) 1
B) 2
C) 3
D) 4
E) 1 and 4Solution: EBoth algorithms are design for classification as well as regression task.4) In Random forest you can generate hundreds of trees (say T1, T2 ..Tn) and then aggregate the results of these tree. Which of the following is true about individual(Tk) tree in Random Forest?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: ARandom forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees.5) Which of the following is true about max_depth hyperparameter in Gradient Boosting?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: AIncrease the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always prefer the small depth in final model building.6) Which of the following algorithm doesnt uses learning Rate as of one of its hyperparameter?A) 1 and 3
B) 1 and 4
C) 2 and 3
D) 2 and 4Solution: DRandom Forest and Extra Trees dont have learning rate as a hyperparameter.7) Which of the following algorithm would you take into the consideration in your final model building on the basis of performance?Suppose you have given the following graph which shows the ROC curve for two different classification algorithms such as Random Forest(Red) and Logistic Regression(Blue)A) Random Forest
B) Logistic Regression
C) Both of the above
D) None of theseSolution: ASince, Random forest has largest AUC given in the picture so I would prefer Random Forest8) Which of the following is true about training and testing error in such case?Suppose you want to apply AdaBoost algorithm on Data D which has T observations. You set half the data for training and half for testing initially. Now you want to increase the number of data points for training T1, T2  Tn where T1 < T2. Tn-1 < Tn.A) The difference between training error and test error increases as number of observations increases
B) The difference between training error and test error decreases as number of observations increases
C) The difference between training error and test error will not change
D) None of TheseSolution: BAs we have more and more data, training error increases and testing error de-creases. And they all converge to the true error.9) In random forest or gradient boosting algorithms, features can be of any type. For example, it can be a continuous feature or a categorical feature. Which of the following option is true when you consider these types of features?A) Only Random forest algorithm handles real valued attributes by discretizing them
B) Only Gradient boosting algorithm handles real valued attributes by discretizing them
C) Both algorithms can handle real valued attributes by discretizing them
D) None of theseSolution: CBoth can handle real valued features.10) Which of the following algorithm are not an example of ensemble learning algorithm?A) Random Forest
B) Adaboost
C) Extra Trees
D) Gradient Boosting
E) Decision TreesSolution: EDecision trees doesnt aggregate the results of multiple trees so it is not an ensemble algorithm.11) Suppose you are using a bagging based algorithm say a RandomForest in model building. Which of the following can be true?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: ASince Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building. Random Forest is a black box model you will lose interpretability after using it.Context 12-15 Consider the following figure for answering the next few questions. In the figure, X1 and X2 are the two features and the data point is represented by dots (-1 is negative class and +1 is a positive class). And you first split the data based on feature X1(say splitting point is x11) which is shown in the figure using vertical line. Every value less than x11 will be predicted as positive class and greater than x will be predicted as negative class.12) How many data points are misclassified in above image?A) 1
B) 2
C) 3
D) 4Solution: AOnly one observation is misclassified, one negative class is showing at the left side of vertical line which will be predicting as a positive class.13) Which of the following splitting point on feature x1 will classify the data correctly?A) Greater than x11
B) Less than x11
C) Equal to x11
D) None of aboveSolution: DIf you search any point on X1 you wont find any point that gives 100% accuracy.14) If you consider only feature X2 for splitting. Can you now perfectly separate the positive class from negative class for any one split on X2?A) Yes
B) NoSolution: BIt is also not possible.15) Now consider only one splitting on both (one on X1 and one on X2) feature. You can split both features at any point. Would you be able to classify all data points correctly?A) TRUE
B) FALSESolution: BYou wont find such case because you can get minimum 1 misclassification.Context 16-17 Suppose, you are working on a binary classification problem with 3 input features. And you chose to apply a bagging algorithm(X) on this data. You chose max_features = 2 and the n_estimators =3. Now, Think that each estimators have 70% accuracy.Note: Algorithm X is aggregating the results of individual estimators based on maximum voting16) What will be the maximum accuracy you can get? A) 70%
B) 80%
C) 90%
D) 100%Solution: DRefer below table for models M1, M2 and M3.17) What will be the minimum accuracy you can get? A) Always greater than 70%
B) Always greater than and equal to 70%
C) It can be less than 70%
D) None of theseSolution: CRefer below table for models M1, M2 and M3.18) Suppose you are building random forest model, which split a node on the attribute, that has highest information gain. In the below image, select the attribute which has the highest information gain?
A) Outlook
B) Humidity
C) Windy
D) TemperatureSolution: AInformation gain increases with the average purity of subsets. So option A would be the right answer.19) Which of the following is true about the Gradient Boosting trees?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth are true and self explanatory20) True-False: The bagging is suitable for high variance low bias models?
A) TRUE
B) FALSE
Solution: AThe bagging is suitable for high variance low bias models or you can say for complex models.
21) Which of the following is true when you choose fraction of observations for building the base learners in tree based algorithm?
A) Decrease the fraction of samples to build a base learners will result in decrease in variance
B) Decrease the fraction of samples to build a base learners will result in increase in variance
C) Increase the fraction of samples to build a base learners will result in decrease in variance
D) Increase the fraction of samples to build a base learners will result in Increase in variance
Solution: AAnswer is self explanatoryContext 22-23Suppose, you are building a Gradient Boosting model on data, which has millions of observations and 1000s of features. Before building the model you want to consider the difference parameter setting for time measurement.
22) Consider the hyperparameter number of trees and arrange the options in terms of time taken by each hyperparameter for building the Gradient Boosting model?Note: remaining hyperparameters are sameA) 1~2~3
B) 1<2<3C) 1>2>3
D) None of theseSolution: BThe time taken by building 1000 trees is maximum and time taken by building the 100 trees is minimum which is given in solution B23) Now, Consider the learning rate hyperparameter and arrange the options in terms of time taken by each hyperparameter for building the Gradient boosting model?Note: Remaining hyperparameters are same1. learning rate = 1
2. learning rate = 2
3. learning rate = 3
A) 1~2~3
B) 1<2<3C) 1>2>3
D) None of theseSolution: ASince learning rate doesnt affect time so all learning rates would take equal time.24) In greadient boosting it is important use learning rate to get optimum output. Which of the following is true abut choosing the learning rate?A) Learning rate should be as high as possible
B) Learning Rate should be as low as possible
C) Learning Rate should be low but it should not be very low
D) Learning rate should be high but it should not be very highSolution: CLearning rate should be low but it should not be very low otherwise algorithm will take so long to finish the training because you need to increase the number trees.25) [True or False] Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.A) TRUE
B) FALSESolution: A26) When you use the boosting algorithm you always consider the weak learners. Which of the following is the main reason for having weak learners?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: ATo prevent overfitting, since the complexity of the overall learner increases at each step. Starting with weak learners implies the final classifier will be less likely to overfit.27) To apply bagging to regression trees which of the following is/are true in such case?A) 1 and 2
B) 2 and 3
C) 1 and 3
D) 1,2 and 3Solution: DAll of the options are correct and self explanatory28) How to select best hyperparameters in tree based models?A) Measure performance over training data
B) Measure performance over validation data
C) Both of these
D) None of theseSolution: BWe always consider the validation results to compare with the test result.29) In which of the following scenario a gain ratio is preferred over Information Gain?A) When a categorical variable has very large number of category
B) When a categorical variable has very small number of category
C) Number of categories is the not the reason
D) None of theseSolution: AWhen high cardinality problems, gain ratio is preferred over Information Gain technique.30) Suppose you have given the following scenario for training and validation error for Gradient Boosting. Which of the following hyper parameter would you choose in such case?A) 1
B) 2
C) 3
D) 4Solution: BScenario 2 and 4 has same validation accuracies but we would select 2 because depth is lower is better hyper parameter.Below is the distribution of the scores of the participants:You can access the scores here. More than 350 people participated in the skill test and the highest score obtained was 28.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/
30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm,Learn everything about Analytics|Introduction|Helpful Resources|Skill test Questions and Answers|Overall Distribution|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Tree Based Models|How to create jaw dropping Data Visualizations on the web with D3.js?|
Sunil Ray
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you were to ask me 2 most intuitive algorithms in machine learning  it would be k-Nearest Neighbours (kNN) and tree based algorithms. Both of them are simple to understand, easy to explain and perfect to demonstrate to people. Interestingly, we had skill tests for both these algorithms last month.If you are new to machine learning, make sure you test yourself on understanding of both of these algorithms. They are simplistic, but immensely powerful and used extensively in industry. This skill test will help you test yourself on k-Nearest Neighbours. It is specially designed for you to test your knowledge on kNN and its applications.More than 650 people registered for the test. If you are one of those who missed out on this skill test, here are the questions and solutions. Here is the leaderboardfor the participants who took the test.Here are some resources to get in depth knowledge in the subject.1) [True or False] k-NN algorithm does more computation on test time rather than train time.
A) TRUE
B) FALSE
Solution: AThe training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.In the testing phase, a test point is classified by assigning the label which are most frequent among the k training samples nearest to that query point  hence higher computation.2) In the image below, which would be the best value for k assuming that the algorithm you are using is k-Nearest Neighbor.Solution: BValidation error is the least when the value of k is 10. So it is best to use this value of k3) Which of the following distance metric can not be used in k-NN?
A) Manhattan
B) Minkowski
C) Tanimoto
D) Jaccard
E) Mahalanobis
F) All can be used
Solution: FAll of these distance metric can be used as a distance metric for k-NN.
4) Which of the following option is true about k-NN algorithm?
A) It can be used for classification
B) It can be used for regression
C) It can be used in both classification and regression
Solution: CWe can also use k-NN for regression problems. In this case the prediction can be based on the mean or the median of the k-most similar instances.
5) Which of the following statement is true about k-NN algorithm?A) 1 and 2
B) 1 and 3
C) Only 1
D) All of the aboveSolution: DThe above mentioned statements are assumptions of kNN algorithm6) Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuous variables?
A) K-NN
B) Linear Regression
C) Logistic Regression
Solution: Ak-NN algorithm can be used for imputing missing value of both categorical and continuous variables.
7) Which of the following is true about Manhattan distance?
A) It can be used for continuous variables
B) It can be used for categorical variables
C) It can be used for categorical as well as continuous
D) None of these
Solution: AManhattan Distance is designed for calculating the distance between real valued features.
8) Which of the following distance measure do we use in case of categorical variables in k-NN?A) 1
B) 2
C) 3
D) 1 and 2
E) 2 and 3
F) 1,2 and 3Solution: ABoth Euclidean and Manhattan distances are used in case of continuous variables, whereas hamming distance is used in case of categorical variable.9) Which of the following will be Euclidean Distance between the two data point A(1,3) and B(2,3)?A) 1
B) 2
C) 4
D) 8Solution: Asqrt( (1-2)^2 + (3-3)^2) = sqrt(1^2 + 0^2) = 110) Which of the following will be Manhattan Distance between the two data point A(1,3) and B(2,3)?A) 1
B) 2
C) 4
D) 8Solution: Asqrt( mod((1-2)) + mod((3-3))) = sqrt(1 + 0) = 1Context: 11-12Suppose, you have given the following data where x and y are the 2 input variables and Class is the dependent variable.Below is a scatter plot which shows the above data in 2D space.B)  ClassC) Cant sayD) None of theseSolution: AAll three nearest point are of +class so this point will be classified as +class.12) In the previous question, you are now want use 7-NN instead of 3-KNN which of the following x=1 and y=1 will belong to?B)  ClassC) Cant saySolution: BNow this point will be classified as  class because there are 4  class and 3 +class point are in nearest circle.Context 13-14:Suppose you have given the following 2-class data where + represent a postive class and  is represent negative class.13) Which of the following value of k in k-NN would minimize the leave one out cross validation accuracy?A) 3
B) 5
C) Both have same
D) None of theseSolution: B5-NN will have least leave one out cross validation error.14) Which of the following would be the leave on out cross validation accuracy for k=5?A) 2/14
B) 4/14
C) 6/14
D) 8/14
E) None of the aboveSolution: EIn 5-NN we will have 10/14 leave one out cross validation accuracy.15) Which of the following will be true about k in k-NN in terms of Bias?A) When you increase the k the bias will be increases
B) When you decrease the k the bias will be increases
C) Cant say
D) None of theseSolution: Alarge K means simple model, simple model always condider as high bias16) Which of the following will be true about k in k-NN in terms of variance?A) When you increase the k the variance will increases
B) When you decrease the k the variance will increases
C) Cant say
D) None of theseSolution: BSimple model will be consider as less variance model17) The following two distances(Eucludean Distance and Manhattan Distance) have given to you which generally we used in K-NN algorithm. These distance are between two points A(x1,y1) and B(x2,Y2).Your task is to tag the both distance by seeing the following two graphs. Which of the following option is true about below graph ?Solution: BLeft is the graphical depiction of how euclidean distance works, whereas right one is of Manhattan distance.18) When you find noise in data which of the following option would you consider in k-NN?A) I will increase the value of k
B) I will decrease the value of k
C) Noise can not be dependent on value of k
D) None of theseSolution: ATo be more sure of which classifications you make, you can try increasing the value of k.19) In k-NN it is very likely to overfit due to the curse of dimensionality. Which of the following option would you consider to handle such problem?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CIn such case you can use either dimensionality reduction algorithm or the feature selection algorithm20) Below are two statements given. Which of the following will be true both statements?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth are true and self explanatory21) Suppose you have given the following images(1 left, 2 middle and 3 right), Now your task is to find out the value of k in k-NN in each image where k1 is for 1st, k2 is for 2nd and k3 is for 3rd figure.Solution: D
Value of k is highest in k3, whereas in k1 it is lowest22) Which of the following value of k in the following graph would you give least leave one out cross validation accuracy?Solution: BIf you keep the value of k as 2, it gives the lowest cross validation accuracy. You can try this out yourself.23) A company has build a kNN classifier that gets 100% accuracy on training data. When they deployed this model on client side it has been found that the model is not at all accurate. Which of the following thing might gone wrong? Note: Model has successfully deployed and no technical issues are found at client side except the model performance

A) It is probably a overfitted model
B) It is probably a underfitted model
C) Cant say
D) None of theseSolution: AIn an overfitted module, it seems to be performing well on training data, but it is not generalized enough to give the same results on a new data.24) You have given the following 2 statements, find which of these option is/are true in case of k-NN?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the options are true and are self explanatory.25) Which of the following statements is true for k-NN classifiers?A) The classification accuracy is better with larger values of k
B) The decision boundary is smoother with smaller values of k
C) The decision boundary is linear
D) k-NN does not require an explicit training stepSolution: DOption A: This is not always true. You have to ensure that the value of k is not too high or not too low.Option B: This statement is not true. The decision boundary can be a bit jaggedOption C: Same as option BOption D: This statement is true26) True-False: It is possible to construct a 2-NN classifier by using the 1-NN classifier?A) TRUE
B) FALSESolution: AYou can implement a 2-NN classifier by ensembling 1-NN classifiers27) In k-NN what will happen when you increase/decrease the value of k?A) The boundary becomes smoother with increasing value of K
B) The boundary becomes smoother with decreasing value of K
C) Smoothness of boundary doesnt dependent on value of K
D) None of theseSolution: A
The decision boundary would become smoother by increasing the value of K28) Following are the two statements given for k-NN algorthm, which of the statement(s)is/are true?A) 1
B) 2
C) 1 and 2
D) None of theseSolution: CBoth the statements are trueContext 29-30:Suppose, you have trained a k-NN model and now you want to get the prediction on test data. Before getting the prediction suppose you want to calculate the time taken by k-NN for predicting the class for test data.
Note: Calculating the distance between 2 observation will take D time.29) What would be the time taken by 1-NN if there are N(Very large) observations in test data?A) N*D
B) N*D*2
C) (N*D)/2
D) None of theseSolution: AThe value of N is very large, so option A is correct30) What would be the relation between the time taken by 1-NN,2-NN,3-NN. A) 1-NN >2-NN >3-NN
B) 1-NN < 2-NN < 3-NN
C) 1-NN ~ 2-NN ~ 3-NN
D) None of theseSolution: CThe training time for any value of k in kNN algorithm is the same.Below is the distribution of the scores of the participants:
You can access the scores here. More than 250 people participated in the skill test and the highest score obtained was 24.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/
How to create jaw dropping Data Visualizations on the web with D3.js?,"Learn everything about Analytics|Introduction|Table of Contents|1. Refreshing previous concepts of D3.js|2. A glance at advanced concepts: Scales, Axes and Reading data from external sources|3. Building basic charts and code reusability|4. Visualizing the Game of Thrones Social Network: Force-Directed Graph in action!|5. Case Studies  Different Charts using D3.js|6. A Brief Introduction to dimple.js  D3 made easy!|End Notes","2.1 Scales|2.2 Axes|2.3 Loading data from external sources|3.1 Line Chart|3.2 Area Chart|3.3 Chart as a modular structure|3.4 Case Study|Task  Add Step 7. Extras to the bar chart|Bubble Chart  Visualizing 3 Dimensional data|Concept Map  Relationship between concepts|Map Chart  Visualizing Demographical Data|Sankey Diagrams|Parallel Coordinates|Share this:|Related Articles|30 Questions to test a data scientist on K-Nearest Neighbors (kNN) Algorithm|Solving Multi-Label Classification problems (Case studies included)|
Mohd Sanad Zaki Rizvi
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Selections|Adding Elements|Setting attributes|Appending data to DOM|About Dataset|Data preprocessing  Handling date and type conversions|Plotting line|Bringing it all together|Step 1. Basic HTML and CSS|Step 2. Setting the stage|Step 3. Visualization specific code|Step 4. Create the SVG|Step 5. Loading external data|Step 6. Bringing it all together|Tooltips|Interactivity|Animation|A little context about Game of Thrones|About the dataset|Data format for force-directed graph,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Visualization is the way a data scientist expresses himself / herself. Creating a meaningful visualization requires you to think about the story, the aesthetics of the visualization and various other aspects.If you are planning to create custom visualizations on the web, chances are that youd have already heard aboutD3.js. A web based visualization library that features a plethora of APIs to handle the heavy lifting of creating advanced, dynamic and beautiful visualization content on the web. In this article, we would learn some of D3s powers and use them to create magic of our own! By end of the article, you will be able to create awesome stories for yourself!This article is a continuation of my previous article, Beginners guide to build data visualizations on the web with D3.js . Which I would recommend you to read before going further for a better understanding  Note: The code in this article uses D3 v3.
Lets quickly refresh some of the important concepts we have learned in the previous article!In order to perform an operation on an element in the DOM(our HTML page), we would first need to select it. We do that using d3.select(..) method. If you want to select the <body> tag, you would do :In case you want to add an element to your DOM, you use .append(..) method. For example, to add a <svg> element to your body, you would do :Notice that here we use another important D3 concept, method chaining. Here, we reuse the output of the d3.select(body) and apply the .append() method to add a <svg> element to it.D3 gives you a clean way to add/remove attributes for a selected DOM element. For example, youd like to set the width and height of your <svg> : In D3, if you want to add some data to DOM elements, you would create what is called a Enter, Update, Exit cycle. For example, You want to create a set of rectangles each having a height as given in an array youd do : Now, having refreshed some of the important concepts, lets learn some new topics.These are some of the most useful features that come out of the box with D3. We will cover them one by one:As data enthusiasts, you had probably come across cases where you have data in different scales. For any meaningful inference, it had to be scaled to a common range. For example, when you normalize for values between 0 and 1. Here if you want to rescale a data set, youd need to create a linear scale with fixed domain and range values. Then call the scale with the required value:Here domain(..) is used to set the max and min values of the input data and range(..) is used to set the max and min values of the output needed. A domain of [0,50] and a range of [0,1] means we have created a linear scale thatd map any value in [0,50] to a value in [0,1]. Lets check how well our scale works : It looks the scale is working quite well! D3 supports plenty of scales to cover almost all of your data needs. Read more here.Axes are a very important part of a visualization. They add a great deal of useful information like the magnitude of data, measurement unit and the direction of magnitude etc. We will create an axis with the domain of data [0,50] and the range of our scale will be in [10,400] which represent the pixels. Lets look at the code to fit into our <svg> :Lets beautify the axis a little by adding the following code to the CSS :That looks quite better! I personally feel CSS is like parameter tuning of visualization with D3.What happened here?We created an axis using d3.svg.axis() method. The scale() method then sets the scale to our newly created scale. ticks() sets the number of ticks our axis will have, the default is 10. The above axis without ticks(5) would look like :An axis a combination of many SVG elements that for its scale, ticks, labels etc. So it is better to group them using <g> in the SVG. Thats we appended a group and called xAxis() function to plot the axis. You can read more about axes in D3 here. Now that we know how to plot axes, lets learn how to load external data into D3!As data scientists , we deal with a variety of data formats like JSON, CSV, TSV, XML,HTML etc. D3 supports all of these formats and much more by default. For example, loading data from a tsv is as simple as calling a function :Things to note:Now that we have learned how to create the building blocks of a visualization, lets create some charts using D3.All of the concepts that we have learned till now are used as modules for creating a visualization in D3. We will create two charts of our own, a line chart and an area chart.Line charts are one of the most widely used chart types when it comes to showing time series based data to depict trends over time. Lets create the above line chart!We will be using a sample data set in the form of tab-separated-values. You can find it here. There are three columns in our data set  index , date and close. While index is the index of the entry, date denotes the recorded date of stock and close is the closing price of the stock at the given date.Notice that both the date and price is in string format. Before we can use them to make plots, we need to convert them into usable formats. Make the following changes to your earlier code of loading external data:Lets view our data once again : Looks like we have successfully formatted our data. Lets take a closer look at the code:Now that our data is properly preprocessed, we are ready to convert the data into visuals.When we plot a line in an SVG, we give coordinates of the path to follow but thats a tedious process. Enter D3, here we just need to provide data values and the scale they should follow and D3 does all the heavy lifting of calculating coordinates for us! The following code will create a reusable line generator for our data :Now that we have all the pieces together, lets build the final chart! The following code should make the above chart:One of the magical things about D3 is if you design your code smartly, a lot of it can be reused in other visualizations. An area chart can be thought as a modified version of the line chart, here we shade the region of the chart that comes under the line. The interesting thing here is, you can create the above chart by changing 4-5 lines in the previous charts code!The final code for area chart would be : Notice that we have only made the following changes : Now that we have successfully built some visualizations lets look at some case studies with D3.You have already built a bunch of charts using D3, did you notice any pattern or structure in them? After working with D3 for a while, I have noticed a general structure that I follow in my code to create charts. The following structure is how I logically layout my code :Note that sometimes you wont be able to follow the order because of other factors and that is alright. This is not a rigid rule but just a way to divide and conquer the process of building a chart in D3 so that it can be easily understood and reused whenever possible. Confused much? Lets understand it with a practice problem!This section is very important because here, its your turn to build! You will be building the following beautiful bar chart. After the basic chart is built a huge task awaits you, give your best.This chart is built on a data set of character frequency. The dataset can be found here. Lets follow the general structure that we just learned and try to build this chart!In the above code, we started out by setting up basic margins and width, height values. Then we created a percentage formatter using D3s format(..) so that we can convert our y axis labels to %. Then we defined scales for x and y axis and defined the range for the same. Lastly, since we would be needing color for our bars, we will use one of D3s color bands, category10. The category10 contains the following 10 colors : D3 has more color scales. Read more here.For the above simple bar chart, we dont need any visualization specific code.Now that we have set the stage, it is time to create the SVG along with the above settings:Since we have a URL to load data from and it is in tsv format, we can just make a single function call to load data:Remember some preprocessing of data? Now that we get our data, we need to set the domain of our x and y scales which we couldnt set earlier because we didnt have the data:We have our data nicely formatted and ready, we also have our axes, colors ready. Lets bring it all together and attach them to our chart. Notice the new code added in external data loading function : Now that you have the basic chart drawn, it is time for you to jump into action and do something creative!This is the step where you come in. After all, who doesnt like something extra? Visualization is no different! Since we already have a basic bar chart, I challenge you to go ahead and add animation/interactivity to this chart. The following is a list of common effects that you can easily add to your chart using D3: The above is just a minor set of options. Youll find enough resources in the endnotes to refer. You can post about your approach/discuss your doubts with the community here :https://discuss.analyticsvidhya.com/Go ahead, surprise me with your creativity!Now that you have pretty much learnt all the basics of D3 and made plenty of charts on your own, it is time to move to the next level and do something that is niche to D3, youll be building one of the case study  Force-Directed Graph to visualize a very popular TV Series, Game of Thrones.The TV sensation Game of Thrones is based on George R. R. Martins epic fantasy novel series, A Song of Ice and Fire. The series is famous for its sweeping plotlines, its cast of hundreds of characters, and its complex web of character dynamics. In addition, Game of Thrones is an ensemble piece, featuring many prominent characters with intertwined relationships.Here, interaction among characters and the strength of their relationship is important. Also, there are some characters who are much more influential than others and steer the course of the story.The dataset for this visualization is based on Andrew Beveridges data set of A Storm of Swords, the third book in the series. In Andrews words,We represented each character in the book as a vertex. We then added a link between two characters whenever their names appeared within 15 words of one another. So a link between characters means that they interacted, spoke of one another, or that another character spoke of them together. Characters that interacted frequently are connected multiple times.Using the above data set, I calculated the influence of character based on the the number of times her/his interaction has appeared in the book. For example, if Sansa has 6 records where she is the source and 4 records where she is the target of an interaction, her influence will be 6+4=10.I then formatted the data so that D3 can easily use it. The result was a simple JSON file here.The force directed graph can be divided into two major components  nodes and links. The nodes represent entities whose relationship is to be plotted and similarly links are those relationships. These are analogous to vertex and edge in a graph.D3 expects two arrays for force layout. The first one should be an array of nodes and the second one an array of links. The links refer to the index of the node and should have two attributes source and target. This is exactly how I have laid out our JSON. Now that we are versed in the basics, lets build our Social Network Graph by following the earlier steps : Step 1. Basic HTML and CSS Step 2. Setting the stageStep 3. No visualization specific codeStep 4. Create the SVGStep 5. Load external dataStep 6. Bringing it all together  force layout, nodes, linksStep 7. Adding extras  Labels, Starting the simulationWhats happening here?First, we perform steps 1 to 4 of building a D3 chart as discussed earlier. Then we create a force layout here : When the above code is executed, it asks D3 to make the necessary calculations for the position of each node, the distance between them, calculating coordinates of links joining them etc. All of this happens in the background and D3 takes care of it all. While the above calculations are being done, we need to keep updating the position of the nodes and links. We attach a listener to our force layout : The above code basically tells D3 for every tick (single step of the simulation), redraw(updated) all the nodes and their links. Now when everything is done, we start the simulation : If you want to dig deeper into force layouts, you can read its documentation here.With this, you have your very own GoT Social Graph!Live DemoFew things to note about the visualization:D3 has been used on a versatile set of visualization problems. Lets look into some of the interesting ones.Source LinkWhat would you do if you want to show 3-dimensional data in 2 dimensions? Youd use a bubble chart! The area of the circle and the x and y coordinates are used to encode all the 3 dimensions.Source LinkA concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as causes, requires, or contributes to. In this example, D3 is used to show an improvised concept map to better convey the story. Notice how the branches get highlighted when you select a topic. This is a good example of interactivity D3 enables in your visualizations.Source LinkD3 provides amazing inbuilt support to create interactive, map-based visualizations that can be used to show demographically distributed data.Source LinkSankey diagrams visualize the magnitude of flow between nodes in a network. This intricate diagram shows a possible scenario for UK energy production and consumption in 2050: energy supplies are on the left, and demands are on the right. Intermediate nodes group related forms of production and show how energy is converted and transmitted before it is consumed (or lost!). The thickness of each link encodes the amount of flow from source to target.Source LinkParallel coordinates is a visualization technique used to plot individual data elements across many dimensions. Each of the dimensions corresponds to a vertical axis and each data element is displayed as a series of connected points along the dimensions/axes. In the above example, the very famous iris dataset is plotted.D3 code can be quite verbose. That begs the question, is there a simpler way of creating basic charts using D3 but with fewer lines of code? Thats exactly what dimple.js lets you do. It is a library that is directly built on top of D3 and provides an easy interface to build visualizations. It can be loosely compared to the relationship between keras and tensorflow in python. Lets build a basic visualization using dimple :What happened here?Note: that with only a few lines of code, you were able to create a beautiful visualization. Also, dimple took care of basic interaction and animation. If you want to explore dimple more, check out these other examples created using dimple.js.In this article, we refreshed some of D3s basics and further learned a lot of techniques and new functionalities. We also learned how to preprocess data and successfully build a line chart and modified it into an area chart. We used a practice problem to create a colorful bar chart. We also saw some of the most useful and amazing visualizations created using D3.js. We ended up analyzing Game of Thrones data using one of the case studies  Force-Directed Graph and with this, I hope, this article has given you the necessary impetus on your D3 journey! All of the code used in this article is available at GitHub.Here are some useful links for D3.js :",https://www.analyticsvidhya.com/blog/2017/08/visualizations-with-d3-js/
Solving Multi-Label Classification problems (Case studies included),Learn everything about Analytics|Introduction|Table of Contents|1. What is Multi-Label Classification?|2.Multi-Label v/s Multi-Class|3.Loading and Generating Multi-Label Datasets|4.Techniques for Solving a Multi-Label classification problem|5.Case Studies|6. End Notes,"4.1 Problem Transformation|4.2 Adapted Algorithm|4.3 Ensemble Approaches|1. Audio Categorization|2. Image Categorization|3. Bioinformatics|4. Text Categorization|Share this:|Like this:|Related Articles|How to create jaw dropping Data Visualizations on the web with D3.js?|Getting Started with Audio Data Analysis using Deep Learning (with case study)|
Shubham Jain
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",|4.1.1 Binary Relevance||4.1.2 Classifier Chains||4.1.3 Label Powerset,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"For some reason, Regression and Classification problems end up taking most of the attention in machine learning world. People dont realize the wide variety of machine learning problems which can exist.I, on the other hand, love exploring different variety of problems and sharing my learning with the community here.Previously, Ishared my learnings on Genetic algorithms with the community. Continuing on with my search, I intend to cover a topic which has much less widespread but a nagging problem in the data science community  which is multi-label classification.In this article, I will give you an intuitive explanation of what multi-label classification entails, along with illustration of how to solve the problem. I hope it will show you the horizon of what data science encompasses. So lets get on with it!Let us take a look at the image below.What if I ask you that does this image contains a house? The option will be YES or NO. Consider another case, like what all things (or labels) are relevant to this picture? These types of problems, where we have a set of target variables, are known asmulti-label classification problems. So, is there any difference between these two cases? Clearly, yes because in the second case any image may contain a different set of these multiple labels for different images.But before going deep into multi-label, I just wanted to clear one thing as many of you might be confused that how this is different from the multi-class problem. So, lets us try to understand the difference between these two sets of problems. Consider an example to understand the difference between these two. For this, I hope that below image makes things quite clear. Lets try to understand it. For any movie, Central Board of Film Certification, issue a certificate depending on the contents of the movie. For example, if you look above, this movie has been rated as U/A (meaning Parental Guidance for children below the age of 12 years) certificate. There are other types of certificates classes like A (Restricted to adults) or U (Unrestricted Public Exhibition), but it is sure that each movie can only be categorized with only one out of those three type of certificates. In short, there are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem. Again, if you look back at the image, this movie has been categorized into comedy and romance genre. But there is a difference that this time each movie could fall into one or more different sets of categories. Therefore, each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels. Great! Now you can distinguish between a multi-label and multi-class problem. So, lets start how to deal with these types of problems. Scikit-learn has provided a separate library scikit-multilearn for multi label classification. For better understanding, let us start practicing on a multi-label dataset. You can find a real-world data set from the repository provided by MULAN package. These datasets are present in ARFF format. So, for getting started with any of these datasets, look at the python code below for loading it onto your jupyter notebook. Here I have downloaded the yeast data set from the repository.There is how the data set looks like.Here, Att represents the attributes or the independent variables and Class represents the target variables.For practice purpose, we have another option to generate an artificial multi-label dataset. Let us understand the parameters used above. sparse: If True, returns a sparse matrix, where sparse matrix means a matrix having a large number of zero elements. n_labels: The average number of labels for each instance. return_indicator: If sparse return Y in the sparse binary indicator format.allow_unlabeled: If True, some instances might not belong to any class.You must have noticed that we have used sparse matrix everywhere, and scikit-multilearn also recommends to use data in the sparse form because it is very rare for a real-world data set to be dense. Generally, the number of labels assigned to each instance is very less.Okay, now we have our datasets ready so let us quickly learn the techniques to solve a multi-label problem.Basically, there are three methods to solve a multi-label classification problem, namely: In this method, we will try to transform our multi-label problem into single-label problem(s). This method can be carried out in three different ways as: This is the simplest technique, which basically treats each label as a separate single class classification problem. For example, let us consider a case as shown below. We have the data set like this, where X is the independent feature and Ys are the target variable.
In binary relevance, this problem is broken into 4 different single class classification problems as shown in the figure below.We dont have to do this manually, the multi-learn library provides its implementation in python. So, lets us quickly look at its implementation on the randomly generated data.NOTE: Here, we have used Naive Bayes algorithm but you can use any other classification algorithm.Now, in a multi-label classification problem, we cant simply use our normal metrics to calculate the accuracy of our predictions. For that purpose, we will use accuracy score metric. This function calculates subset accuracy meaning the predicted set of labels should exactly match with the true set of labels.So, let us calculate the accuracy of the predictions.It is most simple and efficient method but the only drawback of this method is that it doesnt consider labels correlation because it treats every target variable independently.In this, the first classifier is trained just on the input data and then each next classifier is trained on the input space and all the previous classifiers in the chain. Lets try to this understand this by an example. In the dataset given below, we have X as the input space and Ys as the labels.In classifier chains, this problem would be transformed into 4 different single label problems, just like shown below. Here yellow colored is the input space and the white part represent the target variable.This is quite similar to binary relevance, the only difference being it forms chains in order to preserve label correlation. So, lets try to implement this using multi-learn library.We can see that using this we obtained an accuracy of about 21%, which is very less than binary relevance. This is maybe due to the absence of label correlation since we have randomly generated the data. In this, we transform the problem into a multi-class problem with one multi-class classifier is trained on all unique label combinations found in the training data. Lets understand it by an example.In this, we find that x1 and x4 have the same labels, similarly, x3 and x6 have the same set of labels. So, label powerset transforms this problem into a single multi-class problem as shown below. So, label powerset has given a unique class to every possible label combination that is present in the training set. Lets us look at its implementation in python. This gives us the highest accuracy among all the three we have discussed till now. The only disadvantage of this is that as the training data increases, number of classes become more. Thus, increasing the model complexity, and would result in a lower accuracy. Now, let us look at the second method to solve multi-label classification problem. Adapted algorithm, as the name suggests, adapting the algorithm to directly perform multi-label classification, rather than transforming the problem into different subsets of problems. For example, multi-label version of kNN is represented by MLkNN. So, let us quickly implement this on our randomly generated data set. Great! You have achieved an accuracy score of 69% on your test data. Sci-kit learn provides inbuilt support of multi-label classification in some of the algorithm like Random Forest and Ridge regression. So, you can directly call them and predict the output. You can check the multi-learn library if you wish to learn more about other types of adapted algorithm. Ensemble always produces better results. Scikit-Multilearn library provides different ensembling classification functions, which you can use for obtaining better results. For the direct implementation, you can check out here. Multi-label classification problems are very common in the real world. So, let us look at some of the areas where we can find the use of them.We have already seen songs being classified into different genres. They are also been classified on the basis of emotions or moods like relaxing-calm, or sad-lonely etc. Source: linkMulti-label classification using image has also a wide range of applications. Images can be labeled to indicate different objects, people or concepts.Multi-Label classification has a lot of use in the field of bioinformatics, for example, classification of genes in the yeast data set.It is also used to predict multiple functions of proteins using several unlabeled proteins. You can check this paper for more information.You all must once check out google news. So, what google news does is, it labels every news to one or more categories such that it is displayed under different categories. For example, take a look at the image below.Image source: Google newsThat same news is present under the categories of India, Technology, Latest etc. because it has been classified into these different labels. Thus making it a multi label classification problem.There are plenty of other areas, so explore and comment down below if you wish to share it with the community. In this article, I introduced you to the concept of multi-label classification problems. I have also covered the approaches to solve this problem and the practical use cases where you may have to handle it using multi-learn library in python.
I hope this article will give you a head start when you face these kinds of problems. If you have any doubts/suggestions, feel free to reach out to me below!",https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/
Getting Started with Audio Data Analysis using Deep Learning (with case study),Learn everything about Analytics|Introduction|Table of Contents|What do you mean by Audio data?|Data Handling in Audio domain|Lets solve the UrbanSound challenge!|Intermission: Our first submission|Lets solve the challenge! Part 2: Building better models|Future steps to explore|End Notes,"The abundance on unstructured data|Applications of Audio Processing|Step 1 and 2 combined: Load audio files and extract features|Step 3: Convert the data to pass it in our deep learning model|Step 4: Run a deep learning model and get results|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Solving Multi-Label Classification problems (Case studies included)|Building your first machine learning model using KNIME (no coding required!)|
Faizan Shaikh
|40 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When you get started with data science, you start simple. You go through simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format. In other words, you are spoon-fed the hardest part in data science pipeline.The datasets in real life are much more complex.You first have to understand it, collect it from various sources and arrange it in a format which is ready for processing. This is even more difficult when the data is in an unstructured format such as image or audio. This is so because you would have to represent image/audio data in a standard way for it to be useful for analysis.Interestingly, unstructured data represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. It also contains a lot of useful & powerful information. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice.Also the body language of the person can show you many more features about a person, because actions speak louder than words! So in short, unstructured data is complex but processing it can reap easy rewards.In this article, I intend to cover an overview of audio / voice processing with a case study so that you would get a hands-on introduction to solving audio processing problems.Lets get on with it!Directly or indirectly, you are always in contact with audio. Your brain is continuously processing and understanding audio data and giving you information about the environment. A simple example can be your conversations with people which you do daily. This speech is discerned by the other person to carry on the discussions. Even when you think you are in a quiet environment, you tend to catch much more subtle sounds, like the rustling of leaves or the splatter of rain. This is the extent of your connection with audio.So can you somehow catch this audio floating all around you to do something constructive? Yes, of course! There are devices built which help you catch these sounds and represent it in computer readable format. Examples of these formats areIf you give a thought on what an audio looks like, it is nothing but a wave like format of data, where the amplitude of audio change with respect to time. This can be pictorial represented as follows.Although we discussed that audio data can be useful for analysis. But what are the potential applications of audio processing? Here I would list a few of themHeres an exercise for you; can you think of an application of audio processing that can potentially help thousands of lives?As with all unstructured data formats, audio data has a couple of preprocessing steps which have to be followed before it is presented for analysis.. We will cover this in detail in later article, here we will get an intuition on why this is done.The first step is to actually load the data into a machine understandable format. For this, we simply take values after every specific time steps. For example; in a 2 second audio file, we extract values at half a second. This is called sampling of audio data, and the rate at which it is sampled is called the sampling rate. Another way of representing audio data is by converting it into a different domain of data representation, namely the frequency domain. When we sample an audio data, we require much more data points to represent the whole data and also, the sampling rate should be as high as possible.On the other hand, if we represent audio data in frequency domain, much less computational space is required. To get an intuition, take a look at the image belowSourceHere, we separate one audio signal into 3 different pure signals, which can now be represented as three unique values in frequency domain.There are a few more ways in which audio data can be represented, for example. using MFCs (Mel-Frequency cepstrums. PS: We will cover this in the later article). These are nothing but different ways to represent the data.Now the next step is to extract features from this audio representations, so that our algorithm can work on these features and perform the task it is designed for. Heres a visual representation of the categories of audio features that can be extracted.After extracting these features, it is then sent to the machine learning model for further analysis.Let us have a better practical overview in a real life project, the Urban Sound challenge. This practice problem is meant to introduce you to audio processing in the usual classification scenario.The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, namely:Heres a sound excerpt from the dataset. Can you guess which class does it belong to?To play this in the jupyter notebook, you can simply follow along with the code.Now let us load this audio in our notebook as a numpy array. For this, we will use librosa library in python. To install librosa, just type this in command lineNow we can run the following code to load the dataWhen you load the data, it gives you two objects; a numpy array of an audio file and the corresponding sampling rate by which it was extracted. Now to represent this as a waveform (which it originally is), use the following codeThe output comes out as followsLet us now visually inspect our data and see if we can find patterns in the dataWe can see that it may be difficult to differentiate between jackhammer and drilling, but it is still easy to discern between dog_barking and drilling. To see more such examples, you can use this codeWe will do a similar approach as we did for Age detection problem, to see the class distributions and just predict the max occurrence of all test cases as that class.Let us see the distributions for this problem.We see that jackhammer class has more values than any other class. So let us create our first submission with this idea.This seems like a good idea as a benchmark for any challenge, but for this problem, it seems a bit unfair. This is so because the dataset is not much imbalanced.Now let us see how we can leverage the concepts we learned above to solve the problem. We will follow these steps to solve the problem.Step 1: Load audio files
Step 2: Extract features from audio
Step 3: Convert the data to pass it in our deep learning model
Step 4: Run a deep learning model and get resultsBelow is a code of how I implemented these stepsNow let us train our modelThis is the result I got on training for 5 epochsSeems ok, but the score can be increased obviously. (PS: I could get an accuracy of 80% on my validation dataset). Now its your turn, can you increase on this score? If you do, let me know in the comments below!Now that we saw a simple applications, we can ideate a few more methods which can help us improve our scoreIn this article, I have given a brief overview of audio processing with an case study on UrbanSound challenge. I have also shown the steps you perform when dealing with audio data in python with librosa package. Giving this shastra in your hand, I hope you could try your own algorithms in Urban Sound challenge, or try solving your own audio problems in daily life. If you have any suggestions/ideas, do let me know in the comments below!Podcast: Play in new window | Download",https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/
Building your first machine learning model using KNIME (no coding required!),Learn everything about Analytics|Introduction|1. Setting Up Your System|2. Introducing KNIME|3. How do you clean your Data?|4. Training your First Model|5. Submitting your Solution|6. Limitations|End Notes,"Why KNIME?|Table of Contents|1.1 Creating your First Workflow|2.1 Importing the data files|2.2 Visualization and Analysis:|3.1 Finding Missing Values|3.2 Imputations|4.1 Implementing a Linear Model|Learn, engage , hack and get hired!|Share this:|Like this:|Related Articles|Getting Started with Audio Data Analysis using Deep Learning (with case study)|Finding chairs the data scientist way! (Hint: using Deep Learning)  Part I|
Analytics Vidhya Content Team
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the biggest challenges for beginners in machine learning / data science is that there is too much to learn simultaneously. Especially so, if you do not know how to code. You need to quickly get used to Linear Algebra, Statistics, other mathematical concepts and learn how to code them! It might end up being a bit overwhelming for the new users.If you have no background in coding and find it difficult to cope with, you can start learning data science with a tool which is GUI driven. This enables you to focus your efforts on learning the subject in initial days. Once you are comfortable with basic concepts, you can always learn how to code later on.In todays article, I will get you started with one such GUI based tool  KNIME. By end of this article, you will be able to predict sales for a retail store without writing a piece of code!Lets get started!KNIME is a platform built for powerful analytics on a GUI based workflow. This means, you do not have to know how to code to be able to work using KNIME and derive insights.You can perform functions ranging from basic I/O to data manipulations, transformations and data mining. It consolidates all the functions of the entire process into a single workflow.To begin with KNIME, you first need to install it and set it up on your PC.Step 1: Go to www.knime.com/downloadsStep 2: Identifying the right version for your PCStep 3: Install the platform and set the working directory for KNIME to store its files.This is how your home screen at KNIME would look like.Before we delve more into how KNIME works, lets define a few key terms to help us in our understanding and then see how to open up a new project in KNIME.Node: A node is the basic processing point of any data manipulations. It can do a number of actions based on what you choose in your workflow.Workflow: A workflow is the sequence of steps or actions you take in your platform to accomplish a particular task.The workflow coach on the left top corner will show you what percentage of the community of KNIME recommends a particular node for usage. The node repository will display all nodes that a particular workflow can have, depending on your needs. You can also go to Browse Example Workflows to check out more workflows once you have created your first one. This is the first step towards building a solution to any problem.To setup a workflow, you can follow these steps.Step 1: Go to File menu, and click on New.Step 2: Create a new KNIME Workflow in your platform and name it Introduction.Step 3: Now when you click on Finish, you should have successfully created your first KNIME workflow.This is your blank Workflow on KNIME. Now, youre ready to explore and solve any problem by dragging any node from the repository to your workflow.KNIME is a platform that can help us solve any problem that we could possibly think of, in the boundaries of data science today. Topics that range from the most basic visualizations or linear regressions to advanced deep learning, KNIME can do it all.As a sample use case, the problem were looking to solve in this tutorial is the practice problem Big Mart Sales that can be accessed at Datahack.The problem statement is as follows,The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.Let us start with the first yet a very important step in understanding the problem; importing our data.Drag and drop the file reader node to the workflow and double click on it. Next, browse the file you need to import into your workflow.In this article, as we will be learning how to solve the practice problem Big Mart Sales, I will import the training dataset from Big Mart Sales.This is what the preview would look like, once you import the dataset.Let us visualize some relevant columns and find the correlation between them. Correlation helps us find what columns might be related to each other and have a higher predictive power to help us in our final results.To create a correlation matrix, we type Linear Correlation in the node repository, then drag and drop it to our workflow.After we drag and drop it like shown, we will connect the output of the file reader to the input of the node Linear Correlation.Click the green button Execute on the topmost panel. Now right click the correlation node and select View: Correlation Matrix to generate the image below.This will help you select the features that are important and required for better predictions by hovering over the particular cell.Next, we will visualize the range and patterns of the dataset to understand it better.One of the primary things we would like to know from our data would be that what item is sold the maximum out of the others.There would be two ways to interpret the information:Search for Scatter Plot under the Views tab in our node repository. Drag and drop it in a similar fashion to your workflow, and connect the output of File Reader to this node.Next, configure your node to select how many rows of the data you need and wish to visualize. [I chose 3000]
Click execute, and then View: Scatter Plot.I have selected the X axis to be Item_Type and the Y axis to be Item_Outlet_Sales.The plot above represents the sales of each item type individually, and shows us that fruits and vegetables are sold in the highest numbers.To understand an average sales estimate of all product types in our database, we will use a pie chart.Click on the Pie Chart node under Views and connect it to your File Reader. Choose the columns you need for segregation and choose your preferred aggregation methods, then apply.This chart shows us that sales were averagely divided over all kinds of products. Starchy Foods amassed the highest average sales of 7.7%.I have used only two types of visuals although you can explore the data in numerous forms while you browse through the Views tab. You can use histograms, line plots etc. to better visualize your data.The other things you can include in your approach before training your model are Data Cleaning and Feature Extraction. Here I will cover an overview of data cleaning steps in KNIME. For further understanding, follow this article on Data Exploration and Feature Engineering.Before we impute values, we need to know which ones are missing.Go to the node repository again, and find the node Missing Values. Drag and drop it, and connect the output of our File Reader to the node.To impute values, select the node Missing Value and click configure. Select the appropriate imputations you want for your data depending on the type of data it is, and Apply.Now when we execute it, our complete dataset with imputed values is ready in the output port of the node Missing Value. For my analysis, I have chosen the imputation methods as:String: Most Frequent ValueNumber (Double): MedianNumber (Integer): MedianYou can choose from a variety of imputation techniques such as:String: Number (Double and Integer):Let us take a look at how we would build a machine learning model in KNIME.To start with the basics, we will first train a Linear Model encompassing all the features of the dataset just to understand how to select features and build a model.Go to your node repository and drag the Linear Regression Learner to your workflow. Then connect the clean data that you gathered in the Output Port of the Missing Value node.This should be your screen visual as of now. In the configuration tab, exclude the Item_Identifier and select the target variable on top. After you complete this task, you need to import your Test data to run your model.Drag and drop another file reader to your workflow and select the test data from your system.As we can see, the Test data contains missing values as well. We will run it through the Missing Value node in the same way we did for the Training data.After weve cleaned our Test data as well, we will now introduce a new node Regression Predictor.Load your Model into the predictor by connecting the learners output to the predictors input. In the predictors second input, load your test data. The predictor will automatically adjust the prediction column based on your learner, but you can alter it manually as well.KNIME has the capability to train some very specialised models as well under the Analytics tab. Here is an in-exhaustive listAfter you execute your predictor now, the output is almost ready for submission.Find the node Column Filter in your node repository and drag it to your workflow. Connect the output of your predictor to the column filter and configure it to filter out the columns you need. In this case, you need Item_Identifier, Outlet_Identifier and the Prediction of Outlet_Sales.Execute the Column Filter and finally, search for the node CSV Writer and document your predictions on your hard drive.Adjust the path to set it where you want the .csv file stored, and execute this node. Finally, open the .csv file to correct the column names as according to our solution. Compress the .csv file into a .zip file and submit your solution!This is the final workflow diagram that was obtained.KNIME workflows are very handy when it comes to portability. They can be sent to your friends or colleagues to build on together, adding to the functionality of your product!To export a KNIME workflow, you can simply click on File -> Export KNIME WorkflowAfter that, select the suitable workflow that you need to export and click finish!This will create a .knwf file that you can send across to anyone and they will be able to access it with one click!KNIME being a very powerful open source tool, has its own set of limitations. The primary ones being:KNIME is a platform that can be used for almost any kind of analysis. In this article, we explored how to visualise a dataset and extract important features from it. Predictive modelling was undertaken as well, using a linear regression predictor to estimate sales for each item accordingly. Finally, we filtered out the required columns and exported it to a .csv file.Hope this tutorial has helped you uncover aspects of the problem that you might have overlooked before. It is very important to understand the data science pipeline and the steps we take to train a model, and this should surely help you build better predictive models soon. Good luck with your endeavors!",https://www.analyticsvidhya.com/blog/2017/08/knime-machine-learning/
Finding chairs the data scientist way! (Hint: using Deep Learning)  Part I,Learn everything about Analytics|Introduction|Table of Contents|Why did I choose to solve this problem?|Simplifying the problem: Chair Recognition in clear image|Taking a step further: Detecting chair location|Challenges and Future Steps|End Notes,"Tasks in the problem|Solving the chair  desk problem|Applying YOLO for Chair Detection|Issue 1|Issue 2|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Building your first machine learning model using KNIME (no coding required!)|CatBoost: A machine learning library to handle categorical (CAT) data automatically|
Faizan Shaikh
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have been going through the deep learning literature for quite some time now. I have also participated in a few challenges to get my hands dirty.But what I enjoy the most is to apply deep learning in a real life problem. A real life problem which encompasses my daily life. This is partly why I picked up this problem of chair count recognition, to finally solve a problem which was unsolved till now!In this article, I will cover how I defined the problem. I will also mention what were the steps I took to solve the problem. Consider it as a raw uncut version of my experience as I tried to solve the problem Let me provide a bit of background about why I wanted to count chairs in a photograph.At Analytics Vidhya, we usually have 10-15 people in the office. But in summers, interns crowd our habitat. So, if we have to do an all team meetings in Summers  we end up pulling chairs from all other roomsGiven my laziness, I thought what if there was an algorithm which could suggest us which room has an unoccupied chair? This will save us the hassle of going from one room to the other in search of a chair.This seemed to be a simple and mundane enough problem, but I saw it as a chance to try out my newly acquired prowess! Can deep learning be used to solve this problem? Well, honestly I dont know how much it could help, but no harm in trying it out right?Now you know what the problem was, let me explain to you my thought process in solving the problem. We can break down the problem into four tasks I decided that I should go from a comparatively easy problem to a more complex problem to reach my goal. That is the reason I divided the problem into these specific four tasks. In this article, I will cover how I attempted the first two tasks and then in the subsequent article, I will show you my attempts for the next two problems.The first and the simplest task for our problem is to find out whether we have a chair in the picture clicked in a room. As of now, I simplified the problem by ignoring the need of video feed by manually taking pictures of the room.For example, if I give you two images, can you tell which one is of a chair? If you have guessed correctly, its the first one. So how did you guess it?You have probably seen a chair so many times that it is not difficult for you to infer if there is a chair in the image or not. In short, you have prior knowledge of what a chair looks like in reality. Similarly, we can have a trained artificial neural network which can do the exact thing for us.By the way, we choose to use artificial neural network over other algorithms because right now, Neural nets are the most powerful and state-of-the-art techniques for solving image processing problems.So what I did was, I took an out-of-the-box pre-trained neural network and applied it to these images. This network was previously trained on ImageNet dataset, which has an assortment of all sorts of classes that are found in the reality.But there was an issue when I let the model recognize an object in the image. It could not correctly classify what object was present in the image. For example, here is an output for the image given belowOn the contrary, it predicted that the image contained a desk rather than a chair. This seemed disheartening because a desk and a chair have very few similarities. A desk is much broader in shape than a chair.As mentioned in m previous article, whenever I encounter a problem when building neural networks, I go through a stepwise approach to tackle the issue. Ill just list down the steps:Step 1: Check the architecture
Step 2: Check the hyper-parameters of neural network
Step 3: Check the Complexity of network
Step 4: Check the Structure of Input data
Step 5: Check the Distribution of dataHere after evaluation, I found that the image input I was giving to the model was incorrect. I was not properly handling the aspect ratio of the image. So to take care of this, I added a custom code which was mentioned in one the kerass issues on github. The updated image looked like this.After taking care of the issue, the model started working correctly and giving out right results.Now that we have recognized that our image contains a chair, the next step was to identify where in the image is the chair present. Along with the chair, we also have to recognize and identify a person in the image. We need to identify a person to discern the occupancy of the chair. Both of these tasks (task 2 and task 3 respectively) will help us to solve a much bigger task of finding out if the chair is occupied or not.For this too, as with the previous task, we will use a pre-trained network which will give us an acceptable score out-of-the-box. For object detection, currently, YOLO network is one the best models which gives a great performance in real time. I have covered a bit about YOLO and how it works in this article. Let us look at how we can leverage this to solve our problem.To setup YOLO in the system, the following simple steps can be followed:Step 1:Step 2:Now to run this to solve our problem, you have to type the below command and give the location of your own imageAfter applying YOLO on our images, I saw that it gave pretty good results. Let me show you some examples of what it can do.                                   Although we have a decent start, there are still some issues which would hinder the deployment of the project as a full-fledged product. I will list down a few of them:The YOLO model still made some mistakes, i.e. it was not a 100% accurate model. For example, in the image below; even a dustbin is categorized as a person!What if in an image, a chair obstructs the view of another chair? Would our algorithm be able to identify the hidden chair? This is a point to ponder upon.Along with these issues, there are some more practical implementation details, like how much time does our algorithm take to recommend a solution, what kind of hardware does it require to run etc. These all things are certainly to be considered before selling our algorithm as a product!Also, as I said earlier that we have only considered the first two tasks and havent touched upon the next two tasks. Our next steps would be to identify the count of chairs in the room and then build an end-to-end product.In this article, I described my personal experience of solving a real life problem. This article covers object detection and recognition in an image; the object specifically being a chair. For recognition, we used a simple pre-trained model for predicting the object in an image. On the other hand, for detection, we used YOLO, which is a state-of-the-art real time technique for object detection.I will continue on with chair count in the next part of the article, where we will cover how to calculate the count of chairs. I hope this will help you solve your own problem someday. Good luck!",https://www.analyticsvidhya.com/blog/2017/08/finding-chairs-deep-learning-part-i/
CatBoost: A machine learning library to handle categorical (CAT) data automatically,Learn everything about Analytics|Introduction|Table of Contents|1. What is CatBoost?|2. Advantages of CatBoost Library|3. CatBoost  Comparison to other boosting libraries|4. Installing CatBoost|5. Solving ML challenge using CatBoost|6. End Notes,"Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Finding chairs the data scientist way! (Hint: using Deep Learning)  Part I|Mining frequent items bought together using Apriori Algorithm (with code in R)|
Sunil Ray
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How many of you have seen this error while building your machine learning models usingsklearn?
I bet most of us! At least in the initial days.This error occurs when dealing with categorical (string) variables. In sklearn, you are required to convert these categories in the numerical format.In order to do this conversion, we use several pre-processing methods like label encoding, one hot encoding and others.In this article, I will discuss arecently open sourced library  CatBoost developed and contributed by Yandex. CatBoost can use categorical features directly and is scalable in nature.This is the first Russian machine learning technology thats an open source, said Mikhail Bilenko, Yandexs head of machine intelligence and research.P.S.You can also read this article written by me before How to deal with categorical variables?.CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Googles TensorFlow and Apples Core ML. It can work with diverse data types to help solve a wide range of problems that businesses face today. To top it up, it provides best-in-class accuracy.It is especially powerful in two ways:CatBoost name comes from two words Category and Boosting.As discussed, thelibrary works well with multiple Categories of data, such as audio, text, image including historical data.Boost comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good result with relatively less data, unlike DL models that need to learn from a massive amount of data.Here is a video message ofMikhail Bilenko, Yandexs head of machine intelligence and research and Anna Veronika Dorogush, Head of Tandex machine learning systems.We have multiple boosting libraries like XGBoost, H2O and LightGBM and all of these perform well on variety of problems. CatBoost developer have compared the performance with competitors on standard ML datasets:The comparison above shows the log-loss value for test data and it is lowest in the case of CatBoost in most cases. It clearly signifies that CatBoost mostly performs better for both tuned and default models.In addition to this, CatBoost does not require conversion of data set to any specific format like XGBoost and LightGBM.CatBoost is easy to install for both Python and R. You need to have 64 bit version of python and R.Below is installation steps for Python and R:4.1 Python Installation:4.2 R InstallationThe CatBoost library can be used to solve both classification and regression challenge. For classification, you can use CatBoostClassifier and for regression, CatBoostRegressor.Heres a live coding window for you to play around the CatBoost code and see the results in real-time:In this article, Im solving Big Mart Sales practice problem using CatBoost. It is a regression challenge so we will use CatBoostRegressor, first I will read basic steps (Ill not perform feature engineering just build a basic model).Now, youll see that we will only identify categorical variables. We will not perform any preprocessing steps for categorical variables:As you can see that a basic model is giving a fair solution and training & testing error are in sync. You can tune model parameters, features to improve the solution.Now, the next task is to predict the outcome for test data set.Thats it! We have built first model with CatBoostIn this article, we saw a recently open sourced boosting library CatBoost by Yandex which can provide state of the art solution for the variety of business problems.One of the key features which excites me about this library is handling categorical values automatically using various statistical methods.We have covered basic details about this library and solved a regression challenge in this article. Ill also recommend you to use this library to solve a business solution and check performance against another state of art models.",https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/
Mining frequent items bought together using Apriori Algorithm (with code in R),Learn everything about Analytics|Introduction:|Table of Contents: |1. The Approach(Apriori Algorithm)|2. Implementing Apriori Algorithm and Key Terms and Usage|3. Interpretations and Analysis|4. End Notes and Summary,"1.1 Handling and Readying The Dataset|1.2 Structural Overview and Prerequisites|3.1 The Item Frequency Histogram|3.2 Graphical Representation|3.3 Individual Rule Representation|3.4 Interactive Scatterplot|Share this:|Like this:|Related Articles|CatBoost: A machine learning library to handle categorical (CAT) data automatically|10 Advanced Deep Learning Architectures Data Scientists Should Know!|
Analytics Vidhya Content Team
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We live in a fast changing digital world. In todays age customers expect the sellers to tell what they might want to buy. I personally end up using Amazons recommendations almost in all my visits to their site.This creates an interesting threat / opportunity situation for the retailers.If you can tell the customers what they might want to buy  it not only improves your sales, but also the customer experience and ultimately life time value.On the other hand, if you are unable to predict the next purchase, the customer might not come back to your store.In this article, we will learn one such algorithm which enables us to predict the items bought together frequently. Once we know this, we can use it to our advantage in multiple ways.When you go to a store, would you not want the aisles to be ordered in such a manner that reduces your efforts to buy things?For example, I would want the toothbrush, the paste, the mouthwash & other dental products on a single aisle  because when I buy, I tend to buy them together. This is done by a way in which we find associations between items.In order to understand the concept better, lets take a simple dataset (lets name it as Coffee dataset) consisting of a few hypothetical transactions. We will try to understand this in simple English.The Coffee dataset consisting of items purchased from a retail store.Coffee dataset:The Association Rules: For this dataset, we can write the following association rules: (Rules are just for illustrations and understanding of the concept. They might not represent the actuals).Rule 1: If Milk is purchased, then Sugar is also purchased.Rule 2: If Sugar is purchased, then Milk is also purchased.Rule 3: If Milk and Sugar are purchased, Then Coffee powder is also purchased in 60% of the transactions.Generally, association rules are written in IF-THEN format. We can also use the term Antecedent for IF (LHS) and Consequent for THEN (RHS).From the above rules, we understand the following explicitly:For example, if we see {Milk} as a set with one item and {Coffee} as another set with one item, we will use these to find sets with two items in the dataset such as {Milk,Coffee} and then later see which products are purchased with both of these in our basket.Therefore now we will search for a suitable right hand side or Consequent. If someone buys Coffee with Milk, we will represent it as {Coffee} => {Milk} where Coffee becomes the LHS and Milk the RHS.When we use these to explore more k-item sets, we might find that {Coffee,Milk} => {Tea}.That means the people who buy Coffee and Milk have a possibility of buying Tea as well.Let us see how the item sets are actually built using the Apriori.Apriori envisions an iterative approach where it uses k-Item sets to search for (k+1)-Item sets. The first 1-Item sets are found by gathering the count of each item in the set. Then the 1-Item sets are used to find 2-Item sets and so on until no more k-Item sets can be explored; when all our items land up in one final observation as visible in our last row of the table above. One exploration takes one scan of the complete dataset.An Item set is a mathematical set of products in the basket.The first part of any analysis is to bring in the dataset. We will be using an inbuilt dataset Groceries from the arules package to simplify our analysis.All stores and retailers store their information of transactions in a specific type of dataset called the Transaction type dataset.The pacman package is an assistor to help load and install the packages. we will be using pacman to load the arules package.The p_load() function from pacman takes names of packages as arguments.If your system has those packages, it will load them and if not, it will install and load them.Example:pacman::p_load(PACKAGE_NAME)pacman::p_load(arules, arulesViz)ORLibrary(arules)Library(arulesViz)data(Groceries"")Before we begin applying the Apriori algorithm on our dataset, we need to make sure that it is of the type Transactions.str(Groceries)The structure of our transaction type dataset shows us that it is internally divided into three slots: Data, itemInfo and itemsetInfo.The slot Data contains the dimensions, dimension names and other numerical values of number of products sold by every transaction made.These are the first 12 rows of the itemInfo list within the Groceries dataset. It gives specific names to our items under the column labels. The level2 column segregates into an easier to understand term, while level1 makes the complete generalisation of Meat.The slot itemInfo contains a Data Frame that has three vectors which categorizes the food items in the first vector Labels.The second & third vectors divide the food broadly into levels like baby food,bags etc.The third slot itemsetInfo will be generated by us and will store all associations.This is what the internal visual of any transaction dataset looks like and there is a dataframe containing products bought in each transaction in our first inspection. Then, we can group those products by TransactionID like we did in our second inspection to see how many times each is sold before we begin with associativity analysis.The above datasets are just for a clearer visualisation on how to make a Transaction Dataset and can be reproduced using the following code:data <- list(
c(""a"",""b"",""c""),
c(""a"",""b""),
c(""a"",""b"",""d""),
c(""b"",""e""),
c(""b"",""c"",""e""),
c(""a"",""d"",""e""),
c(""a"",""c""),
c(""a"",""b"",""d""),
c(""c"",""e""),
c(""a"",""b"",""d"",""e""),
c(""a"",'b','e','c')
)
data <- as(data, ""transactions"")inspect(data)#Convert transactions to transaction ID liststl <- as(data, ""tidLists"")
inspect(tl)Let us check the most frequently purchased products using the summary function.summary(Groceries)The summary statistics show us the top 5 items sold in our transaction set as Whole Milk,Other Vegetables,Rolls/Buns,Soda and Yogurt. (Further explained in Section 3)To parse to Transaction type, make sure your dataset has similar slots and then use the as() function in R.rules <- apriori(Groceries,parameter = list(supp = 0.001, conf = 0.80))We will set minimum support parameter (minSup) to .001.We can set minimum confidence (minConf) to anywhere between 0.75 and 0.85 for varied results.I have used support and confidence in my parameter list. Let me try to explain it:Support: Support is the basic probability of an event to occur. If we have an event to buy product A, Support(A) is the number of transactions which includes A divided by total number of transactions.Confidence: The confidence of an event is the conditional probability of the occurrence; the chances of A happening given B has already happened.Lift: This is the ratio of confidence to expected confidence.The probability of all of the items in a rule occurring together (otherwise known as the support) divided by the product of the probabilities of the items on the left and right side occurring as if there was no association between them.The lift value tells us how much better a rule is at predicting something than randomly guessing. The higher the lift, the stronger the association.Lets find out the top 10 rules arranged by lift.inspect(rules[1:10])As we can see, these are the top 10 rules derived from our Groceries dataset by running the above code.The first rule shows that if we buy Liquor and Red Wine, we are very likely to buy bottled beer. We can rank the rules based on top 10 from either lift, support or confidence.Lets plot all our rules in certain visualisations first to see what goes with what item in our shop.Let us first identify which products were sold how frequently in our dataset.These histograms depict how many times an item has occurred in our dataset as compared to the others.The relative frequency plot accounts for the fact that Whole Milk and Other Vegetables constitute around half of the transaction dataset; half the sales of the store are of these items.arules::itemFrequencyPlot(Groceries,topN=20,col=brewer.pal(8,'Pastel2'),main='Relative Item Frequency Plot',type=""relative"",ylab=""Item Frequency (Relative)"")This would mean that a lot of people are buying milk and vegetables!What other objects can we place around the more frequently purchased objects to enhance those sales too?For example, to boost sales of eggs I can place it beside my milk and vegetables.Moving forward in the visualisation, we can use a graph to highlight the support and lifts of various items in our repository but mostly to see which product is associated with which one in the sales environment.plot(rules[1:20],method = ""graph"",control = list(type = ""items""))This representation gives us a graph model of items in our dataset.The size of graph nodes is based on support levels and the colour on lift ratios. The incoming lines show the Antecedants or the LHS and the RHS is represented by names of items.The above graph shows us that most of our transactions were consolidated around Whole Milk.We also see that all liquor and wine are very strongly associated so we must place these together.Another association we see from this graph is that the people who buy tropical fruits and herbs also buy rolls and buns. We should place these in an aisle together.The next plot offers us a parallel coordinate system of visualisation. It would help us clearly see that which products along with which ones, result in what kinds of sales.As mentioned above, the RHS is the Consequent or the item we propose the customer will buy; the positions are in the LHS where 2 is the most recent addition to our basket and 1 is the item we previously had.The topmost rule shows us that when I have whole milk and soups in my shopping cart, I am highly likely to buy other vegetables to go along with those as well.plot(rules[1:20],method = ""paracoord"",control = list(reorder = TRUE))If we want a matrix representation, an alternate code option would be:plot(rules[1:20],method = ""matrix"",control = list(reorder = TRUE)These plots show us each and every rule visualised into a form of a scatterplot. The confidence levels are plotted on the Y axis and Support levels on the X axis for each rule. We can hover over them in our interactive plot to see the rule.Plot: arulesViz::plotly_arules(rules)The plot uses the arulesViz package and plotly to generate an interactive plot. We can hover over each rule and see the Support, Confidence and Lift.As the interactive plot suggests, one rule that has a confidence of 1 is the one above. It has an exceptionally high lift as well, at 5.17.By visualising these rules and plots, we can come up with a more detailed explanation of how to make business decisions in retail environments.Now, we would place Whole Milk and Vegetables beside each other; Wine and Bottled Beer alongside too.I can make some specific aisles now in my store to help customers pick products easily from one place and also boost the store sales simultaneously.Aisles Proposed: This analysis would help us improve our store sales and make calculated business decisions for people both in a hurry and the ones leisurely shopping.Happy Association Mining!",https://www.analyticsvidhya.com/blog/2017/08/mining-frequent-items-using-apriori-algorithm/
10 Advanced Deep Learning Architectures Data Scientists Should Know!,Learn everything about Analytics|Introduction||Table of Contents|What do we mean by an Advanced Architecture?||Types of Computer Vision Tasks|List of Deep Learning Architectures||||||||End Notes,"1. AlexNet|2. VGG Net|3. GoogleNet|4. ResNet|5. ResNeXt|6. RCNN (Region Based CNN)|7. YOLO (You Only Look Once)|8. SqueezeNet|9. SegNet|10. GAN (Generative Adversarial Network)|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Mining frequent items bought together using Apriori Algorithm (with code in R)|DataHack Summit 2017  Indias largest conference for data science practitioners|
Faizan Shaikh
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is becoming very hard to stay up to date with recent advancements happening in deep learning. Hardly a day goes by without a new innovation or a new application of deep learning coming by. However, most of these advancements are hidden inside a large amount of research papers that are published on mediums like ArXiv / SpringerTo keep ourselves updated, we have created a small reading group to share our learnings internally at Analytics Vidhya. One such learning I would like to share with the community is a a survey of advanced architectures which have been developed by the research community.This article contains some of the recent advancements in Deep Learning along with codes for implementation in keras library. I have also provided links to the original papers, in case you are interested in reading them or want to refer them.To keep the article concise, I have only considered the architectures which have been successful in Computer Vision domain.If you are interested, read on!P.S.: This article assumes the knowledge of neural networks and familiarity with keras. If you need to catch up on these topics, I would strongly recommend you read the following articles first:Deep Learning algorithms consists of such a diverse set of models in comparison to a single traditional machine learning algorithm. This is because of the flexibility that neural network provides when building a full fledged end-to-end model.Neural network can sometimes be compared with lego blocks, where you can build almost any simple to complex structure your imagination helps you to build.We can define an advanced architecture as one that has a proven track record of being a successful model. This is mainly seen in challenges like ImageNet, where your task is to solve a problem, say image recognition, using the data given. Those who dont know what ImageNet is, it is the dataset which is provided in ILSVR (ImageNet Large Scale Visual Recognition) challenge.Also as described in the below mentioned architectures, each of them has a nuance which sets them apart from the usual models; giving them an edge when they are used to solve a problem. These architectures also fall in the category of deep models, so they are likely to perform better than their shallow counterparts.This article is mainly focused on Computer Vision, so it is natural to describe the horizon of computer vision tasks. Computer Vision; as the name suggests is simply creating artificial models which can replicate the visual tasks performed by a human. This essentially means what we can see and what we perceive is a process which can be understood and implemented in an artificial system.The main types of tasks that computer vision can be categorised in are as follows:Now that we have understood what an advanced architecture is and explored the tasks of computer vision, let us list down the most important architectures and their descriptions:AlexNet is the first deep architecture which was introduced by one of the pioneers in deep learning  Geoffrey Hinton and his colleagues. It is a simple yet powerful network architecture, which helped pave the way for groundbreaking research in Deep Learning as it is now. Here is a representation of the architecture as proposed by the authors.When broken down, AlexNet seems like a simple architecture with convolutional and pooling layers one on top of the other, followed by fully connected layers at the top. This is a very simple architecture, which was conceptualised way back in 1980s. The things which set apart this model is the scale at which it performs the task and the use of GPU for training. In 1980s, CPU was used for training a neural network. Whereas AlexNet speeds up the training by 10 times just by the use of GPU.Although a bit outdated at the moment, AlexNet is still used as a starting point for applying deep neural networks for all the tasks, whether it be computer vision or speech recognition.The VGG Network was introduced by the researchers at Visual Graphics Group at Oxford (hence the name VGG). This network is specially characterized by its pyramidal shape, where the bottom layers which are closer to the image are wide, whereas the top layers are deep.As the image depicts, VGG contains subsequent convolutional layers followed by pooling layers. The pooling layers are responsible for making the layers narrower. In their paper, they proposed multiple such types of networks, with change in deepness of the architecture.The advantages of VGG are :On the other hand, its main disadvantage is that it is very slow to train if trained from scratch. Even on a decent GPU, it would take more than a week to get it to work.GoogleNet (or Inception Network) is a class of architecture designed by researchers at Google. GoogleNet was the winner of ImageNet 2014, where it proved to be a powerful model.In this architecture, along with going deeper (it contains 22 layers in comparison to VGG which had 19 layers), the researchers also made a novel approach called the Inception module.As seen above, it is a drastic change from the sequential architectures which we saw previously. In a single layer, multiple types of feature extractors are present. This indirectly helps the network perform better, as the network at training itself has many options to choose from when solving the task. It can either choose to convolve the input, or to pool it directly.The final architecture contains multiple of these inception modules stacked one over the other. Even the training is slightly different in GoogleNet, as most of the topmost layers have their own output layer. This nuance helps the model converge faster, as there is a joint training as well as parallel training for the layers itself.The advantages of GoogleNet are :GoogleNet does not have an immediate disadvantage per se, but further changes in the architecture are proposed, which make the model perform better. One such change is termed as an Xception Network, in which the limit of divergence of inception module (4 in GoogleNet as we saw in the image above) are increased. It can now theoretically be infinite (hence called extreme inception!)ResNet is one of the monster architectures which truly define how deep a deep learning architecture can be. Residual Networks (ResNet in short) consists of multiple subsequent residual modules, which are the basic building block of ResNet architecture. A representation of residual module is as followsIn simple words, a residual module has two options, either it can perform a set of functions on the input, or it can skip this step altogether.Now similar to GoogleNet, these residual modules are stacked one over the other to form a complete end-to-end network.A few more novel techniques which ResNet introduced are:The main advantage of ResNet is that hundreds, even thousands of these residual layers can be used to create a network and then trained. This is a bit different from usual sequential networks, where you see that there is reduced performance upgrades as you increase the number of layers.ResNeXt is said to be the current state-of-the-art technique for object recognition. It builds upon the concepts of inception and resnet to bring about a new and improved architecture. Below image is a summarization of how a residual module of ResNeXt module looks like.Region Based CNN architecture is said to be the most influential of all the deep learning architectures that have been applied to object detection problem. To solve detection problem, what RCNN does is to attempt to draw a bounding box over all the objects present in the image, and then recognize what object is in the image. It works as follows:The structure of RCNN is as follows:YOLO is the current state-of-the-art real time system built on deep learning for solving image detection problems. As seen in the below given image, it first divides the image into defined bounding boxes, and then runs a recognition algorithm in parallel for all of these boxes to identify which object class do they belong to. After identifying this classes, it goes on to merging these boxes intelligently to form an optimal bounding box around the objects.All of this is done in parallely, so it can run in real time; processing upto 40 images in a second.Although it gives reduced performance than its RCNN counterpart, it still has an advantage of being real time to be viable for use in day-to-day problems. Here is a representation of architecture of YOLOThe squeezeNet architecture is one more powerful architecture which is extremely useful in low bandwidth scenarios like mobile platforms. This architecture has occupies only 4.9MB of space, on the other hand, inception occupies ~100MB! This drastic change is brought up by a specialized structure called the fire module. Below image is a representation of fire module.The final architecture of squeezeNet is as follows:SegNet is a deep learning architecture applied to solve image segmentation problem. It consists of sequence of processing layers (encoders) followed by a corresponding set of decoders for a pixelwise classification . Below image summarizes the working of SegNet.One key feature of SegNet is that it retains high frequency details in segmented image as the pooling indices of encoder network is connected to pooling indices of decoder networks. In short, the information transfer is direct instead of convolving them. SegNet is one the the best model to use when dealing with image segmentation problemsGAN is an entirely different breed of neural network architectures, in which a neural network is used to generate an entirely new image which is not present is the training dataset, but is realistic enough to be in the dataset. For example, below image is a breakdown of GANs are made of. I have covered how GANs work in this article. Go through it if you are curious.In this article, I have covered an overview of major deep learning architectures that you should get familiar with. If you have any questions on deep learning architectures, please feel free to share them with me through comments.",https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/
DataHack Summit 2017  Indias largest conference for data science practitioners,Learn everything about Analytics|What is DataHack Summit?|Why do DataHack Summit?|What can you expect from DataHack Summit?|List of confirmed speakers / events|Early Bird Pricing,"Share this:|Like this:|Related Articles|10 Advanced Deep Learning Architectures Data Scientists Should Know!|30 Questions to test your understanding of Logistic Regression|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Vidhya is on a mission  to create next generation of data scientists in India.Over the last 4 years, we have helped millions of people learn data science. We have enabled career transitions and we have seen our community members go through a transformation in their data science journey. I am pretty sure this is just the start  a signal of things to come. I feel fortunate to be able to create a change at this scale!Today, I am excited to announce our largest initiative in this direction  DataHack Summit 2017. If you follow us, you would have already seen some details about DataHack Summit. But, let me present our perspective.DataHack Summit (DHS) is a conference aimed towards data science practitioners. It is a conference which celebrates the awesome work being done by data scientists across the globe and showcases it to the rest of the community. It is a conference where we talk data science, we breathe data science and we experience data science like never before.DataHack Summit is a festival for those who dont see numbers as just numbers. It is a festival for those who see numbers as designs and patterns and trends, those who see and appreciate this art of dealing with data.DHS 2017 aims to show the bleeding edge, the horizon and the impact of data science to the professionals who perform it every day. It aims to inspire you with the new tools, techniques and applications by bringing the best in data science together.We at Analytics Vidhya feel passionately about enhancing data science culture in India. We want people to talk, showcase and learn data science.However, most of the events we have attended in past tend to leave you high and dry. People talk about benefits of data science from a high level and leave out the details. We havent seen people talk about the real challenges faced while solving data problems. For the last few years, we have never walked out of a conference in India feeling satisfied from the learning from the event.DataHack Summit aims to change this. We aim to create a conference where people take pride in talking about the intricacies in data science. We aim to create a conference which leaves you in company of several like minded passionate people. We aim to create a event where data science evolves in India.You can check out the latest update here. Here is a screenshot of some of the confirmed speakers:Dr. Kirk Borne has graciously accepted to be a Keynote speaker for the event. Every speaker is a practicing Data Scientist and comes with tremendous experience in the domain. We will put all our experience in creating top quality content with these world class speakers to make these talks awesome.In addition to the talks and workshops, we will have a fun filled evening. More details to come out soon.The early bird tickets are available till 7th August 2017. Make sure you buy your tickets before the early bird goes away! You will only regret missing it later.Hope this preview gets you as excited as we are about DataHack Summit 2017. If you have any thoughts / suggestions / questions, happy to take them here.Looking forward to see you in Bangalore.",https://www.analyticsvidhya.com/blog/2017/08/datahack-summit-2017-india-data-science-practitioners-conference/
30 Questions to test your understanding of Logistic Regression,Learn everything about Analytics|Introduction|Overall Distribution||Helpful Resources|Skill test Questions and Answers|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|DataHack Summit 2017  Indias largest conference for data science practitioners|30 Questions to test a Data Scientist on Deep Learning (Solution  Skill test, July 2017)|
Ankit Gupta
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Logistic Regression is likely the most commonly used algorithm for solving all classification problems. It is also one of the first methods people get their hands dirty on.We saw the same spirit on the test we designed to assess people on Logistic Regression. More than 800 people took this test. This skill test is specially designed for you to test your knowledge on logistic regression and its nuances.If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how manycould have answered correctly.Here is the leaderboardfor the participants who took the test.Below is the distribution of the scores of the participants:You can access the scores here. More than 800 people participated in the skill test and the highest score obtained was 27.Here are some resources to get in depth knowledge in the subject.1) True-False: Is Logistic regression a supervised machine learning algorithm?A)TRUE
B)FALSESolution: ATrue, Logistic regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variables (x) and an target variable (Y) when you train the model .2) True-False: Is Logistic regression mainly used for Regression?A)TRUE
B)FALSESolution: BLogistic regression is a classification algorithm, dont confuse with the name regression.3) True-False: Is it possible to design a logistic regression algorithm using a Neural Network Algorithm?A)TRUE
B)FALSESolution: ATrue, Neural network is a is a universal approximator so it can implement linear regression algorithm.4) True-False: Is it possible to apply a logistic regression algorithm on a 3-class Classification problem?A)TRUE
B)FALSESolution: AYes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression.5) Which of the following methods do we use to best fit the data in Logistic Regression?A)Least Square Error
B)Maximum Likelihood
C)Jaccard distance
D)Both A and BSolution: BLogistic regression uses maximum likely hood estimate for training a logistic regression.6) Which of the following evaluation metrics can not be applied in case of logistic regression output to compare with target?A)AUC-ROC
B)Accuracy
C) Logloss
D) Mean-Squared-ErrorSolution: DSince, Logistic Regression is a classification algorithm so its output can not be real time value so mean squared error can not use for evaluating it7) One of the very good methods to analyze the performance of Logistic Regression is AIC, which is similar to R-Squared in Linear Regression. Which of the following is true about AIC?A) We prefer a model with minimum AIC value
B) We prefer a model with maximum AIC value
C)Both but depend on the situation
D)None of theseSolution: AWe select the best model in logistic regression which can least AIC. For more information refer this source: http://www4.ncsu.edu/~shu3/Presentation/AIC.pdf8) [True-False] Standardisation of features is required before training a Logistic Regression.A)TRUE
B)FALSESolution: BStandardization isnt required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.9) Which of the following algorithms do we use for Variable Selection?A) LASSO
B)Ridge
C)Both
D)None of theseSolution: AIn case of lasso we apply a absolute penality, after increasing the penality in lasso some of the coefficient of variables may become zero.Context: 10-11Consider a following model for logistic regression: P (y =1|x, w)= g(w0 + w1x)
where g(z) is the logistic function.In the above equation the P (y =1|x; w) , viewed as a function of x, that we can get by changing the parameters w.10) What would be the range of p in such case?
A)(0, inf)
B)(-inf, 0 )
C)(0, 1)
D)(-inf, inf)Solution: CFor values of x in the range of real number from  to + Logistic function will give the output between (0,1)11) In above question what do you think which function would make p between (0,1)?

A)logistic function
B) Log likelihood function
C) Mixture of both
D) None of themSolution:AExplanation is same as question number 10Context: 12-13Suppose you train a logistic regression classifier and your hypothesis function H is12) Which of the following figure will represent the decision boundary as given by above classifier?A)B)C)D)Solution: BOption B would be the right answer. Since our line will be represented by y = g(-6+x2) which is shown in the option A and option B. But option B is the right answer because when you put the value x2 = 6 in the equation then y = g(0) you will get that means y= 0.5 will be on the line, if you increase the value of x2 greater then 6 you will get negative values so output will be the region y =0.13) If you replace coefficient of x1 with x2 what would be the output figure?A)
B)
C)D)Solution: DSame explanation as in previous question.14) Suppose you have been given a fair coin and you want to find out the odds of getting heads. Which of the following option is true for such a case?A)odds will be 0
B)odds will be 0.5
C)odds will be 1
D)None of these
Solution: COdds are defined as the ratio of the probability of success and the probability of failure. So in case of fair coin probability of success is 1/2 and the probability of failure is 1/2 so odd would be 115) The logit function(given as l(x)) is the log of odds function. What could be the range of logit function in the domain x=[0,1]?A)(  , )
B)(0,1)
C)(0, )
D)(- , 0)Solution: AFor our purposes, the odds function has the advantage of transforming the probability function, which has values from 0 to 1, into an equivalent function with values between 0 and . When we take the natural log of the odds function, we get a range of values from - to .16) Which of the following option is true?A)Linear Regression errors values has to be normally distributed but in case of Logistic Regression it is not the case
B)Logistic Regression errors values has to be normally distributed but in case of Linear Regression it is not the case
C)Both Linear Regression and Logistic Regression error values have to be normally distributed
D)Both Linear Regression and Logistic Regression error values have not to be normally distributedSolution:AOnly A is true. Refer this tutorial https://czep.net/stat/mlelr.pdf17) Which of the following is true regarding the logistic function for any value x?Note:
Logistic(x): is a logistic function of any number xLogit(x): is a logit function of any number xLogit_inv(x): is a inverse logit function of any number xA)Logistic(x) = Logit(x)
B)Logistic(x) = Logit_inv(x)
C)Logit_inv(x) = Logit(x)
D)None of theseSolution: BRefer this link for the solution: https://en.wikipedia.org/wiki/Logit18) How will the bias change on using high(infinite) regularisation?Suppose you have given the two scatter plot a and b for two classes( blue for positive and red for negative class). In scatter plot a, you correctly classified all data points using logistic regression ( black line is a decision boundary).A)Bias will be high
B) Bias will be low
C)Cant say
D)None of theseSolution: AModel will become very simple so bias will be very high.19) Suppose, You applied a Logistic Regression model on a given data and got a training accuracy X and testing accuracy Y. Now, you want to add a few new features in the same data. Select the option(s) which is/are correct in such a case.Note: Consider remaining parameters are same.A) Training accuracy increases
B)Training accuracy increases or remains the same
C)Testing accuracy decreases
D)Testing accuracy increases or remains the sameSolution: A and D
Adding more features to model will increase the training accuracy because model has to consider more data to fit the logistic regression. But testing accuracy increases if feature is found to be significant20) Choose which of the following options is true regarding One-Vs-All method in Logistic Regression.A)We need to fit n models in n-class classification problem
B)We need to fit n-1 models to classify into n classes
C)We need to fit only 1 model to classify into n classes
D)None of theseSolution: AIf there are n classes, then n separate logistic regression has to fit, where the probability of each category is predicted over the rest of the categories combined.21) Below are two different logistic models with different values for 0 and 1.Which of the following statement(s) is true about 0 and 1 values of two logistics models (Green, Black)?Note: consider Y = 0 + 1*X. Here, 0 is intercept and 1 is coefficient.A)1 for Green is greater than Black
B)1 for Green is lower than Black
C)1 for both models is same
D)Cant SaySolution: B0 and 1: 0 = 0, 1 = 1 is in X1 color(black) and 0 = 0, 1 = 1 is in X4 color (green)Context 22-24Below are the three scatter plot(A,B,C left to right) and hand drawn decision boundaries for logistic regression.22) Which of the following above figure shows that the decision boundary is overfitting the training data?A)A
B)B
C)C
D)None of theseSolution: CSince in figure 3, Decision boundary is not smooth that means it will over-fitting the data.23) What do you conclude after seeing this visualization?A)1 and 3
B)1 and 3
C)1, 3 and 4
D)5Solution: CThe trend in the graphs looks like a quadratic trend over independent variable X. A higher degree(Right graph) polynomial might have a very high accuracy on the train population but is expected to fail badly on test dataset. But if you see in left graph we will have training error maximum because it underfits the training data24) Suppose, above decision boundaries were generated for the different value of regularization. Which of the above decision boundary shows the maximum regularization?A)A
B)B
C)C
D)All have equal regularizationSolution: ASince, more regularization means more penality means less complex decision boundry that shows in first figure A.25) The below figure shows AUC-ROC curves for three logistic regression models. Different colors show curves for different hyper parameters values. Which of the following AUC-ROC will give best result?
A)Yellow
B)Pink
C)Black
D)All are sameSolution: AThe best classification is the largest area under the curve so yellow line has largest area under the curve.26) What would do if you want to train logistic regression on same data that will take less time as well as give the comparatively similar accuracy(may not be same)?Suppose you are using a Logistic Regression model on a huge dataset. One of the problem you may face on such huge data is that Logistic regression will take very long time to train.A)Decrease the learning rate and decrease the number of iteration
B)Decrease the learning rate and increase the number of iteration
C)Increase the learning rate and increase the number of iteration
D)Increase the learning rate and decrease the number of iterationSolution: DIf you decrease the number of iteration while training it will take less time for surly but will not give the same accuracy for getting the similar accuracy but not exact you need to increase the learning rate.27) Which of the following image is showing the cost function for y =1.Following is the loss function in logistic regression(Y-axis loss function and x axis log probability) for two class classification problem.Note: Y is the target classA)A
B)B
C)Both
D)None of theseSolution: AA is the true answer as loss function decreases as the log probability increases
28) Suppose, Following graph is a cost function for logistic regression.
Now, How many local minimas are present in the graph?A)1
B)2
C)3
D)4Solution: CThere are three local minima present in the graph29) Imagine, you have given the below graph of logistic regression which is shows the relationships between cost function and number of iteration for 3 different learning rate values (different colors are showing different curves at different learning rates ).Suppose, you save the graph for future reference but you forgot to save the value of different learning rates for this graph. Now, you want to find out the relation between the leaning rate values of these curve. Which of the following will be the true relation?Note:A)l1>l2>l3
B)l1 = l2 = l3
C)l1 < l2 < l3D)None of theseSolution: CIf you have low learning rate means your cost function will decrease slowly but in case of large learning rate cost function will decrease very fast.30) Can a Logistic Regression classifier do a perfect classification on the below data?Note: You can use only X1 and X2 variables where X1 and X2 can take only two binary values(0,1).A)TRUE
B)FALSE
C)Cant say
D)None of theseSolution: BNo, logistic regression only forms linear decision surface, but the examples in the figure are not linearly separable.https://www.cs.cmu.edu/~tom/10701_sp11/midterm_sol.pdfI tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/
"30 Questions to test a Data Scientist on Deep Learning (Solution  Skill test, July 2017)",Learn everything about Analytics|Introduction|Overall Distribution||Helpful Resources|Skill test Questions and Answers|End Notes,"Learn, engage,compete,andget hired!|Share this:|Like this:|Related Articles|30 Questions to test your understanding of Logistic Regression|PyData Delhi 2017|
Dishashree Gupta
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whether you are a novice at data science or a veteran, Deep learning is hard to ignore. And it deserves the attention, as deep learning is helping us achieve the AI dream of getting near human performance in every day tasks.Given the importance to learn Deep learning for a data scientist, we created a skill test to help people assess themselves on Deep Learning. A total of 644 people registered for this skill test.If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how manycould have answered correctly.Here is the leaderboardfor the participants who took the test.Below is the distribution of the scores of the participants:You can access the scores here. More than 200 people participated in the skill test and the highest score obtained was 26. Interestingly, the distribution of scores ended up being very similar to past 2 tests:Clearly, a lot of people start the test without understanding Deep Learning, which is not the case with other skill tests. This also means that these solutions would be useful to a lot of people.Here are some resources to get in depth knowledge in the subject.1) Is the data linearly separable?Solution: BIf you can draw a line or plane between the data points, it is said to be linearly separable.2) Which of the following are universal approximators?A)Kernel SVM
B) Neural Networks
C)Boosted Decision Trees
D)All of the aboveSolution: DAll of the above methods can approximate any function.3) In which of the following applications can we use deep learning to solve the problem?A)Protein structure prediction
B)Prediction of chemical reactions
C)Detection of exotic particles
D)All of theseSolution: DWe can use neural network to approximate any function so it can theoretically be used to solve any problem.4) Which of the following statements is true when you use 11 convolutions in a CNN?A)It can help in dimensionality reduction
B)It can be used for feature pooling
C)It suffers less overfitting due to small kernel size
D)All of the aboveSolution: D11 convolutions are called bottleneck structure in CNN.5) Question Context:Statement 1: It is possible to train a network well by initializing all the weights as 0
Statement 2: It is possible to train a network well by initializing biases as 0Which of the statements given above is true?A)Statement 1 is true while Statement 2 is false
B)Statement 2 is true while statement 1 is false
C)Both statements are true
D)Both statements are falseSolution: BEven if all the biases are zero, there is a chance that neural network may learn. On the other hand, if all the weights are zero; the neural neural network may never learn to perform the task.6) The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer areA)50
B)Less than 50
C)More than 50
D)It is an arbitrary valueSolution: ASince MLP is a fully connected directed graph, the number of connections are a multiple of number of nodes in input layer and hidden layer.7) The input image has been converted into a matrix of size 28 X 28 and a kernel/filter of size 7 X 7 with a stride of 1. What will be the size of the convoluted matrix?A)22 X 22
B)21 X 21
C)28 X 28
D)7 X 7Solution: AThe size of the convoluted matrix is given by C=((I-F+2P)/S)+1, where C is the size of the Convoluted matrix, I is the size of the input matrix, F the size of the filter matrix and P the padding applied to the input matrix. Here P=0, I=28, F=7 and S=1. There the answer is 22.8) In a simple MLP model with 8 neurons in the input layer, 5 neurons in the hidden layer and 1 neuron in the output layer. What is the size of the weight matrices between hidden output layer and input hidden layer?A)[1 X 5] , [5 X 8]
B)[8 X 5] , [ 1 X 5]
C)[8 X 5] , [5 X 1]
D)[5 x 1] , [8 X 5]
Solution: DThe size of weights between any layer 1 and layer 2 Is given by [nodes in layer 1 X nodes in layer 2]
9) Given below is an input matrix named I, kernel F and Convoluted matrix named C. Which of the following is the correct option for matrix C with stride =2 ?Solution: C1 and 2 are automatically eliminated since they do not conform to the output size for a stride of 2. Upon calculation option 3 is the correct answer.10) Given below is an input matrix of shape 7 X 7. What will be the output on applying a max pooling of size 3 X 3 with a stride of 2?Solution: AMax pooling takes a 3 X 3 matrix and takes the maximum of the matrix as the output. Slide it over the entire input matrix with a stride of 2 and you will get option (1) as the answer.11) Which of the following functions can be used as an activation function in the output layer if we wish to predict the probabilities of n classes (p1, p2..pk) such that sum of p over all n equals to 1?A)Softmax
B)ReLu
C)Sigmoid
D)TanhSolution: ASoftmax function is of the form in which the sum of probabilities over all k sum to 1.12) Assume a simple MLP model with 3 neurons and inputs= 1,2,3. The weights to the input neurons are 4,5 and 6 respectively. Assume the activation function is a linear constant value of 3. What will be the output ?A)32
B)643
C)96
D)48Solution: CThe output will be calculated as 3(1*4+2*5+6*3) = 9613) Which of following activation function cant be used at output layer to classify an image ?A)sigmoid
B)Tanh
C)ReLU
D)If(x>5,1,0)
E)None of the above
Solution: CReLU gives continuous output in range 0 to infinity. But in output layer, we want a finite range of values. So option C is correct.14) [True | False] In the neural network, every parameter can have their different learning rate.A)TRUE
B)FALSESolution: AYes, we can define the learning rate for each parameter and it can be different from other parameters.15) Dropout can be applied at visible layer of Neural Network model?A)TRUE
B)FALSESolution: ALook at the below model architecture, we have added a new Dropout layer between the input (or visible layer) and the first hidden layer. The dropout rate is set to 20%, meaning one in 5 inputs will be randomly excluded from each update cycle.16) I am working with the fully connected architecture having one hidden layer with 3 neurons and one output neuron to solve a binary classification challenge. Below is the structure of input and output:Input dataset: [ [1,0,1,0] , [1,0,1,1] , [0,1,0,1] ]Output: [ [1] , [1] , [0] ]To train the model, I have initialized all weights for hidden and output layer with 1.What do you say model will able to learn the pattern in the data?A)Yes
B)NoSolution: BAs all the weights of the neural network model are same, so all the neurons will try to do the same thing and the model will never converge.17) Which of the following neural network training challenge can be solved using batch normalization?A)Overfitting
B)Restrict activations to become too high or low
C)Training is too slow
D)Both B and C
E)All of the aboveSolution: DBatch normalization restricts the activations and indirectly improves training time.18) Which of the following would have a constant input in each epoch of training a Deep Learning model?A)Weight between input and hidden layer
B)Weight between hidden and output layer
C)Biases of all hidden layer neurons
D)Activation function of output layer
E) None of the aboveSolution: AWeights between input and hidden layer are constant.19) True/False: Changing Sigmoid activation to ReLu will help to get over the vanishing gradient issue? A)TRUE
B)FALSESolution: AReLU can help in solving vanishing gradient problem.20) In CNN, having max pooling always decrease the parameters? A)TRUE
B)FALSESolution: BThis is not always true. If we have a max pooling layer of pooling size as 1, the parameters would remain the same.21) [True or False] BackPropogation cannot be applied when using pooling layersA)TRUE
B)FALSESolution: BBackPropogation can be applied on pooling layers too.22) What value would be in place of question mark?A)3
B)4
C)5
D)6Solution: BOption B is correct23) For a binary classification problem, which of the following architecture would you choose?A)1
B)2
C)Any one of these
D)None of theseSolution: CWe can either use one neuron as output for binary classification problem or two separate neurons.24) Suppose there is an issue while training a neural network. The training loss/validation loss remains constant. What could be the possible reason?A)Architecture is not defined correctly
B)Data given to the model is noisy
C)Both of theseSolution: CBoth architecture and data could be incorrect. Refer this article https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/25)The red curve above denotes training accuracy with respect to each epoch in a deep learning algorithm. Both the green and blue curves denote validation accuracy.Which of these indicate overfitting?A)Green Curve
B)Blue CurveSolution: BBlue curve shows overfitting, whereas green curve is generalized.26) Which of the following statement is true regrading dropout?1: Dropout gives a way to approximate by combining many different architectures
2: Dropout demands high learning rates
3: Dropout can help preventing overfittingA)Both 1 and 2
B)Both 1 and 3
C)Both 2 and 3
D)All 1, 2 and 3Solution: BStatements 1 and 3 are correct, statement 2 is not always true. Even after applying dropout and with low learning rate, a neural network can learn.27) Gated Recurrent units can help prevent vanishing gradient problem in RNN.A) True
B)FalseSolution: AOption A is correct. This is because it has implicit memory to remember past behavior.28) Suppose you are using early stopping mechanism with patience as 2, at which point will the neural network model stop training?A)2
B)3
C)4
D)5Solution: CAs we have set patience as 2, the network will automatically stop training after epoch 4.29) [True or False] Sentiment analysis using Deep Learning is a many-to one prediction taskA) TRUE
B)FALSESolution: AOption A is correct. This is because from a sequence of words, you have to predict whether the sentiment was positive or negative.30) What steps can we take to prevent overfitting in a Neural Network?A)Data Augmentation
B)Weight Sharing
C)Early Stopping
D)Dropout
E)All of the aboveSolution: EAll of the above mentioned methods can help in preventing overfitting problem.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skill test.For more such skill tests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/08/skilltest-deep-learning/
PyData Delhi 2017,Learn everything about Analytics|About PyData Delhi 2017,"Buy Tickets|Venue|Registration Fee|Share this:|Like this:|Related Articles|30 Questions to test a Data Scientist on Deep Learning (Solution  Skill test, July 2017)|Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience)|
Deepika
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"PyData conferences bring together users and developers of data analysis tools to share ideas and learn from each other. The PyData community gathers to discuss how best to apply Python tools, as well as tools using R and Julia, to meet evolving challenges in data management, processing, analytics, and visualization.We aim to be an accessible, community-driven conference, with tutorials for novices, advanced topical workshops for practitioners, and opportunities for package developers and users to meet in person.For more information about the conference series, visithttp://pydata.org/Website:http://bit.do/pydd17
Submit CFP:http://bit.do/pydd17-cfpPurchase here :http://bit.do/pydd17-ticketsIIIT Delhi, Okhla
Location:https://goo.gl/maps/7hpVhds7JRN2INR 1000",https://www.analyticsvidhya.com/blog/2017/08/pydata-delhi-2017/
Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|PyData Delhi 2017|Data Scientist/Analyst- Chennai (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Experience in credit banking industry required Hands on experience on SAS and Excel/VBA 2-5 years of total experience of Data Analytics / Data Insights Generation  Proven experience in delivering Analytics Solutions  Good communication/problem solving/analytical bent of mind
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : 
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/08/senior-business-analyst-credit-risk-analytics-delhincrbangaloregurgaon-2-5-years-of-experience/
Data Scientist/Analyst- Chennai (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst (Credit Risk Analytics)- Delhi/NCR/Bangalore/Gurgaon (2-5 Years Of Experience)|Introduction to Genetic Algorithm & their application in data science|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Ability to code in SAS, R and Python is a must  Should have handled both structured and unstructured data  Exposure to CPG industry preferred  Ability to learn new techniques and technologies  Strong Academic/professional background is a must
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : 
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/08/data-scientistanalyst-chennai-2-5-years-of-experience/
Introduction to Genetic Algorithm & their application in data science,Learn everything about Analytics|Introduction|Table of Content|1. Intuition behind Genetic Algorithms|2. Biological Inspiration|3. What is a Genetic Algorithm?|4. Steps Involved in Genetic Algorithm|5. Application of Genetic Algorithm|6. Applications in Real World|7. End Notes,"4.1 Initialisation|4.2 Fitness Function |4.3 Selection |4.4 Crossover |4.5 Mutation |5.1 Feature Selection|5.2 Implementation using TPOT library||6.1 Engineering Design|6.2 Traffic and Shipment Routing (Travelling Salesman Problem)|6.3 Robotics|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Data Scientist/Analyst- Chennai (2-5 Years Of Experience)|Tableau for Beginners  Data Visualisation made easy|
Shubham Jain
|31 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Few days back, I started working on a practiceproblem  Big Mart Sales. After applying some simple models and doing some feature engineering, I landed up on 219th position on the leader board.Not bad  but I needed something better.So,I started searching for optimization techniques which could improve my score. It was during this search that I was introduced to genetic algorithms. After applying Genetric algorithm to the practice problem, I ended up taking a considerable leap on the leaderboard.Yes, a jump from 219th to 15th position just on the basis on genetic algorithm. Isnt that great? By end of this article, you will be comfortable applying genetic algorithms and can expect similar benefit on the problems you are working on.Lets start with the famous quote by Charles Darwin:It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line. Let us understand with a basic example: Lets take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this. Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So lets us quickly grasp some little concepts, so that we can draw a parallel line between them.I am sure you would remember:Cells are the basic building block of all living things. Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.Traditionally, these chromosomes are represented in binary as strings of 0s and 1s. Source : linkA chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color. I wanted you to recall these basics concept of biology before going further. Lets get back and understand what actually is a genetic algorithm?Lets get back to the example we discussed above and summarize what we did.This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent. So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results. The working of a genetic algorithm is also derived from biology, which is as shown in the image below.Source: linkSo, let us try to understand the steps one by one.Here, to make things easier, let us understand it by the famous Knapsack problem.If you havent come across this problem, let me introduce my version of this problem. Lets say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of 30 kg. Now you have different survival items, each having its own Survival Points (which are given for each item in the table). So, your objective is maximise the survival points. Here is the table giving details about each item. To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped. This set of chromosome is considered as our initial population.Let us calculate fitness points for our first two chromosomes.For A1 chromosome [100110],Similarly for A2chromosome [001110],So, for this problem, our chromosome will be considered as more fit when it contains more survival points. Therefore chromosome 1 is more fit than chromosome 2.Now, we can select fit chromosomes from our population which can mate and create their off-springs.General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity. Therefore, we generally use Roulette Wheel Selection method. Dont be afraid of name, just take a look at the image below.I suppose we all have seen this, either in real or in movies. So, lets build our roulette wheel. Consider a wheel, and lets divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value. Based on these values, let us create our roulette wheel.So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated. Sometimes we mark two fixed point as shown in the figure below. So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below. This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs. If you take two crossover point, then it will called as multi point crossover which is as shown below. Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population. A simple method of mutation is shown in the image below. So the entire process is summarise as shown in the figure. Source : linkThe off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population. But the question is how we will get to know that we have reached our best possible solution? So basically there are different termination conditions, which are listed below: Now, I suppose you have grasp the basic understanding of the genetic algorithm. So now let us look at some of the application of genetic algorithm in data science.Every time you participate in a data science competition, how do you select features that are important in prediction of the target variable? You always look at the feature importance of some model, and then manually decide the threshold, and select the features which have importance above that threshold.Is there any better way to deal with this kind of situations? Actually one of the most advanced algorithms for feature selection is genetic algorithm.The method here is completely same as the one we did with the knapsack problem.We will again start with the population of chromosome, where each chromosome will be binary string. 1 will denote inclusion of feature in model and 0 will denote exclusion of feature in the model.And another difference would be that the fitness function would be changed. The fitness function here will be our accuracy metric of the competition. The more accurate our set of chromosome in predicting value, the more fit it will be.I suppose, you would now be thinking is there any use of such tough tasks. I will not answer this question now, rather let us look at the implementation of it using TPOT library and then you decide this.So finally, here the comes the part for which you have been waiting from the beginning of this article. First, lets take a quick view on the TPOT (Tree-based Pipeline Optimisation Technique) which is build upon scikit-learn library. A basic pipeline structure is shown in the image below.So the highlighted grey section in the image above is automated using TPOT. This automation is achieved using genetic algorithm.So, without going deep into this, lets directly try to implement it.For using TPOT library, you first have to install some existing python libraries on which TPOT is build. So let us quickly install them.For the implementation part, here I have used Big Mart Sales dataset. So quickly download the train and test file.Now lets look at its python code. Once this code finishes running,tpot_exported_pipeline.pywill contain the Python code for the optimised pipeline. We can see that ExtraTreeRegressor worked best for this problem.If you submit this csv, you will notice that what I promised in the start has not been fulfilled. Was I lying to make you study all of these?No, actually there is a simple rule of TPOT library, if you dont run TPOT for very long, then it may not find the best possible pipeline for your problem.So, increase the number of generations, grab a cup of coffee and go out for a walk. TPOT will finish your work.You can also do classification problems with this library. For more, I would suggest you to once check out its documentation.Besides competitions, genetic algorithm also have many applications in the real world.Genetic algorithm has many applications in real world. Here I have listed some of the interesting application, but explaining each one of them will require me an extra article.Engineering design has relied heavily on computer modeling and simulation to make design cycle process fast and economical. Genetic algorithm has been used to optimize and provide a robust solution.Resources: linkThis is a famous problem and has been efficiently adopted by many sales-based companies as it is time saving and economical. This is also achieved using genetic algorithm.Source: linkThe use of genetic algorithm in the field of robotics is quite big. Actually, genetic algorithm is being used to create learning robots which will behave as a human and will do tasks like cooking our meal, do our laundry etc.Resources: linkNow after these I suppose, you must have developed enough curiosity to look out for some more other interesting applications of genetic algorithms. Also you can comment down if you want to share that with us.I hope that now you have gain enough understanding about what genetic algorithm is and also how to implement it using TPOT library. But this knowledge is not enough, if you dont apply it somewhere.So try to implement it whether in any real world application or in a data science competition.If you face any difficulties, feel free to write on our discussion portal.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/07/introduction-to-genetic-algorithm/
Tableau for Beginners  Data Visualisation made easy,Learn everything about Analytics|Table of Contents :|1. Overview of Tableau|2. Getting Started|3. Other Functionalities|4. Dashboard|5. Story  Bringing it all together|6. End Notes,"|Sample Dashboard in Tableau|1.1 What is Tableau?|1.2 What do you need to know before using Tableau?|1.3 Installation :|2.1 Connect to the Data|2.2 Data Visualisations|2.3 Various Graphs and Charts|3.1 Filters|3.2 Drill Down and Drill Up|3.3 Trend Line|3.4 Forecasting|3.5 Clusters||Learn,Engage,Compete&Get Hired|Share this:|Like this:|Related Articles|Introduction to Genetic Algorithm & their application in data science|Web Scraping in Python using Scrapy (with multiple examples)|
Pavleen Kaur
|48 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The idea is to go from numbers to information to understanding Hans RoslingHave a look at thevisualisation below, which was created by a famous Swedish statistician, Hans Rosling. Hecompiled roughly 200 years of World Development Data and presented it in a very simple manner:This above is an excellent example of Data Visualisation, which rather than focussing on what the numbers are, focuses on telling their story. You can find the interactive version of this visual here.There are multiple Software that are available now at instant access which assist in such easy visualisations and one tool that we are going to cover in this article is Tableau.What can you make out from the picture below?This Dashboard, made on Tableau, represents the Sales and Profit Analysis of a Supermarket. At a glance, you can see:So, in this article, we will learn how to make such simple visualisations in Tableau to understand our data well.Tableau is a Data Visualisation tool that is widely used for Business Intelligence but is not limited to it. It helps create interactive graphs and charts in the form of dashboards and worksheets to gain business insights. And all of this is made possible with gestures as simple as drag and drop!What Products does Tableau offer?You dont need to know much to use Tableau, but still a basic awareness of all the types of graphs such as bar graph, line charts, histograms etc is preferred.Along with that it will be beneficial if you possessed some basic understanding of database management ( datatypes, joins, drill down, drill up etc ) too. Even if you dont, not a reason to worry since I will be covering all such concepts in this and forthcoming articles.To work on Tableau, you need Tableau right?Out of the five above mentioned products, Tableau Desktop, Public and Online offer Data Visual Creation.Tableau Desktop It is available in the following three formats :Tableau PublicTableau Publicis purely free of all costs and does not require any licence. But it comes with a limitation that all of your data and workbooks are made public to all Tableau users.Tableau OnlineTableau Online is the best option for you, if you wish to make your Workbooks on the Cloud and be able to access them from anywhere.Now that you have the suitable product installed and set up, I am pretty sure that your hands must be tingling with anticipation to finally begin! Well lets not keep you waiting then, go ahead and launch the tool.You should see a screen similar to the one above. This is where youimport your data. As is visible, there are multiple formats that your data can be in. It can be in a flat file such as Excel, CSV or you can directly load it from data servers too.You can see that Tableau itself offers some Sample Workbooks, with pre-drawn charts and graphs. I would suggest going through these later for further exploration.The best way to learn is to get your hands dirty. Let us start withour Data, which can be found here.The data is that of a United States Superstore which is deliberating over its expansion. It wishes to know the prospective regions of the country where it could and hence requiresyour help.The first thing that you will obviously need to do is import the data onto Tableau. So quickly follow the below steps:Uh oh, the imported data looks a bit different for the first few rows. Dont worry, the solution lies right ahead.Data Interpreter   3. You see the option of Use Data Interpreter? Click on it to get the following clean view :All that messy data magically disappeared! If you open the Excel data file, you will seesome metadata in it, i.e. information about data :Tableau imports the entire data file as is, but anticipating suchdiscrepancies, explicitly provides a solution in the form of a Data Interpreter.If you wish to view the exact changes that it made, click on Review the results, and choose the Orders tab in the opened Excel sheet.As it will show, it simply removed the erroneous data.As soon as you had imported your dataset, next to the Data Source tab near the bottom of the screen, you immediately must have seen Go to Worksheet. A Worksheet is where you make all of your graphs, so click on that tab to reach the following screen :Dont get overwhelmed by the various elements that you see here, we will cover them all one by one.Lets start with Dimensions and Measures :Moving ontoShelves :Visualisation in Tableau is possible through dragging and dropping Measures and Dimensions onto these different Shelves.Rows and Columns : Represent the x and y  axis of your graphs / charts.
Filter : Filters help you view a strained version of your data. For example, instead of seeing the combined Sales of all the Categories, you can look at a specific one, such as just Furniture.
Pages :Pages work on the same principle as Filters, with the difference that you can actually see the changes as you shift between the Paged values.Remember that Rosling chart? You can easily make one of your own using Pages.
Marks : The Marks property is used to control the mark types of your data. You may choose to represent your data using different shapes, sizes or text.And finally there is Show Me, the brain of Tableau!When you drag and drop fields onto the visualisation area, Tableau makes default graphs for you, as we shall see soon, but you can change these by referring to theShow Me option. Note :Not every graph can be made with any combination of Dimensions or Measures. Each graph has its own conditions for the number and types of fields that can be used, which we shall discuss next.So far we have pretty much covered the requisite theoretical knowledge. Lets finally begin with some visualisations now.I personally prefer to start from the shallow side of the pool, slowly swimming towards the deeper end. So I would suggest beginningbygetting an overview of the Superstore Sales and Profit Statistics.That would include the Net Sales, the Net Profit and the growth of the two measures, to name a few. Here is a gist of what we will be making :From what can be observed, the net Sales are on the rise, but the Profit is creeping up slowly. We can also quite clearly see the peak Sales Months, which could be attributed to various reasons. We can only know more as we explore more.Before we start, there is one thing that I would like torecommend and that is you name yourWorksheets as being done here. Since I will be referencing them back and forth throughout the article, it will be easier for you to follow.Lets begin with the simplest visualisation, and that is displaying the Net Statistics numbers.Tableau, being as smart as it is, automatically computes such valuesunder Measure Names and Measure Values. Follow these steps to make what is called a Text Table:Note : Dont get confused by the different colours of the fields that you see. Just remember one small trick : Blue means Discrete and Green, Continuous.So we have the net Sales and Profit values, lets delve a little deeper by getting the Sales and Profit Values over the years. Lets make another, but a more detailed, Text Table :We have just covered the numeric part of the Dashboard, but that is not its selling point. Its the Line Charts. Lets quickly learn how to make one :If you were to click on Show Me, you will see the different types of Line Charts that you can make, and if you were to hover over each of them, you will get to see their Dimension and Measure requirements too. In case you ever feel lost, I recommend referring to Show Me.With the previous visualisations, we had gotten a brief overview of the Superstore. Lets dig a little deeper now.The next thing that I can think of exploring is the demographic of the Sales and Profit. What are the States that have the highest Sales Revenue, which ones are generating the maximum Profits:Before discussing the inferences, lets first create the Pie Chart ofRegion Sales :From the visual its pretty evident that the two opposite ends, East and West are leading in the Sales game. Lets dissect this a bit more.Note : Whenever you have some geographical data, it is always advisable to plot and see it on a Map to gain better insights.So, we are now going to make the Map Chartof State Sales Distribution :California and New York are the top most sellers from West and East region, butunfortunately there are other States such as Texas, Colorado which even after having good Sales, have negative Profits! This is certainly not good news for the Superstore. You can perceive a good analysis for the other States as well.And lastly, here are the steps for making the Scatter Plotof Sales and Profit Analysis :The findings from the Map chart become more prominent with the following Scatter plot inferences :One of the great things about Tableau is that it lets you interact with the visuals. Have a look at an example :When we clicked on the Central Region, it highlighted and showed the Central States of US, along with their respective Sales and Profit scatter. Here we used the chart as a Filter itself which is a feature of aDashboard. We shall learn how to make one at a later stage.There is onepretty important analysis that we have yet to touch, and that is Product Statistics. High Sales could be easily attributed to the high cost of the products being sold.Also, whenyou are considering expansion, you will want to know the Sales distribution of the Products too:Here we have visualised not just the Sales but also the Profits.Its quite surprising to see Categories that have high Sales, generating negative profits, like Technology in November 2015, or Furniture in October 2016 and this is inferred from thefirst chart, which is also called a Highlight Table. As the name suggests, it highlights the relative proportion of the Measure Values of our data. So lets learn how to make one :The Product Sub Category Salesis a Bar Chart, which is also quite easy to make :From the the above graph, we are getting a good idea of the Net Sales and Profit margins of the various products. Notice that even though Tables Sales are quite high on the scale, its the only product with the least profit.Now, just like before, consider an interaction with the visualisation :We are now able to view each Categorys Products Sales and Profits, at a low level granularity of Year and Month!Congratulations! You have now covered one of the important aspects of Tableau!But its not the end of your learning just yet. Tableau offers some advanced functionalities too, some of which we will cover next :Till now we have only made simple charts, that actuallyprovide cumulative data, that is combined data over the lifetime of the Superstore. To look atSales of a particular Year, a Month, for a certain Product, or to basically view the distinct aspects of the data,Filters are the way to go.Lets head back to the first ever Chart that we had made, of Peak Sales and Profit Months :The visual here is an accumulationof all 4 years of data, for all Regions, States, Categories and Sub Categories.The steps of turning any Dimension into a Filter are the same.Lets first experiment with the Order Date ( formatted to Year ) :By now you must have gotten some picture of the way our Data is built. We have Category as the main Field, divided into Sub  Category, which is further distinguished into the various Product IDs and their corresponding Product Names.This concept of breaking down our data to reach the absolute depth is called Drilling Down :Similarly you can drill down from Order Date to Order ID to Ship Date to Ship Mode. This is also referred to as making an Hierarchy of data.Lets consider the ProductDrillDown first, which is really a Bar Graph :   4. To finally plot your data, dragthe Product Hierarchy onto Rows and Sales onto Columns, and get:This was just a simple Bar Graph, but if you hover over the Category axis, you will see a small plus sign. Click on it toget a granulated version of your data. Do the same for the other generated axis as well to get to the absolute depth.The Tree Analysis of Product Salesis a Tree Map,which is agreat way of representing Drilled Down data, and is quite easy to make :5. Following the drill down fromStep 4, simplygo to Show Me and select the Tree Map chart, to get the following:So far you have analysed the present scenario, but for expansion consideration,lets try and analyse the future too.With the following Dashboard, you can not only see the Trends over the Sales Months, but also a Forecast over the Years too. And both of them tell a different story altogether :Although the Sales of the Superstore are increasing over the the months of a Year, the future in general looks a bit bleak. The sales seem to become constant for the next 3 years, but fortunately for the Superstore, the Profitisincreasing steadily. Lets get to making the above now.Traverse back to the Peak Sales and Profit Month Chart and follow these steps to make a Trend Line of your own :   2. To get the Trend Line, go to Analytics, and simply drag Trend Lineover the chart, to get :For forecasting, we are going to deal with the Sales and Profit Growth chart. The construction is similar to that of Trend Lines, but with a small change. The steps are :Lets head back to the Sales and Profit Analysis chart that we had made.Remember the detailed inference that we had generated from it? We are just going to make that a bit more prominent now, using Clusters.To make them :I am sure by now you must have gotten a pretty good idea of what a Dashboard is, having seen it plenty of times all throughout this article.If not, well then a Dashboard is simply a means of combining Worksheets together so that they convey some message.Without much further ado,lets get right to it! Consider the State Sales Distribution Map chart and Product Sub Categories.What if you wanted to know the various Sales margin of each Product within separate States? We had observed that Texas was one of the States with the lowest Profits. By looking at the following Dashboard, you will see that the reason is its not managing to generate Profits in majority of the Products :Now consider the state wise Sales distribution of a Sub  Category:The above beautifully shows the distribution of Appliances over the country, whereCalifornia seems to be the major Profit contributor.Making such a Dashboard is actually quite easy. Lets see how :Note : Even after the creation of the Dashboards, you can still edit your Worksheets, and the same changes shall be reflected here.If you were to click on the States or the Products after creating your first ever Dashboard, you wont observe any change. Because for such visuals, we first have to convert the Charts themselves into filters.4. Simply click on the small Down Arrow on each chart you wish to turn into a Filter, and select Use as Filter:Note :While making Dashboards, it is preferred to use your charts as Filters, rather than cluttering up the view with custom ones.Just like Dashboards were a way to combine the Worksheets, a Story is where you combine all the dashboards, and if need be individual Sheets as well, to convey, as the name suggests  a Story.So lets combine all those Dashboards that we had made into what could perhaps make a decent presentation for a beginner. Do ensure to Add a Caption to all of your Dashboards, to convey your message clearly :If you have ever come across Tableau Stories online, the ones which you could actually interact with, instead of just viewing, that is made possible by publishing your Workbooks onto the Tableau Server.If you have one set up, then all you need to do, after creating your Stories, is go to Server -> Publish Workbook and enter the Server Name :What we have covered so far is pretty much the basics of Tableau. It has various other features which I will be covering in my forthcoming articles.As it is said With practice, comes perfection, it is suggested that you experiment as much as you can with Tableau.Below is a sample Dashboard that I would encourage everyone of you to try and make. You will not only get to test the skills that you have learned so far, but also hopefully acquiremore. The dataset used is the same as the onewe had been working with so far :If there are ever any doubts, do leave them as comments All the best on your journey as a Data Explorer, and stay tuned for my next article on Tableau!",https://www.analyticsvidhya.com/blog/2017/07/data-visualisation-made-easy/
Web Scraping in Python using Scrapy (with multiple examples),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Overview of Scrapy|2. Write your first Web Scraping code with Scrapy|3. Case studies using Scrapy|End Notes,"2.1 Set up your system|2.2 Scraping Reddit: Fast Experimenting with Scrapy Shell|2.3 Writing Custom Spiders|Scraping an E-Commerce site|Scraping Techcrunch: Creating your own RSS Feed Reader|Learn,Engage,Compete&Get Hired|Share this:|Like this:|Related Articles|Tableau for Beginners  Data Visualisation made easy|Data Scientist  Ahmedabad (4-9 years Of Experience)|
Mohd Sanad Zaki Rizvi
|93 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Scrapy Shell|About Reddit |Extracting title of posts|Extracting Vote counts for each post|Dealing with relative time stamps: extracting time of post creation|Extracting Number of comments:|Creating a scrapy project|Creating a spider|Exporting scraped data as a csv |Extracting image URLs of the product|Extracting product name from <img> tags|How to download product images?|Overview of XPath and XML|Extracting title of post|Extracting author name: Dealing with namespaces in XML,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The explosion of the internet has been a boon for data science enthusiasts. The variety and quantity of data that is available today through the internet islike a treasure trove of secrets and mysteries waiting to be solved. For example, you are planning to travel  how about scraping a few travel recommendation sites, pull out comments about various do to things and see which property is getting a lot of positive responses from the users! The list of use cases is endless.Yet, there is no fixed methodology to extract such data and much of it is unstructured and full of noise.Such conditions make web scraping a necessary technique for a data scientists toolkit. As it is rightfully said,Any content that can be viewed on a webpage can be scraped. Period.With the same spirit, you will be building different kinds of web scraping systems using Python in this article and will learn some of the challenges and ways to tackle them.By end of this article, you would know a framework to scrape the web and would have scrapped multiple websites  lets go!Scrapy is a Python framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format.As diverse the internet is, there is no one size fits all approach in extracting data from websites. Many a time ad hoc approaches are taken and if you start writing code for every little task you perform, you will eventually end up creating your own scraping framework. Scrapy is that framework.With Scrapy you dont need to reinvent the wheel.Note: There are no specific prerequisites of this article, a basic knowledge of HTML and CSS is preferred. If you still think you need a refresher, do a quick read of this article.We will first quickly take a look at how to setup your system for web scraping and then see how we can build a simple web scraping system for extracting data from Reddit website.Scrapy supports both versions of Python 2 and 3. If youre using Anaconda, you can install the package from the conda-forge channel, which has up-to-date packages for Linux, Windows and OS X.To install Scrapy using conda, run:Alternatively, if youre on Linux or Mac OSX, you can directly install scrapy by:Note:This article will followPython 2 with Scrapy.Recently there was a season launch of a prominent TV series (GoTS7) and the social media was on fire, people all around were posting memes, theories, their reactions etc. I had just learnt scrapy and was wondering if it can be used to catch a glimpse of peoples reactions?I love the python shell, it helps me try out things before I can implement them in detail. Similarly, scrapy provides a shell of its own that you can use to experiment. To start the scrapy shell in your command line type:Woah! Scrapy wrote a bunch of stuff. For now, you dont need to worry about it. In order to get information from Reddit (about GoT) you will have to first run a crawler on it. A crawler is a program that browses web sites and downloads content. Sometimes crawlers are also referred as spiders.Reddit is a discussion forum website. It allows users to create subreddits for a single topic of discussion. It supports all the features that conventional discussion portals have like creating a post, voting, replying to post, including images and links etc. Reddit also ranks the post based on their votes using a ranking algorithm of its own.A crawler needs a starting point to start crawling(downloading) content from. Lets see, on googling game of thrones Reddit I found that Reddit has a sub-reddit exclusively for game of thrones at https://www.reddit.com/r/gameofthrones/ this will be the crawlers start URL.To run the crawler in the shell type:When you crawl something with scrapy it returns a response object that contains the downloaded information. Lets see what the crawler has downloaded:This command will open the downloaded page in your default browser.Wow that looks exactly like the website, the crawler has successfully downloaded the entire web page.Lets see how does the raw content looks like:Thats a lot of content but not all of it is relevant. Lets create list of things that need to be extracted :Scrapy provides ways to extract information from HTML based on css selectors like class, id etc. Lets find the css selector for title, right click on any posts title and select Inspect or Inspect Element:This will open the the developer tools in your browser:As it can be seen, the css class title is applied to all <p> tags that have titles. This will helpful in filtering out titles from rest of the content in the response object:Here response.css(..) is a function that helps extract content based on css selector passed to it. The . is used with the title because its a css . Also you need to use ::text to tell your scraper to extract only text content of the matching elements. This is done because scrapy directly returns the matching element along with the HTML code. Look at the following two examples:Notice how ::text helped usfilterand extract only the text content.Now this one is tricky, on inspecting, you get three scores:The score class is applied to all the three so it cant be used as a unique selector is required. On further inspection, it can be seen that the selector that uniquely matches the vote count that we need is the one that contains both score and unvoted.When more than two selectors are required to identify an element, we use them both. Also since both are CSS classes we have to use . with their names. Lets try it out first by extracting the first element that matches:See that the number of votes of the first post is correctly displayed. Note that on Reddit, the votes score is dynamic based on the number of upvotes and downvotes, so itll be changing in real time. We will add ::text to our selector so that we only get the vote value and not the complete vote element. To fetch all the votes:Note:Scrapy has two functions to extract the contentextract() and extract_first().On inspecting the post it is clear that the time element contains the time of the post.There is a catch here though, this is only the relative time(16 hours ago etc.) of the post. This doesnt give any information about the date or time zone the time is in. In case we want to do some analytics, we wont be able to know by which date do we have to calculate 16 hours ago. Lets inspect the time element a little more:The title attribute of time has both the date and the time in UTC. Lets extract this instead:The .attr(attributename) is used to get the value of the specified attribute of the matching element.I leave this as a practice assignment for you. If you have any issues, you can post them here: https://discuss.analyticsvidhya.com/ and the community will help you out  .So far:Note: CSS selectors are a very important concept as far as web scraping is considered, you can read more about it hereand how to use CSS selectors with scrapy.As mentioned above, a spider is a program that downloads content from web sites or a given URL. When extracting data on a larger scale, you would need to write custom spiders for different websites since there is no one size fits all approach in web scraping owing to diversity in website designs. You also would need to write code to convert the extracted data to a structured format and store it in a reusable format like CSV, JSON, excel etc. Thats a lot of code to write, luckily scrapy comes with most of these functionality built in.Lets exit the scrapy shell first and create a new scrapy project:This will create a folder ourfirstscraper with the following structure:For now, the two most important files are:Lets change directory into our first scraper and create a basic spider redditbot :This will create a new spider redditbot.py in your spiders/ folder with a basic template:Few things to note here:After every successful crawl the parse(..) method is called and so thats where you write your extraction logic. Lets add the earlier logic wrote earlier to extract titles, time, votes etc. in the parse function:Note: Here yield scraped_info does all the magic. This line returns the scraped info(the dictionary of votes, titles, etc.) to scrapy which in turn processes it and stores it.Save the file redditbot.py and head back to shell. Run the spider with the following command:Scrapy would print a lot of stuff on the command line. Lets focus on the data.Notice that all the data is downloaded and extracted in a dictionary like object that meticulously has the votes, title, created_at and comments.Getting all the data on the command line is nice but as a data scientist, it is preferable to have data in certain formats like CSV, Excel, JSON etc. that can be imported into programs. Scrapy provides this nifty little functionality where you can export the downloaded content in various formats. Many of the popular formats are already supported.Open the settings.py file and add the following code to it:And run the spider :This will now export all scraped data in a file reddit.csv. Lets see how the CSV looks:What happened here:There are a plethora of forms that scrapy support for exporting feed if you want to dig deeper you can check here and using css selectors in scrapy.Now that you have successfully created a system that crawls web content from a link, scrapes(extracts) selective data from it and saves it in an appropriate structured format lets take the game a notch higher and learn more about web scraping.Lets now look at a few case studies to get more experience of scrapy as a tool and its various functionalities.The advent of internet and smartphones has been an impetus to the e-commerce industry. With millions of customers and billions of dollars at stake, the market has started seeing the multitude of players. Which in turn has led to rise of e-commerce aggregator platforms which collect and show you the information regarding your products from across multiple portals? For example when planning to buy a smartphone and you would want to see the prices at different platforms at a single place. What does it take to build such an aggregator platform? Heres my small take on building an e-commerce site scraper.As a test site, you will scrape ShopClues for 4G-SmartphonesLets first generate a basic spider:This is how the shop clues web page looks like:The following information needs to be extracted from the page:On careful inspection, it can be seen that the attribute data-img of the <img> tag can be used to extract image URLs:Notice that the title attribute of the <img> tag contains the products full name:Similarly, selectors for price(.p_price) and discount(.prd_discount).Scrapy provides reusable images pipelines for downloading files attached to a particular item (for example, when you scrape products and also want to download their images locally).The Images Pipeline has a few extra functions for processing images. It can:In order to use the images pipeline to download images, it needs to be enabled in the settings.py file. Add the following lines to the file :you are basically telling scrapy to use the Images Pipeline and the location for the images should be in the folder tmp/images/. The final spider would now be:A few things to note here:On running the spider the output can be read from tmp/shopclues.csv:You also get the images downloaded. Check the folder tmp/images/full and you will see the images:Also, notice that scrapy automatically adds the download path of the image on your system in the csv:There you have your own little e-commerce aggregator If you want to dig in you can read more about scrapys Images Pipeline hereTechcrunch is one of my favourite blogs that I follow to stay abreast with news about startups and latest technology products. Just like many blogs nowadays TechCrunch gives its own RSS feed here : https://techcrunch.com/feed/ . One of scrapys features is its ability to handle XML data with ease and in this part, you are going to extract data from Techcrunchs RSS feed.Create a basic spider:Lets have a look at the XML, the marked portion is data of interest:Here are some observations from the page:XPath is a syntax that is used to define XML documents. It can be used to traverse through an XML document. Note that XPaths follows a hierarchy.Lets extract the title of the first post. Similar to response.css(..) , the function response.xpath(..) in scrapy to deal with XPath. The following code should do it:Output :Wow! Thats a lot of content, but only the text content of the title is of interest. Lets filter it out:Output :This is much better. Notice that text() here is equivalent of ::text from CSS selectors. Also look at the XPath //item/title/text() here you are basically saying find the element item and extract the text content of its sub element title.Similarly, the xpaths for link, pubDate as :Notice the <creator> tags:The tag itself has some text dc: because of which it cant be extracted using XPath and the author name itself is crowded with ![CDATA.. irrelevant text. These are just XML namespaces and you dont want to have anything to do with them so well ask scrapy to remove the namespace:Now when you try extracting the author name , it will work :Output : uOphir Tanz,Cambron CarterThe complete spider for TechCrunch would be:Lets run the spider:And there you have your own RSS reader :)!In this article, we have just scratched the surface of Scrapys potential as a web scraping tool. Nevertheless, if you have experience with any other tools for scraping it would have been evident by now that in efficiency and practical application, Scrapy wins hands down. All the code used in this article is available on github. Also, check out some of the interesting projects built with Scrapy:",https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/
Data Scientist  Ahmedabad (4-9 years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Web Scraping in Python using Scrapy (with multiple examples)|Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  9 years
Requirements : 
Task Info : Job Description and Responsibilities: Deploy machine learning methodologies and algorithms to build data products that streamline the process from data collection to insight generation.  Assemble data sets from disparate sources and analyze using appropriate quantitative methodologies, computational frameworks and systems  Disseminate findings to non-technical audiences through a variety of media, including interactive visualizations, reports and presentations Work closely with customers and delivery teams to find applications and business use cases for advanced analytical capabilities.  Generating business insight using data analytics and information visualization methods to answer business problems in different domains.  Designing and developing analytics solutions to move towards data-driven innovation in alignment with business priorities and key targets. Desired Skills and Experience :  5+ years working with data and relevant computational frameworks and systems  Have Expert knowledge with Big Data tools like Hadoop and with other Statistical tools like SAS, Matlab, R, SQL, and VBA  Hands-on experience with Machine Learning algorithms, natural language processing, data analytics, and information visualization  Be Fluent with at least one programming language like Java or Python  Have Strong commitment to quality and client service  Have excellent interpersonal skills Have a PhD or a Masters Degree in a Quantitative discipline from a Tier 1 Institute like the IITs, ISI, IIMs, DSE etc.,
College Preference : tier1-any
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-scientist-ahmedabad-4-9-years-of-experience/
Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Ahmedabad (4-9 years Of Experience)|Senior Business Analyst  Bangalore (4-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : 
Task Info : Job Description and Responsibilities:We are looking for a Data Analytics Manager to organize our analytics function and manage our team of data scientists. You will implement tools and strategies to translate raw data into valuable business insights. Key Responsibility: Oversee all analytics operations to correct discrepancies and ensure quality May collect and analyze external market data to provide benchmarks for comparison purposes. Presents reports to management for use in decision making and strategic planning Manages team of data scientists that use business data and statistical methods to provide insight into business performance and suggest area for and methods of improving operations  Implements analytical approaches and methodologies and assists in the interpretation of results.Skill Sets / Requirements:  Proven experience as a Data Analytics Expert  Very good knowledge of predictive modeling and statistical techniques  Knowledge of Excel, R and SQL; familiarity with business intelligence tools  Problem solving attitude  Ability to learn and adapt quickly and to correctly apply new tools and technology.  Strong leadership and mentoring skillsQualification: B.tech/BE (Technology or Statistics related) Masters in Business or Analytics related (preferred)
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-analytics-manager-delhincr-5-8-years-of-experience/
Senior Business Analyst  Bangalore (4-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analytics (Manager)- Delhi/NCR (5-8 Years Of Experience)|Manager (Data Analyst)- Pune (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Work with engineering and research teams on designing, building and deploying data analysis systems for large data sets  Design, develop and implement R&D and pre-product prototype solutions and implementations using off the shelf tools (e.g. R, SAS,SPSS), and software (e.g. Python, Java, C/C++, .NET)  Create algorithms to extract information from large data sets.  Establish scalable, efficient, automated processes for model development, model validation, model implementation and large scale data analysis.  Develop metrics and prototypes that can be used to drive business decisions. Should Have: Strong background in statistical concepts and calculations with 3.5+ yrs experience with real data  Innovative and strong analytical and algorithmic problem solvers.  Proficiency with statistical analysis tools (e.g. R, SAS,SPSS), software development technologies (e.g. Python, Java, C/C++, .Net)  Extensive experience solving analytical problems using quantitative approaches (e.g. Bayesian Analysis, Reduced Dimensional Data Representations, and Multi-scale Feature Identification).  Expert at data visualization and presentation.  Excellent critical thinking skills, combined with the ability to present your beliefs clearly and compellingly verbally and in written form.Qualification: Bachelor Degree in Engineering from top institute/college  IIT, NIT, BITS Pilani etc. Or MS / MSc in Statistics only
College Preference : tier1-any
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/senior-business-analyst-bangalore-4-5-years-of-experience/
Manager (Data Analyst)- Pune (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst  Bangalore (4-5 Years Of Experience)|Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Strong Influencing skills to coordinate/ get things done cross functionally across the organization  Strong analytical skills & proficiency in MS Office to provide analytics & presentations enabling the Senior Management to take decisions.  Strong Project Management & Coordination Skills.  Strong problem solving, process & structured approach to work.  Ability to lead & effectively manage Direct & Indirect teams.  Emotionally balanced and mature to handle pressure & build relationships.  Willingness to stretch & work beyond role boundary. Educational Level & Work Experience : Graduate Degree in Commerce/ Economics/Business Management/ Ideal would be a MBA Degree  Experience in Fraud/ Risk/ Compliance Management  High Proficiency in Data Analytics
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/manager-data-analyst-pune-2-5-years-of-experience/
Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager (Data Analyst)- Pune (2-5 Years Of Experience)|Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Programming and analytical skills in a major analytics software package (e.g. R, Python)  Superior analytical and quantitative skills, with experience working with large, complex data sets.  Strong troubleshooting, analytical and creative problem solving skills. Ability to collect, manipulate, cleanse and interpret data from multiple data sources, required.  Proven experience in analytically driven supply chain management techniques  Candidate should have excellent written and verbal communication  He/she should be highly motivated and self-directed, capable of multi-tasking and should be able to work with minimal supervision  Strong interpersonal skills, including ability to work in a cross functional environment and communicate with all levels of the organization.  Should have strong work ethic and positive team attitude  Candidate should be open for learning and should have the ability to adapt and learn different reporting tools, languages and applications  Experience with Supply Chain technologies and tools. Bachelors degree with minimum work experience of 2 years with at least 2 years of experience in Analytics or Supply Chain in a corporate setting. Candidate should have an advance knowledge of MS Excel,MS word, Power point
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/data-analyst-delhincrgurugram-3-5-years-of-experience/
Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- Delhi/NCR/Gurugram- (3-5 Years Of Experience)|Beginners guide to build data visualisations on the web with D3.js|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 
Task Info : Job Description and Responsibilities: Create scalable, efficient, automated processes for large scale data analysis. Perform comprehensive analysis to support ad-hoc reporting requests. Develop and maintain predictive models for customer attrition and upsell. Own demand generation analytics and reporting which includes proposing, driving creation of and managing metrics and analyses that monitor and improve marketing effectiveness. Develop and execute standardized reporting and analysis to improve our understanding of customer behavior. Retrieve and analyze complex and large data sets to provide insights on customer life cycle and customer engagement. Analyze historical data to identify significant trends and drivers.  Develop A/B test scenarios and monitor results of customer engagement initiatives. Propose recommendations based on the analysis of those results.  Provide support for new functionalities and product launches within the Amazon Digital Music ecosystem. Assist in the definition and delivery of a marketing strategy for these new releases.  Have the obsession to drive a better customer experience through everything that we do at Amazon. Basic qualifications: BA/BS degree in Mathematics, Statistics, Computer Science, or any other quantitative field; MBA preferred.  5+ years professional experience in Analytics, Business Intelligence, or a Data Science role preferably in an e-commerce or retail company with large, complex data sources.  Advanced SQL writing skills and experience querying very large relational databases  Experience building and applying statistical models for forecasting, predicting outcomes, or understanding relationships in data  Must be passionate about data analytics and have a natural curiosity and ability to dive deep into large datasets uncovering insights and presenting relevant findings to business partners  Ability to quickly gain a deep understanding of our BI platforms, database schemas, ETL- s, and business logic with an ability to apply critical thinking skills towards gathering and analyzing data  Excellent communication skills, both verbal and written, are required as you will draft narratives outlining your findings, explaining variances against goals, and present these finding in a review forum  Strong ownership and bias for action; ability to internalize goals and work independently to crate appropriate action plans for those goals  Strong problem-solving and critical thinking skills; ability to analyze issues and create appropriate tactical plans  Meticulous attention to detail; ability to juggle many tasks in parallel without lowering quality bar  Ability to succeed in a fast-paced, innovative, and rapidly evolving industry and business organization
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/business-intelligence-analyst-mumbai-4-8-years-of-experience/
Beginners guide to build data visualisations on the web with D3.js,"Learn everything about Analytics|Introduction|The Web as a platform for visualisation|How D3.js fits in the picture?|Setup your machine|Technical Interlude  Brushing up basics of HTML, CSS, JavaScript and SVG|D3.js  Unpacking the name|Building with D3.js|End Notes","Storytelling through Data Visualisation|Create a simple web server in Python|Task 1. Create a basic HTML page with the text Hello World :|Task 2. Add formatting and styling to the text using CSS. Specifically, we want to increase size, bring the text to the center of the page, add colour and change its typeface:|Task 3. We want to add a behavior to our page. We want to show some text initially and change it when someone clicks on it. We also want to show an image on that click.|Task 4. Draw a group of two blue circles of radius 50px each. Fill the circle with blue colour and red boundary of some thickness using an SVG.|Including D3.js library|Adding an element|Adding CSS styling |Data binding|Where is our data?|Drawing SVG with D3|What is the i |Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Business Intelligence Analyst- Mumbai (4- 8 Years Of Experience)|Debugging & Visualising training of Neural Network with TensorBoard|
Mohd Sanad Zaki Rizvi
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I am sure you have heard this many timesA picture is worth a thousand words.I think with the proliferation of data, this statement can easily be modified toA picture is worth thousand(s) of data points.If you are not convinced, look at the example below.Lets look at the following statement:In 2013, Gun Deaths Claimed 11,419 lives in the U.S.What comes to your mind first?Without visualisation the above are just a set of statistics. Though this stat isuseful, it still falls short in conveying the bigger picture. You dont get the complete context  What kind of people were involved? How much was the nations loss? etc.Lets see what happens when we visualise it:Periscopes US Gun Deaths Visualisation (takes some time to load)  Periscope has made this visualisation that shows the number of people died due to the gun violence in the U.S. in 2013. The orange line denotes the age at which they died and the white line denotes the number of years those people could have lived had they died of natural causes. This visualisation shows what 11,419 lost lives looks like and just how many years of life was stolen from the victims. It is a good example of how we can express the data in a more sublime and impactful manner by using basic animations and interactivity.When we present the data as a story, it gives a range of new perspectives and a much more holistic view of the situation which helps policy makers in making informed decisions.The web is becoming more accessible day by day and with advancements in browser technology, it is now possible to render complex visualisations on the fly across a variety of devices. This combination of accessibility and complexity makes the web an apt platform to reach out to large audiences.A lot of organisations are already using web / mobile applications showing dashboards to your mobile as well at your laptop and computer.Enter D3.js, a powerful library that enables you to build customizedvisualisations for any kind of storytelling you could imagine for the web.Lets look at another amazing visualisation :Hans Roslings 200 Countries, 200 Years, 4 Minutes  Hans tries to explain the change in health and income in the last 200 years across 200 countries of the world in just 4 minutes by utilising the power of effective visualisation.The properties of this graph are as follows:Notice how by using simple visual encodings like shape, size, colour, etc. such complex information(200 years of data) is conveyed across. We will talk about what is visual encoding and this visualisation in detail in the upcoming articles.What if I tell you the above visualisation is created using D3.js? Thats the potential of D3.js! Hopefully, we will learn to create magic of our own too We would be needing a few things before starting:Note: Since D3.js is designed to make visualisations on the web, it can fetch data over the internet from a web server to show visualisations. Hence we will also be working with a small server of our own.Step 1: Go to the directory you want to keep your D3.js project.Step 2: Python 2.7.x users can start the server by typing the code in the command line:Python 3.x users can do so by type the code in command line:You can check your python version by:Note: For windows users, please install anaconda and then open anaconda prompt.This should give an output similar to the following:Step 3: Here 0.0.0.0 is your servers IP address and 8000 is the port number. Imagine you have ordered your favorite pizza and you need to give your house address to complete the order. You will first give the Street Name and then the House Number, here IP address is analogous tostreet name (i.e. It identifies your computer) and Port number is analogous to thehouse number that helps you reach a particular house (i.e. identifies the process on your computer) in a building.Step 4: To check if your server is working, head to your browser and type the link : http://0.0.0.0:8000/You will get an output like this:Congratulations, you are now on your way to build your first d3.js visualisation!Before diving into D3, we need to know some web development basics. The web is built on three main pillars: HTML, CSS and JavaScript. Also, for a clearer understanding of D3.js, you should know the basics of SVG. We will have a crash course on these topics with some task based questions.Note: You can skip this section if you are familiar with HTML, CSS and JavaScript. In case you have some doubts or need help, each task is followed by a stepwise guide We have a list of four tasks to complete before we can get started with D3. Tasks 1 to 3 are based on HTML,CSS and JavaScript. Task 4 is based on SVG and related topics.Lets start with our first task!For this task, you would need to :Try doing this task on your own before checking the solution Solution:HTML or Hypertext Markup Language describes the structure of your web pages. HTML uses tags, a kind of syntax to form the skeletal structure of a web page. Specific text enclosed between angular brackets form a tag.Lets create our first HTML !Congratulations! You just made your first web page. Lets get into some details:For example :For this task you would need to :Try doing this task on your own before checking the solution Solution:CSS or Cascading Style Sheets is used to add styling and formatting to web pages. In other words, it helps to set a variety of rules(like: position, color, formatting etc.) that make the HTML look pretty. Lets make our own web page pretty!Note: The text between /**/ are comments and are ignored by the browser. They are for the programmer to keep notes of his / her code.Note: that we use the # symbol to call id in our CSS. This tells CSS to look for an element with the id = myText.CSS Selectors  Selectors are way our CSS is able to know which HTML elements does the rule applies to? We have already learned about the id selector.Now imagine, that if we have multiple paragraphs and we want to do the same styling to each? Surely we can repeat the ids for all elements but an id is supposed to be unique. We can use a class selector to group the elements that belong to the same class of styling.Note: We use . (dot) to tell our CSS we mean a class.For this task you would need to:Try doing this task on your own before checking the solution Solution:JavaScript is a programming language that adds immense functionality to a basic HTML web page. Almost all the cool things that you see in a website are because of the amazing power and flexibility JavaScript provides. It runs in the browser, unlike python and other programming languages you dont need to install JavaScript, as long as you have a browser you can easily run it. Lets add some special effects to impress our friend on his / her birthday!We will first show a generic message to our friend and when he /she clicks on the text, surprise! Happy Birthday! Lets code it up :Isnt it amazing what 5 lines of JavaScript can achieve? Let us dive into the code and understand what each line does:OrCongratulations! So far so good, you have successfully learnt the building blocks of web! We will be learning more about them as we go ahead with our D3 journey If you want to dig deeper into Web Technologies, you can look at W3Schools . For D3.js only basic understanding is required.In order to build with D3 we also need a basic knowledge of SVG. The following tasks are designed to give you a brief intro into the world of SVG:For this task you would need to :Try doing this task on your own before checking the solution Solution:SVG or Scalable Vector Graphics is a format used to draw xml based graphics and animations. SVG graphics do NOT lose any quality if they are zoomed or resized.SVG provides some basic shapes like lines, rectangles, circle, ellipse,polygon to work with. You can also create custom shapes by combining or tweaking these shapes. For extremely customisable graphics(like country maps, etc.) we use paths. A lot of these will refresh your high school geometry  Lets go through each one:LineOne of the simplest graphic. A line is formed when we join two points separated by a distance. There can be many such lines joining two points but we are interested in a straight line. For example, you and your friend are standing some distance apart, if you walk straight to him the path youll follow will make a straight line.A Side Note  Coordinate space in SVG:Those of you who know a little about coordinate system in geometry would be finding something strange with the line. The line should be like this :This is because the origin(0,0) in SVG is at the top-left corner as opposed to conventional bottom-left.As we move towards the right the x value increases and on moving down y value increases.Enough of serious talk, lets draw !CircleGroupsYou can read more about svg here, for using D3, only basic understanding of svg is required.D3.js is a javascript library written by Mike Bostock. It takes advantage of already established web technologies like canvas, svg to make out of the world visualisations.D3s name comes from the fact that it is designed to act as a driver for web documents(web page) based on the available data(json, csv, tsv etc.). In other words,D3s magic helps you to create beautiful, interactive visualisations from seemingly boring data(json,csv,etc.)We will get to know more about it as we go deeper into D3. You can also look at some of the awesome visualisations created using D3.js Lets get our hands dirty with D3!We will create a new html file , with name barchart.html in our D3 folder. Let us also put some basic html in our file :In order to use D3 we have to include its library, There are many ways to do so but we will stick to loading it from a url. D3 is completely written in javascript so we can include just like we include our javascript code with <script></script> tags like this:Note:This article usesversion 3 of D3.js . You can find thedocumentation here.This line tells your browser to load the d3 file from the given URL. We will place this tag inside our <head> tag just below the <title> tags. We will also create another <script> tag inside our body for us to write javascript code. Our final HTML code should look like this :Our webpage looks empty, lets add a paragraph here with D3. Write the below line between the <script> tags inside <body>:Save the page and head to your browser. This time you will have to use the url http://0.0.0.0:8000/barchart.html because your html file is named barchart.html. Lets see what happened in our page:Wasnt that easy? Lets try to understand our D3 code:When we include D3s library, it gives us a global object d3 that we can use to call various D3 functions. Here we are asking d3 to select <body> of our DOM and append a paragraph there with the text content Our First Paragraph using D3!.Lets walk through what just happened. In sequence, we:All of those crazy dots are just part of D3s chain syntax. The chain syntax is possible because every time you call a D3 function on an object it performs some operations on it and returns a reference to the new object, which in turn gets picked up by the next function in the chain.This is called method chaining. Note that the above task could have also achieved by calling each function separately like we do conventionally.Our text looks boring let us add some css styling to it and see what happens:Much better! We use .style(.) to add css styling in D3. Our chain has become too long to fit in a single line, lets us arrange it :Notice how we first created a <p> element with some text and stored in a variable p and then later used the same variable to add css styling to it. We can now use this variable whenever we want to make changes or edits to the paragraph.What is data binding and why should I do it?Hence you need to bind your data to the DOM elements in order to represent it as a visualisation. D3.js provides powerful ways to bind data in different forms(csv,tsv, json etc.) to the DOM.Lets create a data array :Note: In javascript an array is an equivalent of collections of R or lists of Python.Now that we have our data, lets bind our data to the DOM. We want to create <p> elements per data value and display it. Our new code will be like:Lets see what does our browser show:Whoa that was a lot of method chaining. We have got our text nine times on the page each as a paragraph. Let us break down the code:Since we dont have any paragraphs it will return an empty selection. Think of this as the selection of paragraphs that will soon exist.This is how the entire process looked like:First select body  from body select all the <p> elements that we are soon going to add  read the data values and count it, the following lines will be executed once for each value  create placeholders for the elements going to be added and return references to them  append elements based on the references returned earlier  add text content to each element.We follow the similar process all throughout our code in D3. Hence it is important that you understand it thoroughly.Did you notice that even though we got nine paragraphs, but we dont see any data? Where did itgo?Remember the line where I said that every line following data(..) will be executed nine times, once for each value? D3 provides us a way to access each value while it iterates over them. Lets check that value, edit the code to:This time instead of directly providing text content we create a function inside text(..). D3 executes this function every time it goes over our data. Each time this function is called, it is passed the value for which it is called. That means if we are on the first element of the array this function will be called with 5. We return the same value to our text(..) function so that it can add it to the text content of the paragraph. Heres our data:Just like html elements, creating svg with D3 is quite easy(one line easy!). For example we want to create an svg with width 500px and height 500px :We append an <svg> like did for <p> element. The attr(..) function is used to set attributes of the svg. Here we want to set width and height of the svg.Let us draw a circle in this svg. We want to create a circle of radius 30 and centre at (50,50):Notice how we used append(..) to add a circle to our svg.Let us add fill to our circle and increase the radius:You can see that the same attributes we learnt in the svg work here. D3 provides an easy interface to create SVGs programmatically.Simple BarchartA bar chart is a graphic that presents grouped data with rectangular bars. Here lengths of bars are proportional to the values they represent.Drawing barsWe will be taking advantage of SVG rectangle to make rectangles for the bar chart. A rectangle has four properties : Height, Width, x start position, y start position. Lets create a rectangle using D3:Notice how the (x,y) control the starting point of the rectangle. Let us add colour to our bar but before we will store our rectangle in a variable so that we can reuse it again. Lets do that:Let us create bars based on our data values. We will use the same process that we used to bind data earlier. Our bars variable will change to:We have chosen a fixed width of 25 pixels for each bar and we are setting height based our data_values array. Let us see how the graphic looks:Werent we supposed to get nine rectangles? We have nine rectangles but since we have not given different x values they all lie on the same position. Lets add different x values to our bars:We already know D3 passes the current data value while iterating through our data. Along with the current value it also passes the current values index. This index helps us keep track of the iteration count. What we are doing is we are giving i*30 as x value for our bars. This is because the i will denote the index of the current value and 30 is there because we know each bars width is 25 pixels so we give a padding of 5 pixels between each bar.Lets refresh our page and we got our 9 data bars !:Isnt our bars a little funky? Lets scale our heights a little bit. Lets multiply d by 5 in our height function:Much better, but why are the bars upside down?Remember that SVG Coordinate space is different as compared to conventional? Here Origin is at the top left corner so our y-axis starts from the top. Lets adjust this so that we get a proper bar chart, we can change this by subtracting the height of each bar from the total height of the svg(400px).Lets give a nice color to our bars:Lets add some more values to our data_values array , it should now look like this:Do you think this visualisation is efficiently conveying the information? No, there are many things that can be improved so that we are able to convey inferences in as efficient manner as possible. One such improvement can be :Highlighting min and max bars so that as soon as we put our gaze on the visualisation we get an idea of the two extremes, we wont have to go through each bars and hence this will save our time.Lets do that!We first need to find the two extremes in our dataset. Lucky for us, D3 provides d3.max(..) and d3.min(..) functions that we can use:We will have to now select those two bars two have max and min value respectively , for that well use D3s filter(..) method. What this will do is select only those bars that match the filter. In this case we will have filter for maximum value. We will add green colour fill to the bar:Similarly for the smallest bar, we will add red fill colour:There we have our simple barchart in less than 20 lines of code!Congratulations for coming this far, we have covered a lot of D3 basics these will help you pursue your own adventures someday! D3.js is a very powerful library and we have just scratched the surface. There is still a complex but immensely useful functionality it has to offer; that is making better charts, visualisations loaded with animations and interactivity.I have included all the code of this article is available on github. Also check out these small visualisations that I created for fun to get a glimpse of some cool stuff we can do by knowing just the basics of D3 :I urge you to try them out on your own. Good luck!You can check out my next article on D3.js here",https://www.analyticsvidhya.com/blog/2017/07/beginner-guide-build-data-visualisations-web-d3-js/
Debugging & Visualising training of Neural Network with TensorBoard,Learn everything about Analytics|Introduction|Table of Contents|Training a neural network: a boon or a curse?|Solving Age Detection Problem with Neural Networks|Intermission: an overview of Tensorboard|Back to solving the problem|Conclusion|Useful Resources|End Notes,"Testing the neural network architecture|Testing the data|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Beginners guide to build data visualisations on the web with D3.js|Text Classification & Word Representations using FastText (An NLP library by Facebook)|
Faizan Shaikh
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","Step 1: Check the architecture|Step 2: Check the hyper-parameters of neural network|Step 3: Check the Complexity of network|Step 4: Check the Structure of Input data
|Step 5: Check the Distribution of data",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I started my deep learning journey a few years back. I have learnt a lot in this period. But, even after all these efforts, every Neural network I train provides me with a new experience.If you have tried to train a neural network, you must know my plight!But, through all this time, I have now made a workflow, which I will share with you today.I am sharingmy learning / experience about building Neural Network with all of you.I cannot guarantee it will work all the time, but at least it may guide you as to how would you approach to solve the problem. I will also share with you a tool which I find is a useful addition to the deep learning toolbox  TensorBoard.P.S.: This article assumes that you know the basics of building a neural network and have a fair degree of knowledge about keras. If not, I would recommend going through these articles first and then come back to this article.Training a neural network is usually a high risk  high reward strategy. If you have tuned it right, you could potentially have a state-of-the-art working model for the task.Just to give an example, image generation is a recent breakthrough of neural networks. With just a few strokes of artificial brush, you can create an authentic looking image of natural scenes.SourceBut on the other hand, it is extremely difficult to train a generative neural network. You would have to go through a rigorous trial and error phase to get things right.I had to go through a similar experience of training a neural network few days back when trying to build a working model for Age detection problem. When I made my first model for the problem, it absolutely refused to train. In this article, I will share my approach of how I debugged the neural network. Stay tuned, because theres a happy ending!Before jumping on to solve the problem, let me give an overview of what the problem was.Age Detection of Indian Actors is a recently released practice problem which deals with extracting facial features for age analysis. As this is a image processing problem, I naturally jumped on to picking up neural networks to solve the problem. But as I said earlier, the network would not train. There was no change in the accuracy rates whatsoever, even when I let it train for more than an hour for 200 iterations. Heres a proof!Clearly this was not working!Now, my years of experience came in handy!  The steps which I mention below are those which I usually follow when Im stuck with this kind of problem. If you carefully consider, there may be two major reasons by which your neural network may not work correctly Lets go through these reasons one by one and eliminate the improbable.The first thing you should check when building a neural network is whether you have defined the architecture properly. Here we have a three class problem with varying sizes of images in the dataset. To simplify things, I converted all the images to size 3232.So according to this, I had defined the architecture as followsI dont see anyissue in the architecture of neural network.According to me, this isthe most important step when dealing with neural network. Its because there are so many parameters to tune that it may sometimes be frustrating to try them all. (P.S.: I did a survey of hyper-parameters and how to tune them in this article)Fortunately, I had used a very simple neural network with only one hidden layer trained with classical gradient descent algorithm (SGD).Here is a thing to look out for; it is said that when you train a neural network with SGD, it may train slowly. To overcome this, we can use adaptive gradient descent to train the network faster.But the thing was, even on changing the training algorithm from SGD to Adam (aka adaptive gradient descent), the network did not train. This simply meant that something was fundamentally wrong with the network. I had to pull Thors hammer to break through this problem!Throughout my journey of understanding the neural network, I have gone through a number of tools for building and visualizing a neural network. Of all of them, I have found tensorboard to be an important asset. It can give you useful insight when training a neural network.Along with this, it gives a nice dashboard like view of the findings, which is very important when explaining your findings to the stakeholders  The image you saw above of a proof was a dashboard of tensorboard itself.I will mention the steps for how to install tensorboard in your system. I suggest you to try it out for yourself.You can install Tensorboard using pip the python package managerAfter installation you can open tensorboard by going to the terminal and typingHeres a view of how it would look on the browserThe logs/ folder mentioned above should have the history of how the neural network was trained. You can simply get this by including a tensorboard callback in kerasIn this example, I have passed all the arguments so that everything gets saved. The meaning of the arguments are as follows:Coming back from our short excursion, the next thing I tried was to check if the network I had built was enough to learn the distributions of the problem or not. For this, instead of a simple neural network I changed the architecture to a convolutional neural network. The effect was that the accuracy increased drastically from the beginning (from 33% to 54%). But still it remained constant even after training.It seemed that our little experiment failed After thoroughly checking the network architecture, it was time to check if we have the proper dataset itself.A few things to check are:As discussed in step 1, we have already ensured that the images are of same size before sending it to the network, so thats out of question.The dataset is not that imbalanced, as we still have ample amount of images per classHere we should check if we have a properly processed input. For example in an image processing problem, if we processed the image and the resultant input has irregular aspect ratios, the neural network would obviously be flummoxed by it.Here as it was a simple network, we havent really done any preprocessing steps. So thats out of question too.Having exhausted most of the problems that we might encounter, I was getting a bit frustrated as to what might be the real problem. Luckily my journey came to an end, as I found a weird bug which I should have caught on much earlier.The problem was that the input data I was sending to the network had a range of 0 to 255. Ideally this range should be between 0 and 1. The distribution of input data looked as below:Let me explain to you why normalizing (setting the range from 0 to 1) is important (as I have personally gone through the trouble of finding what is it so  )SourceYou can see that if your data is does not have a simple distribution, the neural network might find it hard to learn this distribution. It will surely try to converge, but it wont guarantee complete convergence. An extended version of this concept is batch normalization, which ensures the data is normalized after a layer of neural network. This paper covers in-depth analysis of how batch normalization helps train a neural network better.Heres the code I used to read images and normalize them.Voila! After I added a simple step of normalization, I saw that the neural network started to train.I was so happy!!Just to summarize, these are the steps you should look at when debugging a neural network.Step 1: Check the architecture
Step 2: Check the hyper-parameters of neural network
Step 3: Check the Complexity of network
Step 4: Check the Structure of Input data
Step 5: Check the Distribution of dataHere are some resources that I would recommend you to go through when you are stuck like meIn this article, I have given my personal experience of debugging a neural network and a brief overview of visualization and debugging tool called tensorboard. I hope this will guide you solve the problems you would face in your own incursions. Please feel free to try out tensorboard and share your experiences in the comment below.",https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/
Text Classification & Word Representations using FastText (An NLP library by Facebook),Learn everything about Analytics|Introduction|Table of contents|What is FastText?|Installation|Implementation|Pros and Cons of FastText|Projects|End Notes,"Learning Word Representations|Text Classification|Computing Sentence Vectors (Supervised)|Pros|Cons|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Debugging & Visualising training of Neural Network with TensorBoard|Chief Risk Officer- Mumbai (5-7 years of experience)|
NSS
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you put a status update on Facebook about purchasing a car -dont be surprised if Facebook serves you a car ad on your screen. This is not black magic! This is Facebook leveraging the text data to serve you better ads.The picture below takes a jibe at a challenge while dealing with text data.Well, it clearly failed in the above attempt to deliver the right ad. It is all the more important to capture the context in which the word has been used. This is a common problem in Natural Processing Language (NLP) tasks.A single word with the same spelling and pronunciation (homonyms) can be used in multiple contexts and a potential solution to the above problem is computing word representations.Now, imagine the challenge for Facebook.Facebook deals with enormous amount of text data on a daily basis in the form of status updates, comments etc. And it is all the more important for Facebook to utilise this text data to serve its users better. And using this text data generated by billions of users to compute word representations was a very time expensive task until Facebook developed their own library FastText, for Word Representations and Text Classification.In this article, we will see how we can calculate Word Representations and perform Text Classification, all in a matter of seconds in comparison to the existing methods which took days to achieve the same performance.FastText is a library created by the Facebook Research Team for efficient learning of word representations and sentence classification.This library has gained a lot of traction in the NLP community and is a possible substitutionto the gensim package which provides the functionality of Word Vectors etc. If you are new to the Word Vectors and word representations in general then, I suggest you read this article first.But the question that we should be really asking is  How is FastText different from gensim Word Vectors?FastText differs in the sense that word vectors a.k.a word2vec treats every single word as the smallest unit whose vector representation is to be found but FastText assumes a word to be formed by a n-grams of character, for example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny] etc, where n could range from 1 to the length of the word. This new representation of word by fastText provides the following benefits over word2vec or glove.We will now look at the steps to install the fastText library below.To make full use of the FastText library, please make sure you have the following requirements satisfied:If you do not have the above pre-requisites, I urge you to go ahead and install the above dependencies first.To install FastText, type the code below-You can check whether FastText has been properly installed by typing the below command inside the FastText folder.
./fasttextIf everything was installed correctly then, you should see the list of available commands for FastText as the output.As stated earlier, FastText was designed for two specific purposes- Word Representation Learning and Text Classification. We will see each of these steps in detail. Let us get started with learning word representations.Words in their natural form cannot be used for any Machine Learning task in general. One way to use the words is to transform these words into some representations that capture some attributes of the word. It is analogous to describing a person as  [height:5.10 ,weight:75, colour:dusky, etc.] where height, weight etc are the attributes of the person. Similarly, word representations capture some abstract attributes of words in the manner that similar words tend to have similar word representations. There are primarily two methods used to develop word vectors  Skipgram and CBOW.We will see how we can implement both these methods to learn vector representations for a sample text file using fasttext.Learning word representations using Skipgram and CBOW modelsLet us see the parameters defined above in steps for easy understanding../fasttext  It is used to invoke the FastText library.
skipgram/cbow  It is where you specify whether skipgram or cbow is to be used to create the word representations.
-input  This is the name of the parameter which specifies the following word to be used as the name of the file used for training. This argument should be used as is.
data.txt  a sample text file over which we wish to train the skipgram or cbow model. Change this name to the name of the text file you have.
-output  This is the name of the parameter which specifies the following word to be used as the name of the model being created. This argument is to be used as is.
model  This is the name of the model created.Running the above command will create two files named model.bin and model.vec.model.bin contains the model parameters, dictionary and the hyperparameters and can be used to compute word vectors. model.vec is a text file that contains the word vectors for one word per line.Now since we have created our own word vectors lets see if we can do some common tasks like print word vectors for a word, find similar words, analogies etc. using these word vectors.Print word vectors of a wordIn order to get the word vectors for a word or set of words, save them in a text file. For example, here is a sample text file named queries.txt that contains some random words. We will get the vector representation of these words using the model we trained above../fasttext print-word-vectors model.bin < queries.txtTo check word vectors for a single word without saving into a file, you can doecho ""word"" | ./fasttext print-word-vectors model.binFinding similar wordsYou can also find the words most similar to a given word. This functionality is provided by the nnparameter. Lets see how we can find the most similar words to happy../fasttext nn model.binAfter typing the above command, the terminal will ask you to input a query word.happyby 0.183204
be 0.0822266
training 0.0522333
the 0.0404951
similar 0.036328
and 0.0248938
The 0.0229364
word 0.00767293
that 0.00138793
syntactic -0.00251774The above is the result returned for the most similar words to happy.Interestingly, this feature could be used to correct spellings too. For example, when you enter a wrong spelling, it shows the correct spelling of the word if it occurred in the training file.wrdword 0.481091
words. 0.389373
words 0.370469
word2vec 0.354458
more 0.345805
and 0.333076
with 0.325603
in 0.268813
Word2vec 0.26591
or 0.263104AnalogiesFastText word vectors can also be used on analogies task of the kind, what is to C, what B is to A. Here, A, B and C are the words.The analogies functionality is provided by the parameter analogies. Lets see this with the help of an example../fasttext analogies model.binThe above command will ask to input the words in the form A-B+C, but we just need to give three words separated by space.happy sad angryof 0.199229
the 0.187058
context 0.158968
a 0.151884
as 0.142561
The 0.136407
or 0.119725
on 0.117082
and 0.113304
be 0.0996916Training on a very large corpus will produce better results.As suggested by the name, text classification is tagging each document in the text with a particular class. Sentiment analysis and email classification are classic examples of text classification. In this era of technology, millions of digital documents are being generated each day. It would cost a huge amount of time as well as human efforts to categorise them in reasonable categories like spam and non-spam, important and unimportant and so on. Text classification techniques of NLP come here to our rescue. Lets see how by doing hands-on practice based on a sentiment analysis problem. I have taken the data for this analysis from kaggle.Before we jump upon the execution, there is a word of caution about the training file. The default format of text file on which we want to train our model should be _ _ label _ _ <X> <Text>Where _ _label_ _ is a prefix to the class and <X> is the class assigned to the document. Also, there should not be quotes around the document and everything in one document should be on one line.
In fact, the reason why I have selected this data for this article is that the data is already available exactly in the required default format.If you are completely new to FastText and implementing text classification for very first time in FastText, I would strongly recommend using the data mentioned above.In case your data has some other formats of the label, dont be bothered. FastText will take care of it once you pass a suitable argument. We will see how to do it in a moment. Just stick to the article.After this briefing about text classification, lets move ahead and land on the implementation part. We will be using the train.ft text file to train the model and test.ft file to predict.#training the classifier
./fasttext supervised -input train.ft.txt -output model_kaggle -label __label__Here, the parameters are same as the one mentioned while creating word representations. The only additional parameter is -label.This argument takes care of the format of the label specified. The file that you downloaded contains labels with the prefix __label__.If you do not wish to use default parameters for training the model, then they can be specified during the training time. For example, if you explicitly want to specify the learning rate of the training process then you can use the argument -lrto specify the learning rate../fasttext supervised -input train.ft.txt -output model_kaggle -label __label__ -lr 0.5The other available parameters that can be tuned are The values in the square brackets [] represent the default values of the parameters passed.# Testing the result
./fasttext test model_kaggle.bin test.ft.txtN 400000
[emailprotected] 0.916
[emailprotected] 0.916Number of examples: 400000
[emailprotected] is the precision
[emailprotected] is the recall# Predicting on the test dataset
./fasttext predict model_kaggle.bin test.ft.txt# Predicting the top 3 labels
./fasttext predict model_kaggle.bin test.ft.txt 3This model can also be used for computing the sentence vectors. Let us see how we can compute the sentence vectors by using the following commands.echo ""this is a sample sentence"" | ./fasttext print-sentence-vectors model_kaggle.bin
0.008204 0.016523 -0.028591 -0.0019852 -0.0043028 0.044917 -0.055856 -0.057333 0.16713 0.079895 0.0034849 0.052638 -0.073566 0.10069 0.0098551 -0.016581 -0.023504 -0.027494 -0.070747 -0.028199 0.068043 0.082783 -0.033781 0.051088 -0.024244 -0.031605 0.091783 -0.029228 -0.017851 0.047316 0.013819 0.072576 -0.004047 -0.10553 -0.12998 0.021245 0.0019761 -0.0068286 0.021346 0.012595 0.0016618 0.02793 0.0088362 0.031308 0.035874 -0.0078695 0.019297 0.032703 0.015868 0.025272 -0.035632 0.031488 -0.027837 0.020735 -0.01791 -0.021394 0.0055139 0.009132 -0.0042779 0.008727 -0.034485 0.027236 0.091251 0.018552 -0.019416 0.0094632 -0.0040765 0.012285 0.0039224 -0.0024119 -0.0023406 0.0025112 -0.0022772 0.0010826 0.0006142 0.0009227 0.016582 0.011488 0.019017 -0.0043627 0.00014679 -0.003167 0.0016855 -0.002838 0.0050221 -0.00078066 0.0015846 -0.0018429 0.0016942 -0.04923 0.056873 0.019886 0.043118 -0.002863 -0.0087295 -0.033149 -0.0030569 0.0063657 0.0016887 -0.0022234Like every library in development, it has its pros and cons. Let us state them explicitly.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:This article was aimed at making you aware of the FastText library as an alternative to the word2vec model and also letting you make your first vector representation and text classification model.For people who want to go in greater depth of the difference in performance of fastText and gensim, you can visit this link, where a researcher has carried out the comparison using a jupyter notebook and some standard text datasets.Please feel free to try out this library and share your experiences in the comment below.",https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/
Chief Risk Officer- Mumbai (5-7 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Text Classification & Word Representations using FastText (An NLP library by Facebook)|Machine Learning Developer- Bangalore (2- 4 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  7 years
Requirements : 
Task Info : Company Profile:We are a young and vibrant P2P lending marketplace and at the very start of our entrepreneurial journey. We have a vision to reinvent the way banking is transacted in India. We are highly focused on improving the borrower experience and improve Turn -around time and (TAT), encourage out of the box thinking and innovation. We would be present in small ticket granular lending.Job Description:Chief Risk officer Key Responsibilities & Job Functions:As part of core committee, ideate and construct credit underwriting processand system. The ideal candidate in this venture would be open to think of innovative ways to do small ticket retail credit underwriting . We want to focus on new ideas to increase usage of low cost tech in banking, do credit differently and provide a seamless experience to all the stakeholders .Qualification:Post graduate with atleast 5-7 years of experience in banking operations. Candidate should have worked in small ticket retail credit underwriting/building scorecard /alternative forms of credit evaluation.
College Preference : no-bar
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/chief-risk-officer-mumbai-5-7-years-of-experience/
Machine Learning Developer- Bangalore (2- 4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Chief Risk Officer- Mumbai (5-7 years of experience)|Qlikview Consultant- Gurgaon (1-2 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : 
College Preference : no-bar
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/machine-learning-developer-bangalore-2-4-years-of-experience/
Qlikview Consultant- Gurgaon (1-2 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Developer- Bangalore (2- 4 years of experience)|Covariate Shift  Unearthing hidden problems in Real World Data Science|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : Self-motivated, ability to work independently with minimal direction and be team oriented with ability to communicate to a wide variety of audiences
Task Info : Key responsibilities:
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/07/qlikview-consultant-gurgaon-1-2-years-of-experience/
Covariate Shift  Unearthing hidden problems in Real World Data Science,Learn everything about Analytics|Introduction|Table of Contents|1. What is Dataset Shift?|2. What causes Dataset Shift?|3. Types of Dataset Shift|4. Covariate Shift|5. Identification|6. Treatment|7. End Notes,"|Steps to identify drift||6.1 Dropping|6.2 Importance weight using Density Ratio Estimation|Share this:|Like this:|Related Articles|Qlikview Consultant- Gurgaon (1-2 years of experience)|Tutorial on Automated Machine Learning using MLBox|
Shubham Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You may have heard from various people that data science competitions are a good way to learn data science, but they are not as useful in solving realworld data science problems. Why do you think this is the case?One of the differences lies in the quality of data that has been provided. In Data Science Competitions, the datasets are carefully curated. Usually, a single large dataset is split into train and test file. So, most of the times the train and test have been generated from the same distribution.But this is not the case when dealing with real world problems, especially when the data has been collected over a long period of time. In such cases, there may be multiple variables / environment changes might have happened during that period. If proper care is not taken then, the training dataset cannot be used to predict anything about the test dataset in a usable manner.In this article, we will see the different types of problems or Dataset Shift that we might encounter in the real world. Specifically, we will be talking in detail about one particular kind of shift in the Dataset (Covariate shift), the existing methods to deal with this kind of shift and an in depth demonstration of a particular method to correct this shift.Everytime youparticipate in a competition, your journey willlookquitesimilar to the one shown in the figure below.Let me explain this with the help of a scenario depicted in the picture below. You are given a train and a test file in a competition. You complete the preprocessing, the feature engineering and the cross validation part on the model created but you do not get the same result as the one you get on the cross-validation. No matter what validation strategy you try, it seems like you are bound to get different results in comparison to the cross validation.Image source: linkWhat can be a possible reason for this failure? So, if you carefully notice the first picture, you will find that you did all the manipulation by justlookingat the train file. Therefore, you completely ignoredthe information contained in the test file.Now take a look back on the second picture, you will notice that the training file contains information about male and females of fairly younger age while the test file contains information about people of older age. Therefore it means that the distribution of data contained in the train and test file is significantly different.So, if youbuildyour model basedon the data set containing information about people having lower age and predict on a data set containinghigher values of age, that will definitely give you a low score. The reason is thatthere will a wide gap in the interest and the activitiesbetween these two groups.So your model will fail in these conditions.This change in the distribution of data contained in train and test file is called dataset shift(or drifting).Try to think some of the examples, where you can encounter the problem of dataset shift.Basically, in the real world, dataset shift mainly occurs because of the change of environments(popularly called as non-stationary environment), where the environment can be referred as location, time, etc.Let us consideran example. We collected the sales ofvariousitemduring the period of July-September. Now your job is to predict the sales during the period of Diwali.The visual representation of sales in the train (blue line) and test (black line) file would be similar to the imageshown below.Image source: linkClearly, the sales during the time ofDiwaliwould be much higher as compared to routine days.Thereforewe can say thatit is the situation of dataset shift, which occurred due to change of time period between our train and test file.But our machine learning algorithms work by ignoring these changes. They presume that the train and test environments match and even if they dont, it assumes that it makes no differenceif the environment changes.Now take a look back at both of the examples that we discussed above. Is there any difference betweenthem?Yes, in the firstscenario, there was a shift in theage(independent variable or predictor)of the population due to which we were getting wrong predictions. While in the latter one, there was a shift in thesales(target variable)of the items. This brings the next topic to the table  Different types of Dataset shifts.Dataset shift could be divided intothree types:In this article, we will discuss only covariate shift in this article since the other two topics are still an active research area and there has not been any substantial work to mitigate these problems.We will also see the methods to identify Covariate shift and the proper measures that can be taken in order to improve the predictions.Covariateshiftrefers to the change in the distribution of the input variables present in the training and the test data. It is the most common type of shift and it is now gaining more attention as nearly every real-world dataset suffers from this problem.First, let us try to understand how does thechange in distribution creates a problem for us. Take a look at the image shown below.Image source: linkIf you carefully notice the image given above, our learning function tries to fit the training data. But here, we can see that the distribution of training and test is different, so predicting using this learned function will definitelygive us wrong predictions.So our first step should be toidentify this shift in the distribution. Lets try and understand it.Here, I have used a quick and dirty machine learning technique to check whether there is a shiftbetween the training data and the test data.For this purpose, I will useSberbank Russian Housing Marketdataset from Kaggle.The basic idea to identify shift If there exists a shift in the dataset, then on mixing the train and test file, you should still be able to classify an instance of the mixed dataset as train or test with reasonable accuracy. Why?Because, if the features in both the dataset belong to different distributions then, they should be able to separate the dataset into train and test file significantly.Lets try to make it simple. Take a look at the distribution of the feature id in both thedataset.By looking at theirdistribution, we can clearly see that after a certain value (=30,473), all the instances will belong to test dataset.So if we create a dataset which is a mixture of training and test instances, where we havelabelled each instance of training data as training and test as test before mixing.In this new dataset,if wejust lookat thefeature id, we can clearly classifyanyinstancethatwhether it belongsto training data or test data. Therefore,we canconcludethatid is a drifting feature for this dataset.So this wasfairly easy.But we cant visualise every variable and check whether it is drifting or not. For that purpose,let us try to code this in Python as a simple classification problemand identify the drifting features.The basic steps that we will follow are:Note thatwe generally take 0.80 as the threshold value, but the value can be altered based on the situation.So that is enough of theory, now lets code this and find whichof thefeatures are drifting in this problem.## importing libraries
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import os
import matplotlib.pyplot as plt
get_ipython().magic('matplotlib inline')
os.chdir('/media/shubham/3AA25FBFA25F7DF7/Kaggle/russian housing market')
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.preprocessing import LabelEncoder## reading files
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')#### preprocessing ###### missing values
for i in train.columns:
  if train[i].dtype == 'object':
   train[i] = train[i].fillna(train[i].mode().iloc[0])
  if (train[i].dtype == 'int' or train[i].dtype == 'float'):
   train[i] = train[i].fillna(np.mean(train[i]))
for i in test.columns:
  if test[i].dtype == 'object':
   test[i] = test[i].fillna(test[i].mode().iloc[0])
  if (test[i].dtype == 'int' or test[i].dtype == 'float'):
   test[i] = test[i].fillna(np.mean(test[i]))## label encoding
number = LabelEncoder()
for i in train.columns:
  if (train[i].dtype == 'object'):
   train[i] = number.fit_transform(train[i].astype('str'))
   train[i] = train[i].astype('object')for i in test.columns:
  if (test[i].dtype == 'object'):
   test[i] = number.fit_transform(test[i].astype('str'))
   test[i] = test[i].astype('object')## creating a new feature origin
train['origin'] = 0
test['origin'] = 1
training = train.drop('price_doc',axis=1) #droping target variable## taking sample from training and test data
training = training.sample(7662, random_state=12)
testing = test.sample(7000, random_state=11)## combining random samples
combi = training.append(testing)
y = combi['origin']
combi.drop('origin',axis=1,inplace=True)## modelling
model = RandomForestClassifier(n_estimators = 50, max_depth = 5,min_samples_leaf = 5)
drop_list = []
for i in combi.columns:
score = cross_val_score(model,pd.DataFrame(combi[i]),y,cv=2,scoring='roc_auc')
 if (np.mean(score) > 0.8):
 drop_list.append(i)
 print(i,np.mean(score))
# Drifting features : {id, life_sq, kitch_sq, hospital_beds_raion, cafe_sum_500_min_price_avg, cafe_sum_500_max_price_avg, cafe_avg_price_500 }Here we have classifiedsevenfeatures as drifting.You can also manually check their difference indistributionthrough some visualisation or by using 1-wayANOVAtest.So, now theimportantquestion is how to treat them effectively such that we can improve our predictions.There are different techniques by which we can treat these features in order to improve our model. Let usdiscuss some of them.So lets try to understand them.This method is quite simple, as in this, we basically drop the features which are being classified as drifting. But just give it a thought, that simply dropping features might result in some loss of information.To deal with this, we have defined a simple rule.Features having a drift value greater than 0.8 and arenot importantin our model, we drop them.So, lets try this in our problem.Here, I have used a basic random forest model just to check which features are important.# using a basic model with all the features
training = train.drop('origin',axis=1)
testing = test.drop('origin',axis=1)rf = RandomForestRegressor(n_estimators=200, max_depth=6,max_features=10)
rf.fit(training.drop('price_doc',axis=1),training['price_doc'])
pred = rf.predict(testing)
columns = ['price_doc']
sub = pd.DataFrame(data=pred,columns=columns)
sub['id'] = test['id']
sub = sub[['id','price_doc']]
sub.to_csv('with_drifting.csv', index=False)On submitting this file on Kaggle, we are getting armsescore of0.40116on private leaderboard.So, lets check first 20important featuresfor this model.### plotting importances
features = training.drop('price_doc',axis=1).columns.values
imp = rf.feature_importances_
indices = np.argsort(imp)[::-1][:20]#plot
plt.figure(figsize=(8,5))
plt.bar(range(len(indices)), imp[indices], color = 'b', align='center')
plt.xticks(range(len(indices)), features[indices], rotation='vertical')
plt.xlim([-1,len(indices)])
plt.show()Now, if we compare our drop list and feature importance, we will find that the features life_sq and kitch_sq are common.So, we will keep these two features in our model, whiledropping therest of thedrifting features.NOTE: Before dropping any feature, just make sure youif there any possibility to createa new feature from it.Lets try this and check whether it improves our prediction or not.## dropping drifting features which are not important.
drift_train = training.drop(['id','hospital_beds_raion','cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg','cafe_avg_price_500'], axis=1)
drift_test = testing.drop(['id','hospital_beds_raion','cafe_sum_500_min_price_avg','cafe_sum_500_max_price_avg','cafe_avg_price_500'], axis=1)rf = RandomForestRegressor(n_estimators=200, max_depth=6,max_features=10)
rf.fit(drift_train.drop('price_doc',axis=1),training['price_doc'])
pred = rf.predict(drift_test)
columns = ['price_doc']
sub = pd.DataFrame(data=pred,columns=columns)
sub['id'] = test['id']
sub = sub[['id','price_doc']]
sub.to_csv('without_drifting.csv', index=False)On submission of this file on Kaggle, we got armsescore of0.39759on the private leaderboard.Congratulations,we have successfully improved our performanceusing this technique.In this method, the approach toimportance estimation would be to first estimate the training and test densities separately and then estimate the importance by taking the ratio of the estimated densities of test and train.Then these densities act as weights for each instance in the training data.But giving weights to each instance based on the density ratio could be a rigorous task in higher dimensional data sets. I tried this method on an i7 processor with 128 GB RAM and it took around 3 minutes to calculate the ratio density for a single feature. Also, I could not find any improvement in the score on applying the weights to the training data.Also scaling this feature for 200 features would be a very time-consuming task.Therefore, this method is only good up to research papers but the application of this in the real world is still questionable. Also, this is an active area of research.I hope that now you have a better understanding about drift, how you can identify it and treat it effectively. Ithas now become a common problem in real world dataset. So you should develop a habit to check this every time while solving problems, and surely it will give you positive results.Did you find this article helpful? Please share your opinions/thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/
Tutorial on Automated Machine Learning using MLBox,Learn everything about Analytics|Introduction|Table of Contents|1. What is MLBox?|2. MLBox in comparison to the other Machine Learning Libraries|3. Installing MLBox|4. Layout / Pipeline of MLBox|5. Building a Machine Learning Regressor using MLBox|6. Basic Understanding of Drift|7. Basic Understanding of Entity Embedding|8. Pros and Cons of MLBox|9. End Notes,"Pre-Processing|Optimisation|Prediction|Learn, Engage, Compete & Get Hired|Share this:|Like this:|Related Articles|Covariate Shift  Unearthing hidden problems in Real World Data Science|30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest  NLP]|
NSS
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Reading and cleaning a file|Removing the Drifting Variables|Most of the images have been taken from the documentation of MLBox itself.,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Recently, one of my friends and I were solving a practice problem. After 8 hours of hard work & coding, my friend Shubham got a score of 1153 (position 219). Here is his position on leaderboard:On the other hand, I was able to achieve this by writing only 8 lines of code:How did I get there?What if I tell you there exists a library called MLBox ,which does most of the heavy lifting in machine learning for you in minimal lines of code? From missing value imputation to feature engineering using state-of-the-art Entity Embeddingsfor categorical features, MLBoxhas it all.In these 8 lines of code using MLBox, Ihave also performed hyperparameter optimisation and tested around 50 models with blazing speed  isnt that awesome? You will be able to use this library by end of this article.According to the developer of MLBox,MLBox is a powerful Automated Machine Learning Python library.It provides the following features:MLBox focuses on the below three points in particular in comparison to the other libraries:We will be studying about these below in some detail to have an idea about what they do.MLBox is currently available for Linux only. MLBox was primarily developed using Python 2 and last night it was extended to Python 3. We will be installing the latest 3.0 dev version of MLBox. Follow the below steps to install MLBox into your Linux System.Note  This library is currently under very active development and therefore there may be the cases that something that works now may break the next day. For example, this library worked pretty well till 2 days ago for Python 2.7 and didnt work so good for Python 3.6. But at the time of writing, I am experiencing some issues with the 2.7 version and the 3 version is working fine for now. Also please feel free to open issues on the github repository and asking for help in the comments below.The entire pipeline of MLBox looks like below-The entire pipeline of MLBox has been divided into 3 sections/sub-packages.We will be studying about these 3 sub-packages in detail below.All the functionalities inside this sub-package can be used via the command-
from mlbox.preprocessing import *This sub-package provides functionalities related to two major functions.This package supports reading a wide variety of file formats like csv, Excel, hdf5,JSON etc. but in this article, we will be primarily seeing the most common .csvfile format. Follow the below steps to read a csvfile.Step1: Create an object of the Reader class with the separator as a parameter. , is the separator in the case of a csv file.
s="",""
r=Reader(s)  #initialising the object of Reader ClassStep2: Make a list of the train and test file paths and also identify the target variable name.
path=[""path of the train csv file"",""path of the test csv file ""]
target_name=""name of the target variable in the train file""

Step3: Performing the cleaning operation and creating a cleaned train and test file.
data=r.train_test_split(path,target_name)
The cleaning steps performed in the above step are-
-deleting unnamed columns
-removing duplicates
-extracting month, year and day of the week from a Date columnThe drifting Variables are explained in the later section. To remove the drifting variables, follow the below steps.Step1: Create an object of class Drift_thresholder
dft=Drift_thresholder()Step2: Use the fit_transform method of the created object to remove the drift variables.
data=dft.fit_transform(data)All the functionalities inside this sub-package can be used via the command-
from mlbox.optimisation import *This is the section where this library scores themaximum points. This hyper-parameter optimisation method in this library uses the hyperopt library which is very fast and you can almost optimise anything in this library from choosing the right missingvalue imputation method to the depth of an XGBOOST model. This library creates a high-dimensional space of the parameters to be optimised and chooses the best combination of the parameters that lowers the validation score.Below is the table of the four broad optimisations that are done in the MLBox library with terms to the right of hyphen that can be optimised for different values.Missing Values Encoder(ne)  numerical_strategy(when the column to be imputed is a continuous column eg- mean, median etc), categorical_strategy(when the column to be imputed is a categorical column e.g.- NaN values etc)Categorical Values Encoder(ce) strategy(method of encoding categorical variables e.g.- label_encoding, dummification, random_projection, entity_embedding)Feature Selector(fs) strategy(different methods for feature selection e.g. l1, variance, rf_feature_importance), threshold(the percentage of features to be discarded)Estimator(est)strategy(different algorithms that can be used as estimators eg- LightGBM, xgboost etc.),**params(parameters specific to the algorithm being used eg- max_depth, n_estimators etc.)Let us take an example and create a hyperparameter space to be optimised. Let us state all the parameters that I want to optimise:Algorithm to be used- LightGBM
LightGBM max_depth-[3,5,7,9]
LightGBM n_estimators-[250,500,700,1000]
Feature selection-[variance, l1, random forest feature importance]
Missing values imputation  numerical(mean,median),categorical(NAN values)
categorical values encoder- label encoding, entity embedding and random projectionLet us now create our hyper-parameter space. Before that, remember, hyper-parameter is a dictionary of key and value pairs where value is also a dictionary given by the syntax
{search:strategy,space:list}, where strategy can be either choice or uniform and list is the list of values.space={'ne__numerical_strategy':{""search"":""choice"",""space"":['mean','median']},
'ne__categorical_strategy':{""search"":""choice"",""space"":[np.NaN]},
'ce__strategy':{""search"":""choice"",""space"":['label_encoding','entity_embedding','random_projection']},
'fs__strategy':{""search"":""choice"",""space"":['l1','variance','rf_feature_importance']},
'fs__threshold':{""search"":""uniform"",""space"":[0.01, 0.3]},
'est__max_depth':{""search"":""choice"",""space"":[3,5,7,9]},
'est__n_estimators':{""search"":""choice"",""space"":[250,500,700,1000]}}Now we will see the steps to choose the best combination from the above space using the following steps:Step1: Create an object of class Optimiser which has the parameters as scoring and n_folds.Scoring is the metric against which we want to optimise our hyper-parameter space and n_folds is the number of folds of cross-validation
Scoring values for Classification-""accuracy"",""roc_auc"",""f1"",""log_loss"",""precision"",""recall""
Scoring values for Regression-""mean_absolute_error"",""mean_squarred_error"",""median_absolute_error"",""r2""
opt=Optimiser(scoring=""accuracy"",n_folds=5)Step2: Use the optimise function of the object created above which takes the hyper-parameter space, dictionary created by the train_test_split and number of iterations as the parameters. This function returns the best hyper-paramters from the hyper-parameter space.
best=opt.optimise(space,data,40)All the functions in this sub-package can be installed using the command below.
from mlbox.prediction import *This sub-package predicts on the test dataset using the best hyper-parameters calculated using the optimisation sub-package. To predict on the test dataset, go through the following steps.Step1: Create an object of class Predictor
pred=Predictor()Step2: Use the fit_predict method of the object created above which takes a set of hyperparameters and dictionary created through train_test_split as the parameter.
pred.fit_predict(best,data)The above method saves the feature importance, drift variables coefficients and the final predictions into a separate folder named save.We are now going to build a Machine Learning Classifier in just 7 lines of code with hyperparameter optimisation. We are going to solve the Big Marts sales problem. Download the train and test file and keep them in a single folder. Using the MLBox library, we are going to submit our first prediction without even having to look at the data. You can find the code below to make the prediction for the problem.# coding: utf-8# importing the required libraries
from mlbox.preprocessing import *
from mlbox.optimisation import *
from mlbox.prediction import *# reading and cleaning the train and test files
df=Reader(sep="","").train_test_split(['/home/nss/Downloads/mlbox_blog/train.csv','/home/nss/Downloads/mlbox_blog/test.csv'],'Item_Outlet_Sales')# removing the drift variables
df=Drift_thresholder().fit_transform(df)# setting the hyperparameter space
space={'ne__numerical_strategy':{""search"":""choice"",""space"":['mean','median']},
'ne__categorical_strategy':{""search"":""choice"",""space"":[np.NaN]},
'ce__strategy':{""search"":""choice"",""space"":['label_encoding','entity_embedding','random_projection']},
'fs__strategy':{""search"":""choice"",""space"":['l1','variance','rf_feature_importance']},
'fs__threshold':{""search"":""uniform"",""space"":[0.01, 0.3]},
'est__max_depth':{""search"":""choice"",""space"":[3,5,7,9]},
'est__n_estimators':{""search"":""choice"",""space"":[250,500,700,1000]}}# calculating the best hyper-parameter
best=Optimiser(scoring=""mean_squared_error"",n_folds=5).optimise(space,df,40)# predicting on the test dataset
Predictor().fit_predict(best,df)The above code ranked 108(top 1%) on the Public Leaderboard without having to even open the train and test file. I think this is pretty awesome.Below is the image of feature importance as calculated by LightGBM.Drift is not a common topic but a very important one and it deserves an article of its own. But I will try to explain the functionality of Drift_Thresholder in brief.In general, we assume that train and test dataset are created through the same generative algorithm or process but this assumption is quite strong and we do not see this behaviour in the real world. In the real world, the data generator or the process may change. For example, in a sales prediction model, the customer behaviour changes over time and hence the data generated will be different than the data that was used to create the model. This is called drift.Another point to note is that in a dataset, both the independent features and the dependent feature may drift. When the independent features changes, it is called the covariate shift and when the relationship between the independent and dependent features change, it is called the concept shift. MLBox deals with the covariate shift.The general algorithm for detection of drift is as follows-Entity Embeddings owe their existence to the word2vec embeddings in the sense that they function the same way as word vectors do. For example, we know that in word vector representation, we can do things like below.In the similar sense, categorical variables could be encoded to create new informative features. Their effect was evident to the world in Kaggles Rossmann Sales Problem where a team used Entity Embeddings along with Neural Network and came third without performing any significant feature engineering. The entire code and the research paper on Entity Embeddings that resulted from the competition could be found here. The Entity Embeddings were able to capture the relationship between the German states as shown below.I dont want to bog you down with the explanation of Entity Embeddings here. It deserves its own article. In MLBox, you can use Entity Embedding as a black box for encoding categorical variables.This library has its own sets of pros and cons.The pros are The cons are-So, I suggest you weigh the pros and cons before making this your mainstream library for Machine Learning.I was really excited to try this library as soon as I read about its release on Github. I spent the next couple of days studying the library and simplifying it for you to use it on the go. I must say that I am really impressed with the library and am going to explore even more. With just 8 lines of code, I was able to break into top 1% and without having to spendtime explicitly on handling data and hyperparameter optimisation, I could dedicate more time to feature engineering and check them on the fly. Please feel free to comment for any help or ideas below.",https://www.analyticsvidhya.com/blog/2017/07/mlbox-library-automated-machine-learning/
30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest  NLP],Learn everything about Analytics|Introduction||Skill Test Questions and Answers|End Notes,"Overall Distribution|Helpful Resources|Share this:|Like this:|Related Articles|Tutorial on Automated Machine Learning using MLBox|30 Questions to test a data scientist on Linear Regression [Solution: Skilltest  Linear Regression]|
Shivam Bansal
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Humans are social animals and language is our primary tool to communicate with the society. But, what if machines could understand our language and then act accordingly? Natural Language Processing (NLP) is the science of teaching machines how to understand the language we humans speak and write.We recently launched an NLP skill test on which a total of 817 people registered. This skill test was designed to test your knowledge of Natural Language Processing. If you are one of those who missed out on this skill test, here are the questions and solutions.Here are the leaderboard ranking for all the participants.Below are the distribution scores, they will help you evaluate your performance.You can access the scores here. More than 250 people participated in the skill test and the highest score obtained was 24.Here are some resources to get in-depth knowledge of the subject.Ultimate Guide to Understand & Implement Natural Language Processing (with codes in Python)Q1 Which of the following techniques can be used for the purpose of keyword normalization, the process of converting a keyword into its base form?
A) 1 and 2
B) 2 and 4
C) 1 and 3
D) 1, 2 and 3
E) 2, 3 and 4
F) 1, 2, 3 and 4Solution: (C)Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.
2) N-grams are defined as the combination of N keywords together. How many bi-grams can be generated from given sentence: Analytics Vidhya is a great source to learn data scienceA)7
B)8
C)9
D)10
E)11Solution: (C)Bigrams: Analytics Vidhya, Vidhya is, is a, a great, great source,source to, To learn, learn data, data science3) How many trigrams phrases can be generated from the following sentence, after performing following text cleaning steps:#Analytics-vidhya is a great source to learn @data_science.A)3
B)4
C)5
D)6
E)7Solution: (C)After performing stopword removal and punctuation replacement the text becomes: Analytics vidhya great source learn data scienceTrigrams  Analytics vidhya great, vidhya great source, great source learn, source learn data, learn data science4) Which of the following regular expression can be used to identify date(s) present in the text object: The next meetup on data science will be held on 2017-09-21, previously it happened on 31/03, 2016A)\d{4}-\d{2}-\d{2}
B)(19|20)\d{2}-(0[1-9]|1[0-2])-[0-2][1-9]
C)(19|20)\d{2}-(0[1-9]|1[0-2])-([0-2][1-9]|3[0-1])
D)None of the aboveSolution: (D)None if these expressions would be able to identify the dates in this text object.Question Context 5-6:You have collected a data of about 10,000 rows of tweet text and no other information. You want to create a tweet classification model that categorizes each of the tweets in three buckets  positive, negative and neutral.5) Which of the following models can perform tweet classification with regards to context mentioned above? A)Naive Bayes
B)SVM
C)None of the aboveSolution: (C)Since, you are given only the data of tweets and no other information, which means there is no target variable present. One cannot train a supervised learning model, both svm and naive bayes are supervised learning techniques.6) You have created a document term matrix of the data, treating every tweet as one document. Which of the following is correct, in regards to document term matrix?A)Only 1
B)Only 2
C)Only 3
D) 1 and 2
E) 2 and 3
F) 1, 2 and 3Solution: (D)Choices A and B are correct because stopword removal will decrease the number of features in the matrix, normalization of words will also reduce redundant features, and, converting all words to lowercase will also decrease the dimensionality.7) Which of the following features can be used for accuracy improvement of a classification model?A) Frequency count of terms
B) Vector Notation of sentence
C)Part of Speech Tag
D) Dependency Grammar
E) All of theseSolution: (E)All of the techniques can be used for the purpose of engineering features in a model.8) What percentage of the total statements are correct with regards to Topic Modeling?A)0
B)25
C)50
D)75
E)100Solution: (A)LDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant analysis. Selection of the number of topics is directly proportional to the size of the data, while number of topic terms is not directly proportional to the size of the data. Hence none of the statements are correct.9) In Latent Dirichlet Allocation model for text classification purposes, what does alpha and beta hyperparameter represent-A)Alpha: number of topics within documents, beta: number of terms within topics False
B)Alpha: density of terms generated within topics, beta: density of topics generated within terms False
C) Alpha: number of topics within documents, beta: number of terms within topics False
D)Alpha: density of topics generated within documents, beta: density of terms generated within topics TrueSolution: (D)Option D is correct10) Solve the equation according to the sentence I am planning to visit New Delhi to attend Analytics Vidhya Delhi Hackathon.A = (# of words with Noun as the part of speech tag)
B = (# of words with Verb as the part of speech tag)
C = (# of words with frequency count greater than one)What are the correct values of A, B, and C?A)5, 5, 2
B)5, 5, 0
C)7, 5, 1
D)7, 4, 2
E)6, 4, 3Solution: (D)Nouns: I, New, Delhi, Analytics, Vidhya, Delhi, Hackathon (7)Verbs: am, planning, visit, attend (4)Words with frequency counts > 1: to, Delhi (2)Hence option D is correct.11) In a corpus of N documents, one document is randomly picked. The document contains a total of T terms and the term data appears K times. What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term data appears in approximately one-third of the total documents?A)KT * Log(3)
B)K * Log(3) / T
C)T * Log(3) / K
D)Log(3) / KTSolution: (B)formula for TF is K/Tformula for IDF is log(total docs / no of docs containing data)= log(1 / ())= log (3)Hence correct choice is Klog(3)/TQuestion Context 12 to 14:Refer the following document term matrix
12) Which of the following documents contains the same number of terms and the number of terms in the one of the document is not equal to least number of terms in any document in the entire corpus. A)d1 and d4
B)d6 and d7
C)d2 and d4
D)d5 and d6Solution: (C)Both of the documents d2 and d4 contains 4 terms and does not contain the least number of terms which is 3.13) Which are the most common and the rarest term of the corpus?A)t4, t6
B)t3, t5
C)t5, t1
D) t5, t6Solution: (A)T5 is most common terms across 5 out of 7 documents, T6 is rare term only appears in d3 and d414) What is the term frequency of a term which is used a maximum number of times in that document? A)t6  2/5
B)t3  3/6
C)t4  2/6
D) t1  2/6Solution: (B)t3 is used max times in entire corpus = 3, tf for t3 is 3/615) Which of the following technique is not a part of flexible text matching?A)Soundex
B)Metaphone
C)Edit Distance
D)Keyword HashingSolution: (D)Except Keyword Hashing all other are the techniques used in flexible string matchingFeel like improving your skillset? Click Here16) True or False: Word2Vec model is a machine learning model used to create vector notations of text objects. Word2vec contains multiple deep neural networksA) TRUE
B) FALSESolution: (B)Word2vec also contains preprocessing model which is not a deep neural network17) Which of the following statement is(are) true for Word2Vec model?A) The architecture of word2vec consists of only two layers  continuous bag of words and skip-gram model
B) Continuous bag of word (CBOW) is a Recurrent Neural Network model
C) Both CBOW and Skip-gram are shallow neural network models
D) All of the aboveSolution: (C)Word2vec contains the Continuous bag of words and skip-gram models, which are deep neural nets.18) With respect to this context-free dependency graphs, how many sub-trees exists in the sentence?

A)3
B)4
C)5
D)6Solution: (D)Subtrees in the dependency graph can be viewed as nodes having an outward link, for example:Media, networking, play, role, billions, and lives are the roots of subtrees19) What is the right order for a text classification model components
A)12345
B)13425
C)12534
D)13452Solution: (C)A right text classification model contains  cleaning of text to remove noise, annotation to create more features, converting text-based features into predictors, learning a model using gradient descent and finally tuning a model.20) Polysemy is defined as the coexistence of multiple meanings for a word or phrase in a text object. Which of the following models is likely the best choice to correct this problem?A)Random Forest Classifier
B)Convolutional Neural Networks
C)Gradient Boosting
D)All of theseSolution: (B)CNNs are popular choice for text classification problems because they take into consideration left and right contexts of the words as features which can solve the problem of polysemy21) Which of the following models can be used for the purpose of document similarity?A)Training a word 2 vector model on the corpus that learns context present in the document
B)Training a bag of words model that learns occurrence of words in the document
C)Creating a document-term matrix and using cosine similarity for each document
D)All of the aboveSolution: (D)word2vec model can be used for measuring document similarity based on context.Bag Of Words and document term matrix can be used for measuring similarity based on terms.22) What are the possible features of a text corpus A) 1
B)12
C)123
D)1234
E)12345
F)123456Solution: (E)Except for entire document as the feature, rest all can be used as features of text classification learning model.23) While creating a machine learning model on text data, you created a document term matrix of the input data of 100K documents. Which of the following remedies can be used to reduce the dimensions of data  A)only 1
B)2, 3
C) 1, 3
D)1, 2, 3Solution: (D)All of the techniques can be used to reduce the dimensions of the data.24) Google Searchs feature  Did you mean, is a mixture of different techniques. Which of the following techniques are likely to be ingredients?A)1
B)2
C)1, 2
D)1, 2, 3Solution: (C)Collaborative filtering can be used to check what are the patterns used by people, Levenshtein is used to measure the distance among dictionary terms.25) While working with text data obtained from news sentences, which are structured in nature, which of the grammar-based text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection and object detection.A)Part of speech tagging
B) Dependency Parsing and Constituency Parsing
C) Skip Gram and N-Gram extraction
D) Continuous Bag of WordsSolution: (B)Dependency and constituent parsing extract these relations from the text26) Social Media platforms are the most intuitive form of text data. You are given a corpus of complete social media data of tweets. How can you create a model that suggests the hashtags? A)Perform Topic Models to obtain most significant words of the corpus
B) Train a Bag of Ngrams model to capture top n-grams  words and their combinations
C)Train a word2vector model to learn repeating contexts in the sentences
D)All of theseSolution: (D)All of the techniques can be used to extract most significant terms of a corpus.27) While working with context extraction from a text data, you encountered two different sentences: The tank is full of soldiers. The tank is full of nitrogen. Which of the following measures can be used to remove the problem of word sense disambiguation in the sentences? A)Compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood
B)Co-reference resolution in which one resolute the meaning of ambiguous word with the proper noun present in the previous sentence
C) Use dependency parsing of sentence to understand the meaningsSolution: (A)Option 1 is called Lesk algorithm, used for word sense disambiguation, rest others cannot be used.28) Collaborative Filtering and Content Based Models are the two popular recommendation engines, what role does NLP play in building such algorithms.A)Feature Extraction from text
B)Measuring Feature Similarity
C)Engineering Features for vector space learning model
D)All of theseSolution: (D)NLP can be used anywhere where text data is involved  feature extraction, measuring feature similarity, create vector features of the text.29) Retrieval based models and Generative models are the two popular techniques used for building chatbots. Which of the following is an example of retrieval model and generative model respectively.A)Dictionary based learning and Word 2 vector model
B)Rule-based learning and Sequence to Sequence model
C)Word 2 vector and Sentence to Vector model
D)Recurrent neural network and convolutional neural networkSolution: (B)choice 2 best explains examples of retrieval based models and generative models30) What is the major difference between CRF (Conditional Random Field) and HMM (Hidden Markov Model)?A)CRF is Generative whereas HMM is Discriminative model
B)CRF is Discriminative whereas HMM is Generative model
C)Both CRF and HMM are Generative model
D) Both CRF and HMM are Discriminative modelSolution: (B)Option B is correctI tried my best to make the solutions as comprehensive as possible but if you have any questions/doubts please drop in your comments below. And I would love to hear the feedback about the skill test. Feel free to share them in comments below. For latest and upcoming skill test please refer to the DataHack platform of Analytics Vidhya.If you want to learn more about Natural Language Processing and how it is implemented in Python, then check out our video courseon NLP using Python.Happy Learning!",https://www.analyticsvidhya.com/blog/2017/07/30-questions-test-data-scientist-natural-language-processing-solution-skilltest-nlp/
30 Questions to test a data scientist on Linear Regression [Solution: Skilltest  Linear Regression],Learn everything about Analytics|Introduction|Overall Distribution||Helpful Resources|Skill test Questions and Answers|End Notes,"Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Natural Language Processing [Solution: Skilltest  NLP]|Architecture of Convolutional Neural Networks (CNNs) demystified|
Ankit Gupta
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Linear Regression is still the most prominently used statistical technique in data science industry and in academia to explain relationships between features.A total of 1,355 people registered for this skill test. It was specially designed for you to test your knowledge on linear regression techniques. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how manycould have answered correctly.Here is the leaderboardfor the participants who took the test.Below is the distribution of the scores of the participants:You can access the scores here. More than 800 people participated in the skill test and the highest score obtained was 28.Here are some resources to get in depth knowledge in the subject.5 Questions which can teach you Multiple Regression (with R and Python)Going Deeper into Regression Analysis with Assumptions, Plots & Solutions7 Types of Regression Techniques you should know!1) True-False: Linear Regression is a supervised machine learning algorithm.A) TRUE
B) FALSESolution: (A)Yes, Linear regression is a supervised learning algorithm because it uses true labels for training. Supervised learning algorithm should have input variable (x) and an output variable (Y) for each example.2) True-False: Linear Regression is mainly used for Regression.A)TRUE
B)FALSESolution: (A)Linear Regressionhas dependent variables that have continuous values.3) True-False: It is possible to design a Linear regression algorithm using a neural network?A)TRUE
B)FALSESolution: (A)True. A Neural network can be used as a universal approximator, so it can definitely implement a linear regression algorithm.4) Which of the following methods do we use to find the best fit line for data in Linear Regression?A)Least Square Error
B)Maximum Likelihood
C) Logarithmic Loss
D) Both A and BSolution: (A)In linear regression, we try to minimize the least square errors of the model to identify the line of best fit.5) Which of the following evaluation metrics can be used to evaluate a model while modeling a continuous output variable?A)AUC-ROC
B)Accuracy
C)Logloss
D)Mean-Squared-ErrorSolution: (D)Since linear regression gives output as continuous values, so in such case we use mean squared error metric to evaluate the model performance. Remaining options are use in case of a classification problem.6) True-False: Lasso Regularization can be used for variable selection in Linear Regression.A)TRUE
B) FALSESolution: (A)True, In case of lasso regression we apply absolute penalty which makes some of the coefficients zero.7) Which of the following is true about Residuals ?A) Lower is better
B)Higher is better
C)A or B depend on the situation
D)None of theseSolution: (A)Residuals refer to the error values of the model. Therefore lower residuals are desired.8) Suppose that we have N independent variables (X1,X2 Xn) and dependent variable is Y. Now Imagine that you are applying linear regression by fitting the best fit line using least square error on this data.You found that correlation coefficient for one of its variable(Say X1) with Y is -0.95.Which of the following is true for X1?A) Relation between the X1 and Y is weak
B)Relation between the X1 and Y is strong
C)Relation between the X1 and Y is neutral
D)Correlation cant judge the relationshipSolution: (B)The absolute value of the correlation coefficient denotes the strength of the relationship. Since absolute correlation is very high it means that the relationship is strong between X1 and Y.9) Looking at above two characteristics, which of the following option is the correct for Pearson correlation between V1 and V2?If you are given the two variables V1 and V2 and they are following below two characteristics.1. If V1 increases then V2 also increases2. If V1 decreases then V2 behavior is unknownA)Pearson correlation will be close to 1
B)Pearson correlation will be close to -1
C)Pearson correlation will be close to 0
D)None of these
Solution: (D)We cannot comment on the correlation coefficient by using only statement 1. We need to consider the both of these two statements. Consider V1 as x and V2 as |x|. The correlation coefficient would not be close to 1 in such a case.10) Suppose Pearson correlation between V1 and V2 is zero. In such case, is it right to conclude that V1 and V2 do not have any relation between them?A)TRUE
B) FALSESolution: (B)Pearson correlation coefficient between 2 variables might be zero even when they have a relationship between them. If the correlation coefficient is zero, it just means that that they dont move together. We can take examples like y=|x| or y=x^2.11) Which of the following offsets, do we use in linear regressions least square line fit? Suppose horizontal axis is independent variable and vertical axis is dependent variable.A)Vertical offset
B)Perpendicular offset
C)Both, depending on the situation
D)None of aboveSolution: (A)We always consider residuals as vertical offsets. We calculate the direct differences between actual value and the Y labels. Perpendicular offset are useful in case of PCA.12) True- False: Overfitting is more likely when you have huge amount of data to train?A)TRUE
B)FALSESolution: (B)With a small training dataset, its easier to find a hypothesis to fit the training data exactly i.e. overfitting.13) We can also compute the coefficient of linear regression with the help of an analytical method called Normal Equation. Which of the following is/are true about Normal Equation?A)1 and 2
B)1 and 3
C)2 and 3
D)1,2 and 3Solution: (D)Instead of gradient descent, Normal Equation can also be used to find coefficients. Refer this article for read more about normal equation.14) Which of the following statement is true about sum of residuals of A and B?Below graphs show two fitted regression lines (A & B) on randomly generated data. Now, I want to find the sum of residuals in both cases A and B.Note:A) A has higher sum of residuals than B
B)A has lower sum of residual than B
C) Both have same sum of residuals
D)None of theseSolution: (C)Sum of residuals will always be zero, therefore both have same sum of residualsQuestion Context 15-17:Suppose you have fitted a complex regression model on a dataset. Now, you are using Ridge regression with penality x.15) Choose the option which describes bias in best manner.
A) In case of very large x; bias is low
B)In case of very large x; bias is high
C)We cant say about bias
D)None of theseSolution: (B)If the penalty is very large it means model is less complex, therefore the bias would be high.16) What will happen when you apply very large penalty?A)Some of the coefficient will become absolute zero
B) Some of the coefficient will approach zero but not absolute zero
C)Both A and B depending on the situation
D)None of theseSolution: (B)In lasso some of the coefficient value become zero, but in case of Ridge, the coefficients become close to zero but not zero.17) What will happen when you apply very large penalty in case of Lasso?
A)Some of the coefficient will become zero
B)Some of the coefficient will be approaching to zero but not absolute zero
C)Both A and B depending on the situation
D) None of theseSolution: (A)As already discussed, lasso applies absolute penalty, so some of the coefficients will become zero.18) Which of the following statement is true about outliers in Linear regression?A)Linear regression is sensitive to outliers
B)Linear regression is not sensitive to outliers
C)Cant say
D)None of theseSolution: (A)The slope of the regression line will change due to outliers in most of the cases. So Linear Regression is sensitive to outliers.19) Suppose you plotted a scatter plot between the residuals and predicted values in linear regression and you found that there is a relationship between them. Which of the following conclusion do you make about this situation?A)Since the there is a relationship means our model is not good
B)Since the there is a relationship means our model is good
C)Cant say
D)None of theseSolution: (A)There should not be any relationship between predicted values and residuals. If there exists any relationship between them,it means that the model has not perfectly captured the information in the data.Question Context 20-22:Suppose that you have a dataset D1 and you design a linear regression model of degree 3 polynomial and you found that the training and testing error is 0 or in another terms it perfectly fits the data.20) What will happen when you fit degree 4 polynomial in linear regression?
A)There are high chances that degree 4 polynomial will over fit the data
B)There are high chances that degree 4 polynomial will under fit the data
C)Cant say
D)None of theseSolution: (A)Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero.21) What will happen when you fit degree 2 polynomial in linear regression?
A)It is high chances that degree 2 polynomial will over fit the data
B)It is high chances that degree 2 polynomial will under fit the data
C)Cant say
D)None of theseSolution: (B)If a degree 3 polynomial fits the data perfectly, its highly likely that a simpler model(degree 2 polynomial) might under fit the data.22) In terms of bias and variance. Which of the following is true when you fit degree 2 polynomial?
A)Bias will be high, variance will be high
B)Bias will be low, variance will be high
C)Bias will be high, variance will be low
D)Bias will be low, variance will be lowSolution: (C)Since a degree 2 polynomial will be less complex as compared to degree 3, the bias will be high and variance will be low.Question Context 23:Which of the following is true about below graphs(A,B, C left to right) between the cost function and Number of iterations?23) Suppose l1, l2 and l3 are the three learning rates for A,B,C respectively. Which of the following is true about l1,l2 and l3?A)l2 < l1 < l3B)l1 > l2 > l3
C)l1 = l2 = l3
D)None of theseSolution: (A)In case of high learning rate, step will be high,the objective function will decrease quickly initially, but it will not find the global minima and objective function starts increasing after a few iterations.In case of low learning rate, the step will be small. So the objective function will decrease slowlyQuestion Context 24-25:We have been given a dataset with n records in which we have input attribute as x and output attribute as y. Suppose we use a linear regression method to model this data. To test our linear regressor, we split the data in training set and test set randomly.24) Now we increase the training set size gradually. As the training set size increases, what do you expect will happen with the mean training error?A)Increase
B)Decrease
C)Remain constant
D)Cant SaySolution: (D)Training errormay increase or decrease depending on the values that are used to fit the model. If the values used to train contain more outliers gradually, then the error might just increase.25) What do you expect will happen with bias and variance as you increase the size of training data?A)Bias increases and Variance increases
B)Bias decreases and Variance increases
C)Bias decreases and Variance decreases
D)Bias increases and Variance decreases
E) Cant Say FalseSolution: (D)As we increase the size of the training data, the bias would increase while the variance would decrease.Question Context 26:Consider the following data where one input(X) and one output(Y) is given.26) What would be the root mean square training error for this data if you run a Linear Regression model of the form (Y = A0+A1X)?A) Less than 0
B)Greater than zero
C)Equal to 0
D)None of theseSolution: (C)We can perfectly fit the line on the following data so mean error will be zero.Question Context 27-28:Suppose you have been given the following scenario for training and validation error for Linear Regression.27) Which of the following scenario would give you the right hyper parameter?A)1
B)2
C)3
D)4Solution: (B)Option B would be the better option because it leads to less training as well as validation error.28) Suppose you got the tuned hyper parameters from the previous question. Now, Imagine you want to add a variable in variable space such that this added feature is important. Which of the following thing would you observe in such case?A)Training Error will decrease and Validation error will increaseB) Training Error will increase and Validation error will increase
C)Training Error will increase and Validation error will decrease
D)Training Error will decrease and Validation error will decrease
E)None of the aboveSolution: (D)If the added feature is important, the training and validation error would decrease.Question Context 29-30:Suppose, you got a situation where you find that your linear regression model is under fitting the data.29) In such situation which of the following options would you consider?A)1 and 2
B)2 and 3
C)1 and 3
D)1, 2 and 3Solution: (A)In case of under fitting, you need to induce more variables in variable space or you can add some polynomial degree variables to make the model more complex to be able to fir the data better.30) Now situation is same as written in previous question(under fitting).Which of following regularization algorithm would you prefer?A)L1
B)L2
C)Any
D)None of theseSolution: (D)I wont use any regularization methods because regularization is used in case of overfitting.I tried my best to make the solutions as comprehensive as possible but if you have any questions / doubts please drop in your comments below. I would love to hear your feedback about the skilltest.For more such skilltests, check out our current hackathons.",https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/
Architecture of Convolutional Neural Networks (CNNs) demystified,Learn everything about Analytics|Introduction|Table of Contents:|1. How does a machine look at an image?|2. How do we help a neural network to identify images ?|So what did we do ?|3. Defining a Convolutional Neural Network|3. Putting it all together  How does the entire network look like ?|4. Using CNN to classify images in KERAS|Projects||End Notes,"Case 1:|Case 2:|Case 3:|Case 4:|Case 5:|2.1 The Convolution Layer|The concept of stride and padding|Multiple filters and theactivation map|2.2 The Pooling Layer|Output dimensions|2.3 The Output layer|Share this:|Like this:|Related Articles|30 Questions to test a data scientist on Linear Regression [Solution: Skilltest  Linear Regression]|Big Data Architect- Mumbai (7+ Years of Experience)|
Dishashree Gupta
|59 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I will start with a confession  there was a time when I didnt really understand deep learning. I would look at the research papers and articles on the topic and feel like it is a very complex topic. I tried understanding Neural networks and theirvarious types, but it still looked difficult.Then one day, I decided to take one step at a time. I decided to start with basics and build on them. I decided that I will break down the steps applied in these techniques and do the steps (and calculations) manually, until I understand how they work. It was time taking and intense effort  but the results were phenomenal.Now, I can not only understand the spectrum of deep learning, I can visualize things and come up with better ways because my fundamentals are clear. It is one thing to apply neural networks mindlessly and it is other to understand what is going on and how are things happening at the back.Today, I am going to share this secret recipe with you. I will show you how I took the Convolutional Neural Networks and worked on them till I understood them. I will walk you through the journey so that you develop a deep understanding of how CNNs work.In this article I am going to discuss the architecture behind Convolutional Neural Networks, which are designed to address image recognition and classification problems.I am assuming that you have a basic understanding of how a neural network works. If youre not sure of your understanding I would request you to go through this article before you read on.Human brain is a very powerful machine. We see (capture) multiple images every second and process them without realizing how the processing is done. But, that is not the case with machines. The first step in image processing is to understand, how to represent an image so that the machine can read it?In simple terms, every image is an arrangement of dots (a pixel) arranged in a special order. If you change the order or color of a pixel, the image would change as well. Let us take an example. Let us say, you wanted to store and read an image with a number 4 written on it.The machine will basically break this image into a matrix of pixels and store the color code for each pixel at the representative location. In the representation below  number 1 is white and 256 is the darkest shade of green color (I have constrained the example to have only one color for simplicity).Once you have stored the images in this format, the next challenge is to have our neural network understand the arrangement and the pattern.A number is formed by having pixels arranged in a certain fashion.Lets say we try to use a fully connected network to identify it? What does it do ?A fully connected network would take this image as an array by flattening it and considering pixel values as features to predict the number in image. Definitely its tough for the network to understand whats happening underneath.Its impossible even for a human to identifythat this is a representation of number 4. We have lost the spatial arrangement of pixels completely.What can we possibly do? Lets try to to extract features from the original image such that the spatial arrangement is preserved.Here we have used a weight to multiply the initial pixel values.It does get easier for the naked eye to identify that this is a 4. But again to send this image to a fully connected network, we would have to flatten it. We are unable to preserve the spatial arrangement of the image.Now we can see that flattening the image destroys its arrangement completely. we need to devise a way to send images to a network without flattening them and retaining its spatial arrangement. We need to send 2D/3D arrangement of pixel values.Lets try taking two pixel values of the image at a time rather than taking just one. This would give the network a very good insight as to how does the adjacent pixel look like. Now that were taking two pixels at a time, we shall take two weight values too.I hope you noted that the image now became a 3 column arrangement from a 4 columnarrangement initially. The image got smaller since were now moving two pixels at a time (pixels are getting shared in each movement). We made the image smaller and we can still understand that its a 4 to quite a great extent. Also, an important fact to realise is that we were taking two consecutive horizontal pixels, therefore only horizontal arrangement is considered here.This is one way to extract features from an image. Were able to see the left and middle part well, however the right side is not so clear. This is because of the following two problems-Now we have two problems,we shall have two solutions to solve them as well.The problem encountered is that the left and right corners of the image is getting passed by the weight just once. What we need to do is we need the network to consider the corners also like other pixels.We have a simple solution to solve this. Put zeros along the sides of the weight movement.You can see that by adding the zeroes the information from the corners is retained. The size of the image is higher too. This can be used in cases where we dont want the image size to reduce.The problem were trying to address here is that a smaller weight value in the right side corner is reducing the pixel value thereby making it tough for us to recognize. What we can do is, we take multiple weight values in a single turn and put them together.A weight value of (1,0.3) gave us an output of the formwhile a weight value of the form (0.1,5) would give us an output of the formA combined version of these two images would give us a very clear picture. Therefore what we did was simply use multiple weights rather than just one to retain more information about the image. The final output would be a combined version of the above two images.Till now we have used the weights which were trying to take horizontal pixels together. But in most cases we need to preserve the spatial arrangement in both horizontal and vertical direction. We can take the weight as a 2D matrix which takes pixels together in both horizontal and vertical direction. Also, keep in mind that since we have taken both horizontal and vertical movement of weights, the output is one pixel lower in both horizontal and vertical direction.Special thanks to Jeremy Howard for the inspiring me to create these visuals.What we did above was that we were trying to extract features from an image by using the spatial arrangement of the images. To understand an image its extremely important for a network to understand how the pixels are arranged. What we did above is what exactly a convolutional neural network does. We can take the input image, define a weight matrix and the input is convolved to extract specific features from the image without losing the information about its spatial arrangement.Another great benefit this approach has is that it reduces the number of parameters from the image. As you saw above the convolved images had lesser pixels as compared to the original image. This dramatically reduces the number of parameters we need to train for the network.We need three basic components to define a basic convolutional network.Lets see each of these in a little more detailIn this layer, what happens is exactly what we saw in case 5 above. Suppose we have an image ofsize 6*6. We define a weight matrix which extracts certain features from the imagesWe have initialized the weight as a 3*3 matrix. This weight shall now run across the image such that all the pixels are covered at least once, to give a convolved output. The value 429 above, is obtained by theadding the values obtained by element wise multiplication of the weight matrix and the highlighted 3*3 part of the input image.The 6*6 image is now converted into a 4*4 image. Think of weight matrix like a paint brush painting a wall. The brush first paints the wall horizontally and then comes down and paints the next row horizontally. Pixel values are used again when the weight matrix moves along the image. This basically enables parameter sharing in a convolutional neural network.Lets see how this looks like in a real image.The weight matrix behaves like a filter in an image extracting particular information from the original image matrix. A weight combination might be extracting edges, while another one might a particular color, while another one might just blur the unwanted noise.The weights are learnt such that the loss function is minimized similar to an MLP. Therefore weights are learnt to extract features from the original image which help the network in correct prediction. When we have multiple convolutional layers, the initial layer extract more generic features, while as the network gets deeper, the features extracted by the weight matrices are more and more complex and more suited to the problem at hand.As we saw above, the filter or the weight matrix, was moving across the entire image moving one pixel at a time. We can define it like a hyperparameter, as to how we would want the weight matrix to move across the image. If the weight matrix moves 1 pixel at a time, we call it as a stride of 1. Lets see how a stride of 2 would look like.As you can see the size of image keeps on reducing as we increase the stride value. Padding the input image with zeros across it solves this problem for us. We can also add more than one layer of zeros around the image in case of higher stride values.We can see how the initial shape of the image is retained after we padded the image with a zero.This is known as same padding since the output image has the same size as the input.This is known as same padding(which means that we considered only the valid pixels of the input image). The middle 4*4 pixels would be the same. Here we have retained more information from the borders and have also preserved the size of the image.One thing to keep in mind is that the depth dimension of the weight would be same as the depth dimension of the input image. The weight extends to the entire depth of the input image. Therefore, convolution with a single weight matrix would result into a convolved output with a single depth dimension. In most cases instead of a single filter(weight matrix), we have multiple filters of the same dimensions applied together.The output from the each filter is stacked together forming the depth dimension of the convolved image. Suppose we have an input image of size 32*32*3. And we apply 10 filters of size 5*5*3 with valid padding. The output would have the dimensions as 28*28*10.You can visualize it as This activation map is the output of the convolution layer.Sometimes when the images are too large,we would need to reduce the number of trainable parameters. It is then desired to periodically introduce pooling layers between subsequent convolution layers. Pooling is done for the sole purpose of reducing the spatial size of the image. Pooling is done independently on each depth dimension, therefore the depth of the image remains unchanged. The most common form of pooling layer generally applied is the max pooling.Here we have taken stride as 2, while pooling size also as 2. The max operation is applied to each depth dimension of the convolved output. As you can see, the 4*4 convolved output has become 2*2 after the max pooling operation.Lets see how max pooling looks on a real image.As you can see I have taken convoluted image and have applied max pooling on it. The max pooled image still retains the information that its a car on a street. If you look carefully, the dimensions if the image have been halved. This helps to reduce the parameters to a great extent.Similarly other forms of pooling can also be applied like average pooling or the L2 norm pooling.It might be getting a little confusing for you to understand the input and output dimensions at the end of each convolution layer. I decided to take these few lines to make you capable of identifying the output dimensions. Three hyperparameter would control the size of output volume.We can apply a simple formula to calculate the output dimensions. The spatial size of the output image can be calculated as( [W-F+2P]/S)+1. Here, W is the input volume size, F is the size of the filter, P is the number of padding applied and S is the number of strides. Suppose we have an input image of size 32*32*3, we apply 10 filters of size 3*3*3, with single stride and no zero padding.Here W=32, F=3, P=0 and S=1. The output depth will be equal to the number of filters applied i.e. 10.The size of the output volume will be ([32-3+0]/1)+1 = 30. Therefore the output volume will be 30*30*10.After multiple layers of convolution and padding, we would need the output in the form of a class. The convolution and pooling layers would only be able to extract features and reduce the number of parameters from the original images. However, to generate the final output we need to apply a fully connected layer to generate an output equal to the number of classes we need. It becomes tough to reach that number with just the convolution layers. Convolution layers generate 3D activation maps while we just need the output as whether or not an image belongs to a particular class. The output layer has a loss function like categorical cross-entropy, to compute the error in prediction. Once the forward pass is complete the backpropagation begins to update the weight and biases for error and loss reduction.CNN as you can now see is composed of various convolutional and pooling layers. Lets see how the network looks like.Lets try taking an example where we input several images of cats and dogs and we try to classify these images into their respective animal category. This is a classic problem of image recognition and classification. What the machine needs to do is it needs to see the image and understand by the various features as to whether its a cat or a dog.The features can be like extracting the edges, or extracting the whiskers of a cat etc. The convolutional layer would extract these features. Lets take a hand on the data set.These are the examples of some of the images in the dataset.   we would first need to resize these images to get them all in the same shape. This is something we would generally need to do while handling images, since while capturing images, it would be impossible to capture all images of the same size.For simplicity of your understanding I have just used a single convolution layer and a single pooling layer, which generally doesnt happen when were trying to make predictions. Dataset used can be downloaded from here.#import various packagesimport os
import numpy as np
import pandas as pd
import scipy
import sklearn
import keras
from keras.models import Sequential
import cv2
from skimage import io
%matplotlib inline#Defining the File Pathcat=os.listdir(""/mnt/hdd/datasets/dogs_cats/train/cat"")
dog=os.listdir(""/mnt/hdd/datasets/dogs_cats/train/dog"")
filepath=""/mnt/hdd/datasets/dogs_cats/train/cat/""
filepath2=""/mnt/hdd/datasets/dogs_cats/train/dog/""#Loading the Imagesimages=[]
label = []
for i in cat:
 image = scipy.misc.imread(filepath+i)
 images.append(image)
 label.append(0) #for cat imagesfor i in dog:
 image = scipy.misc.imread(filepath2+i)
 images.append(image)
 label.append(1) #for dog images#resizing all the imagesfor i in range(0,23000):
 images[i]=cv2.resize(images[i],(300,300))#converting images to arraysimages=np.array(images)
label=np.array(label)# Defining the hyperparametersfilters=10
filtersize=(5,5)epochs =5
batchsize=128input_shape=(300,300,3)#Converting the target variable to the required sizefrom keras.utils.np_utils import to_categorical
label = to_categorical(label)#Defining the modelmodel = Sequential()model.add(keras.layers.InputLayer(input_shape=input_shape))model.add(keras.layers.convolutional.Conv2D(filters, filtersize, strides=(1, 1), padding='valid', data_format=""channels_last"", activation='relu'))
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.Flatten())model.add(keras.layers.Dense(units=2, input_dim=50,activation='softmax'))model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(images, label, epochs=epochs, batch_size=batchsize,validation_split=0.3)model.summary()In this model, I have only used a single convolution and Pooling layer and the trainable parameters are 219,801. Wonder how many would I have had if i had used an MLP in this case. You can reduce the number of parameters by further by adding more convolution and pooling layers. The more convolution layers we add the features extracted would be more specific and intricate.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:I hope through this article I was able to provide you an intuition into convolutional neural networks. I did not go into the complex mathematics of CNN. In case youre fond of understanding the same  stay tuned, theres much more lined up for you. Try building your own CNN network to understand how it operates and makes predictions on images. Let me know your findings and approach using the comments section.",https://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/
Big Data Architect- Mumbai (7+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Architecture of Convolutional Neural Networks (CNNs) demystified|Hands on with Deep Learning  Solution for Age Detection Practice Problem|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  15 years
Requirements : 
Task Info : Work on state-of-the art technologies and build things at a scale never seen before in India.This profile is a mix of: implementing complex data processing flows to crunch data and produce insights leading and supporting data scientists to create world class products Bringing about significant innovation and solving complex problems in projects based on analytics-Evaluating impact of software performance, and recommending changes to software design team.Desired qualifications MTech/MS in Computer Science. Typically 7 or more years of experience executing on projects as a lead. In depth knowledge of data science principles and best practices Programming experience in Scala, Java or Python Experience with Apache Spark platform. Experience with big data technologies and databases like Redshift ,Elasticsearch, and Hadoop Hands-on with Data Handling  data acquisition, data transformation, and data cleaning. Expertise in Big Data with experience to handle and work with terabytes of data Familiarity with modern machine learning methods for regression and classification.
College Preference : tier1-entire
Min Qualification : pg
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/big-data-architect-mumbai-7-years-of-experience/
Hands on with Deep Learning  Solution for Age Detection Practice Problem,Learn everything about Analytics|Introduction||Table of Contents|Why Participate in a Practice Problem (Deep Learning)?|What is the problem again?|Lets solve the problem!|Intermission: Our first submission!|Lets solve the problem! Part 2: Building better models||Intermission: Visual Inspection of our predictions|Whats Next|End Notes:,"Share this:|Like this:|Related Articles|Big Data Architect- Mumbai (7+ Years of Experience)|SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience)|
Faizan Shaikh
|25 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is one thing to learn data science by reading or watching a video / MOOC and other to apply it on problems. You need to do both the things to learn the subject effectively. Todays article is meant to help you apply deep learning on an interesting problem.If you are questioning, why learn or apply deep learning  you have most likely come out of a cave just now. Deep learning in already powering face detection in cameras, voice recognition on mobile devices to deep learning cars. Today, we will solve age detection problem using deep learning.If you are new to deep learning, I would recommend you to refer the articles below before going through this tutorial and making a submission.If you have learnt or read about Deep learning in last few days / months, and are looking for applying your new skill  practice problems are the right place to start. I say so because they provide you an experience of solving problems from scratch without making them too competitive.Here are the reasons why you should pick up a few practice problems:The first step to do when participating in a hackathon is to understand the problem. In this article, we will look at a recently published practice problem: Age Detection of Indian Actors. You can view the problem statement on the hackathon page, but I will briefly mention it here.The task is to predict the age of a person from his or her facial attributes. For simplicity, the problem has been converted to a multi-class problem with classes as Young, Middle and Old.Seems easy at a first glance right?If you actually see the data, it seems hard even for a human! Lets check for ourselves! Here are some random good examples from our data.Middle Aged:Old:Young:But can you guess these?Apparently both are middle aged actors!To solve these problems, you need to have a streamlined approach. We will see this in the next section.Now that you know the problem, let us get started. I assume you have numpy, scipy, pandas, scikit-learn and kerasinstalled. If you dont, please install them. The articles above should help you.First things first; let us download the data and load it into our jupyter notebooks! If you havent already been introduced; here is the link to the practice problem (https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/).Before building a model, I urge you to solve this simple exercise:Can you write a script that will randomly load an image into jupyter notebook and print it? (PS: Dont look at the answer below!). Post your code in this discuss thread.Heres my approach to the exercise; As always, I first imported all the necessary modules,Then I loaded the csv files, so that it would be easier to locate the filesThen I wrote a script to randomly choose an image and printed itHeres what I got:Age: YOUNGThe motive of this exercise was that you can randomly view the dataset and check what problems you could possibly face when building the model.Here are a few of my hypotheses of what problems we could faceside viewfront viewFor now, let us focus on only one problem, viz how to handle variations in shape?We can do this by simply resizing the image. Let us load all the images and resize them into a single numpy arrayAnd similarly for test imagesWe can do one more thing that could help us build a better model; i.e. we can normalize our images. Normalizing the images will make our train faster.Now lets take a look at our target variable. I have another exercise for you; What is the distribution of classes in our data? Could you say it is a highly imbalanced problem?Heres my try for the exercise;On the basis of distribution of our data, we can create a simple submission. We see that most of the actors are middle aged. So we can say that all the actors in our test dataset are middle aged!Upload this file on the submission page to see the result!Before creating something substantial, let us bring our target variable in shape. We will convert our target into dummy columns so that it will be easier for our model to ingest it. Heres how I would do it.Now comes the main part, building a model! As the problem is related to image processing, it is wiser to use neural networks to solve the problem. We will too build a simple feedforward neural network for this problem.First we should specify all the parameters we will be using in our networkThen we will import the necessary keras modulesAfter that, we will define our networkTo see how our model looks like; lets print itNow lets compile our network and let it train for a whileSeems like its training! But we still havent validated it. Validation is necessary if we want to ensure that our model will perform well on both the data it is training on and on a new testing dataLets tweak the code a little bit to cross validate it.The model seems to perform good for a first model. Lets submit the result.Heres another simple exercise for you; Print the image along with the predictions of your trained model. Do this preferably on your training dataset so that you can check your predictions along with the real targetHeres my take on the exercise,We have built a benchmark solution with a simple model at hand. What more can we do?Here are some tips I can suggestIn this article, I have explained a simple benchmark solution for Age Detection Practice Problem. Theres many things you could do which even I havent mentioned in the article. You can suggest them in the comments below!",https://www.analyticsvidhya.com/blog/2017/06/hands-on-with-deep-learning-solution-for-age-detection-practice-problem/
SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hands on with Deep Learning  Solution for Age Detection Practice Problem|Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : Job Description and Responsibilities: Very strong SAS, SQL, data modelling and transformation skills  with ability to manipulate large data sets, automate processes and debug code with minimal steer. -Strong passion for data exploration, manipulation, data quality and bench marking utilizing SAS or any other analytical tools -Become data and process experts across different clusters feeding information into the DBM models. -Excellent knowledge of data warehouse and MI environments  Knowledge discovery and presenting findings for range of stakeholders. This involves importing, cleaning, transforming, validating and bench marking or modelling data with the purpose of understanding or making conclusions from the data for decision making purposes  End-to-end project planning and delivery. Ability to work dynamically on multiple projects independently and strong stakeholder management while putting customers in the heart of everything we do  Validate, track, and monitor delivered projects.  Upkeep of BAU processes by ensuring regular review of DBM data preparation processes to identify areas of improvement, focusing on accuracy, robustness and controls  Produce robust documentation to ensure reliability of results Essential Skills :  A high level of analytical work experience in financial services strongly preferred.  Extensive knowledge of SAS, Rational Databases (Oracle, Teradata etc.), UNIX, Tivoli and other analytic toolsets.  Experience in consumer credit risk and/or finance analytics across customer lifecycle  Knowledge of financial services portfolios. Awareness of the economy, market and customer trends affecting the business.  Familiarity with analytical techniques and their value in business.  Strong communication and interpersonal skills. Preferred Skills :  Experience in working on Hadoop platform and tools such as Impala, Hive, Spark, Pig, etc. is a bonus  knowledge of Collections and Recoveries in customer life cycle  Experience with other analytical tools such as R, WPS and SPSS Knowledge of SAP Business Objects is desirable but not essential  Familiar modelling (PD, LGD and EaD) and Regulatory reporting  University degree in quantitative discipline is an advantage (e.g. Computer Science, Statistics, Operations Research, Economics and Engineering)
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/sasdata-manager-bfsi-delhincrgurugram-3-7-years-of-experience/
Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SAS/Data Manager (BFSI)- Delhi/NCR/Gurugram (3-7 Years Of Experience)|Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Job Description and Responsibilities: Require professionals with retail analytics experience (2-6 years) preferably in Retail Domain would be ideal.  2 to 3 years of banking experience.  Has worked in a model development in retail environment within acquisitions/existing customer management.  The role will support model development team for retail portfolio  Mathematical / statistical background.  Strong SAS analytics and programming skills  MS Office skills: Excel (Advance, including pivot tables, and macro), Word and PowerPoint (intermediate).  MI report development and assisting with other ad hoc reporting requirements  High-level attention to detail.  Ability to work independently with minimal supervision. Skill Matrix :  Proficiency in MS Excel  Proficiency in SAS and UNIX  Strong QA acumen  Excellent communication skills
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-retail-analytics-bangalore-2-7-years-of-experience/
Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager (Retail Analytics)- Bangalore (2-7 Years Of Experience)|Course Review  PG Diploma in Data Analytics by UpGrad & IIIT-B|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : Job Description and Responsibilities: Gather and document business requirements  Analyze and interpret data to identify trends, patterns and opportunities for the business and clients  Communicate analysis and interpretation to appropriate audience  Produce, publish and distribute scheduled and ad-hoc client and operational reports relating to the development and performance of products, processes and technologies Required Qualifications : Strong 4+ years U.S. healthcare domain knowledge. 3+ years SQL / PL SQL knowledge. Good knowledge of ETL concepts. Ability to identify sets and subsets of information across multiple joins or unions of tables Ability to effectively participate in multiple, concurrent projects Experience analyzing and drawing valuable conclusions from the data profiling results Preferred Qualifications : Experience with more than one relational database Knowledge of, or experience working with SAS Knowledge of trend analytics concepts Knowledge of data modeling concepts Required Qualifications : Bachelors degree in Business, Finance, Health Administration, related field or equivalent work experience 1+ years of experience in business/finance including analysis experience with a solid understanding of data storage structures1+ years of experience with project methodology (requirements, design, development, test and implementation) 
College Preference : no-bar
Min Qualification : ug
Skills : Array
Location : Array
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-healthcare-hyderabad-1-4-years-of-experience/
Course Review  PG Diploma in Data Analytics by UpGrad & IIIT-B,Learn everything about Analytics|Table of Contents|1. The Review process|2. My first impressions about the course|3. Partnership with IIIT-Bangalore|4. Application|5 The curriculum|6. Interaction with Stakeholders|My view  who is this course for?|7. Conclusion,"4.1 The application/selection process:|4.2 Applicant profile|5.1 Pre-requisites:|5.2 Course Outline:|5.3 Content of the course|5.4 Career Guidance sessions|6.1 My interaction with Faculty|6.2 Interaction with Industry experts|6.3 My interaction with students||Other details|Share this:|Like this:|Related Articles|Business Analyst (Healthcare)- Hyderabad (1-4 Years Of Experience)|Machine Learning Lead- Bangalore (3 to 7 years of experience)|
Kunal Jain
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The analytics training landscape in India is changing very rapidly and for the right reasons. There is a huge talent gap to solve for. The U.S. alone is expected to have a shortfall of 150,000 data scientists by 2018. The number for India would also be similar.People are inventing and discovering new ways to try and bridge this skill gap in the industry. As a result, not surprisingly, many new online and offline courses are emerging for people to upgrade their skills to match the industry demand.In the last few months, we received several queries regarding the PG Diploma in Data Analytics Program being offered by UpGrad and IIIT-Bangalore. The course has been running for more than a year now and I thought it would be good to review their material and publish my review to help people make the right career decision.Like other reviews which we have done in the past, we interacted with various stakeholders across the chain to review the course. This includes Management, Faculty, past students and current students of the program. This time too, I reached out to the UpGrad team and asked them for access to the course material and they happily obliged.While the material is provided by UpGrad  all views expressed in the article are my own.When I first came across the UpGrad & IIIT-B offering, it looked like just another offering from a top-tier education institute in India, aimed at working professionals. I thought that, like most of the other competing programs out there  they would take 2  3 years to learn about the industry before they get the program right. Thankfully, I was wrong.When I interacted with the UpGrad team, a couple of things stood out compared to other institutes / courses I have reviewed in past:Since the program is run online, I wasnt sure how the partnership with IIIT-Bangalore works. Hence, I spent some time understanding the model. IIIT-B and UpGrad collaborate on every aspect of the program including curriculum creation, program development, program delivery, career support etc. The learners receive a PG Diploma in Data Analytics from IIIT-B post successful completion.This is a unique programs launched by IIIT-B. The other programs that IIITB offers are full time, offline programs providing degrees such as M. Tech and PhD. This program, despite being online, has tried to keep most of the features of the universitys offline experience, such as a robust credit structure in place.I was happy that UpGrad was applying this selection criteria instead of accepting most of the applications they get. Hopefully, that should improve the experience of the people enrolled in the course.The PGDDA has two ongoing batches of 300 and 500 learners. The program boasts a healthy mix of individuals drawn from a variety of backgrounds and bringing in a wealth of prior experience.All the qualified learners are expected to go through a preparatory program which covers some mathematical  such as matrix, linear algebra  and technical prerequisites such as R, SQL, Tableau, Excel, Python.The 11 month PG Diploma program is a 36 credit program which comprises 7 courses (please check this on course website as it might change over time). The first 6 courses are:Course OutlineAll of these courses carry 4 credits each. The 7th course is a 3 month long capstone project, which carries 12 credits, and has been developed in collaboration with various companies/Industry professionals.This definitely looks comprehensive for an 11 month program  but let us look at the content a bit more closely.The interface of the UpGrad portal (LMS) looks good and intuitive to use. You can see a summary of your progress on the course screen. Additionally, there is a discussion portal (the participation in which is counted towards evaluation) and a browser based coding window. You can access past lectures and industry interactions here too.Discussion portalThe videos were of high quality and contained interesting story lines. The participant needs to watch the entire video before unlocking the exercises and moving ahead. I personally am not a big fan of this feature as I like to see the content selectively. But, I can understand why UpGrad decided to keep it this way to enforce the participation.There is a sharp focus on preparing the learners for jobs in the industry. The support provided to learners is on the following front:My interaction with Prof. Tricha from IIIT Bangalore gave me an inside view about the efforts which have gone in creating the course and some of the challenges which the team is working on. We also discussed how the assessment was different for this course compared to other courses. Interestingly, the assessment includes grading activities on the discussion portal. Here is the weightage of various activities in the course assessment:UpGrad has partnered with companies and senior individual contributors from the industry to create and deliver this program. They have experts from Uber, Fractal Analytics, Genpact, Viacom, Tata iQ etc. who work closely with them to develop the case studies. These experts are also involved in delivering lectures and providing industry perspective to the learners.Overall the industry engagement during the program would be around 40%. This can be experienced by learners in the form of recorded lectures, live sessions and even mentorship sessions. This link contains the list of faculty who are part of this program.During my interaction with industry faculty members, they told me that they are very excited about what UpGrad is creating and have high confidence in the team. They said that the willingness to try new things, really stands out.The students were overall happy about their learning and also the effort that was put in by the UpGrad team. They said that the team was highly approachable. The students mentioned that the course is fairly intensive and their schedule ends up being hectic  all for a good cause though.The students appreciated the connect with industry experts and the effort from UpGrad team. They also suggested that having a full time instructor with industry experience might further improve their learning.Manshul was working as a consultant at TCS when he decided to take a plunge into analytics after 9+ years of experience.Initially I was quite apprehensive about the program quality and given my 9-year experience, how would I keep pace with young people in my batch. But my experience has only been rewarding. From program content to networking to career support, this program offers an end-to-end solution. I got placed in Opera Solutions as Sr. Data Engineer through UpGrad IIIT-B career support.-Manshul Goel, Sr. Data Engineer, Opera Solutions.Sajal Roychowdhury was working with Amazon as a risk analyst and then transitioned to Shopclues as a Business Analyst. He enjoyed the peer to peer interaction during the program.It is brilliant to collaborate and learn with otherseven though it is an online program. IIIT-B is a good brand to have and the curriculum of the program is solid.-Sajal Roychowdhary, Business Analyst, ShopcluesThis course is the right fit for people with experience who want to understand / experience analytics, but cannot / do not want to take time out for attending an offline course. The UpGrad platform has been created after a lot of careful consideration, research and testing. The team is dedicated to improving the platform regularly.The content is of high quality and is delivered by experienced faculty. I can say very confidently that among all the courses coming up in India, the UpGrad & IIITB course has evolved the fastest and will continue to do so.While all the efforts are in the right direction, it will become clear in the next 6 months how the industry accepts those who have graduated from this course.I enjoyed talking to the UpGrad team and would thank them for providing me unrestricted access to their faculty / students. I loved their focus on the platform, the attitude of the team to get things done, quality of their content and the innovation they are bringing in.What would I want to see more? I would want them to bring on-board a full time specialist for the course. Also, the statistics about the placements would only be available in next 6  12 months.Disclosure: This sponsored post has been written by Analytics Vidhya on behalf of, and with inputs from UpGrad. All opinions expressed in this post are entirely those of Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2017/06/course-review-pg-diploma-in-data-analytics-by-upgrad-iiit-b/
Machine Learning Lead- Bangalore (3 to 7 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Course Review  PG Diploma in Data Analytics by UpGrad & IIIT-B|Big Data Engineer- Bangalore (2 to 3 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : Target Companies: Google, Quest Global, Tata Elxsi, GE Digital, Robert Bosh Engineering Services, Fractal.
Task Info : 1. Min: 7 year of total experience with recent & relevant experience of 3 year on Machine Learning.2. Strong hands on experience on Python, Pyspark and R programming language3. Hands on experience in identifying the right algorithm / technique to solve a particular business problem4. Experience in building and scaling models for sensor/time series data5. Experience in building products6. Experience in building predictive maintenance algorithms7. Hackers mind-set / Quick learner8. Should be able to guide a team of developers
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, Pyspark, python, r, time series
Location : Bangalore
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/machine-learning-lead-bangalore-3-to-7-years-of-experience/
Big Data Engineer- Bangalore (2 to 3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Lead- Bangalore (3 to 7 years of experience)|Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : Should have prior customer facing experience.
Ability to lead all requirement gathering sessions with the Customer
Strong co-ordination and interpersonal skills to handle complex projects.
Task Info : Relevant Experience (In Years) :1. Minimum 2 years of strong experience on Core Java, Hadoop ecosystem and any NoSQL Database.2. Minimum 1.5 or 2 Years of strong experience on Spark/Storm/Cassandr/Kafka/Scala.Technical/Functional Skills :1. Core Java, Multi-Threading, OOPS, Writing Parsers2. Hadoop/Hive/Pig/MapReduce3. Spark/Storm/Kafka/Scala/Cassandra4. SQL5. Cloud Computing(AWS/Azure etc)Roles & Responsibilities:1. Strong on Core Java, Multi-Threading, OOPS Concept, writing parsers in Core Java2. Should have strong knowledge on Hadoop ecosystem such as Hive/Pig/MapReduce3. Strong in SQL, NoSQL, RDBMS and Data warehousing concepts4. Writing complex MapReduce programs5. Should have strong experience on pipeline building such Spark or Storm or Cassandra or Scala.6. Designing efficient and robust ETL workflows7. Gather and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.).8. Tuning Hadoop solutions to improve performance and end-user experience;9. Processing unstructured data into a form suitable for analysis  and then do the analysis.10. Creating Big Data reference architecture deliverable11. Performance optimization in a Big Data environmentGeneric Leadership Skills:1. Should have prior customer facing experience.2. Ability to lead all requirement gathering sessions with the Customer3. Strong co-ordination and interpersonal skills to handle complex projects.
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, Cloud Computing, Data Warehouse, etl, hadoop, hive, java, mapreduce, pig, RDBMS, Scala, spark, sql, storm
Location : Bangalore
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/big-data-engineer-bangalore-2-to-3-years-of-experience/
Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Engineer- Bangalore (2 to 3 years of experience)|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : Job Description and Responsibilities:A very strong statistical modeller with expertise in building statistical models from scratch, expertise in R, Python, Matlab, C++ and comfortable working with large data sets. Manage clients expectations and manage advanced analytics delivery. Work with client to understand the business problem and convert the same into analytical problem Hands on involvement in extracting, collating, performing data integrity checks, manipulating and analysing data Hands on involvement in suggesting and building an appropriate statistical model relevant to the industry vertical Develop analytical strategies for business using model results of predictive modelling, time series forecasting, clustering and segmentation of customers Documenting and presenting work to the client. Proactively recommending and influencing changes in business decision-making. Excellent verbal and written communication and form liason between business and technical architects and developers, Strong inclination towards consulting engagements involving machine learning and artificial intelligence. To work in agile methodology for project delivery Good foundation in data structures, software design, algorithms and natural language processing Required skills: B.Tech., Dual Degree M.Tech., or M.Sc. (Integrated) in Computer Science, Mathematics, Statistics, or similar quantitative disciplines from the Indian Institutes of Technology (IIT) or Indian Statistical Institute (ISI). Demonstrated ability to conduct independent research utilizing large datasets At least 1 year of experience in programming, quantitative analysis or as a data scientist Strong programming skills in one of the following: Python, C++, C# or Java
College Preference : tier1-any
Min Qualification : ug
Skills : c++, matlab, predictive modeling, python, r, statistical modeling
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-iitisi-mumbai-1-4-years-of-experience/
Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist -( IIT/ISI)- Mumbai (1-4 Years Of Experience)|Senior Manager/AVP  Analytics  (BFSI)- Chennai (9-14 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : Job Description and Responsibilities: 3-7 yrs. exp. in solving optimization problems using statistical, operations research, design of experiments approach.  Adept with applying Machine Learning and Data Analytics concepts for mostly structured data.  Expertise in building regression models, linear programming, mixed models, logistics models.  Programming exp. in R/ Python/ JAVA.  Individual Contributor role.Education:PhD in engg/statistics/economics/mathematics or any quantitative field. 
College Preference : no-bar
Min Qualification : pg
Skills : java, linear regression, logistic, machine learning, python, r, regression, statistical techniques, structured thinking
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-optimization-bangalore-3-7-years-of-experience/
Senior Manager/AVP  Analytics  (BFSI)- Chennai (9-14 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 9  14 years
Requirements : 
Task Info : Job Description and Responsibilities: To understand current critical MI reports & their requirements; develop and deliver such reports in Tableau BI tool  To ensure project milestones are met as per agreed book of work.  Identify and drive process enhancements to ensure efficient and seamless delivery.  Ensure that the standards of the process consistently meet or exceed the requirements set under the Service Level Agreements (SLA) agreed with the in-country team, identifying and implementing innovative opportunities to add value to the deliverables and improving the quality of deliverables.  Upholding the Values of the Group and Company at all times  Compliance with all applicable Rules/ Regulations and Company and Group Policies; and  Periodic review key controls and ensure compliance with operational risk policy framework QUALIFICATION & SKILLS:  Highly motivated individuals with a strong track record of achievement, especially in Finance area.  A team player and enjoys interacting with people of all levels in a multicultural environment.  Able to creatively apply analytical solutions to issues and pain areas.  Ability to manage pressure of tight deadlines and deliver high quality output.  Ability to challenge status quo. Key Skills / Knowledge  Good knowledge about the Tableau BI tool is mandatory.  Experience in working with reporting tools such as Pipeline, Hyperion Planning, Cube, RMI or PMI would be useful  Knowledge about the reporting process would be useful  Any experience with Tableau application is an added advantage Education : MBA / CA
College Preference : no-bar
Min Qualification : ug
Skills : analytics, banking, bfsi, business intelligence, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-manageravp-analytics-bfsi-chennai-9-14-years-of-experience-2/
Senior Manager/AVP  Analytics  (BFSI)- Chennai (9-14 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist ( Optimization )- Bangalore (3-7 Years Of Experience)|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 9  14 years
Requirements : 
Task Info : Job Description and Responsibilities: To understand current critical MI reports & their requirements; develop and deliver such reports in Tableau BI tool  To ensure project milestones are met as per agreed book of work.  Identify and drive process enhancements to ensure efficient and seamless delivery.  Ensure that the standards of the process consistently meet or exceed the requirements set under the Service Level Agreements (SLA) agreed with the in-country team, identifying and implementing innovative opportunities to add value to the deliverables and improving the quality of deliverables.  Upholding the Values of the Group and Company at all times  Compliance with all applicable Rules/ Regulations and Company and Group Policies; and  Periodic review key controls and ensure compliance with operational risk policy framework QUALIFICATION & SKILLS:  Highly motivated individuals with a strong track record of achievement, especially in Finance area.  A team player and enjoys interacting with people of all levels in a multicultural environment.  Able to creatively apply analytical solutions to issues and pain areas.  Ability to manage pressure of tight deadlines and deliver high quality output.  Ability to challenge status quo. Key Skills / Knowledge  Good knowledge about the Tableau BI tool is mandatory.  Experience in working with reporting tools such as Pipeline, Hyperion Planning, Cube, RMI or PMI would be useful  Knowledge about the reporting process would be useful  Any experience with Tableau application is an added advantage Education : MBA / CA
College Preference : no-bar
Min Qualification : ug
Skills : analytics, banking, bfsi, business intelligence, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-manageravp-analytics-bfsi-chennai-9-14-years-of-experience/
"A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R",Learn everything about Analytics|Introduction|A small exercise to get your mind racing|Table of Contents|1. Simple models for Prediction|2. Linear Regression|3. The Line of Best Fit|4. Gradient Descent|5. Using Linear Regression for Prediction|6. Evaluating your Model  R square and adjusted R- square|7. Using all the features for prediction|8. Polynomial Regression|9. Bias and Variance in regression models|10. Regularization|11. Ridge Regression|12. Lasso regression|13. Elastic Net Regression|Implementation in R|||14. Types of Regularization Techniques [Optional]|End Notes,"Model 1  Mean sales:||Model 2  Average Sales by Location:|Model 3  Enter Linear Regression:||Model 4  Linear regression with more variables|Adjusted R-square|Data pre-processing steps for regression model|Building the model|Selecting the right features for your model|Interpretation of Regression Plots|Important Points:|Important Points:|Share this:|Like this:|Related Articles|Senior Manager/AVP  Analytics  (BFSI)- Chennai (9-14 Years Of Experience)|How to create animated GIF images for data visualization using gganimate (in R)?|
Shubham Jain
|68 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I was talking to one of my friends who happen to be an operations manager at one of the Supermarket chains in India. Over our discussion, we started talking about the amount of preparation the store chain needs to do before the Indian festive season (Diwali) kicks in.He told me how critical it is for them to estimate/predict which product will sell like hotcakes and which would not prior to the purchase. A bad decision can leave your customers to look for offers and products in the competitor stores. The challenge does not finish there  you need to estimate the sales of products across a range of different categories for stores in varied locations and with consumers having different consumption techniques.While my friend was describing the challenge, the data scientist in me started smiling! Why? I just figured out a potential topic for my next article. In todays article, I will tell you everything you need to know about regression models and how they can be used to solve prediction problems like the one mentioned above.Take a moment to list down all those factors you can think, on which the sales of a store will be dependent on. For each factor create an hypothesis about why and how that factor would influence the sales of various products. For example  I expect the sales of products to depend on the location of the store, because the local residents in each area would have different lifestyle. The amount of bread a store will sell in Ahmedabad would be a fraction of similar store in Mumbai.Similarly list down all possible factors you can think of.Location of your shop, availability of the products, size of the shop, offers on the product, advertising done by a product, placement in the storecould be some features on which your sales would depend on.How many factors were you able to think of? If it is less than 15, give it more time and think again! A seasoned data scientist working on this problem would possibly think of tens and hundreds of such factors.With that thought in mind, I am providing you with one such data set  The Big Mart Sales. In the data set, we have product wise Sales for Multiple outlets of a chain.Lets us take a snapshot of the dataset:In the dataset, we can see characteristics of the sold item (fat content, visibility, type, price) and some characteristics of the outlet (year of establishment, size, location, type) and the number of the items sold for that particular item. Lets see if we can predict sales using these features.Let us startwith making predictions using a few simple ways to start with. If I were to ask you, what could be the simplest way to predict the sales of an item, what would you say?Even without any knowledge of machine learning, you can say that if you have to predict sales for an item  it would be the average over last few days . / months / weeks.It is a good thought to start, but it also raises a question  how good is that model?Turns out that there are various ways in which we can evaluate how good is our model. The most common way is Mean Squared Error. Let us understand how to measure it.Prediction errorTo evaluate how good is a model, let us understand the impact of wrong predictions. If we predict sales to be higher than what they might be, the store willspend a lot of money making unnecessary arrangement which would lead to excess inventory. On the other side if I predict it too low, I will lose out on sales opportunity.So, the simplest way of calculating error will be, to calculate the difference in the predicted and actual values. However, if we simply add them, they might cancel out, so we square these errors before adding. We also divide them by the number of data points to calculate a mean error since it should not be dependent on number of data points.[each error squared and divided by number of data points]
This is known as the mean squared error.Here e1, e2 . , en are the difference between the actual and the predicted values.So, in our first model what would be the mean squared error? On predicting the mean for all the data points, we get a mean squared error = 29,11,799. Looks like huge error. May be its not so cool to simply predict the average value.Lets see if we can think of something to reduce the error. Here is a live coding window to predict target using mean.We know that location plays a vital role in the sales of an item. For example, let us say, sales of car would be much higher in Delhi than its sales in Varanasi. Therefore let us use the data of the column Outlet_Location_Type.So basically, let us calculate the average sales for each location type and predict accordingly.On predicting the same, we get mse = 28,75,386, which is less than our previous case. So we can notice that by using a characteristic[location], we have reduced the error.Now, what if there are multiple features on which the sales would depend on. How would we predict sales using this information? Linear regression comes to our rescue.Linear regression is the simplest and most widely used statistical technique for predictive modeling. It basically gives us an equation, where we have our features as independent variables, on which our target variable [sales in our case] is dependent upon.So what does the equation look like? Linear regression equation looks like this:Here, we have Y as our dependent variable (Sales), Xs are the independent variables and all thetas are the coefficients. Coefficients are basically the weights assigned to the features, based on their importance. For example, if we believe that sales of an item would have higher dependency upon the type of location as compared to size of store, it means that sales in a tier 1 city would be more even if it is a smaller outlet than a tier 3 city in a bigger outlet. Therefore, coefficient of location type would be more than that of store size.So, firstly let us try to understand linear regression with only one feature, i.e., only one independent variable. Therefore our equation becomes,This equation is called a simple linear regression equation, which represents a straight line, where 0 is the intercept, 1 is the slope of the line. Take a look at the plot below between sales and MRP.Surprisingly, we can see that sales of a product increases with increase in its MRP. Therefore the dotted red line represents our regression line or the line of best fit. But one question that arises is how you would find out this line?As you can see below there can be so many lines which can be used to estimate Sales according to their MRP. So how would you choose the best fit line or the regression line?The main purpose of the best fit line is that our predicted values should be closer to our actual or the observed values, because there is no point in predicting values which are far away from the real values. In other words, we tend to minimize the difference between the values predicted by us and the observed values, and which is actually termed as error. Graphical representation of error is as shown below. These errors are also called as residuals.The residuals are indicated by the vertical lines showing the difference between the predicted and actual value.Okay, now we know that our main objective is to find out the error and minimize it. But before that, lets think of how to deal with the first part, that is, to calculate the error. We already know that error is the difference between the value predicted by us and the observed value. Lets just consider three ways through which we can calculate error:Therefore, sum of squares of these residuals is denoted by:where, h(x) is the value predicted by us, h(x) =1*x +0 , y is the actual values and m is the number of rows in the training set.The cost FunctionSo lets say, you increased the size of a particular shop, where you predicted that the sales would be higher. But despite increasing the size, the sales in that shop did not increase that much. So the cost applied in increasing the size of the shop, gave you negative results.So, we need to minimize these costs. Therefore we introduce a cost function, which is basically used to define and measure the error of the model.If you look at this equation carefully, it is just similar to sum of squared errors, with just a factor of 1/2m is multiplied in order to ease mathematics.So in order to improve our prediction, we need to minimize the cost function. For this purpose we use the gradient descent algorithm. So let us understand how it works.Let us consider an example, we need to find the minimum value of this equation,Y= 5x + 4x^2. In mathematics, we simple take the derivative of this equation with respect to x, simply equate it to zero. This gives us the point where this equation is minimum. Therefore substituting that value can give us the minimum value of that equation.Gradient descent works in a similar manner. It iteratively updates , to find a point where the cost function would be minimum. If you wish to study gradient descent in depth, I would highly recommend going through this article.Now let us consider using Linear Regression to predict Sales for our big mart sales problem.From the previous case, we know that by using the right features would improve our accuracy. So now let us use two features, MRP and the store establishment year to estimate sales.Now, let us built a linear regression model in python considering only these two features.# importing basic librariesimport numpy as np import pandas as pdfrom pandas import Series, DataFramefrom sklearn.model_selection import train_test_splitimport test and train filetrain = pd.read_csv('Train.csv')test = pd.read_csv('test.csv')# importing linear regressionfrom sklearnfrom sklearn.linear_model import LinearRegressionlreg = LinearRegression()splitting into training and cv for cross validationX = train.loc[:,['Outlet_Establishment_Year','Item_MRP']]x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales)training the modellreg.fit(x_train,y_train)predicting on cvpred = lreg.predict(x_cv)calculating msemse = np.mean((pred - y_cv)**2)In this case, we got mse = 19,10,586.53, which is much smaller than our model 2. Therefore predicting with the help of two features is much more accurate.Let us take a look at the coefficients of this linear regression model.# calculating coefficients coeff = DataFrame(x_train.columns) coeff['Coefficient Estimate'] = Series(lreg.coef_)coeffTherefore, we can see that MRP has a high coefficient, meaning items having higher prices have better sales.How accurate do you think the model is? Do we have any evaluation metric, so that we can check this? Actually we have a quantity, known as R-Square.R-Square: It determines how much of the total variation in Y (dependent variable) is explained by the variation in X (independent variable). Mathematically, it can be written as:The value of R-square is always between 0 and 1, where 0 means that the model does not model explain any variability in the target variable (Y) and 1 meaning it explains full variability in the target variable.Now let us check the r-square for the above model.lreg.score(x_cv,y_cv) 0.3287In this case, R is 32%, meaning, only 32% of variance in sales is explained by year of establishment and MRP. In other words, if you know year of establishment and the MRP, youll have 32% information to make an accurate prediction about its sales.Now what would happen if I introduce one more feature in my model, will my model predict values more closely to its actual value? Will the value of R-Square increase?Let us consider another case.We learnt, by using two variables rather than one, we improved the ability to make accurate predictions about the item sales.So, let us introduce another feature weight in case 3. Now lets build a regression model with these three features.X = train.loc[:,['Outlet_Establishment_Year','Item_MRP','Item_Weight']]splitting into training and cv for cross validationx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales) ## training the modellreg.fit(x_train,y_train)ValueError: Input contains NaN, infinity or a value too large for dtype(float64).It produces an error, because item weights column have some missing values. So let us impute it with the mean of other non-null entries.train['Item_Weight'].fillna((train['Item_Weight'].mean()), inplace=True)Let us try to run the model again.training the model lreg.fit(x_train,y_train) ## splitting into training and cv for cross validationx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales) ## training the model lreg.fit(x_train,y_train)predicting on cv pred = lreg.predict(x_cv)calculating msemse = np.mean((pred - y_cv)**2)mse1853431.59 ## calculating coefficientscoeff = DataFrame(x_train.columns)coeff['Coefficient Estimate'] = Series(lreg.coef_)calculating r-squarelreg.score(x_cv,y_cv) 0.32942Therefore we can see that the mse is further reduced. There is an increase in the value R-square, does it mean that the addition of item weight is useful for our model?The only drawback of R2 is that if new predictors (X) are added to our model, R2 only increases or remains constant but it never decreases. We can not judge that by increasing complexity of our model, are we making it more accurate?That is why, we use Adjusted R-Square.The Adjusted R-Square is the modified form of R-Square that has been adjusted for the number of predictors in the model. It incorporates models degree of freedom. The adjusted R-Square only increases if the new term improves the model accuracy.whereR2 = Sample R squarep = Number of predictorsN = total sample sizeNow let us built a model containing all the features. While building the regression models,I have only used continuous features. This is because we need to treat categorical variables differently before they can used in linear regression model. There are different techniques to treat them, here I have used one hot encoding(convert each class of a categorical variable as a feature). Other than that I have also imputed the missing values for outlet size.# imputing missing valuestrain['Item_Visibility'] = train['Item_Visibility'].replace(0,np.mean(train['Item_Visibility']))train['Outlet_Establishment_Year'] = 2013 - train['Outlet_Establishment_Year']train['Outlet_Size'].fillna('Small',inplace=True) # creating dummy variables to convert categorical into numeric valuesmylist = list(train1.select_dtypes(include=['object']).columns)dummies = pd.get_dummies(train[mylist], prefix= mylist)train.drop(mylist, axis=1, inplace = True) X = pd.concat([train,dummies], axis =1 )import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib.pyplot as plt%matplotlib inlinetrain = pd.read_csv('training.csv')test = pd.read_csv('testing.csv')# importing linear regression from sklearn from sklearn.linear_model import LinearRegressionlreg = LinearRegression()# for cross validationfrom sklearn.model_selection import train_test_splitX = train.drop('Item_Outlet_Sales',1)x_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales, test_size =0.3)# training a linear regression model on trainlreg.fit(x_train,y_train) # predicting on cvpred_cv = lreg.predict(x_cv)# calculating msemse = np.mean((pred_cv - y_cv)**2)mse1348171.96# evaluation using r-squarelreg.score(x_cv,y_cv)0.54831541460870059Clearly, we can see that there is a great improvement in both mse and R-square, which means that our model now is able to predict much closer values to the actual values.When we have a high dimensional data set, it would be highly inefficient to use all the variables since some of them might be imparting redundant information. We would need to select the right set of variables which give us an accurate model as well as are able to explain the dependent variable well. There are multiple ways to select the right set of variables for the model. First among them would be the business understanding and domain knowledge. For instance while predicting sales we know that marketing efforts should impact positively towards sales and is an important feature in your model. We should also take care that the variables were selecting should not be correlated among themselves.Instead of manually selecting the variables, we can automate this process by using forward or backward selection.Forward selection starts with most significant predictor in the model and adds variable for each step.Backward elimination starts with all predictors in the model and removes the least significant variable for each step. Selecting criteria can be set to any statistical measure like R-square, t-stat etc.Take a look at the residual vs fitted values plot.residual plotx_plot = plt.scatter(pred_cv, (pred_cv - y_cv), c='b')plt.hlines(y=0, xmin= -1000, xmax=5000)plt.title('Residual plot')We can see a funnel like shape in the plot. This shape indicates Heteroskedasticity.The presenceof non-constant variance in the error terms results in heteroskedasticity. We can clearly see that the variance of error terms(residuals) is not constant. Generally, non-constant variance arises in presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the models performance.When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow.We can easily check this by looking at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern as shown above. This indicates signs of non linearity in the data which has not been captured by the model.I would highly recommend going through this article for a detailed understanding of assumptions and interpretation of regression plots.In order to capture this non-linear effects, we have another type of regression known as polynomial regression. So let us now understand it.Polynomial regression is another form of regression in which the maximum power of the independent variable is more than 1. In this regression technique, the best fit line is not a straight line instead it is in the form of a curve.Quadratic regression, or regression with second order polynomial, is given by the following equation:Y =1 +2*x +3*x2Now take a look at the plot given below.Clearly the quadratic equation fits the data better than simple linear equation. In this case, what do you think will the R-square value of quadratic regression greater than simple linear regression? Definitely yes, because quadratic regression fits the data better than linear regression. While quadratic and cubic polynomials are common, but you can also add higher degree polynomials.Below figure shows the behavior of a polynomial equation of degree 6.So do you think its always better to use higher order polynomials to fit the data set. Sadly, no. Basically, we have created a model that fits our training data well but fails to estimate the real relationship among variables beyond the training set. Therefore our model performs poorly on the test data. This problem is called as over-fitting. We also say that the model has high variance and low bias.Similarly, we have another problem called underfitting, it occurs when our model neither fits the training data nor generalizes on the new data.Our model is underfit when we have high bias and low variance.What does that bias and variance actually mean? Let us understand this by an example of archery targets.Lets say we have model which is very accurate, therefore the error of our model will be low, meaning a low bias and low variance as shown in first figure. All the data points fit within the bulls-eye. Similarly we can say that if the variance increases, the spread of our data point increases which results in less accurate prediction. And as the bias increases the error between our predicted value and the observed values increases.Now how this bias and variance is balanced to have a perfect model? Take a look at the image below and try to understand.As we add more and more parameters to our model, its complexity increases, which results in increasing variance and decreasing bias, i.e., overfitting. So we need to find out one optimum point in our model where the decrease in bias is equal to increase in variance. In practice, there is no analytical way to find this point. So how to deal with high variance or high bias?To overcome underfitting or high bias, we can basically add new parameters to our model so that the model complexity increases, and thus reducing high bias.Now, how can we overcome Overfitting for a regression model?Basically there are two methods to overcome overfitting,Here we would be discussing about Regularization in detail and how to use it to make your model more generalized.You have your model ready, you have predicted your output. So why do you need to study regularization? Is it necessary?Suppose you have taken part in a competition, and in that problem you need to predict a continuous variable. So you applied linear regression and predicted your output. Voila! You are on the leaderboard. But wait what you see is still there are many people above you on the leaderboard. But you did everything right then how is it possible?Everything should be made simple as possible, but not simpler  Albert EinsteinWhat we did was simpler, everybody else did that, now let us look at making it simple. That is why, we will try to optimize our code with the help of regularization.In regularization, what we do is normally we keep the same number of features, but reduce the magnitude of the coefficients j. How does reducing the coefficients will help us?Let us take a look at the coefficients of feature in our above regression model.checking the magnitude of coefficientspredictors = x_train.columns coef = Series(lreg.coef_,predictors).sort_values()coef.plot(kind='bar', title='Modal Coefficients')We can see that coefficients of Outlet_Identifier_OUT027 and Outlet_Type_Supermarket_Type3(last 2) is much higher as compared to rest of the coefficients. Therefore the total sales of an item would be more driven by these two features.How can we reduce the magnitude of coefficients in our model? For this purpose, we have different types of regression techniques which uses regularization to overcome this problem. So let us discuss them.Let us first implement it on our above problem and check our results that whether it performs better than our linear regression model.from sklearn.linear_model import Ridge ## training the modelridgeReg = Ridge(alpha=0.05, normalize=True)ridgeReg.fit(x_train,y_train)pred = ridgeReg.predict(x_cv)calculating msemse = np.mean((pred_cv - y_cv)**2)mse 1348171.96 ## calculating score ridgeReg.score(x_cv,y_cv) 0.5691So, we can see that there is a slight improvement in our model because the value of the R-Square has been increased. Note that value of alpha, which is hyperparameter of Ridge, which means that they are not automatically learned by the model instead they have to be set manually.Here we have consider alpha = 0.05. But let us consider different values of alpha and plot the coefficients for each case.   You can see that, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero.But if you calculate R-square for each alpha, we will see that the value of R-square will be maximum at alpha=0.05. So we have to choose it wisely by iterating it through a range of values and using the one which gives us lowest error.So, now you have an idea how to implement it but let us take a look at the mathematics side also. Till now our idea was to basically minimize the cost function, such that values predicted are much closer to the desired result.Now take a look back again at the cost function for ridge regression.Here if you notice, we come across an extra term, which is known as the penalty term.  given here, is actually denoted by alpha parameter in the ridge function. So by changing the values of alpha, we are basically controlling the penalty term. Higher the values of alpha, bigger is the penalty and therefore the magnitude of coefficients are reduced.Now let us consider another type of regression technique which also makes use of regularization.LASSO (Least Absolute Shrinkage Selector Operator), is quite similar to ridge, but lets understand the difference them by implementing it in our big mart problem.from sklearn.linear_model import LassolassoReg = Lasso(alpha=0.3, normalize=True)lassoReg.fit(x_train,y_train)pred = lassoReg.predict(x_cv) # calculating msemse = np.mean((pred_cv - y_cv)**2) mse1346205.82lassoReg.score(x_cv,y_cv)0.5720As we can see that, both the mse and the value of R-square for our model has been increased. Therefore, lasso model is predicting better than both linear and ridge.Again lets change the value of alpha and see how does it affect the coefficients.So, we can see that even at small values of alpha, the magnitude of coefficients have reduced a lot. By looking at the plots, can you figure a difference between ridge and lasso?We can see that as we increased the value of alpha, coefficients were approaching towards zero, but if you see in case of lasso, even at smaller alphas, our coefficients are reducing to absolute zeroes. Therefore, lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge.Mathematics behind lasso regression is quiet similar to that of ridge only difference being instead of adding squares of theta, we will add absolute value of .Here too,  is the hypermeter, whose value is equal to the alpha in the Lasso function.Now that you have a basic understanding of ridge and lasso regression, lets think of an example where we have a large dataset, lets say it has 10,000 features. And we know that some of the independent features are correlated with other independent features. Then think, which regression would you use, Rigde or Lasso?Lets discuss it one by one. If we apply ridge regression to it, it will retain all of the features but will shrink the coefficients. But the problem is that model will still remain complex as there are 10,000 features, thus may lead to poor model performance.Instead of ridge what if we apply lasso regression to this problem. The main problem with lasso regression is when we have correlated variables, it retains only one variable and sets other correlated variables to zero. That will possibly lead to some loss of information resulting in lower accuracy in our model.Then what is the solution for this problem? Actually we have another type of regression, known as elastic net regression, which is basically a hybrid of ridge and lasso regression. So lets try to understand it.Before going into the theory part, let us implement this too in big mart sales problem. Will it perform better than ridge and lasso? Lets check!from sklearn.linear_model import ElasticNetENreg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)ENreg.fit(x_train,y_train)pred_cv = ENreg.predict(x_cv) #calculating msemse = np.mean((pred_cv - y_cv)**2)mse 1773750.73ENreg.score(x_cv,y_cv)0.4504So we get the value of R-Square, which is very less than both ridge and lasso. Can you think why? The reason behind this downfall is basically we didnt have a large set of features. Elastic regression generally works well when we have a big dataset.Note, here we had two parameters alpha and l1_ratio. First lets discuss, what happens in elastic net, and how it is different from ridge and lasso.Elastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters. So it uses both L1 and L2 penality term, therefore its equation look like as follows:So how do we adjust the lambdas in order to control the L1 and L2 penalty term?Let us understand by an example. You are trying to catch a fish from a pond. And you only have a net, then what would you do? Will you randomly throw your net? No, you will actually wait until you see one fish swimming around, then you would throw the net in that direction to basically collect the entire group of fishes. Therefore even if they are correlated, we still want to look at their entire group.Elastic regression works in a similar way. Let say, we have a bunch of correlated independent variables in a dataset, then elastic net will simply form a group consisting of these correlated variables. Now if any one of the variable of this group is a strong predictor (meaning having a strong relationship with dependent variable), then we will include the entire group in the model building, because omitting other variables (like what we did in lasso) might result in losing some information in terms of interpretation ability, leading to a poor model performance.So, if you look at the code above, we need to define alpha and l1_ratio while defining the model. Alpha and l1_ratio are the parameters which you can set accordingly if you wish to control the L1 and L2 penalty separately. Actually, we haveAlpha = a + b  and  l1_ratio = a / (a+b)where, a and b weights assigned to L1 and L2 term respectively. So when we change the values of alpha and l1_ratio, a and b are set aaccordingly such that they control trade off between L1 and L2 as:a * (L1 term) + b* (L2 term)Let alpha (or a+b) = 1, and now consider the following cases:So let us adjust alpha and l1_ratio, and try to understand from the plots of coefficient given below. Now, you have basic understanding about ridge, lasso and elasticnet regression. But during this, we came across two terms L1 and L2, which are basically two types of regularization. To sum up basically lasso and ridge are the direct application of L1 and L2 regularization respectively.But if you still want to know, below I have explained the concept behind them, which is OPTIONAL but before that let us see the same implementation of above codes in R.Step 1: Linear regression with two variables Item MRP and Item Establishment Year.OutputAlso, the value of r square is 0.3354391 and the MSE is 20,28,538.Step 2: Linear regression with three variables Item MRP, Item Establishment Year, Item Weight.OutputAlso, the value of r square is0.3354657 and the MSE is 20,28,692.Step 3: Linear regression with all variables.OutputAlso, the value of r square is 0.3354657 and the MSE is 14,38,692.Step 4: Implementation of Ridge regressionOutputStep 5: Implementation of lasso regressionOutputFor better understanding and more clarity on all the three types of regression, you can refer to this Free Course: Big Mart Sales In R.Lets recall, both in ridge and lasso we added a penalty term, but the term was different in both cases. In ridge, we used the squares of theta while in lasso we used absolute value of theta. So why these two only, cant there be other possibilities?Actually, there are different possible choices of regularization with different choices of order of the parameter in the regularization term, which is denoted by. This is more generally known as Lp regularizer.Let us try to visualize some by plotting them. For making visualization easy, let us plot them in 2D space. For that we suppose that we just have two parameters. Now, lets say if p=1, we have term as . Cant we plot this equation of line? Similarly plot for different values of p are given below.In the above plots, axis denote the parameters(1 and2). Let us examine them one by one.For p=0.5, we can only get large values of one parameter only if other parameter is too small. For p=1, we get sum of absolute values where the increase in one parameter is exactly offset by the decrease in other. For p =2, we get a circle and for larger p values, it approaches a round square shape.The two most commonly used regularization are in which we have p=1 and p=2, more commonly known as L1 and L2 regularization.Look at the figure given below carefully. The blue shape refers the regularization term and other shape present refers to our least square error (or data term).The first figure is for L1 and the second one is for L2 regularization. The black point denotes that the least square error is minimized at that point and as we can see that it increases quadratically as we move from it and the regularization term is minimized at the origin where all the parameters are zero .Now the question is that at what point will our cost function be minimum? The answer will be, since they are quadratically increasing, the sum of both the terms will be minimized at the point where they first intersect.Take a look at the L2 regularization curve. Since the shape formed by L2 regularizer is a circle, it increases quadratically as we move away from it. The L2 optimum(which is basically the intersection point) can fall on the axis lines only when the minimum MSE (mean square error or the black point in the figure) is also exactly on the axis. But in case of L1, the L1 optimum can be on the axis line because its contour is sharp and therefore there are high chances of interaction point to fall on axis. Therefore it is possible to intersect on the axis line, even when minimum MSE is not on the axis. If the intersection point falls on the axes it is known as sparse.Therefore L1 offers some level of sparsity which makes our model more efficient to store and compute and it can also help in checking importance of feature, since the features that are not important can be exactly set to zero.I hope now you understand the science behind the linear regression and how to implement it and optimize it further to improve your model.Knowledge is the treasure and practice is the key to itTherefore, get your hands dirty by solving some problems. You can also start with the Big mart sales problem and try to improve your model with some feature engineering. If you face any difficulties while implementing it, feel free to write on our discussion portal.Did you find this article helpful? Please share your opinions / thoughts in the comments section below.",https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/
How to create animated GIF images for data visualization using gganimate (in R)?|,"Learn everything about Analytics|Introduction|Example  GDP vs. Life expectancy over time|Pre-requisites|Get the Data|Data Manipulation|R Codes|Split the Date into year, month and date|Speed up projection in .gif using animation package|Conclusion","Earthquake magnitude of 7 points on Richter Scale from 1965-2016|Share this:|Like this:|Related Articles|A comprehensive beginners guide for Linear, Ridge and Lasso Regression in Python and R|Director/ Head  Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience)|
Guest Blog
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data visualization is probably the most important and typically the least talked about area of data science.I say that because how you create data stories and visualization has a huge impact on howyour customers look at your work. Ultimately, data science is not only about how complicated and sophisticated your models are. It is about solving problems using data based insights. And in order to implement these solutions, your stakeholders need to understand what you are proposing.One of the challenges in creating effective visualizations is to create images which speak for themselves. This article will tell one of the ways to do so using animated GIF images (Graphics Interchangeable format). This would be particularly helpful when you want to show time / flow based stories. Using animation in images, you canplot comparable data over time for specific set of parameters. In other words, it is easy to understand and see the growth of certain parameter over time.Let me show this with an exampleLet us say you want to show how GDP and life expectancy have changed for various continents / countries over time. What do you think is the best way to represent this relationship?You can think of multiple options like:Now, let us look at this using an animated plot using.gif file:The recent development of gganimate package had made this possible and easier. By the end of this article, you will be able to make your own .gif file and create your own customised frame to compare different parameters on global or local scale.Please install the following packages:In addition to the above libraries in R, you will also need Image Magick Software in your system. You may download and install the same from Image MagickThis article is an attempt to make .gif file on earthquake data from 1965-2016. It is better to plot year wise global seismic activity rather than a static look of all the values on the map. The data set for earthquake is available on Kaggle.
The data set contains data for global seismic activity from 1965 to 2016. Please visit the above link and scroll down to get the .csv file.The dataset had been modified and only seismic value of 7 points on richter scale has been considered for the study.From the .csv file we have only selected few parameters for the sake of simplicity.We are all set to start coding in R. I have usedRStudio environment. You are free to use any environment you prefer.This is done in order to get the frame which is important for the plot. In other words, The core of the approach is to treat frame
(as in, the time point within an animation) as another dimension, just like x, y, size, color, or so on. Thus, a variable in your data can be mapped to frame just as others are mapped to x or y.EarthquakeAs we can see that plot has too many years from 1965 to 2016. Thus, in order to speed up the visualization, we can use the animation package to fast forward using ani.option()Earthquake  1.5x speedThis article was an introductory tutorial to the world of animated map. Readers can try this and apply the same in other projects. Some of the example are,Hope you found the article useful. If you have any questions, please feel free to ask in comments below.Aritra Chatterjee is a professional in the field of Data Science and Operation Management having experience of more than 5 years. He aspires to develop skill in the field of Automation, Data Science and Machine Learning.This post was received as part of our blogging competition  The Mightiest Pen. Check out other competitions here.",https://www.analyticsvidhya.com/blog/2017/06/a-study-on-global-seismic-activity-between-1965-and-2016/
Director/ Head  Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|How to create animated GIF images for data visualization using gganimate (in R)?|Director/ VP (Analytics)  Gurugram (8-14 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  10 years
Requirements : 
Task Info : About the Company:We are the leading A+/A1+ rated non-banking finance company providing the crucial link between debt capital markets and high quality originators who reach the emerging consumer andbusiness owner. Using its deep experience, unrivalled data, proven and proprietary risk management processes and innovative structured finance techniques we continues to deliver superior risk-adjusted returns to a growing client base of Indian and international investors keen to tap into a growing market opportunity.We have employed talented capital market professionalswith extensive experience to helpit deliver this proposition. By allowing lenders in remote areas of India to increase the volume andlower the cost of borrowing for low-income and financially excluded families and businesses, ouractivities are now benefiting some 15 million individuals. The company also helps its clientsbuild better operating and oversight systems and to implement customer protection principles, thusimproving the quality of products and services that end borrowers receive.Till date, we havecompleted over 300 rated capital market transactions and raised over USD 3 billion in financing forits clients with a zero delinquency track record.About the positionWe are looking to hire an experienced, innovative, passionate and knowledgeable candidates Head  Risk Analytics and Modelling to lead a high performance team and deliver data-driven business-decision oriented risk analytics and modelling. The team uses borrower and loanperformance data from retail and wholesale plain vanilla and structured finance portfolios across multiple asset classes  including but not limited to- microfinance, small business loan, housing finance, vehicle finance and agriculture-allied finance as well as corporate finance sector. Such analytics and modelling is done at a transaction/portfolio level as well as for ours portfolio at an aggregate level.Key Result Areas Managing and further building a high performance risk analytics and modelling team focused on innovation to deliver relevant business-decision oriented risk analytics and reporting Enhancing the existing framework and evolving the estimation of economic capital and value-at risk for taking into account various risks and managing this proactively as the portfolio and business evolves Evolving a robust stress testing framework and continuously developing tools and perform stress testing at pool, transaction, and portfolio level to estimate the worst case losses and assess possible risk mitigation (such as credit enhancements) Further build the capability of the team in portfolio analytics and to analyse the granular loan level performance data to get insights into the borrower credit behaviour and share findings in the form of reports, views and special maps with internal and external teams Manage portfolio analytics and other analytics advisory engagements and ensure timely execution and high quality deliverable for our partners, regulators and other entities to create a differentiated market position for us Design and drive research on questions related to risk management and financial access for low income households and enterprises based on performance data of own portfolio, data subscribed from credit bureaus and other sources to consolidate our position thoughtleadership in these sectorsKey ResponsibilitiesPortfolio Risk ManagementTransaction Management Oversee the team:Data AnalyticsRisk Mitigation  Product DevelopmentEssential Skills and ExperienceThe successful candidate1. Must be passionate about managing, mentoring and developing a high performance riskanalytics and modelling team, while possessing the personal qualities and skills to fosteran innovative, forward looking, collaborative and cohesive team culture2. Will hold a Doctoral or Masters degree in a quantitative field such as statistics,econometrics, economics, mathematical finance, finance, or applied science such asphysics
College Preference : no-bar
Min Qualification : phd
Skills : banking, bfsi, machine learning, matlab, python, r, Risk Analytics, sql, statistics
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/director-head-risk-analytics-modelling-chennai-7-to-9-years-of-experience/
Director/ VP (Analytics)  Gurugram (8-14 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Director/ Head  Risk Analytics & Modelling- Chennai (7 to 9 Years of Experience)|Introductory guide to Generative Adversarial Networks (GANs) and their promise!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  14 years
Requirements : 
Task Info : We are looking to hire a person who can lead our analytics strategy. This is a key role as the person would be defining the analytics road map for the organization.This would include:
College Preference : tier1-entire
Min Qualification : pg
Skills : bfsi, machine learning, predictive modeling, Risk Analytics
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/director-vp-analytics-gurugram-8-14-years-of-experience/
Introductory guide to Generative Adversarial Networks (GANs) and their promise!,"Learn everything about Analytics|Introduction|Excuse me, but what is a GAN?|How do GANs work?|Challenges with GANs|Implementing a Toy GAN|Applications of GAN|Resources|End Notes","|Another analogy from real life||Parts of training GAN||Steps to train a GAN|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Director/ VP (Analytics)  Gurugram (8-14 Years of Experience)|Data Scientist  Bangalore (4-8 Years Of Experience)|
Faizan Shaikh
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural Networks have made great progress. They nowrecognize images and voice at levels comparable to humans. They are also able to understand natural language with a good accuracy.But, even then, the talk of automating human tasks with machines looks a bit far fetched. After all, we do much more than just recognizing image / voice or understanding what people around us are saying  dont we?Let us see a few examples where we need human creativity (at least as of now):Do you think, these tasks can be accomplished by machines? Well  the answer might surprise you These are definitely difficult to automate tasks, butGenerative Adversarial Networks (GANs) have started making some of these tasks possible.If you feel intimidated by the name GAN  dont worry! You will feel comfortable with them by end of this article.In this article, I will introduce you to the concept of GANs and explain how they work along with the challenges. I will also let you know of some cool things people have done using GAN and give you links to some of the important resources for getting deeper into these techniques.Yann LeCun, a prominent figure in Deep Learning Domain said in his Quora session that(GANs), and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.Surely he has a point. When I saw the implicationsGenerative Adversarial Networks (GANs) can have if they were executed to their fullest extent, I was impressed too.But what is a GAN?Let us take an analogy to explain the concept:If you want to get better at something, say chess; what would you do? You would compete with an opponent better than you. Then you would analyze what you did wrong, what he / she did right, and think on what could you do to beat him / her in the next game.You would repeat this step until you defeat the opponent. This concept can be incorporated to build better models. So simply, for getting a powerful hero (viz generator), we need a more powerful opponent (viz discriminator)!A slightly more real analogy can be considered as a relation between forger and an investigator.The task of a forger is to create fraudulent imitations of original paintings by famous artists. If this created piece can pass as the original one, the forger gets a lot of money in exchange of the piece.On the other hand, an art investigators task is to catch these forgers who create the fraudulent pieces. How does he do it? He knows what are the properties which sets the original artist apart and what kind of painting he should have created. He evaluates this knowledge with the piece in hand to check if it is real or not.This contest of forger vs investigator goes on, which ultimately makes world class investigators (and unfortunately world class forger); a battle between good and evil.We got a high level overview of GANs. Now, we will go on to understand their nitty-gritty of these things.As we saw, there are two main components of a GAN  Generator Neural Network and Discriminator Neural Network.The Generator Network takes an random input and tries to generate a sample of data. In the above image, we can see that generatorG(z) takes a input z from p(z), where z is a sample from probability distribution p(z). It then generates a data which is then fed into a discriminator network D(x). The task of Discriminator Network is to take input either from the real data or from the generator and try to predict whether the input is real or generated. It takes an input x from pdata(x) where pdata(x) is our real data distribution. D(x) then solves a binary classification problem using sigmoid function giving output in the range 0 to 1.Let us define the notations we will be using to formalize our GAN,Pdata(x) -> the distribution of real data
X -> sample from pdata(x)
P(z) -> distribution of generator
Z -> sample from p(z)
G(z) -> Generator Network
D(x) -> Discriminator NetworkNow the training of GAN is done (as we saw above) as a fight between generator and discriminator. This can be represented mathematically asIn our function V(D, G) the first term is entropy that the data from real distribution (pdata(x)) passes through the discriminator (aka best case scenario). The discriminator tries to maximize this to 1. The second term is entropy that the data from random input (p(z)) passes through the generator, which then generates a fake sample which is then passed through the discriminator to identify the fakeness (aka worst case scenario). In this term, discriminator tries to maximize it to 0 (i.e. the log probability that the data from generated is fake is equal to 0). So overall, the discriminator is trying to maximize our function V.On the other hand, the task of generator is exactly opposite, i.e. it tries to minimize the function V so that the differentiation between real and fake data is bare minimum. This, in other words is a cat and mouse game between generator and discriminator!Note: This method of training a GAN is taken from game theory called the minimax game.So broadly a training phase has two main subparts and they are done sequentiallyStep 1: Define the problem. Do you want to generate fake images or fake text. Here you should completely define the problem and collect data for it.Step 2: Define architecture of GAN. Define how your GAN should look like. Should both your generator and discriminator be multi layer perceptrons, or convolutional neural networks? This step will depend on what problem you are trying to solve.Step 3: Train Discriminator on real data for n epochs. Get the data you want to generate fake on and train the discriminator to correctly predict them as real. Here value n can be any natural number between 1 and infinity.Step 4: Generate fake inputs for generator and train discriminator on fake data. Get generated data and let the discriminator correctly predict them as fake.Step 5: Train generator with the output of discriminator. Now when the discriminator is trained, you can get its predictions and use it as an objective for training the generator. Train the generator to fool the discriminator.Step 6: Repeat step 3 to step 5 for a few epochs.Step 7: Check if the fake data manually if it seems legit. If it seems appropriate, stop training, else go to step 3. This is a bit of a manual task, as hand evaluating the data is the best way to check the fakeness. When this step is over, you can evaluate whether the GAN is performing well enough.Now just take a breath and look at what kind of implications this technique could have. If hypothetically you had a fully functional generator, you can duplicate almost anything. To give you examples, you can generate fake news; create books and novels with unimaginable stories; on call support and much more. You can have artificial intelligence as close to reality; a true artificial intelligence! Thats the dream!!You may ask, if we know what could these beautiful creatures (monsters?) do; why havent something happened? This is because we have barely scratched the surface. Theres so many roadblocks into building a good enough GAN and we havent cleared many of them yet. Theres a whole area of research out there just to find how to train a GANThe most important roadblock while training a GAN is stability. If you start to train a GAN, and the discriminator part is much powerful that its generator counterpart, the generator would fail to train effectively. This will in turn affect training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless.Another way to glance at stability of GAN is to look as a holistic convergence problem. Both generator and discriminator are fighting against each other to get one step ahead of the other. Also, they are dependent on each other for efficient training. If one of them fails, the whole system fails. So you have to make sure they dont explode.This is kind of like the shadow in Prince of Persia game . You have to defend yourself from the shadow, which tries to kill you. If you kill the shadow you die, but if you dont do anything, you will definitely die!There are other problems too, which I will list down here. (Reference: http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)Note: Below mentioned images are generated by a GAN trained on ImageNet dataset.A substantial research is being done to take care of these problems. Newer types of models are proposed which give more accurate results than previous techniques, such as DCGAN, WassersteinGAN etcLets see a toy implementation of GAN to strengthen our theory. We will try to generate digits by training a GAN on Identify the Digits dataset. A bit about the dataset; the dataset contains 2828 images which are black and white. All the images are in .png format. For our task, we will only work on the training set. You can download the dataset from here.You also need to setup the libraries , namelyBefore starting with the code, let us understand the internal working thorugh pseudocode. A pseudocode of GAN training can be thought out as followsSource: http://papers.nips.cc/paper/5423-generative-adversarialNote: This is the first implementation of GAN that was published in the paper. Numerous improvements/updates in the pseudocode can be seen in the recent papers such as adding batch normalization in the generator and discrimination network, training generator k times etc.Now lets start with the code!Let us first import all the modulesTo have a deterministic randomness, we set a seed valueWe set the path of our data and working directoryLet us load our dataTo visualize what our data looks like, let us plot one of the imageDefine variables which we will be using later# define vars g_input_shape = 100 d_input_shape = (28, 28) hidden_1_num_units = 500 hidden_2_num_units = 500 g_output_num_units = 784 d_output_num_units = 1 epochs = 25 batch_size = 128Now define our generator and discriminator networksHere is the architecture of our networksWe will then define our GAN, for that we will first import a few important modulesLet us compile our GAN and start the trainingHeres how our GAN would look like,We get a graph like after training for 10 epochs.After training for 100 epochs, I got the following generated images   And voila! You have built your first generative model!We saw an overview of how these things work and got to know the challenges of training them. We will now see the cutting edge research that has been done using GANsHere are some resources which you might find helpful to get more in-depth on GANPhew! I hope you are now as excited about the future as I was when I first read about GANs. They are set to change what machines can do for us. Think of it  from preparing new recipes of food to creating drawings. The possibilities are endless.In this article, I tried to cover a general overview of GAN and its applications. GAN is very exciting area and thats why researchers are so excited about building generative models and you can see that new papers on GANs are coming out more frequently.If you have any questions on GANs, please feel free to share them with me through comments.",https://www.analyticsvidhya.com/blog/2017/06/introductory-generative-adversarial-networks-gans/
Data Scientist  Bangalore (4-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Introductory guide to Generative Adversarial Networks (GANs) and their promise!|Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 
Task Info : Job Description and Responsibilities: Excellent coding skills.  Curious and detailed oriented.  Great analytical skills and problem solving capabilities.  Advanced R or Python knowledge.  Experience working with relational databases and SQL.  Familiarity with GIT.  Basic statistics knowledge. Desirable skills:  Experience with Java or C++.  Working experience with distributed processing. Experience with one of Hadoop, Spark, Netezza, Hive, SparkSQL, SparkR.  Experience in database development with strong skills in schema design, development and performance tuning.  Experience developing software in collaboration with others and maintaining a coding library.  Familiarity with investment theory and data.  Experience working in continuous integration using Jenkins.  Working knowledge of Linux.  Experience with visualization tools such as Tableau.  Knowledge of Tensorflow and/or Theano. Education : Advanced degree (MS/PhD) in physics, mathematics, statistics, engineering or computer science. Undergraduate degree with strong experiences can be considered as well.
College Preference : no-bar
Min Qualification : pg
Skills : c++, hadoop, hive, java, linux, python, r, spark, sql, statistics, tableau, tensor flow, Theano
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-bangalore-4-8-years-of-experience/
Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Bangalore (4-8 Years Of Experience)|Assistant Manager (Data Analytics)  Kolkata (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  4 years
Requirements : 
Task Info : Job Description and Responsibilities: Strong background in statistical concepts and calculations with 2.5+ yrs experience with real data  Innovative and strong analytical and algorithmic problem solvers.  Proficiency with statistical analysis tools (e.g. R, SAS,SPSS), software development technologies (e.g. Python, Java, C/C++, .Net)  Extensive experience solving analytical problems using quantitative approaches (e.g. Bayesian Analysis, Reduced Dimensional Data Representations, and Multi-scale Feature Identification).  Expert at data visualization and presentation.  Excellent critical thinking skills, combined with the ability to present your beliefs clearly and compellingly verbally and in written form.Key Role & Responsibilities:  Work with engineering and research teams on designing, building and deploying data analysis systems for large data sets  Design, develop and implement R&D and pre-product prototype solutions and implementations using off the shelf tools (e.g. R, SAS,SPSS), and software (e.g. Python, Java, C/C++, .NET)  Create algorithms to extract information from large data sets.  Establish scalable, efficient, automated processes for model development, model validation, model implementation and large scale data analysis.  Develop metrics and prototypes that can be used to drive business decisions.Qualification: Bachelor Degree in Engineering from top institute/college  IIT, NIT, BITS Pilani etc. Or MS / MSc in Statistics
College Preference : tier1-any
Min Qualification : ug
Skills : c++, data visualization, java, problem solving, python, r, sas, spss
Location : Bengaluru, Delhi, Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-delhincrbangalorehyderabad-3-4-years-of-experience/
Assistant Manager (Data Analytics)  Kolkata (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Delhi/NCR/Bangalore/Hyderabad- (3-4 Years Of Experience)|Which algorithm takes the crown: Light GBM vs XGBOOST?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : Job Description and Responsibilities: Data integration and building a data warehouse for automating the Business Intelligence system  Running and maintaining the reporting system, presenting insights at a weekly meeting  Building templates, dashboards in Excel or on other third-party analytics tool for operational and management reporting  Data extraction as per business request for Ad hoc analysis  Business analysis and understanding  Evaluating metrics to be tracked as per business goals, exploring other available metrics for deeper understanding of business performance  Statistical and Analytical Models and methods for data analysis  Interacting with cross-functional teams to schedule campaigns and activities and track performance Preferred Qualification & Skills :  BE / B. Tech from premier schools  Proficient in R and Data Modelling  3+ years of experience in working on reporting / business intelligence systems  Strong database concepts and experience in SQL  can convert any business requirement into a SQL statement  Familiar with Google Analytics, can build custom reports and dashboards for any business query  Quick learner and ability to work in dynamic work environment  Team player and comfortable interacting with people from multiple disciplines
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, excel, google analytics, modeling, r, sql
Location : Kolkata
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-data-analytics-kolkata-3-7-years-of-experience/
Which algorithm takes the crown: Light GBM vs XGBOOST?,Learn everything about Analytics|Introduction|Table of Contents|1. What is Light GBM?|2. Advantages of Light GBM|3. Installing Light GBM|4. Important Parameters of light GBM|5. LightGBM vs XGBoost|6. Tuning Parameters of Light GBM|7. End Notes,"For Windows|For Linux
|For OSX|#Applying xgboost|# Light GBM|For best fit|For faster speed|For better accuracy|Share this:|Like this:|Related Articles|Assistant Manager (Data Analytics)  Kolkata (3-7 Years Of Experience)|Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience)|
Pranjal Khandelwal
|18 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you are an active member of the Machine Learning community, you must be aware of Boosting Machines and their capabilities. The development of Boosting Machines started from AdaBoost to todays favorite XGBOOST. XGBOOST has become a de-facto algorithm for winning competitions at Analytics Vidhya and Kaggle, simply because it is extremely powerful. But given lots and lots of data, even XGBOOST takes a long time to train.Enter. Light GBM.Many of you might not be familiar with the Light Gradient Boosting, but you will be after reading this article. The most natural question that will come to your mind is  Why another boosting machine algorithm? Is it superior to XGBOOST?Well, you very well must have guessed the answer otherwise why would a topic deserve its own article :pP.S. This article assumes knowledge about GBMs and XGBoost. If you dont know them, you should first look at these articles.Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word Light.Before is a diagrammatic representation by the makers of the Light GBM to explain the difference clearly.Level-wise tree growth in XGBOOST.Leaf wise tree growth in Light GBM.Leaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur.Below, we will see the steps to install Light GBM and run a model using it. We will be comparing the results with XGBOOST results to prove that you should take Light GBM in a LIGHT MANNER.Let us look at some of the advantages of Light GBM.I guess you must have got excited about the advantages of Light GBM. Let us now proceed to install the library into our system.Using Visual Studio (Or MSBuild)
-Installgit for windows,cmakeandMS Build(Not need the MSbuild if you already installVisual Studio).
-Run following command:The exe and dll will be inLightGBM/Releasefolder.Using MinGW64
-Installgit for windows,cmakeand MinGW64.
-Run following command:The exe and dll will be inLightGBM/folder.Light GBM usescmaketo build. Run following:LightGBM depends on OpenMP for compiling, which isnt supported by Apple Clang.Please use gcc/g++ instead.
-Run following:Now before we dive head first into building our first Light GBM model, let us look into some of the parameters of Light GBM to have an understanding of its underlying procedures.Also, go through this article explaining parameter tuning in XGBOOST in detail.So now lets compare LightGBM with XGBoostby applying both the algorithms to a dataset and then comparing the performance.Here we are using dataset that contains the information about individuals from various countries. Our target is to predict whether a person makes <=50k or >50k annually on basis of the other information available. Dataset consists of 32561 observations and 14 features describing individuals.Here is the link to the dataset: http://archive.ics.uci.edu/ml/datasets/Adult.Go through the dataset to have a proper intuition about predictor variables and so that you could understand the code below properly.Before we get to the code for this dataset, did you know that you can now code your own model in this very window? Thats right! Heres a live coding window to play around with the code and see the results in real-time:Performance comparisonThere has been only a slight increase in accuracy and auc score by applying Light GBM over XGBOOST but there is a significant difference in the execution time for the training procedure. Light GBM is almost 7 times faster than XGBOOST and is a much better approach when dealing with large datasets.This turns out to be a huge advantage when you are working on large datasets in limited time competitions.Light GBM uses leaf wise splitting over depth wise splitting which enables it to converge much faster but also leads to overfitting. So here is a quick guide to tune the parameters in Light GBM.In this blog, Ive tried to give an intuitive idea of Light GBM. One of the disadvantages of using this algorithm currently is its narrow user base  but that is changing fast. This algorithm apart from being more accurate and time-saving than XGBOOST has been limited in usage due to less documentation available.However, this algorithm has shown far better results and has outperformed existing boosting algorithms. Ill strongly recommend you to implement Light GBM over the other boosting algorithms and see the difference yourself.It might be still early days to crown LightGBM  but it has clearly challenged XGBoost. A word of caution  like all other ML algorithms, make sure you properly tune the parameters before training the model!Do let us know your thoughts and opinions in the comment section below.",https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/
Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Which algorithm takes the crown: Light GBM vs XGBOOST?|Business Analyst  Bangalore (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : We are focused on BFSI, Retail & Healthcare space use cases for companies that need technology and professional services for their functional and operational analytics projects. We offer a range of solutions that bring immediate business benefits to our global customers leveraging big data, statistical and mathematical modeling techniques, social analytics, and mobile descriptive analytics for new business insights.Role Description:The role will participate in requirement gathering, system design, model implementation, code reviews, testing, and maintenance of the platform. You will be part of a highly focused development team that includes data scientists, data engineers and business analysts to help build products and specialized services on offer to clients across multi-platform environment. The role offers a high degree of challenge and provides opportunity to experiment offerings that speaks of innovation with a high velocity and quality. Skills required: 2+ years of Analytics experience in developing applications using Python, predictive modeling and analysis.  Experienced in writing a good, clean, testable, Python code.  Experience with MySQL, Django and git.  Knowledge of Image processing libraries such as OpenCV, PIL, and pytesseract would be an added advantage.  Basic knowledge of Agile development practices.  Good understanding of numpy, scipy, pandas and scikit-learn libraries.  Strong analytical and problem solving skills.  Familiarity with the fast-paced startup environment and culture.  A team player with excellent written and verbal communication skills. 
College Preference : no-bar
Min Qualification : pg
Skills : numpy + scipy, predictive modeling, problem solving, python, scikit learn, statistics
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-python-expert-hyderabad-2-4-years-of-experience/
Business Analyst  Bangalore (3-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Python Expert ) -Hyderabad- (2-4 Years Of Experience)|Data Scientist  Delhi/NCR (0-2 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Job Description: First and foremost, problem solving ability you will be dealing with a very ambiguous set of problems  Very action oriented (we will have a bunch of things that keep coming up- taking them to closure is a critical skill)  Strong project management skills and communication / influencing skills.  Tech orientation is a must, past background in tech is not mandatory but preferred. Responsibilities :  Define business problems and define key metrics and indicators for measurement  Conduct analysis for assigned business problem by collecting data from all internal and external sources and perform analysis to get answers to business questions.  Conduct Market research for an assigned topic to dig out relevant data and analytics  Deeper analysis of consumer behaviors & trends, by reviewing the internal & external data Must have :  Strong communication skills (verbal & written)  Strong Analytical Skills and Process Orientation  The ability to thrive in a fast-paced, start-up environment Education : IIT/IIM Fresher
College Preference : tier1-any
Min Qualification : ug
Skills : analytics, business analysis, data management, problem solving, python, r, sas, sql, statistics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-bangalore-3-5-years-of-experience/
Data Scientist  Delhi/NCR (0-2 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Bangalore (3-5 Years Of Experience)|Analytics Specialist- Mumbai (2-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  2 years
Requirements : 
Task Info : Job Description and Responsibilities:Working in a small team led by the Chief Risk Officer and the co-founder of company, the data scientist will work on improving the data science capabilities. We currently have 2 key models, admit predictor model and the employability assessment model, and the selected candidate will work on improvising the same. The admit predictor model helps students shortlist the colleges they should target for higher studies. The employability assessment model helps lending partners make better offers to students who want to pursue higher studies. The ideal candidate can think big picture, and is comfortable with getting his/her hands dirty with data. The candidate should be resourceful in getting the data needed to support ones hypothesis. Prior experience through internships / job experience / Kaggle contests would be highly valued. The candidate should understand that in the real world data is never clean and data collection / data gathering is a significant part of the daily job requirements. Do you appreciate the difference between causation and correlation- If you answered yes, can see through the hype of Big Data, and know when to use an OLS and when to use GBM, we want you here! Qualification: Prior experience working on data science projects Experience in NLP will be preferred 
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, nlp, predictive modeling, python, r, statistical modeling
Location : Delhi
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-delhincr-0-2-years-of-experience/
Analytics Specialist- Mumbai (2-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Delhi/NCR (0-2 Years Of Experience)|Assistant Manager  (Strategic Research)- Mumbai (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Job Description:Analyze & interpret data and communicate results to clients, often with the aid of mathematical/statistical techniques and software. This role requires:  Data exploration, mathematical/statistical modelling, data analysis and hypothesis testing.  Design, development and deployment of Predictive models and frameworks.  Complex statistical concepts are explained in a way that clients can understand and advice on strategy. Responsibilities: Well-versed in quantitative analysis, research, data mining, trend analysis, customer profiling, clustering, segmentation and predictive modelling.  Executing the data-driven planning process by building models and frameworks that connect business unit drivers to company financials and forecast to take the correct decision as per the business need.  Design and build dashboards and other visualizations in BI tools.  Should be able to handle assigned tasks in the capacity of individual contributor.  Demonstrate excellent teamwork.  Collaborate with other data analysts to provide development coverage, support, and knowledge sharing and mentoring of junior team members. Education & Skills Summary: Bachelors in Computer Science/ Engineering, Statistics, Math or related quantitative degree or equivalent work experience.  Experience with modelling techniques such as clustering, linear & logistic regression, random forest etc.  Must have a passion for data, structured or unstructured.  2-4 years of hands-on experience with R (is a must)  SAS, SQL and python would be a plus.  Should have sound experience in data mining and data analysis.  Excellent critical thinking, verbal and written communications skills.  Ability and desire to work in a proactive, highly engaging, high-pressure, client service environment.  Good presentation skills.
College Preference : no-bar
Min Qualification : ug
Skills : clustering, data analysis, data mining, linear regression, logistic regression, python, r, random forest, sas, sql, statistics
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/analytics-specialist-mumbai-2-4-years-of-experience/
Assistant Manager  (Strategic Research)- Mumbai (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Specialist- Mumbai (2-4 Years Of Experience)|APM/PM  (Predictive Analytics)  Mumbai/Pune (2-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and Responsibilities: Strong quantitative and analytical skills with ability to translate data into meaningful insights is a big plus  Demonstrated accuracy, efficiency and attention to detail  Advanced skills in MS Office and various databases  Intermediate to advanced skills in MS-Excel will be an added advantage General Skills  Exceptional verbal and written communication and interpersonal skills  A high-minded team player  Strong intellect with proficient commercial and entrepreneurial instinct  Ability to work within tight deadlines  Ability to challenge upwards  Ability to adapt own work in a fast changing environment  Ability to operate in an ambiguous environment, break clichs and introduce a dynamic approachPrior Experience  Between 2 to 6 years of experience in industry/country/business/market research with in-depth knowledge of global marketplace, key trends, economic and geo-political developments, global supply chain, and key business and financial risks  Experience of working with a UK/US based insurance/re-insurance/insurance broking/consulting firm of global repute will be looked at preferably
College Preference : no-bar
Min Qualification : ug
Skills : analytics, database, excel
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/assistant-manager-strategic-research-mumbai-2-5-years-of-experience/
APM/PM  (Predictive Analytics)  Mumbai/Pune (2-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager  (Strategic Research)- Mumbai (2-5 Years Of Experience)|Data Scientist/Business Analyst  Mumbai (5- 10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  8 years
Requirements : 
Task Info : Job Description and Responsibilities:2-5 years of experience in advanced analytics/ predictive modeling. Candidate should have worked hands on in developing predictive models in some of these areas  cross-sell / up-sell strategies, market segmentation, demand forecasting, price optimization, time to event, survival analysis etcMandatory Skills :  Must be well versed with atleast one (1) of following tools  R, SAS, SPSS  Must have good knowledge / hands-on experience of statistical concepts like hypothesis testing, distributions etc  Must have good knowledge / hands-on experience of atleast some of the applied statistical techniques including multivariate regression, logistic regression, ARMA/ ARIMA, clustering, survival etc  Excellent presentation and communication skills (written and verbal) and comfortable presenting at senior levels  Conceptualize, design and deliver high-quality solutions and insightful analysis on a variety of projects ranging in both complexity and scopeQualification: Masters in statistics/mathematics/economics or MBA from a reputed institute Preferred Skills :  Knowledge / hand-on experience of at least two (2) of the following tools  VBA, SQL, MS Excel, Tableau  Ability to present results of statistical models in business language  Experience for building predictive models for varied industries  Retail, hospitality, technology hardware, software etc  Experience in handling large datasets  Exposure to multiple statistical tools  Excellent interpersonal skills  ability to network and earn confidence of diverse Client personnel plus interaction with management of eClerx India based operations team
College Preference : no-bar
Min Qualification : ug
Skills : arima, excel, logistic regression, multivariate analysis, predictive modeling, r, Retail Analytics, sas, spss, sql, statistical techniques, tableau, VBA
Location : Mumbai, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/apmpm-predictive-analytics-mumbaipune-2-8-years-of-experience/
Data Scientist/Business Analyst  Mumbai (5- 10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|APM/PM  (Predictive Analytics)  Mumbai/Pune (2-8 Years Of Experience)|Operational Risk Manager  Hyderabad (10-12 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  10 years
Requirements : 
Task Info : Job Description and Responsibilities: Help Leadership to use data to come up with intelligent and usable recommendations for business problems  Develop Metrics and Dashboards using MS SQL Access, Excel, Execute high priority (i.e. cross functional, high impact) projects to improve operations performance.  Create performance metrics and present them to the team with recommendations on areas to improve.  Getting report requirements from business teams and building and maintaining complex SQL queries to gather business data from data warehouse and financial reporting systems. Academic Qualification & Skill Set :  Graduate with 1st Class from a recognized institute  Post qualification relevant work experience as Analyst with a reputed organization.  Knowledge of RDBMS, MS SQL ( including SQL server), database design & development  Ability to identify suitable processes and automate MSSQL, MS Excel, Tasks.  Strong analytical business skills is a must.
College Preference : tier1-any
Min Qualification : ug
Skills : database, excel, RDBMS, sql server
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientistbusiness-analyst-mumbai-5-10-years-of-experience/
Operational Risk Manager  Hyderabad (10-12 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist/Business Analyst  Mumbai (5- 10 Years Of Experience)|Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  12 years
Requirements : 
Task Info : Job Description and Responsibilities Responsible for planning, development, implementation, and maintenance of EDA Risk metrics and KRIs (Key Risk Indicators) for the EDA group  Provide thought leadership and apply areas of expertise to data domain to develop a risk metrics program that is effective and efficient in discovering and measuring the inherent risks across the line of business  Development and implementation of the deliverables for Risk Metrics work stream, a work stream within the EDA transformation program  Development and execution of Risk Metrics reporting  Preparation of documentation for deliverables within the Risk Metrics work stream  Build strong relationships and partner with business partners throughout the EDA group related to Business Processes and the related KRIs  Participate in strategic team and enterprise initiatives for risk metrics and KRIs  Ensure appropriate prioritization, status reporting, escalation and closure of outstanding Risk Metrics issues  Provide risk metrics support, expertise and consulting to identify, assess and manage risk in support of EDA for projects and initiatives  Review metrics and decipher their usability from the risk and /or the performance angle  Synthesize risk related data for business, customers, and/or products/services/portfolios to form a story  Provide administrative oversight for EDA risk management offshore resources at EGS (please note that these resources will get work direction from their functional manager in US) Work with senior managers in India to provide required reporting re vacations/time taken off/time planning for offshore resources and facilitate performance management process on the groundDesired Qualifications :  Experience with enterprise database technologies (data storage, data movement, and data access etc.) and business intelligence tools  Experience utilizing Excel, PowerPoint and visualization skills (such as tableau)  Experience utilizing software such as SAS, SQL, Teradata, Oracle, Informatica, Ab Initio, Cognos etc.  Exposure to our data including BMG, MDSS, STS, Hogan, and/or Hemisphere databases  Exposure to or experience working with products in the financial industry  Professional certifications such as Certified IT Risk Professional (CRISC), Certified Information Systems Security Professional (CISSP), Certified in the Governance of Enterprise IT (CGEIT) or Certified Information Systems Auditor (CISA) 
College Preference : no-bar
Min Qualification : pg
Skills : Abinitio, business intelligence, excel, modeling, oracle, Power point, risk, sas, sql, tableau, teradata
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/operational-risk-manager-hyderabad-10-12-years-of-experience/
Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Operational Risk Manager  Hyderabad (10-12 Years Of Experience)|Getting started with Deep Learning using Keras and TensorFlow in R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  9 years
Requirements : 
Task Info : Job Description and Responsibilities: Developing advanced algorithms that solve problems of large dimensionality in a computationally efficient and statistically effective manner  Implementing statistical and data mining techniques e.g. hypothesis testing, machine learning, and retrieval processes on a large amount of data to identify trends, figures and other relevant information Collaborating with clients and other our stakeholders to effectively integrate and communicate analysis findings Providing guidance and project management support to the Associates on the teamResearch and Firm Contribution : Evaluating emerging datasets and technologies that may contribute to our analytical platform Owning the development of select assets/accelerators for efficient scaling of capability Contributing to the thought leadership of the firm by helping in researching the evolving topics and publishing them.Qualifications : BE / BTech / BS (stats) from Tier-1 institute would be required. PhD in Computer Science (OR Statistics) / MTech / MS from a premier institute would be highly preferred Substantial experience in Machine Learning. Knowledge of big data / advanced analytics concepts and algorithms text mining, social listening, recommender systems, predictive modeling, etc. Knowledge of programming Java/Python/R Exposure to tools/platforms Hadoop eco system and DB systems Agile project planning and project management skills Excellent communication skills Domain knowledge / expertise (Pharma / Healthcare /Travel / Hi-tech/Insurance) is preferred though not mandatory.
College Preference : tier1-any
Min Qualification : ug
Skills : algorithms, bigdata, Hadoop Ecosystem, java, machine learning, python, r, statistics, text mining
Location : Delhi, Gurugram, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-data-scientist-delhincrpunegurgaon-4-9-years-of-experience/
Getting started with Deep Learning using Keras and TensorFlow in R,Learn everything about Analytics|Introduction|Table of contents|1. Installation of Keras with tensorflow at the backend.|2. Different types of models that can be built in R using keras|3. Classifying MNIST handwritten digits using an MLP in R|4. MLP using keras  R vs Python|5. End Notes,"Share this:|Like this:|Related Articles|Senior Data Scientist- Delhi/NCR/Pune/Gurgaon (4-9 Years of Experience)|My journey from being an IT engineer to Head of Analytics|
NSS
|56 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It has always been a debatable topic to choose between R and Python. The Machine Learning world has been divided over the preference of one language over the other. But with the explosion of Deep Learning, the balance shifted towards Python as it had an enormous list of Deep Learning libraries and frameworks which R lacked (till now).I personally switched to Python from R simply because I wanted to dive into the Deep Learning space but with an R, it was almost impossible. But not anymore!With the launch of Keras in R, this fight is back at the center. Python was slowly becoming the de-facto language for Deep Learning models. But with the release of Keras library in R with tensorflow (CPU and GPU compatibility) at the backend as of now, it is likely that R will again fight Python for the podium even in the Deep Learning space.Below we will see how to install Keras with Tensorflow in R and build our first Neural Network model on the classic MNIST dataset in the RStudio.The steps to install Keras in RStudio is very simple. Just follow the below steps and you would be good to make your first Neural Network Model in R.install.packages(""devtools"")devtools::install_github(""rstudio/keras"")The above step will load the keras library from the GitHubrepository. Now it is time to load keras into R and install tensorflow.library(keras)By default RStudio loads the CPU version of tensorflow. Use the below command to download the CPU version of tensorflow.install_tensorflow()To install the tensorflow version with GPU support for a single user/desktop system, use the below command.install_tensorflow(gpu=TRUE)For multi-user installation, refer this installation guide.Now that we have keras and tensorflow installed inside RStudio, let us start and build our first neural network in R to solve the MNIST dataset.Below is the list of models that can be built in R using Keras.Let us start with building a very simple MLP model using just a single hidden layer to try and classify handwritten digits.#loading keras library
library(keras)#loading the keras inbuilt mnist dataset
data<-dataset_mnist()#separating train and test file
train_x<-data$train$x
train_y<-data$train$y
test_x<-data$test$x
test_y<-data$test$yrm(data)# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix
train_x <- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255
test_x <- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255#converting the target variable to once hot encoded vectors using keras inbuilt function
train_y<-to_categorical(train_y,10)
test_y<-to_categorical(test_y,10)#defining a keras sequential model
model <- keras_model_sequential()#defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]
#i.e number of digits from 0 to 9model %>% 
 layer_dense(units = 784, input_shape = 784) %>% 
 layer_dropout(rate=0.4)%>%
 layer_activation(activation = 'relu') %>% 
 layer_dense(units = 10) %>% 
 layer_activation(activation = 'softmax')#compiling the defined model with metric = accuracy and optimiser as adam.
model %>% compile(
 loss = 'categorical_crossentropy',
 optimizer = 'adam',
 metrics = c('accuracy')
)#fitting the model on the training dataset
model %>% fit(train_x, train_y, epochs = 100, batch_size = 128)#Evaluating model on the cross validation dataset
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128)The above code had a training accuracy of 99.14 and validation accuracy of 96.89. The code ran on my i5 processor and took around 13.5s for a single epoch whereas, on a TITANx GPU, the validation accuracy was 98.44 with an average epoch taking 2s.For the sake of comparison, I implemented the above MNIST problem in Python too. There should not be any difference since keras in R creates a conda instance and runs keras in it. But still, you can find the equivalent python code below.#importing the required libraries for the MLP model
import keras
from keras.models import Sequential
import numpy as np#loading the MNIST dataset from keras
from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()#reshaping the x_train, y_train, x_test and y_test to conform to MLP input and output dimensions
x_train=np.reshape(x_train,(x_train.shape[0],-1))/255
x_test=np.reshape(x_test,(x_test.shape[0],-1))/255import pandas as pd
y_train=pd.get_dummies(y_train)
y_test=pd.get_dummies(y_test)#performing one-hot encoding on target variables for train and test
y_train=np.array(y_train)
y_test=np.array(y_test)#defining model with one input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer [10 #neurons]
model=Sequential()from keras.layers import Densemodel.add(Dense(784, input_dim=784, activation='relu'))
keras.layers.core.Dropout(rate=0.4)
model.add(Dense(10,input_dim=784,activation='softmax'))# compiling model using adam optimiser and accuracy as metric
model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])
# fitting model and performing validationmodel.fit(x_train,y_train,epochs=50,batch_size=128,validation_data=(x_test,y_test))The above model achieved a validation accuracy of 98.42 on the same GPU. So, as we guessed initially, the results are the same.If this was your first Deep Learning model in R, I hope you enjoyed it. With a very simple code, you were able to classify hand written digits with 98% accuracy. This should be motivation enough to get you started with Deep Learning.If you have already worked on keras deep learning library in Python, then you will find the syntax and structure of the keras library in R to be very similar to that in Python. In fact, the keras package in R creates a conda environment and installs everything required to run keras in that environment. But, I am more excited to now see data scientists building real life deep learning models in R. As it is said  The competition should never stop. I would also like to hear your views on this new development for R. Feel free to comment.",https://www.analyticsvidhya.com/blog/2017/06/getting-started-with-deep-learning-using-keras-in-r/
My journey from being an IT engineer to Head of Analytics,Learn everything about Analytics,"Background  when sufficient is not enough|The Analytics Adventure|The Challenges|[emailprotected]|Myfirst break|Learning through different roles and Executive program|Head of [emailprotected]|Reflections on the journey|End Notes|Share this:|Like this:|Related Articles|Getting started with Deep Learning using Keras and TensorFlow in R|Consultant  SAS/SQL/ Tableau  Bangalore (1-3 Years Of Experience)|
Guest Blog
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I was an engineer working with an MNC ina pretty cushy job. It would have been a pretty happy life for people, but I had some other dreams. I wanted to do anMBA from outside India. Unfortunately, the plan didnt work out  there were issues on the financial and personal fronts  and eventually I figured that maybe my ambitions were somewhat unrealistic.Although my MBA plan had fizzled out, I realized that I needed to chart out a new route. I researched the internet and discovered a new field called Analytics. I had no idea about Business Analytics at the time and neither did I know much about the programs available for formal training. A deeper study convinced me that Analytics was an interesting domain but it was complex as well. It was evident to me that in order to make analytics as my career I had to devote substantial time to learning it.I therefore sought a full-time classroom program and discovered Praxis (courtesy a banner on Analytics Vidhya)  a comparatively emerging business school but a kind of pioneer in formal, full-time teaching of Business Analytics. Praxis had a pretty comprehensive curriculum, an impressive set of professors and a growing bunch of alumni. Convinced that this was the career I wanted, I applied to Praxis and fortunately got selected.I was thus far used to taking the tried and tested route to a career option. This was the first time I was trying to do something different. Obviously, it was not going to be easy.Thefirst change was adapting to a different kind of academic environment  one that encouraged debate, discussion and critical thinking;Thesecond challenge was to grapple with a variety of subjects that were all new for me. Though I had pursued an engineering degree, I was never a numbers or tech geek. When I joined the program, the scope and range of the course, with subjects as varied and complex as advance statistics, data mining, machine learning, econometrics, visualization etc. along with tools like SAS and R was quite intimidating at first.However, the presence of the professors and my peers made sure that the learning, though rigorous, was effective and enjoyable.The program at Praxis was intense to say the least.The program design combines classroom lectures with case-studies, labs and projects; there is continual assessment in the form of quizzes, assignments, exams and project presentations. Some of the courses were entirely taught by industry experts.We all spent multiple sleepless nights solving assignments with midnight deadlines, doing projects and preparing for tests that tested and rewarded thinking. I got a new sense of confidence by solving variousproblems and completing projects on new subjects.Knowing that this was all in preparation for the professional world of my choice and liking made the exertions substantially sweeter.The year spent in studying analytics set me up for a career in this domain. My first job (that I landed through the campus placement program) was 3i Infotech. They were building an analytics team at that time and we were some of the initial members. We used to work mainly on Excel and R to create project demos for clients. The 3i journey was interesting, but it ended abruptly when the Head of analytics decided to leave the firm and work on his start-up.My next assignment was with Nabler Bangalore and they promptly sent me to the USA to work on building a product recommendation engine for a Fortune 30 company called Lowes. This was a golden opportunity and thanks to some wonderful colleagues, I managed to work hard and learnt a lot. The work culture and practices at Lowes helped me grow as a professional and I was ready to lead a team.During this year, I also did a 1 year Executive program in Business Analytics and Intelligence from IIM Bangalore to help me transition smoothly in to the senior management roles that I was aspiring to get in to.CarDekho was building their analytics team and I joined them as Senior Manager  Analytics. The role was quite different  plus you had to be intrapreneurial  you had to dig down and conceptualise a project, take it to the CEO, get it approved and implement it  kind of on your own. Also, I was actively involved in hiring and managing people.Eventually, I moved to Yatra.com as Head of analytics. The stakes were high and so were the expectations. It took me some time to settle down and understand the system, the product, the people and of course the data. We have a dedicated dashboard team which builds executive dashboards for C level executives and business heads.We have a data warehousing team which collates the data from different sources i.e. booking data, CRM data and web data and stores it in a usable format. Then we have a predictive analytics team which is involved into long term projects such as customer churn analytics, cross-sell and recommendation engine based on Machine leaning concepts. My role involves end to end delivery of the projects, from the conceptualization to implementation. A good chunk of my time also goes into project and people management.From the time I graduated with a qualification in Analytics in 2013 to my taking up a leading position as Head of Analytics at Yatra.com in 2016, it has been a remarkable journey. I attribute this success to the following factors:Life comes back a full circle and I am sharing this journey at the same portal where it all started! I am sharing this story so that people like me  who want to do something different and out of box can see what it takes and how to go about planning their career.If you have any question, feel free to ask questions in the comments below.Ritesh is an adroit professional more than 8 years of work experience in the area of Analytics and Product management (Statistical analysis, Social Media/ Digital Marketing), Strategy, Product development and Marketing and served various domain such as BFSI, Technology, Education, IT & FMCG.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/06/journey-from-it-engineer-head-of-analytics/
Consultant  SAS/SQL/ Tableau  Bangalore (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|My journey from being an IT engineer to Head of Analytics|SQL Expert  Delhi/NCR/Gurgaon (2-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : Job Description and Responsibilities 2. Strong communication skills  3. Tableau experience  4. Good amount of SAS statistical knowledge  5. Strong SAS, Strong Excel and PPT making skills  6. Willingness to work from Banglore  Good to have :   1. Quick learner of new technologies and business functions  2. Demonstrated maturity to support client in different time zone  3. Masters degree in Management /Finance /Mathematics /Statistics /Operations or Engineering graduate  4. Knowledge of basic statistical techniques such as clustering, segmentation, ranking, correlation or regression etc.  5. Highly motivated, self-starter and team player
College Preference : no-bar
Min Qualification : pg
Skills : sas, sql, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/consultant-sassql-tableau-bangalore-1-3-years-of-experience/
SQL Expert  Delhi/NCR/Gurgaon (2-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant  SAS/SQL/ Tableau  Bangalore (1-3 Years Of Experience)|Senior Data Analyst-Chennai (2-4 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Job Description and Responsibilities: Understand business requirements of customer and convert them into technical  Complex manipulation and treatment of data in SQL, excel, R/Python/SAS (whichever applicable in particular case)  Integration of data from different sources  Application of data analytics and machine learning techniques on datasets to obtain the desired output  Do deep data dives to find out insights  Interact and collaborate with internal teams for project requirement and delivery  Create front-ends or dashboards or slide decks to deliver the result to the end user  Present outputs to end users Skills:  MySQL/ Oracle database  R/Python.  SAS is good to have, but not mandatory  Statistical / Machine Learning techniques  Tableau / Qlikview is good to have  MS Excel / MS Access Experience & Education:  B.Tech/ M.Tech / MBA/ MS in Statistics or Machine Learning  2-5 years of experience 
College Preference : no-bar
Min Qualification : ug
Skills : excel, machine learning, MS Acces, oracle, python, qlikview, r, sas, statistics, tableau
Location : Delhi, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/sql-expert-delhincrgurgaon-2-6-years-of-experience/
Senior Data Analyst-Chennai (2-4 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SQL Expert  Delhi/NCR/Gurgaon (2-6 Years Of Experience)|An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Job Description and Responsibilities: Understand business problems of various industries  Design analytical frameworks to address them  Apply leading-edge statistical modeling techniques and machine learning techniques  Work with state-of-the-art technologies  Work with complex data / big data  Be part of a growing team that is excited about the work we do  we would love to talk to you. The position offers a unique opportunity to be part of a small, fast-paced, challenging, and entrepreneurial environment, with a high degree of individual responsibility. Significant opportunities for professional development exist, as we continue to grow. Compensation packages among the best in the industry. Desired Skills and Experience: 2+ years of programming experience which should include hands-on programming in Java/C/C++  Knowledge of one of statistical/general purpose scripting languages software such as R, SAS, Python, SPSS etc is mandatory.  Excellent written and verbal communication skills
College Preference : no-bar
Min Qualification : ug
Skills : c++, C, java, python, r, sas, spss
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/senior-data-analyst-chennai-2-4-years-of-experience/
An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec,Learn everything about Analytics|Introduction|Project to apply Word Embeddings for Text Classification|Problem Statement|Table of Contents|1. What are Word Embeddings?|2. Different types of Word Embeddings|2.2 Prediction based Vector|2.2.2 Skip  Gram model|3. Word Embeddings use case scenarios|4. Using pre-trained word vectors|5. Training your own word vectors|Projects|6. End Notes,"2.1 Frequency based Embedding|2.2.1 CBOW (Continuous Bag of words)|Advantages of Skip-Gram Model|Share this:|Like this:|Related Articles|Senior Data Analyst-Chennai (2-4 Years of Experience)|Data Science Evangelist- Gurgaon (2 to 3 years of experience)|
NSS
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",2.1.1 Count Vector|2.1.2 TF-IDF vectorization|2.1.3 Co-Occurrence Matrix with a fixed context window,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Before we start, have a look at the below examples.So what do the above examples have in common?You possible guessed it right  TEXT processing. All the above three scenarios deal with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation in the third.Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing the above the three tasks. It is neither scalable nor effective.So, how do we make computers of today perform clustering, classification etc on a text data since we know that they are generally inefficient at handling and processing strings or texts for any fruitful outputs?Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that Apple in Apple is a tasty fruit is a fruit that can be eaten and not a company?The answer to the above questions lie in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in.And all of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them.Below, we will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results.The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.Formally, given a training sample of tweets and labels, where label 1 denotes the tweet is racist/sexist and label 0 denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.Practice NowIn very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked  Why do we need Word Embeddings?As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing stringsor plain textin their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications. Some real world applications of text applications are  sentiment analysis of reviews by Amazon etc., document or news classification or clustering by Googleetc.Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.Take a look at this example  sentence= Word Embeddings are Word converted into numbersA wordin this sentence may be Embeddings or numbers  etc.A dictionarymay be the list of all unique words in the sentence.So, a dictionary may look like  [Word,Embeddings,are,Converted,into,numbers]
A vector representation of a wordmay be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of numbersin this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].This is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest.The different types of word embeddings can be broadly classified into two categories-Let us try to understand each of these methods in detail.There are generally three types of vectors that we encounter under this category.Let us look into each of thesevectorization methods in detail.Consider a Corpus C of D documents {d1,d2..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).Let us understand this using a simple example.D1: He is a lazy boy. She is also lazy.D2: Neeraj is a lazy person.The dictionary created may be a list of unique tokens(words) in the corpus =[He,She,lazy,boy,Neeraj,person]
Here, D=2, N=6The count matrix M of size 2 X 6 will be represented as Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for lazy in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as  D2 contains lazy: once, Neeraj: once and person once.Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-Below is a representational image of the matrix M for easy understanding.This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.Common words like is, the, a etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word Messi in comparison to other documents. But common words like the etc. are also going to be present in higher frequency in almost every document.Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.So, how exactly does TF-IDF work?Consider the below sample table which gives the count of terms(tokens/words) in two documents.Now, let us define a few terms related to TF-IDF.TF = (Number of times term t appears in a document)/(Number of terms in the document)So, TF(This,Document1) = 1/8TF(This, Document2)=1/5It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word Messi in large number.IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.where N is the number of documents and n is the number of documents a term t has appeared in.So, IDF(This) = log(2/2) = 0.So, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.Let us compute IDF for the word Messi.IDF(Messi) = log(2/1) = 0.301.Now, let us compare the TF-IDF for a common word This and a word Messi which seems to be of relevance to Document 1.TF-IDF(This,Document1) = (1/8) * (0) = 0TF-IDF(This, Document2) = (1/5) * (0) = 0TF-IDF(Messi, Document1) =(4/8)*0.301 = 0.15As, you can see for Document1 , TF-IDF method heavily penalises the word This but assigns greater weight to Messi. So, this may be understood as Messi is an important word for Document1 from the context of the entire corpus.The big idea  Similar words tend to occur together and will have similar context for example  Apple is a fruit. Mango is a fruit.
Apple and mango tend to have a similar context i.e fruit.Before I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified  Co-Occurrence and Context Window.Co-occurrence  For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.Context Window  Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,The green words are a 2 (around) context window for the word Fox and for calculating the co-occurrence only these words will be counted. Let us see context window for the word Over.Now, let us take an example corpus to calculate a co-occurrence matrix.Corpus = He is not lazy. He is intelligent. He is smart.Let us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.Red box- It is the number of times He and is have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.while the word lazy has never appeared with intelligent in the context window and therefore has been assigned 0 in the blue box.Variations of Co-occurrence MatrixLets say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-But, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.Let me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.And, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.So, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.Advantages of Co-occurrence MatrixDisadvantages of Co-Occurrence MatrixPre-requisite: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through this awesome article by Sunil to gain a very good understanding of how NN works.So far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.Word2vec is not a single algorithm but a combination of two techniques  CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working.The way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.Suppose, we have a corpus C = Hey, this is sample corpus using only one context word. and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.The target for a single datapoint say Datapoint 4 is shown as belowThis matrix shown in the above image is sent into a shallow neural network with three layers: aninput layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.Let us first see a diagrammatic representation of the CBOW model.The matrix representation of the above image for a single data point is below.The flow is as follows:We saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.Below is a matrix representation of the above architecture for an easy understanding.The image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.So, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.The steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.In both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article  Understanding and Coding Neural Networks from scratch.The differences between MLP and CBOW are mentioned below for clarification:wo : output word
wi: context words2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP.Advantages of CBOW:Disadvantages of CBOW:Skip  gram follows the same topology as of CBOW. It just flips CBOWs architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=Hey, this is sample corpus using only one context word. Let us construct the training data.The input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be two one hot encoded target variables and two corresponding outputs as can be seen by the blue section in the image.Two separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.The weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.The skip-gram architecture is shown below.For a better understanding, matrix style structure with calculation has been shown below.Let us break down the above image.Input layer size  [1 X V], Input hidden weight matrix size  [V X N], Number of neurons in hidden layer  N, Hidden-Output weight matrix size  [N X V], Output layer size  C [1 X V]
In the above example, C is the number of context words=2, V= 10, N=4Thisis an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.Since word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-Below is one interesting visualisation of word2vec.The above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.5. It can be used to perform Machine Translation.
The above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.We are going to use googles pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.from gensim.models import Word2Vec
#loading the downloaded model
model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)#the model is loaded. It can be used to perform all of the tasks mentioned above.# getting word vectors of a word
dog = model['dog']#performing king queen magic
print(model.most_similar(positive=['woman', 'king'], negative=['man']))#picking odd one out
print(model.doesnt_match(""breakfast cereal dinner lunch"".split()))#printing similarity index
print(model.similarity('woman', 'man'))We will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.word2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I wont be covering the pre-preprocessing part here. So lets take an example list of list to train our word2vec model.sentence=[[Neeraj,Boy],[Sarwan,is],[good,boy]]
#training word2vec on 3 sentences
model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)Let us try to understand the parameters of this model.sentence  list of list of our corpus
min_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.
size=300  the number of dimensions in which we wish to represent our word. This is the size of the word vector.
workers=4  used for parallelization#using the model
#The new trained model can be used similar to the pre-trained ones.#printing similarity index
print(model.similarity('woman', 'man'))Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:Word Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.Note: We also have a video course on Natural Language Processing covering many NLP topics including word embeddings. Do check it out!",https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
Data Science Evangelist- Gurgaon (2 to 3 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec|Transfer learning and the art of using Pre-trained Models in Deep Learning|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,What should you expect?|Who can fill in the shoes?|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : 
Task Info : Do you feel passionately about solving problems through data? Have you spent a few years solving business problems through data?Do you aspire to take data science to millions of people out there? Can the leader in you make people follow data science out of sheer passion? Would you enjoy helping people solve problems with out expecting any thing in return?If the answer to all the questions is yes  look no more.Analytics Vidhya is looking for evangelists who can carry and deliver their baton to the world.A team of best data scientists and thought leaders from industryWhat is the role?Being a startup, the role would evolve over time. But, here are a few things you can expect:
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, data science, deep learning, machine learning, python, r
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/
Transfer learning and the art of using Pre-trained Models in Deep Learning,Learn everything about Analytics|Introduction|Table of Contents|What is transfer learning?|What is a Pre-trained Model?|Why would we use Pre-trained Models?|How can I use Pre-trained Models?|Ways to Fine tune the model|Use the pre-trained models to identify handwritten digits|Projects|End Notes,"Share this:|Like this:|Related Articles|Data Science Evangelist- Gurgaon (2 to 3 years of experience)|Data Scientist -Mumbai (1-4 Years Of Experience)|
Dishashree Gupta
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Neural networks are a different breed of models compared to the supervised machine learning algorithms. Why do I say so? There are multiple reasons for that, but the most prominent is the cost of running algorithms on the hardware.In todays world, RAM on a machine is cheap and is available in plenty. You need hundreds of GBs of RAM to run a super complex supervised machine learning problem  it can be yours for a little investment / rent. On the other hand, access to GPUs is not that cheap. You need access to hundred GB VRAM on GPUs  it wont be straight forward and would involve significant costs.Now, that may change in future. But for now, it means that we have to be smarter about the way we use our resources in solving Deep Learning problems. Especially so, when we try to solve complex real life problems on areas like image and voice recognition. Once you have a few hidden layers in your model, adding another layer of hidden layer would need immense resources.Thankfully, there is something called Transfer Learning which enables us to use pre-trained models from other people by making small changes. In this article, I am going to tell how we can use pre-trained models to accelerate our solutions.You can check out our list of the top pretrained models for computer vision and NLP here:Note  This article assumes basic familiarity with Neural networks and deep learning. If you are new to deep learning, I would strongly recommend that you read the following articles first:What is deep learning and why is it getting so much attention?Deep Learning vs. Machine Learning  the essential differences you need to know!25 Must Know Terms & concepts for Beginners in Deep LearningWhy are GPUs necessary for training Deep Learning models?Let us start with developing an intuition for transfer learning. Let us understand from a simple teacher  student analogy.A teacher has years of experience in the particular topic he/she teaches. With all this accumulated information, the lectures that students get is a concise and brief overview of the topic. So it can be seen as a transfer of information from the learned to a novice.Keeping in mind this analogy, we compare this to neural network. A neural network is trained on a data. This network gains knowledge from this data, which is compiled as weights of the network. These weights can be extracted and then transferred to any other neural network. Instead of training the other neural network from scratch, we transfer the learned features.Now, let us reflect on the importance of transfer learning by relating to our evolution. And what better way than to use transfer learning for this! So I am picking on a concept touched on by Tim Urban from one of his recent articles on waitbutwhy.comTim explains that before language was invented, every generation of humans had to re-invent the knowledge for themselves and this is how knowledge growth was happening from one generation to other:Then, we invented language! A way to transfer learning from one generation to another and this is what happened over same time frame:Isnt it phenomenal and super empowering? So, transfer learning by passing on weights is equivalent of language used to disseminate knowledge over generations in human evolution.Simply put, a pre-trained model is a model created by some one else to solve a similar problem. Instead of building a model from scratch to solve a similar problem, you use the model trained on other problem as a starting point.For example, if you want to build a self learning car. You can spend years to build a decent image recognition algorithm from scratch or you can take inception model (a pre-trained model) from Google which was built on ImageNet data to identify images in those pictures.A pre-trained model may not be 100% accurate in your application, but it saves huge efforts required to re-invent the wheel. Let me show this to you with a recent example.I spent my last week working on a problem at CrowdAnalytix platform  Identifying themes from mobile case images. This was an image classification problem where we were given 4591 images in the training dataset and 1200 images in the test dataset. The objective was to classify the images into one of the 16 categories. After the basic pre-processing steps, I started off with a simple MLP model with the following architecture-To simplify the above architecture after flattening the input image [224 X 224 X 3] into [150528], I used three hidden layers with 500, 500 and 500 neurons respectively. The output layer had 16 neurons which correspond to the number of categories in which we need to classify the input image.I barely managed a training accuracy of 6.8 % which turned out to be very bad. Even experimenting with hidden layers, number of neurons in hidden layers and drop out rates. I could not manage to substantially increase my training accuracy. Increasing the hidden layers and the number of neurons, caused 20 seconds to run a single epoch on my Titan X GPU with 12 GB VRAM.Below is an output of the training using the MLP model with the above architecture.Epoch 10/1050/50 [==============================]  21s  loss: 15.0100  acc: 0.0688As, you can see MLP was not going to give me any better results without exponentially increasing my training time. So I switched to Convolutional Neural Network to see how they perform on this dataset and whether I would be able to increase my training accuracy.The CNN had the below architecture I used 3 convolutional blocks with each block following the below architecture-The result obtained after the final convolutional block was flattened into a size [256] and passed into a single hidden layer of with 64 neurons. The output of the hidden layer was passed onto the output layer after a drop out rate of 0.5.The result obtained with the above architecture is summarized below-Epoch 10/1050/50 [==============================]  21s  loss: 13.5733  acc: 0.1575Though my accuracy increased in comparison to the MLP output, it also increased the time taken to run a single epoch  21 seconds.But the major point to note was that the majority class in the dataset was around 17.6%. So, even if we had predicted the class of every image in the train dataset to be the majority class, we would have performed better than MLP and CNN respectively. Addition of more convolutional blocks substantially increased my training time. This led me to switch onto using pre-trained models where I would not have to train my entire architecture but only a few layers.So, I used VGG16 model which is pre-trained on the ImageNet dataset and provided in the keras library for use. Below is the architecture of the VGG16 model which I used.The only change that I made to the VGG16 existing architecture is changing the softmax layer with 1000 outputs to 16 categories suitable for our problem and re-training the dense layer.This architecture gave me an accuracy of 70% much better than MLP and CNN. Also, the biggest benefit of using the VGG16 pre-trained model was almost negligible time to train the dense layer with greater accuracy.So, I moved forward with this approach of using a pre-trained model and the next step was to fine tune my VGG16 model to suit this problem.What is our objective when we train a neural network? We wish to identify the correct weights for the network by multiple forward and backward iterations. By using pre-trained models which have been previously trained on large datasets, we can directly use the weights and architecture obtained and apply the learning on our problem statement. This is known as transfer learning. We transfer the learning of the pre-trained model to our specific problem statement.You should be very careful while choosing what pre-trained model you should use in your case. If the problem statement we have at hand is very different from the one on which the pre-trained model was trained  the prediction we would get would be very inaccurate.For example, a model previously trained for speech recognition would work horribly if we try to use it to identify objects using it.We are lucky that many pre-trained architectures are directly available for us in the Keras library. Imagenetdata set has been widely used to build various architectures since it is large enough (1.2M images) to create a generalized model.The problem statement is to train a model that can correctly classify the images into 1,000 separate object categories. These 1,000 image categories represent object classes that we come across in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types etc.These pre-trained networks demonstrate a strong ability togeneralize to images outside the ImageNet dataset viatransfer learning. We make modifications in the pre-existing model by fine-tuning the model. Since we assume that the pre-trained network has been trained quite well, we would not want to modify the weights too soon and too much. While modifying we generally use a learning rate smaller than the one used for initially training the model.The below diagram should help you decide on how to proceed on using the pre trained model in your case Scenario 1  Size of the Data set is small while the Data similarity is very high  In this case, since the data similarity is very high, we do not need to retrain the model. All we need to do is to customize and modify the output layers according to our problem statement. We use the pretrained model as a feature extractor. Suppose we decide to use models trained on Imagenet to identify if the new set of images have cats or dogs. Here the images we need to identify would be similar to imagenet, however we just need two categories as my output  cats or dogs. In this case all we do is just modify the dense layers and the final softmax layer to output 2 categories instead of a 1000.Scenario 2  Size of the data is small as well as data similarity is very low In this case we can freeze the initial (lets say k) layers of the pretrained model and train just the remaining(n-k) layers again. The top layers would then be customized to the new data set. Since the new data set has low similarity it is significant to retrain and customize the higher layers according to the new dataset. The small size of the data set is compensated by the fact that the initial layers are kept pretrained(which have been trained on a large dataset previously) and the weights for those layers are frozen.Scenario 3  Size of the data set is large however the Data similarity is very low  In this case, since we have a large dataset, our neural network training would be effective. However, since the data we have is very different as compared to the data used for training our pretrained models. The predictions made using pretrained models would not be effective. Hence, its best to train the neural network from scratch according to your data.Scenario 4  Size of the data is large as well as there is high data similarity  This is the ideal situation. In this case the pretrained model should be most effective. The best way to use the model is to retain the architecture of the model and the initial weights of the model. Then we can retrain this model using the weights as initialized in the pre-trained model.Lets now try to use a pretrained model for a simple problem. There are various architectures that have been trained on the imageNet data set. You can go through various architectures here. I have used vgg16 as pretrained model architecture and have tried to identify handwritten digits using it. Lets see in which of the above scenarios would this problem fall into. We have around 60,000 training images of handwritten digits. This data set is definitely small. So the situation would either fall into scenario 1 or scenario 2. We shall try to solve the problem using both these scenarios. The data set can be downloaded from here.# importing required librariesfrom keras.models import Sequential
from scipy.misc import imread
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pdfrom keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
train=pd.read_csv(""R/Data/Train/train.csv"")
test=pd.read_csv(""R/Data/test.csv"")
train_path=""R/Data/Train/Images/train/""
test_path=""R/Data/Train/Images/test/""from scipy.misc import imresize
# preparing the train datasettrain_img=[]
for i in range(len(train)): temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224)) temp_img=image.img_to_array(temp_img) train_img.append(temp_img)#converting train images to array and applying mean subtraction processingtrain_img=np.array(train_img) 
train_img=preprocess_input(train_img)
# applying the same procedure with the test datasettest_img=[]
for i in range(len(test)): temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224)) temp_img=image.img_to_array(temp_img) test_img.append(temp_img)test_img=np.array(test_img) 
test_img=preprocess_input(test_img)# loading VGG16 model weights
model = VGG16(weights='imagenet', include_top=False)
# Extracting features from the train dataset using the VGG16 pre-trained modelfeatures_train=model.predict(train_img)
# Extracting features from the train dataset using the VGG16 pre-trained modelfeatures_test=model.predict(test_img)# flattening the layers to conform to MLP inputtrain_x=features_train.reshape(49000,25088)
# converting target variable to arraytrain_y=np.asarray(train['label'])
# performing one-hot encoding for the target variabletrain_y=pd.get_dummies(train_y)
train_y=np.array(train_y)
# creating training and validation setfrom sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=42)# creating a mlp model
from keras.layers import Dense, Activation
model=Sequential()model.add(Dense(1000, input_dim=25088, activation='relu',kernel_initializer='uniform'))
keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)model.add(Dense(500,input_dim=1000,activation='sigmoid'))
keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)model.add(Dense(150,input_dim=500,activation='sigmoid'))
keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)model.add(Dense(units=10))
model.add(Activation('softmax'))model.compile(loss='categorical_crossentropy', optimizer=""adam"", metrics=['accuracy'])# fitting the modelmodel.fit(X_train, Y_train, epochs=20, batch_size=128,validation_data=(X_valid,Y_valid))2. Freeze the weights of first few layers Here what we do is we freeze the weights of the first 8 layers of the vgg16 network, while we retrain the subsequent layers.This is because the first few layers capture universal features like curves and edges that are also relevant to our new problem. We want to keep those weights intact and we will get the network to focus on learning dataset-specific features in the subsequent layers.Code for freezing the weights of first few layers.from keras.models import Sequential
from scipy.misc import imread
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pdfrom keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
from keras.utils.np_utils import to_categoricalfrom sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.optimizers import SGD
from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activationfrom sklearn.metrics import log_loss
train=pd.read_csv(""R/Data/Train/train.csv"")
test=pd.read_csv(""R/Data/test.csv"")
train_path=""R/Data/Train/Images/train/""
test_path=""R/Data/Train/Images/test/""from scipy.misc import imresizetrain_img=[]
for i in range(len(train)): temp_img=image.load_img(train_path+train['filename'][i],target_size=(224,224)) temp_img=image.img_to_array(temp_img) train_img.append(temp_img)train_img=np.array(train_img) 
train_img=preprocess_input(train_img)test_img=[]
for i in range(len(test)):temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,224)) temp_img=image.img_to_array(temp_img) test_img.append(temp_img)test_img=np.array(test_img) 
test_img=preprocess_input(test_img)from keras.models import Modeldef vgg16_model(img_rows, img_cols, channel=1, num_classes=None):  model = VGG16(weights='imagenet', include_top=True)  model.layers.pop()  model.outputs = [model.layers[-1].output]  model.layers[-1].outbound_nodes = []    x=Dense(num_classes, activation='softmax')(model.output)  model=Model(model.input,x)#To set the first 8 layers to non-trainable (weights will not be updated)    for layer in model.layers[:8]:   layer.trainable = False# Learning rate is changed to 0.001
  sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)
  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])  return modeltrain_y=np.asarray(train['label'])le = LabelEncoder()train_y = le.fit_transform(train_y)train_y=to_categorical(train_y)train_y=np.array(train_y)from sklearn.model_selection import train_test_split
X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)# Example to fine-tune on 3000 samples from Cifar10img_rows, img_cols = 224, 224 # Resolution of inputs
channel = 3
num_classes = 10
batch_size = 16 
nb_epoch = 10# Load our model
model = vgg16_model(img_rows, img_cols, channel, num_classes)model.summary()
# Start Fine-tuning
model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))# Make predictions
predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)# Cross-entropy loss score
score = log_loss(Y_valid, predictions_valid)Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your deep learning journey with the following Practice Problems:I hope that you would now be able to apply pre-trained models to your problem statements. Be sure that the pre-trained model you have selected has been trained on a similar data set as the one that you wish to use it on. There are various architectures people have tried on different types of data sets and I strongly encourage you to go through these architectures and apply them on your own problem statements. Please feel free to discuss your doubts and concerns in the comments section.",https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
Data Scientist -Mumbai (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Transfer learning and the art of using Pre-trained Models in Deep Learning|Strategic Analytics  Bangalore/Mumbai (1-6 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : Job Description and ResponsibilitiesExcellent verbal and written communication and form liason between business and technical architects and developers, Strong inclination towards consulting engagements involving machine learning and artificial intelligence.Manage clients expectations and manage advanced analytics delivery.Hands on involvement in extracting, collating, performing data integrity checks, manipulating and analysing data Hands on involvement in suggesting and building an appropriate statistical model relevant to the industry vertical Develop analytical strategies for business using model results of predictive modelling, time series forecasting, clustering and segmentation of customers Documenting and presenting work to the client. Proactively recommending and influencing changes in business decision-making.Required skills: At least 1 year of experience in programming, quantitative analysis or as a data scientist
College Preference : tier1-any
Min Qualification : ug
Skills : artificial intelligence, c++, C, java, machine learning, predictive modeling, python, statistics
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/data-scientist-mumbai-1-4-years-of-experience/
Strategic Analytics  Bangalore/Mumbai (1-6 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist -Mumbai (1-4 Years Of Experience)|Business Analyst  Mumbai- (1-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  6 years
Requirements : 
Task Info : Job Description and Responsibilities Identify and understand trends and make recommendations. Create clear written communications and routinely present results to senior leaders including the ability to influence toward a particular decision Work with Operations partners and leaders to ensure any new developments will be deliverable and executable in an Operations environment  Perform analysis as it pertains to eligibility and performance of portfolio management strategies. Utilize SAS, SQL, Oracle, Knowledge Seeker and MS Office analysis tools to complete analysis requirements Partner with technical groups to develop requirements for changes to rules engines and automated strategies  Forecast performance changes that tie to segmentation or treatment changesQualifications  A minimum of 1 years of relevant analytics/modeling experience. Previous experience in credit card risk is strongly preferred  Bachelors or MS degree(preferred) in Statistics, Econometrics, Mathematics or Finance (or equivalent quantitative field) is required Knowledge of credit card P&L is preferred  Knowledge and experience in gathering requirements, building and presenting strategies, data mining and presentation of findings is a must  Experienced programming knowledge in both SAS and SQL, Operating systems experience with UNIX and relational database knowledge such as ORACLE/TERADATA  Strong analytical, interpretive and problem solving skills with the ability to interpret large amounts of data and its impact in both operational and financial areas  Demonstrated excellent written and oral communication skills to clearly present analytical findings and make business recommendations via the use of Microsoft Word, Excel, and PowerPoint
College Preference : no-bar
Min Qualification : ug
Skills : credit risk, excel, oracle, Power point, sas, sql, statistics, teradata, unix
Location : Bengaluru, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/strategic-analytics-bangaloremumbai-1-6-years-of-experience/
Business Analyst  Mumbai- (1-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Strategic Analytics  Bangalore/Mumbai (1-6 Years Of experience)|Building Trust in Machine Learning Models (using LIME in Python)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  5 years
Requirements : 
Task Info : Job Description and Responsibilities Identify & analyze business requirements through discussion with users and direct research  Prepare relevant test scenarios to ensure that testing results correspond to the business expectations  Participate in and manage in systems validation and business-level testing and quality assurance. Devise, implement and extend, data-modelling solutions in Excel/VBA, Python, R, and associated technologies Develop portfolio performance reporting, simulations and optimization  Implementation of new functionalities in products  Assist in and author presentation materials  Conducting client trainings and demos, participate and present in client meetings  Understand business process flows for investment management, and other financial markets businesses Required Experience and Skills:  1-to-5 years of relevant work experience,  Bachelor-level education in streams such as Business, Engineering, Finance, Economics, Statistics, Computer Systems, or equivalent,  Industry credentials such as CA, CFA, FRM or similar,  Experience in financial data analysis and modelling in Excel/VBA, R, Python, C++, and/or similar  Quantitative, analytical and critical thinking approach to solution finding  Demonstrated understanding of business process modeling, business systems development and analysis  Authored research reports and delivered presentations  Worked in/delivered to international team/clients  Excellent presentation, and communication skills 
College Preference : no-bar
Min Qualification : ug
Skills : c++, excel, modeling, python, r, VBA
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/06/business-analyst-mumbai-1-5-years-of-experience/
Building Trust in Machine Learning Models (using LIME in Python),Learn everything about Analytics|Introduction|Table of Contents|1. Motivation|2. The Problem|3. End Notes,"2.1 Steps for Model Building|2.2 Steps for using Lime to make your model interpretable|Share this:|Like this:|Related Articles|Business Analyst  Mumbai- (1-5 Years Of Experience)|Assistant Manager  Credit Risk  Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience)|
Guest Blog
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The value is not in software, the value is in data, and this is really important for every single company, that they understand what data theyve got.-John StrawMore and more companies are now aware of the power of data. Machine Learning models are increasing in popularity and are now being used to solve a wide variety of business problems using data. Having said that, it is also true that there is always a trade-off between accuracy of models & its interpretability.In general, if accuracy has to be improved, data scientists have to resort to using complicated algorithms like Bagging, Boosting, Random Forests etc. which are Blackbox methods. It is no wonder that many of the winning entries in Kaggle or Analytics Vidhya competitions tend to use algorithms like XGBoost, where there is no requirement to explain the process of generating the predictions to a business user. On the other hand, in a business setting, simpler models that are more interpretable like Linear Regression, Logistic Regression, Decision Trees, etc. are used even if the predictions are less accurate.This situation has got to change  the trade-off between accuracy & interpretability is not acceptable. We need to find ways to use powerful black-box algorithms even in a business setting and still be able to explain the logic behind the predictions intuitively to a business user. With increased trust in predictions, organisations will deploy machine learning models more extensively within the enterprise. The question is  How do we build trust in Machine Learning Models?It is in this context, I find the paper titled Why Should I Trust You?- Explaining the Predictions of Any Classifier [1] intriguing & interesting. In this paper [1] , the authors explain a framework called LIME (Locally Interpretable Model-Agnostic Explanations), which is an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. In the paper, there are many examples of problems where the predictions from black-box algorithms (even as extreme as Deep Learning) can be formulated in an interpretable fashion. I am not going to explain the paper in this blog but rather show how it can be implemented in our own classification problems.Sigma Cabs Surge Pricing Type ClassificationIn February this year, Analytics Vidhya ran a machine learning competition in which the objective was to predict the Surge Pricing Type for Sigma Cab  a taxi aggregation service. This was a multiclass classification problem. In this blog, we will see how to make predictions for this dataset and use LIME to make the predictions interpretable. The intent here is not to build the best possible model but rather the focus is on the aspect of interpretability.# Step 1  Import all libraries# Step 2  Define Functions, variables and dictionaries# Step 3  Load Training dataset# Step 4  Understand the data (Descriptive Statistics, Visualization)# Step 5  Data Pre-processing (Handle Missing data & outliers, Feature Engineering, Feature Transformation etc.)# Step 6  Feature Selection# Step 7  Create the validation set# Step 8  Compare Algorithms to find candidate algorithms?# Step 9  Algorithm(s) Tuning# Step 10  Finalize Model(s)In this case, we are fitting 3 models on the training data so that we can compare the explanations provided. The 3 models are a) Logistic Regression, b) Random Forests, c) XGBoost.LIME Step 1  After installing LIME (On ANACONDA distribution  pip install LIME), import the relevant libraries as shown below:LIME Step 2  Create a lambda function for each classifier that will return the predicted probability for the target variable (surge pricing type) given the set of featuresLIME Step 3  Create a concatenated list of all feature names which will be utilised by the LIME explainer in subsequent stepsLIME Step 4  This is the magical step that creates the explainerThe parameters used by the function are:LIME Step 5  Obtain the explanations from LIME for particular values in the validation datasetPick particular observations in the validation dataset to get their probability values for each class. LIME will provide an explanation as to the reason for assigning the probability. Compare the probability values to the actual class of the target variable for that prediction.Output is shown for 2 observations:(Note: The visualisation is not powerful enough to show the feature weights for all classes in a multi-class scenario but the same process is applicable to differentiate between class 1 & 3 also)2. Id = 45 in validation set: In this case, only Random Forest is able to assign a higher probability to type 1 which is the actual value. Both Logistic Regression & XGBoost predicts that type 2 has a higher probability. Also, when you look at the NOT 2 | 2 table, you can see the weights assigned by different algorithms to each feature. For example, Trip Distance > 0.35 is assigned a weight of 0.01 in the case of Logistic Regression, a weight of 0.02 in the case of Random Forest and 0.01 in the case of Xgboost. Each feature is then color-coded to indicate whether it is contributing to the prediction of 2 (Orange) or NOT 2 (Grey) in the feature-value-table. The Feature-Value table by itself shows the actual values of the features for that particular record (in this case Id = 45)(Note: The visualisation is not powerful enough to show the feature weights for all classes in a multi-class scenario but the same process is applicable to differentiate between class 1 & 3 also)The probability values for each class is different for each algorithm as the feature weights computed by each algorithm are different. Depending on the actual value of the features for a particular record and the weights assigned to those features, the algorithm computes the class probability and then predicts the class having the highest probability. These results can be interpreted by a subject matter expert to see which algorithm is picking up the right signals / features to make the prediction. Essentially, the black box algorithms have become white box in the sense that now we know what drives the algorithms to make its predictions.Heres the whole code for your referenceI hope you are as excited as me after looking at these results. The output of LIME provides an intuition into the inner workings of machine learning algorithms as to the features that are being used to arrive at a prediction. If LIME or similar algorithms can help in providing interpretable output for any type of blackbox algorithm, it will go a long way in getting the buy-in from business users to trust the output of machine learning algorithms. By building such trust, powerful methods can be deployed in a business context achieving the twin benefits of higher accuracy and interpretability. Please do check out the LIME paper for the math behind this fascinating development.ReferencesKarthikeyan Sankaran is currently a Director at LatentView Analytics which provides solutions at the intersection of Business, Technology & Math to business problems across a wide range of industries. Karthik has close to two decades of experience in the Information Technology industry having worked in multiple roles across the space of Data Management, Business Intelligence & Analytics.This story was received as part of The Mightiest Pen contest on Analytics Vidhya. Karthikeyans entry was one of the winning entries in the competition.",https://www.analyticsvidhya.com/blog/2017/06/building-trust-in-machine-learning-models/
Assistant Manager  Credit Risk  Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Building Trust in Machine Learning Models (using LIME in Python)|Business Analyst  Gurgaon-(2-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Job Description and Responsibilities 3-5 years of total experience of Data Analytics / Data Insights Generation Hands on experience on SAS and Excel/VBA Proven experience in delivering Analytics Solutions  Experience in banking industry required  Good communication/problem solving/analytical bent of mind
College Preference : no-bar
Min Qualification : ug
Skills : Data analytics, excel, problem solving, sas, VBA
Location : Bengaluru, Delhi, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/assistant-manager-credit-risk-delhincrbangaloregurgaon-3-6-years-of-experience/
Business Analyst  Gurgaon-(2-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager  Credit Risk  Delhi/NCR/Bangalore/Gurgaon (3-6 Years Of Experience)|Director  Analytics  CPG/Retail Domain- Bangalore- (12-15 Years Of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  7 years
Requirements : 
Task Info : Job Description and Responsibilities : Team responsible for developing and executing the most effective collections placement, segmentation, and pricing strategies, as well as driving analytics to improve the operational efficiency of fraud and in-house collections.  Candidate will be responsible for conducting analysis and generating insights into developing the optimal OA placement and pricing structure.  She/He will also work closely with Global Collections team to drive best placement / pricing practices across all markets, and the infrastructure team to ensure flawless execution. This role may be subject to additional background verification checks.  This role may be subject to additional background verification checks. Qualifications :  Strong analytical and problem solving skills. Experienced in SAS preferred. Good communication skills, Good relationship building and influencing skills, Motivated, results oriented, and effective at handling multiple projects at the same time.  Advanced degree in business, economics, statistics, operations research, or other quantitative fields preferred. Thorough understanding of Global Risk Systems (GRMS, WCC, and RMS) or IDN will be a plus for internal candidates 
College Preference : no-bar
Min Qualification : ug
Skills : business analysis, sas, strategy
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analyst-gurgaon-2-7-years-of-experience/
Director  Analytics  CPG/Retail Domain- Bangalore- (12-15 Years Of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst  Gurgaon-(2-7 Years Of Experience)|Understanding and coding Neural Networks From Scratch in Python and R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 12  15 years
Requirements : 
Task Info : Job Description and Responsibilities : Contributing to the thought capital through the creation of executive presentations, architecture documents, and IT position papers Developing and maintaining strong client relations with senior and C-level executives Leading and mentoring other IT consultants within the account and across analytics practice  Maintain high profitability while addressing the risks and issues of the account  Supporting business development and ensuring high levels of client satisfaction during delivery Developing new insights into the clients business model and pain points, and delivering actionable, high-impact results MandatorySkills : Should be able to Mentor the team members to groom them for the future roles  Ability to produce high quality outputs under pressure and within deadlines In depth knowledge of any of CPG/Retail is mandatory and more is a plus. Knowledge of analytical tool kits like SAS, SPSS, R  Knowledge of data analysisExperience Required : 12 to 15 years of demonstrated hands-on experience in the area of Business Analytics  At least 5 Full cycle Business Analytics engagements with at-least two engagements with role of Analytics Manager Education : Bachelors degree in Computer Science or a related field preferred (BE/B.Tech, BSc Stats)  Masters degree in a related field preferred (MBA, MSc Stats) 
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, r, Retail Analytics, sas, spss
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/director-analytics-cpgretail-domain-bangalore-12-15-years-of-experience/
Understanding and coding Neural Networks From Scratch in Python and R,Learn everything about Analytics|Overview||Introduction|Table of Contents:|Simple intuition behind neural networks||Multi Layer Perceptron and its basics|Stepsinvolved in Neural Network methodology|Visualization of steps for Neural Network methodology|Implementing NN using Numpy (Python)|Implementing NN in R|[Optional] Mathematical Perspective of Back Propagation Algorithm|End Notes:,"What is an activation function?||Forward Propagation, Back Propagation and Epochs||Multi-layer perceptron|Full Batch Gradient Descent and Stochastic Gradient Descent|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Director  Analytics  CPG/Retail Domain- Bangalore- (12-15 Years Of Experience )|Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience)|
Sunil Ray
|65 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You can learn and practice a concept in two ways:I prefer Option 2 and take that approach to learning any new topic. I might not be able to tell you the entire math behind an algorithm, but I can tell you the intuition. I can tell you the best scenariosto apply analgorithm based on my experiments and understanding.In my interactions with people, I find that people dont take time to develop this intuition and hence they struggle to apply things in the right manner.In this article, I will discuss the building block of a neural network from scratch and focus more on developingthis intuition to apply Neural networks. Wewill code in both Python and R. By end of this article, you will understand how Neural networks work, how do we initialize weigths and how do we update them usingback-propagation.Lets start.If you have been a developer or seen one work  you know how it is to search for bugs in a code. You would fire various test cases by varying the inputs or circumstances and look for the output. The change in output provides you a hint on where to look for the bug  which module to check, which lines to read. Once you find it, you make the changes and the exercise continues until you have the right code / application.Neural networks work in very similar manner.It takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as Forward Propagation.Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. Each ofthese neurons are contributing some error to final output. How do you reduce the error?We try to minimize the value/ weight of neurons those are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as Backward Propagation.In order to reduce these number of iterations to minimize the error, the neural networksuse a common algorithm known as Gradient Descent, which helps to optimize the task quickly and efficiently.Thats it  this is how Neural network works! I know this is a very simple representation, but it would help you understand things in a simple manner.Just like atoms form the basics of any material on earth  the basic forming unit of a neural network is a perceptron. So, what is a perceptron?A perceptron can be understood as anything that takes multiple inputs and produces one output. For example, look at the image below.PerceptronThe above structure takes three inputs and produces one output. The next logical question is what is the relationship between input and output? Let us start with basic ways and build on to find more complex ways.Below, I have discussed three ways of creating input output relationships:But, all of this is still linear which is what perceptrons used to be. But that was not as much fun. So, people thought of evolving a perceptron to what is now called as artificial neuron. A neuron applies non-linear transformations (activation function) to the inputs and biases.Activation Functiontakes the sum of weighted input (w1*x1 + w2*x2 + w3*x3 + 1*b) as an argument and return the output of the neuron.In above equation, we have represented 1 as x0 and b as w0.The activation function is mostly used to make a non-linear transformation which allows us to fit nonlinear hypotheses or to estimate the complex functions. There are multiple activation functions, like: Sigmoid, Tanh, ReLu and many other.Till now, we have computed the output and this process is known as Forward Propagation. But what if the estimated output is far away from the actual output (high error). In the neural network what we do, we update the biases and weights based on the error. This weight and bias updating process is known as Back Propagation.Back-propagation (BP) algorithms work by determining the loss (or error) at the output and then propagatingit back into the network. The weights are updated to minimize the error resulting from each neuron. The first step in minimizing the error is to determine the gradient (Derivatives) of each node w.r.t. the final output. To get a mathematical perspective of the Backward propagation, refer below section.This one round of forward and back propagation iteration is known as one training iteration aka Epoch.Now, lets move on to next part of Multi-Layer Perceptron. So far, we have seen just a single layer consisting of 3 input nodes i.e x1, x2 and x3 and an output layer consisting of a single neuron. But, for practical purposes, the single-layer network can do only so much. An MLP consists of multiple layers called Hidden Layersstacked in between the Input Layer and the Output Layer as shown below.The image above shows just a single hidden layer in green but in practice can contain multiple hidden layers. Another point to remember in case of an MLP is that all the layers are fully connected i.e every node in a layer(except the input and the output layer) is connected to every node in the previous layer and the following layer.Lets move on to the next topic which is training algorithm for a neural network (to minimize the error). Here, we will look at most common training algorithm known asGradient descent.Both variants of Gradient Descent perform the same work of updating the weights of the MLP by using the same updating algorithm but the difference lies in the number of training samples used to update the weights and biases.Full Batch Gradient Descent Algorithm as the name implies uses all the training data points to update each of the weights once whereas Stochastic Gradient uses 1 or more(sample) but never the entire training data to update the weights once.Let us understand this with a simple example of a dataset of 10 data pointswith two weights w1 and w2.Full Batch: You use 10 data points (entire training data) and calculate the change in w1 (w1) and change in w2(w2) and update w1 and w2.SGD: You use 1st data point and calculate the change in w1 (w1) and change in w2(w2) and update w1 and w2. Next, when you use 2nd data point, you will work on the updated weightsFor a more in-depth explanation of both the methods, you can have a look atthis article.Lets look at the step by step building methodology of Neural Network (MLP with one hidden layer, similar to above-shown architecture). At the output layer, we have only one neuron as we are solving a binary classification problem (predict 0 or 1). We could alsohave two neurons for predicting each of both classes.First look at the broad steps:0.) We take input and output1.) We initialize weights and biases with random values (This is one time initiation. In the next iteration, we will use updated weights, and biases). Let us define:2.) We take matrix dot product of input and weights assigned to edges between the input and hidden layer then add biases of the hidden layer neurons to respective inputs, this is known as lineartransformation:hidden_layer_input= matrix_dot_product(X,wh) + bh3) Perform non-linear transformation using an activation function (Sigmoid). Sigmoid will return the output as1/(1 + exp(-x)).hiddenlayer_activations = sigmoid(hidden_layer_input)4.) Perform a linear transformation on hidden layer activation (take matrix dot product with weights and add a bias of the output layer neuron) then apply an activation function (again used sigmoid, but you can use any other activation function depending upon your task) to predict the outputoutput_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
output = sigmoid(output_layer_input)
All above steps are known as Forward Propagation5.) Compare prediction with actual output and calculate the gradient of error (Actual  Predicted). Error is the mean square loss = ((Y-t)^2)/2E = y  output6.) Compute the slope/ gradient of hidden and output layer neurons (To compute the slope, we calculate the derivatives of non-linear activations x at each layer for each neuron). Gradient of sigmoid can be returned asx * (1  x).slope_output_layer = derivatives_sigmoid(output)
slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)7.) Computechange factor(delta) at output layer, dependent on the gradient of error multiplied by the slope of output layer activationd_output = E * slope_output_layer8.) At this step, the error will propagate back into the network which means error at hidden layer. For this, we will take the dot product of output layer delta with weight parameters of edges between the hidden and output layer (wout.T).Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)9.) Computechange factor(delta) at hidden layer, multiply the error at hidden layer with slope of hidden layer activationd_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer10.) Update weights at the output and hidden layer: The weights in the network can be updated from the errors calculated for training example(s).wout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate
wh = wh + matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_ratelearning_rate: The amount that weights are updated is controlled by a configuration parameter called the learning rate)11.) Update biases at the output and hidden layer: The biases in the network can be updated from the aggregated errors at that neuron.bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
bout = bout + sum(d_output, axis=0)*learning_rateSteps from 5 to 11 are known as Backward PropagationOne forward and backward propagation iteration is considered as one training cycle. As I mentioned earlier, When do we train second time then update weights and biases are used for forwardpropagation.Above, we have updated the weight and biases for hidden and output layer and we have used full batch gradient descent algorithm.We will repeat the above steps and visualize the input, weights, biases, output, error matrix to understand working methodology of Neural Network (MLP).Note:Step 0: Read input and outputStep 1: Initialize weights and biases with random values (There are methods to initialize weights and biases but for now initialize with random values)Step 2: Calculate hidden layer input:
hidden_layer_input= matrix_dot_product(X,wh) + bhStep 3:Perform non-linear transformation on hidden linear input
hiddenlayer_activations = sigmoid(hidden_layer_input)Step 4:Perform linear and non-linear transformation of hidden layer activation at output layeroutput_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
output = sigmoid(output_layer_input)Step 5: Calculate gradient of Error(E) at output layer
E = y-outputStep 6:Compute slope at output and hidden layer
Slope_output_layer= derivatives_sigmoid(output)
Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)Step 7: Compute delta at output layerd_output = E * slope_output_layer*lrStep 8: Calculate Error at hidden layerError_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)Step 9:Compute delta at hidden layerd_hiddenlayer = Error_at_hidden_layer * slope_hidden_layerStep 10: Update weight at both output and hidden layerwout = wout + matrix_dot_product(hiddenlayer_activations.Transpose, d_output)*learning_rate
wh = wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rateStep 11: Update biases at both output and hidden layerbh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
bout = bout + sum(d_output, axis=0)*learning_rateAbove, you can see that there is still a good error not close to actual target value because we have completed only one training iteration. If we will train model multiple times then it will be a very close actual outcome. I have completed thousands iteration and my result is close to actual target values ([[ 0.98032096] [ 0.96845624] [ 0.04532167]]).# input matrix
X=matrix(c(1,0,1,0,1,0,1,1,0,1,0,1),nrow = 3, ncol=4,byrow = TRUE)# output matrix
Y=matrix(c(1,1,0),byrow=FALSE)#sigmoid function
sigmoid<-function(x){
1/(1+exp(-x))
}# derivative of sigmoid function
derivatives_sigmoid<-function(x){
x*(1-x)
}# variable initialization
epoch=5000
lr=0.1
inputlayer_neurons=ncol(X)
hiddenlayer_neurons=3
output_neurons=1#weight and bias initialization
wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=0,sd=1), inputlayer_neurons, hiddenlayer_neurons)
bias_in=runif(hiddenlayer_neurons)
bias_in_temp=rep(bias_in, nrow(X))
bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE)
wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=0,sd=1), hiddenlayer_neurons, output_neurons)bias_out=runif(output_neurons)
bias_out_temp=rep(bias_out,nrow(X))
bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE)
# forward propagation
for(i in 1:epoch){hidden_layer_input1= X%*%wh
hidden_layer_input=hidden_layer_input1+bh
hidden_layer_activations=sigmoid(hidden_layer_input)
output_layer_input1=hidden_layer_activations%*%wout
output_layer_input=output_layer_input1+bout
output= sigmoid(output_layer_input)# Back PropagationE=Y-output
slope_output_layer=derivatives_sigmoid(output)
slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)
d_output=E*slope_output_layer
Error_at_hidden_layer=d_output%*%t(wout)
d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer
wout= wout + (t(hidden_layer_activations)%*%d_output)*lr
bout= bout+rowSums(d_output)*lr
wh = wh +(t(X)%*%d_hiddenlayer)*lr
bh = bh + rowSums(d_hiddenlayer)*lr}
outputLet Wi be the weights between the input layer and the hidden layer. Wh be the weights between the hidden layer and the output layer.Now, h= (u)=  (WiX), i.e h is a function of u and u is a function of Wi and X. here we represent our function as Y=  (u)=  (Whh), i.e Y is a function of u and u is a function of Wh and h.We will be constantly referencing the above equations to calculate partial derivatives.We are primarily interested in finding two terms, E/Wi and E/Wh i.e change in Error on changing the weights between the input and the hidden layer and change in error on changing the weights between the hidden layer and the output layer.But to calculate both these partial derivatives, we will need to use the chain rule of partial differentiation since E is a function of Y and Y is a function of u and u is a function of Wi.Lets put this property to good use and calculate the gradients.E/Wh = (E/Y).( Y/u).( u/Wh), ..(1)We know E is of the form E=(Y-t)2/2.So, (E/Y)= (Y-t)Now,  is a sigmoid function and has an interesting differentiation of the form (1- ). I urge the readers to work this out on their side for verification.So, (Y/u)= ( (u)/ u= (u)(1- (u)).But, (u)=Y, So,(Y/u)=Y(1-Y)Now, ( u/Wh)= ( Whh)/ Wh = hReplacing the values in equation (1) we get,E/Wh = (Y-t). Y(1-Y).hSo, now we have computed the gradient between the hidden layer and the ouput layer. It is time we calculate the gradient between the input layer and the hidden layer.E/Wi =( E/ h). (h/u).( u/Wi)But, ( E/ h) = (E/Y).( Y/u).( u/h). Replacing this value in the above equation we get,E/Wi =[(E/Y).( Y/u).( u/h)]. (h/u).( u/Wi)(2)So, What was the benefit of first calculating the gradient between the hidden layer and the output layer?As you can see in equation (2) we have already computed E/Y and Y/u saving us space and computation time. We will come to know in a while why is this algorithm called the back propagation algorithm.Let us compute the unknown derivatives in equation (2).u/h = (Whh)/ h = Whh/u = ( (u)/ u= (u)(1- (u))But, (u)=h, So,(Y/u)=h(1-h)Now, u/Wi = (WiX)/ Wi = XReplacing all these values in equation (2) we get,E/Wi = [(Y-t). Y(1-Y).Wh].h(1-h).XSo, now since we have calculated both the gradients, the weights can be updated asWh = Wh +  . E/WhWi = Wi +  . E/WiWhere  is the learning rate.So coming back to the question: Why is this algorithm called Back Propagation Algorithm?The reason is: If you notice the final form of E/Wh and E/Wi , you will see the term (Y-t) i.e the output error, which is what we started with and then propagated this back to the input layer for weight updation.So, where does this mathematics fit into the code?hiddenlayer_activations=hE= Y-tSlope_output_layer = Y(1-Y)lr = slope_hidden_layer = h(1-h)wout = WhNow, you can easily relate the code to the mathematics.This article is focused on the building a Neural Network from scratch and understanding its basic concepts. I hope now you understand the working of a neural networklike how does forward and backward propagation work, optimization algorithms (Full Batch and Stochastic gradient descent), how to update weights and biases, visualization of each step in Excel and on top of that code in python and R.Therefore, in myupcoming article, Ill explain the applications of using Neural Network in Python and solving real-life challenges related to:I enjoyed writing this article and would love to learn from your feedback. Didyou findthis article useful? I would appreciate yoursuggestions/feedback.Please feel free to ask your questions through comments below.",https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Understanding and coding Neural Networks From Scratch in Python and R|Technical Lead  Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,"1. Use your knowledge of machine learning and classification algorithms to develop code that accurately predicts the output of complex engineering models.2. Apply pattern recognition technologies to recognize features in large GIS databases.
||1. 1-3.5 years of relevant work experience|2. B.Tech / B.E., MSc / M.Tech / ME.|3. Familiar with Java / J2EE.|4. Experience deploying / building a fast, distributed, and fault-tolerant AI infrastructure.","Experience : 1  3.5 years
Requirements : 
Task Info : Job Description and ResponsibilitiesDesired Candidate Profil

College Preference : no-bar
Min Qualification : ug
Skills : artificial intelligence, J2EE, java
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/artificial-intelligence-developer-ahmedabad-1-3-years-of-experience/
"Technical Lead  Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Artificial Intelligence Developer- Ahmedabad (1-3 Years Of Experience)|Sr. Consultant  Data Scientist-Bengaluru (9-14 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Job Responsibilities : To help designing, innovating and building our next generation ML architecture  Working with a group (Data Science & Machine Learning Group) of ML engineers, Data Scientists and Product Analysts  Define and drive API oriented solutions for data and machine learning services  6 years of full time programming experience within an operations or technical department.  3+ years of direct experience with multiple Agile teams.  Be able to distill business objectives into technical solutions through effective system design and architecture  Be able to work independently on a project-by project basis and also work in a collaborative and fast-paced team environment  Be able to provide technical and analytical solutions to evaluate the merits and challenges of a product idea  Create applications on both the server-side and on the web interface  Perform high complexity integration testing and validate all services integrate according to specifications  Responsible for prevention and early detection of defects through verification and validation activities ensuring the integrity and quality of all work products Industry : IT-Software / Software Services Functional Area : IT Software  DBA, Datawarehousing Role Category : Programming & Design Role : Team Lead/Technical Lead Keyskills : Programming, Machine Learning, Artificial Intelligence, Data Science, product engineering, Elastic Search, NLP, DB NoSql  MongoDB, Neo4J, MySql. Desired Candidate Profile Technology Competencies Must Have :  Java, Play, Scala.  DB/NoSql  MongoDB, Neo4J, MySql. Good to have :  Elastic Search, NLP background and Machine Learning Platforms  Strong End User focus, understanding the needs of both technical and non-technical end users. Non-Technical Competencies :  Think big & scale  Thought leadership in respective Technology domains  Passionate & Obsessive about problem solving  Self-starter, Complete Ownership and absolute pro-activeness  Fast Learning curve with eagerness to keep in pace with the tech trends Work Experience : More than 5 years of hands on product engineering experience
College Preference : no-bar
Min Qualification : ug
Skills : artificial intelligence, java, machine learning, MongoDB, nlp, Scala
Location : Bengaluru, Delhi, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/technical-lead-machine-learningartificial-intelligence-mumbai-bengaluru-delhi-2-6-years-of-experience/
Sr. Consultant  Data Scientist-Bengaluru (9-14 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Technical Lead  Machine Learning/artificial Intelligence- Mumbai, Bengaluru, Delhi (2- 6 Years Of Experience)|Launching Analytics Industry Report 2017  Trends and Salaries in India|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 9  14 years
Requirements : 
Task Info : Job Description:Expertise in AI, ML, Deep Learning, Text MiningExperience of creating a big data analytics using either AWS, Azure ML or Google ML Hands on expert level experience in Spark or StormExperience in image or video analytics Must have right attitude and right level of flexibility to in our kind of environment. (in contrast to LAB environment). Should be a good team PlayerShould have reasonable good communication skills and strong consulting mindset to engage with Global stakeholdersGood to have:Experience of working on Scala is useful but not essential Experience/exposure to Tensorflow will be an advantage Good to have experience in PySpark or something similar Academic Proficiency:Ph.D preferred Or, Masters in Mathematics/Statistics Or, ME/M.Tech in Computer Science Work Experience: Minimum 10 years 
College Preference : no-bar
Min Qualification : pg
Skills : artificial intelligence, aws, deep learning, machine learning, Scala, spark, storm, tensor flow, text mining
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/sr-consultant-data-scientist-bengaluru-9-14-years-of-experience/
Launching Analytics Industry Report 2017  Trends and Salaries in India,Learn everything about Analytics|Introduction|Analytics Industry Report 2017  Trends and Salaries in India|Key Insights in the report|A few trends from the report|Download Complete Report,"Share this:|Like this:|Related Articles|Sr. Consultant  Data Scientist-Bengaluru (9-14 Years of Experience)|Data scientist|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Let me start with laying out a recent experience:I was in middle of a large audienceup-skilling themselvesin analytics. Average experience in room was more than 5 years and people came from diverse background. My first question to the group:Kunal  How many of you are looking for a transition in Analytics / Data Science?As expected, a large percentage of the group raised their hands. It felt good that people are increasingly becoming aware about importance of analytics.The group has invested close to INR 4,00,000 (~$6,000) as part of the program they were undergoing. Given the maturity of the audience and the investment they had made, I was hoping that people would have done some real ground work before making the investment. So I asked:Kunal  How many jobs are there in India for people with advanced analytics / predictive modeling skills?To my surprise  there was no informed answer in the entire group! I thought may be I need to ask the question in a different manner  so I rephrased itKunal  Given that you all have made huge investment in upskilling yourself, you would have done some research about expected increase in your salary after this course. So, how much increase do you expect after you have undergone the course.Surprisingly and sadly  no informed answer again. There were expectations  but little research. Even more surprisingly, this was not a one off occurrence  this happens with most of the people I have been interacting in past few years.It was scenarios like these, which made us think what is the best best way to bring knowledge about Indian Analytics Industry to everyone. I am excited to announce launch of AnalyticsIndustry Report 2017  Trends and Salaries in India .I am pleased to announce the industry report 2017 for our followers and analytics industry in India. This is the most extensive effort I know of in Indian analytics industry. We, along with Jigsaw Academy have used all our experience / intelligence and perspective to come up with this report. The aim is to provide the ground realities to people about what is happening in industry.So, if you had questions like:This Industry report should help you answer these questions. We had launched aAnalytics and Big Data Salary Report 2016last year. This year, we decided that we will provide the best of insights and trends to make this a complete industry report. Hope you enjoy it.Click Here to go to Download ReportsMost in demand tools in industryAverage Salary by toolsAverage Salary by location  Indian cities
Salary by cities and experiencePlease enter your Email ID
Are you :

Recruiter
Data Science Professional

Your City

Mumbai
Delhi-NCR
Bengaluru
Chennai
Hyderabad
Kolkata
Pune
Ahmedabad
Indore
Others



* By filling this form you agree to receive jobs alerts and industry newsletter from Analytics Vidhya. Furthermore your account with Analytics Vidhya will also be created, the details of which shall be mailed to you.
Dont worry we will not spam you. ",https://www.analyticsvidhya.com/blog/2017/05/launching-analytics-industry-report-2017-trends-and-salaries-in-india/
Data scientist,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Launching Analytics Industry Report 2017  Trends and Salaries in India|Senior MSBI Developer- Ahmedabad (4-8 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Job Description and Responsibilities
College Preference : tier1-entire
Min Qualification : ug
Skills : 
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-scientist-2/
Senior MSBI Developer- Ahmedabad (4-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data scientist|ETL Developer- Pune (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  9 years
Requirements : 
Task Info : Job Description and ResponsibilitiesThis fulltime in house position is of Senior MSBI Developer (SSIS, SSAS, SSRS) with a software consulting company. We are looking for someone who can contribute to the full development lifecycle of reporting, database, data integration, cube and data warehousing solutions. Working with Microsoft SQL Server and the full Microsoft BI Stack (SSIS, SSAS, SSRS), you will be working on large scale BI projects.
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, sql server, sql server integration service (SSIS), SQL Server Reporting Services (SSRS)
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-msbi-developer-ahmedabad-4-8-years-of-experience/
ETL Developer- Pune (3-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior MSBI Developer- Ahmedabad (4-8 Years of Experience)|Business Intelligence Analyst- Bangalore (3-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : Job Description and Responsibilities Assist in the ongoing development of technical best practices for data movement, data quality, data cleansing and other ETL-related activities. Provide technical knowledge of Extract/Transform/Load (ETL) solutions for Business Intelligence projects Excellent written and verbal communication skills and be able to lead meetings with technical peers regarding the solution designs. Ability to communicate with the Business Analysts, Data Modelers and Solution Architects when it comes to converting the ETL design into specific development activities Ability to do ETL development and to supervise and guide ETL development activities of other developers Work closely with project Business Analyst, Data Modeler and BI Lead to ensure that the end to end designs meet the business and data requirements Work closely with Project Manager to develop and update the task plan for ETL work and to keep the manager aware of any critical task issues and dependencies on other teams Supervise, lead and assist the creation of the data layer Perform checkpoints/reviews on presentation layer designs and code being created by the ETL developers Ensure that developers are applying Schumacher Group standards in all deliverables Lead Unit and Integration Testing activities and assist in User Acceptance Testing as neededRequirements: Bachelors Degree in Computer Science, Information Systems, or other related field or equivalent work experience At least 5 years overall experience in IT applications development or consulting related to IT applications development At least 5 years in technical development and leadership roles on BI projects  with some experience along the way doing these types of activities: Designing and developing with ETL Tools Designing and developing with Relational Reporting Tools Managing or coordinating the time and activities of other people and other groups Demonstrated experience working on strategy projects and evaluating software in the Business Intelligence, Data Warehouse and/or Data Management space
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, Data Warehouse, etl
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/etl-developer-pune-3-7-years-of-experience/
Business Intelligence Analyst- Bangalore (3-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|ETL Developer- Pune (3-7 Years Of Experience)|A comprehensive beginners guide to Linear Algebra for Data Scientists|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : Job Description and Responsibilities
College Preference : tier1-entire
Min Qualification : open
Skills : business intelligence, oracle, Power point
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-intelligence-analyst-bangalore-3-6-years-of-experience/
A comprehensive beginners guide to Linear Algebra for Data Scientists,Learn everything about Analytics|Introduction|Table of contents|1. Motivation  Why learn Linear Algebra?|2. Representation ofproblems in Linear Algebra|3. Matrix|4. Solving the Problem||5. Eigenvalues and Eigenvectors|6. Singular Value Decomposition|7. End notes,"Scenario 1:|Scenario 2:|Scenario 3:|Scenario 4:|2.1 Visualise the problem|2.2 Lets complicate the problem|2.3 Planes|3.1 Terms related to Matrix|3.2 Basic operations on matrix|3.3 Representing equations in matrix form|4.1 Row Echelon form|4.2 Inverse of a Matrix|5.1 How to find Eigenvectors of a matrix?|5.2 Use of Eigenvectors in Data Science|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Business Intelligence Analyst- Bangalore (3-6 Years of Experience)|Business Analyst/senior Business Analyst  Data Analytics  BFSI Gurgaon (2-5 Years Of Experience)|
Vikas kumar Yadav
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",4.2.1 Finding Inverse of a matrix|4.2.2 Power of matrices|4.2.3 Application of inverse in Data Science,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most common questions we get on Analytics Vidhya is,How much maths do I need to learn to be a data scientist?Even though the question sounds simple, there is no simple answer to the the question. Usually, we say that you need to know basic descriptive and inferential statistics to start. That is good to start.But, once you have covered the basic concepts in machine learning, you will need to learn some more math. You need it to understand how these algorithms work. What are their limitations and in case they make any underlying assumptions. Now, there could be a lot of areas to study includingalgebra, calculus, statistics, 3-D geometry etc.If you get confused (like I did) and ask experts what should you learn at this stage, most of them would suggest / agree that you go ahead with Linear Algebra.But, the problem does not stop there. The next challenge is to figure out how to learn Linear Algebra. You can get lost in the detailed mathematics and derivation and learning them would not help as much! I went through that journey myself and hence decided to write this comprehensive guide.If you have faced this question about how to learn & what to learn in Linear Algebra  you are at the right place. Just follow this guide.And if youre looking to understand where linear algebra fits into the overall data science scheme, heres the perfect article:I would like to present 4 scenarios to showcase why learning Linear Algebra is important, if you are learning Data Science and Machine Learning.What do you see when you look at the image above? You most likely said flower, leaves -not too difficult. But, if I ask you to write that logic so that a computer can do the same for you  it will be a very difficult task (to say the least).You were able to identify the flower because the human brain has gone through million years of evolution. We do not understand what goes in the background to be able to tell whether the colour in the picture is red or black. We have somehow trained our brains to automatically perform this task.But making a computer do the same task is not an easy task, and is an active area of research in Machine Learning and Computer Science in general. But before we work on identifying attributes in an image, let us ponder over a particular question- How does a machine stores this image?You probably know that computers of today are designed to process only 0 and 1. So how can an image such as above with multiple attributes like colour be stored in a computer? This is achieved by storing the pixel intensities in a construct called Matrix.Then, this matrix can be processed to identify colours etc.So any operation which you want to perform on this image would likely use Linear Algebra and matrices at the back end.If you are somewhat familiar with the Data Science domain, you might have heard about the world XGBOOST  an algorithm employed most frequently by winners of Data Science Competitions. It stores the numeric data in the form of Matrix to give predictions. It enables XGBOOST to process data faster and provide more accurate results. Moreover, not just XGBOOST but various other algorithms use Matrices to store and process data.Deep Learning- the new buzz word in town employs Matrices to store inputs such as image or speech or text to give a state-of-the-art solution to these problems. Weights learned by a Neural Network are also stored in Matrices. Below is a graphical representation of weights stored in a Matrix.Another active area of research in Machine Learning is dealing with text and the most common techniques employed are Bag of Words, Term Document Matrix etc. All these techniques in a very similar manner store counts(or something similar) of words in documents and store this frequency count in a Matrix form to perform tasks like Semantic analysis, Language translation, Language generation etc.So, now you would understand the importance of Linear Algebra in machine learning. We have seen image, text orany data, in general, employing matrices to store and process data. This should be motivation enough to go through the material below to get you started on Linear Algebra. This is a relatively long guide, but it builds Linear Algebrafrom the ground up.Lets start with a simple problem. Supposethat price of 1 ball & 2 bat or 2 ball and 1 bat is 100 units. We need to find price of a ball and a bat.Suppose the price of a bat is Rs x and the price of a ball is Rs y. Values of x and y can be anything depending on the situation i.e. x and y are variables.Lets translate this in mathematical form 2x + y = 100 ...........(1)Similarly, for the second condition-x + 2y = 100 ..............(2)Now, to find the prices of bat and ball, we need the values of x and y such that it satisfies both the equations. The basic problem of linear algebra is to find these values of x and y i.e. the solution of a set of linear equations.Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors.The number of variables as well as the number of equations may vary depending upon the condition, but the representation is in form of matrices and vectors.It is usually helpful to visualize data problems. Let us see if that helps in this case.Linear equations represent flat objects. We will start with the simplest one to understand i.e. line. A line corresponding to an equation is the set of all the points which satisfy the given equation. For example,Points (50,0) , (0,100), (100/3,100/3) and (30,40) satisfy our equation (1) . So these points should lie on the line corresponding to our equation (1). Similarly, (0,50),(100,0),(100/3,100/3) are some of the points that satisfy equation (2).Now in this situation, we want both of the conditions to be satisfied i.e. the point which lies on both the lines. Intuitively, we want to find the intersection point of both the lines as shown in the figure below.Lets solve the problem by elementary algebraic operations like addition, subtraction and substitution.2x + y = 100 .............(1)x + 2y = 100 ..........(2)from equation (1)-y = (100- x)/2put value of y in equation (2)-x + 2*(100-x)/2 = 100......(3)Now, since the equation (3) is an equation in single variable x, it can be solved for x and subsequently y.That looks simple  lets go one step further and explore.Now, suppose you are given a set of three conditions with three variables each as given below and asked to find the values of all the variables. Lets solve the problem and see what happens.x+y+z=1.......(4)2x+y=1......(5)5x+3y+2z=4.......(6)From equation (4) we get,z=1-x-y....(7)Substituting value of z in equation(6), we get 5x+3y+2(1-x-y)=43x+y=2.....(8)Now, we can solve equations (8) and (5) as a case of two variables to find the values of x and y in the problem of bat and ball above. Once we knowx and y, we can use(7) to find the value of z.As you might see, adding an extra variable has tremendously increased our efforts for finding the solution of the problem. Now imagine having 10 variables and 10 equations. Solving 10 equations simultaneously can prove to be tedious and time consuming. Now dive into data science. We have millions of data points. How do you solve those problems?We have millions of data points in a real data set. It is going to be a nightmare to reach to solutions using the approach mentioned above. And imagine if we have to do it again and again and again. Its going to take ages before we can solve this problem. And now if I tell you that its just one part of the battle, what would you think? So, what should we do? Should we quit and let it go? Definitely NO. Then?Matrix is used to solve a large set of linear equations. But before we go further and take a look at matrices, lets visualise the physical meaning of our problem. Give a little bit of thought to the next topic. It directly relates to the usage of Matrices.A linear equation in 3 variables represents the set of all points whose coordinates satisfy the equations. Can you figure out the physical object represented by such an equation? Try to think of 2 variables at a time in any equation and then add the third one. You should figure out that it represents a three-dimensional analogue of line.Basically, a linear equation in three variables represents a plane. More technically, a plane is a flat geometric object which extends up to infinity.As in the case of a line, finding solutions to 3 variables linear equation means we want to find the intersection of those planes. Now can you imagine, in how many ways a set of three planes can intersect? Let me help you out. There are 4 possible cases Can you imagine the number of solutions in each case? Try doing this. Here is an aid picked from Wikipedia to help you visualise.So, what was the point of having you to visualise all graphs above?Normal humans like us and most of the super mathematicians can only visualise things in 3-Dimensions, and having to visualise things in 4 (or 10000) dimensions is difficultimpossible for mortals. So, how do mathematicians deal with higher dimensional data so efficiently? They have tricks up their sleeves and Matrices is one such trick employed by mathematicians to deal with higher dimensional data.Now lets proceed with our main focus i.e. Matrix.Matrix is a way of writing similar things together to handle and manipulate them as per our requirements easily. In Data Science, it is generally used to store information like weights in an Artificial Neural Network while training various algorithms. You will be able to understand my point by the end of this article.Technically, a matrix is a 2-D array of numbers (as far as Data Science is concerned). For example look at the matrix A below.Generally, rows are denoted by i and column are denoted by j. The elements are indexed by ith row and jth column.We denote the matrix by some alphabet e.g. A and its elements by A(ij).In above matrixA12 = 2To reach to the result, go along first row and reach to second column.Order of matrix  If a matrix has 3 rows and 4 columns, order of the matrix is 3*4 i.e. row*column.Square matrix  The matrix in which the number of rows is equal to the number of columns.Diagonal matrix  A matrix with all the non-diagonal elements equal to 0 is called adiagonal matrix.Upper triangular matrix  Square matrix with all the elements below diagonal equal to 0.Lower triangular matrix  Square matrix with all the elements above the diagonal equal to 0.Scalar matrix  Square matrix with all the diagonal elements equal to some constant k.Identity matrix  Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.Column matrix  The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.Row matrix  A matrix consisting only of row.Trace  It is the sum of all the diagonal elements of a square matrix.Lets play with matrices and realise the capabilities of matrix operations.Addition  Addition of matrices is almost similar to basic arithmetic addition. All you need is the order of all the matrices being added should be same. This point will become obvious once you will do matrix addition by yourself.Suppose we have 2 matrices A and B and the resultant matrix after the addition is C. ThenCij = Aij + BijFor example, lets take two matrices and solve them.A =B =Then,C =Observe that to get the elements of C matrix, I have added A and B element-wise i.e. 1 to 4, 3 to 5 and so on.Scalar Multiplication  Multiplication of a matrix with a scalar constant is called scalar multiplication. All we have to do in a scalar multiplication is to multiply each element of the matrix with the given constant. Suppose we have a constant scalar c and a matrix A. Then multiplying c with A gives-c[Aij] = [c*Aij]
Transposition  Transposition simply means interchanging the row and column index. For example-AijT= AjiTranspose is used in vectorized implementation of linear and logistic regression.Code in pythonCode in ROutputMatrix multiplicationMatrix multiplication is one of the most frequently used operations in linear algebra. We will learn to multiply two matrices as well as go through its important properties.Before landing to algorithms, there are a few points to be kept in mind.Dont worry if you cant get these points. You will be able to understand by the end of this section.Suppose, we are given two matrices A and B to multiply. I will write the final expression first and then will explain the steps.I have picked this image from Wikipedia for your better understanding.In the first illustration, we know that the order of the resulting matrix should be 3*3. So first of all, create a matrix of order 3*3. To determine (AB)ij , multiply each element of ith row of A with jth column of B one at a time and add all the terms. To help you understand element-wise multiplication, take a look at the code below.import numpy as npA=np.arange(21,30).reshape(3,3)
B=np.arange(31,40).reshape(3,3)A.dot(B)So, how did we get 2250 as first element of AB matrix? 2250=21*31+22*34+23*37.Similarly, for other elements.Code in RNotice the difference between AB and BA.Properties of matrix multiplicationABC = (AB)C = A(BC)import numpy as np
A=np.arange(21,30).reshape(3,3)
B=np.arange(31,40).reshape(3,3)
C=np.arange(41,50).reshape(3,3)temp1=(A.dot(B)).dot(C)temp2=A.dot((B.dot(C)))2. Matrix multiplication is not commutative i.e. AB and BA are not equal. We have verified this result above.Matrix multiplication is used in linear and logistic regression when we calculate the value of output variable by parameterized vector method. As we have learned the basics of matrices, its time to apply them.Let me do something exciting for you. Take help of pen and paper and try to find the value of the matrix multiplication shown belowIt can be verified very easily that the expression contains our three equations. We will name our matrices as A, X and Z.It explicitly verifies that we can write our equations together in one place asAX = ZNext step has to be solution methods.We will go through two methods to find the solution.Now, we will look in detail the two methods to solve matrix equations.Now you have visualised what an equation in 3 variables represents and had a warm up on matrix operations. Lets find the solution of the set of equations given to us to understand our first method of interest and explore it later in detail.I have already illustrated that solving the equations by substitution method can prove to be tedious and time taking. Our first method introduces you with a neater and more systematic method to accomplish the job in which, we manipulate our original equations systematically to find the solution. But what are those valid manipulations? Are there any qualifying criteria they have to fulfil? Well, yes. There are two conditions which have to be fulfilled by any manipulation to be valid.So, what are those manipulations?These points will become more clear once you go through the algorithm and practice it. The basic idea is to clear variables in successive equations and form an upper triangular matrix. Equipped with prerequisites, lets get started. But before that, it is strongly recommended to go through this link for better understanding.I will solve our original problem as an illustration. Lets do it in steps.What I have done is I have just concatenated the two matrices. The augmented matrix simply tells that the elements in a row are coefficients of x, y and z and last element in the row is right-hand side of the equation.Remember to make each leading coefficient, also called pivot equal to 1, by suitable manipulations; in this case multiplying row 2 with -1. Also, if a row consists of 0 only, it should be below each row which consists of a non-zero entry. The resulting form of Matrix is called Row Echelon form. Notice that the planes corresponding to new equations formed by manipulation are not equivalent. Doing these operations, we are just conserving the solution of equations and trying to reach to it.0*x+0*y+1*z=1
z=1Now retrieve equation (2) and put the value of z in it to find y. Do the same for equation (1).Isnt it pretty simple and clean?Lets ponder over another point. Will we always be able to make an upper triangular matrix which gives a unique solution? Are there different cases possible? Recall that planes can intersect in multiple ways. Take your time to figure it out and then proceed further.Different possible cases-Note that in last equation, 0=0 which is always true but it seems like we have got only 2 equations. One of the equations is redundant. In many cases, its also possible that the number of redundant equations is more than one. In this case, the number of solutions is infinite.Lets retrieve the last equation.0*x+0*y+0*z=40=4Is it possible? Very clear cut intuition is NO. But, does this signify something? Its analogous to saying that it is impossible to find a solution and indeed, it is true. We cant find a solution for such a set of equations. Can you think what is happening actually in terms of planes? Go back to the section where we saw planes intersecting and find it out.Note that this method is efficient for a set of 5-6 equations. Although the method is quite simple, if equation set gets larger, the number of times you have to manipulate the equations becomes enormously high and the method becomes inefficient.Rank of a matrix  Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.A set of vectors is linearly dependent if we can express at least one of the vectors as a linear combination of remaining vectors in the set.For solving a large number of equations in one go, the inverse is used. Dont panic if you are not familiar with the inverse. We will do a good amount of work on all the required concepts. Lets start with a few terms and operations.Determinant of a Matrix  The concept of determinant is applicable to square matrices only. I will lead you to the generalised expression of determinant in steps. To start with, lets take a 2*2 matrix A.For now, just focus on 2*2 matrix. The expression of determinant of the matrix A will be:det(A) =a*d-b*cNote that det(A) is a standard notation for determinant. Notice that all you have to do to find determinant in this case is to multiply diagonal elements together and put a positive or negative sign before them. For determining the sign, sum the indices of a particular element. If the sum is an even number, put a positive sign before the multiplication and if the sum is odd, put a negative sign. For example, the sum of indices of element a11 is 2. Similarly the sum of indices of element d is 4. So we put a positive sign before the first term in the expression. Do the same thing for the second term yourself.Now take a 3*3 matrix B and find its determinant.I am writing the expression first and then will explain the procedure step by step.Each term consists of two parts basically i.e. a submatrix and a coefficient. First of all, pick a constant. Observe that coefficients are picked from the first row only. To start with, I have picked the first element of the first row. You can start wherever you want. Once you have picked the coefficient, just delete all the elements in the row and column corresponding to the chosen coefficient. Next, make a matrix of the remaining elements; each one in its original position after deleting the row and column and find the determinant of this submatrix . Repeat the same procedure for each element in the first row. Now, for determining the sign of the terms, just add the indices of the coefficient element. If it is even, put a positive sign and if odd, put a negative sign. Finally, add all the terms to find the determinant. Now, lets take a higher order matrix C and generalise the concept.Try to relate the expression to what we have done already and figure out the final expression.Code in pythonimport numpy as np
 #create a 4*4 matrix
 arr = np.arange(100,116).reshape(4,4)#find the determinant
 np.linalg.det(arr)Code in RMinor of a matrixLets take a square matrix A. then minor corresponding to an element A(ij) is the determinant of the submatrix formed by deleting the ith row and jth column of the matrix. Hope you can relate with what I have explained already in the determinant section. Lets take an example.To find the minor corresponding to element A11, delete first row and first column to find the submatrix.Now find the determinant of this matrix as explained already. If you calculate the determinant of this matrix, you should get 4. If we denote minor by M11, thenM11 = 4Similarly, you can do for other elements.Cofactor of a matrixIn the above discussion of minors, if we consider signs of minor terms, the resultant we get is called cofactor of a matrix. To assign the sign, just sum the indices of the corresponding element. If it turns out to be even, assign positive sign. Else assign negative. Lets take above illustration as an example. If we add the indices i.e. 1+1=2, so we should put a positive sign. Lets say it C11. ThenC11 = 4You should find cofactors corresponding to other elements by yourself for a good amount of practice.Cofactor matrixFind the cofactor corresponding to each element. Now in the original matrix, replace the original element by the corresponding cofactor. The matrix thus found is called the cofactor matrix corresponding to the original matrix.For example, lets take our matrix A. if you have found out the cofactors corresponding to each element, just put them in a matrix according to rule stated above. If you have done it right, you should get cofactor matrixAdjoint of a matrix  In our journey to find inverse, we are almost at the end. Just keep hold of the article for a couple of minutes and we will be there. So, next we will find the adjoint of a matrix.Suppose we have to find the adjoint of a matrix A. we will do it in two steps.In step 1, find the cofactor matrix of A.In step 2, just transpose the cofactor matrix.The resulting matrix is the adjoint of the original matrix.For illustration, lets find the adjoint of our matrix A. we already have cofactor matrix C. Transpose of cofactor matrix should beFinally, in the next section, we will find the inverse.Do you remember the concept of the inverse of a number in elementary algebra? Well, if there exist two numbers such that upon their multiplication gives 1 then those two numbers are called inverse of each other. Similarly in linear algebra, if there exist two matrices such that their multiplication yields an identity matrix then the matrices are called inverse of each other. If you can not get what I explained, just go with the article. It will come intuitively to you. The best way to learning is learning by doing. So, lets jump straight to the algorithm for finding the inverse of a matrix A. Again, we will do it in two steps.Step 1: Find out the adjoint of the matrix A by the procedure explained in previous sections.Step2:Multiply the adjoint matrix by the inverse of determinant of the matrix A. The resulting matrix is the inverse of A.For example, lets take our matrix A and find its inverse. We already have the adjoint matrix. Determinant of matrix A comes to be -2. So, its inverse will beNow suppose that the determinant comes out to be 0. What happens when we invert the determinant i.e. 0? Does it make any sense? It indicates clearly that we cant find the inverse of such a matrix. Hence, this matrix is non-invertible. More technically, this type of matrix is called a singular matrix.Keep in mind that the resultant of multiplication of a matrix and its inverse is an identity matrix. This property is going to be used extensively in equation solving.Inverse is used in finding parameter vector corresponding to minimum cost function in linear regression.What happens when we multiply a number by 1? Obviously it remains the same. The same is applicable for an identity matrix i.e. if we multiply a matrix with an identity matrix of the same order, it remains same.Lets solve our original problem with the help of matrices. Our original problem represented in matrix was as shown belowAX = Z i.e.What happens when we pre multiply both the sides with inverse of coefficient matrix i.e. A. Lets find out by doing.A-1 A X =A-1 ZWe can manipulate it as,(A-1 A) X = A -1ZBut we know multiply a matrix with its inverse gives an Identity Matrix. So,IX = A -1ZWhere I is the identity matrix of the corresponding order.If you observe keenly, we have already reached to the solution. Multiplying identity matrix to X does not change it. So the equation becomesX = A -1ZFor solving the equation, we have to just find the inverse. It can be very easily done by executing a few lines of codes. Isnt it a really powerful method?Code for inverse in pythonimport numpy as np
#create an array arr1
arr1 = np.arange(5,21).reshape(4,4)#find the inverse
np.linalg.inv(arr1)Inverse is used to calculate parameter vector by normal equation in linear equation. Here is an illustration. Suppose we are given a data set as shown below-It describes the different variables of different baseball teams to predict whether it makes to playoffs or not. But for right now to make it a regression problem, suppose we are interested in predicting OOBP from the rest of the variables. So, OOBP is our target variable. To solve this problem using linear regression, we have to find parameter vector. If you are familiar with Normal equation method, you should have the idea that to do it, we need to make use of Matrices. Lets proceed further and denote our Independent variables below as matrix X.This data is a part of a data set taken from analytics edge. Here is the link for the data set.so, X=To find the final parameter vector() assuming our initial function is parameterised by  and X , all you have to do is to find the inverse of (XT X) which can be accomplished very easily by using code as shown below.First of all, let me make the Linear Regression formulation easier for you to comprehend.f  (X)= T X, where is the parameter we wish to calculate and X is the column vector of features or independent variables.import pandas as pd
import numpyas np#you dont need to bother about the following. It just #transforms the data from original source into matrixDf = pd.read_csv( ""../baseball.csv)
Df1 = df.head(14)# We are just taking 6 features to calculate.
X = Df1[[RS, RA, W, OBP,'SLG','BA']]
Y=Df1['OOBP']#Converting X to matrix
X = np.asmatrix(X)#taking transpose of X and assigning it to x
x= np.transpose(X)#finding multiplication
T= x.dot(X)#inverse of T - provided it is invertible otherwise we use pseudoinverse
inv=np.linalg.inv(T)#calculating
theta=(inv.dot(X.T)).dot(Y)Imagine if you had to solve this set of equations without using linear algebra. Let me remind you that this data set is less than even 1% of original date set. Now imagine if you had to find parameter vector without using linear algebra. It would have taken a lots of time and effort and could be even impossible to solve sometimes.One major drawback of normal equation method when the number of features is large is that it is computationally very costly. The reason is that if there are n features, the matrix (XT X) comes to be the order n*n and its solution costs time of order O( n*n*n). Generally, normal equation method is applied when a number of features is of the order of 1000 or 10,000. Data sets with a larger number of features are handled with the help another method called Gradient Descent.Next, we will go through another advanced concept of linear algebra called Eigenvectors.Eigenvectors find a lot of applications in different domains like computer vision, physics and machine learning. If you have studied machine learning and are familiar with Principal component analysis algorithm, you must know how important the algorithm is when handling a large data set. Have you ever wondered what is going on behind that algorithm? Actually, the concept of Eigenvectors is the backbone of this algorithm. Let us explore Eigen vectors and Eigen values for a better understanding of it.Lets multiply a 2-dimensional vector with a 2*2 matrix and see what happens.This operation on a vector is called linear transformation. Notice that the directions of input and output vectors are different. Note that the column matrix denotes a vector here.I will illustrate my point with the help of a picture as shown below.In the above picture, there are two types of vectors coloured in red and yellow and the picture is showing the change in vectors after a linear transformation. Note that on applying a linear transformation to yellow coloured vector, its direction changes but the direction of the red coloured vector doesnt change even after applying the linear transformation. The vector coloured in red is an example of Eigenvector.Precisely, for a particular matrix; vectors whose direction remains unchanged even after applying linear transformation with the matrix are called Eigenvectors for that particular matrix. Remember that the concept of Eigen values and vectors is applicable to square matrices only. Another thing to know is that I have taken a case of two-dimensional vectors but the concept of Eigenvectors is applicable to a space of any number of dimensions.Suppose we have a matrix A and an Eigenvector x corresponding to the matrix. As explained already, after multiplication with matrix the direction of x doesnt change. Only change in magnitude is permitted. Let us write it as an equation-Ax = cx(A-c)x = 0 .(1)Please note that in the term (A-c), c denotes an identity matrix of the order equal to A multiplied by a scalar cWe have two unknowns c and x and only one equation. Can you think of a trick to solve this equation?In equation (1), if we put the vector x as zero vector, it makes no sense. Hence, the only choice is that (A-c) is a singular matrix. And singular matrix has a property that its determinant equals to 0. We will use this property to find the value of c.Det(A-c) = 0Once you find the determinant of the matrix (A-c) and equate to 0, you will get an equation in c of the order depending upon the given matrix A. all you have to do is to find the solution of the equation. Suppose that we find solutions as c1 , c2 and so on. Put c1 in equation (1) and find the vector x1 corresponding to c1. The vector x1 that you just found is an Eigenvector of A. Now, repeat the same procedure with c2, c3 and so on.Code for finding EigenVectors in pythonimport numpy as np#create an array
arr = np.arange(1,10).reshape(3,3)#finding the Eigenvalue and Eigenvectors of arr
np.linalg.eig(arr)Code in R for finding Eigenvalues and Eigenvectors:OutputThe concept of Eigenvectors is applied in a machine learning algorithm Principal Component Analysis. Suppose you have a data with a large number of features i.e. it has a very high dimensionality. It is possible that there are redundant features in that data. Apart from this, a large number of features will cause reduced efficiency and more disk space. What PCA does is that it craps some of lesser important features. But how to determine those features? Here, Eigenvectors come to our rescue.Lets go through the algorithm of PCA. Suppose we have an n dimensional data and we want to reduce it to k dimensions. We will do it in steps.Step 1: Data is mean normalised and feature scaled.Step 2:We find out the covariance matrix of our data set.Now we want to reduce the number of features i.e. dimensions. But cutting off features means loss of information. We want to minimise the loss of information i.e. we want to keep the maximum variance. So, we want to find out the directions in which variance is maximum. We will find these directions in the next step.Step 3:We find out the Eigenvectors of the covariance matrix. You dont need to bother much about covariance matrix. Its an advanced concept of statistics. As we have data in n dimensions, we will find n Eigenvectors corresponding to n Eigenvalues.Step 4: We will select k Eigenvectors corresponding to the k largest Eigenvalues and will form a matrix in which each Eigenvector will constitute a column. We will call this matrix as U.Now its the time to find the reduced data points. Suppose you want to reduce a data point a in the data set to k dimensions. To do so, you have to just transpose the matrix U and multiply it with the vector a. You will get the required vector in k dimensions.Once we are done with Eigenvectors, lets talk about another advanced and highly useful concept in Linear algebra called Singular value decomposition, popularly called as SVD. Its complete understanding needs a rigorous study of linear algebra. In fact, SVD is a complete blog in itself. We will come up with another blog completely devoted to SVD. Stay tuned for a better experience. For now, I will just give you a glimpse of how SVD helps in data science.Suppose you are given a feature matrix A. As suggested by name, what we do is we decompose our matrix A in three constituent matrices for a special purpose. Sometimes, it is also said that svd is some sort of generalisation of Eigen value decomposition. I will not go into its mathematics for the reason already explained and will stick to our plan i.e. use of svd in data science.Svd is used to remove the redundant features in a data set. Suppose you have a data set which comprises of 1000 features. Definitely, any real data set with such a large number of features is bound to contain redundant features. if you have run ML, you should be familiar with the fact that Redundant features cause a lots of problems in running machine learning algorithms. Also, running an algorithm on the original data set will be time inefficient and will require a lot of memory. So, what should you to do handle such a problem? Do we have a choice? Can we omit some features? Will it lead to significant amount of information loss? Will we be able to get an efficient enough algorithm even after omitting the rows? I will answer these questions with the help of an illustration.Look at the pictures shown below taken from this linkWe can convert this tiger into black and white and can think of it as a matrix whose elements represent the pixel intensity as relevant location. In simpler words, the matrix contains information about the intensity of pixels of the image in the form of rows and columns. But, is it necessary to have all the columns in the intensity matrix? Will we be able to represent the tiger with a lesser amount of information? The next picture will clarify my point. In this picture, different images are shown corresponding to different ranks with different resolution. For now, just assume that higher rank implies the larger amount of information about pixel intensity. The image is taken from this linkIt is clear that we can reach to a pretty well image with 20 or 30 ranks instead of 100 or 200 ranks and thats what we want to do in a case of highly redundant data. What I want to convey is that to get a reasonable hypothesis, we dont have to retain all the information present in the original dataset. Even, some of the features cause a problem in reaching a solution to the best algorithm. For the example, presence of redundant features causes multi co-linearity in linear regression. Also, some features are not significant for our model. Omitting these features helps to find a better fit of algorithm along with time efficiency and lesser disk space. Singular value decomposition is used to get rid of the redundant features present in our data.If you have made this far  give yourself a pat at the back. We have covered different aspects of Linear algebra in this article. I have tried to give sufficient amount of information as well as keep the flow such that everybody can understand the concepts and be able to do necessary calculations. Still, if you get stuck somewhere, feel free to comment below or post on discussion portal.",https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/
Business Analyst/senior Business Analyst  Data Analytics  BFSI Gurgaon (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|A comprehensive beginners guide to Linear Algebra for Data Scientists|Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and Responsibilities 2-5 years of total experience of Data Analytics / Data Insights Generation  Proven experience in delivering Analytics Solutions  Experience in credit card / banking industry required  Hands on experience on SAS and Excel/VBA  Good communication/ problem solving/ analytical bent of mind.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, bfsi, Data analytics, excel, sas, VBA
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analystsenior-business-analyst-data-analytics-bfsi-gurgaon-2-5-years-of-experience/
Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst/senior Business Analyst  Data Analytics  BFSI Gurgaon (2-5 Years Of Experience)|Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  12 years
Requirements : 
Task Info : Key Roles & Responsibilities:Key Requirements  Experience & Skills
College Preference : no-bar
Min Qualification : ug
Skills : analytics, bfsi, business analysis, business analytics, insurance, risk
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-manager-business-analyst-insurance-domain-gurgaon-8-12-years-of-experience/
"Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Manager- Business Analyst (insurance Domain)- Gurgaon (8-12 Years Of experience)|25 Must Know Terms & concepts for Beginners in Deep Learning|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  5 years
Requirements : 
Task Info : Responsibilities:Responsible for understanding Worldwide Alliance &Channel(WWA&C) business objectives and delivering the reporting needs to the business.Responsible for analyzing partner programs and sales data to measure key performance indicator.Responsible for creating and maintaining sales pipeline and forecasting reports for WWA&C.Manage organizational relationships and partner across different groups to drive reporting solutions in different areas within WWA&CResponsible for process documentation for all of BI systems with WWA&C.Support testing and report updates to management.Collaborate with subject matter experts from different LOBs to standardize reporting within OracleDeveloping and presenting proposals of project plan and getting agreement from appropriate stakeholders.Lead contributor individually and as a team member, providing direction and mentoring to others.Skills / Competencies Required:Good working knowledge of Oracle systems, Oracle Sales Cloud(OSC) , Oracle EBusiness SuiteMust have strong analytical skillsMust have experience with analytical reporting on OBIEEGood SQL skills to be able to query databases for data analysisBasic understanding of data model schemas, ETL programming using PL/SQL.Experience with Data Visualization tools (Oracle Data Visualization , Tableau etc..) would be a plus.Strong knowledge of MS Office, in particular the ability to use Excel for data analysis and data manipulationShould be able to do functional and technical presentations to the team.Good communication skills required: must be responsive, clear, and conciseGood problem-solving and analytical skillsStrong levels of enthusiasm, commitment, and energy requiredStrong team ethic: promote team spirit and motivate / lead as and when necessary to ensure good project outcomesStrong planning and coordinating skillsAbility to work under pressure and meet deadlines in a fast-paced environmentPrior experience working in an international environment is an advantageProcess and workflow improvement experience is an advantageBasic Qualifications:Bachelors in Computer Science or equivalent with 4  5 years of related experienceEducation:UG -B.Tech/B.E.  Any Specialization, Computers, B.Sc  Computers, BCA  Computers
College Preference : no-bar
Min Qualification : ug
Skills : business analytics, data visualization, etl, oracle, PLSQL, sql, tableau
Location : Bengaluru, Gurugram, Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/business-analyst-noida-bengaluru-gurgaon-4-5-years-of-experience/
25 Must Know Terms & concepts for Beginners in Deep Learning,Learn everything about Analytics|Who should read this article?|Terms related to topics:|Basics of Neural Networks|Convolutional Neural Networks|Recurrent Neural Network|End Notes,"Commonly applied Activation Functions|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Business Analyst -Noida, Bengaluru, Gurgaon- (4-5 Years Of Experience)|Senior Analyst / Manager  Qlikview Developer- Mumbai (3-8 Years of Experience)|
Dishashree Gupta
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,Source: Wikipedia|source: cs231n|Source: cs231n|Source: Original paper|Source: cs231n|Source: cs231n|Source: cs231n,"Artificial Intelligence, deep learning, machine learningwhatever youre doing if you dont understand itlearn it. Because otherwise youre going to be a dinosaur within 3years.-Mark CubanThis statement from Mark Cuban might sound drastic  but its message is spot on! We are in the middle of a revolution  a revolution caused by Big Huge data and a ton of computational power.For a minute, think how a person would feel in early 20th century if he / she did not understand electricity. You would have been used to doing things in a particular manner for ages and all of a sudden things around you started changing. Things which required many people can now be done with one person and electricity. We are going through a similar journey with machine learning & deep learning today.So, if you havent explored or understood the power of deep learning  you should start it today. I have written this article to help you understand common terms used in deep learning.If you are some one who wants to learn or understand deep learning, this article is meant for you. In this article, I will explain various terms used commonly in deep learning.If you are wondering why I am writing this article  I am writing it because I want you to start your deep learning journey without hassle or without getting intimidated.When I first began reading about deep learning, there were several terms I had heard about, but it was intimidating when I tried to understand them. There are several words which are recurring when we start reading about any deep learning application.In this article, I have created something like a deep learning dictionary for you which you can refer whenever you need the basic definition of the most common terms used. I hope after this article these terms wouldnt haunt you anymore.To help you understand various terms, I have broken them in 3 different groups. If you are looking for a specific term, you can skip to that section. If you are new to the domain, I would recommend that you go through them in the order I have written them.1) Neuron- Just like a neuron forms the basic elementof our brain, a neuron forms the basic structure of a neural network. Just think of what we do when we get new information. When we get the information, we process it and then we generate an output. Similarly, in case of a neural network, a neuron receives an input, processes it and generates an output which is either sent to other neurons for further processing or it is the final output.2) Weights  When input enters the neuron, it is multiplied by a weight. For example, if a neuron has two inputs, then each input will have has an associated weight assigned to it. We initialize the weights randomly and these weights are updated during the model training process. The neural network after training assigns a higher weight to the input it considers more important as compared to the ones which are considered less important. A weight of zero denotes that the particular feature is insignificant.Lets assume the input to be a, and the weight associated to be W1. Then after passing through the node the input becomes a*W13) Bias  In addition to the weights, another linear component is applied to the input, called as the bias. It is added to the result of weight multiplication to the input. The bias is basically added to change the range of the weight multiplied input. After adding the bias, the result would look like a*W1+bias. This is the final linear component of the input transformation.4) Activation Function  Once the linear component is applied to the input, a non-linear function is applied to it. This is done by applying the activation function to the linear combination.The activation function translates the input signals to output signals. The output after application of the activation function would look something like f(a*W1+b) where f() is the activation function.In the below diagram we have n inputs given as X1 to Xn and corresponding weights Wk1 to Wkn. We have a bias given as bk. The weights are first multiplied to its corresponding input and are then added together along with the bias. Let this be called as u.u=w*x+bThe activation function is applied to u i.e. f(u) and we receive the final output from the neuron as yk = f(u)The most commonly applied activation functions are  Sigmoid, ReLU and softmaxa) Sigmoid  One of the most common activation functions used is Sigmoid. It is defined as:The sigmoid transformation generates a more smooth range of values between 0 and 1. We might need to observe the changes inthe output with slight changes in the input values. Smooth curves allow us to do that and are hence preferred over step functions.b) ReLU(Rectified Linear Units)  Instead of sigmoids, the recent networks prefer using ReLu activation functions for the hidden layers. The function is defined as:The output of the function is X when X>0 and 0 for X<=0. The function looks like this:The major benefit of using ReLU is that it has a constant derivative value for all inputs greater than 0. The constant derivative value helps the network to train faster.c) Softmax  Softmax activation functions are normally used in the output layer for classification problems. It is similar to the sigmoid function, with the only difference being that the outputs are normalized to sum up to 1. The sigmoid function would work in case we have a binary output, however in case we have a multiclass classification problem, softmax makes it really easy to assign values to each class which can be easily interpreted as probabilities.Its very easy to see it this way  Suppose youre trying to identify a 6 which might also look a bit like 8. The function would assign values to each number as below. We can easily see that the highest probability is assigned to 6, with the next highest assigned to 8 and so on5) Neural Network  Neural Networks form the backbone of deep learning.The goal of aneural network is to find an approximation of an unknown function. It is formed by interconnected neurons. These neurons have weights, and bias which is updated during the network training depending upon the error. The activation function puts a nonlinear transformation to the linear combination which then generates the output. The combinations of the activated neurons give the output.A neural network is best defined by Liping Yang as Neural networks are made up of numerous interconnected conceptualized artificial neurons, which pass data between themselves, and which have associated weights which are tuned based upon the networks experience. Neurons have activation thresholds which, if met by a combination of their associated weights and data passed to them, are fired; combinations of fired neurons result in learning.6) Input / Output / Hidden Layer  Simply as the name suggests the input layer is the one which receives the input and is essentially the first layer of the network. The output layer is the one which generates the output or is the final layer of the network. The processing layers are the hidden layers within the network. These hidden layers are the ones which perform specific tasks on the incoming data and pass on the output generated by them to the next layer. The input and output layers are the ones visible to us, while are the intermediate layers are hidden.7) MLP (Multi Layer perceptron) A single neuron would not be able to perform highly complex tasks. Therefore, we use stacks ofneurons togenerate the desired outputs. In the simplest network we would have an input layer, a hidden layer and an output layer. Each layer has multiple neurons and all the neurons in each layer are connected to all the neurons in the next layer. These networks can also be called as fully connected networks.8) Forward Propagation  Forward Propagation refers to the movement of the input through the hidden layers to the output layers. In forward propagation, the information travels in a single direction FORWARD. The input layer supplies the input to the hidden layers and then the output is generated. There is no backward movement.9) Cost Function  When we build a network, the network tries to predict the output as close as possible to the actual value. We measure this accuracy of the network using the cost/loss function. The cost or loss function tries to penalize the network when it makes errors.Our objective while running the network is to increase our prediction accuracy and to reduce the error, hence minimizing the cost function. The most optimized output is the one with least value of the cost or loss function.If I define the cost function to be the mean squared error, it can be written as C= 1/m (y  a)2where m is the number of training inputs, a is the predicted value and y is the actual value of that particular example.The learning process revolves around minimizing the cost.10) Gradient Descent  Gradient descent is an optimization algorithm for minimizing the cost. To think of it intuitively, while climbing down a hill you should take small steps and walk down instead of just jumping down at once. Therefore, what we do is, if we start from a point x, we move down a little i.e. delta h, and update our position to x-delta h and we keep doing the same till we reach the bottom. Consider bottom to be the minimum cost point.SourceMathematically, to find the local minimum of a function one takes steps proportional to the negative of the gradient of the function.You can go through this article for a detailed understanding of gradient descent.11) Learning Rate  The learning rate is defined as the amount of minimization in the cost function in each iteration. In simple terms, the rate at which we descend towards the minima of the cost function is the learning rate. We should choose the learning rate very carefully since it should neither be very large that the optimal solution is missed and nor should be very low that it takes forever for the network to converge.Source12) Backpropagation  When we define a neural network, we assign random weights and bias values to our nodes. Once we have received the output for a single iteration, we can calculate the error of the network. This error is then fed back to the network along with the gradient of the cost function to update the weights of the network. These weights are then updated so that the errors in the subsequent iterations is reduced. This updating of weights using the gradient of the cost function is known as back-propagation.In back-propagation the movement of the network is backwards, the error along with the gradient flows back from the out layer through the hidden layers and the weights are updated.13) Batches  While training a neural network, instead of sending the entire input in one go, we divide in input into several chunks of equal size randomly. Training the data on batches makes the model more generalized as compared to themodel built when the entire data set is fed to the network in one go.14) Epochs  An epoch is defined as a single training iteration of all batches in both forward and back propagation. This means 1 epoch is a single forward and backward pass of the entire input data.The number of epochs you would use to train your network can be chosen by you. Its highly likely that more number of epochs would show higher accuracy of the network, however, it would also take longer for the network to converge. Also you must take care that if the number of epochs are too high, the network might be over-fit.15) Dropout  Dropout is a regularization technique which prevents over-fitting of the network. As the name suggests, during training a certain number of neurons in the hidden layer is randomly dropped. This means that the training happens on several architectures of the neural network on different combinations of the neurons. You can think of drop out as an ensemble technique, where the output of multiple networks is then used to produce the final output.16) Batch Normalization  As a concept, batch normalization can be considered as a dam we have set as specific checkpoints in a river. This is done to ensure that distribution of data is the same as the next layer hoped to get. When we are training the neural network, the weights are changed after each step of gradient descent. This changes the how the shape of data is sent to the next layer.But the next layer was expecting the distribution similar to what it had previously seen. So we explicitly normalize the data before sending it to the next layer.17) Filters  A filter in a CNN is like a weight matrix with which we multiply a part of the input image to generate a convoluted output. Lets assume we have an image of size 28*28. We randomly assign a filter of size 3*3, which is then multiplied with different 3*3 sections of the image to form what is known as a convoluted output. The filter size is generally smaller than the original image size. The filter values are updated like weight values during backpropagation for cost minimization.Consider the below image. Here filter is a 3*3 matrixwhich is multiplied with each 3*3 section of the image to form the convolved feature.18) CNN (Convolutional neural network)  Convolutional neural networks are basically applied on image data. Suppose we have an input of size (28*28*3), If we use a normal neural network, there would be 2352(28*28*3) parameters. And as the size of the image increases the number of parameters becomes very large. We convolve the imagesto reduce the number of parameters (as shown above in filter definition). As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the output of that filter at every position. We will stack these activation maps along the depth dimension and produce the output volume.You can see the below diagram for a clearer picture.19) Pooling  It is common to periodically introduce pooling layers in between the convolution layers. This is basically done to reduce a number of parameters and prevent over-fitting. The most common type of pooling is a pooling layer of filter size(2,2) using the MAX operation. What it would do is, it would take the maximum of each 4*4 matrix of the original image.You can also pool using other operations like Average pooling, but max pooling has shown to work better in practice.20) Padding  Padding refers to adding extra layer of zeros across the images so that theoutput image has the same size as the input. This is known as same padding.After the application of filters the convolved layer in the case of same padding has the size equal to the actual image.Valid padding refers to keeping the image as such an having all the pixels of the image which are actual or valid. In this case after the application of filters the size of the length and the width of the output keeps getting reduced at each convolutional layer.21) Data Augmentation  Data Augmentation refers to the addition of new data derived from the given data, which might prove to be beneficial for prediction. For example, it might be easier to view the cat in a dark image if you brighten it, or for instance, a 9 in the digit recognition might be slightly tilted or rotated. In this case, rotation would solve the problem and increase the accuracy of our model. By rotating or brightening were improving the quality of our data. This is known as Data augmentation.22) Recurrent Neuron  A recurrent neuron is one in which the output of the neuron is sent back to it for t time stamps. If you look at the diagram the output is sent back as input t times. The unrolled neuron looks like t different neurons connected together. The basic advantage of this neuron is that it gives a more generalized output.23) RNN(Recurrent Neural Network)  Recurrent neural networks are used especially for sequential data where the previous output is used to predict the next one. In this case the networks have loops within them. The loops within the hidden neuron gives them the capability to store information about the previous words for some time to be able to predict the output.The output of the hidden layer is sent again to the hidden layer for t time stamps. The unfolded neuron looks like the above diagram. The output of the recurrent neuron goes to the next layer only after completing all the time stamps. The output sent is more generalized and the previous information is retained for a longer period.The error is then back propagated according to the unfolded network to update the weights. This is known as backpropagation through time(BPTT).24) Vanishing Gradient Problem  Vanishing gradient problem arises in cases where the gradient of the activation function is very small. During back propagation when the weights are multiplied with these low gradients, they tend to become very small and vanish as they go further deep in the network. This makes the neural network to forget the long range dependency. This generally becomes a problem in cases of recurrent neural networks where long term dependencies are very important for the network to remember.This can be solved by using activation functions like ReLu which do not have small gradients.25) Exploding Gradient Problem  This is the exact opposite of the vanishing gradient problem, where the gradient of the activation function is too large. During back propagation, it makes the weight of a particular node very high with respect to the others rendering them insignificant. This can be easily solved by clipping the gradient so that it doesnt exceed a certain value.I hope you enjoyed going through the article. I have given a high level overview of the basic deep learning terms. I hope you now have a basic understanding of these terms. I have tried to explain everything in a language as easy as possible, however in case of any doubts/clarifications, please feel free to drop in your comments.",https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/
Senior Analyst / Manager  Qlikview Developer- Mumbai (3-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|25 Must Know Terms & concepts for Beginners in Deep Learning|Machine Learning Engineer- Gurugram (4+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  8 years
Requirements : 
Task Info : Job Description and Responsibilities:Involved in Requirement gathering & designing QlikView Applications.Creating and Managing QlikView Reports & Dashboards.Implemented Security & involved in Deployment of QlikView Applications.experience in VBAWriting SQL Scripts to load the relevant data in QlikView Applications.Designing Data Modeling and Loading data from Multiple Data sources.Performance tuning by analyzing and comparing the turnaround times between SQL and QlikView.Designed and developed extensive QlikView reports using combination of charts and tables.Publishing and Deploying the Dashboards based on business requirements.Developing Set Analysis to provide the custom functionality in QlikView Applications.Testing Applications, reports using review checklists, Data quality for quality assurance before delivering to the end users.Involved in complex mappings like Slowly Changing Dimensions.
College Preference : no-bar
Min Qualification : ug
Skills : data modeling, qlikview, sql, VBA
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/senior-analyst-manager-qlikview-developer-mumbai-3-8-years-of-experience/
Machine Learning Engineer- Gurugram (4+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst / Manager  Qlikview Developer- Mumbai (3-8 Years of Experience)|Data Scientist- Mumbai (5 Yrs of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : Company Description:We are a software solutions agency that combines skills of consulting, design, development and management to deliver high-quality mobile and web applications.We offers services for offshore development and application support, coupled with attention to the rapid and clear communications and project management.Job Description Creating Scalable Machine Learning systems that are highly performant  Identifying patterns in data streams and generating actionable insights  Customizing Machine Learning (ML) algorithms for in-house use  Writing production quality code and libraries that can be packaged, installed and deployed  Maintain and improve existing in-house tools and code. Optimize for speed and efficiency Qualifications Experience of handling various data types and structures: structured and unstructured data, extensive prior experience in integrating data, profiling, validating  Algorithms Geek and researcher in field of data science with a strong experience in statistical & analytics packages/tools (R, Alteryx etc.)  Expertise on Machine Learning/Information Retrieval/Artificial Intelligence/Text Mining problems.  Deep understanding & familiarity with Predictive modeling concepts, Recommendation & optimizing algorithms, clustering & classification techniques  High level of proficiency in statistical tools, relational databases & expertise in programming languages like Python/SQL is desired  Strong analytical, troubleshooting and problem-solving skills  Ability to work independently as well as collaboratively within a team  Candidate should open to learn new open source technologies and languages  Must be able to thrive in a fast paced, customer-focused, collaborative environment. Must be able to work hands-on with minimal supervision or assistance.  Masters- degree in operations research, applied statistics, data mining, machine learning & related quantitative discipline (Preferably M.Sc /M. Tech. in Statistics/Mathematics) OR MBA from Tier 1 with relevant experience & expertise
College Preference : tier1-any
Min Qualification : pg
Skills : artificial intelligence, data mining, machine learning, predictive modeling, python, r, sql, statistical modeling, text mining
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/machine-learning-engineer-gurugram-4-years-of-experience/
Data Scientist- Mumbai (5 Yrs of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Engineer- Gurugram (4+ Years of Experience)|Android Developer- Gurgaon (2 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,Role & Responsibilities:|Work Experience & Domain Knowledge::,"Experience : 5  years
Requirements : 
Task Info :  Looking for Dynamic Individual who wants to help develop the next generation of technologies that find, organize, analyze, and personalize the unending flow of information faced by users. Data Scientists work on solving problems in various areas of computing including automated information classification, efficient algorithms for Text mining, Focused Crawler Machine learning and Natural Language Processing to locate information on the web, scalability issues related to filtering and managing large amounts of data, converting unstructured data into structured data to name just a few. Quickly model, prototype, architect and engineer systems requiring machine learning to match business needs Development of predictive models to classify text into multiple categories and extract meaningful information from text to build knowledge base Manage the scale & high volume of TBs of realtime data streams 5+ years of professional experience with core skill sets of Text Mining, Natural Language Processing, Entity Extraction, Relationship Extraction and Machine Learning Experience with text classification algorithms like Nave-Bayes, SVM etc Experience in using tools GATE, UIMA with java or ntlk with python etc Experience with Open source NLP libraries e.g. Corenlp, Opennlp, mallet, etc. Good Knowledge of Elastic Search/Apache Solr, Lucene will be plus Must have strong combination of programming experience in building end to end applications in Java/Python Understanding of statistics is good to have Tweaking algorithms & data-structures for performance fine tunings Exposure to big data & hadoop eco-system is a plusQualifications:B.E/B. tech, MCA, M. Sc (Computer Science/IT)Skills & Attitude: Excellent analytical skills Enthusiasm for working in a team to solve interesting problems
College Preference : no-bar
Min Qualification : ug
Skills : apache spark, bigdata, hadoop, java, machine learning, naive bayes, nlp, python, svm, text mining
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-scientist-mumbai-5-yrs-of-experience/
Android Developer- Gurgaon (2 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Mumbai (5 Yrs of Experience)|Why are GPUs necessary for training Deep Learning models?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : As an Android Developer, you will be responsible for developing habit forming mobile app for Analytics Vidhyas audience. This app would help hundreds of thousands of people across the globe to learn and practice data science on a daily basis.You will be bringing in new ideas to the company and its products and will be responsible to deliver world-class mobile experience to the users. You will work with our server-side engineers to deliver world class mobile experience.Responsibilities: Lead and drive entire development cycle of android app development on your own You will build and deploy android apps from scratch You will work closely with our backend engineering to interface with API services and contribute to the APIs when needed You will have to collaborate with cross-functional teams to develop and ship new features with short development cyclesKnowledge, Skills & Experience: 2-5 years of experience with building android applications Experience with Java, Android SDK, platform tools and optimisation techniques Experience interfacing with REST APIs and Git Should have developed at least one app from scratch on your own that has 10,000+ installs on play store B.E/B.Tech in Computer Science or related field, or equivalent experience Experience of backend development with Python would be a plus point
College Preference : no-bar
Min Qualification : ug
Skills : Android, GIT, java, REST
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/android-developer-gurgaon-2-to-5-years-of-experience/
Why are GPUs necessary for training Deep Learning models?,Learn everything about Analytics|Introduction|Table of Contents|Fact #101: Deep Learning requires a lot of hardware|Training a deep learning model|How to train your neural net faster?|Difference between CPU and GPU|Brief History of GPUs  how did we reach here|Which GPU to use today?|The future looks exciting|End Notes,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Android Developer- Gurgaon (2 to 5 years of experience)|Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python)|
Faizan Shaikh
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most of you would have heard exciting stuff happening using deep learning. You would have also heard that Deep Learning requires a lot of hardware. I have seen people training a simple deep learning model for days on their laptops (typically without GPUs) which leads to an impression that Deep Learningrequires big systems to run execute.However, this is only partly true and this creates a myth around deep learning which creates a roadblock for beginners. Numerous people have asked me as to what kind of hardware would be better for doing deep learning. With this article, I hope to answer them.Note: I assume that you have a fundamental knowledge of deep learning concepts. If not, you should go through this article.When I first got introduced with deep learning, I thought that deep learning necessarily needs large Datacenter to run on, and deep learning experts would sit in their control rooms to operate these systems.This is because every book that I referred or every talk that I heard, the author or speaker always say that deep learning requires a lot of computational power to run on. But when I built my first deep learning model on my meager machine, I felt relieved! I dont have to take over Google to be a deep learning expert This is a common misconception that every beginner faces when diving into deep learning. Although, it is true that deep learning needs considerable hardwareto run efficiently, you dont need it to be infinite to do your task. You can even run deep learning models on your laptop!Just a smalldisclaimer; the smaller your system, more is the time you will need to get a trained model which performs good enough. You may basically look like this:Lets just ask ourselves a simple question; why do we need more hardware for deep learning?The answer is simple, deep learning is an algorithm  a software construct. We define an artificial neural network in our favorite programming language which would then be converted into a set of commands that run on the computer.If you would have to guess which components of neural network do you think would require intense hardware resource, what would be your answer?A few candidates from top of my mind are:Among all these, training the deep learning model is the most intensive task. Lets see in detail why is this so.When you train a deep learning model, two main operations are performed:In forward pass, input is passed through the neural network and after processing the input, an output is generated. Whereas in backward pass, we update the weights of neural network on the basis of error we get in forward pass.Both of these operations are essentially matrix multiplications. A simple matrix multiplication can be represented by the image belowHere, we can see that each element in one row of first array is multiplied with one column of second array. So in a neural network, we can consider first array as input to the neural network, and the second array can be considered as weights of the network.This seems to be a simple task. Now just to give you a sense of what kind of scale deep learning  VGG16 (a convolutional neural network of 16 hidden layers which is frequently used in deep learning applications) has ~140 million parameters; aka weights and biases. Now think of all the matrix multiplications you would have to do to pass just one input to this network! It would take years to train this kind of systems if we take traditional approaches.We saw that the computationally intensive part of neural network is made up of multiple matrix multiplications. So how can we make it faster?We can simply do this by doing all the operations at the same time instead of doing it one after the other. This is in a nutshell why we use GPU (graphics processing units) instead of a CPU (central processing unit) for training a neural network.To give you a bit of an intuition, we go back to history when we proved GPUs were better than CPUs for the task.Before the boom of Deep learning, Google had a extremely powerful system to do their processing, which they had specially built for training huge nets. This system was monstrous and was of $5 billion total cost, with multiple clusters of CPUs.Now researchers at Stanford built the same system in terms of computation to train their deep nets using GPU. And guess what; they reduced the costs to just $33K ! This system was built using GPUs, and it gave the same processing power as Googles system. Pretty impressive right?We can see that GPUs rule. But what exactly is the difference between a CPU and a GPU?To understand the difference, we take a classic analogy which explains the difference intuitively.Suppose you have to transfer goods from one place to the other. You have an option to choose between a Ferrari and a freight truck.Ferrari would be extremely fast and would help you transfer a batch of goods in no time. But the amount of goods you can carry is small, and usage of fuel would be very high.A freight truck would be slow and would take a lot of time to transfer goods. But the amount of goods it can carry is larger in comparison to Ferrari. Also, it is more fuel efficient so usage is lower.So which would you chose for your work?Obviously, you would first see what the task is; if you have to pick up your girlfriend urgently, you would definitely choose a Ferrari over a freight truck. But if you are moving your home, you would use a freight truck to transfer the furniture.Heres how you would technically differentiate the two:SourceHeres another video which would make your concept even clearer.Note: GPU is mostly used for gaming and doing complex simulations. These tasks and mainly graphics computations, and so GPU is graphics processing unit. If GPU is used for non-graphical processing, they are termed as GPGPUs  general purpose graphics processing unitNow, you might be asking this question that why are GPUs so much rage right now. Let us travel through a brief history of development of GPUsBasically a GPGPU is a parallel programming setup involving GPUs & CPUs which can process & analyze data in a similar way to image or other graphic form. GPGPUs were created for better and more general graphic processing, but were later found to fit scientific computing well. This is because most of the graphic processing involves applying operations on large matrices.The use of GPGPUs for scientific computing started some time back in 2001 with implementation of Matrix multiplication. One of the first common algorithm to be implemented on GPU in faster manner was LU factorization in 2005. But, at this time researchers had to code every algorithm on a GPU and had to understand low level graphic processing.In 2006, Nvidia came out with a high level language CUDA, which helps you write programs from graphic processors in a high level language. This was probably one of the most significant change in they way researchers interacted with GPUsHere I will quickly give a few know-hows before you go on to buy a GPU for deep learning.Scenario 1:The first thing you should determine is what kind of resource does your tasks require. If your tasks are going to be small or can fit in complex sequential processing, you dont need a big system to work on. You could even skip the use of GPUs altogether. So, if you are planning to work mainly on other ML areas / algorithms, you dont necessarily need a GPU.Scenario 2:If your task is a bit intensive, and has a handle-able data, a reasonable GPU would be a better choice for you. I generally use my laptop to work on toy problems, which has a slightly out of date GPU (a 2GB Nvidia GT 740M). Having a laptop with GPU helps me run things wherever I go. There are a few high end (and expectedly heavy) laptops with Nvidia GTX 1080 (a 8 GB VRAM) which you can check out at the extreme.Scenario 3:If you are regularly working on complex problems or are a company which leverages deep learning, you would probably be better off building a deep learning system or use a cloud service like AWS or FloydHub. We at Analytics Vidhya built a deep learning system for ourselves, for which we shared our specifications. Heres the article.Scenario 4:If you are Google, you probably need another datacenter to sustain! Jokes aside, if your task is of a bigger scale than usual, and you have enough pocket money to cover up the costs; you can opt for a GPU cluster and do multi-GPU computing. There are also some options which may be available in the near future  like TPUs and faster FPGAs, which would make your life easier.As mentioned above, there is a lot of research and active work happening to think of ways to accelerate computing. Google is expected to come out with Tensorflow Processing Units (TPUs) later this year, which promises an acceleration over and above current GPUs.Similarly Intel is working on creating faster FPGAs, which may provide higher flexibility in coming days. In addition, the offerings from Cloud service providers (e.g. AWS) is also increasing. We will see each of them emerge in coming months.In this article, we covered the motivations of using a GPU for deep learning applications and saw how to choose them for your task. I hope this article was helpful to you. If you have any specific questions regarding the topic, feel free to comment below or ask them on discussion portal.",https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/
Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python),Learn everything about Analytics|Introduction|Table of Contents|1. Elastic Search (ES)|2. Kibana|3. Creating Dashboards,"Installation of Elastic Search|Installation|3.1 Indexing data|3.2 Linking Kibana|3.3 CreateVisualizations|Finally the dashboard with all the visualizations created would look like this!|4. Search bar|Share this:|Like this:|Related Articles|Why are GPUs necessary for training Deep Learning models?|22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)|
Guest Blog
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Reading data|Example 1|Example 2|Example 3|Example,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Exploratory Data Analysis (EDA) helps usto uncover the underlying structure of data and its dynamics through which we can maximize the insights. EDA is also critical to extract important variables and detect outliers and anomalies. Even though there are many algorithms in Machine Learning, EDA is considered to be one of the most critical part to understand and drive the business.There are several ways to perform EDA on various platforms like Python (matplotlib, seaborn), R (ggplot2) and there are a lot of good resources on the web such as Exploratory Data Analysis by John W. Tukey, Exploratory Data Analysis with R by Roger D. Peng and so on..In this article, I am going to talk about performing EDA using Kibana and Elastic Search.Elastic Search is an open source, RESTful distributed and scalable search engine. Elastic search is extremely fast in fetching results for simple or complex queries on large amounts of data (Petabytes) because of its simple design and distributed nature. It is also much easier to work with than a conventional database constrained by schemas, tables.Elastic Search provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.Installation and initialization is quite simple and it is as follows:Elasticsearch instance should be running at http://localhost:9200 in your browser if you run with default configuration.Keep the terminal open where elastic search is running to be able to keep the instance running. you could also use nohup mode to run the instance in the background.Kibana is an open source data exploration and visualization tool built on Elastic Search to help you understand data better.It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.Installation and initialization is similar to that of Elasticsearch:Kibana instance should be running at http://localhost:5601 in your browser if you run with default configuration.Keep the terminal open where Kibana was run to be able to keep the instance running. you could also use nohup mode to run the instance in the background.There are mainly three steps to create dashboards using ES and Kibana. I will be using Loan prediction practice problem data to create a dashboard. Please register for the problem to be able to download the data. Please check the data dictionary for more information.Note: In this article I will be using python to read data and insert data into Elasticsearch for creating visualizations through Kibana. Elastic Search indexes data into its internal data format and stores them in a basic data structure similar to a JSON object. Please find the below python code to insert data into ES. Please install pyelasticsearch library as shown below for indexing through python.Note: Please note that the code assumes that the elastic search is run with default configuration.Repeat the above 4 steps for loan_prediction_test. Now kibana is linked with train and test data present in elastic searchVoila!! Dashboard created.Similarly for Gender distribution. This time we will use pie chart.Beautiful! isnt it?Now I leave you here to explore more of elastic search and kibana and create various kind of visualizationsSearch bar allows you to explore data by string search, which helps us in understanding the changes in data with changes in one particular attribute which is not easy to do with visualizations.BeforeAfterInsight: Most of the clients that had credit history 0 did not receive Loan (Loan status is N = 92.1%)Thats all!!This article was contributed by Supreeth Manyam (@ziron) as part of The Mightiest Pen, DataFest 2017. Supreeth won the competition and also finished second in overall leaderboard of DataFest 2017. Supreeth is a passionate Data Scientist who is keen on bringing insights to business and help it get better by analyzing relevant data using Machine Learning and Artificial Intelligence.",https://www.analyticsvidhya.com/blog/2017/05/beginners-guide-to-data-exploration-using-elastic-search-and-kibana/
"22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)",Learn everything about Analytics|Introduction|Deep Learning talks|Big Data|Data Science|Natural Language Processing|End Notes,"1) Title : Deep Learning at Booking.com|2) Title : Using deep learning in natural language processing|3) Title: Creativity and AI: Deep Neural Nets Going Wild|4) Title : Neural Networks for Recommender Systems|5) Title : Training a TensorFlow model to detect lung nodules on CT scans|6) Title : Siamese LSTM in Keras: Learning Character-Based Phrase|7) Title : Deep learning for time series made easy|8) Title : Deep Reinforcement Learning: theory, intuition, code|9) Title: Different Strategies of Scaling H2O Machine Learning on Apache Spark|10) Title: A billion stars in the Jupyter Notebook|11) Title:Finding Needles in a Growing Haystack|12) Title: Survival analysis for conversion rates|13) Title : Risk Analysis|14) Title: Python vs Orangutan|15) Title : Diagnosing Machine Learning Models|16) Title : Data Science in Internet of Things using Python and Spark|17) Title : Bayesian optimization with Scikit-Optimize|18) Title : Applied Data Science|19) Title : Successfully applying Bayesian statistics to A/B testing in your business|20) Title: Deploying Python Models to Production|21) Title: Pythonic Metal|22) Title:Simulate your language|Share this:|Like this:|Related Articles|Hands on tutorial to perform Data Exploration using Elastic Search and Kibana (using Python)|Data Analyst- Hyderabad ( 3 to 8 years of experience)|
Sunil Ray
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Python is increasingly gaining popularity among machine learning and data science communities across the world  and for the right reasons. It probably has the most developed ecosystem for deep learning, a collection of awesome libraries like pandas and scikit learn and an awesome community.PyData is a community for developers and users for open source data tools. They also conduct several conferences and I came across amazing talks from PyData Amsterdam 2017 recently. Even though I wanted to be part of the conference, it was difficult for me to travel. Thankfully, PyData released all the videos on their YouTube channel.The spread of the talks is amazing.Be it a novice, intermediate or an expert python user, PyData had something for everyone. To help the community, I have summarized the best talks from data science perspective in this article. For your convenience, Ive also added a short summary of each video. We have the videos segregated in 4 categories Deep Learning, Big Data, Data Science and Natural Language Processing.Consume as you want, learn, like and share!Speaker : Emrah Tasli, Stas GirkinDuration :00:32:38 hrsThis talk intrigued me as soon as I read the title. I have always been a booking.com user. To see how they use deep learning to enhance user experience was a treat.Watch this video to get a practical overview of how deep learning is used in the industry. It focuses mainly on the applications of deep learning atbooking.com . Thiscovers applications likeanalyzing image content, analyzing text, understanding speech and building recommendation systems.The speakers then discuss how these techniques are applied at scale, and the tools used by booking.com to handle this scale.Speaker : Rob RomijndersDuration :00:25:42 hrsUnderstanding language nuancesis a difficult problem to solve  but deep learning holds ourhope. This video is a must watch for people who want to use deep learning in natural language processing. Itexplains the motivation for using deep learning for NLP applications such as machine translation. It further explains how RNN works and how they are implemented.Lastly, Rob presents tips for increasing performance of these systems.Speaker: Roelof PietersDuration: 00:33:45 hrsRoelof talks about basics of deep learning with the explosion of research and experiments that deal with creativity and artificial intelligence.He also talks about the wonderful trippy world of neural nets going wild and shows some of the exciting possibilities new technologies have to offer tomake us all more creative. Like, dancing moves, freestyle raps, impressionist paintings and showed some of the exciting possibilities new technologies offer for creative use and explorations of human-machine interaction where the main theorem is augmentation, not automation.He particularly focuses on generative models, and shows the python fanatics how to make your move with a particular form of Deep Neural Nets, to then finish with an experiment.Speaker : Maciej kulaDuration :00:32:55 hrsNeural Networks are constantly replacing every other machine learning algorithm in real life systems and recommendation systems are no exception.In this tutorial, the speaker starts from the advantages of neural networks in recommender systems and goes through various machine learning models used in recommender systems including Factorization models, Bilinear Neural Networks and sampled loss functions. If you are aspiring to make an efficient recommender system, this video is worth watching.Speaker : Mark Jan Harte, Gerben van VeenendaalDuration :00:25:53 hrsIf youre a philanthropist, this video is a must watch for you. It shows one of the numerous breakthrough applications of deep learning  to automate the detection of abnormality in medical imaging.The speakers describe the pipeline devised for automating the process. They explain in detail what are the challenges they faced while approaching the problem, what kind of hardware they utilize and then technically define their pipeline end-to-end. Its inspiring to see what kind of advancements deep learning can achieve.Speaker : Carsten van Weelden, Beata NyariDuration : 00:29:42 hrsIn this talk, the speakers explains how they solved the problem of classifying job titles into a job ontology with more than 5000 different classes. They do this by learning a character-based representation of job titles with a B-LSTM encoder trained as a Siamese network. You will learn about the methods in theory and how these can be implemented with the Keras deep learning library.Speaker : Dafne van KuppeveltDuration : 00:22:47 hrsDeep learning is a state of the art method for many tasks, such as image classification and object detection. For researchers that have time series data, but are not an expert on deep learning, the barrier can be high to start using deep learning.In this talk, the speaker explores how machine learning novices canuse deep learning for time series classification. The speaker then explains mcfly, an open source python library, to help machine learning novices explore the value of deep learning for time series data.Speaker : Maxim LapanDuration : 00:28:27 hrsIn this talk the speaker gives a practical introduction into deep reinforcement learning methods, used to solve complex applications like control problems in robotics, play Atari games, self-driving car control and lots more. Deep Reinforcement Learning is a very hot topic, successfully applied in lots of areas which require planning of actions in complex, noisy and partially-observed environments. Concrete examples vary from playing arcade games, navigating websites, helicopter, quadrocopter and car control, protein folding and lots of others.Speaker: Jakub HavaDuration: 00:32:12 hrsH2O is becoming increasingly popular when handling big data. In this video, Jakub has discussed about basic overview of machine learning on top of H2O and Spark. He explains different ways to scale your tasks on top of these technologies like data munging in spark and model building in H2O or using a mix of both for data munging and model building.Sparkling Water integrates H2O with the capabilities of Apache Spark. It also allows us to leverage H2Os machine learning algorithms with Apache Spark applications via Scala, Python, R or H2Os Flow GUI which makes Sparkling Water a great enterprise solution.This video introduces the basic architecture of Sparkling Water, going over different scaling strategies and explains the pros and cons of each solution. It finishes with a live demo demonstrating the approaches and should give you a real-life experience of configuring and running Sparkling Water for your use case(s).Speaker: Maarten BreddelsDuration: 00:30:58 hrsEver tried to visualise high dimensional data and didnt get good results? Well, this is the right place for you. In this video, Maarten talks about two Python packages: Vaex and ipyvolume.Vaex enables calculating statistics for a billion samples per second and ipyvolume enables to interactively visualise and explore these billion sample tables for high dimensional spaces. He shows the methods to visualize and explore large datasets (>1 billion) instead of using cluttered scatter plots. ipyvolume helps us to visualize higher dimensional data in the notebook interactively which can render 3d volumes and up to a million glyphs (scatter plots and quiver) in the (Jupyter) notebook as a widget.Vaex and ipyvolume can be used together to explore and visualize any large tabular data set, or separately to calculate statistics, and render 3d plots in the notebook and outside.Speaker:Stephen HelmsDuration: 00:31:02 hrsIn this video, Stephen Helms discusses about the architectural designs for big data. As the machines get more and more advanced, well collect more and more data. With high amounts of data, it becomes a challenge to efficiently summarise the data and present relevant data to the users.Stephan addresses this challenge and tries to discuss the architectural designs and implementations which can be scaled to large amounts of data. He uses Bayesian statistics to build the automated reporting system. If youre interested to know more about scaling your analysis to production, you would find this video very interesting.Speaker:Tristan BoudreaultDuration: 00:22:01 hrsDo you buy a product after the free trial ends ? As a product manager, your job might be on the line depending on how many users subscribe to your product after their free trial ends?In this video, Tristan Boudreault tries to estimate as to how many customers would be ready to pay after the trail expires. In business context, he tries to analyse how successful a website is, in converting its trail users into paid ones. When we actually look at the data we realise that people are not as impulsive as we think they are. They spend money after being comfortable with the product.He also discusses that sometimes it might be really tough to actually estimate the conversion by just looking at the numbers especially in cases when the company is growing exponentially. He has taken really interesting examples and its a great video if youre looking for applying analytics to your offering on the web.Speaker : Rogier van der GeerDuration  00:31:20 hrsEver thought that data science can be used to win a game? Well here is a video illustrating how to play risk using python. In this video Rogier van der Geer explained how python based simulation is used to train genetic algorithm to play the game.The video also focusses on designing and implementation of these algorithms in a simplified way that can be optimised for winning the game. A must watch for Data Science enthusiast as it shows how Data Science can be used to win a game!Speaker:Dirk GorissenDuration: 00:35:35 hrsThis is probably the most interesting talk and a Keynote session by Dirk Gorissen. He addresses the problem of locating the orang-utans in the jungle. So, orang-utans are one of the rare forms of apes which need to be located and protected in the jungles. To locate them they have used radio waves and identify the orang-utans when the result is unique/anomalous.This video discusses this problem using a drone based tracking system. He shows beautifully how we can solve this problem analysing the data we receive from each signal.Speaker : Lucas Javier BernardiDuration  00:39:00 hrsA Machine Learning model isnever perfect. If it completely fails, it must be fixed. If it performs well, we want to improve it. In this talk Lucas Javier Bernardi discuss about various techniques and tools needed to diagnose machine learning algorithms and models.The video explains how simple techniques and statistics can be used to improve a model and is a must watch for an aspiring data scientist.Speaker : Rafael Schultze KraftDuration : 00:32:01 hrsTime series forecasting is one of the most interesting application of Data Analysis. In this video Rafael Schultze Kraft discussed about predicting time series forecast using Python and Spark .The videos explains how to build machine learning models using AWS and python on data from sensor after suitable preprocessing which can be further used to predict significant information regarding time series data.Speaker : Gilles LouppeDuration : 00:28:53 hrsOptimization has always been an integral part of problem solving. Bayesian Optimization is a principled approach to optimize an expensive function. In this tutorial, Gilles Louppe demonstrates the use of Bayesian optimization algorithm using a newly built package Scikit-optimize which provides an easy-to-use set of tools to serve the purpose. Here youll understand the steps involved in Bayesian optimization and how to implement it in python, with an interesting analogy with brewing good quality coffee.Speaker : Giovanni LanzaniDuration : 00:35:13 hrsWith the data science and machine learning industry growing at a fast pace and all the companies incorporating these self-learning tools in their businesses, we always strive for developing the best models with the highest achievable accuracy. But this is not always in the best interest of the business, where a combination of practicality with accuracy will deliver a more acceptable end product. In this talk, Giovanni Lanzani discusses about the same while phrasing real life examples from big companies like Amazon and Netflix. Being a data science aspirant one could consider these important details to better optimize the delivered product.Speaker : Ruben MakDuration : 00:38:51 hrsA/B testing in business is a very good way to test which of your variants of product is performing the best and in turn improve the business outcome. In this tutorial, Ruben Mak discusses about applying Bayesian Statistics to improve A/B testing in your business. Shortly discussing the frequentist calculations of an A/B test and common problems in it, he uses this to explain Bayesian Statistics and more specifically hierarchical Bayes to further reduce the probability of making errors in multiple comparisons. The video also focuses on one of the most important aspects from a business perspective: when to stop an insignificant test.Speaker : Niels ZeilemakerDuration : 00:31:45 hrsDeveloping a model is actually half of the battle and you still need to put it in the production. This tutorial is all about doing so. Starting from Gitlab, the speaker covers the tools necessary for deployment of a machine learning model such as Jenkins, Docker, Kuebernetes, json logger and DTAP and goes through why and how of every tool along with codes wherever needed. I would suggest you to take your time and go through every slide of the talk to be a better data science practitioner.Speaker: Iain BarrDuration : 00:26:55 hrsBasics of NLP are always a challenge to conquer. This tutorial discusses the basic concepts of Natural Language Processing like vectorization of words, bag of words, word count as binomial frequency and deriving intelligence from it with the help of an example data set of 200,000 songs. Go ahead and take a look on it if you aspire to learn Natural Language Processing. Keep in mind that this video is a bit demanding, and you should have prior knowledge of basics of data science.Speaker:John PatonDuration: 00:27:36 hrsI was living in another state for almost 6 years and didnt know the native language of the place. I always used to wonder if they hear my words similar to what I think of theirs.John Paten has answered my question here. He tries to demonstrate how our language looks to people who actually dont speak it. He makes simple Markov Models for simulating any language in python. He shows various visualisations to understand the similarity and differences between various languages. There are very simple yet interesting insights about different languages regarding the most commonly used letters or whether a language uses long words or shorter ones to express the feelings. After this video you shall be able to understand the working of Markov models and would be able to understand and analyse languages using your models.Just watching these videos wouldnt make you a better analyst. You need to practice too. For best results, you can take notes from the video. This will help you to quickly refer the topic at a later point in time.While watching these videos, there were several moments when I felt, there are lot many things in Python which I am yet to explore. Once again I would like to thank python community for being so generous, helpful and always being helpful in time of need. If you would like to see more such videos from Pydata, you can check out their Youtube channel.Did you find this list of tutorials helpful? Which tutorial or talk did you like the most? Share your experience/ suggestion in the comments below.",https://www.analyticsvidhya.com/blog/2017/05/pydata-amsterdam-2017-machine-learning-deep-learning-data-science/
Data Analyst- Hyderabad ( 3 to 8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|22 must watch talks on Python for Deep Learning, Machine Learning & Data Science (from PyData 2017, Amsterdam)|Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  years
Requirements : 
Task Info : About the company:We are the information technology service provider that specializes in design, delivery and implementing technology driven business solutions. Through offices in the United States, UK and India, we provides complete range of services leveraging it domain expertise. Clients gain immediate and measurable value from our offerings that span business and technology consulting, application services, systems integration, product development, custom software development, maintenance, re-engineering, testing and validation services, IT infrastructure services and business process outsourcing.Job briefWe are looking for Data Analyst/Scientist to dig insights from various data sources.Data Analyst Job DutiesSource, clean, analyse and report insights using good software life cycle quality controls.Responsibilities Interpret data, analyze results using statistical techniques and provide ongoing reports Develop andimplementdatabases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality Acquiredata from primary or secondary data sources and maintain databases/data systems Identify, analyse, and interpret trends or patterns in complex data sets Filter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems Work with management toprioritizebusiness and information needs Locate and define new process improvement opportunitiesRequirements Strong Python or R programming skills. Prior experience working with Time Series data modelling, especially in Financial and Capital Markets segment. BS in Mathematics, Economics, Computer Science, Information Management or Statistics.
College Preference : tier1-entire
Min Qualification : open
Skills : bfsi, python, r, time series
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/data-analyst-hyderabad-3-to-8-years-of-experience/
"Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- Hyderabad ( 3 to 8 years of experience)|Inspiring story of Deepak Vadithala  from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  10 years
Requirements : 
Task Info : Key Responsibilities: We aims to launch a 9-month online Diploma Program in Big Data (Developers) in partnership with a top Indian University. We envision building this credential into a gold standard for Big Data professionals, akin to a CFA in the Finance world. As a Subject Matter Expert (SME)  Big Data, we would seek your leadership in the following areas to help deliver a rigorous, industry-relevant program:  Curriculum development (10% time): Build a rigorous program by informing decisions on curriculum structure, describing various Big Data Processing concepts in innovative, simple ways, and time allocation across concepts  Industry-relevant cases and projects (40% time): Identify industry-relevant projects/cases and develop them (e.g. data sets, code solutions, evaluation criteria) with industry partners to offer students compelling project experience from a recruiter standpoint. Academic quality assurance and guest faculty (40% time): Help create learning material with an in-house team of instructional designers and review its technical quality. Deliver 1-2 modules to supplement the existing faculty. Thought-leadership (10% time): Leverage our platform to publish original content(e.g. video as guest faculty, articles on blog) to position yourself, and by association weas a thought leader in the Big Data industry Student experience (post-program launch): Lead a team of program associates to assist students with their academic doubts on our platformWe are engaging with various partners like IIIT-B, Citi, Flipkart, Fractal, etc. which will give us the opportunity to build brand equity alongside industry thought leaders through our proprietary events, video content database, blog, among others. Join us on the journey of building a world-class program in Big Data! Desired Profile: 8-10 years in Big Data education and industry Hands-on, workplace experience in applying concepts in Hadoop, Spark, Hadoop Ecosystem Tools, NoSQL, Visualisations, etc. Intermediate to advanced proficiency in Java, SQL, and Python Experience in multiple verticals will be a plus 
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, datavisualization, hadoop, Hadoop Ecosystem, java, nosql, python, spark
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/05/subject-matter-expert-sme-big-data-mumbai-3-to-10-years-of-experience-full-time-part-time/
Inspiring story of Deepak Vadithala  from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary,Learn everything about Analytics|Introduction|My encounter with Deepak|About Deepak Vadithala,"Share this:|Like this:|Related Articles|Subject Matter Expert (SME)- Big Data- Mumbai (3 to 10 years of experience, Full time & Part time)|Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017|
Kunal Jain
|38 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We love heroes. We use them to get our dose of daily inspiration. We look forward to see / hear what they are doing, how they are converting challenges into opportunities and what they are thinking. We pick our heroes based on our experience, preferences, challenges and likings. I am sure you have your own heroes.Today, we bring you a story of one such hero  Deepak Vadithala. Deepak started his career as a paper boy. He has been a booth guy, a nightwatchman at a computer institute to learn Computers and is a Qlik Luminary today.The first time I came across Deepak was when he was running a QlikView course for charity. He asked us if Analytics Vidhya would be able to spread word for his course. Next time I can across him  he was using his Data Science skills for charity work and involving a group of 30 data scientists to solve social problems for underprivileged children.My curiosity about him grew and I looked up what and why is he up to. And I came across an inspirational story  a story of sheer hard work & perseverance. A story which puts all excuses I hear from people to shame and could inspire an entire generation of data scientists. I am going to share this story with our community today.Deepak is currently working as a Lead Data Engineer / Data Scientist.He was recognised as a Qlik Luminary for the year 2014 and 2017. He co-founded Qlik Dev Group and is an author of www.QlikShare.com. He was also rewarded as the top 20 Global QlikView Community contributor. He is also a certified, Scrum Master (PSM) and is currently, pursuing Masters in Data Science from the University of London.Kunal: Hi Deepak, thanks for finding out time for this interview. Tell us briefly about yourself.Deepak:I am born and breed in Hyderabad, India. Currently, I live and work in London. I work as a Lead Data Engineer for Exterion on a TFL project. Also, I am pursuing Masters in Data Science from University of London (Part-time). I am passionate about solving data problems using programming and data analysis. I got awarded as a Qlik Luminary in 2014 and 2017. I have 13+ years experience in application development, database management and building analytics applications. I am working on two charity projects to raise money for the underprivileged children for their education.Thank you for interviewing me, and I am excited to share my story with the AV community. You guys are doing a great job with AV.Kunal: Thanks Deepak! Tell us how did your journey on analytics / data science / visualization start?Deepak: As Steve Jobs quoted:You cannot connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something  your gut, destiny, life, karma, whatever. I had no idea back then that I will become a developer or work with the data. However, every job I did in the past has either direct or indirect contribution to my journey.Before, I talk about my journey into analytics world. I had done 13 different non-tech jobs before becoming a data analyst at Dell. It had been a bumpy ride, to say the least, but in every job, I managed to learn some skills which helped me some way or the other. Perseverance is the key factor and helped me all the way. I had intuition when I was doing odd jobs that things will work out at some point. I made sure that I had learnt something new on every job. This approach has never let me down, and it has made all the difference to my life.My Journey so farCurrently I am working in a Lead Data Engineer in London.I started working when I was 15 years old, and Im 35 years old now. At times, I used to do two jobs, so there is overlap between years.Jobs in chronological orderKunal: Wow! Thats inspiring and it would have been damn tough. How did the transition from aCall Centre Agent to aData Analyst happen?Deepak: My exposure to analytics started started when I was working in Dell call centre, I met a guy called Ram, and he was working on Excel and SQL Server. I observed that all the managers go to him to get their reports/data. I liked the attention he got and thought he was indispensable. That was it! I had decided to become a Data Analyst, so I can avoid taking calls, and at the same time, I could work on something which is a transferable skill set when I move organisations.So, I started learning Excel during weekends in the office as I did not have the computer at home back then. I would spend weekends in office practising various Excel formulas, pivot tables and charts. I showed off my skills to my manager, and he encouraged me to prepare internal team reports in Excel. I did that for about six months when my manager informed me there would be an opportunity in Gurgaon Dell office for Data Analyst role. I was excited and worked hard to prepare for the interview. I attended the interview and showed what I had been doing and some sample reports. I was offered the role, and I moved away from Call Centre to Dell IT team.I did not stop there, I went on acquiring new skills continuously. Learning new things and practising them has become a habit and I had started enjoying the process.Kunal: You had no background in analytics and data science. Neither were you from a strong education background. What kind of challenges did you face in reaching here? And what are key decisions in shaping your career?Deepak:Currently, I am pursuing Masters in Data Science from the University of London as part-time. I study from 6 pm  9 pm while working full time and I have a one-year-old baby to take care of.However, I do not have strong academic and engineering background. I did my bachelors in Journalism and another one in commerce. I had studied while working. So, I had to pursue all my education through distance learning. It was not easy at all as I was doing odd jobs initially. I accepted that learning is a continuous process and it takes the time to master a subject.Here are a few stories about my journey till now:The story of Job Number # 7: Joining as a Night Watchmen / Security Guard for computer institute.During my initial journey I wanted to learn computers, but I couldnt afford to pay for the training institutes neither I had a computer at home. I thought I should find a wayto learn computers. I directly approached computer institutes in Hyderabad with my resume asking for any roles. A lot of them said no, but one of them said  they need someone to sleep in the institute in the night times as security guard and also to open the doors early in the mornings for students.I was embarrassed and felt uncomfortable being a security guard. But I thought who cares, no one is watching me, and at the same time, I will get access to the computers all night. And they kindly agreed for me to sit in whatever training classes I was interested in learning. I saw this as a tremendous opportunity while it is uncomfortable then. I decided to try it anyway, and Im so glad I did that.I tried learning C language but it didnt make sense at that time as I didnt have math or CS foundation and I couldnt follow the lectures. I decided to take design classes  Photoshop, Illustrator and Coreldraw. As they were interesting and I always loved the design. So, I took the classes and practised almost every night for 24 hrs. Within a couple of months, I did pick up decent skills.After six months as a night watchmen or security guard. I got a job as a DTP operator paying me more than bartender job. And I continued to improve my skills on other design packages then.Looking back, this was a good decision as I moved into computer based jobs instead of odd jobs.Story of Job Number # 13: Coming to the UK with Dell Exit CompensationWhile I was working as a Data Analyst role in Delhi. Dell made structural changes to operations team (I was part of Operations team as Data Analyst). And management asked me to take a manager role in the call centre business in Hyderabad or Bangalore (then). My gut feeling was I need to have independent skills and not Dell specific skills. So, I wasnt too interested in the managerial position. Most of my colleagues took that opportunity except two of us. We were asked to leave Dell as they dont have any Data Analysis roles in India. I was offered compensation of 7 months salary as worked for 3.5 years.It was a hard and emotional decision as I enjoyed my time working at Dell and made loads of friends. I took a holiday for two weeks and went looking for a job to assess the market outside. There were no phone calls nor interviews and my two weeks ended quickly. But a friend suggested me to try HSMP (UK Visa). I thought I dont stand a chance as I didnt see myself Highly Skilled then. But I thought I should try or else I will end up regretting not trying. So I took the compensation and applied for HSMP. And thankfully all worked in my favour. I got the visa, and I was left with 700 to come to the UK. I was told there were no jobs in the UK. And it was 2008 beginning and recession started. I decided to come anyway thinking I can work in supermarkets as I had done all sorts of jobs in the past. Again, I got interview call within a week, and Survey Solutions hired me within two weeks of landing in London.Story: Almost redundant In 2009, I was working as a developer however my knowledge was very limited. My company then made a decision to make me redundant because of recession. They kindly gave a months notice and asked me to search for a job. I thought I had experience and skills to survive. However, the market was not great, and I had applied for at least 300 jobs. I had gathered hundreds of email address and sent blanket emails to potential employers. Most of them never replied, and some responded saying they will get back to me in the future. Finally, I found a job in a small village in Cambridge, and they offered me less than my actual salary then. Luckily, on the last day of the notice period my employer changed hismind and asked me to stay with them as they realised my work and how it would affect them. I decided to stick with the same employer and continued working for them for another 18 months.That situation hit me hard and made me realised that I could not be complacent and I need to acquire new skills continuously. I started going to the library over the weekend and spent almost all my weekend studying and preparing for certifications. I did this for two years, and this made a big difference regarding skills. Since then, I chose the job I wanted rather than I was forced to take up a job.Kunal: I see you are also a Qlik Luminary  tell us about that journey.Deepak: I accidentally discovered QlikView. I was asked to attend an interview for a Database developer and DBA role. Moreover, the employer wanted someone with QlikView skills too. I had to clue what was QlikView!I started by installing the desktop edition and instantly loved the associative engine. I decided to learn and spend time preparingfor the interview. I was honest and told my prospective employer that I am super keen to learn. They believed in me offered a role to work on both databases and QlikView.Sounds a bit weird but I read most of the manual as there were no books or courses on QlikView apart from Qliks official training course, which was too expensive. Also, I started answering questions on Qlik community to exposure to a variety of problems. It was like eat-sleep-Qlik-repeat for 18 months. At one point, I was amongst top 30 contributors to Qlik community. Then I thought I should share my knowledge with other developer and started www.QlikShare.com blog. I had recorded almost 100 video tutorials and 250+ quiz questions.Jason and Matt (co-founders) of QlikDevGroup asked me to join them to start the QlikDevGroup. I liked the idea, and we have begun the QlikDevGroup (www.QlikDevGroup.com).Thankfully, Qlik recognised all my work and awarded me Qlik Luminary in 2014 and 2017.Kunal: Coming to the fascinating work you are doing  how did it start  Teaching / Doing Data Science for charity?Deepak: I wanted to support underprivileged children and empower them with education. There are many poor children who cannot afford education. I thought I should pay back somehow and provide support. I realised that running a marathon may not work in my case (I do not have that kind of fitness levels!).I got an idea  Learn while empowering someone to learn.Project # 1:I thought I could create a course which can help students learn QlikView and other things while their money will be contributed to the charity. I am thankful to my friend  Shilpan Patel who also kindly agreed to collaborate with me to create QlikView Server course. Together  we had created the course and published on Udemy. There are 503 students so far, and the course is rated 4.5 out of 5. This course is the most comprehensive QlikView Server and Publisher course available.Project # 2:I am currently working on building a pricing app for the charities to increase their revenue. This project is open sourced. I see an opportunity where Charities could generate more revenue. Charities sell most of the donated goods for less because they have pricing constraints because of time, skills and accessibility of the data and they settle down to sell stuff for less, and their pricing is not on par with eBay.We could solve this problem using data. So, we are building an analytical app which helps charities price better. The project is called DtP  Discover the Potential. Again, I am thankful to all the contributors who are helping me to build this platform. This project is self-funded and open source project, and we are taking data feeds from eBay. We process them on AWS. Enrich the data and process the data to provide search functionality. We are planning to use MongoDB, Python and Jhipster for the web app.Contact me if you are interested to know more about these projects.Kunal: What is the force which drives you? What would you want to achieve and change in next few years?Deepak:Honestly, I feel like a celebrity when you ask that question (laughs!).But Im definitely not.I enjoy the learning process, and I do not feel stressed when I am studying or learning new skills. I like the process. Also, the ability to deal with uncertainties and staying composed helped me a lot. I am sure I will continue to learn as long as I can. And I have accepted the fact that nothing can be attained overnight. It will be a slow process, but surely every job helped me to learn something new. I am thankful to have wife and family who support and encourage me to learn.I want to focus on Data Science applicationsin investment banking/ finance domain. After masters, I am planning to pursue FRM Part 1. Also, I am working on statistical skills. Also, I want to participate in Kaggle competitions. At the same time, I am interested in open source projects to help charities. I am sure there are loads of problems we could solve using data more effectively.Kunal: This kind of impact would feel very empowering  tell us some stories of people/life you have changed?Deepak: I do not think I have changed anyones life yet. However, I am sure I had encouraged all my friends and colleagues to learn and acquire new skills. I would be happy if I can become a good mentor and help change someones life in the future.Kunal: Anything else you want to say to AV community?Deepak: Are you looking to learn QlikView? Please check my courses. I am giving 100 discount coupons exclusively for AV readers. And I would love to hear your suggestions, thoughts or feedback. You can connect me on LinkedIn and Twitter. Im sharing my blog and course links with you below.BlogCourse Links:QlikView Server and PublisherQlikView Mastering Set AnalysisKunal: Thanks Deepak, your story is truly inspiring & motivating. We are glad to interview you and Im sure many people out there willtake away tremendous learning. All the best with your charity work and do let us know, if we can be of any help.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/05/exclusive-interview-with-deepak-vadithala-lead-data-engineer-qlikview-luminary/
"Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017",Learn everything about Analytics|Introduction|Problem Statement|Winners|End Notes,"Piyush Jaiswal, Rank 3|Rohan Rao, Rank 2|Mark Landry, Rank 1|Check out all the upcoming competitions here|Share this:|Like this:|Related Articles|Inspiring story of Deepak Vadithala  from a Paper delivery boy to a Lead Data Engineer & QlikView Luminary|41 questions on Statistics for data scientists & analysts|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The best way to learn data science is to work on data problems and solve them. What is even better is to solve these problems with thousands of data scientists around solving the same problem in a competition.And if you get to know what the winners did to solve the problem  you know you are participating in an Analytics Vidhya Hackathon!We conducted 3 machine learning hackathons as part ofAV DataFest 2017.The QuickSolver was conducted on 30 April 2017. More than 1800 data scientistsfrom across the globe competed to grab the top spots.If you missed this hackathon, you did miss an amazing opportunity. Well, you can still learn from the winners & their strategies. To hear the experiences and strategies of the data scientists who won, join us for the DataFest Closing Ceremony on 10 May 2017.The problem statement revolved around a digital publication house. They publish articles on varied categories & topics likegeneral knowledge, practical well being, sports & health. These articles are written by distinguished authors in this field. To keep the reader engaged on the website,the portal recommends articles to its readers randomly.They want to enhance their customer experience, and understand the interest of their customers in detail.Instead of recommending articles to its reader randomly they would like to recommend articles based on their interest & are more likely to read. Currently, the portal has an option to rate articles based after reading.The data scientists had to predict howmuch would the reader like the article based on the given data.The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1: Mark LandryRank 2: Rohan RaoRank 3: Piyush JaiswalHere are the final rankings of all the participants at theleaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go aheadPiyush JaiswalPiyush is a Data Science Analyst at 64 Squares based in Pune. He is a machine learning enthusiast and has participated in several competitions on Analytics Vidhya.Heres what Piyush shared with us.Piyush says  The model was built around 4 sets of features primarily.1. Meta data on User : Age buckets and variable V12. Meta data on Article : Time since article was published, Number of articles by the same author & Number of articles in the same category3. Article Preferences4. Choice of model was Xgboost. Tried an ensemble between 2 xgb models but it gave little boost (from 1.7899 to 1.7895)Solution: Link to CodeRohan RaoRohan is a Senior Data Scientist at Paytm & a Kaggle Grandmaster. Rohan holds multiple accolades on his name. He has won several competitions on Analytics Vidhya and has been actively participating in machine learning competitions. His approach always has interesting insights.Heres what Rohan shared with us.2. Used raw features + count features initially with XGBoost.3. On plotting feature importance, I found the user-id to be the most important variable. So, split the train data into two halves, and used the average rating of users from one half of the data as a feature in the second half, and built my model on only the second half of the training data. This gave a major boost and the score reached 1.804. I ensembled few XGBoosts with different seeds and to finally get below 1.795. Some final tweaks like directly populating the common IDs between train & test and clipping the predictions between 0 and 6, gave minor improvements as well.Tuning parameters and using linear models didnt really work. I even tried building a multi-class classification model, but that performed much worse than regression, which is natural considering the metric is RMSE.Solution: Link to CodeMark LandryMark is a Competitive Data Scientist & Product manager at H2O.ai. Mark is also an active participant in machine learning competition on Analytics Vidhya & Kaggle. Mark has won several hackathons and ranked highly for his machine learning skills. He has several other accomplishments on his name.Heres what Mark shared with us.Mark says,In short, features were created using data.table in R and the modeling was done with three H2O models: random forest and two GBMs.The progression of the models is actually represented in the way the features are laid out. The initial submission scored 1.94 on the public leaderboard (which scored very close to private) and was quickly tuned to about 1.82. This spanned the first six target encodings, no IDs, only the three Article features directly. Over time, I kept adding more (including a pair that surprisingly reduced model quality) all the way to the end. Light experimentation on the modeling hyperparameters, which are likely underfit. Random forest wound up my strongest model, most likely due to lack of internal CV of the GBMs that led to me staying fairly careful.I kept the User and Article IDs out of the modeling from the start. At one point I used the most frequent User_ID values, but this did not help  the models were already picking up enough of the user qualities. The main feature style is target encoding, so in the code you will see sets of three lines that calculate the sum of response, count of records, and then performs an average that removes the impact of the record to which it is applying the average. Youll see the same style in my Xtreme ML Hack solution (I started from that code, in fact) and also Knocktober 2016.The short duration and unlimited submissions for this competition kept me moving quickly, but at a disadvantage for model tuning. No doubt these parameters are not ideal, but that was just a choice I made to keep the leaderboard as my improvement focus and iterations extremely short, rather than internal validation. Had either the train/test split or public/private split not appeared random, I would have modeled things differently. But this was a fairly simple situation for getting away with such tactics.A 550-tree default Random Forest was my best individual model.Here is the feature utilization.I used a pair of GBMs with slightly different parameters. These features are for the model with 200 trees, a 0.025 learning rate, depth of 5, row sampling of 60% and column sampling of 60%.Thanks to Analytics Vidhya for hosting this competition and all of AV DataFest 2017, as well as all participants who worked on the problem and pushed the leaderboard. This was my first six-hour competition and it was as fun as the half-week competitions.Solution: Link to CodeIt was great fun interacting with these winners and to know their approach during thecompetition. Hopefully, you will be able to evaluate where you missed out.",https://www.analyticsvidhya.com/blog/2017/05/winners-solutions-approach-the-quicksolver-minihack-datafest-2017/
41 questions on Statistics for data scientists & analysts,Learn everything about Analytics|Introduction|Overall Scores,"Questions & Solution|End Notes|Learn, compete, hack and get hired!|Share this:|Related Articles|Winners solutions & approach: The QuickSolver MiniHack, DataFest 2017|42 Questions on SQL for all aspiring Data Scientists|
Dishashree Gupta
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Statistics forms the back bone of data science or any analysis for that matter. Sound knowledge of statistics can help an analyst to make sound business decisions.On one hand, descriptive statistics helps us to understand the data and its properties by use of central tendency and variability. On the other hand, inferential statistics helps us to infer properties of the population from a given sample of data. Knowledge of both descriptive and inferential statistics is essential for an aspiring data scientist or analyst.To help you improve your knowledge in statistics we conducted this practice test. The testcovered both descriptive and inferential statistics in brief. I am providing the answers with explanation in case you got stuck on particular questions.In case you missed the test, try solving the questions before reading the solutions.Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 450 people tookthistest and the highest score obtained was 37. Here are a few statistics about the distribution.Mean Score: 20.40Median Score: 23Mode Score: 251) Which of these measures are used to analyze the central tendency of data? A) Mean and Normal DistributionB) Mean, Median and ModeC) Mode, Alpha & RangeD) Standard Deviation, Range and MeanE) Median, Range and Normal DistributionSolution: (B)The mean, median, mode are the three statistical measures which help us to analyze the central tendency of data. We use these measures to find the central value of the data to summarize the entire data set.2) Five numbers are given: (5, 10, 15, 5, 15). Now, what would be the sum of deviations of individual data points from their mean?A) 10B)25C) 50D) 0E) None of the aboveSolution: (D)The sum of deviations of the individual will always be 0.3) A test is administered annually. The test has a mean score of 150 and a standard deviation of 20. If Ravis z-score is 1.50, what was his score on the test?A)180
B) 130
C)30
D)150
E) None of the aboveSolution: (A)X=+Z where  is the mean,  is the standard deviation and X is the score were calculating. Therefore X = 150+20*1.5 = 1804) Which of the following measures of central tendency will always change if a single value in the data changes? A) MeanB) MedianC) ModeD) All of theseSolution: (A)The mean of the dataset would always change if we change any value of the data set. Since we are summing up all the values together to get it, every value of the data set contributes to its value. Median and mode may or may not change with altering a single value in the dataset.5) Below, we have represented six data points on a scale where vertical lines on scale represent unit. Which of the following line represents the mean of the given data points, where the scale is divided into same units?A)A
B)B
C)C
D)DSolution: (C)Its a little tricky to visualize this one by just looking at the data points. We can simply substitute values to understand the mean. Let A be 1, B be 2, C be 3 and so on. The data values as shown will become {1,1,1,4,5,6} which will have mean to be 18/6 = 3 i.e. C.6) If a positively skewed distribution has a median of 50, which of the following statement is true? A)Mean is greater than 50
B) Mean is less than 50
C)Mode is less than 50
D) Mode is greater than 50
E) Both A and C
F)Both B and DSolution: (E)Below are the distributions for Negatively, Positively and no skewed curves.As we can see for a positively skewed curve, Mode<Median<Mean. So if median is 50, mean would be more than 50 and mode will be less than 50.7) Which of the following is a possible value for the median of the below distribution?A)32
B)26
C)17
D) 40Solution: (B)To answer this one we need to go to the basic definition of a median. Median is the value which has roughly half the values before it and half the values after. The number of values less than 25 are (36+54+69 = 159) and the number of values greater than 30 are (55+43+25+22+17= 162). So the median should lie somewhere between 25 and 30. Hence 26 is a possible value of the median.8) Which of the following statements are true about Bessels Correction while calculating a sample standard deviation?A) Only 2B)Only 3C) Both 2 and 3D) Both 1 and 3Solution: (C)Contrary to the popular belief Bessels correction should not be always done. Its basically done when were trying to estimate the population standard deviation using the sample standard deviation. The bias is definitely reduced as the standard deviation will now(after correction) be depicting the dispersion of the population more than that of the sample.9) If the variance of a dataset is correctly computed with the formula using (n  1) in the denominator, which of the following option is true?A) Dataset is a sample
B)Dataset is a population
C)Dataset could be either a sample or a population
D)Dataset is from a census
E)None of the aboveSolution: (A)If the variance has n-1 in the formula, it means that the set is a sample. We try to estimate the population variance by dividing the sum of squared difference with the mean with n-1.When we have the actual population data we can directly divide the sum of squared differences with n instead of n-1.10) [True or False] Standard deviation can be negative. A) TRUEB) FALSESolution: (B)Below is the formula for standard deviationSince the differences are squared, added and then rooted, negative standard deviations are not possible.11)Standard deviation is robust to outliers?A)TrueB)FalseSolution: (B)If you look at the formula for standard deviation above, a very high or a very low value would increase standard deviation as it would be very different from the mean. Hence outliers will effect standard deviation.12) For the below normal distribution, which of the following option holds true ?1, 2 and 3 represent the standard deviations for curves 1, 2 and 3 respectively.
A)1> 2> 3B)1< 2< 3C) 1= 2= 3D) NoneSolution: (B)From the definition of normal distribution, we know that the area under the curve is 1 for all the 3 shapes. The curve 3 is more spread and hence more dispersed (most of values being within 40-160). Therefore it will have the highest standard deviation. Similarly, Curve 1 has a very low range and all the values are in a small range of 80-120. Hence, curve 1 has the least standard deviation.13) What would be the critical values of Z for 98% confidence interval for a two-tailed test ?A) +/- 2.33
B)+/- 1.96
C)+/- 1.64
D)+/- 2.55Solution: (A)We need to look at the z table for answering this. For a 2 tailed test, and a 98% confidence interval, we should check the area before the z value as 0.99 since 1% will be on the left side of the mean and 1% on the right side. Hence we should check for the z value for area>0.99. The value will be +/- 2.3314) [True or False] The standard normal curve is symmetric about 0 and the total area under it is 1.A)TRUEB) FALSESolution: (A)By the definition of the normal curve, the area under it is 1 and is symmetric about zero. The mean, median and mode are all equal and 0. The area to the left of mean is equal to the area on the right of mean. Hence it is symmetric.Context for Questions 15-17Studies show that listening to music while studying can improve your memory. To demonstrate this, a researcher obtains a sample of 36 college students and gives them a standard memory test while they listen to some background music. Under normal circumstances (without music), the mean score obtained was 25 and standard deviation is 6. The mean score for the sample after the experiment (i.e With music) is 28.15) What is the null hypothesis in this case?A) Listening to music while studying will not impact memory.
B)Listening to music while studying may worsen memory.
C)Listening to music while studying may improve memory.
D)Listening to music while studying will not improve memory but can make it worse.Solution: (D)The null hypothesis is generally assumed statement, that there is no relationship in the measured phenomena. Here the null hypothesis would be that there is no relationship between listening to music and improvement in memory.16) What would be the Type I error?A) Concluding that listening to music while studying improves memory, and its right.
B) Concluding that listening to music while studying improves memory when it actually doesnt.
C)Concluding that listening to music while studying does not improve memory but it does.Solution: (B)Type 1 error means that we reject the null hypothesis when its actually true. Here the null hypothesis is that music does not improve memory. Type 1 error would be that we reject it and say that music does improve memory when it actually doesnt.17) After performing the Z-test, what can we conclude ____ ?A)Listening to music does not improve memory.B)Listening to music significantly improves memory at pC) The information is insufficient for any conclusion.D) None of the aboveSolution: (B)Lets perform the Z test on the given case. We know that the null hypothesis is that listening to music does not improve memory.Alternate hypothesis is that listening to music does improve memory.In this case the standard error i.e.The Z score for a sample mean of 28 from this population isZ critical value for  = 0.05 (one tailed) would be 1.65 as seen from the z table.Therefore since the Z value observed is greater than the Z critical value, we can reject the null hypothesis and say that listening to music does improve the memory with 95% confidence.18) A researcher concludes from his analysis that a placebo cures AIDS. What type of error is he making?A) Type 1 errorB) Type 2 errorC) None of these. The researcher is not making an error.D) Cannot be determinedSolution: (D)By definition, type 1 error is rejecting the null hypothesis when its actually true and type 2 error is accepting the null hypothesis when its actually false. In this case to define the error, we need to first define the null and alternate hypothesis.19) What happens to the confidence interval when we introduce some outliers to the data?A) Confidence interval is robust to outliersB) Confidence interval will increase with the introduction of outliers.C) Confidence interval will decrease with the introduction of outliers.D) We cannot determine the confidence interval in this case.Solution: (B)We know that confidence interval depends on the standard deviation of the data. If we introduce outliers into the data, the standard deviation increases, and hence the confidence interval also increases.Context for questions 20- 22A medical doctor wants to reduce blood sugar level of all his patients by altering their diet. He finds that the mean sugar level of all patients is 180 with a standard deviation of 18. Nine of his patients start dieting and the mean of the sample is observed to 175. Now, he is considering to recommend all his patients to go on a diet.Note: He calculates 99% confidence interval.20) What is the standard error of the mean?A)9
B)6
C)7.5
D)18Solution: (B)The standard error of the mean is the standard deviation by the square root of the number of values. i.e.Standard error =  = 621) What is the probability of getting a mean of 175 or less after all the patients start dieting?A) 20%
B)25%
C)15%
D) 12%Solution: (A)This actually wants us to calculate the probability of population mean being 175 after the intervention. We can calculate the Z value for the given mean.If we look at the z table, the corresponding value for z = -0.833 ~ 0.2033.Therefore there is around 20% probability that if everyone starts dieting, the population mean would be 175.22) Which of the following statement is correct?A) The doctor has a valid evidence that dieting reduces blood sugar level.B) The doctor does not have enough evidence that dieting reduces blood sugar level.C) If the doctor makes all future patients diet in a similar way, the mean blood pressure will fall below 160.Solution: (B)We need to check if we have sufficient evidence to reject the null. The null hypothesis is that dieting has no effect on blood sugar. This is a two tailed test. The z critical value for a 2 tailed test would be 2.58.The z value as we have calculated is -0.833.Since Z value < Z critical value, we do not have enough evidence that dieting reduces blood sugar.Question Context 23-25A researcher is trying to examine the effects of two different teaching methods. He divides 20 students into two groups of 10 each. For group 1, the teaching method is using fun examples. Where as for group 2 the teaching method is using software to help students learn. After a 20 minutes lecture of both groups, a test is conducted for all the students.We want to calculate if there is a significant difference in the scores of both the groups.It is given that:23) What is the value of t-statistic?A)3.191
B)3.395
C) Cannot be determined.
D)None of the aboveSolution: (A)The t statistic of the given group is nothing but the difference between the group means by the standard error.=(10-7)/0.94 = 3.19124) Is there a significant difference in the scores of the two groups?A) Yes
B)NoSolution: (A)The null hypothesis in this case would be that there is no difference between the groups, while the alternate hypothesis would be that the groups are significantly different.The t critical value for a 2 tailed test at  = 0.05 is 2.101. The t statistic obtained is 3.191. Since the t statistic is more than the critical value of t, we can reject the null hypothesis and say that the two groups are significantly different with 95% confidence.25) What percentage of variability in scores is explained by the method of teaching?A)36.13
B)45.21
C)40.33
D)32.97Solution: (A)The % variability in scores is given by the R2 value. The formula for R2 given by R2= The degrees of freedom in this case would be 10+10 -2 since there are two groups with size 10 each. The degree of freedom is 18.R2=  = 36.1326) [True or False] F statistic cannot be negative. A) TRUEB) FALSESolution: (A)F statistic is the value we receive when we run an ANOVA test on different groups to understand the differences between them. The F statistic is given by the ratio of between group variability to within group variabilityBelow is the formula for f Statistic.Since both the numerator and denominator possess square terms, F statistic cannot be negative.27) Which of the graph below has very strong positive correlation?
A)
B)
C)
D)Solution: (B)A strong positive correlation would occur when the following condition is met. If x increases, y should also increase, if x decreases, y should also decrease. The slope of the line would be positive in this case and the data points will show a clear linear relationship. Option B shows a strong positive relationship.28) Correlation between two variables (Var1 and Var2) is 0.65. Now, after adding numeric 2 to all the values of Var1, the correlation co-efficient will_______ ?A)Increase
B)Decrease
C)None of the aboveSolution: (C)If a constant value is added or subtracted to either variable, the correlation coefficient would be unchanged. It is easy to understand if we look at the formula for calculating the correlation.If we add a constant value to all the values of x, the xi and will change by the same number, and the differences will remain the same. Hence, there is no change in the correlation coefficient.29) It is observed that there is a very high correlation between math test scores and amount of physical exercise done by a student on the test day. What can you infer from this? A)Only 1
B) 1 and 3
C)2 and 3
D)All the statements are trueSolution: (C)Though sometimes causation might be intuitive from a high correlation but actually correlation does not imply any causal inference. It just tells us the strength of the relationship between the two variables. If both the variables move together, there is a high correlation among them.30) If the correlation coefficient (r) between scores in a math test and amount of physical exercise by a student is 0.86, what percentage of variability in math test is explained by the amount of exercise?A) 86%
B)74%
C) 14%
D)26%Solution: (B)The % variability is given by r2, the square of the correlation coefficient. This value represents the fraction of the variation in one variable that may be explained by the other variable. Therefore % variability explained would be 0.862.31) Which of the following is true about below given histogram?A) Above histogram is unimodalB) Above histogram is bimodalC) Given above is not a histogramD) None of the aboveSolution: (B)The above histogram is bimodal. As we can see there are two values for which we can see peaks in the histograms indicating high frequencies for those values. Therefore the histogram is bimodal.32) Consider a regression line y=ax+b, where a is the slope and b is the intercept. If we know the value of the slope then by using which option can we always find the value of the intercept?A) Put the value (0,0) in the regression line TrueB) Put any value from the points used to fit the regression line and compute the value of b FalseC) Put the mean values of x & y in the equation along with the value a to get b FalseD) None of the above can be used FalseSolution: (C)In case of ordinary least squares regression, the line would always pass through the mean values of x and y. If we know one point on the line and the value of slope, we can easily find the intercept.33) What happens when we introduce more variables to a linear regression model?A)The r squared value may increase or remain constant, the adjusted r squared may increase or decrease.B) The r squared may increase or decrease while the adjusted r squared always increases.C) Both r square and adjusted r square always increase on the introduction of new variables in the model.D) Both might increase or decrease depending on the variables introduced.Solution: (A)The R square always increases or at least remains constant because in case of ordinary least squares the sum of square error never increases by adding more variables to the model. Hence the R squared does not decrease. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.34) In a scatter diagram, the vertical distance of a point above or below regression line is known as ____ ?A) Residual
B) Prediction Error
C) Prediction
D)Both A and B
E)None of the aboveSolution: (D)The lines as we see in the above plot are the vertical distance of points from the regression line. These are known as the residuals or the prediction error.35) In univariate linear least squares regression, relationship between correlation coefficient and coefficient of determination is ______ ?A)Both are unrelated FalseB) The coefficient of determination is the coefficient of correlation squared TrueC) The coefficient of determination is the square root of the coefficient of correlation FalseD) Both are same FSolution: (B)The coefficient of determination is the R squared value and it tells us the amount of variability of the dependent variable explained by the independent variable. This is nothing but correlation coefficient squared. In case of multivariate regression the r squared value represents the ratio of the sum of explained variance to the sum of total variance.36) What is the relationship between significance level and confidence level?A) Significance level = Confidence level
B)Significance level = 1- Confidence level
C)Significance level = 1/Confidence level
D)Significance level = sqrt (1  Confidence level)Solution: (B)Significance level is 1-confidence interval. If the significance level is 0.05, the corresponding confidence interval is 95% or 0.95. The significance level is the probability of obtaining a result as extreme as, or more extreme than, the result actually obtained when the null hypothesis is true. The confidence interval is the range of likely values for a population parameter, such as the population mean. For example, if you compute a 95% confidence interval for the average price of an ice cream, then you can be 95% confident that the interval contains the true average cost of all ice creams.The significance level and confidence level are the complementary portions in the normal distribution.37) [True or False] Suppose you have been given a variable V, along with its mean and median. Based on these values, you can find whether the variable V is left skewed or right skewed for the conditionA) True
B)FalseSolution: (B)Since, its no where mentioned about the type distribution of the variable V, we cannot say whether it is left skewed or right skewed for sure.38) The line described by the linear regression equation (OLS) attempts to ____ ?A) Pass through as many points as possible.B) Pass through as few points as possibleC) Minimize the number of points it touchesD) Minimize the squared distance from the pointsSolution: (D)The regression line attempts to minimize the squared distance between the points and the regression line. By definition the ordinary least squares regression tries to have the minimum sum of squared errors. This means that the sum of squared residuals should be minimized. This may or may not be achieved by passing through the maximum points in the data. The most common case of not passing through all points and reducing the error is when the data has a lot of outliers or is not very strongly linear.39) We have a linear regression equation ( Y = 5X +40) for the below table.Which of the following is a MAE (Mean Absolute Error) for this linear model?A) 8.4
B) 10.29
C)42.5
D) None of the aboveSolution: (A)To calculate the mean absolute error for this case, we should first calculate the values of y with the given equation and then calculate the absolute error with respect to the actual values of y. Then the average value of this absolute error would be the mean absolute error. The below table summarises these values.40) A regression analysis between weight (y) and height (x) resulted in the following least squares line: y = 120 + 5x. This implies that if the height is increased by 1 inch, the weight is expected toA)increase by 1 pound
B) increase by 5 pound
C)increase by 125 pound
D) None of the aboveSolution: (B)Looking at the equation given y=120+5x. If the height is increased by 1 unit, the weight will increase by 5 pounds. Since 120 will be the same in both cases and will go off in the difference.41) [True or False] Pearson captures how linearly dependent two variables are whereas Spearman captures the monotonic behaviour of the relation between the variables.A)TRUEB) FALSESolution: (A)The statement is true. Pearson correlation evaluated the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable.The spearman evaluates a monotonic relationship. A monotonic relationship is one where the variables change together but not necessarily at a constant rate.I hope you had fun solving the questions and they did make you scratch your head sometime. Please share your thoughts on the above topics and also your feedback.We shall be happy to incorporate your ideas in further articles and tests. Also, one question might have multiple approaches and the solution above might show just one. I have tried to be descriptive with the solutions but feel free to investigate further in case of doubts using the comments below.",https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/
42 Questions on SQL for all aspiring Data Scientists,Learn everything about Analytics|Introduction|Questions & Solution||End Notes,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|41 questions on Statistics for data scientists & analysts|40 Questions to test your skill in Python for Data Science|
Ankit Gupta
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"SQL is a universal tool in data science. Irrespective of which language you use as your main tool  you need to know SQL. There are no 2 ways about it. For most of the organisations, SQL is the way to store and retrieve structured data form underlying systems. So, if you are an aspiring data scientist or already a data science professional, having . expertise in SQL is a big boon.To help our community test themselves on SQL, we designed a SQL skill test as part of DataFest 2017. More than 1500 people registered in the skill test and more than 500 people took the test. Below is the distribution of scores:You can access the final scoreshere. Here are a few statistics about the distribution:Mean Score: 18.41Median Score: 20Mode Score: 281) Which of the following option(s) is/are correct?A) 1B) 2C) Both 1 and 2D) None of theseSolution: (A)SQL is a querying language and it is not case sensitive.2) What is true for a Null value in SQL?A) 1 and 3B) 2 and 4C) 1 and 4D) 2 and 3Solution: (A)NULL represents a unknown value so adding anything to null value will give a null value in result.3) Which of the following is not an aggregrate function in SQL?A) MIN()B) MAX()C) DISTINCT()D) COUNT()Solution: (C)All of the functions except DISTINCT function given in the question is an example of aggregate function.4) Which of the following option is true for the below queries?A) Both queries will give different outputs.B) Both queries will give same outputs.C) Cant saySolution: (B)Both query will return the same output.5) Which of the following cannot be a superkey in a relational schema with attributes A,B,C,D,E and primary key AD?A) A B C DB) A C D EC) A B C ED) A B D ESolution: (C)The attributes A, D should be present in the super key. Option C doent have D attribute so C would be the right answer.Questions Context 6-7You run the following Queries in the following order:Create a table Me using the below SQL query.Next, you create a view based on Me table by using the following query.Finally, you run the following query:6) Which of the following statements are true?A) 1 and 3B) 1 and 4C) 2 and 3D) 2 and 4Solution: (B)Query 2 is used for creating the view on table Me so it would run fine but if you run the Query3 it will generate the below error.ERROR: cannot drop table me because other objects depend on itDETAIL: view me_view depends on table meHINT: Use DROP  CASCADE to drop the dependent objects too.7) Now, you have changed the Query3 as below.And, you also want to run below query on the same table.Which of the following statements are true for such cases?A) 1 and 3B) 1and 4C) 2 and 3D) 2 and 4Solution: (C)If you drop the base table using cascade it will drop the base table as well as view table also so Query 3 will run fine but Query 4 will give an error.8) Imagine, you have a column A in table1 which is a primary key and it refers to column B in table2.Further, column A has only three values (1,2,3). Which of the following options is / are correct?A) 1 and 2B) 2 and 3C) 1 and 2D) 3 and 4Solution: (B)You can insert any value except the duplicate values in column A in table 1 but you cannot insert the values other then 1,2 and 3 in columns B in table2 due to foreign key integrity because it is referenced by the column A.9) Consider a table T1 which contains Column A in it. Column A is declared as a primary key. Which of the following is true for column A in T1?A) 1 and 4B) 2 and 4C) 1 and 3D) 2 and 3Solution: (B)A primary key column cannot contain duplicate and null values.10) Imagine you have a table T1 which has three columns A, B and C where A is a primary key.Which of the following query will return number of rows present in the table T1A) 1 and 2B) 2 and 3C) 1 and 3D) 1, 2 and 3Solution: (A)Query1 and Query2 will return the same output.11) Which of the following statement describes the capabilities of UPDATE command most appropriately?A) It can only modify one value of a single columnB) It can update multiple values of a single columnC) It can update one value of multiple columnsD) It can update multiple values of multiple columnsSolution: (D)12) What is true about indexing in a database?A) Search will be faster after you have indexed the databaseB) Search will be slower after using indexingC) Indexing has nothing to do with searchD) None of theseSolution: (A)Option A is correct. Readmore here.13) Consider three tables T1, T2 and T3. Table T1, T2 and T3 have 10, 20, 30 number of records respectively. Further, there are some records which are common in all three tables.You want to apply a cartesian product on these three tables. How many rows will be available in cartesian product of these tables?A) 6000B) More than 6000C) Less than 6000D) None of theseSolution: (A)14) Tables A, B have three columns (namely: id, age, name) each. These tables have no null values and there are 100 records in each of the table.Here are two queries based on these two tables A and B.Which of the following statement is correct for the output of each query?A) The number of tuples in the output of Query 1 will be more than or equal to the output of Query 2B) The number of tuples in the output of Query 1 will be equal to the output of Query 2C) The number of tuples in the output Query 1 will be less than or equal to the output of Query 2D) Cant saySolution: (C)Answer C is correct because natural join always give either same or less number of rows if you compare it with cartesian product. To know more read from this tutorial.15) What will be the output of the following query in PostgreSQL?A)B)C)D)
Solution: (C)It will give the year differece in output so answer C is correct.16) Imagine you are given the following table named AV.And you want to run the following queries Q1, Q2 and Q3 given below.Which sequence for the three queries will not result in an error?A) Q1 -> Q2 -> Q3B) Q2 -> Q1 -> Q3C) Q3 -> Q1 -> Q2D) Q2 -> Q3 -> Q1Solution: (D)DROP TABLE will drop the table as well as its reference. So, you cant access the table once you have dropped it. But in case of DELETE TABLE reference will not be droped so you can still access the table if you use DELETE TABLE command.17) Imagine you are given the following table named AV.You apply the following query Q1 on AV, which is given below:What will be the output for query Q1?A)B)C)D)Solution: (A)The boundary salaries (200 and 500) will also be in the out so A is the right answer.18) Imagine you are given the following table named AV.What would be the output for the following query?A)B)C)Solution: (B)Question Context 19-21Assume you are given the two tables AV1 and AV2 which represent two different departments of AV.
AV1 TABLEAV2 TABLE19) Now, you want the names of all people who work in both the departments. Which of the following SQL query would you write?A) SELECT NAME FROM AV1 INTERSECT SELECT NAME FROM AV2;B) SELECT NAME FROM AV1 UNION SELECT NAME FROM AV2;C) SELECT NAME FROM AV1 DIFFERENCE SELECT NAME FROM AV2;D) None of theseSolution: (A)INTERSECT would be used for such output.20) What is the output for the below query?A)B)C) ERRORD) None of theseSolution: (A)This query will give the names in AV1 which are not present in AV2.21) What will be the output of below query?A)B)C) None of theseSolution: (B)Question Context 22-24Suppose you are given the below table called A_V.22) What is the output for the below query?A)B)C) ERRORD) None of theseSolution: (B)23) What is the output for the below query?A)
B)C)D) None of theseSolution: (B)First replace null value will be replaced to 2 using COALESCE then 100 will be added.24) What is the output for the below query?A)B) Empty outputC)ErrorD)None of theseSolution: (B)SQL is not case sensitive but when you search for something in a string column it becomes case sensitive. So output will have zero rows because Ankit != ANKIT and Faizan != FAIZAN.25) You are given a string  AnalyticsVidhya . The string contains two unnecessary spaces  one at the start and another at the end. You find out the length of this string by applying the below queries.If op1, op2, op3, op4 are the output of the Query 1, 2, 3 and 4 respectively, what will be the correct relation between these four queries?A) 1 or 2B) 2C) 3D) 1 and 4Solution: (D)Option D is correct. For more information read from this tutorial.Questions Context 26-27Below you are given a table split.26) Now, you want to apply a query on this.What is the output for the above query?A)B)C)D)E) ErrorSolution:(E)The query will give the below error.ERROR: field position must be greater than zero27) In the above table split, you want to replace some characters using translate command. Which of the following will be the output of the following query?A)B)C)ErrorD)None of theseSolution: (A)In the above query character A will replace to 1, B to 2 and C to 3.28) Which of the following query will list all station names which contain their respective city names. For example, station Mountain View Caltrain Station is for city Mountain View.Refer to the table below this question.A) select * from station where station_name like % || city || %;B) select * from station where city like % || station_name || % ;C) ErrorD) None of theseSolution: (A)29) Consider the following legal instance of a relational schema S with attributes ABC.Which of following functional dependencies is/are not possible?A) 1 and 2B) 2 and 3C) 1 and 3D) None of aboveSolution: (D)Read from this tutorial.
30) Suppose you have a table called Student and this table has a column named marks. Now, you apply Query1 on Student table.After this, you create an index on column marks and then you re-run Query 2 (same as Query 1).If Query 1 is taking time T1 and Query 2 is taking time T2.Which of the following is true for the query time?A) T1 > T2B) T2 > T1C) T1 ~ T2D) None of theseSolution: (C)To search fastyou need to create the index on marks*100 but in the question we have created the index on marks.31) Suppose you have 1000 records in a table called Customers. You want to select top 100 records from it. Which of the below commands can you use?A) 1B) 2C) 1 and 2D) None of themSolution: (C) Both query can be used to get the desired output.32) Which of the following is the outcome of the following query?A) Faizan and Ankit are close friendsB) Ankit and Ankit are close friendsC) Faizan and Faizan are close friends
D) Ankit and Faizan are close friendsSolution: (B)Faizan will be replaced by Ankit.33) Which one of the following queries always gives the same answer as the nested Query shown below.A) select R.* from R, S where R.a=S.aB) select distinct R.* from R,S where R.a=S.aC) select R.* from R,(select distinct a from S) as S1 where R.a=S1.aD) None of aboveSolution: (C)Option C is correct.Question Context 34-35Consider the following table avian (id, name, sal).34) Which of the following options will be required at the end of the following SQL query?So that the appended query finds out the name of the employee who has the maximum salary?A) WHERE P1.sal >= Any (select P2.sal from avian P2)B) WHERE P1.sal <= All(select max(P2.sal) from avian P2)C) WHERE P1.sal > Any (select max(P2.sal) from avian P2)D) WHERE P1.sal >= Any (select max(P2.sal) from avian P2)Solution: (D)B  Returns the addresses of all theaters.
C  Returns null set. max() returns a single value and there wont be any value > max.
D  Returns null set. Same reason as C. All and ANY works the same here as max returns a single value.35) Which of the following options can be used to find the name of the person with second highest salary?A) select max(sal) from avian where sal < (select max(sal) from avian)B) BothC) None of theseSolution: (B)Query in the option B(select max(sal) from avian)first return the highest salary(say H) then the query(select max(sal) from avian where sal < H )will search for highest salary which is less then H.Question Context 36-39Suppose you are given a database of bike sharing which has three tables: Station, Trip and Weather.Station TableTrip TableWeather Table36) Imagine, you run following query on above schema.Which of the following option is correct for this query?A) This query will print city name and number of stations sorted by number of stations in increasing magnitude. If number of stations are same, it will print by decreasing order of city name.B) This query will print city name and number of stations sorted by city name in increasing magnitude. For cities with same name, it will print by decreasing order of number of stations.C) None of theseSolution: (A)A is correct answer.37) Which of the following query will find the percentage (round to 5 decimal places) of self-loop trips (which start and end at the same station) among all trips?A)B)C)D) None of theseSolution: (A)Query in option A will give the desired result38 Which of the following statements is / are true for the below query?Note: All the zip_code are present in table weather also present in station tableA) 1 and 2B) 1 and 3C) 1D) 1,2 and 3Solution: (A)39) What will be the output of the following query?C) ErrorD) None of theseSolution: (B)This query will find a cumulative traveling durations of bike 301.Question Context 40-42Suppose you are given 4 tables: Team, Player, Game and GameStats. Below are the SQL statements which create these tables.40) Which of the following query will return distinct names of the players who play at Guard Position and their name contains Jo. (ORDER BY A) A) SELECT name FROM player WHERE position=Guard AND name LIKE %jo% ORDER BY nameB) SELECT name FROM player WHERE position=Guard AND name LIKE %Jo% ORDER BY nameC) Both of themD) None of themSolution: (B)This query Finds any values that have Jo in any position using %jo% expression in command. Notice that Jo is different then jo because expression in like operator is case sensitive.41) What will be the output for the below query?A) Return the number of games where Saurabh scored more points than FaizanB) Return the number of games where Saurabh scored less points than FaizanC) ErrorD) None of theseSolution: (A)42) What is the expected output of the following query?A) List all players playerIDs and their average points in all home games that they played in (ORDER BY Players playerID)B) List all players playerIDs and their average points in all games that they played in (ORDER BY Players playerID)C) ErrorD) None of theseSolution: (A)I hope you enjoyed the questions and were able to test your knowledge about SQL. Irrespective of what role you are in data science, you need to know SQL. If you havent done already, take time out to undergo the test and reflect on where you went wrong.If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/questions-sql-for-all-aspiring-data-scientists/
40 Questions to test your skill in Python for Data Science,Learn everything about Analytics|Questions & Answers,"End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|42 Questions on SQL for all aspiring Data Scientists|40 questions to test your skill on R for Data Science|
Faizan Shaikh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Python is increasingly becoming popular among data science enthusiasts, and for right reasons. It brings the entire ecosystem of a general programming language. So you can not only transform and manipulate data, but you can also create strong pipelines andmachine learning workflows in a single ecosystem.At Analytics Vidhya, we love Python. Most of us use Python as our preferred tool for machine learning. Not only this, if you want to learn Deep Learning, Python clearly has the most mature ecosystem among all other languages.If you are learning Python for Data Science, this test was created to help you assess your skill in Python. This test was conducted as part of DataFest 2017. Close to 1,300 people participated in the test with more than 300 people taking this test.Below are the distribution scoresof the people who took the test:You can access the final scoreshere. Here are a few statistics about the distribution.Mean Score: 14.16Median Score: 15Mode Score: 0Question Context 1You must have seen the show How I met your mother. Do you remember the game where they played, in which each person drinks a shot whenever someone says but, um. I thought of adding a twist to the game. What if you could use your technical skills to play this game?
To identify how many shots a person is having in the entire game, you are supposed to write a code.
Below is the subtitle sample script.Note: Python regular expression library has been imported as re.1) Which of the following codes would be appropriate for this task?A) len(re.findall(But, um, txt))B) re.search(But, um, txt).count()C) len(re.findall([B,b]ut, um, txt))D) re.search([B,b]ut, um, txt)).count()Solution: (C)You have to find both capital and small versions of but So option C is correct.Question Context 2Suppose you are given the below stringstr = Email_Address,Nickname,Group_Status,Join_Year
[emailprotected],aa,Owner,2014
[emailprotected],bb,Member,2015
[emailprotected],cc,Member,2017
[emailprotected],dd,Member,2016
[emailprotected],ee,Member,2020
In order to extract only the domain names from the email addresses from the above string (for eg. aaa, bbb..) you write the following code:2) What number should be mentioned instead of __ to index only the domains?Note: Python regular expression library has been imported as re.A) 0B) 1C) 2D) 3Solution: (C)Read syntax of regular expression re.
Question Context 3Your friend has a hypothesis  All those people who have names ending with the sound of y (Eg: Hollie) are intelligent people. Please note: The name should end with the sound of y but not end with alphabet y.Now you being a data freak, challenge the hypothesis by scraping data from your colleges website. Heres data you have collected. 
You want to make a list of all people who fall in this category. You write following code do to the same:3) What should be the value of pattern in regular expression?Note: Python regular expression library has been imported as re.A) pattern = (i|ie)(,)B) pattern = (i$|ie$)(,)C) pattern = ([a-zA-Z]+i|[a-zA-Z]+ie)(,)D) None of theseSolution: (B)You have to find the pattern the end in either i or ie. So option B is correct.Question Context 4Assume, you are given two lists:a = [1,2,3,4,5] b = [6,7,8,9]The task is to create a list which has all the elements of a and b in one dimension.Output:a = [1,2,3,4,5,6,7,8,9]4) Which of the following option would you choose?A) a.append(b)B) a.extend(b)C) Any of the aboveD) None of theseSolution: (B)Option B is correct5) You have built a machine learning model which you wish to freeze now and use later. Which of the following command can perform this task for you?Note: Pickle library has been imported as pkl.
A) push(model, file)B) save(model, file)C) dump(model, file)D) freeze(model, file)Solution: (C)Option C is correctQuestion Context 6We want to convert the below string in date-time value:6) To convert the above string, what should be written in place ofdate_format?A) %d/%m/%yB) %D/%M/%YC) %d/%M/%yD) %d/%m/%YSolution: (D)Option D is correctQuestion Context 7I have built a simple neural network for an image recognition problem. Now, I want to test if I have assigned the weights & biases for the hidden layer correctly. To perform this action, I am giving an identity matrix as input. Below is my identity matrix:A = [ 1, 0, 0
0, 1, 0
0, 0, 1]7) How would you create this identity matrix in python?Note: Library numpy has been imported as np.A) np.eye(3)B) identity(3)C) np.array([1, 0, 0], [0, 1, 0], [0, 0, 1])D) All of theseSolution: (A)Option B does not exist (it should be np.identity()). And option C is wrong, because the syntax is incorrect. So the answer is option A8) To check whether the two arrays occupy same space, what would you do?I have two numpy arrays e and f.You get the following output when you print e & fWhen you change the values of the first array, the values for the second array also changes. This creates a problem while processing the data.For example, if you set the first 5 values of e as 0; i.e.the final values of e and f areYou surmise that the two arrays must have the same space allocated.A) Check memory of both arrays, if they match that means the arrays are same.B) Do np.array_equal(e, f) and if the output is True then they both are sameC) Print flags of both arrays by e.flags and f.flags; check the flag OWNDATA. If one of them is False, then both the arrays have same space allocated.D) None of theseSolution: (C)Option C is correctQuestion Context 9Suppose you want to join train and test dataset (both are two numpy arrays train_set and test_set) into a resulting array (resulting_set) to do data processing on it simultaneously. This is as follows:9) How would you join the two arrays?Note: Numpy library has been imported as npA) resulting_set = train_set.append(test_set)B) resulting_set = np.concatenate([train_set, test_set])C) resulting_set = np.vstack([train_set, test_set])D) None of theseSolution: (C)Both option A and B would do horizontal stacking, but we would like to have vertical stacking. So option C is correctQuestion Context 10Suppose you are tuning hyperparameters of a random forest classifier for the Iris dataset.10) What would be the best value for random_state (Seed value)?A) np.random.seed(1)B) np.random.seed(40)C) np.random.seed(32)D) Cant saySolution: (D)There is no best value for seed. It depends on the data.Question 11While reading a csv file with numpy, you want to automatically fill missing values of column Date_Of_Joining with date 01/01/2010. 11) Which command will be appropriate to fill missing value while reading the file with numpy?Note: numpy has been imported as npA) filling_values = (-, 0, 01/01/2010, 0)
temp = np.genfromtxt(filename, filling_values=filling_values)B) filling_values = (-, 0, 01/01/2010, 0)
temp = np.loadtxt(filename, filling_values=filling_values)C) filling_values = (-, 0, 01/01/2010, 0)
temp = np.gentxt(filename, filling_values=filling_values)D) None of theseSolution: (A)Option A is correct12) How would you import a decision tree classifier in sklearn?A) from sklearn.decision_tree import DecisionTreeClassifierB) from sklearn.ensemble import DecisionTreeClassifierC) from sklearn.tree import DecisionTreeClassifierD) None of theseSolution: (C)Option C is correct13) You have uploaded the dataset in csv format on google spreadsheet and shared it publicly. You want to access it in python, how can you do this?Note: Library StringIO has been imported as StringIO.C)D) None of theseSolution: (A)Option A is correctQuestion Context 14Imagine, you have a dataframe train file with 2 columns & 3 rows, which is loaded in pandas.import pandas as pdNow you want to apply a lambda function on features column:14) What will be the output of following print command?A)0 A B C
1 A D E
2 C D FB)0 AB1 ADE2 CDFC) Error
D) None of theseSolution: (A)Option A is correctQuestion Context 15We have a multi-class classification problem for predicting quality of wine on the basis of its attributes. The data is loaded in a dataframe dfThe quality column currently has values 1 to 10, but we want to substitute this by a binary classification problem. You want to keep the threshold for classification to 5, such that if the class is greater than 5, the output should be 1, else output should be 0.15) Which of the following codes would help you perform this task?Note: Numpy has been imported as np and dataframe is set as df.A)B)C)D)None of theseSolution: (A)Option A is correctQuestion Context 16Suppose we make a dataframe as16) What is the difference between the two data series given below?Note: Pandas has been imported as pdA) 1 is view of original dataframe and 2 is a copy of original dataframe.B) 2 is view of original dataframe and 1 is a copy of original dataframe.C) Both are copies of original dataframe.D) Both are views of original dataframeSolution: (B)Option B is correct. Refer the official docs of pandas library.Question Context 17Consider a function fun which is defined below:Now you define a list which has three numbers in it.g = [10,11,12]17) Which of the following will be the output of the given print statement:A) [5, 11, 12] [5, 11, 12]
B) [5, 11, 12] [10, 11, 12]
C) [10, 11, 12] [10, 11, 12]
D) [10, 11, 12] [5, 11, 12]
Solution: (A)Option A is correctQuestion Context 18Sigmoid function is usually used for creating a neural network activation function. A sigmoid function is denoted as18) It is necessary to know how to find the derivatives of sigmoid, as it would be essential for backpropagation. Select the option for finding derivative?A)B)C)D) None of theseSolution: (C)Option C is correctQuestion Context 19Suppose you are given a monthly data and you have to convert it to daily data.For example,

For this, first you have to expand the data for every month (considering that every month has 30 days)19) Which of the following code would do this?Note: Numpy has been imported as np and dataframe is set as df.A) new_df = pd.concat([df]*30, index = False)B) new_df = pd.concat([df]*30, ignore_index=True)C) new_df = pd.concat([df]*30, ignore_index=False)D) None of theseSolution: (B)Option B is correctContext: 20-22Suppose you are given a dataframe df.20) Now you want to change the name of the column Count in df to Click_Count. So, for performing that action you have written the following code.What will be the output of print statement below?Note: Pandas library has been imported as pd.A) [Click_Id, Click_Count]
B) [Click_Id, Count]
C) ErrorD) None of theseSolution: (B)Option B is correctContext: 20-22Suppose you are given a data frame df.21) In many data science projects, you are required to convert a dataframe into a dictionary. Suppose you want to convert df into a dictionary such that Click_Id will be the key and Count will be the value for each key. Which of the following options will give you the desired result?Note: Pandas library has been imported as pdA) set_index(Click_Id)[Count].to_dict()B) set_index(Count)[Click_Id].to_dict()C) We cannot perform this task since dataframe and dictionary are different data structuresD) None of theseSolution: (A)Option A is correct22) In above dataframe df. Suppose you want to assign a df to df1, so that you can recover original content of df in future using df1 as below.Now you want to change some values of Count column in df.Which of the following will be the right output for the below print statement?Note: Pandas library has been imported as pd.A) [200 200 300 400 250] [200 200 300 400 250]
B) [100 200 300 400 250] [100 200 300 400 250]
C) [200 200 300 400 250] [100 200 300 400 250]
D) None of theseSolution: (A)Option A is correct23) You write a code for preprocessing data, and you notice it is taking a lot of time. To amend this, you put a bookmark in the code so that you come to know how much time is spent on each code line. To perform this task, which of the following actions you would take?A) 1 & 2B) 1,2 & 3C) 1,2 & 4D) All of the aboveSolution: (C)Option C is correct24) How would you read data from the file using pandas by skipping the first three lines?Note: pandas library has been imported as pd In the given file (email.csv), the first three records are empty.A) read_csv(email.csv, skip_rows=3)B) read_csv(email.csv, skiprows=3)C) read_csv(email.csv, skip=3)D) None of theseSolution: (B)Option B is correct25) What should be written in-place of method to produce the desired outcome?Given below is dataframe df:Now, you want to know whether BMI and Gender would influence the sales.For this, you want to plot a bar graph as shown below:The code for this is:A) stacked=TrueB) stacked=FalseC) stack=FalseD) None of theseSolution: (A)Its a stacked bar chart.26) Suppose, you are given 2 list  City_A and City_B.City_A = [1,2,3,4]City_B = [2,3,4,5]In both cities, some values are common. Which of the following code will find the name of all cities which are present in City_A but not in City_B.A) [i for i in City_A if i not in City_B]
B) [i for i in City_B if i not in City_A]
C) [i for i in City_A if i in City_B]
D) None of theseSolution: (A)Option A is correctQuestion Context 27Suppose you are trying to read a file temp.csv using pandas and you get the following error.27) Which of the following would likely correct this error?Note: pandas has been imported as pdA) pd.read_csv(temp.csv, compression=gzip)B) pd.read_csv(temp.csv, dialect=str)C) pd.read_csv(temp.csv, encoding=utf-8)D) None of theseSolution: (C)Option C is correct, because encoding should be utf-828) Suppose you are defining a tuple given below:tup = (1, 2, 3, 4, 5 )Now, you want to update the value of this tuple at 2nd index to 10. Which of the following option will you choose?A) tup(2) = 10B) tup[2] = 10C) tup{2} = 10D) None of theseSolution: (D)A tuple cannot be updated.29) You want to read a website which has url as www.abcd.org. Which of the following options will perform this task?A) urllib2.urlopen(www.abcd.org)B) requests.get(www.abcd.org)C) Both A and BD) None of theseSolution: (C) Option C is correctQuestion Context 30Suppose you are given the below web page30) To read the title of the webpage you are using BeautifulSoup. What is the code for this?Hint: You have to extract text in title tagSolution: (B)Option B is correct
Question Context 31Imagine, you are given a list of items in a DataFrame as below.D = [A,B,C,D,E,AA,AB]Now, you want to apply label encoding on this list for importing and transforming, using LabelEncoder.31) What will be the output of the print statement below ?Solution: (D)Option D is correct32) Which of the following will be the output of the below print statement?Assume, you have defined a data frame which has 2 columns.A) 0  False
1  False
2  False
3 FalseB) 0 False
1 False
2 True
3 FalseC) 0 True
1 True
2 True
3 TrueD) None of theseSolution: (A)Option A is correct33) Suppose the data is stored in HDFS format and you want to find how the data is structured. For this, which of the following command would help you find out the names of HDFS keys?Note: HDFS file has been loaded by h5py as hf.A) hf.key()B) hf.keyC) hf.keys()D) None of theseSolution: (C)Option C is correctQuestion Context 34You are given reviews for movies below:reviews = [movie is unwatchable no matter how decent the first half is . ,somewhat funny and well paced action thriller that has jamie foxx as a hapless fast talking hoodlum who is chosen by an overly demanding, morse is okay as the agent who comes up with the ingenious plan to get whoever did it at all cost .]Your task is to find sentiments from the review above. For this, you first write a code to find count of individual words in all the sentences.34)What value should we split on to get individual words?Solution: (A)Option A is correct35) How to set a line width in the plot given below?For the above graph, the code for producing the plot wasSolution: (C)Option C is correct36) How would you reset the index of a dataframe to a given list? The new index is given as:new_index=[Safari,Iceweasel,Comodo Dragon,IE10,Chrome]Note: df is a pandas dataframeA) df.reset_index(new_index,)B) df.reindex(new_index,)C) df.reindex_like(new_index,)D) None of theseSolution: (A)Option A is correct
37) Determine the proportion of passengers survived based on their passenger class.Solution: (A)Option A is correct38) You want to write a generic code to calculate n-gram of the text. The 2-gram of this sentence would be [[this, is], [is, a], [a, sample], [sample, text]]
Which of the following code would be correct?For a given a sentence:
this is a sample text.Solution: (B)Option B is correct39) Which of the following code will export dataframe (df) in CSV file, encoded in UTF-8 after hiding index & header labels.Solution: (C)Option C is correct40) Which of the following is a correct implementation of mean squared error (MSE) metric?Note: numpy library has been imported as np.Solution: (B)Option B is correctIf you are learning Python, make sure you go through the test above. It will not only help you assess your skill. You can also see where you stand among other people in the community.If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/questions-python-for-data-science/
40 questions to test your skill on R for Data Science,Learn everything about Analytics|Questions & Answers,"End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|40 Questions to test your skill in Python for Data Science|40 Questions to test a data scientist on Machine Learning [Solution: SkillPower  Machine Learning, DataFest 2017]|
NSS
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"R is one of the most popular language among the data science community. If you are serious about data science, chances are that you either already know R or are learning it. R also has a thriving ecosystem of various statistics and data science libraries. In order to help our community test themselves on their knowledge of R, we created these skill tests as part of DataFest 2017.More than 1500 people registered for this skill test and close to 500 people took this test. Below is the distribution of scores from various participants:You can access the final scoreshere. Here are a few statistics about the distribution:Mean Score: 16.69Median Score: 19Mode Score: 0We are releasing the solutions to the skill tests, so that you can evaluate where you went wrong. If you missed the test, you can still look at the Questions and Answers to see where you stand.Happy Learning!Question Context 1Consider the following function.1)If we execute following commands (written below), what will be the output?z <- 10f(4)A) 12B) 7C) 4D) 16Solution: (A)Scoping rule of R will cause z<-4 to take precedence over z<-10. Hence, g(x) will return a value of 8. Therefore, option A is the correct answer.Question context 2The iris dataset has different species of flowers such as Setosa, Versicolor and Virginica with their sepal length. Now, we want to understand the distribution of sepal length across all the species of flowers. One way to do this is to visualise this relation through the graph shown below.2) Which function can be used to produce the graph shown above?A) xyplot()
B) stripplot()
C) barchart()
D) bwplot()Solution: (B)The plot above is of type strip whereas the options a, c and d will produce a scatter, bar and box whisker plot respectively. Therefore, option B is the correct solution.Question Context 3File Name  Dataframe.csv3) Which of the following commands will correctly read the above csv file with 5 rows in a dataframe?A) csv(Dataframe.csv)B) csv(Dataframe.csv,header=TRUE)C) dataframe(Dataframe.csv)D) csv2(Dataframe.csv,header=FALSE,sep=,)Solution: (D)Options 1 and 2 will read the first row of the above dataframe as header. Option 3 doesnt exist. Therefore, option D is the correct solution.Question Context 4Excel file format is one of the most common formats used to store datasets. It is important to know how to import an excel file into R. Below is an excel file in which data has been entered in the third sheet.File Name  Dataframe.xlsx4) Which of the following codes will read the above data in the third sheet into a dataframe in R?A) Openxlsx::read.xlsx(Dataframe.xlsx,sheet=3,colNames=FALSE)B) Xlsx::read.xlsx(Dataframe.xlsx,sheetIndex=3,header=FALSE)C)XLConnect::readWorksheetFromFile(Dataframe.xlsx,sheet=3,header=FALSE)D)All of the aboveSolution: (D) All of the above options are true, as they give out different methods to read an excel file into R and reads the above file correctly. Therefore, option D is the correct solution.QuestionContext 5File Name  Dataframe.csv5) Missing values in this csv file has been represented by an exclamation mark (!) and a question mark (?). Which of the codes below will read the above csv file correctly into R?A) csv(Dataframe.csv)B) csv(Dataframe.csv,header=FALSE, sep=,,na.strings=c(?))C) csv2(Dataframe.csv,header=FALSE,sep=,,na.strings=c(?,!))D) dataframe(Dataframe.csv)Solution: (C) Option A will not be able to read ? and ! as NA in R. option B will be able to read only ? as NA but not !. Option 4 doesnt exist. Therefore, option C is the correct solution.Question Context 6-7File Name  Dataframe.csv6) The above csv file has row names as well as column names. Which of the following code will read the above csv file properly into R?A) delim(Train.csv,header=T,sep=,,row.names=TRUE)B) csv2(Train.csv,header=TRUE, row.names=TRUE)C) dataframe(Train.csv,header=TRUE,sep=,)D) csv(Train.csv,,header=TRUE,sep=,)Solution: (D)row.names argument in options A and B takes only the vector containing the actual row names or a single number giving the column of the table which contains the row names and not a logical value. Option C doesnt exist. Therefore, option D is the correct solution.Question Context 6-7File Name  Dataframe.csv7) Which of the following codes will read only the first two rows of the csv file?A) csv(Dataframe.csv,header=TRUE,row.names=1,sep=,,nrows=2B) csv2(Dataframe.csv,row.names=1,nrows=2)C) delim2(Dataframe.csv,header=T,row.names=1,sep=,,nrows=2)D) dataframe(Dataframe.csv,header=TRUE,row.names=1,sep=,,skip.last=2)Solution: (A)Option B will not be able to read the csv file correctly since the default separator in csv2 function is ; whereas csv files are of type ,. Option C has wrong header argument value. Option D doesnt exist. Therefore, Option A is the correct answer.Question Context 88) There are two dataframes stored Dataframe1 and Dataframe2 shown above. Which of the following codes will produce the output shown below?A) merge(dataframe[,1:3],dataframe2)B) merge(dataframe1,dataframe2)[,1:3]
C) merge(dataframe1,dataframe2,all=TRUE)D) Both 1 and 2E) All of the aboveSolution: (D)Option C will result in feature 4 being included in the merged dataframe which is what we do not want. Therefore, Option D is the correct solution.Question Context 9dataframe9) A data set has been read in R and stored in a variable dataframe. Which of the below codes will produce a summary (mean, mode, median) of the entire dataset in a single line of code?A) summary(dataframe)B) stats(dataframe)C) summarize(dataframe)D) summarise(dataframe)E) None of the aboveSolution: (E)Option A will give only the mean and the median but not the mode. Option B, C and D will also fail to provide the required statistics. Therefore, Option E is the correct solution.Question Context 10A dataset has been read in R and stored in a variable dataframe. Missing values have been read as NA.dataframe10) Which of the following codes will not give the number of missing values in each column?A) colSums(is.na(dataframe))B) apply(is.na(dataframe),2,sum)C) sapply(dataframe,function(x) sum(is.na(x))D) table(is.na(dataframe))Solution: (D)Option D will give the overall count of the missing values but not column wise. Therefore, Option D is the correct solution.Question context 11One of the important phase in a Data Analytics pipeline is univariate analysis of the features which includes checking for the missing values and the distribution, etc. Below is a dataset and we wish to plot histogram forValuevariable.dataframed11) Which of the following commands will help us perform that task ?A) hist(dataframed$Value)B) ggplot2::qplot(dataframed$Value,geom=Histogram)C)ggplot2::ggplot(data=dataframed,aes(dataframe$Value))+geom_histogram()D) All of the aboveSolution: (D)All of the given options will plot a histogram and that can be used to see the skewness of the desired data.Question Context 12Certain Algorithms like XGBOOST work only with numerical data. In that case, categorical variables present in dataset are first converted to DUMMY variables which represent the presence or absence of a level of a categorical variable in the dataset. For example After creating the Dummy Variable for the featureParameter, the dataset looks like below.12) Which of the following commands will help us to achieve this?A) dummies:: dummy.data.frame(dataframe,names=c(Parameter))B) dataframe$Parameter_Alpha=0dataframe$Gende_Beta=0dataframe$Parameter_Alpha[which(dataframe$Parameter==Alpha)]=1dataframe$Parameter_Beta[which(dataframe$Parameter==Alpha)]=0dataframe$Parameter_Alpha[which(dataframe$Parameter==Beta]=0dataframe$Parameter_Beta[which(dataframe$Parameter==Beta]=1C) contrasts(dataframe$Parameter)D) Both 1 and 2Solution: (D)Option C will encode the Parameter column will 2 levels but will not perform one hot encoding. Therefore, option D is the correct solution.Question context 13dataframe13) We wish to calculate the correlation between Column2 and Column3 of a dataframe. Which of the below codes will achieve the purpose?A) corr(dataframe$column2,dataframe$column3)B) (cov(dataframe$column2,dataframe$column3))/(var(dataframe$column2)*sd(dataframe$column3))C)(sum(dataframe$Column2*dataframe$Column3)-         (sum(dataframe$Column2)*sum(dataframe$Column3)/nrow(dataframe)))/(sqrt((sum(dataframe$Column2*dataframe$Column2)-(sum(dataframe$Column2)^3)/nrow(dataframe))*   (sum(dataframe$Column3*dataframe$Column3)-(sum(dataframe$Column3)^2)/nrow(dataframe))))D) None of the AboveSolution: (D)In option A, corr is the wrong function name. Actual function name to calculate correlation is cor. In option B, it is the standard deviation which should be the denominator and not variance. Similarly, the formula in Option C is wrong. Therefore, Option D is the correct solution.Question Context 14dataframe14) The above dataset has been loaded for you in R in a variable nameddataframewith first row representing the column name. Which of the following code will select only the rows for which parameter is Alpha?A) subset(dataframe, Parameter=Alpha)B) subset(dataframe, Parameter==Alpha)C) filter(dataframe,Parameter==Alpha)D) Both 2 and 3E) All of the aboveSolution: (D)In option A, there should be an equality operator instead of the assignment operator. Therefore, option D is the correct solution.15) Which of the following function is used to view the dataset in spreadsheet like format?A) disp()B) View()C) seq()D) All of the AboveSolution : (B)Option B is the only option that will show the dataset in the spreadsheet format. Therefore, option B is the correct solution.Question Context 16The below dataframe is stored in a variable named data.data16) Suppose B is a categorical variable and we wish to draw a boxplot for every level of the categorical level. Which of the below commands will help us achieve that?A) boxplot(A,B,data=data)B) boxplot(A~B,data=data)C) boxplot(A|B,data=data)D) None of the aboveSolution: (B)Boxplot function in R requires a formula input to draw different boxplots by levels of a factor variable. Therefore, Option B is the correct solution.17) Which of the following commands will split the plotting window into 4 X 3 windows and where the plots enter the window column wise.A) par(split=c(4,3))B) par(mfcol=c(4,3))C) par(mfrow=c(4,3))D) par(col=c(4,3))Solution: (B)mfcol argument will ensure that the plots enter the plotting window column wise. Therefore, Option B is the correct solution.Question Context 18A Dataframedfhas the following data:Dates2017-02-282017-02-272017-02-262017-02-252017-02-242017-02-232017-02-222017-02-21After reading above data, we want the following output:Dates28 Tuesday Feb 1727 Monday Feb 1726 Sunday Feb 1725 Saturday Feb 1724 Friday Feb 1723 Thursday Feb 1722 Wednesday Feb 1721 Tuesday Feb 1718) Which of the following commands will produce the desired output?A) format(df,%d %A %b %y)B) format(df,%D %A %b %y)C) format(df,%D %a %B %Y)D) None of aboveSolution: (D)None of the above options will produce the desired output. Therefore, Option D is the correct solution.19) Which of the following command will help us to rename the second column in a dataframe named table from alpha to beta?A) colnames(table)[2]=betaB) colnames(table)[which(colnames==alpha)]=betaC) setnames(table,alpha,beta)D) All of the aboveSolution: (D)All of the above options are different methods to rename the column names of a dataframe.Therefore, option D is the correct solution.Question Context: 20A majority of work in R uses systems internal memory and with large datasets, situations may arise when the R workspace cannot hold all the R objects in memory. So removing the unused objects is one of the solution.20) Which of the following command will remove an R object / variable named santa from the workspace?A) remove(santa)
B) rm(santa)
C) Both
D) NoneSolution : (C)remove and rm , both can be used to clear the workspace. Therefore, option C is the correct solution.21) dplyr is one of the most popular package used in R for manipulating data and it contains 5 core functions to handle data. Which of the following is not one of the core functions of dplyr package?A) select()B) filter()C) arrange()D) summary()Solution: (D)summary is a function in the R base package and not dplyr.Context  Question 22During Feature Selection using the following dataframe (named table), Column1 and Column2 proved to be non-significant. Hence, we would not like to take these two features into our predictive model.table22) Which of the following commands will select all the rows from column 3 to column 6 for the below dataframe named table?A) dplyr::select(table,Column3:Column6)B) table[,3:6]
C) subset(table,select=c(Column3,Column4,Column5,Column6))D) All of the aboveSolution: (D)Option A, B and C are different column sub setting methods in R. Therefore, option D is the correct solution.Context Question 23-24table23) Which of the following commands will select the rows having Alpha values in Column1 and value less than 50 in Column4? The dataframe is stored in a variable named table.A) dplyr::filter(table,Column1==Alpha, Column4<50)B) dplyr::filter(table,Column1==Alpha & Column4<50)C) Both of the aboveD) None of the aboveSolution: (C)Question Context 23-24table24) Which of the following code will sort the dataframe based on Column2 in ascending order and Column3 in descending order?A) dplyr::arrange(table,desc(Column3),Column2)B) table[order(-Column3,Column2),]
C) Both of the aboveD) None of the aboveSolution: (C)Both order and arrange functions can be used to order the columns in R. Therefore, Option C is the correct solution.25) Dealing with strings is an important part of text analytics and splitting a string is often one of the common task performed while creating tokens, etc. What will be the output of following commands?Bpaste(phi,theta,zeta,sep=)partsstrsplit(c(A,B),split= )A) alphaB) betaC) gammaD) phiE) thetaF) zetaSolution : (B)c(A.B) would concatenate A=alpha beta gamma and B=phithetazeta separated by a white space. Upon using strsplit, the two strings will be separated at the white space between A and B into two lists. Parts[[1]][2] tells us to print the second sub element of the first element of the list which is beta. Therefore, option B is the correct solution.26) What will be the output of the following commandA) [FALSE TRUE TRUE FALSE TRUE]
B) [FALSE TRUE TRUE FALSE FALSE]
C) [FALSE FALSE TRUE FALSE FALSE]
D) None of the aboveSolution: (C)The above command will go for the exact match of the passed argument and therefore Option C is the correct solution.Question Context 27Sometimes as a Data Scientist working on textual data we come across instances where we find multiple occurrences of a word which is unwanted. Below is one such string.A) gsub(because,since,A)B) sub(because,since,AC) regexec(because,since,A)D) None of the aboveSolution: (A)sub command will replace only the first occurrence in a string whereas regexec will return a list of positions of the match or -1 if no match occurs. Therefore, Option A is the correct solution.28) Imagine a dataframe created through the following code.Which of the following command will help us remove the duplicate rows based on both the columns?A) df[!duplicated(df),]
B) unique(df)C) dplyr::distinct(df)D) All of the aboveSolution: (D)All the above methods are different ways of removing the duplicate rows based on both the columns. Therefore, Option D is the correct solution.Question Context 29Grouping is an important activity in Data Analytics and it helps us discover some interesting trends which may not be visible easily in the raw data.Suppose you have a dataset created by the following lines of code.29) Which of the following command will help us to calculate the mean bar value grouped by foo variable?A) aggregate(bar~foo,table,mean)B) table::df[,mean(bar),by=foo]
C) dplyr::table%>%group_by(foo)%>%summarize(mean=mean(bar))D) All of the aboveSolution: (D)All the above methods are used to calculate the grouped statistic of a column. Therefore, Option D is the correct solution.30) If I have two vectors x<- c(1,3, 5) and y<-c(3, 2), what is produced by the expression cbind(x, y)?A) a matrix with 2 columns and 3 rowsB) a matrix with 3 columns and 2 rowsC) a data frame with 2 columns and 3 rowsD) a data frame with 3 columns and 2 rowsSolution: (D)All of the above options define messy data and hence Option D is the correct solution.31) Which of the following commands will convert the following dataframe named maverick into the one shown at the bottom?Input Dataframe maverickOutput dataframeA) tidyr::Gather(maverick, Sex,Count,-Grade)B) tidyr::spread(maverick, Sex,Count,-GradeC) tidyr::collect(maverick, Sex,Count,-Grade)D) None of the aboveSolution: (A)Spread command converts rows into columns whereas there is no collect command in tidyr or base package.Therefore, Option A is the correct solution.32) Which of the following command will help us to replace every instance of Delhi with Delhi_NCR in the following character vector?A) gsub(Delhi,Delhi_NCR,C)B) sub(Delhi,Delhi_NCR,C)C) Both of the aboveD) None of the aboveSolution: (C)Though sub command only replaces the first occurrence of a pattern. In this case, strings have just a single appearance of Delhi. Hence, both gsub and sub command will work in this situation. Therefore, Option C is the correct solution.Question Context 33Sometimes creating a feature which represents whether another variable has missing values or not can prove to be very useful for a predictive model.Below is a dataframe which has missing values in one of its columns.
33) Which of the following commands will create a column named missing with value 1 where variable Feature2 has missing values?A)dataframe$missing<-0dataframe$Missing[is.na(dataframe$Feature2)]<-1B)dataframe$missing<-0dataframe$Missing[which(is.na(dataframe$Feature2))]<-1C) Both of the aboveD) None of the aboveSolution: (C)Option C is the correct answer.34) Suppose there are 2 dataframes A and B. A has 34 rows and B has 46 rows. What will be the number of rows in the resultant dataframe after running the following command?A) 46B) 12C) 34D) 80Solution: (C)all.x forces the merging to take place on the basis of A and hence will contain the same number of rows as of A. Therefore, Option C is the correct solution.Question context 35The very first thing that a Data Scientist generally does after loading dataset is find out the number of rows and columns the dataset has. In technical terms, it is called knowing the dimensions of the dataset. This is done to get an idea about the scale of data that he is dealing with and subsequently choosing the right techniques and tools.35) Which of the following command will not help us to view the dimensions of our dataset?A) dim()B) str()C) View()D) None of the aboveSolution: (C)View command will print the dataset to the console in a spreadsheet like format but will not help us to view the dimensions. Therefore, option C is the correct solution.Question context 36Sometimes, we face a situation where we have two columns of a dataset and we wish to know which elements of the column are not present in another column. This is easily achieved in R using the setdiff command.dataframe36) What will be the output of the following command?A) TRUEB)FALSEC) Cant SaySolution: (B)The order of arguments matter in setdiff function. Therefore, option B is the correct solution.Question Context 37The below dataset is stored in a variable calledframe.37) Which of the following commands will create a bar plot for the above dataset. Use the values from Column B to represent the height of the bar plot.A) ggplot(frame,aes(A,B))+geom_bar(stat=identity)B) ggplot(frame,aes(A,B))+geom_bar(stat=bin)C) ggplot(frame,aes(A,B))+geom_bar()D) None of the aboveSolution: (A)stat=identity will ensure the values in column B become the height of the bar. Therefore, Option A is the correct solution.Question Context 3838) We wish to create a stacked bar chart for cyl variable with stacking criteria Being vs Variable. Which of the following commands will help us perform this action?A)qplot(factor(cyl),data=mtcars,geom=bar,fill=factor(vs)B) ggplot(mtcars,aes(factor(cyl),fill=factor(vs)))+geom_bar()C) All of the aboveD) None of the aboveSolution: (C)Both options A and B will create a stacked bar chart guided by the fill parameter. Therefore, option C is the correct solution.39) What is the output of the command  paste(1:3,c(x,y,z),sep=) ?A) [1 2 3x y z]
B) [1:3x y z]
C) [1x 2y 3z]
D) None of the aboveSolution: (C)Question Context 40R has a rich library reserve for drawing some of the very high end graphs and plots and many a times you want to save the graphs for presenting your findings to someone else. Saving your plots to a PDF file is one such option.40) If you want to save a plot to a PDF file, which of the following is a correct way of doing that?A) Construct the plot on the screen device and then copy it to a PDF file with dev.copy2pdf().B) Construct the plot on the PNG device with png(), then copy it to a PDF with dev.copy2pdf().C) Open the PostScript device with postscript(), construct the plot, then close the device with dev.off().D) Open the screen device with quartz(), construct the plot, and then close the device with dev.off().Solution: (A)The plots are first created on the screen device and then can be copied easily to a pdf file. Therefore, option A is the correct solution.If you are learning R, you should use the test above to check your skills in R. If you have any questions or doubts, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/05/40-questions-r-for-data-science/
"40 Questions to test a data scientist on Machine Learning [Solution: SkillPower  Machine Learning, DataFest 2017]",Learn everything about Analytics|Introduction|Overall Scores|Useful Resources|Questions & Solutions,"End Notes|Share this:|Related Articles|40 questions to test your skill on R for Data Science|5 AI applications in Banking to look out for in next 5 years|
Ankit Gupta
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine Learning is one of the most sought after skills these days. If you are a data scientist, then you need to be good at Machine Learning  no two ways about it. As part of DataFest 2017, we organized various skill tests so that data scientists can assess themselves on these critical skills. These tests included Machine Learning, Deep Learning, Time Series problems and Probability. This article will lay out the solutions to the machine learning skill test. If you missed out on any of the above skill tests, you can still check out the questions and answers through the articles linked above.In Machine Learning skill test, more than 1350 people registered for the test. The test was designed to test your conceptual knowledge in machine learning and make you industry ready. If you missed on the real time test, you can still read this article to find out how you could have answered correctly.Here are the leaderboard rankings for all the participants.These questions, along with hundreds of others, are part of our Ace Data Science Interviews course. Its a comprehensive guide, with tons of resources, to crack data science interviews and land your dream role! And if youre just starting your data science journey, then check out our most popular course  Introduction to Data Science!Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 210 people participated in the skill test and the highest score obtained was 36. Here are a few statistics about the distribution.Mean Score: 19.36Median Score: 21Mode Score: 27Machine Learning basics for a newbieEssentials of Machine Learning Algorithms (with Python and R Codes)Deep Learning vs. Machine Learning  the essential differences you need to know!Introduction to Data Science CourseAce Data Science Interviews CourseQuestion ContextA feature F1 can take certain value: A, B, C, D, E, & F and represents grade of students from a college.1) Which of the following statement is true in following case?A)Feature F1 is an example of nominal variable.
B)Feature F1 is an example of ordinal variable.
C)It doesnt belong to any of the above category.
D)Both of theseSolution: (B)Ordinal variables are the variables which has some order in their categories. For example, grade A should be consider as high grade than grade B.2) Which of the following is an example of a deterministic algorithm?A) PCAB) K-MeansC) None of the aboveSolution: (A)A deterministic algorithm is that in which output does not change on different runs. PCA would give the same result if we run again, but not k-means.3) [True or False] A Pearson correlation between two variables is zero but, still their values can still be related to each other.A)TRUEB)FALSESolution: (A)Y=X2. Note that, they are not only associated, but one is a function of the other and Pearson correlation between them is 0.4) Which of the following statement(s) is / are true for Gradient Decent (GD) and Stochastic Gradient Decent (SGD)?A)Only 1B)Only 2C) Only3D) 1 and 2E)2 and 3F)1,2 and 3Solution: (A)In SGD for each iteration you choose the batch which is generally contain the random sample of data But in case of GD each iteration contain the all of the training observations.5) Which of the following hyper parameter(s), when increased may cause random forest to over fit the data?A)Only 1B)Only 2C) Only3D) 1 and 2E) 2 and 3F)1,2 and 3Solution: (B)Usually, if we increase the depth of tree it will cause overfitting. Learning rate is not an hyperparameter in random forest. Increase in the number of tree will cause under fitting.6) Imagine, you are working with Analytics Vidhya and you want to develop a machine learning algorithm which predicts the number of views on the articles.Your analysis is based on features like author name, number of articles written by the same author on Analytics Vidhya in past and a few other features. Which of the following evaluation metric would you choose in that case?A) Only1B) Only2C)Only 3D)1 and 3E)2 and 3F)1 and 2Solution:(A)You can think that the number of views of articles is the continuous target variable which fall under the regression problem. So, mean squared error will be used as an evaluation metrics.7) Given below are three images (1,2,3). Which of the following option is correct for these images?A)B)C)
A)1 is tanh, 2 is ReLU and 3 is SIGMOID activation functions.B)1 is SIGMOID, 2 is ReLU and 3 is tanh activation functions.C)1 is ReLU, 2 is tanh and 3 is SIGMOID activation functions.D)1 is tanh, 2 is SIGMOID and 3 is ReLU activation functions.Solution: (D)The range of SIGMOID function is [0,1].The range of the tanh function is [-1,1].The range of the RELU function is [0, infinity].So Option D is the right answer.8) Below are the 8 actual values of target variable in the train file.[0,0,0,1,1,1,1,1]What is the entropy of the target variable?A) -(5/8 log(5/8) + 3/8 log(3/8))B) 5/8 log(5/8) + 3/8 log(3/8)C) 3/8 log(5/8) + 5/8 log(3/8)D) 5/8 log(3/8)  3/8 log(5/8)Solution: (A)The formula for entropy is So the answer is A.9) Lets say, you are working with categorical feature(s) and you have not looked at the distribution of the categorical variable in the test data.You want to apply one hot encoding (OHE) on the categorical feature(s). What challenges you may face if you have applied OHE on a categorical variable of train dataset?A) All categories of categorical variable are not present in the test dataset.B) Frequency distribution of categories is different in train as compared to the test dataset.C) Train and Test always have same distribution.D) Both A and BE) None of theseSolution: (D)Both are true, The OHE will fail to encode the categories which is present in test but not in train so it could be one of the main challenges while applying OHE. The challenge given in option B is also true you need to more careful while applying OHE if frequency distribution doesnt same in train and test.10) Skip gram model is one of the best models used in Word2vec algorithm for words embedding. Which one of the following models depict the skip gram model?A) AB)BC) Both A and BD) None of theseSolution: (B)Both models (model1 and model2) are used in Word2vec algorithm. The model1 represent a CBOW model where as Model2 represent the Skip gram model.11) Lets say, you are using activation function X in hidden layers of neural network. At a particular neuron for any given input, you get the output as -0.0001. Which of the following activation function could X represent?A) ReLUB) tanhC) SIGMOIDD) None of theseSolution: (B)The function is a tanh because the this function output range is between (-1,-1).12) [True or False] LogLoss evaluation metric can have negative values.A)TRUE
B)FALSESolution: (B)Log loss cannot have negative values.13) Which of the following statements is/are true about Type-1 and Type-2 errors?A) Only1B) Only2C) Only3D)1 and 2E) 1 and 3F) 2 and 3Solution: (E)In statistical hypothesis testing, a type I error is the incorrect rejection of a true null hypothesis (a false positive), while a type II error is incorrectly retaining a false null hypothesis (a false negative).14) Which of the following is/are one of the important step(s) to pre-process the text in NLP based projects?A)1 and 2B)1 and 3C)2 and 3D)1,2 and 3Solution: (D)Stemming is a rudimentary rule-based process of stripping the suffixes (ing, ly, es, s etc) from a word.Stop words are those words which will have not relevant to the context of the data for example is/am/are.Object Standardization is also one of the good way to pre-process the text.15) Suppose you want to project high dimensional data into lower dimensions. The two most famous dimensionality reduction algorithms used here are PCA and t-SNE. Lets say you have applied both algorithms respectively on data X and you got the datasets X_projected_PCA , X_projected_tSNE.Which of the following statements is true for X_projected_PCA & X_projected_tSNE ?A)X_projected_PCA will have interpretation in the nearest neighbour space.B)X_projected_tSNE will have interpretation in the nearest neighbour space.C) Both will have interpretation in the nearest neighbour space.D) None of them will have interpretation in the nearest neighbour space.Solution: (B)t-SNE algorithm consider nearest neighbour points to reduce the dimensionality of the data. So, after using t-SNE we can think that reduced dimensions will also have interpretation in nearest neighbour space. But in case of PCA it is not the case.Context: 16-17Given below are three scatter plots for two features (Image 1, 2 & 3 from left to right).16) In the above images, which of the following is/are example of multi-collinear features?A) Features in Image 1B) Features in Image 2C) Features in Image 3D) Features in Image 1 & 2E) Features in Image 2 & 3F) Features in Image 3 & 1Solution: (D)In Image 1, features have high positive correlation where as in Image 2 has high negative correlation between the features so in both images pair of features are the example of multicollinear features.17) In previous question, suppose you have identified multi-collinear features. Which of the following action(s) would you perform next?A)Only 1B)Only 2C)Only 3D)Either 1 or 3E)Either 2 or 3Solution:(E)You cannot remove the both features because after removing the both features you will lose all of the information so you should either remove the only 1 feature or you can use the regularization algorithm like L1 and L2.18) Adding a non-important feature to a linear regression model may result in.A)Only 1 is correctB) Only 2 is correctC) Either 1 or 2D) None of theseSolution: (A)After adding a feature in feature space, whether that feature is important or unimportant features the R-squared always increase.19) Suppose, you are given three variables X, Y and Z. The Pearson correlation coefficients for (X, Y), (Y, Z) and (X, Z) are C1, C2 & C3 respectively.Now, you have added 2 in all values of X (i.enew values become X+2), subtracted 2 from all values of Y (i.e. new values are Y-2) and Z remains the same. The new coefficients for (X,Y), (Y,Z) and (X,Z) are given by D1, D2 & D3 respectively. How do the values of D1, D2 & D3 relate to C1, C2 & C3?A)D1= C1, D2 < C2, D3 > C3B) D1 = C1, D2 > C2, D3 > C3C) D1 = C1, D2 > C2, D3 < C3D) D1 = C1, D2 < C2, D3 < C3E) D1 = C1, D2 = C2, D3 = C3F) Cannot be determinedSolution: (E)Correlation between the features wont change if you add or subtract a value in the features.20) Imagine, you are solving a classification problems with highly imbalanced class. The majority class is observed 99% of times in the training data.Your model has 99% accuracy after taking the predictions on test data. Which of the following is true in such a case?A)1 and 3B)1 and 4C)2 and 3D)2 and 4Solution: (A)Refer the question number 4 fromin this article.21) In ensemble learning, you aggregate the predictions for weak learners, so that an ensemble of these models will give a better prediction than prediction of individual models.Which of the following statements is / are true for weak learners used in ensemble model?A)1 and 2B)1 and 3C)2 and 3D)Only 1E)Only 2F)None of the aboveSolution: (A)Weak learners are sure about particular part of a problem. So, they usually dont overfit which means that weak learners have low variance and high bias.22) Which of the following options is/are true for K-fold cross-validation?A) 1 and 2B) 2 and 3C)1 and 3D)1,2 and 3Solution: (D)Larger k value means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) and higher running time (as you are getting closer to the limit case: Leave-One-Out CV). We also need to consider the variance between the k folds accuracy while selecting the k.Question Context 23-24Cross-validation is an important step in machine learning for hyper parameter tuning. Lets say you are tuning a hyper-parameter max_depth for GBM by selecting it from 10 different depth values (values are greater than 2) for tree based model using 5-fold cross validation.Time taken by an algorithm for training (on a model with max_depth 2) 4-fold is 10 seconds and for the prediction on remaining 1-fold is 2 seconds.Note: Ignore hardware dependencies from the equation.23) Which of the following option is true for overall execution time for 5-fold cross validation with 10 different values of max_depth?A) Less than 100 secondsB) 100  300 secondsC) 300  600 secondsD) More than or equal to 600 secondsC) None of the aboveD) Cant estimateSolution: (D)Each iteration for depth 2 in 5-fold cross validation will take 10 secs for training and 2 second for testing. So, 5 folds will take 12*5 = 60 seconds. Since we are searching over the 10 depth values so the algorithm would take 60*10 = 600 seconds. But training and testing a model on depth greater than 2 will take more time than depth 2 so overall timing would be greater than 600.24) In previous question, if you train the same algorithm for tuning 2 hyper parameters say max_depth and learning_rate.You want to select the right value against max_depth (from given 10 depth values) and learning rate (from given 5 different learning rates). In such cases, which of the following will represent the overall time?A)1000-1500 secondB)1500-3000 SecondC) More than or equal to 3000 SecondD) None of theseSolution: (D)Same as question number 23.25) Given below is a scenario for training error TE and Validation error VE for a machine learning algorithm M1. You want to choose a hyperparameter (H) based on TE and VE.Which value of H will you choose based on the above table?A) 1B) 2C)3D)4E)5Solution: (D)Looking at the table, option D seems the best26) What would you do in PCA to get the same projection as SVD?A) Transform data to zero meanB) Transform data to zero medianC) Not possibleD) None of theseSolution:(A)When the data has a zero mean vector PCA will have same projections as SVD, otherwise you have to centre the data first before taking SVD.Question Context 27-28Assume there is a black box algorithm, which takes training data with multiple observations (t1, t2, t3,.. tn) and a new observation (q1). The black box outputs the nearest neighbor of q1 (say ti) and its corresponding class label ci.You can also think that this black box algorithm is same as 1-NN (1-nearest neighbor).27) It is possible to construct a k-NN classification algorithm based on this black box alone.Note: Where n (number of training observations) is very large compared to k.A) TRUEB) FALSESolution: (A)In first step, you pass an observation (q1) in the black box algorithm so this algorithm would return a nearest observation and its class.In second step, you through it out nearest observation from train data and again input the observation (q1). The black box algorithm will again return the a nearest observation and its class.You need to repeat this procedure k times28) Instead of using 1-NN black box we want to use the j-NN (j>1) algorithm as black box. Which of the following option is correct for finding k-NN using j-NN?A) 1B) 2C) 3Solution: (A)Same as question number 2729) Suppose you are given 7 Scatter plots 1-7 (left to right) and you want to compare Pearson correlation coefficients between variables of each scatterplot.Which of the following is in the right order?A)1 and 3B)2 and 3C) 1 and 4D) 2 and 4Solution: (B)from image 1to 4 correlation is decreasing (absolute value). But from image 4 to 7 correlation is increasing but values are negative (for example, 0, -0.3, -0.7, -0.99).30) You can evaluate the performance of a binary class classification problem using different metrics such as accuracy, log-loss, F-Score. Lets say, you are using the log-loss function as evaluation metric.Which of the following option is / are true for interpretation of log-loss as an evaluation metric?A)1 and 3B) 2 and 3C) 1 and 2D)1,2 and 3Solution: (D)Options are self-explanatory.Question 31-32Below are five samples given in the dataset.Note: Visual distance between the points in the image represents the actual distance.31) Which of the following is leave-one-out cross-validation accuracy for 3-NN (3-nearest neighbor)?A) 0D) 0.4C) 0.8D) 1Solution: (C)In Leave-One-Out cross validation, we will select (n-1) observations for training and 1 observation of validation. Consider each point as a cross validation point and then find the 3 nearest point to this point. So if you repeat this procedure for all points you will get the correct classification for all positive class given in the above figure but negative class will be misclassified. Hence you will get 80% accuracy.32) Which of the following value of K will have least leave-one-out cross validation accuracy?A)1NNB)3NNC)4NND) All have same leave one out errorSolution: (A)Each point which will always be misclassified in 1-NN which means that you will get the 0% accuracy.33) Suppose you are given the below data and you want to apply a logistic regression model for classifying it in two given classes.You are using logistic regression with L1 regularization. Where C is the regularization parameter and w1 &w2 are the coefficients of x1 and x2.Which of the following option is correct when you increase the value of C from zero to a very large value?A)First w2 becomes zero and then w1 becomes zeroB) First w1 becomes zero and then w2 becomes zeroC) Both becomes zero at the same timeD) Both cannot be zero even after very large value of CSolution: (B)By looking at the image, we see that even on just using x2, we can efficiently perform classification. So at first w1 will become 0. As regularization parameter increases more, w2 will come more and more closer to 0.34) Suppose we have a dataset which can be trained with 100% accuracy with help of a decision tree of depth 6. Now consider the points below and choose the option based on these points.Note: All other hyper parameters are same and other factors are not affected.A)Only 1B) Only 2C) Both 1 and 2D) None of the aboveSolution: (A)If you fit decision tree of depth 4 in such data means it will more likely to underfit the data. So, in case of underfitting you will have high bias and low variance.35) Which of the following options can be used to get global minima in k-Means Algorithm?A)2 and 3B)1 and 3C) 1 and 2D)All of aboveSolution: (D)All of the option can be tuned to find the global minima.36) Imagine you are working on a project which is a binary classification problem. You trained a model on training dataset and get the below confusion matrix on validation dataset.Based on the above confusion matrix, choose which option(s) below will give you correct predictions?A) 1 and 3B)2 and 4C) 1 and 4D) 2 and 3Solution: (C)The Accuracy (correct classification) is (50+100)/165 which is nearly equal to 0.91.The true Positive Rate is how many times you are predicting positive class correctly so true positive rate would be 100/105 = 0.95 also known as Sensitivity or Recall37) For which of the following hyperparameters, higher value is better for decision tree algorithm?A)1 and 2B) 2 and 3C) 1 and 3D) 1, 2 and 3E) Cant saySolution: (E)For all three options A, B and C, it is not necessary that if you increase the value of parameter the performance may increase. For example, if we have a very high value of depth of tree, the resulting tree may overfit the data, and would not generalize well. On the other hand, if we have a very low value, the tree may underfit the data. So, we cant say for sure that higher is better.Context 38-39Imagine, you have a 28 * 28 image and you run a 3 * 3 convolution neural network on it with the input depth of 3 and output depth of 8.Note: Stride is 1 and you are using same padding.38) What is the dimension of output feature map when you are using the given parameters.A) 28 width, 28 height and 8 depthB) 13 width, 13 height and 8 depthC) 28 width, 13 height and 8 depthD) 13 width, 28 height and 8 depthSolution: (A)The formula for calculating output size isoutput size = (N  F)/S + 1where, N is input size, F is filter size and S is stride.Read this article to get a better understanding.39) What is the dimensions of output feature map when you are using following parameters.A) 28 width, 28 height and 8 depthB) 13 width, 13 height and 8 depthC) 28 width, 13 height and 8 depthD) 13 width, 28 height and 8 depthSolution: (B)Same as above40) Suppose, we were plotting the visualization for different values of C (Penalty parameter) in SVM algorithm. Due to some reason, we forgot to tag the C values with visualizations. In that case, which of the following option best explains the C values for the images below (1,2,3 left to right, so C values are C1 for image1, C2 for image2 and C3 for image3 ) in case of rbf kernel.A) C1 = C2 = C3B) C1 > C2 > C3C) C1 < C2 < C3D)None of theseSolution: (C)Penalty parameter C of the error term. It also controls the trade-off between smooth decision boundary and classifying the training points correctly. For large values of C, the optimization will choose a smaller-margin hyperplane. Read more here.I hope you enjoyed the questions and were able to test your knowledge about machine learning. If you have any questions or doubts, feel free to post them below.Check out all the upcoming eventshere.",https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-machine-learning-solution-skillpower-machine-learning-datafest-2017/
5 AI applications in Banking to look out for in next 5 years,Learn everything about Analytics|Introduction|Top 10 companies using AI|AI & its relevance to Banking|How small banks can make the most of AI?,"About the Author|Share this:|Like this:|Related Articles|40 Questions to test a data scientist on Machine Learning [Solution: SkillPower  Machine Learning, DataFest 2017]|Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine intelligence is the last invention that humanity will ever need to make.Nick BostromArtificial intelligence is a reality today and it is impacting our lives faster than we can imagine. It is already present everywhere, from Siri in your phone to the Netflix recommendations that you receive on your smart TV. The revolution brought by Artificial intelligence has been the biggest in some time. There is no denying that it has already become a crucial and integral part of our life.Artificial intelligence is the blend of three advanced technologies  machine learning, natural language processing and cognitive computing. The concept of Artificial Intelligence is to simulate the intelligence of humans into artificial machines with the help of sophisticated machine learning and natural language processing algorithms. The prime motive for the idea of transferring the intelligence from humans to machines is to overcome the very barrier of human intelligence: scalability. Theres always a limit to the speed with which humans can perform the given tasks. Artificial intelligence looks to overcome this very challenge with human intelligence by transferring the human intelligence to cognitive machines with supreme computational capabilities.Lets take two examples to better understand the concept of artificial intelligence:These are scenarios where artificial intelligence is focusing on, to simulate the mapping of inputs to outputs as it happens in a human brain which makes very difficult tasks for computers like image recognition, sarcasm detection, voice recognition, etc. seamlessly easy for even an 8-year old kid.There are many use cases for AI in a variety of industries. Bizofit, a platform that intelligently connects enterprises with appropriate service providers, has compiled the following list of top 10 AI companies.One of the leading artificial intelligence companies, AIBrain builds AI solutions for smartphones devices primarily. Their key area of expertise is robotics and digital personal assistant.Anki is another company in AI domain which has received funding from over $157.5 million from the likes of J.P. Morgan and other Ventures. The flagship robot of Anki  Cozmo  is one of the most emotionally intelligent robot while dealing with customers.Banjo has raised over $100 million worth of funding till now.They use the strong social media analytics from multiple social media platforms to identify the events taking place around the globe.iCarbonX is an artificial intelligence based startup in health care domain. They provide individualized health analysis and prediction of health index through the use of advanced data mining and machine analysis technologies. iCarbonX is valued at more than $1 billion USD.Jibo is the first robot in the world made to help families with their daily tasks. Also, it learns about the behavior and personality of family as it interacts with them.Next IT applies AI in healthcare and finance industries with focus mainly on natural language processing, chatbots and machine learning.Being one of the most popular iOS app, Prisma brought a revolution in mobile app industry with the use of deep learning algorithms to recreate images as if they were painted.ReSnap using AI and deep learning to take a large number of images from the user and create beautiful photo books out of these images. AI helps in selecting images and chooses best for the photobook.ViSenze is revolutionizing the e-commerce market by recommending visually similar products out of the several millions products. They use deep learning and computer vision. They recently raised $10.5 million for developing their AI technologies.X.ais virtual assistant is helping busy people schedule meetings without any human intervention. As soon as you copy a mail to Amy, it makes sure that with the use of natural language processing and machine learning, it identifies the most suitable time and place for your meeting.In recent years, if Artificial Intelligence has impacted one industry more than any other, its the Banking industry. For organizations working in the banking industry, it has become increasingly crucial to keep up with competition, and increase their standing as an innovative company. The following graphic shows reasons for its widespread adoption in Banking & Financial Services.Source: financialbrand.comArtificial intelligence has several applications in the banking industry.Here are five key applications of artificial intelligence in the Banking industry that will revolutionize the industry in the next 5 years.Anti-money laundering (AML) refers to a set of procedures, laws or regulations designed to stop the practice of generating income through illegal actions. In most cases, money launderers hide their actions through a series of steps that make it look like money that came from illegal or unethical sources are earned legitimately.Most of the major banks across the globe are shifting from rule based software systems to artificial intelligence based systems which are more robust and intelligent to the anti-money laundering patterns. Over the coming years, these systems are only set to become more and more accurate and fast with the continuous innovations and improvements in the field of artificial intelligence.Chat bots are artificial intelligence based automated chat systems which simulate human chats without any human interventions. They work by identifying the context and emotions in the text chat by the human end user and respond to them with the most appropriate reply. With time, these chat bots collect massive amount of data for the behaviour and habits of the user and learns the behaviour of user which helps to adapts to the needs and moods of the end user.Chat bots are already being extensively used in the banking industry to revolutionize the customer relationship management at personal level. Bank of America plans to provide customers with a virtual assistant named Erica who will use artificial intelligence to make suggestions over mobile phones for improving their financial affairs. Allo, released by Google is another generic realization of chat bots.Plenty of Hedge funds across the globe are using high end systems to deploy artificial intelligence models which learn by taking input from several sources of variation in financial markets and sentiments about the entity to make investment decisions on the fly. Reports claim that more than 70% of the trading today is actually carried out by automated artificial intelligence systems. Most of these hedge funds follow different strategies for making high frequency trades (HFTs) as soon as they identify a trading opportunity based on the inputs.A few hedge funds active in AI space are: Two Sigma, PDT Partners, DE Shaw, Winton Capital Management, Ketchum Trading, LLC, Citadel, Voleon, Vatic Labs, Cubist, Point72, Man AHL.Fraud detection is one of the fields which has received massive boost in providing accurate and superior results with the intervention of artificial intelligence. Its one of the key areas in banking sector where artificial intelligence systems have excelled the most. Starting from the early example of successful implementation of data analysis techniques in the banking industry is the FICO Falcon fraud assessment system, which is based on a neural network shell to deployment of sophisticated deep learning based artificial intelligence systems today, fraud detection has come a long way and is expected to further grow in coming years.Recommendation engines are a key contribution of artificial intelligence in banking sector. It is based on using the data from the past about users and/ or various offerings from a bank like credit card plans, investment strategies, funds, etc. to make the most appropriate recommendation to the user based on their preferences and the users history. Recommendation engines have been very successful and a key component in revenue growth accomplished by major banks in recent times.With Big Data and faster computations, machines coupled with accurate artificial intelligence algorithms are set to play a major role in how recommendations are made in banking sector. For further reading on recommendation engines, you can refer to the complete guide of how recommendation engines work.In several of our conversations with executives of smaller banks like Community banks in the US, it became very apparent that they were seeking a differentiator in their intense competition with the larger banks. Big banks are using cutting edge artificial intelligence techniques by using in-house teams of Data Scientists and Quants for risk assessment, financial analysis, portfolio management, credit approval process, KYC & anti-money laundering systems. On the other hand, small banks can use AI for achieving operational efficiency and better customer interactions.Some of the several applications of AI that smaller banks can benefit from are:In conclusion, it is evident that AI is here to stay, and is impacting a large number of industries, Banking is an early adopter of this trend. This trend is likely to grow exponentially in the future. Companies that embrace this trend are likely to be winners over the next 10 years.Devendra Mangani, Sr. Consultant, BizofitHaving 12+ years of experience in strategy, business planning, B2B IT sales and raising capital for startups and companies in IT sector. Experience in managing multiple stakeholders and worked with global teams in previous companies including investment bank. He is guest faculty at management colleges and does workshops on design thinking. Devendra brings in strong understanding of research reports and consulting for building research capabilities of Bizofit. He is an IIT Bombay graduate and MBA from Queens University in Canada.",https://www.analyticsvidhya.com/blog/2017/04/5-ai-applications-in-banking-to-look-out-for-in-next-5-years/
"Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017","Learn everything about Analytics|Introduction|About the Competition|Problem Statement|Winners|Approach & code of all the winners|Sonny Laskar, Rank 2|Rohan Rao & Mark Landry, Rank 1|End Notes","Aanish Singla, Rank 3||Check out all the upcoming competitions here|Share this:|Like this:|Related Articles|5 AI applications in Banking to look out for in next 5 years|Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?|
Sunil Ray
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Key findings from analysis were,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Vidhya just completed 4 years and we had to make sure we mark this event in style! We did that by creating our most unique hackathon  Xtreme ML Hack as part of DataFest 2017. We were looking for an exciting real life problem and would want to thanksour sponsor Aigues de Barcelonafor providing us with an opportunity to host this wonderful competition.Xtreme ML Hackstartedon 20 April 2017 and went on for 4 days. We saw more than2400 participants from across the globe. There were multiple things which were unique for this competition. This was the first time we were working for a Spainish company. This was also an unique hackathon because the participants were allowed to use any available open data. Also, this was the fist time we were working with an utility company.I think this was probably the most challenging hackathon we have released till now  this is as close as you can get to a real life project. Like always, the winners of the competition have generously shared their detailed approach and the codes they used in the competition.If you missed out the fun, make sure you participate in the upcoming MiniHack  The QuickSolver.The problem statement revolved aroundpublic-private company Aiges de Barcelona. It manages the integral cycle of water, from the uptake to the drinking process, transport and distribution, besides the sanitation and the purification of waste water for its return to the natural environment or its re-use. It offers service to about 3 million people in the municipalities of the metropolitan area of Barcelona. It also manages a customer service to ensure excellence in their services.Customer Service is one of the top priorities of the company. They want to redesign the customer service using machine learning.To respond to their customers efficiently they want to predict the volume and typologies of contacts to the call center. They also want to forecast the number of contacts on daily basis. The task was to forecast the number of contacts & resolutions the company should open on daily basis.The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1: Rohan Rao & Mark LandryRank 2: Sonny LaskarRank 3: Aanish SinglaHere are the final rankings of all the participants at theleaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go ahead.Heres what Aanishshared with us.Aanish says I started out by building an understanding of how autonomous communities, provinces, municipalities and districts are related to each other in Spain. Then I looked at the data dictionary and what all was provided and what was expected, followed by exploratory data analysis.For Contacts:For ResolutionsI also figured out that there was a close correlation between new contracts, ended contracts and number of contacts. I had 2 choices.I used the second approach.Predicting contactsI used top down approach as the there was only 1 level involved. I predicted the contacts at day level using Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components (tbats) using weekly, quarterly and yearly seasonality. To accommodate the trends at type level, I found the average contribution by type for 2016 and for all years (some types had different trends in recent years). Using separate contribution percentages by types and for weekdays and weekends, I split the overall forecast into detailed forecast.Predicting ResolutionResolutions had 2 levels (category and subject), hence top down approach was not suitable. Also, breaking the high-level forecast into 2 levels would be quite error prone so I selected hierarchical time series modeling for this. I used tbats() for each subject to account for multiple seasonality on the data. Most of the effort was in preparing data in the format accepted by hierarchical time series modeling function, hts(). I used only last 3 years of data (optimized after first submission) as that was having lesser missing values and avoided the spiked 2013 data. Combining of low level forecast with top level was done using LU decomposition algorithm.Prediction from this resulted in some negative values, especially for the weekend. I replaced them with respective mean from 2016 weekend data.On submission, I had a score of 104.6 but decided not to change the model too much as it was very generic.Solution: Link to CodeHeres what Sonny shared with us.He says This was one of the best competitions till now on Analytics Vidhya. Moreover, the introduction of submission limits made it even more interesting. This competition was definitely not a good place to start from if you had just started learning ML. My first thought on looking at the problem made me think that it had to be solved as a time-series problem which later seemed to be wrong.Modeling ApproachIt was a no-brainer to find that the only data in the Test Set were future dates. Did that look odd? Come to reality! The variable to predict was # of contacts/resolutions. After some initial analysis, I decided that this has to be treated as two different problems.For the modeling purpose, I created day wise-medium wise-department wise aggregated # of contacts and resolutions from 2010 onwards (since we had data on contacts/resolutions only after 2010). For cross-validation, I decided to use the last four months. So my first model was built on few Date features and it scored 101.X which was a very good score on Day1.The next thing that struck me was that holidays should have an impact on these contacts and resolutions. Hence I created a list of holidays in Spain since 2010. Just adding holidays to the above list improved the score to 93.X. I was feeling good but it didnt last longer since I saw Mark jumping to 78.X.Later, I decided to add lag features of # of contacts/resolutions from the past. After a few iterations, I decided to keep lag features of 75, 90 and 120 days. With this, the score improved to 64.X (ahead of Mark J). Rohan was around 114.X so I knew that he was either asleep or solving Soduku. This was on Saturday morning and I was unable to find any additional features that could help me move ahead. So I decided to take a break. By evening, I noticed that Rohan had woken up and was also at around 78.X (Thats when I guess Mark and Rohan decided to team up).On Sunday, I added a feature on number of days (percentile) elapsed since last holiday and that added few points. My final 10-bags XGboost ensemble scored 61.47081 on the public leaderboard which I selected for final submission. I had around 8 submissions left which I wish I could donate to few folks who were not aware that they could not mark their final submissions if they didnt upload code L.Huh!It might sound that the journey of adding features and improving scores was a very smooth line something like this.But it was not. I tried many things which didnt work.Below are few approaches that I tried but didnt seem to add value:Overall, this was a different problem and really enjoyed solving it. Thanks to Mark and Rohan for a tough fight.Heres what Mark & Rohan shared with us.Mark LandryI kept the feature space fairly small.Calendar Features:
day of the week, week of the year, the day of the year & year.Holiday Features (all binary):national, local, observance, common local (perhttps://www.timeanddate.com/holidays/spain/)Average Features (*):average Contacts per day of week and Contact typeaverage Resolution per day of week, category, and subjectOther modeling: used all the dataused all the dataall H2O gbmseparate models for Contacts and Resolutionvalidation: started with Leave-One-Year-Out, finished measuring January  March 2016 (prior year from test predictions)I started modeling with simple averages, as shown above, where the entire data set was used, and those results were applied to the entire dataset. But soon I moved to a more sound version of the calculation where the impact of each record is removed from the calculation so that it is not leaked. And toward the end, I used an entire validation set to where no records of that set were used in the average calculations (similar to the actual prediction environment).Rohan RaoI started exploring the data to get a sense of basic movement of values across the years. I decided to go with granularity and build a model for each type / category in Contacts and Resolution. I ultimately ended up plotting over 2000 graphs and summaries, which really helped identify various patterns.After trying a bunch of features and models, I found historic averages out-performing all ML-based models. So, I decided to focus and tune the average values so as to capture seasonality and trend in the right way. In some ways, this was like a manually-customized time series model.Main features1. There were two highly seasonal components. Weekly and yearly. The following lag features: same week year and weekday in the previous year, previous/next week year and same weekday in the previous year, helped capture seasonality.Besides these core features, other things that worked were:1. Using 21st-Jan, 2016 to 15th-Mar, 2016 as the validation data. This helped in preventing overfitting to public LB.2. Using estimates for holiday dates. Most holidays had 0 volume and highly noisy values. Aligning these correctly across the years helped.3. My non-ML historic averages approach blended extremely well with Marks ML-model which didnt use lag-features. Our simple ensemble gave a lift of almost 10 on RMSE.4. Throw away all data prior to 2015, so as to focus only on the most recent trends.Solution: Link to CodeIt was great fun interacting with these winners and to know their approach during thecompetition. Hopefully, you will be able to evaluate where you missed out.",https://www.analyticsvidhya.com/blog/2017/04/winners-solution-codes-xtreme-mlhack-datafest-2017/
Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?,"Learn everything about Analytics|Brain Teaser|My recent experience with BlackJack||Pre-requisites for the article|What to expect in this article?|Lets get the ball rolling|Deep dive into betting strategy|Enough of this monologue, lets try some hands-on exercise|End Notes","Category 1  Gaming Strategy related questions|Category 2  Betting Strategy|Category 3  A few Fundamental Questions|Simulation 1|Simulation 2|Simulation 5|Simulation 6|Simulation 7|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Winners solutions and approach: Xtreme ML Hack, AV DataFest 2017|Data Scientist- Bangalore (3+ years of experience)|
Tavish Srivastava
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Simulation 3|Simulation 4,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Can we create a flawless winning strategy in a Casino using Data Science?Of course not! Otherwise, all the data scientists out there would be sitting on piles of cash and the casinos would shut us out!But, in this article we will learn how to evaluate if a game in Casino is biased or fair. We will understand the biases working in a casino and create strategies to become profitable. We will also learn howcan we control the probability of goingbankrupt in Casinos.To make the article interactive, I have added few puzzles in the end to use these strategies. If you can crack themthere is no strategy that can make you hedge against loosing in a Casino.Read on!!Before we begin, let me ask you a couple of questions:If your answer for second question is more than half of question one, then you fall in same basket as most of the players going to a Casino (and you make them profitable!).Now heres an interesting fact, for most of the games you play at a Casino, you will win the amount in line with your odds of winning. For example:If the probability of winning in a roulette table is 1/37, you will get 37X of the amount you had bet. Hence, the expected lossesof a trade in Casino is almost equal to zero.Expected gain from roulette = 1/37 * $X * 37 (amount you get if you win)  $X (amount you bet) = 0So, where are we going wrong?Why do our chances of gaining 100%or more are less than 50% but our chances of losing 100% is a lot more than 50%. Lets start with my recent visit to a Casino.Last week, I went to Atlantic City  the casino hub of US east coast. BlackJack has always been my favorite game because of a lot of misconceptions.For the starters, let me take you through how BlackJack is played.There are few important things to note about BlackJack. If you know the game, you can skip the points below:Ace card iscounted as 1 or 11 points, numbered cards (2 to 9) are counted at face value. 10 numbered card and face cards are counted as 10 points.The value of a hand is the sum of the point values of the individual cards. Except, a blackjack is the highest hand, consisting of an ace and any 10-point card, and it outranks all other 21-point hands.If the sum exceeds 21, it is called a burst and whoever has a burst looses right away.The dealer will ask the players for any further bet, and a player can choose to double the bet based on his 2 cards and dealers 1 card. Then the dealer will ask the player if he/sheneeds more cards. Player tries to maximize his score without being burst.After the final bet, the dealer will open more card for him/her. Dealer will keep opening his/her cards, until the value reaches 17 (Dealer might have to open another card if he/she has an Ace card counted as 11, and total sum as 17  this is called soft 17).There are a few more complicated concepts like insurance and split, which is beyond the scope of this article. So, we will keep things simple.I always thought that given the dealer has a constraint of opening cards till he/she reaches 17/18, they cannot stop taking more cards. Hence, a player has better odds of winning than the dealer as he/she has no such constraints. I was excited about all the winning I was about to get!!As soon as I entered the Casino, I separated $X in my purse as Casino money, Got my cash converted into chips, and with in the next hour, I was bankrupt. I was not done yet  In hopes of recovering the money I lost, I donated another $2X in two rounds and I was finally convinced that the game was not as simple as I thoughtand I needed to pull out mydata science hat to win it.Following are a few good to have skills to enjoy the article more:Here are the questions, I will try to answerin this article. They can be broadly classified in three heads:Here is a situation, you see that dealer has a 4 open and following are your cards.Your total score is 14. What would you do?By now, you will know that your cards are really poor but do you take another card and expose yourself to the risk of getting burst OR you will take the chance to stay and let the dealer get burst. A lot of people will recommend you to stay tilldealers cards are as bad as yours  let us check out through simulations.Let us try to calculate the probability of the dealer getting burst. I will first define a few basic functions on R and then simulate dealers hand.So what did I find?Here is the probability distribution given for the first card of the dealer.The probability of the dealer getting burst is 39.6%, which will be players probability of winning. This means you will loose 60% of times  Is that a good strategy? We cant answer that question until unless I know what is the probability of winning if I take one more card and either increase or burst my score.Insight 1  Probability of the dealer getting burst given his/her first card (say is 4) can be found from the table above (39.6% in this case).Now we need to bring in players cards as well and find what is the probability of winning if the player has a 14 in hand. With this additional information, we can make refinement to the probability of winning given our 2 cards and dealers 1 card.The reason the row 21 has a lot of 100% is that with just 2 cards, 21 is a black jack. And if the dealer does not have the same, the Player is definite to win. The probability of winning for the player sum 12-16 should ideally be equal to the probability of dealer going burst. As this is the only way Player is going to win if he/she chooses to stay. Dealer will have to open a new card if it has a sum between 12-16. This is actually the case which validates that our two simulations are consistent. To decide whether it is worth opening another card, calls into question what will be the probability to win if player decides to take another card.Insight 2 If your sum is more than 17 and dealer gets a card 2-6, odds of winning is in your favor. This is even without including Ties.To make this analysis simple, lets say favorable probabilityis Chances to Win + 50% chances to Tie.Favorable probability table if you choose to draw a card is as follows.So what did you learn from here. Is it beneficial to draw a card at 8 + 6 or stay?Favorable probability without drawing a card at 8 + 6 and dealer has 4 ~40%Favorable probability with drawing a card at 8 + 6 and dealer has 4 ~43.5%Clearly you are better off drawing a new card, which was so counter intuitive.Here is the difference of %Favorable events for each of the combination that can help you design a strategy.Cells highlighted in green are where you need to pick a new card. Cells highlighted in pink are all stays. Cells not highlighted are where player can make a random choice, difference in probabilities is indifferent.Insight 2  Follow the below grid to take a decision whether to stay(0s) or hit me (1s).Win Rate = 41.4%Tie Rate = 9.5%Loss Rate = 49.1%SHOCKER!!! Our win rate is far lower than the loss rate of the game. It would have been much better if we just tossed a coin. The biggest difference is that the dealer wins if both the player and the dealer gets burst. If you remove that single condition, here are the win/loss rate.Win Rate = 41.4%Tie Rate = 17.1%Loss Rate = 41.5%As you can see, both dealer and player burst in about 8% of the games. By just changing this small thing, Casinos make sure that we loose much more frequently than the house do.Insight 3  Even with the best strategy, a player wins 41% times as against dealer who wins 49% times. The difference is driven by the tie breaker when both player and dealer goes burst.This is consistent with our burst table, which shows that probability of the dealer getting burst is 28.4%. Hence, both the player and the dealer getting burst will be 28.4% * 28.4% ~ 8%.Now we know what is the right gaming strategy, however, even the best gaming strategy can lead you to about 41% wins and 9% ties, leaving you to a big proportion of losses. Is there a betting strategy that can come to rescue us from this puzzle?The probability of winning in blackjack is known now. To find the best betting strategy, lets simplify the problem. We know that the strategy that works in a coin toss event will also work in black jack. However, coin toss event is significantly less computationally intensive.Avg. number of games won = 50Avg. money you walk out of Casino = $99.74 ~ $100Max money won = $780%times person becomes bankrupt = 63.1%Did any of the above 4 metrics shock you? What got me to thinking was that even though the average value of anyone leaving the casino is same as what one starts with, the percentage times someone becomes bankrupt is much higher than 50%. Also, if you increase the number of games, the percentage times someone becomes bankrupt increases. Why is that?The reason is that we have a lower bound at $0 which is bankruptcy, but we dont have an upper bound. On your lucky days, you can win as much as you can possibly win, and Casino will never stop you saying that Casino is now bankrupt. So in this biased game between you and Casino, for a non-rigged game, both you and Casino has the expected value of no gain no loss. But you have a lower bound and Casino has no lower bound. Simply put, lets assume that you start with $100. If you win you can reach as high as $1000 or even $10k. But the expected value of your final amount is still $100 as the game was even. So, to pull the expected value down, a high number of people like you have to become bankrupt. Let us validate this theory through a simuation using the previously defined functions.In Mathematical style  Hence Proved!Clearly the bankruptcy rate and maximum earning seem correlation. What it means is that the more games you play, your probability of becoming bankrupt and becoming a millionaire both increases simultaneously. So, if it is not your super duper lucky day, you will end up loosing everything. Imagine 10 people P1, P2, P3, P4 .P10.P10 is most lucky, P9 is second in line.P1 is the most unlucky.If all of them start with $100, the first one becoming bankrupt will be P1, and his $100 is divided among the other 9. Next in line of bankruptcy is P2 and so on. So, you might think P3 is on his lucky day, but if he/she plays enough number of games after P4 is gone, he is now fueling the earning of P2 and P1. In no time, P1 and P2 would rob P3. Obviously P2 is next and finally P1 will leave the casino with $1000 in his/her pocket. Casino is just a medium to redistribute wealth if the games are fair and not rigged, which we have already concluded is not the case. If all P1-P10 are playing black jack, I wont be surprised if all of them loose bankrupt as dealer has higher odds of winning a game.Insight 4The more games you play, the chances of your bankruptcy and maximum amount you can win, both increases for a fair game (which itself is a myth).Is there a way to control for this bankruptcy in a non-bias game? Fortunately YES!What if we make the game fair. The lowest you can reach is a loss of $100. Let us now fix the highest profit you reach is also $100, and then you stop no matter what. Lets try to simulate this.Avg. number of games won = 50Avg. money you walk out of Casino = $99.34 ~ $100Max money won = $200%times person becomes bankrupt = 47.5%BINGO! % times reaching $0 is 50% and other 50% reach $200. Now this looks fair! Let us run the same simulation we ran with the earlier strategy.Again mathematician style  Hence Proved! The Bankruptcy rate clearly fluctuates around 50%. You can decrease it even further if you cap your earning at a lower % than 100%. But sadly, no one can cap their winningwhen they are in Casino. And not stopping at 100% makes them more likely to become bankrupt later.Insight 5 The only way to win in a Casino is to decide the limit of winning. On your lucky day, you will actually win that limit. If you do otherwise, you will be bankrupt even in your most lucky day.Here are a few exercise you can try solving and reply back in the comment section.Exercise 1 (Level : Low)  If you set your higher limit of earning as 50% instead of 100%, at what % will your bankruptcy rate reach a stagnation?Exercise 2 (Level : High)  Martingale is a famous betting strategy. The rule is simple, whenever you loose, you make the bet twice of the last bet. Once you win, you come back to the original minimum bet. For instance, your start with $1. You win 3 games and then you loose 3 games and finally you win 1 game. So your series of wins will beBasically what happens is that if you break any loosing streak, you recover all the money you have invested with profit same a minimum bet. For such a betting strategy, find:a. If the expected value of winning changes?b. Does probability of winning changes at the end of a series of game? You can give the results as a function of number of games/minimum bet/total initial money in your pocket.c. Is this strategy any better than our constant value strategy (without any upper bound)? Talk about bankruptcy rate, expect value at the end of series, probability to win more games, highest earning potential.d. Does the strategy makes sense if you play a high number of matches or low number of matches for say  Minimum bet of $1 and Total initial money in pocket $100. High number of matches can be as high as 500, low number of matches can be as low as 10.Exercise 3 (Level  Medium)  For the Martingale strategy, does it make sense to put a cap on earning at 100% to decrease the chances of bankruptcy?Is this strategy any better than our constant value strategy (with 100%upper bound with constantbetting)? Talk about bankruptcy rate, expect value at the end of series, probability to win more games, highest earning potential.Casinos are the best place to apply concepts of mathematics and the worst place to test these concepts. As most of the games are rigged, you will only have fair chances to win while playing against other players, in games like Poker. If there was one thing you want to take away from this article before entering a Casino, that will be always fix the upper bound to %earning. You might think that this is against your winning streak, however, this is the only way to play a level game with Casino.I hope you enjoyed reading this articl. If you use these strategies next time you visit a Casino I bet you will find them extremely helpful. If you have any doubts feel free to post them below.Now, I am sure you are excited enough to solve the three examples referred in this article. Make sure you share your answers with us in the comment section.
You can also read this article on Analytics Vidhya's Android APP ",https://www.analyticsvidhya.com/blog/2017/04/flawless-winning-strategy-casino-blackjack-data-science/
Data Scientist- Bangalore (3+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Creating a flawless winning strategy in a Casino (BlackJack) using Data Science?|Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  8 years
Requirements : 
Task Info : About the company:We provides an AI-augmented on-demand workforce to enterprises for doing human tasks. We believe that in this day & ageenterprises shouldnthire large teams to do operational work.Bybuilding world class enterprise software that combinesthe best of human & machine intelligence, were on our way to displace the incumbents of the BPO industry.We believe that special companies are built only when extremely smartand highlymotivated individuals work towards a common goal. We started from India, went through the Y Combinator & Google Launchpad accelerator programs, and now have an international presence. But its still day one, theres lots to be done.Job SummaryAs our first data scientist, youll be responsible for building our data science products, driving ML innovation across the stack and establishing thought leadership amongst industry experts.Responsibilities and DutiesPositive Background Correlations :Benefits and Perks
College Preference : no-bar
Min Qualification : ug
Skills : Computer Vision, deep learning, machine learning, python, r, statistical techniques, tensor flow
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-scientist-bangalore-3-years-of-experience/
Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Bangalore (3+ years of experience)|Business Analyst- Chennai (2 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 	Strong track record of delivering on high value / critical projects. Ability to communicate with senior stakeholders crisply and confidently on findings, status, bottlenecks and escalations
	Ability to work independently, structure analyses and handle multiple priories
	Proven track record of delivering in a start-up environment would be a plus
Task Info : About CompanyWe are India`s largest financial marketplace for lending and investment products. Our aim is to make personal finance decisions easy, transparent and convenient for India. Here, you can compare loans, credit cards and investment products, from India`s top banks and NBFCs, from anywhere and at any time.We offer a wide suite of products that includePersonal Loan,Credit Card,Home Loan,Loan Against Property,Home Loan Balance Transfer,Car Loan,Education Loan,Savings Accounts,Mutual Funds,Fixed Deposits,Mobile WalletandGold Loan.About DepartmentThe Analytics function supports all business lines from Credit Strategy perspective, with responsibility for driving companywide credit initiatives. Also it drives the other analytics projects of organization importance. The key objectives of the function are:
 Building Predictive models (Application Score Card, Product, Pricing & Portfolio Analytics, Loss Forecasting, Cross Sell Model, Developing Recommendation Algorithm) Creating Credit vision and Framework to ensure we are able to maximise on each visit on our portal Identifying the opportunity area/segments where we can buy the risk and fulfil the lead Business Intelligence: Develop and publish various new age dashboards for the Management & Senior leadership to track key business metrics also real time tracking of each action performed by Customer & Agent on our website and mobile app
The function works directly with CXOs to identify areas of data analysis requirement, define problem statements and develop key insights.Purpose of your role and key accountabilitiesSelected candidate will lead Advance Analytics Team, will have the responsibility of working in a hands-on delivery role with primary focus on building Predictive Models, Credit Score Card, Strategic Analytics Initiatives.
Above responsibilities are just to outline primary focus, however it will not be limited to that. We are an emerging organisation, there is a high possibility that scope of the role will enhance/change time to time based on the needs and organisation priorities.Your skills and experienceAcademic Qualifications : B.E./ Masters in Economics/ Business/ Marketing Science/ Econometrics / Masters in business AnalyticsMandatory Experience: 4-8 year of experience in building Credit & Application Score Card, Predictive Modelling Role (Preferably in Fintech/BFSI/financial service industry)Tools: R, Python
College Preference : no-bar
Min Qualification : ug
Skills : bfsi, credit risk, predictive modeling, python, r
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/sr-manager-manager-data-science-gurgaon-4-to-8-years-of-experience/
Business Analyst- Chennai (2 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Manager/ Manager- Data Science -Gurgaon- (4 to 8 years of experience)|Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : The role needs someone who is good with numbers, has strong communication and interpersonal skills, superior problem solving abilities and analytical thinking with hands on experience in the Retail/ CPG/FMCG analytics domain.
Task Info : About the company:We are working on solving challenges at the point of sale, and in the process enabling retailers to better their business in the evolving world. Webelong to the one of the largest retail merchandising company in India. closeto 5000 employees, 80 offices, operations across 600+ towns and 17 years of deep retail experience.Indian grocery retail is a melting pot of challenges. Some see these challenges as formidable and unsurmountablewe see them as opportunities,we are building solutions to monetise these opportunities.
The role needs someone who is good with numbers, has strong communication and interpersonal skills, superior problem solving abilities and analytical thinking with hands on experience in the Retail/ CPG/FMCG analytics domain. Job DescriptionDesign and Build Statistical and Machine learning Models, Market Basket Analysis and Extraction systemsSkill SetEducation  Graduation in Engineering/mathematics/Science/ Economics/ Statistics/ MBA (preferred) /Masters Preferred Experience:  2-3 yrs. of total exp. in Analytics/ Research. Experience in Retail or FMCG domain will be valuable.Job Location: Chennai
College Preference : no-bar
Min Qualification : pg
Skills : Market Basket analysis, predictive modeling, python, r, Retail Analytics, sas, sql, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/business-analyst-chennai-2-to-5-years-of-experience/
Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience),Learn everything about Analytics|,"Share this:|Like this:|Related Articles|Business Analyst- Chennai (2 to 5 years of experience)|DataHack Hour Revealed  the best way to learn data science through hands on problems!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 	Knowledge of at least a few analytical approaches required
	Should have 1-2 years of experience in project & team management
	Excellent problem solving skills and communication skills required
Task Info : About the company:We are the leading companies leverage big data, analytics & technology to drive smarter, faster & more accurate decisions in every aspect of their business. We serve as a strategic partner to our clients where we consult & deliver a wide range of analytics services for centralized analytics team or individual business units.Job description:Position ExpectationsQualification & ExperienceEducation:Preferably an MBA or an Engineer with relevant experience
College Preference : no-bar
Min Qualification : pg
Skills : marketing analytics, predictive modeling, project management, r, sas
Location : Bengaluru, Gurugram, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/senior-consultant-mumbai-bangalore-gurugram-4-to-5-years-of-experience/
DataHack Hour Revealed  the best way to learn data science through hands on problems!,Learn everything about Analytics|Introduction|Table of Contents|What isDataHack Hour?|Launch of DataHack Hour|Day 1  Webinar|Day 2  Jupyter Notebook and Python Scripts|Day 3  Data Exploration and Visualization|Day 4  Missing Values & Outliers|Day 5  Building a Linear Regression model|Next Steps:|End Notes:,"Share this:|Like this:|Related Articles|Senior Consultant- Mumbai/ Bangalore/ Gurugram (4 to 5 years of experience)|Head of Analytics-Mumbai (7 to 10 years of experience)|
Faizan Shaikh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"As part of DataFest 2017, we launched a new initiative  DataHack Hour. DataHack Hour was inspired by numerous queries we get related to learning Data Science. Questions like How to learn analytics? or How to become a data scientist? are asked to us multiple time every day.While we had written several articles on this subject on Analytics Vidhya  we needed something more definitive to answer these queries. In order to answer these queries, we decided to create an experience to show people how to learn Data Science. This version of DataHack Hour is our answer to the above questions or many more questions which come to us.DataHack Hour is completely free to consume for Analytics Vidhya community and is created with an aim to help more and more people learn data science. This article will tell you the journey participants of DataHack Hour are undergoing. If you are one of the people struggling to learn Data Science  join DataHack Hour today and become part of this awesome experience.DataHack Hour is based on a very simple concept  Daily small improvements or learning in small steps can make a huge difference over time. It is the same principle which Jeff Olson describes in his book The Slight Edge. Let me explain this in a bit more detail.Most of the queries which we receive on Analytics Vidhya about challenges in learning can fall in one of the following categories:We believe that DataHack Hour is the solution to the first 3 problems mentioned here. We believe that by going over one chapter at a time daily, with help of volunteers and mentors from community to help is the most powerful way to learn Data Science. You learn by solving hands on problem, the content has been curated by Analytics Vidhya team and there are mentors to help you out on a daily basis. Honestly, I cant think of a better way to learn!We launched DataHack Hour on 16th April 2017 as part of DataFest 2017. We got outstanding response from the community members and from people who want to really learn the subject.We came across various users like EspyM, who said he does not have access to such resources in his country and in 5 days we have seen him devoting time to build first model and submit a solution to DataHack platform! I am pretty sure that by end of this DataHack Hour, we will have multiple people like EspyM who would enable learning in their own communities later on.In order to raise awareness about DataHack Hour further, we are releasing the content of the first 5 days on our blog. The idea is to put the content out to a larger world and invite people who have missed out on 5 awesome days. You can still join today by learning the content below. You can register for DataHack here.If you registered on DataHack Hour and missed out a particular day, you can go through the content below and come back on track.We kicked offDatahack Hour with this awesome session by Tavish Srivastava. The agenda of the webinar wasHow to convert a business problem to analytics problem? and Importance of hypothesis generation. This is the best place to start your journey about learning analytics. It also touches about the point which gets ignored in a lot of tool focussed courses today.Here is the webinar recording from the session:Hopefully you are all geared up for the hands on exercises to come!From Day 2 onwards we started our 1 hour challenges. The agenda for day 2 included the following:Let us cover them one by one. You can download all Day 1 resources here after Logging in and Signing up. By end of the day you would have installed Anaconda, become comfortable with Jupyter notebook interface and would have written a few simple programs in Python and Pandas. We also cover different data structures in Python, iterative and conditional statement and ways to load and access the data.Our mentor for the day was none other than me This session focussed on some of the practical challenges people face while doing exploratory analysis. Irrespective of how good is your data, you would come across missing values and Outliers. This session was aimed to help people deal with missing values and Outliers in the data. Again, you can access the content here after logging in and registering for DataHack Hour.Topics covered in Missing ValueOutlier detectionOn day 5, people will start build simple predictive models. The sessions starts with talking about what is a predictive model and enables you to build a simple and a multivariate regression model by end of this session. You can download the resources here.Here is the agenda for the coming days. If you think you got stuck in learning analytics and data science in past, come and join us in these DataHack Hour sessions. By end of this DataHack hour, you will be able to work on data science problems independently, would have 10+ mentors you wold have interacted with and a few hundreds of peers. All of this is available freely and can be absorbed as long as you are motivated!Day 6: Feature Engineering and Transformation, helps to improve model performanceDay 7: Validating and measuring model performanceDay 8: Building logistic regression modelDay 9: Building naive bayes modelDay 10: Building a decision tree modelDay 11: Building a k-NN modelDay 12: Ensemble, Methods to combine model outcomesDay 13: Apply your learnings on 6-hours hackathon",https://www.analyticsvidhya.com/blog/2017/04/datahack-hour-solutions-revealed/
Head of Analytics-Mumbai (7 to 10 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|DataHack Hour Revealed  the best way to learn data science through hands on problems!|Senior Machine Learning Scientist- Bangalore (3 to 5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  10 years
Requirements : 	Strong communication and presentation skills
	Attention to detail and ability to work in high pressure environment
Task Info : About the company:We believe buying insurance is not about avoiding risk, its about overcoming it. We hope to create the most simple to use & transparent platform to buy & manage insurance. We exist because buying insurance is currently confusing & painful. It should be easy too understand what is covered in your policy. It should be easy to compare features & pick the right policy. And it should be really easy to buy a policy online. We are here to make it happen, be really easy to buy a policy online. We are here to make it happen.As Head of analytics, you will design, develop and execute data driven solutions for multiple business units across the analytical spectrum for customer engagement and revenue growth.You will be working with the business functions & CXO team to identify business issues and strategic growth areas and enhance quality of decision in various business through analytics. Business Strategy:DataFinanceQualifications & Skills  
College Preference : tier1-any
Min Qualification : pg
Skills : clustering, data visualization, decision trees, linear regression, predictive modeling, python, r, sas, spss
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/head-of-analytics-mumbai-7-to-10-years-of-experience/
Senior Machine Learning Scientist- Bangalore (3 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Head of Analytics-Mumbai (7 to 10 years of experience)|Behavioral Analytics : When Psychology collides with analytics|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 1.Deeply care about the problem we are solving
2. Critical thinking
3. Enjoy user research driven iterative model of development
4. Learn new skills on the go
Task Info : About the company:We build machines that can style people, end to end, just like a personal stylist you can hire. Our vision is to provide every single individual benefit & pleasure of having a personal stylist. Not everyone is celebrity  does not mean they would not desire for it.It is indeed a hard problem & what it takes to solve is proprietary tech including advanced machine learning.Job description:You will play with AI and build machines that can style people, alongside a passionate team having collective work-ex of 40 years.You will work on building a smart vision system that can make styling theory based inferences over images out of thin air. Other streams which you can observe and participate in longer term are recommendation engine and nlp style bot.What you can expect from us: 1. Youll have a voice, opinions arent just welcome, theyre expected 2. Youll work with an experienced mentor in data science 3. Youll work on big problems which have real world applications 4. The problem solving will need you to know,  Computer Vision  Tensorflow, Caffe, Theano, Keras (one of these)  CNN  Predictive modelling  NLP  Python  Algorithms & data structuresWhat we expect from you: 1. Deeply care about the problem we are solving 2. Critical thinking 3. Enjoy user research driven iterative model of development 4. Learn new skills on the go
College Preference : no-bar
Min Qualification : ug
Skills : Caffe, Computer Vision, Keras, nlp, predictive modeling, python, tensor flow, Theano
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/senior-machine-learning-scientist-bangalore-3-to-5-years-of-experience/
Behavioral Analytics : When Psychology collides with analytics,Learn everything about Analytics|Introduction||A small Quiz|Lets analyze your choices now|What tools did I use to make these predictions?|Question 1 underlying principle  The compromise effect|Question 2 & 3underlying principle  The decoyeffect|Question 4 & 5 underlying principle  The anchoringeffect|Question 6 & 7 underlying principle  The steep temporal discountingeffect|Question 8& 9underlying principle  The unknown unknowns effect|Question 10 & 11 underlying principle  The extreme probability bias effect|End Notes,"|On your Marks, Get Set, Go|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Senior Machine Learning Scientist- Bangalore (3 to 5 years of experience)|Lead Data Analyst- Bengaluru (6 to 9 years of experience)|
Tavish Srivastava
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Todays post is going to be very different from all the post I have published till now. For past few months, I have been working on projects which are very close to the end customers. This type of role is uniquely different because I had to take off my analytics hat and place a customers perspective hat.I got shot down a lot with the tools I had in my toolkit. So I thought of upskilling myself with new set of tools. I have been researching a lot on customer behavior and psychology & I was amazed with what all I did not know!Do not expect a lot of machine learning, ensemble modeling, etc. in this article. Against my reputation on AV, I want to take a stab to explain something most of the people in our industry arecompletely unaware of. This article is a crash course on behavioral economics and how it integrates so well with analytical space.Lets start with a simple quiz to show the impact of what I will discuss.Letsstart with a few simple questions before I explainthe keypoints. There is only one rule in this exercise  You need to forget the previous questions completely (when asked) and come back as new person when you move to the next question. The order of the questions is very important and the exercise will be of no use if you do not follow the rules.LoadingI know these were some tough questions, but hopefully I still have your attention now.
Lately, Behavioral economics have caught a lot of my attention. While I read more and more about it, I see a strong opportunity of integrating this study with analytics. In a few articles, I will try to make my point clear. We will try to make predictions in case of above questions using behavioral economics. Then we will see how analytical answer is different from that derived from behavioral economics.Here is my prediction without knowing what you selected:Question 1  Medium Size
Here is the distribution of the response based on survey on readers like you:
Question 2   $10  Here is what the readers have chosen:
Question 3  Option 3 (Even though you might have chosen 1 but you might have found it hard to decide this time between 1 and 3)Question 4  This one is just to set up the grounds for Question 5.Question 5 Say person A selected99 and person B selected 11 as the random number. On an average person A will be willing to pay more than person B. Check out the scatter plot from the responses below:Question 6  Option A  Pay $100 now
Question 7  Option B  Pay $1010
Question 8  Red
Question 9  White (If you did the probability calculations right)
Question 10  Option A  Take $100
Question 11  Option B  30 slots
How many correct answers did you get? Please let me know through the comments belowIf my model was a random chance, the expected number of questions I can get right is less than 50% (as a few questions have more than 1 choice). If Igot more than 50% right, I dont need to convince you to read further.However, if by chance I got lesser, you should know that you are only one of the cases where these theories failed. Analytics and behavioral economics run on very similar foundation i.e. lots and lots of experiments.The two are only different on the following aspect.We define a target population before we test a hypothesis. However, most of the fundamentals in behavioral science apply on any type of population.Even with this difference, I believe these additional tools in your tool kit will take you a long way. I will cover how to use these new tools in following article.In this article, I will introduce you to the concepts applied for the predictions to the 10 questions.This is the easiest one. Human has a tendency to find the middle option. I never told you the price of the 3 options or the size of the 3 options. All the information you had was that the sizes are Large > Medium > Small.Here is an interesting study done by a professor of Duke University you might like  A cafe served three options as Small  30 units @ $3, Medium  50 units @ $4, Large  80 units @ $5.Here is what you get as the distribution of people buying each size.The cafe changed the sizesafter 3 months. Now they had Small  50 units @ $4, Medium  80 units @ $5, Large  100 units @ $6. You are right, they just changed the names from Medium size to Small, Large to Medium and introduced a new Large.What do you expect now?Here is what the distribution looks like:WOW, right!! The distribution did not change a lot but cafe started earning higher cost per coffee sold. Easy money!!Human mind is trained to make choices between similar objects. We get confused when we compare Apples to Oranges. Consider the followingsituation  You need to make choice among A, B and C. You are evaluating it on two possible features  Feature 1 and Feature 2. Higher the value on each feature, better is the choice.So what do you choose ?We know A and B have trade off. B is definitely better than C, but again we have a trade off between A and C.This is because B is better than C. Humans have a tendency to see it as the best choice rather than just a better choice. So we choose B. C is here a decoy effect.So what happens in Question 2 and 3 ?In Question 2 you make a trader off choice between the ease of reading and the cost. You might have chosen either of the two based on your personal preference.However, Question 3 is something especial. Here you have a decoy option which is just a hard copy. You see that Option 3 is like you pay for the hard copy and get the soft copy free. Even though no one will choose the Option 2 in this question, but it is a decoy!!It already did what it was supposed to do. It makes people think Option 3 is not that bad!!!Option 3 in question 3 and Option 2 in question 2 is the same, but the probability of someone choosing Option 3 in question 3 is higher, even though the overall probability of two option together is almost 1 in both the questions (as Option 2 in question 3 is a zero probability event).Do you recognize what a tool you now have to play with. If not, I will talk so much more about this lever in my following articles.Decoy effect is often used by marketing engineers all the time.Anchoring effect is my favorite. Why do you think we are ready to pay more than $600 for an I-Phone 7 and just $200 for an alternate phone with the same features?The answer lies in anchoring effect. The price of any item is based on perception rather than the actual cost of the raw materials used. As there is no logical way to get the right price I wish to pay for an item X, I will use an anchor to find the right price.Question 4 sets an anchor and Question 5 uses that anchor. Even though the anchor here was a random number you chose, I was able to make you believe that the actual cost of something abstract like a special course (which you have no other way to put a value tag) is close to this anchor.If you chose a very high random number, even if you decrease the value enough, you cant go that low. So if you had a high number assumed, you will still value this course high enough compared to the one who chose a small random number. The concept is difficult to digest, but it is being leveraged all around you. Think of a charity auction. We start with a price and start bidding higher. The starting price is an anchor and the final price at which the item is sold is highly correlated to this anchor.Another interesting concept in understanding human psychology is the temporal discounting. We all know that the value of money goes down with time because of inflation. If you are in a developing country, you probably see an inflation of 6-10% and in a developed country, you are looking at 0  3%. We are talking about something very different here. Both the questions, show a value gain of 1% in a week, so the inflation adjustment has no role here. So what makes us choose option 1 in 6 and option 2 in 7?We have a tendency to value money in near future with a strong discounting factor but such discounting factor becomes small when we talk about longer time frames.We see Week 0 and Week 1 as dramatically different. However, Week 52 and 53 look similar. This has been proven in many experiments done in behavioral science.Lets think of Question 8 first. What is the estimated value of winning with choosing Red color?Estimated Probability of getting aRed ball : 30/90Estimated value : (1/3* $30)  $10 = $0What is the estimated value of winning with choosing the Blue or White ball?Estimated number of Blue or White balls = 30 (as both are equally likely)Estimated Probability of getting a Blue ball = 30/90Estimated Value = (1/3 * $30)  $10 = $0Basically, estimated value of each bet $0. So, why do you choose the Red Ball? This again comes to human psychology. There are two separate concepts we talk about in behavioral economics  Risk and Ambiguity.Risk is when you know the probability of event.This is what we call Known Unknown.Ambiguity is when you are not sure about the probability or the chances of favorable events. This is what we call Unknown Unknown.Human has a tendency to underestimate probabilities when they face ambiguity. Choosing Red ball here is a risk and choosing White or Blue ball is ambiguity.If you are not yet convinced, lets look at Question 9.Why did you reverse your choice?This time Blue + White balls is a known number 60 and Red + Blue is unknown. So you switch the choice from Ambiguity to Risk.In both the question 10 & 11 the trade off is the same  you pick option with lower probability to win, you get higher expected value. So what makes you flip the decision between the two questions. Human brain is wired to perceive probability not at the face value but by multiplying it with a predictable function. Following graph will make it more clear.We are aligned with the actual probability at 0 and 1. But we tend to underestimate probability between 0 and 0.5 if event is favorable and exactly opposite happens when event is unfavorable.This is because we want to be risk averse most of the times. With underestimated perceived probability, we underestimate the value of Risky transactions. This is why a no  risk (Probability to win of 1) looks to have much higher estimated value.Question 11 is however a decision between probability that are close to each other. Here we take a decision based on the actual estimated value.We now become smart! when no option has extreme probabilities.In this article, I have introduced you to a new tool kit. Using this tool kit is still a skill set we need to acquire. I consider myself as a student in this field but I can definitely try to bring out precise application of these tools.The examples quoted in this article are variants of the successful behavioral experiments done by Duke University professors  Dan Ariely and Scott Huettel. I have no way done a justice with the depth of studies they have done in this field but this is just spread more awareness of their work. If you liked reading this article, I will encourage you to read their books  Predictably Irrational and Behavioral Economics  When Psychology and Economics Collide. I have definitely been enriched by these concepts and have developed a much better lens to see the world around us.Before I do that, I will want to know your opinion on this new stream of article  Did you learn something new? Did you find it interesting? Will you want to learn more? Do you see any application of these concepts in your world?",https://www.analyticsvidhya.com/blog/2017/04/behavioral-analytics-when-psychology-collides-analytics/
Lead Data Analyst- Bengaluru (6 to 9 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Behavioral Analytics : When Psychology collides with analytics|40 Questions to test a data scientist on Deep Learning [Solution: SkillPower  Deep Learning, DataFest 2017]|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  9 years
Requirements : Self-driven, exploratory mindset and comfortable with detail
Business acumen
Good communication skills
Open to travel/relocate to all worldwide locations
Task Info : About the companyWe are a leading global providerof Business & Operations support system that empowers communication service providers to achieve competitive advantage through business & Capex optimisation- thereby enabling them to improve their operational efficiency to deliver enhanced service experiences to subscribers.Job DescriptionOpening for Lead Data Analyst with 6-9 years of experience. Graduates with majors in statistics/mathematics and MBA preferred but not mandatory.RoleDive deep into data and emerge with actionable nuggets of information thereby helping CSPs to monetize their vast reserves of data. Extract, analyze, correlate, model, interpret and transform data into business insights. Employ descriptive, diagnostic, predictive and prescriptive techniques to derive value. Mentor and guide team members from technical and business perspective.Desired SkillsOne or more of the following:Working proficiencyof machine learning techniques like Bayesian, Decision Trees, Neural Networks, Ensemble etc.Knowledge of advanced statistical methods including multivariate statistical methods, discrete choice modeling, etc.Working proficiencyin at least one data mining tool (SAS, SPSS, R, RapidMiner, etc.)Strong proficiencyin SQL andworking proficiencyin at least one programming language/scripting (R, Python, BASH script, Pl/SQL)Strong logical and quantitative skills with affinity for mathematicsSelf-driven, exploratory mindset and comfortable with detailBusiness acumenGood communication skillsOpen to travel/relocate to all worldwide locationsQualification/ExperienceBachelors with majors in IT/CS or MBA with analytics bentGraduates with majors in statistics/mathematics will have an additional advantage
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, r, sql, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/lead-data-analyst-bengaluru-6-to-9-years-of-experience/
"40 Questions to test a data scientist on Deep Learning [Solution: SkillPower  Deep Learning, DataFest 2017]",Learn everything about Analytics|Introduction|Overall Scores|Useful Resources|Questions & Answers,"End Notes|Check out all the upcoming skilltestshere.|Share this:|Like this:|Related Articles|Lead Data Analyst- Bengaluru (6 to 9 years of experience)|Solutions Architect, Data Finance- Chennai (8 years of Experience)|
Faizan Shaikh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning has made many practical applications of machine learning possible. Deep Learning breaks down tasks in a way that makes all kinds of applications possible. This skilltest was conducted to test your knowledge of deep learning concepts.A total of 853 people registered for this skill test. The test was designed to test the conceptual knowledge of deep learning. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how you could have answered correctly.Here are the leaderboard ranking for all the participants.Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 270 people participated in the skill test and the highest score obtained was 38. Here are a few statistics about the distribution.Mean Score: 15.05Median Score: 18Mode Score: 0A Complete Guide on Getting Started with Deep Learning in PythonThe Evolution and Core Concepts of Deep Learning & Neural NetworksPractical Guide to implementing Neural Networks in Python (using Theano)Fundamentals of Deep Learning  Starting with Artificial Neural NetworkAn Introduction to Implementing Neural Networks using TensorFlowFine-tuning a Keras model using Theano trained Neural Network & Introduction to Transfer Learning6 Deep Learning Applications a beginner can build in minutes (using Python)1) The difference between deep learning and machine learning algorithms is that there is no need of feature engineering in machine learning algorithms, whereas, it is recommended to do feature engineering first and then apply deep learning.A) TRUEB) FALSESolution:(B)Deep learning itself does feature engineering whereas machine learning requires manual feature engineering.2) Which of the following is a representation learning algorithm?A) Neural networkB) Random ForestC) k-Nearest neighborD) None of the aboveSolution: (A)Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning.3) Which of the following option is correct for the below-mentioned techniques?A) 1 and 2B) 3 and 4C) 1 and 4D) 2 and 3Solution: (A)Option A is correct.4) Increase in size of a convolutional kernel would necessarily increase the performance of a convolutional neural network.A) TRUEB) FALSESolution: (B)Kernel size is a hyperparameter and therefore by changing it we can increase or decrease performance.Question ContextSuppose we have a deep neural network model which was trained on a vehicle detection problem. The dataset consisted of images on cars and trucks and the aim was to detect name of the vehicle (the number of classes of vehicles are 10).
Now you want to use this model on different dataset which has images of only Ford Mustangs (aka car) and the task is to locate the car in an image.5) Which of the following categories would be suitable for this type of problem?A) Fine tune only the last couple of layers and change the last layer (classification layer) to regression layerB) Freeze all the layers except the last, re-train the last layerC) Re-train the model for the new datasetD) None of theseSolution: (A)6) Suppose you have 5 convolutional kernel of size 7 x 7 with zero padding and stride 1 in the first layer of a convolutional neural network. You pass an input of dimension 224 x 224 x 3 through this layer. What are the dimensions of the data which the next layer will receive?A) 217 x 217 x 3B) 217 x 217 x 8C) 218 x 218 x 5D) 220 x 220 x 7Solution: (C)7)Suppose we have a neural network with ReLU activation function. Lets say, we replace ReLu activations by linear activations.Would this new neural network be able to approximate an XNOR function?Note: The neural network was able to approximate XNOR function with activation function ReLu.A) YesB) NoSolution: (B)If ReLU activation is replaced by linear activation, the neural network loses its power to approximate non-linear function.8) Suppose we have a 5-layer neural network which takes 3 hours to train on a GPU with 4GB VRAM. At test time, it takes 2 seconds for single data point.Now we change the architecture such that we add dropout after 2nd and 4th layer with rates 0.2 and 0.3 respectively.What would be the testing time for this new architecture?A) Less than 2 secsB) Exactly 2 secsC) Greater than 2 secsD) Cant SaySolution: (B)The changes is architecture when we add dropout only changes in the training, and not at test time.9) Which of the following options can be used to reduce overfitting in deep learning models?A) 1, 2, 3B) 1, 4, 5C) 1, 3, 4, 5D) All of theseSolution: (D)All of the above techniques can be used to reduce overfitting.10) Perplexity is a commonly used evaluation technique when applying deep learning for NLP tasks. Which of the following statement is correct?A) Higher the perplexity the betterB) Lower the perplexity the betterSolution: (B)11)Suppose an input to Max-Pooling layer is given above. The pooling size of neurons in the layer is (3, 3).What would be the output of this Pooling layer?A) 3B) 5C) 5.5D) 7Solution: (D)Max pooling works as follows, it first takes the input using the pooling size we defined, and gives out the highest activated input.12) Suppose there is a neural network with the below configuration.If we remove the ReLU layers, we can still use this neural network to model non-linear functions.A) TRUEB) FALSESolution: (B)13) Deep learning can be applied to which of the following NLP tasks?A) Machine translationB) Sentiment analysisC) Question Answering systemD) All of the aboveSolution: (D)Deep learning can be applied to all of the above-mentioned NLP tasks.14)Scenario 1: You are given data of the map of Arcadia city, with aerial photographs of the city and its outskirts. The task is to segment the areas into industrial land, farmland and natural landmarks like river, mountains, etc.Scenario 2: You are given data of the map of Arcadia city, with detailed roads and distances between landmarks. This is represented as a graph structure. The task is to find out the nearest distance between two landmarks.Deep learning can be applied to Scenario 1 but not Scenario 2.A) TRUEB) FALSESolution: (B)Scenario 1 is on Euclidean data and scenario 2 is on Graphical data. Deep learning can be applied to both types of data.15) Which of the following is a data augmentation technique used in image recognition tasks?A) 1, 2, 4B) 2, 3, 4, 5, 6C) 1, 3, 5, 6D) All of theseSolution: (D)16) Given an n-character word, we want to predict which character would be the n+1th character in the sequence. For example, our input is predictio (which is a 9 character word) and we have to predict what would be the 10th character.Which neural network architecture would be suitable to complete this task?A) Fully-Connected Neural NetworkB) Convolutional Neural NetworkC) Recurrent Neural NetworkD) Restricted Boltzmann MachineSolution: (C)Recurrent neural network works best for sequential data. Therefore, it would be best for the task.17) What is generally the sequence followed when building a neural network architecture for semantic segmentation for image?A) Convolutional network on input and deconvolutional network on outputB) Deconvolutional network on input and convolutional network on outputSolution: (A)18) Sigmoid was the most commonly used activation function in neural network, until an issue was identified. The issue is that when the gradients are too large in positive or negative direction, the resulting gradients coming out of the activation function get squashed. This is called saturation of the neuron.That is why ReLU function was proposed, which kept the gradients same as before in the positive direction.
A ReLU unit in neural network never gets saturated.A) TRUEB) FALSESolution: (B)ReLU can get saturated too. This can be on the negative side of x-axis.19) What is the relationship between dropout rate and regularization?Note: we have defined dropout rate as the probability of keeping a neuron active?A) Higher the dropout rate, higher is the regularizationB) Higher the dropout rate, lower is the regularizationSolution: (B)Higher dropout rate says that more neurons are active. So there would be less regularization.20) What is the technical difference between vanilla backpropagation algorithm and backpropagation through time (BPTT) algorithm?A) Unlike backprop, in BPTT we sum up gradients for corresponding weight for each time stepB) Unlike backprop, in BPTT we subtract gradients for corresponding weight for each time stepSolution: (A)BPTT is used in context of recurrent neural networks. It works by summing up gradients for each time step21) Exploding gradient problem is an issue in training deep networks where the gradient getS so large that the loss goes to an infinitely high value and then explodes.What is the probable approach when dealing with Exploding Gradient problem in RNNs?A) Use modified architectures like LSTM and GRUsB) Gradient clippingC) DropoutD) None of theseSolution: (B)To deal with exploding gradient problem, its best to threshold the gradient values at a specific point. This is called gradient clipping.22) There are many types of gradient descent algorithms. Two of the most notable ones are l-BFGS and SGD. l-BFGS is a second order gradient descent technique whereas SGD is a first order gradient descent technique.In which of the following scenarios would you prefer l-BFGS over SGD?A)Both 1 and 2B) Only 1C) Only 2D) None of theseSolution: (A)l-BFGS works best for both of the scenarios.23) Which of the following is not a direct prediction technique for NLP tasks?A) Recurrent Neural NetworkB) Skip-gram modelC) PCAD) Convolutional neural networkSolution: (C)24) Which of the following would be the best for a non-continuous objective during optimization in deep neural net?A) L-BFGSB)SGDC) AdaGradD) Subgradient methodSolution: (D)Other optimization algorithms might fail on non-continuous objectives, but sub-gradient method would not.25) Which of the following is correct?A) 1 is True and 2 is FalseB) 1 is False and 2 is TrueC)Both 1 and 2 are TrueD)Both 1 and 2 are FalseSolution: (D)In dropout, neurons are dropped; whereas in dropconnect; connections are dropped. So both input and output weights will be rendered in useless, i.e. both will be dropped for a neuron. Whereas in dropconnect, only one of them should be dropped26) While training a neural network for image recognition task, we plot the graph of training error and validation error for debugging.What is the best place in the graph for early stopping?A) AB)BC)CD) DSolution: (C)You would early stop where the model is most generalized. Therefore option C is correct.27) Research is going on to solve image inpainting problem using deep learning. For this, which loss function would be appropriate for computing the pixel-wise region to be inpainted?Image inpainting is one of those problems which requires human expertise for solving it. It is particularly useful to repair damaged photos or videos. Below is an example of input and output of an image inpainting example.A) Euclidean lossB) Negative-log Likelihood lossC) Any of the aboveSolution: (C)Both A and B can be used as a loss function for image inpainting problem.28) Backpropagation works by first calculating the gradient of ___ and then propagating it backwards.A) Sum of squared error with respect to inputsB) Sum of squared error with respect to weightsC) Sum of squared error with respect to outputsD) None of the aboveSolution: (C)29) Mini-Batch sizes when defining a neural network are preferred to be multiple of 2s such as 256 or 512. What is the reason behind it?A) Gradient descent optimizes best when you use an even numberB) Parallelization of neural network is best when the memory is used optimallyC) Losses are erratic when you dont use an even numberD) None of theseSolution: (B)30) Xavier initialization is most commonly used to initialize the weights of a neural network. Below is given the formula for initialization.Xaviers init helps reduce vanishing gradient problem.Xaviers init is used to help the input signals reach deep into the network. Which of the following statements are true?A) 1, 2, 4B) 2, 3, 4C)1, 3, 4D)1, 2, 3E)1, 2, 3, 4Solution: (D)All of the above statements are true.31) As the length of sentence increases, it becomes harder for a neural translation machine to perform as sentence meaning is represented by a fixed dimensional vector. To solve this, which of the following could we do?A) Use recursive units instead of recurrentB)Use attention mechanismC) Use character level translationD) None of theseSolution: (B)32) A recurrent neural network can be unfolded into a full-connected neural network with infinite length.A) TRUEB) FALSESolution: (A)Recurrent neuron can be thought of as a neuron sequence of infinite length of time steps.33) Which of the following is a bottleneck for deep learning algorithm?A) Data related to the problemB) CPU to GPU communicationC) GPU memoryD)All of the aboveSolution: (D)Along with having the knowledge of how to apply deep learning algorithms, you should also know the implementation details. Therefore you should know that all the above mentioned problems are a bottleneck for deep learning algorithm.34) Dropout is a regularization technique used especially in the context of deep learning. It works as following, in one iteration we first randomly choose neurons in the layers and masks them. Then this network is trained and optimized in the same iteration. In the next iteration, another set of randomly chosen neurons are selected and masked and the training continues.Dropout technique is not an advantageous technique for which of the following layers?A) Affine layerB)Convolutional layerC)RNN layerD)None of theseSolution: (C)Dropout does not work well with recurrent layer. You would have to modify dropout technique a bit to get good results.35) Suppose your task is to predict the next few notes of song when you are given the preceding segment of the song. For example:The input given to you is an image depicting the music symbols as given below,Your required output is an image of succeeding symbols.Which architecture of neural network would be better suited to solve the problem?A) End-to-End fully connected neural networkB) Convolutional neural network followed by recurrent unitsC) Neural Turing MachineD)None of theseSolution: (B)CNN work best on image recognition problems, whereas RNN works best on sequence prediction. Here you would have to use best of both worlds!36) When deriving a memory cell in memory networks, we choose to read values as vector values instead of scalars. Which type of addressing would this entail?A) Content-based addressingB) Location-based addressingSolution: (A)37) It is generally recommended to replace pooling layers in generator part of convolutional generative adversarial nets with ________ ?A) Affine layerB) Strided convolutional layerC) Fractional strided convolutional layerD) ReLU layerSolution: (C)Option C is correct. Go through this link.Question Context 38-40 GRU is a special type of Recurrent Neural Networks proposed to overcome the difficulties of classical RNNs. This is the paper in which they were proposed: On the Properties of Neural Machine Translation: EncoderDecoder Approaches. Read the full paper here.38) Which of the following statements is true with respect to GRU?A)Only 1B) Only 2C) None of themD) Both 1 and 2Solution: (D)39) If calculation of reset gate in GRU unit is close to 0, which of the following would occur?A) Previous hidden state would be ignoredB) Previous hidden state would be not be ignoredSolution: (A)40) If calculation of update gate in GRU unit is close to 1, which of the following would occur?A) Forgets the information for future time stepsB) Copies the information through many time stepsSolution: (B)If you missed out on this competition, make sure you complete in the ones coming up shortly. We are giving cash prizes worth $10,000+ during the month of April 2017.If you have any questions or doubts feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-deep-learning/
"Solutions Architect, Data Finance- Chennai (8 years of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|40 Questions to test a data scientist on Deep Learning [Solution: SkillPower  Deep Learning, DataFest 2017]|Technologist- Pune (2-4 Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  years
Requirements : 
Task Info : Job Description and ResponsibilitiesBachelors degree in Computer Engineering or equivalent is desired8+ years of post-college working experience as a developer and architectin Engineering, or Data-Mining organization8+ years of experience with full life-cycle development in DataWarehousing and Data Integration projects using Teradata. Teradata SQL masters certification isrequired.6+ years Linux/Unix/Perl experience, including scripting and versioncontrol with working knowledge of Teradata utilities : FastExport, BTEQ,FastLoad and MLoad5+ years of Python development experience is required4+ years of Hadoop and / or SPARK experience is requiredExpertise in Shell scripting, system/process automation is requiredExpertisein data research/analysis with a focus on data quality and consistency isrequired
College Preference : no-bar
Min Qualification : ug
Skills : data mining, Data Warehouse, hadoop, linux, perl, python, spark, sql, teradata, Unix shell scripting
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/solutions-architect-data-finance-chennai-8-years-of-experience/
Technologist- Pune (2-4 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Solutions Architect, Data Finance- Chennai (8 years of Experience)|Data Analyst- Bangalore (4-6 Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Job Description and Responsibilities Experience in developing web-based applications using Python, Flask/Django, RESTful API. 2-4 years of experience working with Django framework, libraries and third party integrations Proven expertise in web programming and have worked extensively with any one or more of JavaScript libraries like AngularJs, jQuery, Dojo, etc. Extensive experience in HTML/DHTML/CSS Web 2.0 development or PHP. Proficiency in Relational/ Non-relational databases like Mysql, Postgres or MongoDB. Amazon Web services. Third party integration: Analytics, payment gateway Good knowledge of Linux and Shell/Python scripting ElasticSearch and Linux System Administration experience is a definite bonus Should be proficient in understanding architecture of an existing enterprise class application and give recommendations to enhance and maintain the application from a design/architecture/integration perspective. Good analytical and troubleshooting skills in web and application server environments.
College Preference : no-bar
Min Qualification : ug
Skills : Angular JS, api, aws, CSS3, HTML5, java script, jquery, linux, MongoDB, python, Unix shell scripting, web application
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/technologist-pune-2-4-years-of-experience/
Data Analyst- Bangalore (4-6 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Technologist- Pune (2-4 Years of experience)|Data Science Researcher & Faculty- Full Time|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Job Description and Responsibilities Provide reporting and performance monitoring to game studios using data drawn from diverse sources Perform detailed data exploration and validation to separate genuine phenomena from spurious anomalies Develop new insights and analyses that inform decisions and help us continue to delight the people playing our gamesRequired Skills 4-6 years of experience performing quantitative analysis, preferably for an internet, technology or consumer-centric company Excellent SQL skills: extensive experience working with large, complex data sets High proficiency with Excel including pivoting capabilities and analysis modules Working knowledge of basic statistics; experience with R, Python, SAS or similar a plus Problem solving skills: Ability to execute research projects, and generate practical results and recommendations without much guidance Storytelling abilities to provide recommendations in a structured/logical way Experience with Tableau or other similar tools, to create compelling visualizations to help managers take data-informed decision, is a plus Good client management skills with a strong grasp of both technical and business perspectives Proven ability to work in a fast-paced environment, and to meet changing deadlines and priorities on multiple simultaneous projects Excellent organizational, communication and interpersonal skills Enjoy working in both individual and team settings Experience with perl scripting, php or unix shell commands a plus B.S. or B.A. in math, economics, engineering or other technical field required; advanced degrees a plus
College Preference : no-bar
Min Qualification : ug
Skills : data exploration, data validation, perl, python, r, sas, sql, Unix shell scripting
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-analyst-bangalore-4-6-years-of-experience/
Data Science Researcher & Faculty- Full Time,Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- Bangalore (4-6 Years of experience)|Senior Science Analyst- Bangalore (7+ years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : Excellent communication skills
Task Info : 
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business analytics, Data analytics, data science, logistic regression, machine learning, python, r, random forest, rstudio, statistical modeling
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-science-researcher-faculty-full-time/
Senior Science Analyst- Bangalore (7+ years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Science Researcher & Faculty- Full Time|Data Scientist- kolkata (2-4 years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  years
Requirements : 
Task Info : Job Description and Responsibilities Help improve existing data analyses by always researching cutting edgestatistical modeling and data visualization techniques. Help deliver customized analyses leveraging internal and external,structured and unstructured datasets. Utilize advanced modeling techniques toparse out adverse experience drivers. Manipulate large data sets, using cuttingedge machine learning or statistical modeling techniques and synthesizinginsights. Help develop an analytics platform to house and analyze big data  Help with the process of curating, cleaning and integrating data toenable scalability of analyses. Leverage several internal databases as well asexternal data sources. Collaborate with other analytical teams to leverage tools andtechniques developed across AIG.  Always strive to automate analyses where possible.  Help with advancing Data Visualization capabilities Help train new staff in a growing team. Help manage additional COEresources as needed.Position Requirements:
The ideal candidate needs to be familiar with the following techniquesand tools, with an expert-level experience in some:  Expertise in one or more modeling/machine learning platforms as such asR, SAS, and Python  Expertise in Natural Language Processing techniques Experience with additional programming languages such as C++, Java,Matlab, Octave a plus  Classification methods (e.g., Neural Net, Logistic Regression, DecisionTrees, KNN, SVM, Random Forest)  Regression methods (e.g., Linear, Nonlinear, Boosted Regression Trees )Clustering methods (e.g., K-means, Fuzzy C-means, Hierarchical Clustering,Mixture Modelling)  Time-series Modelling/Forecasting (e.g., AR, ARMA, GARCH, ExponentialSmoothing)  Statistical Analysis (e.g., Hypothesis Testing, Experiment Design,Hierarchical Modeling, Bayesian Inference)  Familiarity with common computing environment (e.g. Linux, ShellScripting) Knowledge about Big Data related techniques (e.g., Map-Reduce,Hadoop, Hive, NoSQL)  Advanced skills in SQLAdditional Position Requirements:
 Masters Degree in a technical field with 7+ years of experience. Advanced data visualization skills Strong and effective communication (both written and verbal) withcolleagues and business leaders  Proven project management skills  ability to manage and coordinatelarge and complex projects across the organization.Actuarial background a plus
College Preference : no-bar
Min Qualification : pg
Skills : bigdata, c++, classification, data visualization, decision trees, forecasting, hadoop, hive, java, linear regression, logistic regression, machine learning, mapreduce, matlab, nlp, nosql, python, qlikview, r, random forest, regression, sas, sql, statistical techniques, tableau, time series
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/senior-science-analyst-bangalore-7-years-of-experience/
Data Scientist- kolkata (2-4 years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Science Analyst- Bangalore (7+ years of Experience)|Data analyst- bangalore (3+ years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Job Description and ResponsibilitiesWork closely with clients requirement and business processWill be directly involved in the data designing, developing, validating and implementing ofWork with the large volume of different kind of data and understands the relevance of each as a possible input variable.Understands the structure of the data.Collaboration with software engineer and web developer.Work with the functional team and deliver the innovative idea.Bachelor degree or Master degree in Mathematics or Statistics.Working experience with business analystCandidates must have to be from Software backgroundMust be working experience with relational and non-relational databases SQL, NOSQLStrong analytical skills and comfort with common statistical techniques.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, apache spark, business analysis, business analytics, nosql, python, r, sql, statistical techniques
Location : Kolkata
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-scientist-kolkata-2-4-years-of-experience/
Data analyst- bangalore (3+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- kolkata (2-4 years Of Experience)|Intern- Data Analytics- Gurgaon (2-6 Months)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Job Description and ResponsibilitiesData Extraction: Take ownership in extracting data from various data sources. This would require an understanding of project objectives, data requirements and working with databases to meet the data needs for the project.Data Harmonization: extraction of data from huge/complex databases Using Advance Excel and SQL is required.
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, database, data extraction, excel, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-analyst-bangalore-3-years-of-experience/
Intern- Data Analytics- Gurgaon (2-6 Months),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data analyst- bangalore (3+ years of experience)|Analytics Vidhya turns 4  A journey from a part-time blog to Top Data Science Knowledge Portal|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  0 years
Requirements : 
Task Info : Salary: 10 K to 15 K per month
College Preference : no-bar
Min Qualification : ug
Skills : python, r, sql, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/intern-data-analytics-gurgaon-2-6-months/
Analytics Vidhya turns 4  A journey from a part-time blog to Top Data Science Knowledge Portal,Learn everything about Analytics|How and why did Analytics Vidhyastart?||Its the will that will take you forward|Getting the initial team together|Growth & Expansion|The First Feature|Launch of DataFest & the story further|April 2017,"The first viral article|On Turning One|Share this:|Like this:|Related Articles|Intern- Data Analytics- Gurgaon (2-6 Months)|Sr. Manager- Business Intelligence- Gurgaon (5-8 years of experience)|
Analytics Vidhya Content Team
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Analytics Vidhya turns 4 todayAnalyticsvidhya.com was registered on this day 4 years ago. In these 4 years, what started as a part time blog has transformed into one of the top data science portal across the globe. What started with an aim to make sure that Kunal (our founder) learns regularly is now helping millions of people learn data science daily.We are celebrating our 4th anniversary this month. We are reminiscing the last 4 years which have brought us here. All the hard work, patience, struggle, joy, excitement and recognition is running through our memory, is reflecting in our sentiments and our discussions.The JourneyToday, we serve millions of data enthusiasts to learn analytics and we have a global community of more than 50,000 registered users. But, these numbers tell you a very small part of the story. We live by a dream of unleashing a new era based on data science. What you might not have seen or heard is the inside story of how things happened during this journey.The story of building Analytics Vidhya is driven by sheer passion & perseverance. We rarely share the inside story of what goes behind building our product / event at Analytics Vidhya whether its a hackathon or an article. If you have been touched by our impact, you will definitely relate to the journey.Lets travel back in time and see the journey of AV!It was during his stay at Aviva that Kunalrealized the need for data science community & lack of hands-on analytics training among professionals.Data Science (popularly referred to as Analytics then) was still an emerging field and people were looking for informative resources in data science. Most of the professionals relied on their natural networks toget thisknowledge or information.To address this gap, Kunal started a part-time blog. The idea was to share his knowledge & perspective with a larger audience. Kunal Jain, the Founderof the company showed Analytics Vidhya the first light of the day in April 2013.Little did Kunal know that we would reach this far.Check out some of the blogs written initially by Kunal.The initial blog page looked something like this.Managing a part time blog with a regular job is definitely not easy. And so was for Kunal. He would work during the day and write blogs at night. There were days when he would often go without sleep. It is his will & passion for data science which hasnt budged till date.Until July 2013, the number of visits on the blog stayedbetween 50-100 visits a day. Kunal was relying on his Linkedin connections to share information about the blog.Then in August 2013, Kunal published this articleMust read books for Analystsand for the first time AV received more than 500 visits a day. As Kunal says That was the moment I knew we were there to stay. He said he even circulated a screenshot of the Google Analytics app to friends and family that day !!A few days later, Kunal had something big happening at the personal front  he was blessed with a baby girl. To make sure he posts an article every week, he announced the birth of his daughter likethissitting beside the baby incubator. A true analyst!After about 6 months of existence of the blog, Kunal got his first team member. Kunal and Tavish had worked together at Aviva and both of them shared the same passion for anything data. Tavish started to contribute to Analytics Vidhya while doing his day job.Sunil also started writing articles on Analytics Vidhya from Dec 2013, while keepinghis job. Now, they were a team of 3 people with only one motive, to make Analytics Vidhya leading data science & analytics knowledge portal in the world.This was also the time Kunal decided to take the plunge and start doing Analytics Vidhya full time.In summer of 2014, Analytics Vidhya hired first two interns  Abhinav Unnan & Adithya Chowdary fromIIT Roorkee & IIT Kharagpur respectively. Internships will go on to play a key role in future.The first office setup was a single room with a few laptops and chairs. It was in the same apartment in which Kunal lived with his family. The company operated from there for 2 years.One thing which defines any company is the leadership of the founder. Kunal has believed in sharing the knowledge & creating more leaders. He has always gone to great lengths to help people & train analytical minds. And this is the same thing he has maintained in Analytics Vidhya. To help people progress in their career we go to any extent.To produce more leaders in data science, in the same year we launched our very first AV apprentice program.To provide a dedicated platform for data science discussion, the company rolled out the Discuss portal. After several testing, the Discuss portal went live in Jan 2015.The first version of Discuss portal looked like this:In a span of 2 years, the company had grown from a small blog to one of the top data science blogs in the world. Two more people were hired and now they were a total team of 5.In April 2015, the company completed 2 years. We celebrated two years of the company and celebrated with the very first competitionAV Turns 2, which was held on Facebook.By then we were getting a lot of request for career guidance on data science. To let people have a chance to interact with Kunal directly and to help people start their data science career we launched AV meetups in May 2015. The first meetup was held in Delhi followed by Bangalore & Mumbai. Chennai meetup started later in the same year.First Delhi MeetupMumbaiMeetupFirst BangaloreMeetupIn July 2015, AV conducted its first hackathon  Predict the Megastar. This was a remarkable moment in the history of AV. The initial plan was to host the hackathon on the Discuss platform and ask participants to mail their solutions. But the night before the hackathon Kunal decided to build a hackathon platform. The first hackathon platform was developed overnight by Kunal with help from a few friends.And it looked something like this:Heres what Kunal wrote about it,I am sure the designers and UI / UX specialists out there would discern at this screen, but there was something magical about it. It just did what it was supposed to  tell people the accuracy of their solutions through a simple interface. The simplicity and responsiveness of platformclicked with the users and we had close to thousand submissions over a 2 day period!This was the Eureka moment when Kunal knew he was about to build something much bigger than what he had thought.Analytics Vidhya was making a mark in the industry. The company started getting featured at various analytics conclaves in colleges, companies, TechStoryand several other industry newsletters.Kunal was soon getting recognized as the leader in analytics space and Analytics Vidhya was gaining recognition for the unique knowledge & opportunities it was providing people.In 2016, Analytics Vidhya published a salary report in association with Jigsaw Academy. The report was featured by Economic Times, The Hindu, and few others.In 2016, on the third anniversary of AV we launched DataFest, our first biggest online event. It was the biggest event in the history of AV andsaw huge participation from data scientists, analyst all over the world.Our first DataFest web page looked like this.After the success of first DataFest, we continued to add more resources for our community to learn & grow in leaps n bounds.User profileswere added to the hackathon platform. We launched several strategic thinking competitions, skill tests, minihacks, AMAs & workshops to help people upskill themselves.By now, we were conducting our signature hackathon almost every 2 months.In the September 2016, we launched our very first student hackathon. The large participation from students left us stunned with their knowledge. We were certain that our mission to spread awareness about data science was getting accomplished.In December 2016, we launched a revampedjob portal. The new portal has individual user profiles and lets you track jobs in a more organized fashion.We have come far from where we started. The journey has not been easy, but it has been immensely fulfilling. The road ahead is even more exciting. We see data science getting tremendous attention and we see making a much larger impact in the coming years.We are a team of dreamers  we now have our awesome volunteers who are building communities on their own.We are celebrating this occasion with DataFest 2017.I am going to end this story by a lovelypoem from Robert Frost:The woods are lovely, dark and deep,But I have promises to keep.And miles to go before I sleep,And miles to go before I sleep",https://www.analyticsvidhya.com/blog/2017/04/analytics-vidhyas-journey-from-a-part-time-blog-to-top-data-science-knowledge-portal/
Sr. Manager- Business Intelligence- Gurgaon (5-8 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Vidhya turns 4  A journey from a part-time blog to Top Data Science Knowledge Portal|40 Questions to test a data scientist on Time Series [Solution: SkillPower  Time Series, DataFest 2017]|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : 	Exposure of working in a digital business model will be an added advantage
	Conversant with project management practices
	Ability to communicate with stakeholders
Task Info : Title:  Sr. Manager  Business IntelligenceExperience:  6-8 Yrs.Department: Credit & Analytics Reports to: Head of Analytics  VP  Job Type: Permanent   
About the company:We are India`s largest financial marketplace for lending and investment products. Our aim is to make personal finance decisions easy, transparent and convenient for India. Here, you can compare loans, credit cards and investment products, from India`s top banks and NBFCs, from anywhere and at any time.We offer a wide suite of products that includePersonal Loan,Credit Card,Home Loan,Loan Against Property,Home Loan Balance Transfer,Car Loan,Education Loan,Savings Accounts,Mutual Funds,Fixed Deposits,Mobile WalletandGold Loan.About DepartmentThe Analytics function supports all business lines from Credit Intelligence perspective. The team engages with organization wide projects and acts as a data brain for the Organization. The key objectives of the function are: Building various Predictive models required in the organization. This includes Application Score Card, Product, Pricing & Portfolio Analytics, Loss Forecasting, Cross Sell Model, Developing Recommendation Algorithm Creating Credit vision and Framework to ensure we prioritize leads coming to our portal Identifying the opportunity area/segments where we can buy the risk and fulfil the lead Business Intelligence: Develop and publish various new age dashboards for the Management & Senior leadership to track key business metrics also real time tracking of each action performed by Customer & Agent on our website and mobile app
The function works directly with Senior Management to identify areas of data analysis requirement, define problem statements and develop key insights.
Purpose of your role and key accountabilitiesSelected candidate will lead Business Intelligence efforts for the Organization. They will have the responsibility of working in a hands-on delivery role with primary focus on reporting and dashboard development in Tableau & Qlik. The person would be responsible to create and scale the infrastructure to make sure quick turnaround time. Experience of delivering projects in any of the BI tool (at least 5+ years); Deep hands-on technical knowledge of the tool is a must Define the right architecture for the Organization to ensure scalable reporting Visualization skills on any UI tool  ability to design front end information delivery platforms Installation and configuration experience is good to have Ability to handle large volume databases, experience of data architect, integration with Big data, Hadoop environment Exposure of working in a digital business model will be an added advantage Conversant with project management practices Ability to communicate with stakeholdersIn addition to these core delivery tasks, selected person will also be responsible for the following additional practice support activities: Effort and commercial estimation for Business Intelligence projects Staffing and review of staff performance RecruitmentAbove responsibilities are just to outline primary focus, however it will not be limited to that. We are an emerging organisation, there is a high possibility that scope of the role will enhance based on the needs and organisation priorities.Your skills and experienceBI DW concepts with special emphasis to TableauOther BI/DW experience around data integration and visualizationBI Project management and estimationAcademic Qualifications B.E./ Masters in Economics/ Business/ Marketing Science/ Econometrics ToolsTableau (Mandatory), Knowledge of R will be an added advantageMandatory Experience 6-8 year of experience in BI Role (Preferably in BFSI/financial service industry)  Experience in analysing large volume data and interpretation of output
College Preference : no-bar
Min Qualification : ug
Skills : bfsi, bigdata, business intelligence, credit risk, dashboard, data visualization, Data Warehouse, etl, financial modeling, forecasting, hadoop, Portfolio analysis, predictive modeling, qlikview, r, recommendation engine, reporting, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/sr-manager-business-intelligence-gurgaon-5-8-years-of-experience/
"40 Questions to test a data scientist on Time Series [Solution: SkillPower  Time Series, DataFest 2017]",Learn everything about Analytics|Introduction|Overall Scores|Useful Resources|End Notes,"Check out all the upcoming skilltestshere.|Share this:|Like this:|Related Articles|Sr. Manager- Business Intelligence- Gurgaon (5-8 years of experience)|40 Questions on Probability for data science|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Time Series forecasting & modeling plays an important role in data analysis. Time series analysis is a specialized branch of statistics used extensively in fields such as Econometrics & Operation Research. This skilltest was conducted to test your knowledge of time series concepts.A total of 1094 people registered for this skill test. The test was designed to test you on the basic & advanced level of time series. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how many could have answered correctly.Here are the leaderboard ranking for all the participants.Below are the distribution scores, they will help you evaluate your performance.You can access the scores here. More than 300 people participated in the skill test and the highest score obtained was 38. Here are a few statistics about the distribution.Mean Score: 17.13Median Score: 19Mode Score: 19A Complete Tutorial on Time Series Modeling in RA comprehensive beginners guide to create a Time Series Forecast (with Codes in Python)1) Which of the following is an example of time series problem?1. Estimating number of hotel rooms booking in next 6 months.
 2. Estimating the total sales in next 3 years of an insurance company.
 3. Estimating the number of calls for the next one week.A) Only 3
B) 1 and 2
C) 2 and 3
D) 1 and 3
E)1,2 and 3Solution: (E)All the above optionshave a time component associated.2) Which of the following is not an example of a time series model?A) Naive approach
B) Exponential smoothing
C) Moving Average
D)None of the aboveSolution:(D)Nave approach: Estimating technique in which the last periods actuals are used as this periods forecast, without adjusting them or attempting to establish causal factors. It is used only for comparison with theforecastsgenerated by the better (sophisticated) techniques.In exponential smoothing, older data is given progressively-less relative importance whereas newer data is given progressively-greater importance.Intime seriesanalysis, themoving-average(MA) model is a common approach for modeling univariatetime series. Themoving-averagemodel specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.3) Which of the following cant be a component for a time series plot?A) Seasonality
B) Trend
C)Cyclical
D) Noise
E) None of the aboveSolution: (E)Aseasonalpattern exists when aseriesis influenced byseasonalfactors (e.g., the quarter of the year, the month, or day of the week).Seasonalityis always of a fixed and known period. Hence,seasonal time seriesare sometimes called periodictime seriesSeasonality is always of a fixed and known period. A cyclic pattern exists when data exhibit rises and falls that are not of fixed period.Trend is defined as the long term movement in a time series without calendar related and irregular effects, and is a reflection of the underlying level. It is the result of influences such as population growth, price inflation and general economic changes. The following graph depicts a series in which there is an obvious upward trend over time.Quarterly Gross Domestic ProductNoise: In discretetime, whitenoiseis a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance.Thus all of the above mentioned are components of a time series.4) Which of the following is relatively easier to estimate in time series modeling?A) Seasonality
B) Cyclical
C) No difference between Seasonality and CyclicalSolution:(A)As we seen in previous solution, as seasonality exhibits fixed structure; it is easier to estimate.5) The below time series plot contains both Cyclical and Seasonality component.
A) TRUE
B) FALSESolution:(B)There is a repeated trend in the plot above at regular intervals of time and is thus only seasonal in nature.6) Adjacent observations in time series data (excluding white noise) are independent and identically distributed (IID).A) TRUEB) FALSESolution: (B)Clusters of observations are frequently correlated with increasing strength as the time intervals between them become shorter. This needs to be true because in time series forecasting is done based on previous observations and not the currently observed data unlike classification or regression.7) Smoothing parameter close to one gives more weight or influence to recent observations over the forecast.A) TRUE
B) FALSESolution: (A)It may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past  the smallest weights are associated with the oldest observations:y^T+1|T=yT+(1)yT1+(1)2yT2+,(7.1)where0101is the smoothing parameter. The one-step-ahead forecast for timeT+1T+1is a weighted average of all the observations in the seriesy1,,yT. The rate at which the weights decrease is controlled by the parameter.8) Sum of weights in exponential smoothing is _____.A) <1
B) 1
C) >1
D) None of the aboveSolution:(B)Table7.1 shows the weights attached to observations for four different values ofwhen forecasting using simple exponential smoothing. Note that the sum of the weights even for a smallwill be approximately one for any reasonable sample size.9) The last periods forecast was 70 and demand was 60. What is the simple exponential smoothing forecast with alpha of 0.4 for the next period.A) 63.8
B)65
C)62
D)66Solution: (D)Yt-1= 70St-1= 60Alpha = 0.4Substituting the values we get0.4*60 + 0.6*70= 24 + 42= 6610) What does autocovariance measure?A) Linear dependence between multiple points on the different series observed at different times
B)Quadratic dependence between two points on the same series observed at different times
C) Linear dependence between two points on different series observed at same time
D) Linear dependence between two points on the same series observed at different timesSolution: (D)Option D is the definition of autocovariance.11) Which of the following is not a necessary condition for weakly stationary time series?A) Mean is constant and does not depend on time
B) Autocovariance function depends on s and t only through their difference |s-t| (where t and s are moments in time)
C) The time series under considerations is a finite variance process
D) Time series is GaussianSolution:(D)A Gaussian time series implies stationarity is strict stationarity.12) Which of the following is not a technique used in smoothing time series?A) Nearest Neighbour Regression
B) Locally weighted scatter plot smoothing
C) Tree based models like (CART)
D) Smoothing SplinesSolution: (C)Time series smoothing and ltering can be expressed in terms of local regression models. Polynomials and regression splines also provide important techniques for smoothing. CART based models do not provide an equation to superimpose on time series and thus cannot be used for smoothing. All the other techniques are well documented smoothing techniques.13) If the demand is 100 during October 2016, 200 in November 2016, 300 in December 2016, 400 in January 2017. What is the 3-month simple moving average for February 2017?A)300
B) 350
C) 400
D)Need more informationSolution: (A)X`= (xt-3 + xt-2 + xt-1 ) /3(200+300+400)/ 3 = 900/3 =30014) Looking at the below ACF plot, would you suggest to apply AR or MA in ARIMA modeling technique?
A) AR
B) MA
C) Cant SaySolution: (A)MA model is considered in the following situation, If theautocorrelation function(ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation isnegativei.e., if the series appears slightly overdifferencedthen consider adding anMAterm to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms.But as there are no observable sharp cutoffs the AR model must be preffered.15) Suppose, you are a data scientist at Analytics Vidhya. And you observed the views on the articles increases during the month of Jan-Mar. Whereas the views during Nov-Dec decreases.Does the above statement represent seasonality?A) TRUE
B) FALSE
C) Cant SaySolution: (A)Yes this is a definite seasonal trend as there is a change in the views at particular times.Remember, Seasonality is a presence of variations at specific periodic intervals.16) Which of the following graph can be used to detect seasonality in time series data?1. Multiple box
 2. AutocorrelationA) Only 1
B) Only2
C) 1 and 2
D) None of theseSolution: (C)Seasonality is a presence of variations at specific periodic intervals.The variation of distribution can be observed in multiple box plots. And thus seasonality can be easily spotted. Autocorrelation plotshould show spikes at lags equal to the period.17) Stationarity is a desirable property for a time series process.A) TRUE
B)FALSESolution: (A)When the following conditions are satisfied then a time series is stationary.These conditions are essential prerequisites for mathematically representing a time series to be used for analysis and forecasting. Thus stationarity is a desirable property.18) Suppose you are given a time series dataset which has only 4 columns (id, Time, X, Target).
What would be the rolling mean of feature X if you are given the window size 2?Note: X column represents rolling mean.A)B)C)D) None of the aboveSolution:(B)X`= xt-2 + xt-1 /2Based on the above formula: (100 +200) /2 =150; (200+300)/2 = 250 and so on.19) Imagine, you are working on a time series dataset. Your manager has asked you to build a highly accurate model. You started to build two types of models which are given below.Model 1: Decision Tree modelModel 2: Time series regression model At the end of evaluation of these two models, you found that model 2 is better than model 1. What could be the possible reason for your inference?A)Model 1 couldnt map the linear relationship as good as Model 2
B) Model 1 will always be better than Model 2
C) You cant compare decision tree with time series regression
D) None of theseSolution:(A)A time series model is similar to a regression model. So it is good at finding simple linear relationships. While a tree based model though efficient will not be as good at finding and exploiting linear relationships.20) What type of analysis could be most effective for predicting temperature on the following type of data.
A) Time Series Analysis
B) Classification
C) Clustering
D) None of the aboveSolution: (A)The data is obtained on consecutive days and thus the most effective type of analysis will be time series analysis.21) What is the first difference of temperature / precipitation variable?
A) 15,12.2,-43.2,-23.2,14.3,-7
B) 38.17,-46.11,-4.98,14.29,-22.61
C) 35,38.17,-46.11,-4.98,14.29,-22.61
D)36.21,-43.23,-5.43,17.44,-22.61Solution: (B)73.17-35 = 38.1727.05-73.17 =  46.11 and so on..13.75  36.36 = -22.6122) Consider the following set of data:{23.32 32.33 32.88 28.98 33.16 26.33 29.88 32.69 18.98 21.23 26.66 29.89}What is the lag-one sample autocorrelation of the time series?A) 0.26
B) 0.52
C) 0.13
D) 0.07Solution: (C)1 = PT t=2(xt1x)(xtx) PT t=1(xtx) 2 = (23.32x)(32.33x)+(32.33x)(32.88x)+ PT t=1(xtx) 2 = 0.130394786 Where x is the mean of the series which is 28.027523) Any stationary time series can be approximately the random superposition of sines and cosines oscillating at various frequencies.A) TRUE
B) FALSESolution: (A)A weakly stationary time series, xt, is a nite variance process such thatrandom superposition of sines and cosines oscillating at various frequencies is white noise. white noise is weakly stationary or stationary. If the white noise variates are also normally distributed or Gaussian, the series is also strictly stationary.24) Autocovariance function for weakly stationary time series does not depend on _______ ?A) Separation of xs and xt
B) h = | s  t |
C)Location of point at a particular timeSolution: (C)By definition of weak stationary time series described in previous question.25) Two time series are jointly stationary if _____ ?A) They are each stationary
B) Cross variance function is a function only of lag hA) Only A
B)Both A and BSolution: (D)Joint stationarity is defined based on the above two mentioned conditions.26) In autoregressive models _______ ?A) Current value of dependent variable is influenced by current values of independent variables
B) Current value of dependent variable is influenced by current and past values of independent variables
C) Current value of dependent variable is influenced by past values of both dependent and independent variables
D) None of the aboveSolution: (C)Autoregressive models are based on the idea that the current value of the series, xt, can be explained as a function of p past values, xt1,xt2,,xtp, where p determines the number of steps into the past needed to forecast the current value. Ex. xt = xt1 .90xt2 + wt,Where xt-1 and xt-2 are past values of dependent variable and wt the white noise can represent values of independent values.The example can be extended to include multiple series analogous to multivariate linear regression.27)For MA (Moving Average) models the pair  = 1 and  = 5 yields the same autocovariance function as the pair  = 25 and  = 1/5. A)TRUE
B)FALSESolution: (A)True, because autocovariance is invertible for MA modelsnote that for an MA(1) model, (h) is the same for  and 1 /try 5 and 1 5, for example. In addition, the pair 2 w = 1 and  = 5 yield the same autocovariance function as the pair 2 w = 25 and  = 1/5.28) How many AR and MA terms should be included for the time series by looking at the above ACF and PACF plots?A) AR (1) MA(0)
B) AR(0)MA(1)
C) AR(2)MA(1)
D)AR(1)MA(2)
E) Cant SaySolution: (B)Strong negative correlation at lag 1 suggest MA and there is only 1 significant lag. Read this article for a better understanding.29) Which of the following is true for white noise?A) Mean =0
B) Zero autocovariances
C) Zero autocovariances except at lag zero
D) Quadratic VarianceSolution: (C)A white noise process must have a constant mean, a constant variance and no autocovariance structure (except at lag zero, which is the variance).30) For the following MA (3) processyt=+t+1t-1+2t-2+3t-3, where tis a zero mean white noise process with variance2A) ACF = 0 at lag 3
B)ACF =0 at lag 5
C) ACF =1 at lag 1
D) ACF =0 at lag 2
E) ACF = 0 at lag 3 and at lag 5Solution: (B)Recall that an MA(q) process only has memory of length q. This means that all of the autocorrelation coefficients will have a value of zero beyond lag q. This can be seen by examining the MA equation, and seeing that only the past q disturbance terms enter into the equation, so that if we iterate this equation forward through time by more than q periods, the current value of the disturbance term will no longer affect y. Finally, since the autocorrelation function at lag zero is the correlation of y at time t with y at time t (i.e. the correlation of y_t with itself), it must be one by definition.31) Consider the following AR(1) model with the disturbances having zero mean and unit variance.yt= 0.4 + 0.2yt-1+utThe (unconditional) variance of y will be given by ?A) 1.5
B) 1.04
C)0.5
D) 2Solution: (B)Variance of the disturbances divided by (1 minus the square of the autoregressive coefficientWhich in this case is : 1/(1-(0.2^2))= 1/0.96= 1.04132) The pacf (partial autocorrelation function) is necessary for distinguishing between ______ ?A) An AR and MA model is_solution: False
B) An AR and an ARMA is_solution: True
C) An MA and an ARMA is_solution: False
D) Different models from within the ARMA familySolution: (B)33) Second differencing in time series can help to eliminate which trend?A)Quadratic Trend
B)Linear Trend
C) Both A & B
D)None of the aboveSolution: (A)The rst dierence is denoted as xt = xt xt1. (1)As we have seen, the rst dierence eliminates a linear trend. A second dierence, that is, the dierence of (1), can eliminate a quadratic trend, and so on.34) Which of the following cross validation techniques is better suited for time series data?A) k-Fold Cross Validation
B)Leave-one-out Cross Validation
C) Stratified Shuffle Split Cross Validation
D) Forward Chaining Cross ValidationTime series is ordered data. So the validation data must be ordered to. Forward chaining ensures this. It works as follows:35) BIC penalizes complex models more strongly than the AIC.A) TRUE
B) FALSESolution: (A)AIC = -2*ln(likelihood) + 2*k,BIC = -2*ln(likelihood) + ln(N)*k,where:k = model degrees of freedomN = number of observationsAt relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2).36) The figure below shows the estimated autocorrelation and partial autocorrelations of a time series of n = 60 observations. Based on these plots, we should.A)Transform the data by taking logs
B)Difference the series to obtain stationary data
C) Fit an MA(1) model to the time seriesSolution: (B)The autocorr shows a definite trend and partial autocorrelation shows a choppy trend, in such a scenario taking a log would be of no use. Differencing the series to obtain a stationary series is the only option.Question Context (37-38)
37) Use the estimated exponential smoothening given above and predict temperature for the next 3 years (1998-2000)These results summarize the fit of a simple exponential smooth to the time series.A) 0.2,0.32,0.6
B) 0.33, 0.33,0.33
C) 0.27,0.27,0.27
D) 0.4,0.3,0.37Solution: (B)The predicted value from the exponential smooth is the same for all 3 years, so all we need is the value for next year. The expression for the smooth issmootht =  yt + (1  ) smooth t-1Hence, for the next point, the next value of the smooth (the prediction for the next observation) issmoothn =  yn + (1  ) smooth n-1= 0.3968*0.43 + (1  0.3968)* 0.3968= 0.329738) Find 95% prediction intervals for the predictions of temperature in 1999.These results summarize the fit of a simple exponential smooth to the time series.A)0.3297 2 * 0.1125
B)0.3297 2 * 0.121
C) 0.3297 2 * 0.129
D)0.3297 2 * 0.22Solution: (B)The sd of the prediction errors is1 period out 0.11252 periods out 0.1125 sqrt(1+2) = 0.1125 * sqrt(1+ 0.39682)  0.12139) Which of the following statement is correct?1. If autoregressive parameter (p) in an ARIMA model is 1, it means that there is no auto-correlation in the series.
 2. If moving average component (q) in an ARIMA model is 1, it means that there is auto-correlation in the series with lag 1.
 3. If integrated component (d) in an ARIMA model is 0, it means that the series is not stationary.A) Only 1
B)Both 1 and 2
C) Only 2
D) All of the statementsSolution: (C)Autoregressive component:AR stands for autoregressive. Autoregressive parameter is denoted by p. When p =0, it means that there is no auto-correlation in the series. When p=1, it means that the series auto-correlation is till one lag.Integrated:In ARIMA time series analysis, integrated is denoted by d. Integration is the inverse of differencing. When d=0, it means the series is stationary and we do not need to take the difference of it. When d=1, it means that the series is not stationary and to make it stationary, we need to take the first difference. When d=2, it means that the series has been differenced twice. Usually, more than two time difference is not reliable.Moving average component:MA stands for moving the average, which is denoted by q. In ARIMA, moving average q=1 means that it is an error term and there is auto-correlation with one lag.40) In a time-series forecasting problem, if the seasonal indices for quarters 1, 2, and 3 are 0.80, 0.90, and 0.95 respectively. What can you say about the seasonal index of quarter 4?A)It will be less than 1
B) It will be greater than 1
C)It will be equal to 1
D) Seasonality does not exist
E)Data is insufficientSolution: (B)The seasonal indices must sum to 4, since there are 4 quarters. .80 + .90 + .95 = 2.65, so the seasonal index for the 4th quarter must be 1.35 so B is the correct answer.If you missed out on this competition, make sure you complete in the ones coming up shortly. We are giving cash prizes worth $10,000+ during the month of April 2017.If you have any questions or doubts feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-time-series-solution-skillpower-time-series-datafest-2017/
40 Questions on Probability for data science,Learn everything about Analytics|Introduction|Overall Scores|Useful Resources||End Notes,"Check out all the upcoming skilltestshere.|Related Articles|40 Questions to test a data scientist on Time Series [Solution: SkillPower  Time Series, DataFest 2017]|Deep Learning vs. Machine Learning  the essential differences you need to know!|
Dishashree Gupta
|40 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Probability forms the backbone of many important data science concepts from inferential statistics to Bayesian networks. It would not be wrong to say that the journey of mastering statistics begins with probability. This skilltest was conducted to help you identify your skill level in probability.A total of 1249 people registered for this skill test. The test was designed to test the conceptual knowledge of probability. If you are one of those who missed out on this skill test, here are the questions and solutions. You missed on the real time test, but can read this article to find out how you could have answered correctly.Here are the leaderboard ranking for all the participants.Are you preparing for your next data science interview? Then look no further! Check out the comprehensive Ace Data Science Interviews course which encompasses hundreds of questions like these along with plenty of videos, support and resources. And if youre looking to brush up your probability sills even more, we have covered it comprehensively in the Introduction to Data Science course!Below are the distribution scores, they will help you evaluate your performance.You can access the final scores here. More than 300 people participated in the skill test and the highest score obtained was 38. Here are a few statistics about the distribution.Mean Score: 19.56Median Score: 20Mode Score: 15This was also the first test where some one scored as high as 38! The community is getting serious about DataFestBasics of Probability for Data Science explained with examplesIntroduction to Conditional Probability and Bayes theorem for data science professionals1) Let A and B be events on the same sample space, with P (A) = 0.6 and P (B) = 0.7. Can these two events be disjoint?A) YesB) NoSolution: (B) These two events cannot be disjoint because P(A)+P(B) >1.P(AB) = P(A)+P(B)-P(AB). An event is disjoint if P(AB) = 0. If A and B are disjoint P(AB) = 0.6+0.7 = 1.3 And Since probability cannot be greater than 1, these two mentioned events cannot be disjoint.2) Alice has 2 kids and one of them is a girl. What is the probability that the other child is also a girl?You can assume that there are an equal number of males and females in the world.Solution: (C)The outcomes for two kids can be {BB, BG, GB, GG}Since it is mentioned that one of them is a girl, we can remove the BB option from the sample space. Therefore the sample space has 3 options while only one fits the second condition. Therefore the probability the second child will be a girl too is 1/3.3) A fair six-sided die is rolled twice. What is the probability of getting 2 on the first roll and not getting 4 on the second roll?Solution: (C)The two events mentioned are independent. The first roll of the die is independent of the second roll. Therefore the probabilities can be directly multiplied. P(getting first 2) = 1/6P(no second 4) = 5/6Therefore P(getting first 2 and no second 4) = 1/6* 5/6 = 5/364)A) TrueB) FalseSolution: (A)P(ACc) will be only P(A). P(only A)+P(C) will make it P(AC). P(BAcCc) is P(only B) Therefore P(AC) and P(only B) will make P(ABC)5) Consider a tetrahedral die and roll it twice. What is the probability that the number on the first roll is strictly higher than the number on the second roll?Note: A tetrahedral die has only four sides (1, 2, 3 and 4).Solution: (B)There are 6 out of 16 possibilities where the first roll is strictly higher than the second roll.6) Which of the following options cannot be the probability of any event?A) -0.00001
B) 0.5
C) 1.001Solution: (F)Probability always lie within 0 to 1.7) Anita randomly picks 4 cards from a deck of 52-cards and places them back into the deck ( Any set of 4 cards is equally likely ). Then, Babita randomly chooses 8 cards out of the same deck ( Any set of 8 cards is equally likely). Assume that the choice of 4 cards by Anita and the choice of 8 cards by Babita are independent. What is the probability that all 4 cards chosen by Anita are in the set of 8 cards chosen by Babita?B)48C4 x 52C8C)48C8 x 52C8Solution: (A)The total number of possible combination would be 52C4 (For selecting 4 cards by Anita) * 52C8 (For selecting 8 cards by Babita). Since, the 4 cards that Anita chooses is among the 8 cards which Babita has chosen, thus the number of combinations possible is 52C4 (For selecting the 4 cards selected by Anita) * 48C4 (For selecting any other 4 cards by Babita, since the 4 cards selected by Anita are common) Question Context 8:A player is randomly dealt a sequence of 13 cards from a deck of 52-cards. All sequences of 13 cards are equally likely. In an equivalent model, the cards are chosen and dealt one at a time. When choosing a card, the dealer is equally likely to pick any of the cards that remain in the deck.8) If you dealt 13 cards, what is the probability that the 13th card is a King?B) 1/13C) 1/26Solution: (B)Since we are not told anything about the first 12 cards that are dealt, the probability that the 13th card dealt is a King, is the same as the probability that the first card dealt, or in fact any particular card dealt is a King, and this equals: 4/529) A fair six-sided die is rolled 6 times. What is the probability of getting all outcomes as unique?Solution: (A)For all the outcomes to be unique, we have 6 choices for the first turn, 5 for the second turn, 4 for the third turn and so onTherefore the probability if getting all unique outcomes will be equal to 0.0154310) A group of 60 students is randomly split into 3 classes of equal size. All partitions are equally likely. Jack and Jill are two students belonging to that group. What is the probability that Jack and Jill will end up in the same class?Solution: (B)Assign a different number to each student from 1 to 60. Numbers 1 to 20 go in group 1, 21 to 40 go to group 2, 41 to 60 go to group 3. All possible partitions are obtained with equal probability by a random assignment if these numbers, it doesnt matter with which students we start, so we are free to start by assigning a random number to Jack and then we assign a random number to Jill. After Jack has been assigned a random number there are 59 random numbers available for Jill and 19 of these will put her in the same group as Jack. Therefore the probability is 19/5911) We have two coins, A and B. For each toss of coin A, the probability of getting head is 1/2 and for each toss of coin B, the probability of getting Heads is 1/3. All tosses of the same coin are independent. We select a coin at random and toss it till we get a head. The probability of selecting coin A is  and coin B is 3/4. What is the expected number of tosses to get the first heads?Solution: (A)If coin A is selected then the number of times the coin would be tossed for a guaranteed Heads is 2, similarly, for coin B it is 3. Thus the number of times would be Tosses = 2 * (1/4)[probability of selecting coin A] + 3*(3/4)[probability of selecting coin B]       = 2.7512) Suppose a life insurance company sells a $240,000 one year term life insurance policy to a 25-year old female for $210. The probability that the female survives the year is .999592. Find the expected value of this policy for the insurance company.B) $140C) $112Solution: (C)P(company loses the money ) = 0.99592P(company does not lose the money ) = 0.000408The amount of money company loses if it loses = 240,000  210 = 239790While the money it gains is $210Expected money the company will have to give = 239790*0.000408 = 97.8Expect money company gets = 210.Therefore the value = 210  98 = $11213)A) TrueB) FalseSolution: (A)The above statement is true. You would need to know thatP(A/B) = P(AB)/P(B)P(CcA|A) = P(CcAA)/P(A) = P(CcA)/P(A)P(B|A  Cc) = P(ABCc)/P(A  Cc)Multiplying the three we would get  P(ABCc), hence the equations holds true14) When an event A independent of itself?Solution: (D)The event can only be independent of itself when either there is no chance of it happening or when it is certain to happen. Event A and B is independent when P(AB) = P(A)*P(B). Now if B=A, P(AA) = P(A) when P(A) = 0 or 1.15) Suppose youre in the final round of Lets make a deal game show and you are supposed to choose from three doors  1, 2 & 3. One of the three doors has a car behind it and other two doors have goats. Lets say you choose Door 1 and the host opens Door 3 which has a goat behind it. To assure the probability of your win, which of the following options would you choose.Solution: (A) I would recommend reading this article for a detailed discussion of the Monty Halls Problem.16) Cross-fertilizing a red and a white flower produces red flowers 25% of the time. Now we cross-fertilize five pairs of red and white flowers and produce five offspring. What is the probability that there are no red flower plants in the five offspring?Solution: (A) The probability of offspring being Red is 0.25, thus the probability of the offspring not being red is 0.75. Since all the pairs are independent of each other, the probability that all the offsprings are not red would be (0.75)5 = 0.237. You can think of this as a binomial with all failures.17) A roulette wheel has 38 slots  18 red, 18 black, and 2 green. You play five games and always bet on red slots. How many games can you expect to win?A) 1.1165B) 2.3684C) 2.6316C) 2.6316Solution: (B)The probability that it would be Red in any spin is 18/38. Now, you are playing the game 5 times and all the games are independent of each other. Thus, the number of games that you can win would be 5*(18/38) = 2.3684 18) A roulette wheel has 38 slots, 18 are red, 18 are black, and 2 are green. You play five games and always bet on red. What is the probability that you win all the 5 games?Solution: (B)The probability that it would be Red in any spin is 18/38. Now, you are playing for game 5 times and all the games are independent of each other. Thus, the probability that you win all the games is (18/38)5 = 0.023819) Some test scores follow a normal distribution with a mean of 18 and a standard deviation of 6. What proportion of test takers have scored between 18 and 24?Solution: (C)So here we would need to calculate the Z scores for value being 18 and 24. We can easily doing that by putting sample mean as 18 and population mean as 18 with  = 6 and calculating Z. Similarly we can calculate Z for sample mean as 24.Z= (X-)/Therefore for 26 as X, Z = (18-18)/6 = 0 , looking at the Z table we find 50% people have scores below 18.For 24 as XZ = (24-18)/6 = 1, looking at the Z table we find 84% people have scores below 24.Therefore around 34% people have scores between 18 and 24.20) A jar contains 4 marbles. 3 Red & 1 white. Two marbles are drawn with replacement after each draw. What is the probability that the same color marble is drawn twice?Solution: (C) If the marbles are of the same color then it will be 3/4 * 3/4 + 1/4 * 1/4 = 5/8.21) Which of the following events is most likely?A) At least one 6, when 6 dice are rolledB) At least 2 sixes when 12 dice are rolledC) At least 3 sixes when 18 dice are rolledSolution: (A)Probability of 6 turning up in a roll of dice is P(6) = (1/6) & P(6) = (5/6). Thus, probability of Case 1: (1/6) * (5/6)5 = 0.06698 Case 2: (1/6)2 * (5/6)10 = 0.00448 Case 3: (1/6)3 * (5/6)15 = 0.0003Thus, the highest probability is Case 122) Suppose you were interviewed for a technical role. 50% of the people who sat for the first interview received the call for second interview. 95% of the people who got a call for second interview felt good about their first interview. 75% of people who did not receive a second call, also felt good about their first interview. If you felt good after your first interview, what is the probability that you will receive a second interview call?B) 56%C) 75%Solution: (B)Lets assume there are 100 people that gave the first round of interview. The 50 people got the interview call for the second round. Out of this 95 % felt good about their interview, which is 47.5. 50 people did not get a call for the interview; out of which 75% felt good about, which is 37.5. Thus, the total number of people that felt good after giving their interview is (37.5 + 47.5) 85. Thus, out of 85 people who felt good, only 47.5 got the call for next round. Hence, the probability of success is (47.5/85) = 0.558. Another more accepted way to solve this problem is the Bayes theorem. I leave it to you to check for yourself.23) A coin of diameter 1-inches is thrown on a table covered with a grid of lines each two inches apart. What is the probability that the coin lands inside a square without touching any of the lines of the grid?You can assume that the person throwing has no skill in throwing the coin and is throwing it randomly.You can assume that the person throwing has no skill in throwing the coin and is throwing it randomly.Solution: (B)Think about where all the center of the coin can be when it lands on 2 inches grid and it not touching the lines of the grid.If the yellow region is a 1 inch square and the outside square is of 2 inches. If the center falls in the yellow region, the coin will not touch the grid line. Since the total area is 4 and the area of the yellow region is 1, the probability is  .24) Therearea total of 8 bows of 2 each of green, yellow, orange & red. In how many ways can you select 1 bow?Solution: (C)You can select one bow out of four different bows, so you can select one bow in four different ways.25) Consider the following probability density function:What is the probability for X6 i.e. P(x6)What is the probability for X6 i.e. P(x6)Solution: (B)To calculate the area of a particular region of a probability density function, we need to integrate the function under the bounds of the values for which we need to calculate the probability.Therefore on integrating the given function from 0 to 6, we get 0.527626) In a class of 30 students, approximately what is the probability that two of the students have their birthday on the same day (defined by same day and month) (assuming its not a leap year)?For example  Students with birthday 3rd Jan 1993 and 3rd Jan 1994 would be a favorable event.A) 49%C) 70%Solution: (C)The total number of combinations possible for no two persons to have the same birthday in a class of 30 is 30 * (30-1)/2 = 435. Now, there are 365 days in a year (assuming its not a leap year). Thus, the probability of people having a different birthday would be 364/365. Now there are 870 combinations possible. Thus, the probability that no two people have the same birthday is (364/365)^435 = 0.303. Thus, the probability that two people would have their birthdays on the same date would be 1  0.303 = 0.696 27) Ahmed is playing a lottery game where he must pick 2 numbers from 0 to 9 followed by an English alphabet (from 26-letters). He may choose the same number both times.If his ticket matches the 2 numbers and 1 letter drawn in order, he wins the grand prize and receives $10405. If just his letter matches but one or both of the numbers do not match, he wins $100. Under any other circumstance, he wins nothing. The game costs him $5 to play. Suppose he has chosen 04R to play.What is the expected net profit from playing this ticket?B) $2.81C) $-1.82C) $-1.82Solution: (B)Expected value in this case E(X) = P(grand prize)*(10405-5)+P(small)(100-5)+P(losing)*(-5)P(grand prize)= (1/10)*(1/10)*(1/26)P(small) = 1/26-1/2600, the reason we need to do this is we need to exclude the case where he gets the letter right and also the numbers rights. Hence, we need to remove the scenario of getting the letter right. P(losing ) = 1-1/26-1/2600Therefore we can fit in the values to get the expected value as $2.8128) Assume you sell sandwiches. 70% people choose egg, and the rest choose chicken. What is the probability of selling 2 egg sandwiches to the next 3 customers?Solution: (C)The probability of selling Egg sandwich is 0.7 & that of a chicken sandwich is 0.3. Now, the probability that next 3 customers would order 2 egg sandwich is 3 * 0.7 * 0.7 *0.3 = 0.44. They can order them in any sequence, the probabilities would still be the same.
Question context: 29  30HIV is still a very scary disease to even get tested for. The US military tests its recruits for HIV when they are recruited. They are tested on three rounds of Elisa( an HIV test) before they are termed to be positive.The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.29) What is the probability that a recruit has HIV, given he tested positive on first Elisa test?The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.Solution: (A)I recommend going through the Bayes updating section of this article for the understanding of the above question.30) What is the probability of having HIV, given he tested positive on Elisa the second time as well.The prior probability of anyone having HIV is 0.00148. The true positive rate for Elisa is 93% and the true negative rate is 99%.Solution: (C)I recommend going through the Bayes updating section of this article for the understanding of the above question.31) Suppose youre playing a game in which we toss a fair coin multiple times. You have already lost thrice where you guessed heads but a tails appeared. Which of the below statements would be correct in this case?Solution: (C)This is a classic problem of gamblers fallacy/monte carlo fallacy, where the person falsely starts to think that the results should even out in a few turns. The gambler starts to believe that if we have received 3 heads, you should receive a 3 tails. This is however not true. The results would even out only in infinite number of trials.
32) The inference using the frequentist approach will always yield the same result as the Bayesian approach.Solution: (B)The frequentist Approach is highly dependent on how we define the hypothesis while Bayesian approach helps us update our prior beliefs. Therefore the frequentist approach might result in an opposite inference if we declare the hypothesis differently. Hence the two approaches might not yield the same results.33) Hospital records show that 75% of patients suffering from a disease die due to that disease. What is the probability that 4 out of the 6 randomly selected patients recover?Solution: (C) Think of this as a binomial since there are only 2 outcomes, either the patient dies or he survives. Here n =6, and x=4. p=0.25(probability if living(success)) q = 0.75(probability of dying(failure))P(X) = nCx pxqn-x = 6C4 (0.25)4(0.75)2 = 0.0329534) The students of a particular class were given two tests for evaluation. Twenty-five percent of the class cleared both the tests and forty-five percent of the students were able to clear the first test.Calculate the percentage of students who passed the second test given that they were also able to pass the first test.B) 42%Solution: (C) This is a simple problem of conditional probability. Let A be the event of passing in first test.B is the event of passing in the second test.P(AB) is passing in both the eventsP(passing in second given he passed in the first one) = P(AB)/P(A)= 0.25/0.45 which is around 55%35) While it is said that the probabilities of having a boy or a girl are the same, lets assume that the actual probability of having a boy is slightly higher at 0.51. Suppose a couple plans to have 3 children. What is the probability that exactly 2 of them will be boys?Solution: (A)Think of this as a binomial distribution where getting a success is a boy and failure is a girl. Therefore we need to calculate the probability of getting 2 out of three successes.P(X) = nCx pxqn-x = 3C2 (0.51)2(0.49)1 = 0.38236) Heights of 10 year-olds, regardless of gender, closely follow a normal distribution with mean 55 inches and standard deviation 6 inches. Which of the following is true?C) A 10-year-old who is 65 inches tall would be considered more unusual than a 10-year-old who is 45 inches tallSolution: (D)None of the above statements are true. 37) About 30% of human twins are identical, and the rest are fraternal. Identical twins are necessarily the same sex, half are males and the other half are females. One-quarter of fraternal twins are both males, one-quarter both female, and one-half are mixed: one male, one female. You have just become a parent of twins and are told they are both girls. Given this information, what is the probability that they are identical?A) 50%Solution: (C)This is a classic problem of Bayes theorem. P(I) denoted Probability of being identical and P(~I) denotes Probability of not being identicalP(Identical) = 0.3P(not Identical)= 0.7P(FF|I)= 0.5P(MM|I)= 0.5P(MM|~I)= 0.25P(FF|~I)= 0.25P(FM|~I)= 0.25P(I|FF) = 0.4638) Rob has fever and the doctor suspects it to be typhoid. To be sure, the doctor wants to conduct the test. The test results positive when the patient actually has typhoid 80% of the time. The test gives positive when the patient does not have typhoid 10% of the time. If 1% of the population has typhoid, what is the probability that Rob has typhoid provided he tested positive?Solution: (B) We need to find the probability of having typhoid given he tested positive. =P(testing +ve and having typhoid) / P(testing positive)= = 0.074
39) Jack is having two coins in his hand. Out of the two coins, one is a real coin and the second one is a faulty one with Tails on both sides. He blindfolds himself to choose a random coin and tosses it in the air. The coin falls down with Tails facing upwards. What is the probability that this tail is shown by the faulty coin?Solution: (B)We need to find the probability of the coin being faulty given that it showed tails.P(Faulty) = 0.5P(getting tails) = 3/4P(faulty and tails) =0.5*1 = 0.5Therefore the probability of coin being faulty given that it showed tails would be 2/340) A fly has a life between 4-6 days. What is the probability that the fly will die at exactly 5 days?A) 1/2B) 1/4Solution: (D)Here since the probabilities are continuous, the probabilities form a mass function. The probability of a certain event is calculated by finding the area under the curve for the given conditions. Here since were trying to calculate the probability of the fly dying at exactly 5 days  the area under the curve would be 0. Also to come to think of it, the probability if dying at exactly 5 days is impossible for us to even figure out since we cannot measure with infinite precision if it was exactly 5 days.If you missed out on this competition, make sure you complete in the ones coming up shortly. We are giving cash prizes worth $10,000+ during the month of April 2017.If you have any questions or doubts feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/
Deep Learning vs. Machine Learning  the essential differences you need to know!,Learn everything about Analytics|Overview|Introduction|Table of Contents|1.What is Machine Learning and Deep Learning?|2. Comparison of Machine Learning and Deep Learning|3. Where is Machine Learning and Deep Learning being applied right now?|4. Pop quiz|5. Future Trends,"1.1 What is Machine Learning?|1.2 What is Deep Learning?|2.1 Data dependencies||2.2 Hardware dependencies|2.3 Feature engineering||2.4 Problem Solving approach|2.5 Execution time|2.6 Interpretability|End notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|40 Questions on Probability for data science|Feature Engineering in IoT Age  How to deal with IoT data and create features for machine learning?|
Faizan Shaikh
|51 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning and deep learning on a rage! All of a sudden every one is talking about them  irrespective of whether they understand the differences or not! Whether you have been actively following data science or not  you would have heard these terms.Just to show you the kind of attention they are getting, here is the Google trend for these keywords,Deep Learning vs. Machine Learning  the essential differences you need to know!If you have often wondered to yourself what is the difference between machine learning and deep learning, read on to find out a detailed comparison in simple layman language. I have explainedeach of these term in detail. Then I have gone ahead to compare both of them and explained where we can use them.Let us start with the basics  What isMachine Learning and What is Deep Learning. If you already know this, feel free to move to section 2.The widely-quoted definition of Machine learning by Tom Mitchell best explains machine learning in a nutshell. Heres what it says:A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E Did that sound puzzling or confusing? Lets break this down with simple examples.Example 1  Machine Learning  Predicting weights based on heightLet us say you want to create a system which tells expected weight based on height of a person. There could be several reasons why some thing like this could be of interest. You can use this to filter out any possible frauds or data capturing errors. The first thing you do is collect data. Let us say this is how your data looks like:Each point on the graph represents one data point. To start with we can draw a simple line to predict weight based on height. For example a simple line:can help us make predictions. While the line does a decent job, we need to understand its performance. In this case, we can say that we want to reduce the difference between the Predictions and actuals. That is our way to measure performance.Further, the more data points we collect (Experience), the better will our model become. We can also improve our model by adding more variables (e.g. Gender) and creating different prediction lines for them.Example 2  Storm prediction SystemLet us take slightly more complex example. Suppose you are building a storm prediction system. You are given the data of all the storms which have occurred in the past, along with the weather conditions three months before the occurrence of these storms.Consider this, if we were to manually build a storm prediction system, what do we have to do?We have to first scour through all the data and find patterns in this data. Our task is to search which conditions lead to a storm.We can either model conditions like  if the temperature is greater than 40-degree celsius, humidity is in the range 80 to 100, etc. And feed these features manually to our system.Or else, we can make our system understand from the data what willbe the appropriate values for these features.Now to find these values, you would go through all the previous data and try to predict if there will be a storm or not. Based on the values of the features set by our system, we evaluate how the system performs, viz how many times the system correctly predicts the occurrence of a storm. We can furtheriterate the above step multiple times, giving performance as feedback to our system.Lets take our formal definition and try to define our storm prediction system: Our task T here is to find what are the atmospheric conditions that would set off a storm. Performance P would be, of all the conditions provided to the system, how many times will it correctly predict a storm. And experience E would be the reiterations of our system.The concept of deep learning is not new. It has been around for a couple of years now. But nowadays with all the hype, deep learning is getting more attention. As we did in Machine Learning, we will look at a formal definition of Deep Learning and then break it down with example.Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.Now  that one would be confusing. Let us break it with simple example.Example 1  Shape detectionLet me start with a simple example which explains how things happen at a conceptual level. Let us try and understand how we recognize a square from other shapes.The first thing our eyes do is check whether there are 4 lines associated with a figure or not (simple concept). If we find 4 lines, we further check, if they are connected, closed, perpendicular and that they are equal as well (nested hierarchy of concept).So, we took a complex task (identifying a square) and broke it in simple less abstract tasks. Deep Learning essentially does this at a large scale.Example 2  Cat vs. DogLets take an example of an animal recognizer, where our system has to recognize whether the given image is of a cat or a dog.If we solve this as a typical machine learning problem, we will define features such as if the animal has whiskers or not, if the animal has ears & if yes, then if they are pointed. In short, we will define the facial features and let the system identify which features are more important in classifying a particular animal.Now, deep learning takes this one step ahead. Deep learning automatically finds out the features which are important for classification, where in Machine Learning we had to manually give the features. Deep learning works as follows:Deep learning works as follows:Now that you have understood an overview of Machine Learning and Deep Learning, we will take a few important points and compare the two techniques.The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms dont perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly. On the other hand, traditional machine learning algorithms with their handcrafted rules prevail in this scenario. Below image summarizes this fact.Deep learning algorithms heavily depend on high-end machines, contrary to traditional machine learning algorithms, which can work on low-end machines. This is because the requirements of deep learning algorithm include GPUs which are an integral part of its working. Deep learning algorithms inherently do a large amount of matrix multiplication operations. These operations can be efficiently optimized using a GPU because GPU is built for this purpose.Feature engineering is a process of putting domain knowledge into the creation of feature extractors to reduce the complexity of the data and make patterns more visible to learning algorithms to work. This process is difficult and expensive in terms of time and expertise.In Machine learning, most of the applied features need to be identified by an expert and then hand-coded as per the domain and data type.For example, features can be pixel values, shape, textures, position and orientation. The performance of most of the Machine Learning algorithm depends on how accurately the features are identified and extracted.Deep learning algorithms try to learn high-level features from data. This is a very distinctive part of Deep Learning and a major step ahead of traditional Machine Learning. Therefore, deep learning reduces the task of developing new feature extractor for every problem. Like,Convolutional NN will try to learn low-level features such as edges and lines in early layers then parts of faces of people and then high-level representation of a face.When solving a problem using traditional machine learning algorithm, it is generally recommended to break the problem down into different parts, solve them individually and combine them to get the result. Deep learning in contrast advocates to solve the problem end-to-end.Lets take an example to understand this.Suppose you have a task of multiple object detection. The task is to identify what is the object and where is it present in the image.In a typical machine learning approach, you would divide the problem into two steps, object detection and object recognition. First, you would use a bounding box detection algorithm like grabcut, to skim through the image and find all the possible objects. Then of all the recognized objects, you would then use object recognition algorithm like SVM with HOG to recognize relevant objects.On the contrary, in deep learning approach, you would do the process end-to-end. For example, in a YOLO net (which is a type of deep learning algorithm), you would pass in an image, and it would give out the location along with the name of object.Learn Computer Vision using Deep Learning hereUsually, a deep learning algorithm takes a long time to train. This is because there are so many parameters in a deep learning algorithm that training them takes longer than usual. State of the art deep learning algorithm ResNet takes about two weeks to train completely from scratch. Whereas machine learning comparatively takes much less time to train, ranging from a few seconds to a few hours.This is turn is completely reversed on testing time. At test time, deep learning algorithm takes much less time to run. Whereas, if you compare it with k-nearest neighbors (a type of machine learning algorithm), test time increases on increasing the size of data. Although this is not applicable on all machine learning algorithms, as some of them have small testing times too.Last but not the least, we have interpretability as a factor for comparison of machine learning and deep learning. This factor is the main reason deep learning is still thought 10 times before its use in industry.Lets take an example. Suppose we use deep learning to give automated scoring to essays. The performance it gives in scoring is quite excellent and is near human performance. But theres is an issue. It does not reveal why it has given that score. Indeed mathematically you can find out which nodes of a deep neural network were activated, but we dont know what there neurons were supposed to model and what these layers of neurons were doing collectively. So we fail to interpret the results.On the other hand, machine learning algorithms like decision trees give us crisp rules as to why it chose what it chose, so it is particularly easy to interpret the reasoning behind it. Therefore, algorithms like decision trees and linear/logistic regression are primarily used in industry for interpretability.The wiki article gives an overview of all the domains where machine learning has been applied. These include:The image given above aptly summarizes the applications areas of machine learning. Although it covers broader topic of machine intelligence as a whole.One prime example of a company using machine learning / deep learning is Google.In the above image, you can see how Google is applying machine learning in its various products. Applications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity!To assess if you really understood the difference, we will do a quiz. You can post your answers in this thread.Please mention the steps below to completely answer it.Scenario 1:You have to build a software component for self-driving car. The system you build should take in the raw pixel data from cameras and predict what would be the angle by which you should steer your car wheel.Scenario 2:Given a persons credentials and background information, your system should assess whether a person should be eligible for a loan grant.Scenario 3:You have to create a system that can translate a message written in Russian to Hindi so that a Russian delegate can address the local masses.The above article would have given you anoverview of Machine Learning and Deep Learning and the difference between them. In this section, Im sharing my viewies on how Machine Learning and Deep Learning would progress in the future.I personally follow these trends closely. I generally get a scoop from Machine Learning/Deep Learning newsletters, which keep me updated with recent happenings. Along with this, I follow arxiv papers and their respective code, which are published every day.In this article, we had a high-level overview and comparison between deep learning and machine learning techniques. I hope I could motivate you to learn further in machine learning and deep learning. Here are the learning path for machine learning & deep learningLearning path for machine learningandLearning path for deep learning.If you have any questions, feel free to drop them below in the comments section.",https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/
Feature Engineering in IoT Age  How to deal with IoT data and create features for machine learning?,Learn everything about Analytics|Introduction|Table of Contents|1. The IoT (Internet of Things) revolution|2. Nature of IoT or sensor data|3. Aggregation of data for feature engineering|||4. Feature generation|6. Further readings,"Usage at an atomic level (without aggregation)  an example|Usage of aggregated sensor data  an example|Selecting the optimal time window for aggregating sensor data|Types of aggregation|Treatment for missing values|Basic features|Features based on relationships|Features based on higher order statistics|Features based on outlier detection|Features based on series transformation|About the Author|Share this:|Like this:|Related Articles|Deep Learning vs. Machine Learning  the essential differences you need to know!|DL Internship- Chennai (3 Months)|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Fast Fourier Transform,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you ask any experienced analytics or data science professional, what differentiates a good model from a bad model  chances are that you will heara uniform answer. Whether you call itcharacteristics generation or variable generation (as it was known traditionally) or feature engineering  the importance of this step is unanimously agreed in the data science / analytics world.This step involves creating a large and diverse set of derived variables from the base data. The richer the set of variables that are generated, the better will beyourmodels. Most of our time and coding efforts are usually spent in the area of feature engineering. Therefore, understanding feature engineering for specific data sources is a key success factors for us. Unfortunately, most analytics courses and text books do not cover this aspect in great detail. This article is a humble effort in that direction.I am sure you know this already  none of us are untouched by the impact of IoT. Look at the forecast for mobile data traffic from Cisco (as done in 2015) below:In the last few years, continuous streaming data from sensors has emerged as one of the most promising data sources, fuelling a number of interesting developments in Internet of Things. Some of the common areas which are being revolutionised by the emergence of IOT include vehicle telematics, predictive maintenance of equipment, manufacturing quality management, connected consumer devices, health care and many more.Given the fast pace of change to connected devices and our perspective of data science, we think that data science professionals need to understandand explore feature engineering of IOT or sensor data. And that is what we will attempt in this article. Just a caveat, though  likeany other area, this is a vast and emerging field and hence, this is not a comprehensive guide. But, it is good enough for you to get started.IoT or sensor data consists of a continuous stream of data, the time interval between successive updates of the data is very small; usually minutes, seconds or even mili-seconds.The data produced, usually pertains to information about the physical state of a system or a human being. Examples being temperature, pressure, voltage, current, flow rate, velocity, acceleration, heart rate, blood pressure etc. In case of many sensors, the data stream also provides continuous information about the state of the system. For instance, in case of vehicle telematics data, along with the information from a particular sensor (e.g. temperature, flow of lubricant etc.), the status of the car (e.g. running, idling, cranking) is also captured as a continuous stream.Many IoT applications also consider other less dynamic data along with the continuous streaming data mentioned above. Such less dynamic data usually pertains to the overall state or working condition of the system. Examples being type of input materials being processed by a machine, the treatment regime that is being administrated to a patient etc.Prior to creation of features from IOT or sensor data, it is important to consider the level of aggregation (across time) of the continuous streaming data. In many cases, the continuous streaming data is not aggregated but is used at the most granular or atomic level. If one assumes that a particular sensor data is available every second, then the signal at each second interval is used for generating features. In most cases where the prediction time windows are very short, this is the most useful level of aggregation. On the other hand, in case where the prediction time window is longer, it may be most optimal to aggregate the signal data over specific time windows.To sum up, while working at the atomic level, the data is not aggregated over time and hence, is used at the most granular level. But for certain problems the data is aggregated over specific time windows. Let me discuss this with an example of each type.As an example of atomic aggregation of sensor data, one may consider a motor that is used as prime mover for a grinding machine. Based on the input particle size and operating parameters of the grinder (e.g. temperature, flow rate etc.) the rotations per minute (RMP) of the motor is controlled dynamically. In this case, the data on input particle size, temperature within various parts of the grinder, and input flow rate are available every second. Based on these inputs, the RPM of the motor needs to be change dynamically. If the motor rotates faster than the optimal speed then it will cause overheating of the grinder, leading to possible damage of certain areas of the grinder. On the other hand, if the RPM is too low then it will cause a decrease in production rate.Data on temperature, particle size and flow rate are taken as inputs and a model is used to determine the right RPM for the motor in such a way that there is very low probability of damage to the grinder. It is apparent that the time window for predicting the right RPM is within a few seconds or maximum a minute. In this case, one will most likely use the input data at every second level to derive features that will be used as possible candidate variables in the model. Usually, applications, involving process control use very short prediction window which necessitates feature generation at the atomic level. Figuire-1 below, illustrates the grinder assembly described above.Figure 1: Grinder AssemblyUsually, one assumes that the process underlying the generation of the sensor data is memory less, which implies that, the value of the sensor data stream within a particular window is not dependent on the values in the previous window.Let us look at the other example, where we aggregate the data. Consider a car which has being driven for a few years. If one is interested in predicting the life time of a specific component within the car or the equipment, then one is faced with a situation where the prediction horizons are longer-term in nature (days, weeks or months). In this case, the sensors inside the car will produce data at frequent time intervals similar to the earlier case. But the data needs to be aggregated over time to understand meaningful trends and changes that will signify an impending failure over a longer time horizon. In these cases, both atomic level and aggregated level are used for generating the features, but in most cases, the aggregated level features prove more productive. This also holds true for a lot of other manufacturing equipments.The next obvious question is what should be the appropriate time window for aggregating the sensor data before feature derivation. Selecting the time window is often an important consideration that drives the success of the feature engineering exercise in IoT. Usually the following types of windows are used for aggregation.Once the window for aggregation has been arrived at, the next step involves aggregating the sensor data over these time windows to create a set of new variables / features from the atomic ones. The aggregations performed are typically driven by the context and the data types. Some of the features are generated by using data across multiple windows; while others are generated by creating features within a single window. The section on feature generation provides a list of the features that are usually generated.Aggregating sensor data across various time windows also helps in treating missing values. In case of sensor data, missing values may arise due to failure of the sensing, transmitting or recording systems. For instance, due to issues in satellite connectivity, a telematics sensor inside a car engine may not be transmitting any signal for a specific period of time. The use of a variable time window allows the data to be aggregated over sufficient lengths of time which allows for certain threshold amount of data. For instance one may define a variable time window as a time window that captures at least 1000 pings from the sensor.Post the aggregation process, one creates a continuous stream of values across time dimensions (if the data is not aggregated and hence, used at an atomic level) or across various windows. The following diagram illustrates a visualization of the data, pertaining to temperature sensor inside a machine. As expected the variability of the data is lower post aggregation; this suggests the reason of using atomic level data for shorter window predictions.Figuire-2: Impact of aggregation windowThe following features can be generated post the creation of the series (either aggregated or atomic).The simplest set of features involve change and rate of change (essentially the first and second differences) and percentage changes (growth or decay). If the series of value is represented by X1..Xn and time is represented as t1.tn then some of basic the features are calculated as follows. It should be noted that these are only a sample set of features possible and is by no means comprehensive. Feature engineering is a combination of science, creativity and domain knowledge.In many cases, one will leverage data coming out of multiple sensors, each pertaining to similar or dis-similar quantities that are measured. In most cases, there may be a natural relationship between the multiple signals emanating from the various types of sensors. However, in certain cases, the signal from a particular sensor may demonstrate a value outside the natural relationship. This may be driven by changes in operating conditions, breakdown etc. As a data scientist, it may be a critical objective to identify such events and convert them into features.To identify such un-natural events, one may first build a simple model; usually, a regression model by taking one of the Xs as a dependent variable and the others as independent variables. In most cases these are simple ordinary least square regression models (but might use squares and cubes of some of the Xs as input variables). The error of that regression, provides a measure of the un-natural behaviour. If in some cases, one observes an outlier in the error term, then the same may be used asa feature. Further, the series of errors can be used as a new X variable and subsequent features may be generated using this new series.It should be noted that while building the regression model to create the error series, care should be taken to ensure that the outliers are excluded and the error term obey the usual assumptions. This will help ensure that the un-natural behaviours are captured appropriately once the outlier data points are included.An example has been provided to illustrate the above proposition, if one refers to the grinder example illustrated earlier, one may have the following continuous streaming sensor data that will be available:As per the usual pattern of expected data, it may be expected that the temperate recorded by each thermocouple is a function of RPM, flow rates (both input and output) and particle size (both input and output). Once the RPM increases there is higher friction and hence, temperatures should naturally increase. Similarly, if the ratio between the input and output particle sizes are higher; it represents higher grinding ratio which may cause heating up. Therefore, one may be interested in knowing if the temperature recorded by any of the thermocouple is above or below this normal behaviour. For instance, if a there is some harder impurity mixed with the input material then it may cause above normal heating. To address this requirement, one may create specific features to capture such un-natural behaviour. For instance, one may build a simple regression model with temperature as the Y and RPM, particle size and flow rate as the Xs. Once this regression is applied on the data series, one should be able to calculate an error series. Under normal operating circumstances, the value of the errors should be within a limit; however, during un-natural working conditions. The error may shoot up or the standard deviation of the error may shoot up. Taking this error series e1.en; one may create a set of features similar to the ones described in the last section.Usually moments (mean, variance, skewness and kurtosis etc.) are calculated within the aggregation window. However, in cases where the series is non-Gaussian, one uses cumulants rather than the moments for obtaining information about the nature of the distribution of the series within the window. The cumulants of a series X are defined using the cumulant-generating function, which is the natural logarithm of the moment-generating function. Till the 3rd cumulant, the value of the moment and the cumulant are the same. However, from the fourth and higher-order cumulants are not equal to moments. Deriving the relationship between moments and cumulants is beyond the scope of the current article; but interested readers should explore the derivation of the same.In many uses cases, it may be required to detect outliers and use presence of outliers within an aggregation window as features. The most popular method of outlier detection involves use of techniques like Kalman filter. Other methods involve creating a principal component analysis using a set of already created features and then identifying the distance of each point within the aggregation window from the origin of this multi-dimensional space. Any point which may be 3 or more standard deviations away from the origin may be considered as an outlier. In a regression type approach described in the last section, Cook distance or similar measures can also be used for outlier detection.Usually a series of continuous streaming data can be considered as a signal in the time domain. Established mathematical transformations can be used to decompose this signal into a set of simpler functions, the forms of those simpler functions can then be analysed across multiple aggregation windows to identify any significant change in the pattern of the signal. Usually the following transformations are attempted. In this article, a brief description of the Fast Fourier Transformation has been provided as is it possibly the most commonly used transformation.The Fourier transform of any signal is used to transform the signal from a time domain to a frequency domain. The transform helps in decomposing the original signal into a series of sinusoidal function of the form of ACos(t) + BjSin(t). Which is essentially a combination of the Cosine and Sine functions in the real and imaginary space. The most amazing aspect of the Fourier transform is that it can transform any function (irrespective of its shape) to a combination of a large number of sinusoidal functions. The Fast Fourier Transform (FFT) is an algorithm designed by Cooley and Tukey in the 1960s. The FFT is used to calculate the amplitude, phase and frequencies of the sinusoids that should be combined to recreate an original signal. This can be intuitively thought as a process of breaking a piece of music into the component notes that can be combined across various scales to recreate the original piece of music.To use this fascinating mathematical transformation for feature engineering, one needs to arrive at the FFT of the signal for each aggregation window. This will produce the frequencies and amplitudes of each component sinusoid across the aggregation windows. Then the series of amplitudes for a given frequency, can be used for further feature generation (similar to the ones described earlier). Alternately, if a new frequency is detected within any aggregation window, then the same may be an indicator of an anomalous behaviour.Taking the music metaphor further, one may consider this process as listening to the music produced by the signal. Wherein one tries to identify any abnormal or discordant note in the music being produced in each aggregation window.The paper titled A Cumulant-Based Method for Gait Identification Using Accelerometer Data with Principal Component Analysis and Support Vector Machine by SEBASTIJAN SPRAGER, DAMJAN ZAZULA; provides a suitable application of cumulants for feature engineering.Readers may refer to the paper titled A Kalman Filter for Robust Outlier Detection by Jo-Anne Ting, Evangelos Theodorou, and Stefan Schaal. For understanding the use of Kalman Filters for feature engineering.For understanding the application of Fourier Transformations, readers may refer to the paper titled Classification of epileptiform EEG using a hybrid system based on decision tree classifier and fast Fourier transform by Kemal Polat, and Salih Gne as a good example. The paper titled Eeg alpha spindle measures as indicators of driver fatigue under real traffic conditions by M. Simon, E. A. Schmidt, W. E. Kincses et al. is also a good exampleSandhya Kuruganti and Hindol Basu are authors of a book on business analytics titledBusiness Analytics: Applications to Consumer Marketing, recently published by McGraw Hill and is available on Flipkart and Amazon India. They are seasoned analytics professionals with a collective industry experience of more than 30 years.",https://www.analyticsvidhya.com/blog/2017/04/feature-engineering-in-iot-age-how-to-deal-with-iot-data-and-create-features-for-machine-learning/
DL Internship- Chennai (3 Months),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Feature Engineering in IoT Age  How to deal with IoT data and create features for machine learning?|Winners Approach  Rampaging DataHulk MiniHack, AV DataFest 2017|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0.1  2 years
Requirements : 
Task Info : About the Company:We are one of the top management consulting firm across the globe.Responsibility:The intern will work with us full time on developing a POC that involves image processing into a client solution.Required skills+ Advanced Python programming (pandas, numpy, scipy, scikit-image, scikit-learn)+ Computer Vision using python (Image processing & object detection , Open-cv frame work)+ Deep learning frameworks  keras & theano+ Prior experience working on Linux systemsGood to have+ Deep Learning frameworks Tensorflow & Caffe+ Expertise in face detection and recognition CTC would be INR 30,000/- per month
College Preference : tier1-any
Min Qualification : ug
Skills : Caffe, Computer Vision, deep learning, Face Detection, Face Recognition, image processing, Keras, linux, Object Detection, python, tensor flow
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/dl-internship-chennai-3-months/
"Winners Approach  Rampaging DataHulk MiniHack, AV DataFest 2017","Learn everything about Analytics|Introduction|The problem statement|Winners|Rank 3, Santanu Pattanayak|Rank 2, Prince Atul|Rank 1, Akash Gupta|End Notes","Check out all the upcoming competitions here.|Share this:|Like this:|Related Articles|DL Internship- Chennai (3 Months)|MGR Analytics Msbi Big Data OR Data Visualization- Hyderabad (7-12 Years Of Experience)|
Sunil Ray
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Feature Engineering|Parameter Tuning|Running the code,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Who are you competing with?While participating in a hackathon, a lot of people think that they are competing against the top data scientists. While, in reality, most of us really compete with ourselves. The ones who improve themselves, the ones whocompeting with their ownprevious self and push theirlimits to become better are always the eventual winners.We see this happen very frequently on Analytics Vidhya. We saw this again in our first ML contest of DataFest 2017 Rampaging DataHulk. In this minihack, we saw experienced professionals, students & previous winners compete with each other for the top 3 ranks. A total of 1458 people participated in the minihack.The competition began at 6 PM on 2 April marking the first competition in DataFest.After a fist to fist battle in true Hulk-athon style, we saw something remarkable. Something which hasnt happened for a while on Analytics Vidhya. The top 3 ranks were bagged by first-time winners. To top it up, the winner is still in his college days! That is just a testimony to the competitiveness and openness of the platform.Like always, the winners of the competition have generously shared their detailed approach and the codes they used in the competition.If you missed out the fun this weekend, make sure you participate in the upcoming Machine Learning Hackathon & The QuickSolver MiniHack.The problem statement revolved around a hedge fund company QuickMoney.They rely on automated systems to carry out trades in the stock market at inter-day frequencies. They wish to create a machine learning-based strategy for predicting the movement in stock prices for maximizing their profit. So they were seeking out a help from top data scientists.Stock markets are known to have a high degree of unpredictability but it is possible to beat the odds and create a system which will outperform others.The participants were required to create a trading strategy for maximizing their profit in the stock market. The task was to predict the probability whether the price for a particular stock for next day market close will be higher(1) or lower(0) compared to the price for market close today.The winners used different approaches and rose up on the leaderboard. Below are the top 3 winners on the leaderboard:Rank 1: Akash GuptaRank 2: Prince AtulRank 3: Santanu PattanayakHere are the final rankings of all the participants at theleaderboard.All the Top 3 winners have shared their detailed approach & code from the competition. I am sure you are eager to know their secrets, go ahead.Santanu PattanayakSantanu Pattanayak is Lead Data Scientist at GE Digital. He often participates in machine learning competitions on Analytics Vidhya. He likes to challenge himself.Followingis the approach he took for the Analytics Vidhya Rampaging Datahulk Competition. He secured 3rd place in the competition with a private Leaderboard Score of 0.678784:1. First, I did some exploratory data analysis. I checked the number of records in train and test datasets and checked whether there is any class imbalance that we need to deal with. The training dataset was quite balanced with 45% of the data belonging to the positive class. Since the dataset sizes were satisfactory i.e. 702739 train records and 101946 test records hence class imbalance adjustments were not necessary. Then I checked the number of different stocks in both train and test and checked whether all the stocks in test are there in train dataset or not. The train dataset has 1955 stocks while the test dataset has 2118 stocks. Since the test has more stocks clearly stock id cannot be used as a feature since the model would learn nothing about those stock ids that are there in test but not in train.2. The main task as in most of the machine learning tasks is to do proper feature engineering. So, spend quite a bit of time thinking what would be good features with respect to the output that we are going to predict  that is whether the sales of tomorrows market close is going to be higher than todays market close.There were missing values in the below fields:I replaced the missing values with 99999 and created indicator variables indicating whether these fields have missing values.Then I created few variables capturing the difference in the moving averages. For example  (Three_Day_Moving_Average  Ten_Day_Moving_Average). I created such variables for each pair of the moving average variables.I created couple of features by taking the sum and difference of the variables Positive_Directional_Movement and Negative_Directional_Movement. Similarly, I created two features by taking the sum and difference of the variables True_Range and Average_True_Range.Also, I created few features to hold the moving average of the days prior to a specific period as below:Here the first variable is computing the average of the 7 days prior to the last 3 days.3. Once I build these features then I split the training data into two parts  80% of the data for training the models and 20% for validation purpose. Below are the models that I tried Gradient boosting from graphlab  Its always easy to work with graphlab since you can input a dataframe along with the features and target unlike most of the other packages wherein you would have to create a numpy matrix or a sparse matrix before the algorithms can be invoked. Experimented with 300,500 and 700 trees, with the class weights set to auto, tree depth of 6, min child weight and minimum loss reduction set to 4 each. Also, the column subsample and the row subsample was set to 80 percent.It gave good performance with validation logloss of around 0.6820 and public leaderboard of around 0.6855I tried my hand at a small neural network through Keras with two hidden layers of 300 units each and dropout of each hidden layer set to 0.5. For the hidden layers I chose activation as RELU and the output layer as sigmoid and got a logloss of around 0.688 in both validation and in leaderboard.Since the neural network and Gradient boosting are very different models I tried to take the mean of their predicted probabilities and the public leaderboard logloss improved to 0.6831.Still I was not able to enter the 0.67 range.Next, I tried my luck at xgboost with kind of similar configuration as that of the graphlab gradient boosting model.I experimented a bit with the number of trees and finally got the best results with the below parameters.The above model gave me 0.6780 logloss on Public leaderboard (9th rank) and 0.6787 logloss on the private leaderboard (3rd place).Solution: Code FilePrince AtulPrince Atul is a Senior Scientist at Cognizant. Prince has been participating in various competitions at Analytics Vidhya. Prince is also a volunteer for Analytics Vidhya and helps us with our community efforts. This is his approach:I decided to approach this hackathon with more focus on feature engineering than on model selection and data processing. After reading the problem, I decided to use gradient boosting with binary logistics.I always submit a preliminary model, generally with all the variables, to set a benchmark score.There were 4 moving averages in the data set and I expected them to be correlated. So, I plotted correlation matrix and as expected 10 days and 20 days moving average were highly correlated with other moving averages. I removed these two variables and trained my model on rest of the data. This model was giving a 0.68 (approx.) score on public leaderboard.I checked for null values and there were 4000+ rows which had missing values. I left it as it because it was very small percentage of the train data set. (Wanted to come back to it, didnt get time)After this I started creating features. Features which improved my score were (1,0,-1 values) :- comparison of 3 days moving average with other moving averages, comparison of 5 days moving average with other moving averages and sum of these comparison value. I created this to use price movement direction based on moving averages. After creating this, my model was giving a score of 0.677(approx.) on public leaderboard.I think that hardest part in any mini-hackathon is to create features. It takes some thinking and not every feature you create will add values. But, it is important to keep on doing it even if first few features are not able to improve your model.Solution: Code FileAkash GuptaAkash Gupta is a final year student at IIT Roorkee. Akash is one of the most competitive students we have come across on Analytics Vidhya. He fetched his last win in The Ultimate Student Hunt competition by securing 5th rank.Find out whats his secret for winning this minihack.Initialization:I started out by trying a basic xgboost model using the given features and filling the missing values with -1. I generally start with xgboost because of its speed and good scores. I had removed the ID and timestamp features.Cross Validation: To set up a quick cross validation, I randomly sampled out 10% of the dataset and set that up as the eval data. I had planned to write for timestamp-based partitioning later. But the initial eval scores for this setup were similar to the ones I got on the public leaderboard, so I persisted with this setup.On plotting the feature importances using the default set of features, I realized that the MA features were not contributing much. Also, to me using the absolute values of these features was not intuitive. Removing these gave me an improvement in the eval score as well as the public leaderboard score. Then I removed the volume traded feature because it was also having a low contribution and removing it gave me an improvement in both eval and public lb. Later, I created 3 new features:I tried creating a feature for differnce between three day moving average of nth day minus the three day moving average of (n-1)th day. This gave me improvement in eval dataset, but not on the public lb. Possibily this had overfit the data, so I removed this feature.max depthI usually start with shallow trees (max depth 3). I prefer to use shallow trees because they dont tend to overfit. I tried increasing the max depth to 4 and 5, but that made the scores worse for public lb. So I stuck to using max depth 3.min_child_weightInitially, I set the min_child_weight to 1000 because of the high number of data points. Later I moved it to 1500 and 500 and saw that 500 gave me a better score. Decreasing further to 300 didnt help so I stuck with 500.Learning Rate, num_rounds and early stoppingI set up the early stopping parameter to 50, i.e. if the eval score doesnt improve in 50 rounds, stop training further. The learning rate was initially set to 0.05 and num rounds were initially set to 1500. But this was very slow and the score was improving even after 1500 rounds. So I changed the learning rate to 0.2 and reduced the num rounds to 800. This gave me stopping near the 600th round and quicker training as a result.Well, thats it, I did not have the time to try ensemble models which I believe could have improved the score further.Solution:Code File",https://www.analyticsvidhya.com/blog/2017/04/winners-approach-of-rampaging-datahulk-minihack/
MGR Analytics Msbi Big Data OR Data Visualization- Hyderabad (7-12 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Winners Approach  Rampaging DataHulk MiniHack, AV DataFest 2017|Data Analytics- Hyderabad (1-2 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  12 years
Requirements : 
Task Info : Job Description and ResponsibilitiesPosition Overview:The purpose of these positions is to perform and improve business reporting and analytics for UHC CIO Business Leadership. The team will be working closely with operations and PM organization to provide business insights, financial reporting and MBO Dashboard refresh and result publication. The team will help PDS Leadership to achieve Innovation solution in reporting, process improvement and analytics across the portfolio of program.Job Responsibility Individual will be responsible for analyzing large amounts of data to produce meaningful insight including Provider Performance Reports for UHC Business Owners Build weekly / monthly dashboards by pulling data from different data sources for Senior Leadership team:. Weekly Status Reporting metrics Weekly Status Report Publication and Distribution Month End UHC IS Timesheet Compliance Validation, Remediation & Support. Month End UHC IS PPM Optics Financial Reporting Monthly Forecast Submission to UHC Finance Weekly Tableau Refresh and Dashboard Publishing Data Quality Management, Validation, and Remediation Supports operations reporting accuracy. Monthly MBO Dashboard Refresh and Results Publication, Monthly PM Analysis to validate PM load across UHC IS and OT. Process, Data, and Environment Documentation, Extra development support as capacity allows. Tools Excel, Tableau, BDPAAS (Datameer)Basic requirement Good communication skills Experience in BI and Data management 7 to 10 years Should have hands-on experience in BI tools MS- BI / (SQL) Should have hands-on experience in Big Data / Datameer expertise Should have hands-on experience in Tableau Desktop/Server MS Excel, Visio, PowerPoint, and Project Expertise SharePoint Designer
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, excel, hadoop, hive, mapreduce, pig, sql, tableau
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/mgr-analytics-msbi-big-data-or-data-visualization-hyderabad-7-12-years-of-experience/
Data Analytics- Hyderabad (1-2 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|MGR Analytics Msbi Big Data OR Data Visualization- Hyderabad (7-12 Years Of Experience)|Engagement Manager, CPG Analytics- Mumbai (8-10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : 
Task Info : Job Description and ResponsibilitiesRequirements : Should have worked on data heavy projects on risk scoring, propensity / classification modeling, segmentation, promotion analytics, campaign optimization, recommender engines, fraud analytics, etc. Strong hands on programming experience in SAS / Tableau / R / Knime and knows the best coding practices. Great analytical skills, with expertise in Machine Learning methods such as supervised and unsupervised learning, SVM, Neural Networks, Nave Bayes Classifier, Decision Trees, PCA etc. is MUST. Able to create / develop a compelling story to answer business questions using analytics outcomes. Must possess good client facing experience with the ability to facilitate requirements sessions and lead teams and present analysis in business terms  Exposure to Big Data technologies Hadoop, Hive, PIG, MapReduce, SparkML, Python, Splunk will be given priority.
College Preference : no-bar
Min Qualification : ug
Skills : classification, decision trees, Fraud Analytics, hadoop, machine learning, mapreduce, naive bayes, neural network, python, r, sas, segmentation, splunk, supervised learning, tableau, unsupervised learning
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/data-analytics-hyderabad-1-2-years-of-experience/
"Engagement Manager, CPG Analytics- Mumbai (8-10 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analytics- Hyderabad (1-2 Years Of Experience)|Moving beyond frontiers in Data Science  Interview with Mahesh Kumar, Founder & CEO, Tiger Analytics|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 9  10 years
Requirements : 
Task Info : Job Description and ResponsibilitiesResponsibilities: Understand clients business questions and develop solution architecture.Make sure that all project deliverables are completed on schedule and in accordance with clients needs and expectations.Lead client engagements and assignments to ensure appropriate staffing, resource utilization, and quality assurance.Capability Development: Work with the team to help build cutting edge solutions.Provide guidance to junior analysts for solving problems, developing analytical strategies, and helping them apply analytical techniques and frameworks.Ensure that all deliverables meet client expectations and all compliance and regulatory requirementsIdentify risks in engagements and develop the mitigation plan.Work with the teams to identify and execute productivity opportunitiesWork closely with the sales team to help acquire new clients and opportunities.Define standard metrics and KPIs for clients and establish processes to measure themRecruit, train, develop and coach talent.Generate insights and present findings to client that meets business requirements.Build and sustain a high performing teamQualification & Experience:8+ years of experience in FMCG/CPG domain into business consulting with an emphasis on Business Insights and solutioningKnowledge of variety analytic and research approaches, including (but not limited to): linear regression, time series, logistic regression, visualizations, databasesAbility to independently manage research engagements from start to finish, delivering actionable insight within established timelines and budgetExperience in customer insights research or analytics is a strong plusExperience working in a consultative capacity (internal or external) is a strong plusStrong analytical thinking skills. Ability to creatively solve business problems, innovating new approaches where required.Ability to interpret research findings for multiple audiences, extract insights and communicate effectivelyExperience interfacing with clients or high-level executivesOutstanding oral and written communication skillsMust have excellent project management skills and have experience managing multiple work streams and projects at one timeMust have the ability to adapt to changing business priorities in a fast-paced business environmentNeeds to be a thought leader and question and push boundariesEducation: MBA or advanced degree in a quantitative discipline is required
College Preference : no-bar
Min Qualification : ug
Skills : database, linear regression, logistic regression, machine learning, time series, visualization
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/engagement-manager-cpg-analytics-mumbai-8-10-years-of-experience/
"Moving beyond frontiers in Data Science  Interview with Mahesh Kumar, Founder & CEO, Tiger Analytics",Learn everything about Analytics|Introduction,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Engagement Manager, CPG Analytics- Mumbai (8-10 Years Of Experience)|Natural Language Processing Made Easy  using SpaCy (in Python)|
Kunal Jain
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This DataFest, we are bringing thoughtleaders & influencers from industry as part ofour interview series Moving Beyond Frontiers in Data Science section. We are featuring founders of top data science & analytics startups.These leadershave challenged the status quo to redefine the industry.We have done these exclusive interviews to understand their journey and learn from their perspective. Heres the first interviewof the month of  Mahesh Kumar, Founder & CEO, Tiger Analytics.Mahesh has 14 years of experience in predictive analytics, statistical modeling and machine learning with applications in the fields of retail management, digital marketing, e-commerce, customer analytics, transportation, energy planning, and healthcare.Mahesh was an Assistant Professor at R.H Smith School of Business before starting Tiger Analytics. He holds a PhD in Operation Research from Massachusetts Institute of Technology.Read on to know about his journey & story of setting up Tiger Analytics.Kunal:Hi Mahesh, thanks for finding out time for this interview. You started Tiger Analytics after teaching Operation Research at R. H. Smith School of Business. What gap did you discover in analytics industry which persuaded you to start Tiger Analytics?Mahesh: When I was teaching data analytics, I used to consult for several clients for their analytics needs. In the process, I realized that there were lots of players in BI & Reporting, but very few focused on advanced analytics  especially being able to use data science to address complex business problems. Hence I started Tiger Analytics with an emphasis of creating an organization that uses advanced analytics focused on delivering tangible business outcomesKunal: How did you meet your co-founder and the initial team?Mahesh: I connected with Pradeep Gulipalli, my co-founder, during the early days, while looking for someone with complementary skills and the passion for building a firm truly specializing in advanced analytics. Between us we covered functional expertise, business operations, revenue generation, global delivery etc.To build the initial team, Pradeep and I primarily used our professional networks to identify personnel with data science and advanced quantitative skills. Today, we are a team of 150+ data scientists and data engineers, solving problems using advanced analytics.Pradeep Gulipalli & Mahesh Kumar (left to right)Kunal: What were the challenges you faced during the initial months of Tiger Analytics?Mahesh:There were several challenges ranging from sales to recruitment to infrastructure. When we started, the only way to convince potential clients to work with us was through our individual resumes. Every sales discussion would be like a business, technical, and data science interview rolled into one. Today, as a company, we have built credibility as a premier data science solutions and services firm.Similar challenges existed when bringing together a team. To join us, one had to believe in our vision to build the worlds best advanced analytics firm, when we even did not have an office. Today, we have a top-notch data science community and attract top talent.Kunal: Tell us about your first success story?Mahesh:One of our initial projects was with a leading North American railroad company. One of their biggest expenses was in replacing damaged wheels. To identify them, they had sensors on the railway tracks, which streamed data when wheels pass over them. The problem was that, at times the sensors would be damaged, because of which they would provide wrong readings, resulting in the replacement of perfectly healthy wheels. This resulted in significant avoidable costs.We built a warning system that would analyze the sensor data to predict if a sensor was damaged and needed inspection. This project was a huge success and today our algorithms process sensor data from railway tracks all over the US and Canada. There are lots of similar interesting stories featured on our website and I would encourage readers to go through them to get a feel of our work.Kunal: What differentiates you from your competitors?Mahesh: Our ability to deliver quality data science and advanced analytics solutions is one of the biggest factors for multiple Fortune 500 companies from a wide range of industries to work with us.Internally, we have a significant emphasis on culture and work-life balance and strive to ensure that people are happy to work with us. The atmosphere is informal and authentic, and very professional. Additionally, opportunities to solve cutting-edge big data analytics problems keeps our team motivated and excited.We are building a company that is focused on quality first  in terms of work, customers, and employees  and are not in a haste to scale rapidly, as it could cause the quality of work to suffer. We will scale but in a gradual manner.Kunal: Did you expect such massive growth & success in a short span of time?Mahesh:The response so far has been phenomenal. We have worked with 50+ clients in a span of 5 years: many of them are marquee names from the Fortune 500 roster, and almost all of them are repeat customers. Our business has been expanding at a healthy pace.In 5 short years, we have established a niche for ourselves in the data science space as a dependable partner for organizations across the globe. We have been ranked by Deloitte as among the Top 10 fastest growing tech firms in India in 2016.Kunal: What are the top priorities as the CEO of Tiger Analytics?Mahesh:My top 2 priorities would be:Kunal: You have a presence in multiple locations  how is the set up in each of these countries?Mahesh:We started in the San Francisco Bay Area and have our primary delivery center in Chennai, India. Our teams work out of both these offices as well as from customer locations globally.Kunal: What are the different analytics solutions offered at Tiger Analytics?Mahesh:We offer custom services and solutions in the areas of Marketing Science, Customer Analytics, Operations & Planning, and Risk Analytics. The services span Data Science, Big Data Engineering, and Business Analytics.We have developed a portfolio of proprietary process accelerators and solution blueprints that underpin our project execution.Kunal: The awareness for analytics is increasing among students & professional. Do you think the candidates are well-prepared for the role today?Mahesh: We find that business analytics courses including platforms such as Analytics Vidhya, do a commendable job in equipping aspirants with the tools required for a career in analytics. However, we see some very frequently occurring gaps  the primary one being of first understanding the business problem and data before beginning a modeling exercise.Kunal: What are the skill sets you look for in a candidate while hiring? And what is the selection process?Mahesh:Depending on the role which we hire for, we look for programming, quantitative, and/or business aptitude. For someone starting their career, deep understanding of data science is not expected, but that becomes the expectation for a relatively senior role.More than the above, culture-fit and coachability of a candidate is very important to us. In a field that is evolving rapidly, one should be able to unlearn and relearn.Kunal: What impact do you think analytics & big data will have in the next 5 years?Mahesh:We are seeing a significant increase in the volume, variety & velocity of data, decrease in data storage costs, increase in computing power, and availability of new methods and tools. Most crucially, with an increase in business adoption, the scope of big data analytics has increased manifold.The emergence of positions such as Chief Analytics Officer and Chief Data Officer indicates how seriously analytics is now being viewed at the highest level. Enterprises are understanding the need to think data-first as it is increasingly playing a critical role in determining business direction.We anticipate that analytics will see significant growth in the next 5 years.
Kunal: Any advice for students and working professionals?Mahesh: Analytics is a very attractive career option today. Whether you are a student or a professional, if you are someone who wants to solve problems, work with data, learn new math concepts and technologies, then you should seriously consider analytics as a career option.There is significant innovation happening in the field and new application areas are emerging. In the future, we will see several professional opportunities that dont exist today. Its exciting to be a part of this new wave!Kunal: Thanks Mahesh for your time and thoughts. Im sure a lot of people will be inspired by your story and will be motivated to join the analytics industry.Check out the activities coming up in AV DataFest 2017",https://www.analyticsvidhya.com/blog/2017/04/interview-with-mahesh-kumar-ceo-tiger-analytics/
Natural Language Processing Made Easy  using SpaCy (in Python),Learn everything about Analytics|Introduction|Table of Content|1. About spaCy and Installation|2. SpaCy Pipeline and Properties|3. Word to Vectors Integration|4. Machine Learning with text using Spacy|5. Comparison with other libraries|||Projects|End Notes,"1.1 About|1.2 Installation|2.1 Tokenization|2.2 Part of Speech Tagging|2.3 Entity Detection|2.4 Dependency Parsing|2.5 Noun Phrases|Feature Availability|Speed: Key Functionalities  Tokenizer, Tagging, Parsing|Accuracy: Entity Extraction|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Moving beyond frontiers in Data Science  Interview with Mahesh Kumar, Founder & CEO, Tiger Analytics|Manager- Wholesale Impairment Analytics- Bangalore (5-10 Years Of Experience)|
Shivam Bansal
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing is one of the principal areas of Artificial Intelligence. NLP plays a critical role in many intelligent applications such as automated chat bots, article summarizers, multi-lingual translation and opinion identification from data. Every industry which exploits NLP to make sense of unstructured text data, not just demands accuracy, but also swiftness in obtaining results.Natural Language Processing is a capacious field, some of the tasks in nlp are  text classification, entity detection, machine translation, question answering, and concept identification. In one of my last article, I discussed various tools and components that are used in the implementation of NLP. Most of the components discussed in the article were described using venerated library  NLTK  (Natural Language Toolkit).In this article, I will share my notes on one of the powerful and advanced libraries used to implement nlp  spaCy.Spacy is written in cython language, (C extension of Python designed to give C like performance to the python program). Hence is a quite fast library. spaCy provides a concise API to access its methods and properties governed by trained machine (and deep) learning models.Spacy, its data, and its models can be easily installed using python package index and setup tools. Use the following command to install spacy in your machine:In case of Python3, replace pip with pip3 in the above command.OR download the source from here and run the following command, after unzipping:To download all the data and models, run the following command, after the installation:You are now all set to explore and use spacy.Implementation of spacy and access to different properties is initiated by creating pipelines. A pipeline is created by loading the models. There are different type of models provided in the package which contains the information about language  vocabularies, trained vectors, syntaxes and entities.We will load the default model which is english-core-web.The object nlp is used to create documents, access linguistic annotations and different nlp properties. Lets create a document by loading a text data in our pipeline. I am using reviews of a hotel obtained from tripadvisors website. The data file can be downloaded here.The document is now part of spacy.english models class and is associated with a number of properties. The properties of a document (or tokens) can listed by using following command:This outputs a wide range of document properties such as  tokens, tokens reference index, part of speech tags, entities, vectors, sentiment, vocabulary etc. Lets explore some of these properties.Every spaCy document is tokenized into sentences and further into tokens which can be accessed by iterating the document:Part-of-speech tags are the properties of the word that are defined by the usage of the word in the grammatically correct sentence. These tags can be used as the text features in information filtering, statistical models, and rule based parsing.Lets check all the pos tags of our documentLets explore some top unigrams of the document. I have created a basic preprocessing and text cleaning function.Spacy consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as  person, location, organization, dates, numerals, etc. These entities can be accessed through .ents property.Lets find all the types of named entities from present in our document.One of the most powerful feature of spacy is the extremely fast and accurate syntactic dependency parser which can be accessed via lightweight API. The parser can also be used for sentence boundary detection and phrase chunking. The relations can be accessed by the properties .children , .root, .ancestor etc.Lets parse the dependency tree of all the sentences which contains the term hotel and check what are the adjectival tokens used for hotel. I have created a custom function that parses a dependency tree and extracts relevant pos tag.Dependency trees can also be used to generate noun phrases:Spacy also provides inbuilt integration of dense, real valued vectors representing distributional similarity information. It uses GloVe  vectors to generate vectors. GloVe is an unsupervised learning algorithm for obtaining vector representations for words.Lets create some word vectors and perform some interesting operations.Integrating spacy in machine learning model is pretty easy and straightforward. Lets build a custom text classifier using sklearn. We will create a sklearn pipeline with following components: cleaner, tokenizer, vectorizer, classifier. For tokenizer and vectorizer we will built our own custom modules using spacy.Lets now create a custom tokenizer function using spacy parser and some basic cleaning. One thing to note here is that, the text features can be replaced with word vectors (especially beneficial in deep learning models)We are now ready to create the pipeline, load the data (sample here), and run the classifier model.Spacy is very powerful and industrial strength package for almost all natural language processing tasks. If you are wondering why?Lets compare Spacy with other famous tools to implement nlp in python  CoreNLP and NLTK.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:In this article we discussed about Spacy  a complete package to implement NLP tasks in python. We went through various examples showcasing the usefulness of spacy, its speed and accuracy. Finally we compared the package with other famous nlp libraries  corenlp and nltk.Once the concepts described in this article are understood, one can implement (really) challenging problems exploiting text data and natural language processing.I hope you enjoyed reading this article, feel free to post your doubts, questions or any thoughts in the comments section.",https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%e2%80%8bin-python/
Manager- Wholesale Impairment Analytics- Bangalore (5-10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Natural Language Processing Made Easy  using SpaCy (in Python)|Senior Business Analyst  Fraud Analytics  BFSI Domain- Delhi NCR, Gurgaon (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  10 years
Requirements : 
Task Info : Job Description and ResponsibilitiesWorking with Group Risk Analytics teams to understand credit risk methodologysCoordinating activities with Stress Testing, Sovereign Risk & Forward Economic Guidance TeamsBeing able to manage all levels of stakeholdersClose collaboration with other global team will be essential to be successful in the role.To be successful in the team, you should meet the following requirements:Have an understanding of risk analytics and credit risk modellingProgramming experience (SAS, SQL)Comfortable to perform data mining activities (for ECL driver analysis)Review Group/regional/business demands for methodology/policy changes and assess impactsProvide business performance benchmark reportsReview credit mitigation measures applied to reduce ECL
College Preference : no-bar
Min Qualification : ug
Skills : data mining, risk modeling, sas, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/manager-wholesale-impairment-analytics-bangalore-5-10-years-of-experience/
"Senior Business Analyst  Fraud Analytics  BFSI Domain- Delhi NCR, Gurgaon (3-5 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager- Wholesale Impairment Analytics- Bangalore (5-10 Years Of Experience)|Analyst/senior Analyst  Analytics & Research- Bangalore (1-4 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Job Description and Responsibilities The candidate should have analytics experience at-least 2+ years in BFSI domain with sufficient predictive modeling experience Must have a strong knowledge of SAS/R and SQL and good in Excel & MIS reporting.Responsibilities : Contribute to how analytical approach is structured for specification of analysis Participate in design of analysis and modeling approach with senior managers and the client Drive project methodology with relative ease Emphasis is on ownership and executing end-to-end delivery of analysis Extremely comfortable working with data, including managing large number of data sources, analyzing data quality and pro-actively working with clients data/ IT teams to resolve issues Use variety of analytical tools (SAS, CART, SPSS etc.) and techniques (regression, logistic, GLM, decision trees, machine learning, artificial intelligence etc.) to carry out analysis and drive conclusions Reformulate highly technical information into concise, understandable terms for presentationsKeyskillsSAS R Predictive Modeling Data Analysis Business Analysis Excel Fraud Analytics SQL Machine Learning SPSSDesired Candidate Profile Masters or Bachelors degree in engineering, math, statistics, economics or related field from top-tier universities with a strong record of achievement, solid analytical ability, and an entrepreneurial hands-on approach to work 3+ years of consulting, analytics delivery experience and overall 3+ years of experience Very strong analytical skills with the demonstrated ability to research and make decisions based on the day-to-day and complex customer problems required, overall 3+ Yrs of experience Basic knowledge of financial instruments  Options, Forwards, Futures etc. CFA/FRM are not mandatory but an added advantage Excellent communication skills  Ability to present analysis and findings to the senior leadership of the client Strong in SAS/R, SQL, Excel and VBAEducation-UG: B.A  Economics, Maths, Statistics, B.Tech/B.E.  Any SpecializationPG:M.A  Maths, Statistics, Economics, M.Tech  Any SpecializationDoctorate:Doctorate Not Required
College Preference : no-bar
Min Qualification : ug
Skills : bfsi, excel, predictive modeling, r, sas, sql, VBA
Location : Delhi, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/senior-business-analyst-fraud-analytics-bfsi-domain-delhi-ncr-gurgaon-3-5-years-of-experience/
Analyst/senior Analyst  Analytics & Research- Bangalore (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst  Fraud Analytics  BFSI Domain- Delhi NCR, Gurgaon (3-5 Years Of Experience)|Data Analyst-Bangalore (0-2 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : Job Description and ResponsibilitiesBusiness Analytics & Research (BA&R), a part of FMR India, is a consulting and insights function that enables business leaders at company Investments to make informed decisions. The team uses a combination of business and investment research, advanced analytics and decision sciences to solve business problems at the operational, tactical and strategic level. The group has a high focus on leveraging big data and technology to facilitate critical decisions and is in the process of setting-up a first of its kind big data lab in India. The group houses top talent with over three-fourths of the associates having a post graduate degree or PhD.The team supports decision making through a combination of external market intelligence based on economy/industry/sector/company analysis and Data Science & Big Data Analytics that focuses on analyzing a wide variety of internal, syndicated and big data using advanced mathematical models.The group provides unique exposure to the financial services industry spanning Workplace Investing, Retail Investing, Institutional Investing, Asset Management and Private Equity.What do we expect from you?Learning Quotient: Having a continuous learning focus around math, science, technology/tools and business domain, taking efforts in increasing knowledge through self-learning and exploration utilizing the repertoire of learning resources available at disposalCommunication: Strong communication skills demonstrated through confident communication and Q&A with various stakeholders including 1:1 interactions with offshore partnersBusiness Understanding: Energy to quickly build a basic understanding of overall Fidelity business and outcome drivers for financial services industryProject Management: Time management and operational discipline with an approach to problem solving in a consultative and structured manner.Self-Development: Demonstrate high degree of self-review and evaluation as well as seeking peer / manager feedback for continuous improvementHow will your typical day look like?Checking emails / having interactions with colleagues to catch up on work delivered and new requests receivedSynthesize structured and unstructured communication with partners to build work planFrame an approach using your understanding of Fidelity business, data and relevant questions at hand. One day it might be an easier solve with you working alone, on another day it may require digging around meeting other team members and collaborating to form an approach and execute.On some days, you will make time to attend knowledge sharing sessions on projects pertaining to your direct work area or what other team members are doing across different competencies and business verticals. You can pick up a thread from these interactions and then see how it can be applied to your projects working with other experts from other teams. Eventually, you will be on the giving end sharing your expertise with others.The day will stretch into evening with you talking to the onshore team, presenting insights from work you performed during the day and addressing concerns, if anyWhat we offer in return?Getting to see how analytics is applied to solve for real business problems at one of the largest financial services company in the world. You get to see and question where the problem emanated from and where your work was applied to make a differenceHigh respect for individuals and their talent in an extended team model where the offshore team is seen as a part of the larger global analytics teamConfluence of different analytics disciplines in one place working to solve problems across different business verticalsWorking with top notch talent with great depth in research and analytics experience combined with domain and business understanding to make their efforts stickQualificationsMasters in statistics/ economics/ management/ operations research from a reputed university1- 4 years of overall analytics experience
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business analytics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/04/analystsenior-analyst-analytics-research-bangalore-1-4-years-of-experience/
Data Analyst-Bangalore (0-2 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst/senior Analyst  Analytics & Research- Bangalore (1-4 Years Of Experience)|Data Scientist- Bangalore (5-10 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  2 years
Requirements : 
Task Info : Job Description and ResponsibilitiesDive deep into data and emerge with actionable nuggets of informationthereby helping CSPs to monetize their vast reserves of data. Extract, analyze,correlate, model, interpret and transform data into business insights. Employdescriptive, diagnostic, predictive and prescriptive techniques to derivevalue. Mentor and guide team members from technical and business perspective.Desired Skills One or more of thefollowing: Working proficiency ofmachine learning techniques like Bayesian, Decision Trees, Neural Networks,Ensemble etc. Knowledge of advancedstatistical methods including multivariate statistical methods, discrete choicemodeling, etc. Working proficiency in atleast one data mining tool (SAS, SPSS, R, RapidMiner, etc.) Strong proficiency in SQLand working proficiency in at least one programming language/scripting (R,Python, BASH script, Pl/SQL) Strong logical andquantitative skills with affinity for mathematics Self-driven, exploratorymindset and comfortable with detail Business acumen Good communication skills Open to travel/relocateto all worldwide locationsQualification/Experience Bachelors with majors inIT/CS or MBA with analytics bentGraduateswith majors in statistics/mathematics will have an additional advantage
College Preference : no-bar
Min Qualification : ug
Skills : decision trees, machine learning, neural network, python, r, sas, spss, sql, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-analyst-bangalore-0-2-years-of-experience/
Data Scientist- Bangalore (5-10 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst-Bangalore (0-2 Years Of experience)|Senior Analytics Position-New Delhi (5-10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  10 years
Requirements : 
Task Info : 
College Preference : no-bar
Min Qualification : ug
Skills : clustering, data mining, decision trees, java, logistic regression, machine learning, matlab, multivariate analysis, nlp, octave, python, r, statistical modeling, text mining
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-scientist-bangalore-5-10-years-of-experience/
Senior Analytics Position-New Delhi (5-10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Bangalore (5-10 Years of Experience)|Principal Data Scientist- Bangalore (10- 20 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  10 years
Requirements : 
Task Info : Job Description and Responsibilities Use of statistical techniqueslike segmentation, clustering, predictive modeling, forecasting etc Deriving valuable businessinsights from the large amount of data Engages well with clients anddesigns solution for their business problems Team Handling experience ispreferable Strong knowledge ofSAS/SQL/R/Python Excellent communication skills(oral and written) Strong interpersonal skills and ability tointeract well with internal and external stakeholders Open to extensive travel andeven relocation for a project for few months (if needed) Base location preferences can be discussed
College Preference : no-bar
Min Qualification : ug
Skills : clustering, forecasting, predictive modeling, python, r, sas, segmentation, sql, statistical techniques
Location : New Delhi
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-analytics-position-new-delhi-5-10-years-of-experience/
Principal Data Scientist- Bangalore (10- 20 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analytics Position-New Delhi (5-10 Years Of Experience)|Senior Business Analyst  Business Intelligence- Bangalore (6-8 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  20 years
Requirements : 1. leading a team of data scientists to create world class products
2. Bringing about significant innovation and solving complex problems in projects based on analytics
3. Typically 7 or more years of experience executing on projects as a lead and 10 or more years of analytic computing experience.
Task Info : Work on state-of-the art technologies and build things that have never existed in the past.This profile is a mix of: building Machine Learning solutions implementing complex data processing flows to crunch data and produce insights leading a team of data scientists to create world class products Bringing about significant innovation and solving complex problems in projects based on analytics Partner with Product team to make data driven decisions that will eventually help us to deliver seamless-Evaluating impact of software performance, and recommending changes to software design team. Design experiments for A/B testing and define best practices and design diagnostics to monitor performance for online experimentsDesired qualifications PhD in Computer Science from a premier institute MTech/MS in Computer Science. Typically 7 or more years of experience executing on projects as a lead and 10 or more years of analytic computing experience. In depth knowledge of data science principles and best practices Programming experience in Scala, Java or Python Experience with Apache Spark platform. Experience with big data technologies and databases like Redshift ,Elasticsearch, and Hadoop Hands-on with Data Handling  data acquisition, data transformation, and data cleaning. Expertise in Big Data with experience to handle and work with terabytes of data Familiarity with modern machine learning methods for regression and classification. Knowledge of experimental design, A/B testing and general statistical modeling. Leveraging NLP techniques and familiarity with AWS preferred
College Preference : no-bar
Min Qualification : pg
Skills : apache spark, aws, bigdata, data science, etl, hadoop, java, machine learning, nlp, python, Scala
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/principal-data-scientist-bangalore-10-20-years-of-experience/
Senior Business Analyst  Business Intelligence- Bangalore (6-8 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Principal Data Scientist- Bangalore (10- 20 years of experience)|Statistical Analyst- Pune (8+ Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  8 years
Requirements : 
Task Info : Job Description and ResponsibilitiesThe role will be responsible for supporting Global teams working with colleagues and stakeholders who are spread across different GEOs such as Americas, EMEA, and Asia Pacific. The position requires the Business Analyst to be a self-starter with good project / program management, analytical, and training skills who is comfortable working with distributed teams, at all levels of the organization without direct management oversight.Responsibilities & ScopeDeliver reporting utilizing our BI tools (including SAP BOBJ and Tableau) to successfully transition and transform current state reporting to improved analytic capabilities, effectively drive and improve the self-service model in the businessDevelop expertise on business processes, life-cycles, and data drivers to recommend BI solutions and drive requirement discussionsCollaborate on the definition of KPIs and metrics to monitor and run the businessPrototype reports and dashboards to get users engaged and excited about BI solutionsCreate in-depth business requirements across a variety of business functions, such as Finance, Sales, Professional Services, Global Support, Marketing, HR, and LegalPerform current state to future state detailed impact analysis activities to provide input to BI data migration strategy and recommended end state solutionPartner closely with IT teams on functional requirements and design to drive successful end state solutions and user experiencesPerform QA activities to validate requirements are being met through front end solutions, data model verification, and source to target mapping.Support communication, training, and documentation to enable successful user adoption and extended team member readinessTake up relevant Industry certifications, for example CBIP certification on Data Warehousing, Information Systems, Data Analysis and Design, Data IntegrationRequired Skills And ExperienceMinimum 6-8 years of experience in Business Analysis, Requirements Development, and BI tool solutions implementationProficient in SQL and build dashboards using BI tools like Tableau, OBIEE or BOBJ.Good data analysis skills required to investigate data issues in reports and dashboards to find the root cause and provide solutions.Good understanding of DWH concepts and BI project life cycle.Proficient in MS Office (e.g. Word, PowerPoint, Excel)Understanding of the primary business functions in a Software organization, such as Sales, Marketing, Professional Services, Renewals, Customer Advocacy, and Global Support. Implementation of Business Intelligence solutions for these areasClear, professional, and concise communications (written/verbal), presentation and facilitation skills.Must have combination of good communication skills (including presentation skills) and technical skillsMust be able to use consultative approach and persuasive communication with stakeholdersAbility to take initiative and to set priorities independentlyStrong organizational skills and attention to detailBachelors degree in technology or equivalent required
College Preference : no-bar
Min Qualification : open
Skills : business analysis, business intelligence, excel, sql, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-business-analyst-business-intelligence-bangalore-6-8-years-of-experience/
Statistical Analyst- Pune (8+ Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Business Analyst  Business Intelligence- Bangalore (6-8 Years of Experience )|Statistical Programmer- Mumbai (3+ Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  years
Requirements : 
Task Info : Job Description and ResponsibilitiesThe Red Hat Sales Reporting and Analysis team is looking for a Statistical Analyst to join us in Pune, India. In this role, you will manage thorough and ongoing data validation, consolidation, cleansing, and maintenance, and youll prepare subsequent data reports and analytics. Youll also reference and reconcile data across different systems like Salesforce, financial and commissions system, Oracle, etc. As a Statistical Analyst, well need you to be able to turn analysis into data tools that Red Hat can use.Primary Job ResponsibilitiesTurn business problems into analytical projects in cooperation with Sales and Services Operations teamsParticipate in the planning and strategy of key business projects, using data to inform about decisionsTranslate abstract data into a highly-visual and comprehensible formatsDevelop machine learning, data mining, and statistical techniques and algorithms to create new, scalable solutions for business problems by analyzing large data setsDesign test experiments that focus on improving customer experience; review, analyze, and share results for process optimization purposesManage, analyze, and report key performance metrics through manual and automated data collection methods; ensure quality checks, context review, and associate revisionsIdentify opportunities and areas of sub-optimization, and work cross-functionally to design and implement solutionsParticipate in various structured and ad hoc planning, analysis, data statistics, and modelling projectsDevelop an understanding of Red Hat workflows and policiesRequired SkillsBachelors or masters degree in data science, analytics, statistics, computer science, or math8+ years of industrial experience in statistical analysis or data mining with industry standard tools like Business Objects (BO), SAS, and SPSS5+ years of experience in R, Python (Scikit-Learn), or another statistical modeling packages or environmentExcellent data manipulation skills, specifically with tools like SQL, or PandasExperience in development of data products using Shiny, Python, or Flask, and basic webdevelopment using HTML, CSS, Java Script, and D3Ability to work under pressure; willingness to work extended hours during quarter-end or year-end periodsExperience with data visualization tools like SAS Visual Analytics, QlikView, or TableauExperience with Platform-as-a-Service (PaaS) offerings like OpenShift, CloudFoundry, or HerokuAbility to understand and transform data to meet the customers needs and business contextExcellent communication skills, with the ability to focus on delivering resultsKnowledge of Hive and Pig are a plusExperience in relational database systems and data warehousing systems like SQL Server, Redshift, Oracle, or MySQL is an advantage
College Preference : no-bar
Min Qualification : ug
Skills : data mining, HTML5, java script, oracle, pandas, python, qlikview, r, sas, spss, sql, statistics, tableau
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/statistical-analyst-pune-8-years-of-experience/
Statistical Programmer- Mumbai (3+ Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Statistical Analyst- Pune (8+ Years of Experience )|AV DataFest 2017  The Panel discussion, Knowledge Intensive Webinars and Prize details!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info :      Job Description and ResponsibilitiesProvide advanced technical expertise as part of a team to develop andmaintain clinical systems to meet internal and external client needs. Plan andcoordinate database design and development for local, regional, ortransnational use. Provide leadership to the Clinical Data Programmingdepartment in the area of technical expertise.RESPONSIBILITIES
Good Clinical Domain knowledge understanding clinical phases and Workexpr must be in clinical domain (PhaseI-IV).
Create Data transfer specificationby referring the eCRF, Annotation of eCRF, knowledge on Protocol.Create SAS datasets of clinical data from raw data; create status andefficacy datasets; create project specific macros and formats; load client datafrom other platforms; prepare data to be sent to clients.Create listings and Custom reports as per customer requirement.Knowledge on Vendor Import programmerStrong Technical knowledge on SAS programming, CDISC SDTM standards, SemiSDTM (Sponsor based standards).SAS /BASE, SAS/Advance SAS, SAS/ Macros, SAS/SQL & SAS/SAS Gridknowledge is required.Data listings, in accordance with approved statistical analysis plan andshell displays for clinical research studies.Code is to be written using Base SAS programs, SAS procedures, orstandardized macros. Perform data checks as needed to understand structure andcontent of data.Coordinate the different project stake holder on project deliverables.Create and maintain Standard QC documentation and maintain the accuracy.Good Communication skillSKILLS AND ABILITIES
Excellent knowledge or Clinical SAS Programming.
Solid understanding of clinical drug development processExcellent organizational, communication, and technical database skillsAbilityto establish and maintain effective working relationships with coworkers,managers and clients
College Preference : no-bar
Min Qualification : ug
Skills : sas, sql
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/statistical-programmer-mumbai-3-years-of-experience/
"AV DataFest 2017  The Panel discussion, Knowledge Intensive Webinars and Prize details!",Learn everything about Analytics|Introduction|Let the Party begin Launch Party &Panel Discussion|The Knowledge Intensive Webinars|AV DataFest 2017 Prize Money & Leaderboard|How are the points calculated?|AV DataFest 2017 Leaderboard|Make sure you compete in all!,"Webinar : Use of Analytics in Telecom Industry|Webinar: Use of Analytics in Healthcare and Pharma|Share this:|Like this:|Related Articles|Statistical Programmer- Mumbai (3+ Years of experience)|Measuring Audience Sentiments about Movies using Twitter and Text Analytics|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If something is important to you, you will try it even when the odds are against you. Elon MuskAs we rush and make finishing touches to the contests and events during AV DataFest 2017, I cant help recall our journey till this point. From a blog being run from a small room to one of the largest community of Analytics and Data Science  the journey has been phenomenal. At times, the impact of Analytics Vidhya overwhelms us.We feel proud every time we come across some one who says how Analytics Vidhya has helped them learn Data Science and Analytics. We feel the energy in our volunteers and see the excitement in their eyes. We believe that we provide an unbiased, top quality content to our community and the knowledge seekers across the globe.We have seen an idea become reality, A reality turn into an obsession. An obsessionthat how do we provide the best learning platform to our followers. DataFest is just a celebration of this journey. It is a celebration to thank all the supporting hands which have helped us build Analytics Vidhya. It is a celebration to tell the world that passion trumps all the hurdles an unconventional startup might face!Date: 1 April 2017As we toast to 4 spectacular years of Analytics Vidhya on 1 April, we want to provide you a remarkable experience throughout the month of April. And what better way than start this event with a thought provoking discussion with top data scientists, leaders and academicians. We are conducting a Panel Discussion on 1 April at 8 PM (IST). The agenda for the panel discussion is Best Ways to Learn Data Science: Training Courses Vs Projects.The panelists are:1. Sudalai Rajkumar, Lead Data Scientist, FreshDesk2. Bishwarup Bhattacharjee, Senior Data Scientist, VMware3. Shekhar Jadhav,Lead Solutions Architecture -Big Data & Advanced Analytics, IBM4.Bappaditya Mukhopadhyay, Professor, Economics and Finance, Great Lakes Institute of ManagementYour host for the evening would be none other than me Join us for an intriguing discussion and find out what is their take on it. You will be able to post your questions to the panelists. We will pick up the best questions and present it to the panel.We are bringing together leaders, data science evangelists & academicians from the industry to share their experience & expertise in their respective fields.Following is just a glimpse of the webinars coming your way:Date: 5 April 2017This webinar is being conducted by Amit Kumar, Director of Analytics at Positive Integers. Amit has a rich experience of 12+ years in analytics & data science. He has previously worked with Aviva, GE & Vodafone to name a few. Amit has extensive knowledge in use of analytics to solve several problems faced by telecom companies and he will discuss the same in this webinar.Date: 25 April 2017This webinar is being conducted by Satish Patil, Founder & Chief Data Scientist at Lemoxo Technologies. Satish has 10+ years of experience in the field of drug discovery and development. Find out how healthcare & pharma industry is reaching new heights by using analytics.In addition to these, there are other webinars coming up from various experts in the industry. These includeAjay LavakareAngel Investor,Durga GadirajuFounder of IT Versity,Dipyaman SanyalProgram Head and Faculty at Bridge School of Management,Vatsal Agarwal, Subject Matter Expert at Innoplexus Consulting ServicesThese details would be coming up soon on DataFest page, so stay tuned!A lot of you have been asking about the points for the overall ranking. Here are the details you all have been waiting for. The overall ranking on the DataFest leaderboard will be based oncumulative scores of all the competitions.The table below summarizes the maximum number of points available for various competitions:For each event, points are calculated using the followingformula:where,Using the formula above, individual points for each competition are calculated.To amplify your competitive streak, we have introduced a special DATAFEST leaderboard. The ranking on this leaderboard is decided by the cumulative sum of all the points of all events.This means to top the DATAFEST leaderboard you would have to maintain a relative score across all events. These points would decide the top 3 winners of DATAFEST.These 3 winners will be the ultimate champions of DATAFEST and will take away additional cash prizes of INR 1,80,000 (~$2750).This is your chance to prove your mettle among the top data scientists from all over the world.Participate in all the above events and be prepared for loads of fun. Let us know if you have any doubts or confusion. Make sure yousubscribe for DATAFEST for all the updates & notifications.If you have questions post them in the comments sections below. See you at the battle ground for Datavengers!",https://www.analyticsvidhya.com/blog/2017/03/datafest-panel-discussion-webinars-event-point-calculator/
Measuring Audience Sentiments about Movies using Twitter and Text Analytics,Learn everything about Analytics|Introduction|Table of Contents|1. Introduction to Text Analytics|2. Objective of the Analysis|3. Data for the Analysis|4. Conducting the analysis Approach 1 using the tm package|5. Conducting the analysis  Approach 2 using the syuzhet package|6. Conclusions|7. Making a Prediction Algorithm for Movie Success,"Positive Words|Negative Sentiments|End Notes|About the Author|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|AV DataFest 2017  The Panel discussion, Knowledge Intensive Webinars and Prize details!|Extracting information from reports using Regular Expressions Library in Python|
Guest Blog
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The practice of using analytics to measure movies success is not a new phenomenon. Most of these predictive models are based on structured data with input variables such as Cost of Production, Genre of the Movie, Actor, Director, Production House, Marketing expenditure, no of distribution platforms, etc.However, with the advent of social media platforms, young demographics, digital media and the increasing adoption of platforms like Twitter, Facebook, etc to express views and opinions. Social Media has become a potent tool to measure Audience Sentiments about a movie.This report is an attempt to understand one such platform, i.e., Twitter. The movie chosen is Rangoon, which is a 2017 Bollywood movie, directed by Vishal Bhardwaj and produced by Sajid Nadiadwala and Viacom 18 Motion Pictures. The lead actors areSaif Ali Khan, Shahid Kapoor and Kangana Ranaut. The film was released on 24 February 2017 in India on a weekend.I will be using R, an open source statistical programming tool, to carry out the analysis.Note: I will focus on the approach and the findings. The R Code to carry out the analysis can be found at the end of the article.Before moving ahead with the analysis, its only relevant to ask the question.What is Text Analytics?.In simple words, it is the process of converting unstructured text data into meaningful insights to measure customer opinion, product reviews, sentiment analysis, customer feedback, etc.Text Analytics is completely different from the traditional approach, as the latter works primarily on structured data. On the other hand, texts (such as tweets) are loosely structured, have poor spelling, often contain grammar errors, and they are multilingual.This makes the process much more challenging and interesting!There are two common methodologies for Text Mining Sentiment Parsing and Bag of Words.Sentiment Parsing emphasises on the structure and grammer of words. Whereas Bag of words disregards the grammer and word type, instead focusing on representing text (a sentence, tweet, document) as the bag (multiset) of words.Any analytics project should have a well defined objective. In our case, the goal is to us eText Aalytics on Twitter data to gauge audience sentiments about the movie Rangoon.To start with, we need to have the data. Fortunately, in case of twitter, its not that difficult. Twitter data is publicly available and one can collect it through scraping the website or by using a special interface for programmers that Twitter provides, called an API.I have used the twitteR package in R to collect 10,000 tweets/retweets about Rangoon. The movie released on 24th of February 2017, and the tweets were extracted on 25th Feb. The tweets were then saved as the csv file, and read into R using the readr package. The process of obtaining tweets from Twitter is beyond the scope of the article.The tm package is a framework for text mining applications within R. It works on arguably the most widely used Text Mining Bag of Words Principle. Bag of Words approach of handling texts is very simple. It just counts the frequency of each word appearance in the text and uses these counts as the independent variables. This simple approach is often very effective and its used as a baseline in text analytics projects and for natural language processing.The main steps are outlined below.Step 1: Lets load the required libraries for the analysis and then extract the tweets from the file rangoon.Step 2: Data PreprocessingPreprocessing the text can dramatically improve the performance of the Bag of Words method (or for that matter, any method)The first step towards doing this is Creating a Corpus, which in simple terms, is nothing but a collection of text documents. Once the Corpus is created, we are ready for preprocessing.First, let us remove Punctuations. The basic approach to deal with this is to remove everything that isnt a standard number or letter. It should be borne in mind that sometimes punctuations can be really useful, like web addresses, where the punctuation often defines the web address. Therefore, the removal of punctuation should be tailored to the specific problem. In our case, we will remove all punctuations.Next, we change the case of the word to lowercase so that same words are not counted as different because of lower or upper case.Another preprocessing task we have to do is to remove unhelpful terms. Many words are frequently used but are only meaningful in a sentence. These are called stop words. Examples are the, is, at, and which. Its unlikely that these words will improve our ability to understand sentiments, so we want to remove them to reduce the size of the data.Another important preprocessing step is stemming, motivated by the desire to represent words with different endings as the same word. For example, there is hardly any distinction between love,loved, and loving and all these could be represented by a common stem, lov. The algorithmic process of performing this reduction is called stemming.Once we have preprocessed our data, were now ready to extract the word frequencies to be used in our prediction problem. The tm package provides a function called DocumentTermMatrix that generates a matrix where the rows correspond to documents, in our case tweets, and the columns correspond to words in those tweets. The values in the matrix are the counts of how many times that word appeared in each document.Lets go ahead and generate this matrix and call it dtm_up.Step 3: Calculating SentimentNow its time to get into the world of sentiment scoring. This is done in R using the calculate_sentiment function. This function loads text and calculates sentiment of each sentence. The syntax is that it takes text as arguments and outputs a vector containing sentiment of each sentence as value.Lets run the code and movie have been received well by the audience.We see that the ratio of positive to negative words is 5780/3238 = 1.8 which prima facie indicates that the movie has been received well by the audience.Lets deep dive into the positive and negative sentiments separately to understand it better.The table below shows the frequency of words in our text classified as positive. The same has been generated using the data table function.The words love, best and brilliant are the three top positive words in terms of frequency.Lets now visualise the positive words using a word cloud function. Word clouds can be a visually appealing way to display the most frequent words in a body of text. How it works is that a word cloud arranges the most common words and uses size to indicate the frequency of a word. Lets make word cloud of positive terms.Word Cloud of Positive WordsThe word cloud also shows that love is the most frequent positive term used in the tweets.Lets repeat the same exercise for negative words. We see that words like miss, dismal, hell, etc are the most frequent words with negative connotations. Lets visualize this with word cloud.Word Cloud of Negative WordsWe see the most frequently used negative words are miss, dismal and hell.Word of Caution: while doing text analytics, it is important to have some prior understanding of the subject. For instance, negative words like bloody or hell might be representing the famous song bloody hell of the movie. Similarly, miss might represent the title and in the case of Rangoon, one of the leading characters screen name is Miss Julia, portrayed by Kangana Ranaut. So it can be tricky to consider miss as a negative word.Once we have accounted for these possible anamloies, we can make further adjustments in our analysis. For instance, we have earlier calculated that the ratio of positive to negative words is 5780/3238 = 1.8. Of these 3238 negative word count, let us not consider 144 counts of the word hell, and relook at the ratio. We see that the ratio increases to 5780/3094 = 1.87.This was the first approach of gauging audience opinion aboout the movie Rangoon.It seems that positive emotions are more than the negative ones. We can further check this using another method of polarity discussed below.The syuzhet Package extracts sentiments from text using three sentiment dictionaries. The difference between this and the above approach is that this approach is based on a much wider range of sentiments. The first step,as always, is to prepare the data for text analytics. This will include cleaning html links,Post processing, we will use the get_nrc_sentiment function to extract sentiments from the tweets. Howthis function works is that it Calls the NRC sentiment dictionary to calculate the presence of different emotions and their corresponding valence in a text file.The output is a data frame where each row represents a sentence from the original file. The columns include one for each emotion type as well as a positive or negative valence. The ten columns are as follows: anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive.Lets create a visual how our movie performs as per the emotions.Looking at the bar chart and the sum of these emotions, we can see that the positive sentiments (positive, joy, trust) comfortably outscore the negative emotions (negative, disgust, anger). This may be a hint that may be the audience has recieved the movie positively.Both the approaches seem to suggest that the movie Rangoon has been well received by the audiences, as measured by the PT/NT ratio (Positive Tweet to Negative Tweet) as well as by the visual representation of various emotions.This article has focused on gauging audience sentiments expressed about the movie Rangoon via tweets.However, this may not be an accurate yardstick to predict box office success.We all know that many critically acclaimed movies falter at the box office and many dont-bring-your-brains-tothe-theater movies become an astounding success.So whats the solution?The solution is to analyze the historical records of how the PT/NT ratio has translated into box office collections for the similar genre of movies and create well-trained and validated predictive models on the data. This model then can be used to predict the box office success of the movie. In case of Rangoon, he PT/NT ratio of 1.87 will be the input value.Since that is beyond the scope of this article, we will not be covering it here, but its important to highlight that text analytics can also be used as an alternative to measuring box office success.In this article, we have seen how we can use twitter analytics to not just analyze sentiments but also to predict box office revenues.It should be noted that the analysis will vary depending on when the tweets were extracted  pre or post movie release. It is very much possible that the same analysis done on tweets from different time frame can yield different results. Also, different steps of preprocessing can alter the results.The objective of this article was not to conclude the success or failure of the movie Rangoon, but solely on steps and thought process behind doing text analytics on twitter reviews of the movie.There may be more advanced methods to do the similar analysis, but have confined to these two approaches as found them simple and intuitive.Feel free to connect to discuss any suggestions or observations on the article. Also, if there are other more effective methods, please feel free to share them as well.By Analytics Vidhya Team:This article was contributed by Vikash Singh and is the first rank holder of Blogathon 3.Vikash Singh Decision Scientist who specializesin integrating Strategy with Data Science for effective decision making and solving Business Problems. An MBA in International Business from Banaras Hindu University, he has also completed one-year certificate program on Business Analytics from IIM, Calcutta. Outside of work, Vikash loves learning new things in analytics, follow his favorite sports and play cricket.",https://www.analyticsvidhya.com/blog/2017/03/measuring-audience-sentiments-about-movies-using-twitter-and-text-analytics/
Extracting information from reports using Regular Expressions Library in Python,Learn everything about Analytics|Introduction|Basic syntax|Use Cases|End Note,"Finding email|Finding telephone number|Finding date|Finding account /credit card number|Adding linked Information|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Measuring Audience Sentiments about Movies using Twitter and Text Analytics|TensorFlow 101: Understanding Tensors and Graphs to get you started in Deep Learning|
Yogesh Kulkarni
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Many times it is necessary to extract key information from reports, articles, papers, etc. For example names of companies  prices from financial reports, names of judges  jurisdiction from court judgments, account numbers from customer complaints, etc.These extractions are part of Text Mining and are essential in converting unstructured data to a structured form which are later used for applying analytics/machine learning.Such entity extraction uses approaches like lookup, rules and statistical/machine learning. In lookup based approaches, words from input documents are searched against pre-defined data dictionary. In rules based approach, pattern searches are made to find key information. Whereas in statistical approachsupervised-unsupervised methods are used to extract the information.Regular expression (RegEx) is one of the rules based pattern search method.Python supports regular expressions by the library called re(though its not fully Perl-compatible). Instead of regular strings, search patterns are specified using raw strings r, so that backslashes and meta characters are not interpreted by python but sent to RegEx directly.Go through the following table of basic syntax.Go through the following Python sample code for usage of RegEx.Instead of re.search, which returns all the exact matches, re.findall() can be used to return all captured groups. re.sub is used to substitute another pattern as a replacement for the given search pattern. For performance reasons, it is recommended to compile the pattern first using re.compile and then use the RegEx object for searching, as shown below.More information about RegEx usage in Python can be found at Regex One and in this AV article.Imagine writing code for searching telephone numbers like +91-9890251406 in a document, with multiple variations in format. With validations, the code will typically be surely more than 10 lines (sample here). But with RegEx, its just about 2/3 of lines of code, and with high customizability.Following are some of the frequently occurring scenarios where RegEx can offer substantial help. Please note that the examples shown could have alternate ways of getting same results, especially by using meta characters such as /d for [0-9] representing digits. In most of the examples, expressive and simplistic patterns are used here just for clarity and understandability.The first two cases differ only in -or . and thus can combined using (-|\.)A sample code for more elaborate phone number is as follows:The date pattern used above are only numeric. There are other usages such as 27-Mar-1973 or 27 March 1973. I would leave this as an open quiz and would want the panelist to think about their RegEx patterns!!Citations in search papers or judgments have pre-defined formats and they refer to an external document. It is possible to append the hyperlink information by replacing the citation text.For example, US legal judgments citation looks like 17 U.S.C.  107. The pattern is : text U.S.C., another space, a  mark, another space, a set of numbers, and optionally, a year inside a parenthetical. It can be replaced with a <a href /a> hyper link to actual judgment it refers to.Tools for development, testing and debuggingAlthough RegEx is powerful but it can get complicated for non-trivial tasks. More challenging it would be if you must understand (and debug) RegEx by someone else!There are quite a few friendly utilities which help in development and testing of RegEx. Try Regex 101. It gives facility to put your own text and try RegEx pattern.Here is one sample text to try on Phone number, account number, date patterns, mentioned before.9890251406 1.4142135623 01101001 27/3/1973 987-01-666101110011 202.555.9355 00100000 01101001 91-020-25898963912025898963 3.1415926535897932384626433832795 666-12-4895 01100001 202-555-9355 27-03-1973 0010000001101000 (555) 867-5309 27-Mar-1973 2.718281828459 555-867-5309 01100101 01110011 01110011 555/867-5309Sites like RegExper give visual representation of the RegEx search pattern for better understanding. Refer to the below visualization for PhoneRegex search pattern mentioned earlier, used for phone numbers.Once RegEx gives acceptable matches, the pattern can be used in programs. After good enough practice one can directly code the search patterns in the program itself.All the RegEx patterns used here, with some minor modifications, can be used in programming language like Python, Perl, Java, etc. It can also be used in some of the popular text editors for Find-Replace functionality, like Microsoft Word (keep Use Wildcard option ON), OpenOffice and in IDEs like PyCharm. Read the comprehensive information about RegEx here.RegEx is a versatile, portable and powerful way of extracting key information from textual data. Mastery over it can help automate many mundane tasks. Although, at times, it can get complicated and hard to develop-debug but owing to its immense capabilities it has become a must weapon in every programmers armour, especially for text analytics data scientists.Let me conclude by giving a food for thought: Can RegEx be used to solve a crossword puzzle?Drop your answers below. If you have questions feel free to post them in the comments section.By Analytics Vidhya Team: This article was contributed by Yogesh Kulkarni who is the second rank holder of Blogathon 3.",https://www.analyticsvidhya.com/blog/2017/03/extracting-information-from-reports-using-regular-expressons-library-in-python/
TensorFlow 101: Understanding Tensors and Graphs to get you started in Deep Learning,Learn everything about Analytics|Introduction|What are Tensors?|Why we need Tensors in TensorFlow?|What are Graphs?,"Variables|End Notes|References|About the Author|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Extracting information from reports using Regular Expressions Library in Python|Business Analyst- Mumbai (2-5 Years of Experience)|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"TensorFlow is one of the most popular libraries in Deep Learning. When I started with TensorFlow it felt like an alien language. But after attending couple of sessions in TensorFlow, I got the hang of it. I found the topic so interesting that I delved further into it.While reading about TensorFlow, I understood one thing. In order to understand TensorFlow one needs to understand Tensors and Graphs. These are two basic things Google tried to incorporate in its Deep Learning framework.In this article, I have explained the basics of Tensors & Graphs to help you better understand TensorFlow.As per the wiki definition ofTensors:Tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps. Geometric vectors, often used in physics and engineering applications, and scalars themselves are also tensors.As the definition goes, Deep Learning wantsus to think that Tensors as Multidimensional Arrays.In a recent talk by one of my colleagues, he was required to show the difference between a Neural Network made in NumPy and Tensors. While creating the material for the talk, he observed that NumPy and Tensors take almost the same time to run (with different optimizers).We both banged our headache over it in order to prove TensorFlow is better but we couldnt. This kept disturbing me and I decided to delve further into it.Now, we need tounderstand Tensors and NumPy first.As per the NumPy official website, it says:NumPy can also be used as an efficient multidimensional container of generic data. Arbitrary datatypes can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.After reading this Im sure the same question must have popped in your head as in mine. Whats the difference betweenTensors and NDimensional Arrays?As per Stackexchange, Tensor : Multidimensional array :: Linear transformation : Matrix.The above expression means tensors and multidimensional arrays are different types of object. The first is a type of function, the second is a data structure suitable for representing a tensor in a coordinate system.Mathematically, tensors are defined as a multilinear function. A multi-linear function consists of various vector variables.A tensor field is a tensor valued function. For a rigorous mathematical explanation you can read here.Which meanstensors are functions or containers which we need to define. The actual calculation happens when theres data fed. What we see as NumPy arrays (1D, 2D, , ND) can be considered as generic tensors.I hope now you would have some understanding of what are Tensors.Now, the big questions is why we need to deal with Tensorsin Tensorflow. The big revelation is what NumPy lacks is creating Tensors.We can convert tensors to NumPy and viceversa. That is possible since the constructs are defined definitely as arrays/matrices.I could get a few answers reading and searching for Tensors and NumPy arrays. For more reading, theres no better resources than the official documentations.Theanos meta-programming structure seems to be an inspiration for Google to create Tensorflow, but folks at Google took it to a next level.According to the official Tensorflow blog on Getting Started.A computational graph is a series of TensorFlow operations arranged into a graph of nodes.Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally.I think the above statement holds true as we have seen that constructing a computational graph to multiply two values is rather a straight forward task. But we need the value at the end. We have defined the two constants, at and bt, along with their values. What if we dont define the values?Lets check:I guess the constant needs a value. Next step would be to find out why we didnt get any output. It seems that to evaluate the graph that we made, it needs to be run in a session.To understand this complexity, we need to understand what our computational graph has:To execute mult, the computational graph needs a session where the tensors and operations would be evaluated. Lets now evaluate our graph in a session.The above graph would print the same value since we are using constants. There are 2 more ways we could send values to the graph - Variables and Placeholders.When you train a model, you use variables to hold and update parameters. Variables are in memory buffers containing tensors. They must be explicitly initialized and can be saved to disk during and after training. You can later restore saved values to exercise or analyze the model.Variable initializers must be run explicitly before other ops in your model can run. The easiest way to do that is to add an op that runs all the variable initializers, and run that op before using the model.Read more here.We can initialize variables from another variables too. Constants cant be updated, thats a shame everywhere. Need to check whether dynamically variables can be created.We can conclude that placeholders is a way to define variables without actually defining the values to be passed to it when we create a computational graph.tf.placeholder() is the norm, used by all the Tensorflow folks writing code daily.For a more in depth reading: I/O for Tensorflow.We would check out Variables and Placeholders below.In this article, we observed the basics of Tensors and what do these do in a computational graph. The actual objective for creating this is to make Tensors flow through the graph. We write the tensors and through sessions we make them flow.I hope you enjoyed reading this article.If you have any questions or doubts feel free to post them below.1. Tensorflow  Getting Started
2. CS224d
3. MetaFlow Blog
4. Theano vs Tensorflow
5. Machine Learning with Tensorflow
6. Read about Graphs herePrathamesh Sarang works as a Data Scientist at Lemoxo Technologies. Data Engineering is his latest love, turned towards the *nix faction recently. Strong advocate of Markdown for everyoneBy Analytics Vidhya Team: This article was contributed by Pratham Sarangwho is the third rank holder of Blogathon 3.",https://www.analyticsvidhya.com/blog/2017/03/tensorflow-understanding-tensors-and-graphs/
Business Analyst- Mumbai (2-5 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|TensorFlow 101: Understanding Tensors and Graphs to get you started in Deep Learning|Business Analyst- Chennai (2-5 Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and ResponsibilitiesRole Overview: Business Analyst will be responsible for managing the analytics assignment. The role requires good blend of functional and technical know-how and the ability to connect with the clients. The role requires good learning agility on both domain and technology front.
Job Responsibilities Understand client requirements, develop project plans and coordinate with data scientists and developers on the delivery Closely engage with client teams/stake holders for requirement gathering and coordination required for data verification and extraction Engage with datascientists/developers on putting together required solutions Manage the client relationship including pre-sales, proposal creation, change request management,solution planning and testing Maintain governance andstandards compliance
Education (Required)MBA + BE/ME
Technical Skills Experience in delivery of analytics/software solutions Proven experience of managing client relationships and project management Excellent understanding of business models and at least few business domains Understanding of various technologies  databases, business intelligence tools, ETL etc Experience of leading a team is preferred Strong debugging, troubleshooting, and diagnostic skills
Personality Traits Excellent written and verbal communication skills Ability to quickly learn new concepts/technologies Good collaboration and communication skills Excellent problem solving skills Strong sense of team work,ownership, and account ability Takes initiatives and self-motivated
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, database, etl
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/business-analyst-mumbai-2-5-years-of-experience/
Business Analyst- Chennai (2-5 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Mumbai (2-5 Years of Experience)|Sr. Data Analyst- Bangalore (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and ResponsibilitiesJob Summary: We are looking for strong Business Analysts and reporting, who will generate high end reports, with meaningful insights and recommendations, for various business heads and top management; part of the work will involve analyzing large sets of data and derive some insights, using Python & Sql.Roles and ResponsibilitiesHarness the data to get meaningful insightsApplies advanced analytical skills, including a thorough understanding of how to interpret business needs and translate them into analytical and reporting deliverables, with actionable recommendations.Qualification & Experience2+ years of experience in the field of analysis and report generationStrong with programming languages like Python and data processing using SQL or equivalentExperience in Excel data manipulationGood presentation and communication skills in PPT and other MS Office productsExperience with BFSI domain is an additional plusB Tech or Post Graduate in Quantitative fields
College Preference : no-bar
Min Qualification : ug
Skills : analytics, bfsi, excel, python, sql
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/business-analyst-chennai-2-5-years-of-experience/
Sr. Data Analyst- Bangalore (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Chennai (2-5 Years of experience)|Data Scientist  Advanced Analytics- Bangalore (4-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job Description and ResponsibilitiesDive deep into data and emerge with actionable nuggets of information thereby helping CSPs to monetize their vast reserves of data. Extract, analyze, correlate, model, interpret and transform data into business insights. Employ descriptive, diagnostic, predictive and prescriptive techniques to derive value. Mentor and guide team members from technical and business perspective.Desired Skills One or more of the following: Working proficiency of machine learning techniques like Bayesian, Decision Trees, Neural Networks, Ensemble etc.  Knowledge of advanced statistical methods including multivariate statistical methods, discrete choice modeling, etc.  Working proficiency in at least one data mining tool (SAS, SPSS, R, RapidMiner, etc.) Strong proficiency in SQL and working proficiency in at least one programming language/scripting (R, Python, BASH script, Pl/SQL) Strong logical and quantitative skills with affinity for mathematics Self-driven, exploratory mindset and comfortable with detail Business acumen Good communication skills Open to travel/relocate to all worldwide locationsQualification/Experience Bachelors with majors in IT/CS or MBA with analytics bent Graduates with majors in statistics/mathematics will have an additional advantage
College Preference : no-bar
Min Qualification : ug
Skills : decision trees, ensemble methods, machine learning, multivariate analysis, neural network, python, r, sas, spss, sql, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/sr-data-analyst-bangalore-2-5-years-of-experience/
Data Scientist  Advanced Analytics- Bangalore (4-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Data Analyst- Bangalore (2-5 Years Of Experience)|Senior Analyst  Quantitative Research  Research & Data Analytics Symmetrical- Delhi/ NCR (3-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 
Task Info : Job Description and ResponsibilitiesSkills: R Python Qlikview SQL / Teradata / Hive / Spark Regression / Multinomial regression / Logistic Regression Multinomial logit Integrated forecasting using GLM / Mix Models Optimization using Simulated Annealing / Genetic Algorithms Different types of simulations and simulations software Cluster AnalysisExperience: 5-8 years, with atleast 3 years in advanced analytics doing complex models CPG experience preferred
College Preference : no-bar
Min Qualification : ug
Skills : hive, logistic regression, python, qlikview, r, regression, sql, teradata
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-scientist-advanced-analytics-bangalore-4-8-years-of-experience/
Senior Analyst  Quantitative Research  Research & Data Analytics Symmetrical- Delhi/ NCR (3-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist  Advanced Analytics- Bangalore (4-8 Years Of Experience)|ACN  DIGITAL  ANALYTICS  CUSTOMER ANALYTICS ANALYST- Chennai (1-3 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : Job Description and ResponsibilitiesWe are looking for ""Senior Analyst- Quantitative Research"" for Noida Location. Our client is a Multinational Research & Data Analytics company in India.Job Description :-Knowledge of various financial asset classes (Fixed Income, Credit, Equity, FX etc..)Conduct exhaustive research and understand the research in terms of the direction and usability towards the projectAnalyze data using various statistical and financial methods like using Regression, Time series analysis, PCA etc.Must be independent enough to look for solutions to problems, but keep detailed records of what assumptions and steps were taken, and be able to communicate the logic in a clear and concise manner.Ability to analyze model output and comment on the valuesTechnical Skills :Intermediate Excel Skills, coding in VBA, SQL, Exposure to databases like Bloomberg, Thomson ReutersWork Experience3-4 years relevant experience in Quantitative Research/Financial MarketExposure in researching/implementing quantitative models, valuation and identifying investment strategiesKnowledge of the working of financial markets, different asset classes (FX, Rates, Equity, Credit), macro economic factors influences and trading techniquesKnowledge of various statistical techniques to analyze data, such as Regression, PCA etc.Experience in coding
College Preference : no-bar
Min Qualification : ug
Skills : database, excel, regression, sql, statistical techniques, VBA
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-analyst-quantitative-research-research-data-analytics-symmetrical-delhi-ncr-3-6-years-of-experience/
ACN  DIGITAL  ANALYTICS  CUSTOMER ANALYTICS ANALYST- Chennai (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst  Quantitative Research  Research & Data Analytics Symmetrical- Delhi/ NCR (3-6 Years Of Experience)|Beginners Guide on Web Scraping in R (using rvest) with hands-on example|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : Key ResponsibilitiesUtilize state of the art Machine learning and optimization algorithms for targeting customers to increase profitability, acquire new customers and increase retention. Propose and apply new algorithms for the sameDemonstrated analytical expertise, including the ability to synthesize complex data, effectively manage complex analyses, technical understanding of system capabilities and constraintsDevelop methodologies to support Customer Analytical project execution for CMT / Telecom clientsDevelop predictive analytics based solutions for o Customer Segmentation o Statistical Models across customer Lifecycle o Attrition / Cross-Sell / Upsell Propensity Models o Customer Lifetime Value o Pricing Analytics o Web AnalyticsApply appropriate techniques, such as exploratory data analysis, regression, bootstrapping, trees, cluster analysis, survival analysis and so onDevelop and articulate strategic recommendations based on rigorous data analysisPartner with client teams to understand business problems and marketing strategiesKnowledge and skill requirements1-3 years of analytics overall experience, including at least 1 year of quantitative analysis in the CMT/Telecom Industry Exposure to Machine Learning with at least 1 year of practical experience in one or more approaches such as Random Forest, Neural Networks, Support Vector Machines, Gradient Boosting, Bayesian Networks, Deep Learning etc.Hands on experience in Predictive analytics projects involving statistical modeling, customer segmentation etc.Post Graduate degree in Statistics, Data Mining, Econometrics, Applied Mathematics, Computer Science or related field or MBA (Preferred)Experience of working with US/ overseas markets is preferable SET YOURSELF APART: Key CompetenciesProficiency in two or more of analytical tools such as SAS product suite (Base Stats, E-miner, SAS EGRC), SPSS, SQL, KXEN and any other statistical tools such as R, MATLAB etc. (SAS/R are must) Good knowledge of one of more programming language such as Python, Java, C++ is a plusAdvanced Excel including VBA and PowerPoint skills Willingness to be flexible and work on traditional techniques as per business need Consulting skills and project management experience is preferred Excellent communication and interpersonal skills as well as collaborative, team player Ability to tie analytic solutions to business/industry value and outcomesAutonomous, self-starter with a passion for analytics and problem solving Must have skills: Hands on experience in Predictive analytics projects involving statistical modeling, customer segmentation etc. Good to have skills: Exposure to Machine Learning with at least 1 year of practical experience in one or more approaches such as Random Forest, Neural Networks, Support Vector Machines, Gradient Boosting, Bayesian Networks, Deep Learning etc. ""
College Preference : no-bar
Min Qualification : pg
Skills : data mining, deep learning, excel, java, machine learning, neural network, predictive modeling, python, r, random forest, sas, segmentation, spss, sql, statistical modeling, Support vector machine, VBA
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/acn-digital-analytics-customer-analytics-analyst-chennai-1-3-years-of-experience/
Beginners Guide on Web Scraping in R (using rvest) with hands-on example,"Learn everything about Analytics|Introduction|Table of Contents|1. What is Web Scraping?|2. Why do we need Web Scraping?|3. Ways to scrape data|4. Pre-requisites|4. Scraping a webpage using R|6. Analyzing scraped data from the web|End Notes|Learn, compete, hack and get hired","Share this:|Related Articles|ACN  DIGITAL  ANALYTICS  CUSTOMER ANALYTICS ANALYST- Chennai (1-3 Years Of Experience)|Analytics Manager  R/python  Iit/nit/iiit/nsit/bits- Delhi, NCR, Noida (5-8 Years Of Experience)|
Saurav Kaushik
|47 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data and information on the web isgrowing exponentially. All of us today use Google as our first source of knowledge  be it about finding reviews about a place to understanding a new term. All this information is available on the web already.With the amount of data available over the web, it opens newhorizons of possibility for a Data Scientist. I strongly believe web scraping is a must have skill for any data scientist. In todays world, all the data that you need is already available on theinternet  the only thing limiting you from using it is the ability to access it. With the help of this article, you will be able to overcome that barrier as well.Most of the data available over the webis not readily available. It is present in an unstructured format (HTML format) and is not downloadable. Therefore, it requires knowledge & expertise to use this data to eventually build a useful model.In this article, I am going to take you through the process of web scraping in R. With this article, you will gain expertise to use any type of data available over the internet.Web scraping is a technique for converting the data present in unstructured format (HTML tags) over the web to the structured format which can easily be accessed and used.Almost all the main languages provide ways for performing web scraping. In this article, well use R for scraping the data for the most popular feature films of 2016 from the IMDb website.Well get a number of features for each of the 100 popular feature films released in 2016. Also, well look at the most common problems that one might face while scraping data from the internet because of the lack of consistency in the website code and look at how to solve these problems.If you are more comfortable using Python, Ill recommend you to go through this guide for getting started with web scraping using Python.I am sure the first questions that must have popped in your headtill now is Why do we need web scraping? As I stated before, the possibilities with web scraping are immense.To provide you with hands-on knowledge, we are going to scrape data from IMDB. Some other possible applications that you can use web scraping for are:There are several ways of scraping data from theweb. Some of the popular ways are:Well use the DOM parsing approach during the course of this article. And rely on the CSS selectors of the webpage for finding the relevant fields which contain the desired information. But before we begin there are a few prerequisites that one need in order to proficiently scrape data from any website.The prerequisites for performing web scraping in R are divided into two buckets:Using this you can select the parts of any website and get the relevant tags to get access to that part by simply clicking on that part of the website. Note that, this is a way around to actually learning HTML & CSS and doing it manually. But to master the art of Web scraping, Ill highly recommend you to learn HTML & CSS in order to better understand and appreciate whats happening under the hood.Now, lets get started with scraping the IMDb website for the 100 most popular feature films released in 2016. You can access them here.Now, well be scraping the following data from this website.Heres a screenshot that contains how all these fields are arranged.Step 1: Now, we will start by scraping the Rank field. For that, well use the selector gadget to get the specific CSS selectors that encloses the rankings. You can click on the extension in your browser and select the rankings field with the cursor.Make sure that all the rankings are selected. You can select some more ranking sections in case you are not able to get all of them and you can also de-select them by clicking on the selected section to make sure that you only have those sections highlighted that you want to scrape for that go.Step 2: Once you are sure that you have made the right selections, you need to copy the corresponding CSS selector that you can view in the bottom center.Step 3: Once you know the CSS selector that contains the rankings, you can use this simple R code to get all the rankings:Step 4: Once you have the data, make sure that it looks in the desired format. I am preprocessing my data to convert it to numerical format.Step 5: Now you can clear the selector section and select all the titles. You can visually inspect that all the titles are selected. Make any required additions and deletions with the help of your curser. I have done the same here.Step 6: Again, I have the corresponding CSS selector for the titles  .lister-item-header a. I will use this selector to scrape all the titles using the following code.Step 7: In the following code, I have done the same thing for scraping  Description, Runtime, Genre, Rating, Metascore, Votes, Gross_Earning_in_Mil , Director and Actor data.But, I want you to closely follow what happens when I do the same thing for Metascore data.Step 8: The length of the metascore data is 96 while we are scraping the data for 100 movies. The reason this happened is that there are 4 movies that dont have the corresponding Metascore fields.Step 9: It is a practical situation which can arise while scraping any website. Unfortunately, if we simply add NAs to last 4 entries, it will map NA as Metascore for movies 96 to 100 while in reality, the data is missing for some other movies. After a visual inspection, I found that the Metascore is missing for movies 39, 73, 80 and 89. I have written the following function to get around this problem.Step 10: The same thing happens with the Gross variable which represents gross earnings of that movie in millions. I have use the same solution to work my way around:Step 11: Now we have successfully scraped all the 11 features for the 100 most popular feature films released in 2016. Lets combine them to create a dataframe and inspect its structure.You have now successfully scraped the IMDb website for the 100 most popular feature films released in 2016.Once you have the data, you can perform several tasks like analyzing the data, drawing inferences from it, training machine learning models over this data, etc. I have gone on to create some interesting visualization out of the data we have just scraped. Follow the visualizations and answer the questions given below. Post your answers in the comment section below.Question 1: Based on the above data, which movie from which Genre had the longest runtime?Question 2: Based on the above data, in the Runtime of 130-160 mins, which genre has the highest votes?Question 3: Based on the above data, across all genres which genre has the highest average gross earnings in runtime 100 to 120.I believe this article would have given you a complete understanding of the web scraping in R. Now, you also have a fair idea of the problems which you might come across and how you can make your way around them. As most of the data on theweb is present in an unstructured format, web scraping is a really handy skill for any data scientist.Also, you can post the answers to the above three questions in the comment section below. Did you enjoy reading this article? Do share your views with me. If you have any doubts/questionsns feel free to drop them below.",https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/
"Analytics Manager  R/python  Iit/nit/iiit/nsit/bits- Delhi, NCR, Noida (5-8 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Beginners Guide on Web Scraping in R (using rvest) with hands-on example|Manager  Data & Analytics- Hyderabad (4-5 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : 
Task Info : Job Description and ResponsibilitiesResponsibilities:Analytics Managers (AMs) design, develop, and deploy state-of-the-art, data-driven predictive models to solve business problems using the latest and most appropriate technologies in machine learning, statistical modeling and Operations Research.They deliver value in the marketplace by supporting teams serving clients; spearheading the analytics/technology component of new product development; and/or leading key research initiatives.AMs responsibilities include development of analytic solutions; pattern identification and feature extraction, variable selection; experimentation with algorithms, performance analysis and preparation for communication with internal and external clients, and training and development of junior analytics staff.Work will require effective coordination of global analytics and software development resources, clients, and business consultants. Some travel may be required to client locations.Desired Candidate ProfileRequirements:Software skills : Experience in R and Python. Deep knowledge of analytic methodologies (regression, signal processing, optimization, neural networks, etc.) Track record of analytical product development, innovation, or research (patents or publications). Industry Experience (eCommerce, Financial Services, Retail, Airlines, Defense, etc.). In addition, we prefer that candidates have experience in managing global analytics teams in multiple locations, a strong UNIX background, and experience with statistics packages/SAS.Education-UG: B.Tech/B.E.  Any SpecializationDoctorate:Doctorate Not Required
College Preference : tier1-any
Min Qualification : ug
Skills : machine learning, neural network, optimization, python, r, regression
Location : Delhi, Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/analytics-manager-rpython-iitnitiiitnsitbits-delhi-ncr-noida-5-8-years-of-experience/
Manager  Data & Analytics- Hyderabad (4-5 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Manager  R/python  Iit/nit/iiit/nsit/bits- Delhi, NCR, Noida (5-8 Years Of Experience)|Sr Analyst Data Science and Analytics- Hyderabad (6-10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  5 years
Requirements : 
Task Info : 
College Preference : tier1-any
Min Qualification : ug
Skills : database, data management, data modeling, etl, hadoop, hive, mapreduce, nosql, pig, python, qlikview, r, sas, sql, tableau
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/manager-data-analytics-hyderabad-4-5-years-of-experience/
Sr Analyst Data Science and Analytics- Hyderabad (6-10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Data & Analytics- Hyderabad (4-5 Years of Experience )|Big Data Learning Path for all Engineers and Data Scientists out there|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  10 years
Requirements : 
Task Info : Job Description and ResponsibilitiesThe Data Scientist in Procurement Analytics team will be responsible to provide quantitative analytical support to the Procurement Performance Management team. These tasks are to be performed independently or with minimal supervision. These inputs will be used to make operational decision- making within Procurement Domain especially in building category strategy.Masters/ Bachelor s Degree in any of the following:Minimum of 6 years working experience, ideally in multi- national organizations Experience in Data analysis Understanding of current trends in Big Data and Cognitive Computing Understanding or experience of building logistic regression, Machine learning algorithms like Support Vector Machines, Artificial neural networks etc. Technical expertise in analytical programming languages like R & Python and hands on experience on data mining/ statistical tools like SAS, XLMiner and MatLab Strong communications skills Fair Business acumen and knowledge of Procurement function is desirabl
College Preference : no-bar
Min Qualification : ug
Skills : analytics, data mining, logistic regression, machine learning, matlab, neural network, python, r, sas, Support vector machine
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/sr-analyst-data-science-and-analytics-hyderabad-6-10-years-of-experience/
Big Data Learning Path for all Engineers and Data Scientists out there,"Learn everything about Analytics|Introduction|Table of Content|1. How to get started?|2. What roles are up for grabs in the big data industry?|3.What is your profile and where do you fit in?|4. Mapping roles to profiles|5. How to be a big data Engineer?|6.Big Data Learning Path|7. Resources|Learn, compete, hack and get hired","4.1 Big Data Engineering roles|4.2 Big data Analytics roles|5.1 The Big Data jargon|5.1.1 Data Requirements jargon|5.1.2 Processing Requirements jargon|5.2 Systems and architecture you need to know|5.3 Learn to design solutions and technologies|5.3.1 Data related Requirements|5.3.2 Processing related Requirements|End Notes|Share this:|Like this:|Related Articles|Sr Analyst Data Science and Analytics- Hyderabad (6-10 Years Of Experience)|How I created a package in R & published it on CRAN / GitHub (and you can too)?|
Analytics Vidhya Content Team
|37 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The field of big data is quite vast and it can be a very daunting task for anyone who starts learning big data & its related technologies. The big data technologies are numerous and it can be overwhelming to decide from where to begin.This is the reason I thought of writing this article. This article provides you a guided path to start your journey to learn big data and will help you land a job in big data industry. The biggest challenge we face is identifying the right role as per our interest and skillsets.To tackle this problem, I have explained each big data role in detail and also consideringdifferent job roles of engineers and computer science graduates.I have tried to answer all your questions which you have or will encounter while learning big data.To help you choose a path according to your interest I have added a tree map which will help you identify the right path.One of the very first questions that people ask me when they want to start studying Big data is, Do I learn Hadoop, Distributed computing, Kafka, NoSQL or Spark?Well, I always have one answer: It depends on what you actually want to do.So, lets approach this problem in a methodical way. We are going to go through this learning path step by step.There are many roles in the big data industry. But broadly speaking they can be classified in two categories:These fields are interdependent but distinct.The Big data engineering revolves around the design, deployment, acquiring and maintenance (storage) of a large amount of data. The systems which Big data engineers are required to design and deploy make relevant data available to various consumer-facing and internal applications.While Big Data Analytics revolves around the concept of utilizing the large amounts of data from the systems designed by big data engineers. Big Data analytics involves analyzing trends, patterns and developing various classification, prediction & forecasting systems.Thus, in brief, Big data analytics involves advanced computations on the data. Whereas big data engineering involves the designing and deployment of systems & setups on top of which computation must be performed.Now, we know what categories of roles are available in the industry, let us try to identify which profile is suitable for you. So that, you can analyze where you may fit in the industry.Broadly, based on your educational background and industry experience we can categorize each person as follows:(This includes interests and doesnt necessarily point towards your college education).Thus, by using the above categories you can define your profile as follows:Eg 1: I am a computer science grad with no experience with fairly solid math skills.You have an interest in Computer science or Mathematics but with n o prior experience you will be considered a Fresher.Eg 2: I am a computer science grad working as a database developer.Your interest is in computer science and you are fit for a role of a Computer Engineer (data related projects).Eg 3: I am a statistician working as a data scientist.You have an interest inMathematics and fit for a role of a Data Scientist.So, go ahead and define your profile.(The profiles we define here are essential in finding your learning path in the big data industry).Now that you have defined your profile, lets go ahead and map the profiles you should target.If you have good programming skills and understand how computers interact over the internet (basics) but you have no interest in mathematics and statistics. In this case, you should go for Big data engineering roles.If you are good at programming and have your education and interest lies in mathematics & statistics, you should go for Big data Analytics roles.Let us first define what a big data Engineer needs to know and learn to be considered for a position in the industry. The first and foremost step is to first identify your needs. You cant just start studying big data withoutidentifying your needs. Otherwise, you would just be shooting in that dark.In order to define your needs, you must know the common big data jargon.So lets find out what does big data actually means?A Big data project has two main aspects  data requirements and the processing requirements.Structure: As you are aware that data can either be stored in tables or in files. If data is stored in a predefined data model (i.e has a schema) it is called structured data. And if it is stored in files and does not have a predefined model it is called unstructured data. (Types: Structured/ Unstructured)Size: With size we assess the amount of data. (Types: S/M/L/XL/XXL/Streaming)Sink Throughput: Defines at what rate data can be accepted into the system. (Types: H/M/L)Source Throughput: Defines at what rate data can be updated and transformed into the system. (Types: H/M/L)Query time: The time that a system takes to execute queries. (Types: Long/ Medium /Short)Processing time: Time required to process data (Types: Long/Medium/Short)Precision: The accuracy of data processing (Types: Exact/ Approximate) Scenario 1:Design a system for analyzing sales performance of a company by creating a data lake from multiple data sources like customer data, leads data, call center data, sales data, product data, weblogs etc.Solution for Scenario 1: Data Lake for sales data(This is my personal solution, you may come up with a more elegant solution if you do please share below.)So, how does a data engineer go about solving the problem?A point to remember is that a big data system must not only be designed to seamlessly integrate data from various sources to make it available all the time, but it must also be designed in a way to make the analysis of the data and utilization of data for developing applications easy, fast and always available (Intelligent dashboard in this case).Defining the end goal:Now that we know what our end goals are, let us try to formulate our requirements in more formal terms.Structure: Most of the data is structured and has a defined data model. But data sources like weblogs, customer interactions/call center data, image data from the sales catalog, product advertising data. Availability and requirement of image and multimedia advertising data may depend on from company to company.Conclusion: Both Structured and unstructured dataSize: L or XL (choice Hadoop)Sink throughput: HighQuality: Medium (Hadoop & Kafka)Completeness: IncompleteQuery Time: Medium to LongProcessing Time: Medium to ShortPrecision: ExactAs multiple data sources are being integrated, it is important to note that different data will enter the system at different rates. For example, the weblogs will be available in a continuous stream with a high level of granularity.Based on the above analysis of our requirements for the system we can recommend the following big data setup.Now, you have an understanding of the big data industry, the different roles and requirements from a big data practitioner. Lets look at what path you should follow to become a big data engineer.As we know the big data domain is littered with technologies. So, it is quite crucial that you learn technologies that are relevant and aligned with your big data job role. This is a bit different than any conventional domains like data science and machine learning where you start at something and endeavor to complete everything in the field.Below you will find a tree which you should traverse in order to find your own path. Even though some of the technologies in the tree are pointed to be data scientists forte but it is always good to know all the technologies till the leaf nodes if you embark on a path. The tree is derived from the lambda architectural paradigm.With the help of this tree map, you can select the path as per your interest and goals. And then you can start your journey to learn big data. Click here to download the infographic.One of the essential concepts that any engineer who wants to deploy applications must know is Bash Scripting. You must be very comfortable with linux and bash scripting. This is the essential requirement for working with big data.At the core, most of the big data technologies are written in Java or Scala. But dont worry, if you do not want to code in these languages ou can choose Python or R because most of the big data technologies now support Python and R extensively.Thus, you can start with any of the above-mentioned languages. I would recommend choosing either Python or Java.Next, you need to be familiar with working on the cloud. This is because nobody is going to take you seriously if you havent worked with big data on the cloud. Try practicing with small datasets on AWS, softlayer or any other cloud provider. Most of them have a free tier so that students can practice. You can skip this step for the time being if you like but be sure to work on the cloud before you go for any interview.Next, you need to learn about a Distributed file system. The most popular DFS out there is Hadoop distributed file system. At this stage you can also study about some NoSQL database you find relevant to your domain. The diagram below helps you in selecting a NoSQL database to learn based on the domain you are interested in.The path until now are the mandatory basics which every big data engineer must know.Now is the point that you decide whether you would like to work with data streams or dormant large volumes of data. This is the choice between two of the four Vs that are used to define big data (Volume, Velocity, Variety and Veracity).So lets say you have decided to work with data streams to develop real-time or near-realtime analysis systems. Then you should take the Kafka path. Else you take the Mapreduce path. And thus you follow the path that you create. Do note that, in the Mapreduce path you do not need to learn pig and hive. Studying only one of them is sufficient.In summary: The way to traverse the tree.Did the last step (#7) baffle you! Well truth be told, no application has only stream processing or slow velocity delayed processing of data. Thus, you technically need to be a master at executing the complete lambda architecture.Also, note that this is not the only way you can learn big data technologies. You can create your own path as you go along. But this is a path which can be used by anybody.If you want to enter the big data analytics world you could follow the same path but dont try to perfect everything.For a Data Scientist capable of working with big data you need to add a couple of machine learning pipelines to the tree below and concentrate on the machine learning pipelines more than the tree provided below. But we can discuss ML pipeline later.Add a NoSQL database of choice based on the type of data you are working with in the above tree.As you can see there are loads of NoSQL databases to choose from. So it always depends on the type of data that you would be working with.And providing a definitive answer to what type of NoSQL database you need to take into account your system requirements like latency, availability, resilience, accuracy and of course the type of data that you are dealing with.1.Bash Scripting2.Python3. Java4.Cloud5. HDFS6. Apache Zookeeper7. Apache Kafka8. SQL9. Hive10. Pig11. Apache Storm12. Apache Kinesis13. Apache Spark14. Apache Spark StreamingI hope you enjoyed reading this article. With the help of this learning path, you will be able to embark upon your journey in big data industry. I have covered most of the major concepts which you will require to land a job.If you have any doubts or questions, feel free to post them below.",https://www.analyticsvidhya.com/blog/2017/03/big-data-learning-path-for-all-engineers-and-data-scientists-out-there/
How I created a package in R & published it on CRAN / GitHub (and you can too)?,"Learn everything about Analytics|Introduction|Table of Contents|1. What is a R package?|2. Why I created my first R Package?|3. Advantages and Challenges in creating a R package|4. Pre-requisites|5. Writing your first package from scratch|6. Publishing a package|7. My experience after CRAN contribution|8.Additional tips|9. Additional Resources|End Notes|Learn, compete, hack and get hired","6.1 Publishing your package on CRAN|6.2 Publishing your package on GitHub|Share this:|Like this:|Related Articles|Big Data Learning Path for all Engineers and Data Scientists out there|Business Analytics- Bangalore (1-4 Years Of Experience)|
Saurav Kaushik
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Most popular programming languageshaveone thing in common  they are all Open source. Open source is a decentralised development model which is based on community participation. The community members contribute tothe development of the programming language and these contributions are publicly available to be accessed by anyone.Community participation is the prime reason for continuous development and innovation in these open source languages like R, C++, C#, Java, PHP, Python, Ruby, etc. For data science, R is one of the most popular language. The main reason for its popularity is continuous contribution and support from R practitioners in the data science community. These packages form the backbone of R programming language.While a lot of tutorials are being shared across on solving problems using R, the open source development gets lesser attention. For me, creating a package and giving it back to the community meant a huge thing. It was my way to starting to give back and I know this is just the start.In order to help expand the community further, I decided to write an article about the process of package creation and how to contribute a package to open source R community. Also,we are going to create a package and contribute it to the open source community.Read on!A package in R is simply a reusable R function(s) with standard and self-explanatory documentation on how to use it. Sometimes, packages come with sample data as well.There are 10,000+ packages on CRAN until today and majority of these packages have dependencyon some other R package(s). This signifies that most of the packages are built over the functionality of some other package(s).For example, a package I authored named ensembleR has main dependency on caret package along with some other packages: e1071, ipred, knitr, rmarkdown which are used for running the examples and creating vignettes.You can realise the importance of packages in R by this handy infographic with the most commonly used libraries in R:Sometime back in one of the competition on Analytics Vidhya, I was trying to ensemble varioud models. I realized that there is no easy to use open source package for ensembling in R.Thats when I decided to take this opportunity to create a simple package that will enable people to perform ensembling (stacking) using a few lines of code.Hence, I have created a package named ensembleR, which can be accessed on CRAN. This package enables people to create ensemble of several models in R. To know more about ensembling in R,read here.This package can create millions of unique ensembles (Stacked models) and give predictions using all of them within a single line of code. The current version on CRAN is the initial release of the package. You can find more details on how this package works here.The process of creating a package in R is both challenging as well as exciting, especially for the first time. I started off with learning the basic structure and process of creating a package.Once I coded the package, I learnt how to submit it on CRAN to make it available to other community members. Getting it on CRAN was the toughest part because of extensive and rigorous testing of the package, which is also responsible for maintaining the quality and consistency of packages that go on CRAN.In this article, Ill take you through the complete process of creating a package from scratch and getting it on CRAN and/ or GitHub to be made available publicly.The advantages for creating a R package are:Also, there are certain challenges also in creating a package:Before, beginning to write a package, there are few prerequisites you should be familiar with. These prerequisites are:Now lets get started with creating a simple package of our own. Within this package, well create a function to offer the functionality of predicting the stock price movement for tomorrow using simple logistic regression given the stock symbol. Simple enough. Lets begin!Here,Now check the Generate documentation with Roxygen option and put as-cran under Check Package space to simulate the CRAN package checking and testing.As you have successfully created a package in R, youll like to share it with others to let them use the functions in your package. For the process of publishing a package, there are two popular platforms: CRAN and GitHub.Getting your package on CRAN is a difficult task due to the extensive and rigorous testing carried out on the packages before they can be published on CRAN. Along with passing these tests, you need to have comprehensive vignettes describing the working of your package. These vignettes will be stored in vignettes folder that you can create in the main project directory.Once youre sure that your package is doing well against the local simulation tests and is well documented, you need to create the source package by going to Build > Build Source Package.After the source package is created, you can then submit an application to publish it on CRAN here.Generally, a much easier way to make your package public is to publish it on GitHub. The simplest way to publish your package on GitHub is to create a new repository and upload the contents of the main folder (StockPredictor in our case) to that repository. I have done the same here.Now, anyone can install and use this package using the following command:devtools::install_github(sauravkaushik8/SamplePackage)I cant express how I feel after putting a package on CRAN. The usability of the package probably means very little to the outside world, but that is irrelevant. For me, I know that I have started the journey to make my favourite tool even stronger.After making the CRAN contribution, I have realised that it has helped me in number of ways:Hopefully, you would have found thisarticle helpful in creating your first open source package in R. Based on my experience, Ill like to make a fewuseful suggestions:I believe this article would have given you a good understanding of the process involved in creating your own packages in R from scratch. By following this tutorial, by now you would have hand-ons experience in creating a package in R.Packages form the backbone of R programming language and Ill highly encourage you to contribute to the development of the R language as well.Did you enjoy reading this article? Do share your views in the comment section below. If you have any doubts / questions feel free to drop them in the comments below.",https://www.analyticsvidhya.com/blog/2017/03/create-packages-r-cran-github/
Business Analytics- Bangalore (1-4 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|How I created a package in R & published it on CRAN / GitHub (and you can too)?|Manager  Business Analytics  CE- Bengaluru (2-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : Job Description and Responsibilities""Data Analysis Statistical Knowledge MS Excel (proficiency) Working knowledge of Tools like  R- or  Python- (SQL, SAS and VBA knowledge is added advantage) Previous experience of Business Analytics is desirable Six Sigma Green Belt certified and BPO experience are added advantages Research experience is not mandate Good communications skills General day shifts 10AM- 7PMProvide Insights to decision makers to make an informed decision based on data and facts : Provide the Early Warning Indicators  Inform business leaders about the problem in the horizon Eliminate the challenge of ops managers to figure out the challenge that he/she is going to deal with To provide 3600 view of the current business and business intelligence To exploit the unexplored unstructured data in the organization To enable business to operate better, faster, efficient and accelerate Enables to optimize key business processes To drive more engaging customer experience Uncover new monetization opportunities Identify innovative /breakthrough opportunitiesDesired skill sets : Experience in Strategic & Marketing Insight & Intelligence, BI, Research, Analytics, Market & Competitive Analysis, Segmentation & Propensity ModelingSound Knowledge and hands on experience in : Data Analysis Statistical Knowledge Analytical Tools MS Excel (proficiency) Working knowledge of Tools like R, SAS, SQL, EXCEL,VBA Ability to collaboratively work with different work groups geographically spread with diverse experience Ability to work in a completely ambiguous environment  Highly Analytical, Eye for Details, Critical & Creative Thinking skills Business Acumen and understanding business strategies Proven communication, Presentation Visual Modeling, Facilitation and Elicitation Skills Previous experience of Business Analytics is desirable Personal Attributes: Positive, Self starter, Inquisitive, Imaginative, Factual, self disciplined Six Sigma Green Belt certified and BPO experience are added advantagesDesired qualifications (including certifications) : Minimum Post Graduate in Management/ Statistics/ Operations ResearchDesired years of experience : Minimum 3- 6 years in any Industry  Work experience in MNCs or reputed organizations will be an added advantage.Internal Customers, if any : Operations Team and Senior Management of the geography and client services Support functions""
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, excel, python, r, sas, sql, statistics, VBA
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/business-analytics-bangalore-1-4-years-of-experience/
Manager  Business Analytics  CE- Bengaluru (2-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analytics- Bangalore (1-4 Years Of Experience)|Hadoop Developers (Engineer)- Gurgaon (2-3 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Job Description and ResponsibilitiesBusinessAnalyst Responsibilities:Own a key function within the CE team to drive short and long termbusinessprocess improvements and strategic bets to create outstanding customer experienceAlign with the organization wide functions and senior managementCollaborate within the CE team and all functions across the org to develop, conceptualize, build, validate, grow, enhance features that drive better customer experienceDefinebusinessproblems, conduct RCA, design solutions, define key metrics and indicators for measurement and successDeeper analysis of consumer behaviors & trends, by reviewing the internal & external dataExecute identified project/initiatives and drivebusinessoutcomeStrong problem solving ability Candidate needs to exhibit structured thinking, very strongAnalytical Skills and Process Orientation.BusinessAnalysts will be dealing with a very ambiguous set of problems (e.g. improving returns experience, product quality, Supply Chain led experience).Highly Action Oriented company as a culture has a strong bias for action and speed of implementation. Abusinessanalyst will need to own projects from conceptualization to implementation and typically close (go live with results) within a quarter.Strong stakeholder management skillsBusinessAnalysts are expected to drive projects independently with senior management stakeholders across the organization. The individual needs to have strong people and stakeholder management aptitude.Strong Project Management skills to independently track multiple projects and collaboratively drive them to closure.Orientation / past background in Technology is preferred, not mandatory.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business analytics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/manager-business-analytics-ce-bengaluru-2-6-years-of-experience/
Hadoop Developers (Engineer)- Gurgaon (2-3 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Manager  Business Analytics  CE- Bengaluru (2-6 Years of Experience)|Advanced Analytics Senior Consultant- US (4-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : 
Task Info : Job Description and Responsibilities
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, database, hadoop, hive, java, mapreduce, pig
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/hadoop-developers-engineer-gurgaon-2-3-years-of-experience/
Advanced Analytics Senior Consultant- US (4-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Developers (Engineer)- Gurgaon (2-3 Years of Experience)|Assistant Manager  Data Scientist (Technology COE)- Hyderabad (5+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  7 years
Requirements : 
Task Info : We are looking for professionals with outstanding analytical capabilities to join the Strategic Analytics group within Corporate Finance; must be a self-starter and enthusiastic team member who is highly organized with excellent attention to detail. As an Advanced Analytics Senior Consultant, youll be a critical part of a specialized team that helps shape Deloittes future with new growth opportunities. What Youll DoIn this role, you will be focused on advanced analytical modeling. This will involve conducting analyses using advanced statistics and data mining techniques to enable better decision making. This includes:How Youll GrowDue to the Strategic Analytics groups specialized and focused mission, our team members have the incredible opportunity to work with senior executive partners on high impact projects across the finance organization. Furthermore, professional development and helping our people grow are two core beliefs at Deloitte. We provide access to advanced training programs and expert-taught workshops as well as mentoring and coaching to help you grow. Within Strategic Analytics and across Deloitte we nurture talent by providing supportive leadership for growth opportunities, from hands-on experience to increases in responsibility to rewarding teamwork. We believe that development has no ceiling and we offer lifetime learning opportunities for people at every level of the organization.What Youll NeedWhats Nice to Have
College Preference : no-bar
Min Qualification : ug
Skills : Data analytics, data mining, sas, spss, statistical modeling
Location : United States
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/advanced-analytics-senior-consultant-us-4-7-years-of-experience/
Assistant Manager  Data Scientist (Technology COE)- Hyderabad (5+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Advanced Analytics Senior Consultant- US (4-7 Years Of Experience)|Senior Analyst  Dashboard and Analytics  Hyderabad (1- 4+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Key job responsibilities:MD- Technology team is recruiting a Data Scientist at Senior Analyst level. The individual should be creative and forward-thinking, answering the burning research questions by diving into data to test hypotheses and evolving trends.The candidate should have strong statisticaland analytical background as she/he will be positioned, within the Technology team, as a subject matter specialist for data analytics. The individual will drive research projects exploring data analytics and visualization using diverse statistical tools/techniques. The Data Scientist should have experience evaluating relationships between multiple variables and using patterns in past data to shape predictive insights. Strong computer programming, statistical, and data visualization skills are required (preferred languages and software listed below). Findings will need to be distilled into broadly digestible formats via compelling visualizations and well written reports. The candidate will contribute to the development of externally-facing thought ware, across ideation of research topics to analyses and creation of the final reports targeted to senior executives.Strong foundation in modeling, statistics, analytics, mathematicsandComputer Sciences. Ability to understand data structures usingcomputerprograms/algorithms, explain complex dataarchitectures to preparedatafor analysis, and make sense out of dataStrong understanding of machine learning methods such ask-nearestneighbors, random forests, Naive Bayes,ensemble methods.Understanding of broad strokes and when it isappropriate to use differenttechniques is a must.Strong Knowledge of Multivariable Calculus and LinearAlgebra to buildour own implementations in house when necessary. Candidateshould be able tomake improvements in predictive performance or algorithmoptimization.Ability to design solutions for complex business problemsrelated tolarge volume of data by using NLP/ Machine Learning/ Text MiningtechniquesStrong business acumen to understand context of theresearch situationand identify dataanalytics opportunities to differentiate analysis. Guideanalytics process by framing hypothesesand stating research problem to accurately reflect the research situationPerform statistical analysis of large data sets to better understandtrends and relationships between variables to inform predictiveinsightsDemonstrate ability to creativelyemploy new research tools to analyzedata and derive unique findings for marketingdataCommunicate research findings using compelling visualizations and wellwritten reports. Design and build data visualizations using diverse setsofstructured and unstructured data. Write engaging synopsis to informmethodology,assumptions, and conclusions of theanalysisLead development of research reports that are created using crediblequalitative and quantitative methodologies and based on key insights andfindings from the studies. Liaisewiththe U.S. colleagues to understand, refine, and manage clients expectations andprioritiesReview outputs of junior colleagues and guide cross-team analysts to deploy statistical tools and techniques in their research projects. Conducttrainings for broader teams on statistical analysisPropose creative analytics ideas and encourage fellow team members to explore advanced analytic techniques, such as predictive analytics, datamining, text mining, and/or sentiment analysis.To pursue new ideas  design research proposal, buildconsensus, and formula teresearch plan.Identify internal and external information sources, build effectiveworking relationships with subject matter experts within the firm and theexternal marketplaceKeySkills:Strong computing, analytical, statistical, data visualization, andwriting skillsFamiliarity with SQL and other database tools to store, organize,retrieve, and manage data for doing analysisStrong statistical programming skills in RProficient in designing efficient and robust ETL workflowsCapable of quickly, regularly, and independently learning newtechnologies under the pressure of consistent high-profile project deliverableswith review by firm thought leadersGood familiarity with Tableau or other data visualization tools. Portfolioo previously developed analysis and visualization work products is preferredAdvanced Microsoft Office skillsStrong verbal and written communication skills. Demonstrate strong consultative and engagingpresentation skillsExperience in leading and managing teams.Ability to work well in in-person and remote team situationsExperience with data visualization tools, such as D3.js,GGplot, etc.preferredSelf-motivated and strong team player.Highly developed personal and professional ethics
College Preference : no-bar
Min Qualification : ug
Skills : algorithms, etl, Linear algebra, machine learning, r, statistical modeling, text mining
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/assistant-manager-data-scientist-technology-coe-hyderabad-5-years-of-experience/
Senior Analyst  Dashboard and Analytics  Hyderabad (1- 4+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant Manager  Data Scientist (Technology COE)- Hyderabad (5+ Years Of Experience)|40 Must know Questions to test a data scientist on Dimensionality Reduction techniques|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  4 years
Requirements : 
Task Info : QualificationsRequired:1 to 4+ years of experience with at least 1year of relevant experienceUndergraduate university graduateExperience working in a fast-paced, teamenvironmentExperience working independently on multiple simultaneousassignments or engagementsEffective interpersonal and written andverbal communication skillsLogical thinking ability and strong comprehensionAbility to be flexibleExperience in key data, analyticsvisualization tools, with certification in at least one of the following:oQuickviewoTableauoSharePointoSalesforceDatabased management experience andunderstanding of data structure and designPreferred:Deloitte experienceExperience in a professional services firm ora strong understandingExperience working or supporting teamsExperience working on a global teamUnderstanding of Cognitive Tools (e.g.Watson)Understanding of APIs
College Preference : no-bar
Min Qualification : ug
Skills : database management, qlikview, tableau
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-analyst-dashboard-and-analytics-hyderabad-1-4-years-of-experience/
40 Must know Questions to test a data scientist on Dimensionality Reduction techniques,"Learn everything about Analytics|Introduction|Overall Scores|Useful Resources||Questions & Answers|Learn, compete, hack and get hired!","End Notes|Share this:|Like this:|Related Articles|Senior Analyst  Dashboard and Analytics  Hyderabad (1- 4+ Years Of Experience)|Team Lead, Data Quality- Gurgaon, India (3+ Years Of Experience)|
Ankit Gupta
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you come across a dataset with hundreds of columns and wondered how to build a predictive model on it? Or have come across a situation where a lot of variables might be correlated? It is difficult to escape these situations while working on real life problems.Thankfully, dimensionality reduction techniques come to our rescue here.Dimensionality Reduction is an important technique in data science. It is a must have skill set for any data scientist. To test your knowledge in dimensionality reduction techniques, we are conducted this skill test. These questions include topics like Principal Component Analysis (PCA), t-SNE and LDA.Check out more challenging competitions coming up hereA total of 582 people participated in this skill test. The questions varied from theoretical to practical.If you missedtaking the test, here is your opportunity for you to find out how many questions you could have answered correctly.Read on!Below is thedistribution of scores, this will help you evaluate your performance:You can access your performance here. More than 180 people participated in the skill testand the highest score was 34.Here are a few statistics about the distribution.Overall distributionMean Score: 19.52Median Score: 20Mode Score: 19Beginners Guide To Learn Dimension Reduction TechniquesPractical Guide to Principal Component Analysis (PCA) in R & PythonComprehensive Guide on t-SNE algorithm with implementation in R & Python1) Imagine, you have 1000 input features and 1 target feature in a machine learning problem. You have to select 100 most important features based on the relationship between input features and the target features.Do you think, this is an example of dimensionality reduction?A.YesB.NoSolution: (A)2) [ True or False ] It is not necessary to have a target variable for applying dimensionality reduction algorithms. A. TRUEB. FALSESolution: (A)LDA is an example of supervised dimensionality reduction algorithm.3) I have 4 variables in the dataset such as  A, B, C & D. I have performed the following actions:Step 1: Using the above variables, I have created two more variables, namely E = A + 3 * B and F = B + 5 * C + D.Step 2: Then using only the variables E and F I have built a Random Forest model.Could the steps performed above represent a dimensionality reduction method?A. TrueB. FalseSolution: (A)Yes, Because Step 1 could be used to represent the data into 2 lower dimensions.4) Which of the following techniques would perform better for reducing dimensions of a data set?A. Removing columns which have too many missing valuesB. Removing columns which have high variance in dataC. Removing columns with dissimilar data trendsD. None of theseSolution: (A)If a columns have too many missing values, (say 99%) then we can remove such columns.5) [ True or False ] Dimensionality reduction algorithms are one of the possible ways to reduce the computation time required to build a model. A. TRUEB. FALSESolution: (A)Reducing the dimension of data will take less time to train a model.6) Which of the following algorithms cannot be used for reducing the dimensionality of data?A. t-SNEB. PCAC. LDA FalseD. None of theseSolution: (D)All of the algorithms are the example of dimensionality reduction algorithm.7) [ True or False ] PCA can be used for projecting and visualizing data in lower dimensions. A. TRUEB. FALSESolution: (A)Sometimes it is very useful to plot the data in lower dimensions. We can take the first 2 principal components and then visualize the data using scatter plot.8) The most popularly used dimensionality reduction algorithm is Principal Component Analysis (PCA). Which of the following is/are true about PCA?A. 1 and 2B. 1 and 3C. 2 and 3D. 1, 2 and 3E. 1,2 and 4F. All of the aboveSolution: (F)All options are self explanatory.9) Suppose we are using dimensionality reduction as pre-processing technique, i.e, instead of using all the features, we reduce the data to k dimensions with PCA. And then use these PCA projections as our features. Which of the following statement is correct?A. Higher k means more regularizationB. Higher k means less regularizationC. Cant SaySolution: (B)Higher k would lead to less smoothening as wewould be able to preserve more characteristics in data, hence less regularization.10) In which of the following scenarios is t-SNE better to use than PCA for dimensionality reduction while working on a local machine with minimal computational power?A. Dataset with 1 Million entries and 300 featuresB. Dataset with 100000 entries and 310 featuresC. Dataset with 10,000 entries and 8 featuresD. Dataset with 10,000 entries and 200 featuresSolution: (C)t-SNE has quadratic time and space complexity. Thus it is a very heavy algorithm in terms of system resource utilization.11) Which of the following statement is true for a t-SNE cost function?A. It is asymmetric in nature.B. It is symmetric in nature.C. It is same as the cost function for SNE.Solution: (B)Cost function of SNE is asymmetric in nature. Which makes it difficult to converge using gradient decent. A symmetric cost function is one of the major differences between SNE and t-SNE.Question 12Imagine you are dealing with text data. To represent the words you are using word embedding (Word2vec). In word embedding, you will end up with 1000 dimensions. Now, you want to reduce the dimensionality of this high dimensional data such that, similar words should have a similar meaning in nearest neighbor space.In such case, which of the following algorithm are you most likely choose?A. t-SNEB. PCAC. LDAD. None of theseSolution: (A)t-SNE stands for t-Distributed Stochastic Neighbor Embedding which consider the nearest neighbours for reducing the data.13) [True or False] t-SNE learns non-parametric mapping. A. TRUEB. FALSESolution: (A)t-SNE learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. For more information read from this link.14) Which of the following statement is correct for t-SNE and PCA?A. t-SNE is linear whereas PCA is non-linearB. t-SNE and PCA both are linearC. t-SNE and PCA both are nonlinearD. t-SNE is nonlinear whereas PCA is linearSolution: (D)Option D is correct. Read the explanation from this link15) In t-SNE algorithm, which of the following hyper parameters can be tuned?A. Number of dimensionsB. Smooth measure of effective number of neighboursC. Maximum number of iterationsD. All of the aboveSolution: (D)All of the hyper-parameters in the option can tuned.16) What is of the following statement is true about t-SNE in comparison to PCA?A. When the data is huge (in size), t-SNE may fail to produce better results.B. T-NSE always produces better result regardless of the size of the dataC. PCA always performs better than t-SNE for smaller size data.D. None of theseSolution: (A)Option A is correct17) Xi and Xj are two distinct points in the higher dimension representation, where as Yi & Yj are the representations of Xi and Xj in a lower dimension.1. The similarity of datapoint Xi to datapoint Xj is the conditional probability p (j|i) .2. The similarity of datapoint Yi to datapoint Yj is the conditional probability q (j|i) .Which of the following must be true for perfect representation of xi and xj in lower dimensional space?A. p (j|i) = 0 and q (j|i) = 1B. p (j|i) < q (j|i)C. p (j|i) = q (j|i)D. p (j|i) > q (j|i)Solution: (C)The conditional probabilities for similarity of two points must be equal because similarity between the points must remain unchanged in both higher and lower dimension for them to be perfect representations.18) Which of the following is true about LDA?
A. LDA aims to maximize the distance between class and minimize the within class distanceB. LDA aims to minimize both distance between class and distance within classC. LDA aims to minimize the distance between class and maximize the distance within classD. LDA aims to maximize both distance between class and distance within classSolution: (A)Option A is correct.19) In which of the following case LDA will fail?A. If the discriminatory information is not in the mean but in the variance of the dataB. If the discriminatory information is in the mean but not in the variance of the dataC. If the discriminatory information is in the mean and variance of the dataD. None of theseSolution: (A)OptionA is correct20) Which of the following comparison(s) are true about PCA and LDA?A. 1 and 2B. 2 and 3C. 1 and 3D. Only 3E. 1, 2 and 3Solution: (E)All of the options are correct21) What will happen when eigenvalues are roughly equal?A. PCA will perform outstandinglyB. PCA will perform badlyC. Cant SayD.None of aboveSolution: (B)When all eigen vectors are same in such case you wont be able to select the principal components because in that case all principal components are equal.22) PCA works better if there is?A. 1 and 2B. 2 and 3C. 1 and 3D. 1 ,2 and 3Solution: (C)Option C is correct23) What happens when you get features in lower dimensions using PCA?A. 1 and 3B. 1 and 4C. 2 and 3D. 2 and 4Solution: (D)When you get the features in lower dimensions then you will lose some information of data most of the times and you wont be able to interpret the lower dimension data.24) Imagine, you are given the following scatterplot between height and weight.Select the angle which will capture maximum variability along a single axis?A. ~ 0 degreeB. ~ 45 degreeC. ~ 60 degreeD. ~ 90 degreeSolution: (B)Option B has largest possible variance in data.25) Which of the following option(s) is / are true?A. 1 and 3B. 1 and 4C. 2 and 3D. 2 and 4Solution: (D)PCA is a deterministic algorithm which doesnt have parameters to initialize and it doesnt have local minima problem like most of the machine learning algorithms has.Question Context 26The below snapshot shows the scatter plot of two features (X1 and X2) with the class information (Red, Blue). You can also see the direction of PCA and LDA.26) Which of the following method would result into better class prediction? A. Building a classification algorithm with PCA (A principal component in direction of PCA)B. Building a classification algorithm with LDAC. Cant sayD. None of theseSolution: (B)If our goal is to classify these points, PCA projection does only more harm than goodthe majority of blue and red points would land overlapped on the first principal component.hence PCA would confuse the classifier.27) Which of the following options are correct, when you are applying PCA on a image dataset?A. 1 and 2B. 2 and 3C. 3 and 4D. 1 and 4Solution: (C)Option C is correct28) Under which condition SVD and PCA produce the same projection result? A. When data has zero medianB. When data has zero meanC. Both are always sameD. None of theseSolution: (B)When the data has a zero mean vector, otherwise you have to center the data first before taking SVD.Question Context 29Consider 3 data points in the 2-d space: (-1, -1), (0,0), (1,1).29) What will be the first principal component for this data?A. 1 and 2B. 3 and 4C. 1 and 3D. 2 and 4Solution: (C)The first principal component is v = [  2 /2 ,  2/ 2 ] T (you shouldnt really need to solve any SVD or eigenproblem to see this). Note that the principal component should be normalized to have unit length. (The negation v = [  2/ 2 ,   2/ 2 ] T is also correct.)30) If we project the original data points into the 1-d subspace by the principal component [  2 /2,  2 /2 ] T. What are their coordinates in the 1-d subspace?A. (  2 ), (0), ( 2)B. ( 2 ), (0), ( 2)C. (  2 ), (0), (- 2)D. (- 2 ), (0), (- 2)Solution: (A)The coordinates of three points after projection should be z1 = x T 1 v = [1, 1][  2/ 2 ,  2 /2 ] T =   2, z2 = x T 2 v = 0, z3 = x T 3 v =  2.31) For the projected data you just obtained projections ( (  2 ), (0), ( 2) ). Now if we represent them in the original 2-d space and consider them as the reconstruction of the original data points, what is the reconstruction error? Context: 29-31: A. 0%B. 10%C. 30%D. 40%Solution: (A)The reconstruction error is 0, since all three points are perfectly located on the direction of the first principal component. Or, you can actually calculate the reconstruction: z1 v.x1 =   2[  2/ 2 ,  2/2 ] T = [1, 1]T
x2 = 0*[0, 0]T = [0,0]
x3 =  2* [1, 1]T = [1,1]
which are exactly x1, x2, x3.32) In LDA, the idea is to find the line that best separates the two classes. In the given image which of the following is a good projection?
A. LD1B. LD2C. BothD. None of theseSolution: (A)LD1 Is a good projection because it best separates the class.Question Context 33PCA is a good technique to try, because it is simple to understand and is commonly used to reduce the dimensionality of the data. Obtain the eigenvalues 1  2      N and plot.To see how f(M) increases with M and takes maximum value 1 at M = D. We have two graph given below:33) Which of the above graph shows better performance of PCA? Where M is first M principal components and D is total number of features?A. LeftB. RightC. Any of A and BD. None of theseSolution: (A)PCA is good if f(M) asymptotes rapidly to 1. This happens if the first eigenvalues are big and the remainder are small. PCA is bad if all the eigenvalues are roughly equal. See examples of both cases in figure.34) Which of the following option is true?A. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class.B. Both attempt to model the difference between the classes of data.C. PCA explicitly attempts to model the difference between the classes of data. LDA on the other hand does not take into account any difference in class.D. Both dont attempt to model the difference between the classes of data.Solution: (A)Options are self explanatory.35) Which of the following can be the first 2 principal components after applying PCA?A. 1 and 2B. 1 and 3C. 2 and 4D. 3 and 4Solution: (D)For the first two choices, the two loading vectors are not orthogonal.36) Which of the following gives the difference(s) between the logistic regression and LDA?A. 1B. 2C. 1 and 2D. None of theseSolution: (C)Refer this video37) Which of the following offset, do we consider in PCA?
A. Vertical offsetB. Perpendicular offsetC. BothD. None of theseSolution: (B)We always consider residual as vertical offsets. Perpendicular offset are useful in case of PCA38) Imagine you are dealing with 10 class classification problem and you want to know that at most how many discriminant vectors can be produced by LDA. What is the correct answer?A. 20B. 9C. 21D. 11E. 10Solution: (B)LDA produces at most c  1 discriminant vectors. You may refer this link for more information.Question Context 39The given dataset consists of images of Hoover Tower and some other towers. Now, you want to use PCA (Eigenface) and the nearest neighbour method to build a classifier that predicts whether new image depicts Hoover tower or not. The figure gives the sample of your input training images.39) In order to get reasonable performance from the Eigenface algorithm, what pre-processing steps will be required on these images?A. 1B. 2C. 1 and 2D. None of theseSolution: (C)Both the statements are correct.40) What are the optimum number of principle components in the below figure ?
A. 7B. 30C. 40D. Cant SaySolution: (B)We can see in the above figure that the number of components = 30 is giving highest variance with lowest number of components. Hence option B is the right answer.I hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge ofdimensionality reduction.If you have any doubts in the questions above, let us know through comments below. Also, If you have any suggestions or improvements you think we should make in the next skill test, you can let us know by dropping your feedback in the comments section. Also, checkout DATAFEST 2017.",https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/
"Team Lead, Data Quality- Gurgaon, India (3+ Years Of Experience)","Learn everything about Analytics|
Description
","Share this:|Like this:|Related Articles|40 Must know Questions to test a data scientist on Dimensionality Reduction techniques|Analyst, Data Quality- Gurgaon, India (1-3 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : The Data Quality and Management team is the foundation of our success. From over 100 million addresses we have delivered packages to, we have records of addresses, consumer habit and demand for products, on the movement products and services.. to name a few. Having a strong Data Quality Management management with regard to the acquisition, maintenance, disposition and distribution of data is crucial for Delhiverys future success.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, data cleaning, data management, data modeling, r, sql
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/team-lead-data-quality-gurgaon-india-3-years-of-experience/
"Analyst, Data Quality- Gurgaon, India (1-3 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Team Lead, Data Quality- Gurgaon, India (3+ Years Of Experience)|Director, Business Analytics- Gurgaon, India (8-13 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : The Data Quality and Management team is the foundation of our success. From over 100 million addresses we have delivered packages to, we have records of addresses, consumer habit and demand for products, on the movement products and services.. to name a few. Having a strong Data Quality Management management with regard to the acquisition, maintenance, disposition and distribution of data is crucial for company future success.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, database, excel, sql
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/analyst-data-quality-gurgaon-india-1-3-years-of-experience/
"Director, Business Analytics- Gurgaon, India (8-13 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst, Data Quality- Gurgaon, India (1-3 Years Of Experience)|AV DataFest 2017  Out in its Full Glory|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  13 years
Requirements : 
Task Info : If you want to forecast Indias eCommerce demand within 3% margin, build consumer models and scores for 50 million online shoppers, or decide the location of eCommerce fulfilment center based on future growth, our data team is the place to be. We are countrys premium data science team featured widely in press and news.
College Preference : tier1-any
Min Qualification : ug
Skills : data analysis, excel, Power point, python, r, statistical modeling
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/director-business-analytics-gurgaon-india-8-13-years-of-experience/
AV DataFest 2017  Out in its Full Glory,Learn everything about Analytics|AV DataFest 2017  Here we begin !!|Come 1 April 2017|Launch Party|Competitions & Events|How to Participate?|Whats else is expected?,"1. Celebrating Top Visionaries and Thought Leaders|2. Machine Learning Hackathon|3. MiniHacks|4. Skill Tests|5. The Strategic Thor|6. Practice tests|7. DataHack Hour|8.The Mightiest Pen|9.Rise of the Hero|Share this:|Like this:|Related Articles|Director, Business Analytics- Gurgaon, India (8-13 Years Of Experience)|Analyst, Business Analytics- Gurgaon, India (1-5 Years Of Experience)|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This April, the world will see a battle fought by data scientists and data managers across the world. Tonnes of data will be transferred from one camp to another. Every weapon of machine learning will be applied and only the best, the most knowledgable, the most practical hackers and problem solvers will survive. There will be new heroes, who will rise to their glory and their will be empires who will fall.Yes  I am talking aboutAV DataFest 2017.As you know, this April we are turning 4 and AV DataFest is our way of celebrating the occasion. In our previous article, we showered some light on how DataFest 2017is going to turn out. Now, is the time to raise the curtains. Stay with me, as I unveil one of the grandest event in data science & analytics  DATAFEST 2017.In the battle of the Datavengers, we are assembling a team of best data scientists who will find answers to data science problems for human race. It is the survivalof this fittest, sodont miss this opportunity to prove your mettle against top data scientists from all over the world.DataFest 2017commences on 1 April 2017. It is a month-long event which brings together data scientists, analysts, data science evangelists, leaders & influencers from the industry. There areexhilarating competitions in machine learning, deep learning, probability, analytics & much more. Check out our latest DataFest websitehere.Not only we are giving prizes to heroes in individual competitions, there will be additional prizes to overall DataFest. So, the best among all the winners will take away even more!The ranking on this leaderboard is cumulative of scores from all the competitions such as  Minihacks,skilltests, machine learning hackathon, blogathonand whats your story.The celebration begins with a Launch Party on 1 April @ 8 PM (IST). As we complete 4 years, our hearts are filled with exuberance and immense joy. We are raising our glasses to 4 years of hard work, passion, patience & success. Come toast with us and be part of our happiness.To commemorate 4 years of our existence we bring to an exclusive session with one of the top thought leader in the industry. Come to the launch party and find out who it is.Now, lets find out the events whichawait you.Aspromised DataFest is going to bigger & better and one of a kind event. Before I introduce the competitions, let me bring out the biggest surprise of the year.The top 3 rankers of DataFestwill be our 3 most powerful Datavengers. These 3 Datavengers will be awarded cash prizes worth INR 1,80,000 (~$2750). After all, they have left no stone unturned in data science and machine learning. Isnt it?Rank 1: INR 100,000 (~$1525)Rank 2: INR 50,000 (~$762)Rank 3: INR 30,000 (~$457)During the DataFest, we bring you interactions with top visionaries and thought leaders from various founders in analytics & data science industry. Find out about their journey, startup story and how they were never stopped running to achieve their goal. These include inspiring stories from:and many moreThis Machine Learning Hackathonis not like every otherhackathonyou participate in. As we said, everything is going to bigger & better. So is this hackathon.Date: 20 April  23 April 2017Top 5 rankers will take away cash prizes & merchandises.Stay tuned to know more details.These are our short duration hackathons. The problem statement is going to be challenging & intense. Top 3 winners of each MiniHack will take away cash prizes worth INR 60,000 (~$916)As Datavengers, you need powers to buck up your chances to win.Attain these skill powers to rise up on the leaderboard. To all the top 5 rankers of each skill power we are giving out AV branded merchandises.The Strategic Thor is a strategic thinking competition to challenge your analytical thinking & decision making skills. Test how well equipped are your strategic skills. Top 3 winners will take away cash prizes worth INR 55,000 (~$810).These are powerful tools which are essential for any data scientist to be adept in.If you are as aspiring data scientist then take these practice tests to find your skill level.This is a new learning initiative from Analytics Vidhya DataHack hour is for all data science beginners out there. Come to DataHack platform daily and solve interesting challenges & quizzes in an hour. You will have a problem, community members and all the help you need to solve these problems.It is a 15 day journey to start and learn data science in most practical manner.This starts 16 April 2017.The Mightiest Penis our blogging competition to evoke the data science evangelist in you. In this competitions are inviting all data science experts to share their knowledge with the world. Top 3 winners will take away cash prizes worth INR 10,000 (~$152).Rise of the Hero competitionencourages all heros who have succeeded in data science industry against all adversities. Come share your story with us and we will publish 3 best stories on our blog. Top 3 winners will take away cash prizes worth INR 10,000 (~$152).To participate in all the above events. Follow the basic steps:Well  watch the stage unfold in the coming days as we send out invites to the mightiest heroes across the world. Subscribefor DataFest updatesto know about the upcoming events. We will soon be sharing more details with you shortly. Look out forour next article.This is just the beginning because the celebration is yet to start. Dont miss it !!If you wish you associated with the DATAFEST as speaker, sponsor or partner then write to us at [emailprotected].",https://www.analyticsvidhya.com/blog/2017/03/datafest-2017-out-in-its-full-glory/
"Analyst, Business Analytics- Gurgaon, India (1-5 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|AV DataFest 2017  Out in its Full Glory|Data Scientist- Gurgaon, India (2-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  5 years
Requirements : 
Task Info : If you want to forecast Indias eCommerce demand within 3% margin, build consumer models and scores for 50 million online shoppers, or decide the location of eCommerce fulfilment center based on future growth, our data team is the place to be. We are countrys premium data science team featured widely in press and news.
College Preference : tier1-any
Min Qualification : ug
Skills : data analysis, excel, machine learning, nosql, Power point, python, r, regression, sql, statistical modeling, time series
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/analyst-business-analytics-gurgaon-india-1-5-years-of-experience/
"Data Scientist- Gurgaon, India (2-7 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst, Business Analytics- Gurgaon, India (1-5 Years Of Experience)|Backend Developer- Gurgaon, India (3-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  7 years
Requirements : 
Task Info : If you want to forecast Indias eCommerce demand within 3% margin, build consumer models and scores for 50 million online shoppers, or decide the location of eCommerce fulfilment center based on future growth, our data team is the place to be. We are countrys premium data science team featured widely in press and news.
College Preference : tier1-any
Min Qualification : pg
Skills : c++, decision trees, hadoop, logistic regression, machine learning, MongoDB, neural network, nosql, python, spark, statistical modeling, supervised learning, unsupervised learning
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-scientist-gurgaon-india-2-7-years-of-experience/
"Backend Developer- Gurgaon, India (3-7 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Gurgaon, India (2-7 Years Of Experience)|How to handle Imbalanced Classification Problems in machine learning?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,Description|Responsibility|Qualifications|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  7 years
Requirements : 
Task Info : We have started an exciting journey towards a full-scale event driven micro-service architecture to enable data consistency while scaling to a range of platforms and devices  web, mobile browsers, native android apps, handheld devices, sortation systems etc. We are actively looking for people who like to get their hands dirty with AWS Kinesis / Lambda, Python, Java, Node or similar architectureAs a backend developer you are expected to work closely with Product team to do rapid development of new features / enhancements, work with the Data Services team to design and build data stores, write effective map-reduce / aggregations, write quality code and work with your lead and team to drive quality and efficiency
College Preference : no-bar
Min Qualification : ug
Skills : c++, hbase, java, MongoDB, nosql, python, RDBMS
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/backend-developer-gurgaon-india-3-7-years-of-experience/
How to handle Imbalanced Classification Problems in machine learning?|Introduction,Learn everything about Analytics|Table of Content|1.Challenges faced with Imbalanced datasets|2. Approachto handling Imbalanced Datasets|3.Illustrative Example|4. Conclusion,"|Example of imbalanced classes||Challenges with standard Machine learning techniques||Examples of imbalanced classes||Dataset used|2.1 Data Level approach: Resampling Techniques|2.2Algorithmic Ensemble Techniques|3.1. Data Description|3.2 Description of Methodologies||About the Author||Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Backend Developer- Gurgaon, India (3-7 Years Of Experience)|BI Architect- US (8 Years Of experience)|
Guest Blog
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",2.1.1 Random Under-Sampling|2.1.2 Random Over-Sampling|2.1.3 Cluster-Based Over Sampling|2.1.4 Informed Over Sampling: Synthetic Minority Over-sampling Technique|2.1.5 Modified synthetic minority oversampling technique (MSMOTE)|2.2.1. Bagging Based|2.2.2. Boosting-Based|2.2.2.1.Adaptive Boosting- Ada Boost|2.2.2.2 Gradient Tree Boosting,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have spent some time in machine learning and data science, you would have definitely come across imbalanced class distribution. This is a scenario wherethe number of observations belonging to one class is significantly lower than those belonging to the other classes.This problem is predominant in scenarios where anomaly detection is cruciallike electricity pilferage, fraudulent transactions in banks, identification of rare diseases, etc. In this situation,the predictive model developed using conventional machine learning algorithms could be biased and inaccurate.This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes.This guide describes variousapproaches for solving such class imbalance problems using various sampling techniques. We also weigh each technique for its pros and cons. Finally, I reveal an approach using which you cancreate a balanced class distribution and apply ensemble learning technique designed especially for this purpose.One of the main challenges faced by the utility industry today is electricity theft. Electricity theft is the third largest form of theft worldwide. Utility companies are increasingly turning towards advanced analytics and machine learning algorithms to identify consumption patterns that indicate theft.However, one of the biggest stumbling blocks is the humongous data and its distribution. Fraudulent transactions are significantly lower than normal healthy transactions i.e. accounting it to around 1-2 % of the total number of observations. The ask is to improve identification of the rare minority class as opposed to achieving higher overall accuracy.Machine Learning algorithms tend to produce unsatisfactory classifiers when faced with imbalanced datasets. For any imbalanced data set, if the event to be predicted belongs to the minority class and the event rate is less than 5%, it is usually referred to as a rare event.Lets understand this with the help of an example.Ex: In an utilities fraud detection data set you have the following data:Total Observations = 1000Fraudulent Observations = 20Non Fraudulent Observations = 980Event Rate= 2 %The main question faced during data analysisis How to get a balanced dataset by getting a decent number of samples for these anomalies given the rare occurrence for some them?The conventional model evaluation methods do not accurately measure model performance when faced with imbalanced datasets.Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards classes which have number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class.Evaluation of a classification algorithm performance is measured by the Confusion Matrix which contains information about the actual and the predicted class.
Accuracy of a model = (TP+TN) / (TP+FN+FP+TN)However, while working in an imbalanced domain accuracy is not an appropriate measure to evaluate model performance. For eg: A classifier which achieves an accuracy of 98 % with an event rate of 2 % is not accurate, if it classifies all instances as the majority class. And eliminates the 2 % minority class observations as noise.Thus, to sum it up, while trying to resolve specific business challenges with imbalanced data sets, the classifiers produced by standard machine learning algorithms might not give accurate results. Apart from fraudulent transactions, other examples of a common business problem with imbalanced dataset are:In this article,we will illustrate the various techniques to train a model to perform well against highly imbalanced datasets. And accurately predict rare events using the following fraud detection dataset:Total Observations = 1000Fraudulent Observations =20Non-Fraudulent Observations = 980Event Rate= 2 %Fraud Indicator = 0 for Non-Fraud InstancesFraud Indicator = 1 for FraudDealing with imbalanced datasets entails strategies such as improving classification algorithms or balancing classes in the training data (data preprocessing) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application.The main objective of balancing classes is to either increasing the frequency of the minority class or decreasing the frequency of the majority class. This is done in order to obtain approximately the same number of instances for both the classes. Let us look at a few resampling techniques:Random Undersampling aims to balance class distribution by randomly eliminating majority class examples. This is done until the majority and minority class instances are balanced out.Total Observations = 1000Fraudulent Observations =20Non Fraudulent Observations = 980Event Rate= 2 %In this case we are taking 10 % samples without replacement from Non Fraud instances. And combining them with Fraud instances.Non Fraudulent Observations after random under sampling = 10 % of 980 =98Total Observations after combining them with Fraudulent observations = 20+98=118Event Rate for the new dataset after under sampling = 20/118 = 17%Over-Sampling increases the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.Total Observations = 1000Fraudulent Observations =20Non Fraudulent Observations = 980Event Rate= 2 %In this case we are replicating 20 fraud observations 20 times.Non Fraudulent Observations =980Fraudulent Observations after replicating the minority class observations= 400Total Observations in the new data set after oversampling=1380Event Rate for the new data set after under sampling= 400/1380 = 29 %In this case, the K-means clustering algorithm is independently applied to minority and majority class instances. This is to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size. Total Observations = 1000Fraudulent Observations =20Non Fraudulent Observations = 980Event Rate= 2 %After oversampling of each cluster, all clusters of the same class contain the same number of observations.Event Rate post cluster based oversampling sampling = 500/ (1020+500) = 33 %This technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset. A subset of data is taken from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models.Total Observations = 1000Fraudulent Observations = 20Non Fraudulent Observations = 980Event Rate = 2 %A sample of 15 instances is taken from the minority class and similar synthetic instances are generated 20 timesPost generation of synthetic instances, the following data set is createdMinority Class (Fraudulent Observations) = 300Majority Class (Non-Fraudulent Observations) = 980Event rate= 300/1280 = 23.4 %**N is the number of attributesFigure 1: Synthetic Minority Oversampling AlgorithmFigure 2: Generation of Synthetic Instances with the help of SMOTEIt is a modified version of SMOTE. SMOTE does not consider the underlying distribution of the minority class and latent noises in the dataset. To improve the performance of SMOTE a modified method MSMOTE is used.This algorithm classifies the samples of minority classes into 3 distinct groups  Security/Safe samples, Border samples, and latent nose samples. This is done by calculating the distances among samples of the minority class and samples of the training data.Security samples are those data points which can improve the performance of a classifier. While on the other hand, noise are the data points which can reduce the performance of the classifier. The ones which are difficult to categorize into any of the two are classified as border samples.While the basic flow of MSOMTE is the same as that of SMOTE (discussed in the previous section). In MSMOTE the strategy of selecting nearest neighbors is different from SMOTE. The algorithm randomly selects a data point from the k nearest neighbors for the security sample, selects the nearest neighbor from the border samples and does nothing for latent noise.The above section, deals with handling imbalanced data by resampling original data to provide balanced classes. In this section, we are going to look at an alternate approach i.e. Modifying existing classification algorithms to make them appropriate for imbalanced data sets.The main objective of ensemble methodology is to improve the performance of single classifiers. The approach involves constructing several two stage classifiers from the original data and then aggregate their predictions.
  Figure 3: Approach to Ensemble based Methodologies Bagging is an abbreviation of Bootstrap Aggregating. The conventional bagging algorithm involves generating n different bootstrap training samples with replacement. And training the algorithm on each bootstrapped algorithm separately and then aggregating the predictions at the end.Bagging is used for reducing Overfitting in order to create strong learners for generating accurate predictions. Unlike boosting, bagging allows replacement in the bootstrapped sample. Figure 4: Approach to Bagging Methodology Total Observations = 1000Fraudulent Observations =20Non Fraudulent Observations = 980Event Rate= 2 %There are 10 bootstrapped samples chosen from the population with replacement. Each sample contains 200 observations. And each sample is different from the original dataset but resembles the dataset in distribution & variability. The machine learning algorithms like logistic regression, neural networks, decision tree are fitted to each bootstrapped sample of 200 observations. And the Classifiers c1, c2c10 are aggregated to produce a compound classifier. This ensemble methodology produces a stronger compound classifier since it combines the results of individual classifiers to come up with an improved one.Boosting is an ensemble technique to combine weak learners to create a strong learner that can make accurate predictions. Boosting starts out with a base classifier / weak classifier that is prepared on the training data.What are base learners / weak classifiers?The base learners / Classifiers are weak learners i.e. the prediction accuracy is only slightly better than average. A classifier learning algorithm is said to be weak when small changes in data induce big changes in the classification model.In the next iteration, the new classifier focuses on or places more weight to those cases which were incorrectly classified in the last round.Figure 5: Approach to Boosting Methodologies Ada Boost is the first original boosting technique which creates a highly accurate prediction rule by combining many weak and inaccurate rules. Each classifier is serially trained with the goal of correctly classifying examples in every round that were incorrectly classified in the previous round.For a learned classifier to make strong predictions it should follow the following three conditions:Each of the weak hypothesis has an accuracy slightly better than random guessing i.e. Error Term  (t) should be slightly more than - where  >0. This is the fundamental assumption of this boosting algorithm which can produce a final hypothesis with a small errorAfter each round, it gives more focus to examples that are harder to classify. The quantity of focus is measured by a weight, which initially is equal for all instances. After each iteration, the weights of misclassified instances are increased and the weights of correctly classified instances are decreased. Figure 6: Approach to Adaptive Boosting For example in a data set containing 1000 observations out of which 20 are labelled fraudulent. Equal weights W1 are assigned to all observations and the base classifier accurately classifies 400 observations.Weight of each of the 600 misclassified observations is increased to w2 and weight of each of the correctly classified observations is reduced to w3.In each iteration, these updated weighted observations are fed to the weak classifier to improve its performance. This process continues till the misclassification rate significantly decreases thereby resulting in a strong classifier.In Gradient Boosting many models are trained sequentially. It is a numerical optimization algorithm where each model minimizes the loss function, y = ax+b+e, using the Gradient Descent Method.Decision Trees are used as weak learners in Gradient Boosting.While both Adaboost and Gradient Boosting work on weak learners / classifiers. And try to boost them into a strong learner, there are some fundamental differences in the two methodologies. Adaboost either requires the users to specify a set of weak learners or randomly generates the weak learners before the actual learning process. The weight of each learner is adjusted at every step depending on whether it predicts a sample correctly.On the other hand, Gradient Boosting builds the first learner on the training dataset to predict the samples, calculates the loss (Difference between real value and output of the first learner). And use this loss to build an improved learner in the second stage.At every step, the residual of the loss function is calculated using the Gradient Descent Method and the new residual becomes a target variable for the subsequent iteration.Gradient Boosting can be done using the Gradient Boosting Node in SAS Miner and GBM package in R   Figure 7: Approach to Gradient Boosting For example: In a training data set containing 1000 observations out of which 20 are labelled fraudulent an initial base classifier. Target Variable Fraud =1 for fraudulent transactions and Fraud=0 for not fraud transactions.For eg: Decision tree is fitted which accurately classifying only 5 observations as Fraudulent observations. A differentiable loss function is calculated based on the difference between the actual output and the predicted output of this step. The residual of the loss function is the target variable (F1) for the next iteration.Similarly, this algorithm internally calculates the loss function, updates the target at every stage and comes up with an improved classifier as compared to the initial classifier.2.2.2.3XG Boost XGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm discussed in the previous section.Advantages over Other Boosting TechniquesExtreme gradient boosting can be done using the XGBoost package in R and PythonThe illustrative telecom churn dataset has 47241 client records with each record containing information about 27 key predictor variables.The data structure of the rare event data set is shown below post missing value removal, outlier treatment and dimension reduction.Download the Dataset from here: Sample DatasetThe unbalanced dataset is balanced using Synthetic Minority oversampling technique (SMOTE) which attempts to balance the data set by creating synthetic instances. And train the balanced data set using Gradient Boosting Algorithm as illustrated by the R codes in the next sectionR Codes#Load DataResultsThis approach of balancing the data set with SMOTE and training a gradient boosting algorithm on the balanced set significantly impacts the accuracy of the predictive model. By increasing its lift by around 20% and precision/hit ratio by 3-4 times as compared to normal analytical modeling techniques like logistic regression and decision trees. When faced with imbalanced data sets there is no one stop solution to improve the accuracy of the prediction model. One may need to try out multiple methods to figure out the best-suited sampling techniques for the dataset. In most cases, synthetic techniques like SMOTE and MSMOTE will outperform the conventional oversampling and undersampling methods.For better results, one can use synthetic sampling methods like SMOTE and MSMOTE along with advanced boosting methods like Gradient boosting and XG Boost.One of the advanced bagging techniques commonly used to counter the imbalanced dataset problem is SMOTE bagging. It follows an entirely different approach from conventional bagging to create each Bag/Bootstrap. It generates the positive instances by the SMOTE Algorithm by setting a SMOTE resampling rate in each iteration. The set of negative instances is bootstrapped in each iteration.Depending on the characteristics of the imbalanced data set, the most effective techniques will vary. Relevant evaluation parameters should be considered during the model comparison.While comparing multiple prediction models built through an exhaustive combination of the above-mentioned techniques Lift & Area under the ROC Curve will be instrumental in determining which model is superior to the others.If you have any questions or doubts, feel free to drop them in the comments below.ReferencesUpasana holds a Post Graduate diploma in Management from Indian Institute of Management, Indore. She is currently working as a Consultant in the Data & Analytics Practice of KPMG. She has around 3.5 + years of work experience and has worked in multiple advanced analytics and data science engagements spanning industries like Telecom, utilities, banking , manufacturing. She has worked extensively on SAS, Data Management & Advanced Analytics, R, Tableau, Oracle and SQL.",https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/
BI Architect- US (8 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|How to handle Imbalanced Classification Problems in machine learning?|ETL Designer- Mumbai, US, (5 Years Of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  years
Requirements : 
Task Info : Position:BI Architect Location:US Industry:Financial, Telecommunications, High Technology Employment Type:Full-time Visa Sponsorship:Available for on-site client work Fundamental Competency:Proficiency at developing intricate and well-defined data visualization, analytics and reporting architectures using one or more of the following technologies: Microstrategy, Tableau, SSAS/SSRS and Qlikview to maximize potential of data assets paired with a complete understanding of data management processes and implementation lifecycle to ensure successful end-to-end project execution. Position Overview:The Business Intelligence Architect will be responsible for leading data visualization and reporting initiatives for Exusias clients using both on-premise technology and Exusias cloud delivery model while working alongside a diverse and talented on-site/off-site project team ensuring a smooth process and acting as a contact between Exusias consultants and the clients employees that can be spread across offices and regions. Responsibilities:   Requirements:  
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business intelligence, data management, microstrategy, qlikview, sql, ssrs, tableau
Location : United States
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/bi-architect-us-8-years-of-experience/
"ETL Designer- Mumbai, US, (5 Years Of Experience )",Learn everything about Analytics,"Share this:|Like this:|Related Articles|BI Architect- US (8 Years Of experience)|Data Quality Architect- Mumbai, US (7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Position:ETL Designer Location:US, India Industry:Financial, Healthcare, Telecommunications, High Technology Employment Type:Full-time Visa Sponsorship:Available for on-site client work  Fundamental Competency:Expert knowledge of the entire data management landscape and deep understanding of the end-to-end ETL lifecycle and the associated challenges involved at each stage of a global implementation, including interacting with multiple related technologies and solutions. Excellent communication skills and experience working closely and efficiently with cross-geographic teams are essential.  Position Overview:The ETL Designer will be responsible for establishing superior ETL strategies, architectures and applications for Exusias clients while working alongside a diverse and talented off-site project team comprised of consultants and client employees that can be spread across offices and regions. The candidate must be able to successfully communicate with internal teams as well as client personnel in order to deliver industry-leading solutions, detailed functional designs and complex application implementation components throughout the entire lifecycle of the project.  Responsibilities:   Requirements:  
College Preference : no-bar
Min Qualification : ug
Skills : database management, data modeling, etl
Location : Mumbai, United States
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/etl-designer-mumbai-us-5-years-of-experience/
"Data Quality Architect- Mumbai, US (7 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|ETL Designer- Mumbai, US, (5 Years Of Experience )|Introduction to Conditional Probability and Bayes theorem in R for data science professionals|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  years
Requirements : 
Task Info : Position:Data Quality Architect Location:US, India Industry:Financial, Healthcare, Telecommunications, High Technology Employment Type:Full-time Visa Sponsorship:Available for on-site client work Fundamental Competency:Expert knowledge of the entire data management landscape and deep understanding of core data quality design patterns and the associated challenges involved with data analysis, certification, modeling, quality improvement and data management implementation projects including deep experience using one or more of the following technologies:Informatica IDQ, Global IDs, Harte Hanks Trillium, IBM Quality Stage. Position Overview:The Data Quality Architect will be responsible for data profiling analysis, superior design of generic, rules driven data quality platform architectures as well data visualization components to support data stewards and data governance processes and procedures. Additionally the Data Quality Architect will be responsible for thorough analysis and implementation oversight leveraging metadata, data quality and business intelligence knowledge to provide meaningful insights to clients throughout the entire lifecycle of the project. Responsibilities:   Requirements:  
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, Data analytics, data management, etl
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-quality-architect-mumbai-us-7-years-of-experience/
Introduction to Conditional Probability and Bayes theorem in R for data science professionals,"Learn everything about Analytics|Introduction|Table of Contents|1.Events  Union, Intersection & Disjoint events|2.Independent, Dependent & Exclusive Events|3. Conditional Probability|4.Bayes Theorem|5. Example of Bayes Theorem and Probability trees|6. Frequentist vs Bayesian Definitions of probability|7. Open Challenges|End Notes|Learn, compete, hack and get hired","1.1 EVENTS|1.2 Union of Events|1.3. Intersection of Events|1.4 Disjoint Events|Probability of independent events|Now, its time to implement the independent tests in R|Probability of dependent events|Mutually exclusive and Exhaustive events|3.1 Reversing the condition|5.1 Bayes Updating|Share this:|Related Articles|Data Quality Architect- Mumbai, US (7 Years Of Experience)|Celebrating Womens Day: 33 Women in Data Science from around the World & AV Community|
Dishashree Gupta
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Understanding of probability is must for a data science professional.Solutions to many data science problems are often probabilistic in nature. Hence, a better understanding of probability will help you understand & implement these algorithms more efficiently.In this article, I will focus on conditional probability. For beginners in probability, I would strongly recommend that you go through this article before proceeding further.A predictive model can easily be understood as a statement of conditional probability. For example, the probability of a customer from segment A buying a product of category Z in next 10 days is 0.80. In other words, the probability of a customer buying product from Category Z, given that the customer is from Segment A is 0.80.In this article, I will walk you through conditional probability in detail. Ill be using examples & real-life scenarios to help you improve yourunderstanding.You can also check out our new article on Bayes Theorem here. It contains a ton of examples and real-world applications  something every data science professional must be aware of.Before we explore conditional probability, let us define some basic common terminologies:An event is simply the outcome of a random experiment. Getting a heads when we toss a coin is an event. Getting a 6 when we roll a fairdie is an event. We associate probabilities to these events by defining the event and the sample space.The sample space is nothing but the collection of all possible outcomes of an experiment. This means that if we perform a particular task again and again, all the possible results of the task are listed in the sample space.For example: A sample space for a single throw of a die will be {1,2,3,4,5,6}. One of these is bound to occur if we throw a die. The sample space exhausts all the possibilities that can happen when that experiment is performed.An event can also be a combination of different events.We can define an event (C) of getting a 4 or 6 when we roll a fair die. Here event C is a union of two events:Event A = Getting a 4Event B = Getting a 6P (C) = P (A  B)In simple words we can say that we should consider the probability of (A  B) when we are interested in combined probability of two (or more)events.Lets look at another example.Let C be the event ofgetting a multiple of 2 and 3 when you throw a fairdie.Event A = Getting a multiple of 2 when you throw a fair dieEvent B = Getting a multiple of 3 when you throw a fair dieEvent C = Getting a multiple of 2 and 3Event C is an intersection of event A & B.Probabilities are then defined as follows.P (C) = P (A  B)We can now say that the shaded region is the probability of both events A and B occurring together.What if, you come across a case when any two particular events cannot occur at the same time.For example: Lets say you have a fair die and you have only one throw.Event A = Getting a multiple of 3Event B = Getting a multiple of 5You want both event A & B should occur together.Lets find the subspace for Event A & B.Event A = {3,6}Event B = {5}Sample Space= {1,2,3,4,5,6}As you cansee, there is no case for which event A & B can occur together. Such events are called disjoint event. To represent this using a Venn diagram:Now that we are familiar with the terms Union, intersection and disjoint events, we can talk about independence of events.Suppose we have two events  event A and event B.If the occurrence of event A doesnt affect the occurrence of event B, these events are called independent events.Lets see some examples of independent events.In each of these cases the probability of outcome of the second event is not affected at all by the outcome of the first event.In this case the probability of P (A  B) = P (A) * P (B)Lets take an example here. Suppose we win the game if we pick a red marble from a jar containing 4 red and 3 black marbles and we get heads on the toss of a coin. What is the probability of winning?Lets define event A, as getting red marble from the jarEvent B is getting heads on the toss of a coin.We need to find the probability of both getting a red marble and a heads in a coin toss.P (A) = 4/7P (B) = 1/2We know that there is no affect of the color of the marble on the outcome of the coin toss.P (A  B) = P (A) * P (B)P (A  B) = (4/7) * (1/2) = (2/7)Let us consider the following example:Example:
We have two cards on which numbers are written on both front and back (1 at front and 2 at back). We will toss those two cards in air and see if the results of the two cards are independent. We toss the cards 2000 times, and then compute the joint distribution of the results of the toss from the two cards.OutputWe see that prob_table1 and prob_table2 have close values, indicating that the toss of the two cards are probably independent.Next, can you think of examples of dependent events ?In the above example, lets define event A as getting a Red marble from the jar. We then keep the marble out and then take another marble from the jar.Will the probabilities in the second case still be the same as that in the first case?Lets see. So, for the first time there are 4/7 chances of getting a red marble. Lets assume you got a red marble on the first attempt. Now, for second chance, to get a red marble we have 3/6 chances.If we didnt get a red marble on the first attempt but a white marble instead. Then, there were 4/6 chances to get the red marble second time. Therefore the probability in the second case was dependent on what happened the first time.Quiz1: If you have a Jack and your next card is dealt with a new deck of cards the probability of you obtaining a jack again is? Are these events dependent or independent?Mutually exclusive events are those events where two eventscannot happen together.The easiest example to understand this is the toss of a coin. Getting a head and a tail are mutually exclusive because we can either get heads or tails but never both at the samein a single coin toss.A set of events is collectively exhaustive when the set should contain all the possible outcomes of the experiment. One of the events from the list must occur for sure when the experiment is performed.For example, in a throw of a die, {1,2,3,4,5,6} is an exhaustive collection because, it encompasses the entire range of the possible outcomes.Consider the outcomes even (2,4 or 6) and not-6 (1,2,3,4, or 5) in a throw of a fair die. They are collectively exhaustive but not mutually exclusive.Quiz2: Check whether the below events are mutually exclusive:Conditional probabilities arise naturally in the investigation of experiments where an outcome of a trial may affect the outcomes of the subsequent trials.We try to calculate the probability of the second event (event B) given that the first event (event A) has already happened. If the probability of the event changes when we take the first event into consideration, we can safely say that the probability of event B is dependent of the occurrence of event A.Lets think of cases where this happens:And so on.Here we can define, 2 events:We can write the conditional probability as , the probability of the occurrence of event A given that B has already happened.Lets play a simple game of cards for you to understand this. Suppose you draw two cards from a deck and you win if you get a jack followed by an ace (without replacement). What is the probability of winning, given we know that you got a jack in the first turn?Let event A be getting a jack in the first turnLet event B be getting an ace in the second turn.We need to findP(A) = 4/52P(B) = 4/51 {no replacement}P(A and B) = 4/52*4/51= 0.006Here we are determining the probabilities when we know some conditions instead of calculating random probabilities. Here we knew that he got a jackin the first turn.Lets take another example.Suppose you have a jar containing 6 marbles  3 black and 3 white. What is the probability of getting a black given the first one was black too.P (A) = getting a black marble in the first turnP (B) = getting a black marble in the second turnP (A) = 3/6P (B) = 2/5P (A and B) = *2/5 = 1/5Let us now consider a new example and implement in R.Example:
A research group collected the yearly data of road accidents with respect to the conditions of
following and not following the traffic rules of an accident prone area. They are interested in calculating the probability of accident given that a person followed the traffic rules. The table of the data is given as follows:Now here our equation becomes:P(Accident | A person follow Traffic Rule) = P(Accident and follow Traffic Rule) / P(Follow Traffic Rule)Solution:OutputExample: Rahuls favorite breakfast is bagels and his favorite lunch is pizza. The probability of Rahul having bagels for breakfast is 0.6. The probability of him having pizza for lunch is 0.5. The probability of him, having a bagel for breakfast given that he eats a pizza for lunch is 0.7.Lets define event A as Rahul having a bagel for breakfast, Event B as Rahul having a pizza for lunch.P (A) = 0.6P (B) = 0.5If we look at the numbers, the probability of having a bagel is different than the probability of having a bagel given he has a pizza for lunch. This means that the probability of having a bagel is dependent on having a pizza for lunch.Now what if we need to know the probability of having a pizza given you had a bagel for breakfast. i.e. we need to know . Bayes theorem now comes into the picture.The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .How can we do that?The above statement is the general representation of the Bayes rule.For the previous example  if we now wish to calculate the probability of having a pizza for lunch provided you had a bagel for breakfast would be = 0.7 * 0.5/0.6.We can generalize the formula further.If multiple events Ai form an exhaustive set with another event B.We can write the equation asNow, implementing the example in R:OutputLets take the example of the breast cancer patients. The patients were tested thrice before the oncologist concluded that they had cancer. The general belief is that 1.48 out of a 1000 people have breast cancer in the US at that particular time when this test was conducted. The patients were tested over multiple tests. Three sets of test were done and the patient was only diagnosed with cancer if she tested positive in all three of them.Lets examine the test in detail.Sensitivity of the test (93%)  true positive RateSpecificity of the test (99%)  true negative RateLets first compute the probability of having cancer given that the patient tested positive in the first test.P (has cancer | first test +)P (cancer) = 0.00148Sensitivity can be denoted as P (+ | cancer) = 0.93Specificity can be denoted as P (- | no cancer)Since we do not have any other information, we believe that the patient is a randomly sampled individual. Hence our prior belief is that there is a 0.148% probability of the patient having cancer.The complement is that there is a 100  0.148% chance that the patient does not have CANCER. Similarly we can draw the below tree to denote the probabilities.Lets now try to calculate the probability of having cancer given that he tested positive on the first test i.e. P (cancer|+)P (cancer and +) = P (cancer) * P (+) = 0.00148*0.93P (no cancer and +) = P (no cancer) * P(+) = 0.99852*0.01To calculate the probability of testing positive, the person can have cancer and test positive or he may not have cancer and still test positive.This means that there is a 12% chance that the patient has cancer given he tested positive in the first test. This is known as the posterior probability.Lets now try to calculate the probability of having cancer given the patient tested positive in the second test as well.Now remember we will only do the second test if she tested positive in the first one. Therefore now the person is no longer a randomly sampled person but a specific case. We know something about her. Hence, the prior probabilities should change. We update the prior probability with the posterior from the previous test.Nothing would change in the sensitivity and specificity of the test since were doing the same test again. Look at the probability tree below.Lets calculate again the probability of having cancer given she tested positive in the second test.P (cancer and +) = P(cancer) * P(+) = 0.12 * 0.93P (no cancer and +) = P (no cancer) * P (+) = 0.88 * 0.01To calculate the probability of testing positive, the person can have cancer and test positive or she may not have cancer and still test positive.Now we see, that a patient who tested positive in the test twice, has a 93% chance of having cancer.A frequentist defines probability as an expected frequency of occurrence over large number of experiments.P(event) = n/N, where n is the number of times event A occurs in N opportunities.The Bayesian view of probability is related to degree of belief. It is a measure of the plausibility of an event given incomplete knowledge.The frequentist believes that the population mean is real but unknowable and can only be estimated from the data. He knows the distribution of the sample mean and constructs a confidence interval centered at the sample mean. So the actual population mean is either in the confidence interval or not in it.This is because he believes that the true mean is a single fixed value and does not have a distribution. So the frequentist says that 95% of similar intervals would contain the true mean, if each interval were constructed from a different random sample.The Bayesian definition has a totally different view point. They use their beliefs to construct probabilities. They believe that certain values are more believable than others based on the data and our prior knowledge.The Bayesian constructs a credible interval centered near the sample mean and totally affected by the prior beliefs about the mean. The Bayesian can therefore make statements about the population mean by using the probabilities.The debate between Bayesian and frequentist approaches has been going on for a long while. We have an amazing article which has gone deep into both these approaches. It has explained in detail the two approaches and Bayesian Inference.The aim of this article was to introduce you to conditional probability and Bayes theorem. Bayes theorem forms the backbone of one of very frequently used classification algorithms in data science  Naive Bayes.Once the above concepts are clear you might be interested to open the doors the naive Bayes algorithm and be stunned by the vast applications of Bayes theorem in it.Please post your answers to the open challenges in the comments section. And feel free to ask doubts or questions. Hope you enjoyed reading!",https://www.analyticsvidhya.com/blog/2017/03/conditional-probability-bayes-theorem/
Celebrating Womens Day: 33 Women in Data Science from around the World & AV Community,"Learn everything about Analytics|Introduction|Women Data Scientist in AV community|End Notes|Learn, compete, hack and get hired","1. Ada Lovelace|2. Florence Nightingale|3.Gertrude Mary Cox|4. Grace Hopper
|5. Dame Mary Lucy Cartwright|6. Corinna Cortes|7. Hilary Mason|8. Monica Rogati|9. Daphne Koller|10. Carla Gentry|11. Radhika Kulkarni|12. Yael Garten|13. Claudia Perlich|14.Caitlin Smallwood |15. Shafirira Goldwasser|16. June Andrews|17. Alice Zheng|18. Majken Sander|19. Crystal (Kahn) Valentine|20. Margot Gerritsen|21.Megan Price|22.Lori Sherer|23. Raia Hadsell|24. Cynthia Dwork|25. Animashree Anandkumar|1. Prarthana Bhatt|2. Yaasna Dua|3. Rashmi Nalavade|4. Bolaka Mukherjee|5. Anchal Gupta|6. Tulika Singh|AV Volunteers|1. Preeti Agrawal|2. Tanvi Purohit|Share this:|Like this:|Related Articles|Introduction to Conditional Probability and Bayes theorem in R for data science professionals|Senior Data Scientist- Pune (3 to 5 years of experience)|
Analytics Vidhya Content Team
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"She Believed, she could. So, she didThis Womens Day we are celebrating the women power. We are celebrating all those women who have made a name for themselves in the analytics & data science industry despite all odds.Today, I have decided to bring your attention to all the women out there, who deserve a mention. Whether she is your colleague, subordinate or friend, acknowledge her efforts & recognize her hard work. Through this article, I am acknowledging the works of various women in data science from theearly 1800s till 2017.Yes, we are celebrating the women in data science, machine learning and analytics. The gender gap in data science industry is not any news to you. But the big revelation is that the gap is shrinking. More & more women are making their career move in data science & analytics.From tech giants to revolutionary startups, women have done it all.And this Womens Day, we are gonna recognize the women achievers & their significant contribution.Come find out with me how many of these women do you know about.Through this article, I also want to encourage more women to join the data science industry. And this article will tell you how perseverance and hard workcan beat all obstacles.Remember, if they could do it, so can you. After all, we all have thewonder women inside all of us.Lets tag the women leaders of tomorrow. Tag your friends or colleagues who deserve a mention.And to all the women out there, Happy International Womens Day !!Ada Lovelace was a gifted mathematician. Her contributions, to mathematics & computer scienceare widely recognized. Under the mentorship of Charles Babbage, she started her mathematics career. Along with CharlesBabbage, she has significant contributions to the Analytical Engine. She was the first to recognize that machines have applications beyond calculations. She created the first algorithm for operation of a machine, because of which she is also known as the first computer programmer.Florence Nightingale was a statistician and social reformer. She was a gifted mathematician and excelled under the guidance of her father. She is a pioneer in thevisual representation of data and statistical graphics. She is credited for developing polar area diagram a form of ahistogram, also known as Nightingale rose diagram. She extensively used these graphs to present reports on medical conditions of people during Crimean War. She also made a statistical study on sanitation of India rural life and which led to improving the medical & public health services in India.Gertrude Mary Cox is an American statistician. She is well known for her work in experimental design.Her book Experimental Design is a well-acclaimedcontribution to thedesign of experiments. It is a classic for design and analysis of replicated experiments. She is the founder of department of Experimental Statistics at North Carolina State University. She is one of the well-known names in te field of Statistics and has several accolades to her name.Grace Hopper is an American Computer Scientist and US Navy rear admiral. She was the first one to invent a compiler for a computer programming language.She believed programs should be written in a simplified language like English and popularized the term machine independent programming language. She developed COBOL one of the widely programming languages in every business till date. She developed validation software for COBOL and its compiler.Dame Mary Lucy Cartwright was a British mathematician. She is well known for her work in dynamic systems and chaos. Shegave the proof for the analytical function and certain differential equations for radar work. The periodicity and stability of solutions of non-linear differential equations obtained by her along with a fellow form the basis for the modern theory of dynamical systems and chaos.Corinna Cortes is the Head of Google Research. Corinna is well known for her research work and in particular, she is known for her contributions to the theoretical foundations of support vector machines (SVMs). Before joining Google, Corinna worked with AT&T labs where her work in data mining for large datasets earned her recognition and awards. Corinna has received awards such as Paris Kanellakis Theory and Practice Award,AT&T Science and Technology Medal.Hilary Mason is the founder of FastForward Labs. She is one of the top influencers in data science industry. Before founding FastForward labs, Hilary was the chief data scientist at Bitly. She also co-founded HackNY, a not-for-profit organisation for students of NY. She has received several awards such as- TechFellows Engineering Leadership Forbes 40 under Forty List, Crains New York 40 under Forty list.
Monica Rogati is an independent Data Science Advisor where she works with various companies in education, healthcare, fitness, IoT, etc. She guides them for buildingdata into products. Earlier she was theVice President of Data at Jawbone. Prior to Jawbone, Shewas one of the early members of the LinkedIn data science team and has worked there for more than 5 years as Senior Data Scientist.
Daphne is Chief Computing Officer at CalicoLabs. She is also the co-founder of Coursera. Daphne has also taught at Stanford for 18 years as a Professor of Computer Science. She has beenfelicitated withvarious awards & accolades which includeACM Infosys Awards, MacArthur Foundation Fellowship and more.Her research focus lies inusing probabilistic models and machine learning to understand complex domains that involve large amounts of uncertainty. Her work at CalicoLabs involves applying machine learning to healthcare.Carla Gentryis a Data Scientist at Talent Analytics, Corps. She carries an invaluable experience of over 20 years which includes working forFortune 500 companies like Hershey, Kraft,Johnson & Johnson, Kelloggs and Firestone.She is one of the most popular personalities in Big Data community to follow on Twitter.Currently, She is the VP of Advanced Analytics R&D at SAS Institute. Radhika oversees software development in many analytical areas including Statistics, Operations Research, Econometrics, Forecasting and Data Mining. Radhikais a Member of the Board of Directors for IDeaS, a SAS Company.Yael Garten is the Director of Data Science at LinkedIn. She leads the growth and engagement team at LinkedIn. She leads a team of data scientists to drive business insights from understanding trends and data patterns. Before joining LinkedIn, Yael was pursuing her PhD in Biomedical Informatics at Stanford. She also advisesbiomedical companies to tranform their high throughput data into insights. She is also a speaker at various conferences.Claudia Perlich is the Chief Data Scientist at Distillery. She is one of the powerful women achievers in the data science industry. Having spend a major part of her life in analytics industry, Claudia marks as one of the first women to step into data science. She leads the machine learning team for Distillerys digital intelligence for marketers and media companies. With more than 50 published articles on her name Claudia is a wellwell-recognizedert in big data and machine learning.Caitlin Smallwood is the Vice President of Algorithms and Science at Netflix. At Netflix Caitlin leads a team of mathematician, statistician and data scientist. She works on algorithms research, development, predictive modeling to bring to you a seamless experience while using Netflix. She is one of the women leaders in the tech and embrace the challenges thrown her way.Shafirira Goldwasser is an American-born Israeli computer scientist and professor at MIT. She is well known for her work in cryptography and computational number theory. She is the co-inventor of zero knowledge proofs, which demonstrates validity of an assertion without revealing any additional knowledge. This always plays an important role in defining crypotgraphy protocols. Shafirira has won several awards for her work  Godel Prize, ACM Grace Murray Hopper Award, Turing Award and many more.June Andrews is a data scientist at Pinterest. June is an applied mathematician specializing in social network analysis. She has worked on search algorithms at Yelp, designed algorithms for computing large structure of networks. June has worked with LinkedIn & Yelp in the past. At Pinterest, she uses data science & machine learning to aggregate content generated by 50Bn users over a month.Alice Zheng is a Senior Manager for applied science at Amazon. Alice is a data scientist, researchers and author. Alice has worked on optimization team for Amazons Ad Platform, machine learning research scientist. She has written two books  Mastering Feature Engineering and Evaluating Machine Learning Models.In the past Alice has worked with Microsoft and Turi. She is an active member of data science meetups in Seattle.Majken Sander is a solution architect at TimeXtender. She is a business analyst and business developer with a strong analytical mind. She has worked in analytics, business intelligence, IT and software development for 20+ years. She is keen on everything data, math and data driven as a business principle.Crystal Valentine is Vice President of Technology Strategy at MapR. Crystal has a background in Big data research and practice. She is the author of various academic publications in the area of algorithm, high-performance computing and computational biology. She also has a patent for Extreme Virtual Memory.Margot Gerritsen is the Director atInstitute for Computational and Mathematical Engineering at Stanford. Margot is a well known researcher and specializes in design & analysis of efficient numerical solution methods for partial differential equations that arise in fluid dynamics. She is also the author of two books Introduction to Matrix Computations and,Numerical Analysis in Science & Engineering.Megan Price is the Executive Director at Human Rights Data Analysis Group. Megan designs &strategizes methods for data analysis of human rights data for various countries like Guatemala, Colombia, Syria, etc. Using her data science skills she has helped solve several human rights problems in these countries. She is well acclaimed for her work in Syria and Guatemala.Lori Sherer is Partner at Bain & Company. She is the leader of Bains Advanced analytics & best practices. Lori has been in the industry for over 20 years and is adept in advanced analytics & decision science. Lori is one of the leaders in analytics industry. In the past she has worked with McKinsey, FICO and RMS.Raia Hadsell is a research scientist at Google Deepmind. Raia has worked on the deepmind and robotics platform for more than 10 years. Her efforts in deep learning and reinforcement learning have won her recognition in the field of robotics. At Google, she leads a team of researchers in robot navigation and lifelong learning.
Cynthia Dwork is an American computer scientist. She is well known for her work in privacy-preserving data analysis, cryptography and distributed computing. Cynthia is a Gordon McKay Professor of Computer Science at the Harvard University andRadcliffe Alumnae Professor at the Radcliffe Institute for Advanced Study. She is the recipient of Edsger W. Dijkstra Prize.Animashree Anandkumar is a Principal Research Scientist at Amazon Web Services. Animashree was a professor at University of California Irvine before joining Amazon.She has worked on inference of graphical models, random-access algorithms and transaction tracking using timestamps. Her work has earned her recognition in the industry.In this section, I am going to introduce you tosome of the women achievers from our community.Prarthana Bhat is a Data Scientist at Flutura Desicion Science and Analytics. Prarthana is skilled in SQL, R and Business Intelligence. Prarthana is an active member on Analytics Vidhya andwas the first female data scientist to secure a rank in top 3on Analytics Vidhya. Prarthana did her BE in Computer Science fromMangalore Institute of Technology and Engineering and Diploma in Business Intelligence from NIIT. Prarthana has over 4+ years of experience.Yaasna Dua is a Data Scientist at Info Edge India Ltd. Yaasna is skilled in machine learning, Python and Statistical Inference. Yaasna is one of the highly skilled data scientist in the AV community.She secured 2nd rank in one of the hackathons making it to the top list of women data scientist on AV. Yaasna did her B.Tech from Delhi School of Engineering and has around 3 years of experience.Rashmi Nalavade is an Artificial Intelligence & ML Practitioner at L & T Technology Services. She is working with machine learning & artificial intelligence withUtilities, Pharmaceutical, Manufacturing & FMCG companies. Rashmi is an active member of Analytics Vidhya and has over 20+ years of experience in data science industry. She has worked with companies like Accenture, PwC, Tech Mahindra and NIIT in the past.Bolaka is the Tech Architecture Delivery Associate Manager at Accenture. Bolaka has been working with NLP, Predictive Analytics & Product Enginnering from past 8+years. Bolaka is also an active member of Analytics Vidhya and is skilled in Python, R & Weka. In the past, Bolaka has worked with companies like Abzooba, Ayata and others.Anchal is a Data Scientist at Emmfer Pvt Ltd. She is a keen data scientist and is a regular participant in machine learning hackathons on Analytics Vidhya. Shes passionate about data science and wants to explore the field beyond its boundaries. You can akways find her actively participating in discussions on our slack channel.Tulika is a Technical Specialist at Syntel. She often participates in Strategic Thinking competitions and almost everytime makes it to the top 10 ranks. Tulika has more than 12+ years of experience in financial & valuation research. She is well versed with Predictive Analytics & modelling techniques like Machine Learning, Linear & Logistic Regressions, Time Series Analysis & Clustering Methods.Preeti is AVP at JP Morgan Chase & Co. and she has recently joined us as a Data Science Volunteer for Mumbai. She has 10+ years of experience in Telecom & Banking domain. She loves data science and wants to evanglize data science knowledge to the world. Preeti is a graduate from Nagpur Univeristy.Tanvi Purohit is a Consultant with Deloitte. She is skilled in RDBMS and passionate about data science. In her 3 years of career in data science, she has worked in data warehousing, data analytics and is a trained strategist. Tanvi has also recently joined us as AV Volunteer for Mumbai. Tanvi completed her Bachelors in Computer Science from Univeristy of Mumbai.I hope you enjoyed reading the article. I want all women out there to get motivated & join data science industry. To motivate you further, there are several conferences & workshops which happen every year to exclusively bring together all the women data scientist & ML practitioners.Check out these conferences Women in Data Science, Women in Machine Learning, Women inBig Data and Bay Area Womens Summit.Let me know your thoughts on this article and dont forget to tag your friends.",https://www.analyticsvidhya.com/blog/2017/03/celebrating-womens-day-33-women-in-data-science-from-around-the-world-av-community/
Senior Data Scientist- Pune (3 to 5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Celebrating Womens Day: 33 Women in Data Science from around the World & AV Community|Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

 A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 	Presents and depicts the rationale of findings in easy to understand terms for the business
	Presents back results that contradict common belief, if needed
	Communicates and works with business subject matter experts and organizational leadership
Task Info : Handling Business andDatarequirements Leads discovery processes with stakeholders to identify the business requirements and the expected outcome Models and frames business scenarios that are meaningful and which impact on critical business processes and/or decisions Identifies whatdatais available and relevant, including internal and externaldatasources, leveraging newdatacollection processesDataAnalysis Develops innovative and effective approaches to solve clients analytics problems and communicates results and methodologies Works in iterative processes with the client and validates findings Develops experimental design approaches to validate finding or test hypotheses Validates analysis using scenario modelling Identifies/creates the appropriate algorithm to discover patterns Qualifies where information can be stored or what information, external to the organization, may be used in support of the use case.Communication & Presentation Presents and depicts the rationale of findings in easy to understand terms for the business Presents back results that contradict common belief, if needed Communicates and works with business subject matter experts and organizational leadershipInnovation Improves organizational performance though the application of original thinking to existing and emerging methods, processes, products and services. Employs sound judgment in determining how innovations will be deployed to produce return on investment.Typical Experience / Education Bachelor degree in mathematics, statistics or computerscienceor related field; Master degree preferred. Typically 3-5 years of relevant quantitative and qualitative research and analytics experience. The ability to come up with solutions to loosely defined business problems by leveraging pattern detection over potentially large datasets. Strong programming skills (such as Hadoop MapReduce or other bigdataframeworks, Java), and statistical modeling (like R or Python). Experience in using machine learning algorithms. Proficiency in statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and optimization algorithms
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, forecasting, hadoop, machine learning, mapreduce, multivariate analysis, optimization, predictive modeling, python, r, spss, statistical modeling, statistics
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-data-scientist-pune-3-to-5-years-of-experience/
Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning,"Learn everything about Analytics|Introduction|Table of Content|1. What is Gradient Descent?|2. Challenges in executing Gradient Descent|3. Variants of Gradient Descent algorithms|4. Implementation of Gradient Descent|5. Practical tips on applying gradient descent|6. Additional Resources|End Notes|Learn, compete, hack and get hired!","Broad applications of Optimization|2.1 Data Challenges|2.2 Gradient Challenges|2.3 Implementation Challenges|3.1 Vanilla Gradient Descent|3.2 Gradient Descent with Momentum|3.3ADAGRAD|3.4 ADAM|Share this:|Like this:|Related Articles|Senior Data Scientist- Pune (3 to 5 years of experience)|Lead Big Data Engineer-Bengaluru/ Mumbai /Gurgaon (4-8 Years Of Experience)|
Faizan Shaikh
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Optimization is always the ultimate goal whether you are dealing with a real life problem or building a software product. I, as a computer science student, always fiddled with optimizing my code to the extent that I could brag about its fast execution.Optimization basically means getting the optimal output for your problem. If you read the recent article on optimization, you would be acquainted with how optimization plays an important role in our real-life.Optimization in machine learning has a slight difference. Generally, whileoptimizing, we know exactly how our data looks like and what areas we want to improve. But in machine learning we have no clue how our new data looks like, let alone try to optimize on it.So in machine learning, we perform optimization on the training data and check its performance on a new validation data.There are variouskinds of optimization techniqueswhich are applied across various domains such asOptimization has many more advanced applications like deciding optimal route for transportation, shelf-space optimization, etc.Many popular machine algorithms depend upon optimization techniques such as linear regression, k-nearest neighbors, neural networks, etc. The applications of optimization are limitless and is a widely researched topic in both academia and industries.In this article, we will look at a particular optimization technique called Gradient Descent. It is the most commonly used optimization technique when dealing with machine learning.To explain Gradient Descent Ill use the classic mountaineering example.Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake?SourceThe best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake.To represent this graphically, noticethe below graph.SourceLet us now map this scenario in mathematical terms.Suppose we want to find out the best parameters (1) and (2) for our learning algorithm. Similar to the analogy above, we see we find similar mountains and valleys when we plot our cost space. Cost space is nothing but how our algorithm would perform when we choose a particular value for a parameter.So on the y-axis, we have the cost J() against our parameters 1 and 2 on x-axis and z-axis respectively. Here, hills are represented by red region, which have high cost, and valleys are represented by blue region, which have low cost.Now there are many types of gradient descent algorithms. They can be classified by two methods mainly:In full batch gradient descent algorithms, you use whole data at once to compute the gradient, whereas in stochastic you take a sample while computing the gradient.Gradient descent requires calculation of gradient by differentiation of cost function. We can either use first order differentiation or second order differentiation.Gradient Descent is a sound technique which works in most of the cases. But there are many cases where gradient descent does not work properly or fails to work altogether. There are three main reasons when this would happen:Let us look at most commonly used gradient descent algorithms and their implementations.This is the simplest form of gradient descent technique. Here, vanilla means pure / without any adulteration. Its main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.Lets look at its pseudocode.Here, we see that we make an update to the parameters by taking gradient of the parameters. And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. Learning rate is a hyper-parameter and should be treated with care when choosing its value.SourceHere, we tweak the above algorithm in such a way that we pay heed to the prior step before taking the next step.Heres a pseudocode.Here, our update is the same as that of vanilla gradient descent. But we introduce a new term called velocity, which considers the previous update and a constant which is called momentum.SourceADAGRAD uses adaptive technique for learning rate updation. In this algorithm, on the basis of how the gradient has been changing for all the previous iterations we try to change the learning rate.Heres a pseudocodeIn the above code, epsilon is a constant which is used to keep rate of change of learning rate in check.ADAM is one more adaptive technique which builds on adagrad and further reduces it downside. In other words, you can consider this as momentum + ADAGRAD.Heres a pseudocode.Here beta1 and beta2 are constants to keep changes in gradient and learning rate in checkThere are also second order differentiation method like l-BFGS. You can see an implementation of this algorithm in scipy library.We will now look at a basic implementation of gradient descent using python.Here we will use gradient descent optimization to find our best parameters for our deep learning model on an application of image recognition problem. Our problem is an image recognition, to identify digits from a given 28 x 28 image. We have a subset of images for training and the rest for testing our model. In this article we will take a look at how we define gradient descent and see how our algorithm performs. Refer this article for an end-to-end implementation using python.Here is the main code for defining vanilla gradient descent,Now we break it down to understand it better.We defined a function sgd with arguments as cost, params and lr. These represent J() as seen previously,  i.e. the parameters of our deep learning algorithm and our learning rate. We set default learning rate as 0.05, but this can be changed easily as per our preference.We then defined gradients of our parameters with respect to the cost function. Here we used theano library to find gradients and we imported theano as Tand finally iterated through all the parameters to find out the updates for all possible parameters. You can see that we use vanilla gradient descent here.We can use this function to then find the best optimal parameters for our neural network. On using this function, we find that our neural network does a good enough job in finding the digits in our image as seen belowIn this implementation, we see that on using gradient descent we can get optimal parameters for our deep learning algorithm.Each of the above mentioned gradient descent algorithms have their strengths and weaknesses. Ill just mention some quick tips which might help you choose the right algorithm.Now there are many reasons why a neural network fails to learn. But it helps immensely if you can monitor where your algorithm is going wrong.When applying gradient descent, you can look at these points which might be helpful in circumventing the problem:I hope you enjoyed reading this article. After going through this article, you will be adept withthe basics of gradient descent and its variants. I have also given a practical tips for implementing them. Hope you found them helpful!If you have any questions or doubts, feel free to post them in the comments below.",https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/
Lead Big Data Engineer-Bengaluru/ Mumbai /Gurgaon (4-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning|Consultant, Data Analytics- Bangalore (3-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 
Task Info : Our Big Data capability team is hiring technologists who can produce beautiful & functional code to solve complex analytics problems. If you are an exceptional developer and who loves to push the boundaries to solve complex business problems using innovative solutions, then we would like to talk with you.RESPONSIBILITIESProvides technical leadership in Big Data space (Hadoop Stack like M/R, HDFS, Pig, Hive, HBase, Flume, Sqoop, etc..NoSQL stores like Cassandra, HBase etc) across Fractal and contributes to open source Big Data technologies.Visualize and evangelize next generation infrastructure in Big Data space (Batch, Near RealTime, RealTime technologies).Evaluate and recommend Big Data technology stack that would align with companys technologyPassionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigmsDrive significant technology initiatives end to end and across multiple layers of architectureProvides strong technical leadership in adopting and contributing to open source technologies related to BigData across the company.Provide strong technical expertise (performance, application design, stack upgrades) to lead Platform EngineeringDefine and Drive best practices that can be adopted in Big Data stack. Evangelize best practices across teams and BUs.Provide technical leadership and be a role model to data engineers pursuing technical career path in engineeringProvide/inspire innovations that fuel the growth of FractalQUALIFICATIONS & EXPERIENCE4  8 years of demonstrable experience designing technological solutions to complex data problems, developing & testing modular, reusable, efficient and scalable code to implement those solutions.Ideally, This Would Include Work On The Following TechnologiesExpert-level proficiency in at-least one of Java, C++ or Python (preferred). Scala knowledge a strong advantage.Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies  one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc.Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasksAbility to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works.A technologist  Loves to code and design In addition, the ideal candidate would have great problem-solving skills, and the ability & confidence to hack their way out of tight corners.Relevant ExperienceJava or Python or C++ expertiseLinux environment and shell scriptingDistributed computing frameworks (Hadoop or Spark)Cloud computing platforms (AWS)Desirable Experience (would Be a Plus)Statistical or machine learning DSL like RDistributed and low latency (streaming) application architectureRow store distributed DBMSs such as CassandraFamiliarity with API designEDUCATIONB.E/B.Tech/M.Tech in Computer Science or related technical degree OR Equivalent
College Preference : no-bar
Min Qualification : ug
Skills : aws, bigdata, c++, hadoop, hdfs, hive, java, linux, machine learning, oozie, pig, python, Scala, sqoop, statistics
Location : Bengaluru, Gurugram, Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/lead-big-data-engineer-bengaluru-mumbai-gurgaon-4-8-years-of-experience/
"Consultant, Data Analytics- Bangalore (3-6 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead Big Data Engineer-Bengaluru/ Mumbai /Gurgaon (4-8 Years Of Experience)|Senior Consultant, Risk Analytics- Gurgaon (4-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : Job Description and Responsibilities:-Support business in their decision-making needs (Transactional/ Operational) by providing insights from Data. Assess solutions for business opportunities by leveraging data (Transactional Data/ Big Data, External and Internal Data, Structured and Unstructured Data) and analytics methods. Provides advice, training and support in analytics. Furthermore, will build the application related part of analytics knowledge base in the respective domain.Dimensions Relating to Know-How: Technical Know-How, Management Breadth, Human Relation SkillsAbility to communicate clearly (oral, written and remote) to remote audience with clear business articulationExpertise in multiple tools and techniques  SAS/R/ Python, Tableau/Qlikview, SQL, StatisticsUnderstands and supports many business processes with analytics solutions/ deliveriesManaging direct and indirect stakeholders (support functions) like Finance, IT etc.Collaborates and influences with known stakeholdersIdentify and replicate best practices across the domain/tools techniques descriptive and predictive analysis.Keep abreast with latest develop in analytics domain, emerging technologyAdvises business champions in the domain where analytics can contributeYear of industry Exp: 3  6 YearsRelevant Exp in analytics domain: 2-3 yearsDimensions Relating to Problem Solving: Thinking environment & -challengeThinking environment: Impact of cross domain at a business/sector levelOrganizes the required resource and capacity for meeting the solutions deliveryAdopts to the changes in the environment for the organisational strategy for EA (Domain)Understanding the business implications of solutions, and providing directions in defining the solutions to support the business requirementsDimensions Relating to Accountability: Freedom to Act, Magnitude, Nature of impactExecutes analytics roadmap for the business process within the domainAbility to independently contributes towards the project taking accountability on accuracy and timeliness 
College Preference : no-bar
Min Qualification : ug
Skills : python, qlikview, r, sas, sql, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/consultant-data-analytics-bangalore-3-6-years-of-experience/
"Senior Consultant, Risk Analytics- Gurgaon (4-6 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant, Data Analytics- Bangalore (3-6 Years Of Experience)|Statistical programmer- Bangalore, pune , Mumbai, Hyderabad (2-3 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Position ExpectationsInterface with clients to understand business problems and translate them into analytical problems that can be addressed by data analyticsDevelop & conceptualize sophisticated & innovative analytical solutions that integrate diverse information solutions and generate actionable insightsProvide high-end consulting to clients to help them sharpen their business strategy by implementing analytical models that support the realization of business objectivesManage end to end project delivery with project teams and build client relationshipLead with limited direction, usually within a complex environment, to drive rigorous, fact-based solutions and insights for clientsSupport business development teams in preparing client pitchesBuild a team of skilled and motivated analysts/senior analysts to deliver large data-based projects to the financial services clients; outline goals, conduct the performance review and appraisal of the team and coach employeesBe the thought leader in the focus area, the go to person for internal teams, Generate knowledge sharing trends in the team, participate in developing white papers & share new trends in analytics with colleagues, evolve ones own opinion on the conceptsQualification & Experience: Experience in delivery/account management in a data analytics environmentStrong business acumen and ability to translate data insights into meaningful business recommendations Demonstrated ability to handle multiple types of dataShould have hands on experience in R and modelling techniques such as regression, multinomial regression, etcTechnical mastery in tools such as SAS/SPSS/R and Tableau/ Microstrategy/ Spotfire/ Business ObjectsExperience with transactional data and customer dataBanking experienceExperience with Microsoft Word, Excel (including VBA), PowerPoint applicationsStrong interpersonal skills and ability to communicate effectively with clientsExcellent analytical and problem solving skillsShould have worked in the analytics industry for 4-6 yearsExposure to Customer insights research/ analytics would be an added advantageAbility to mentor and lead teamsExcellent written and verbal communication skills
College Preference : no-bar
Min Qualification : ug
Skills : Advanced Excel, Data analytics, microstrategy, r, regression, sas, spss, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-consultant-risk-analytics-gurgaon-4-6-years-of-experience/
"Statistical programmer- Bangalore, pune , Mumbai, Hyderabad (2-3 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Consultant, Risk Analytics- Gurgaon (4-6 Years Of Experience)|Sr. Analyst  Analytics- Gurgaon (3-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : 
Task Info : Summary of Job Responsibilities:Working under the direction of Senior Statistical Programmer/Team Lead, the statistical programmer conducts programming activities for a trial, early phase project, indication, or publication activities.Read and understand the program specifications document.Prepare SAS analysis datasets, tables, listings, and figures as per specifications.Create programs to create graphs and tables that are required in CSRs, safety reports, efficacy reports, etc. ensuring on-time quality delivery.Validate and transform data sets as per client assignment specifications.Validate tables, listings, and figures as per client assignment specifications.Ability to work on data migration from legacy datasets to standards such as CDISC or any other client specific standard.Co-ordinate with the client and US-team for clarity of specifications, data issues, outliers, reviews, schedules, etc.Perform all the above tasks using standard operating procedures (SOPs) as defined in Quality Management System or the respective client(s) as applicable.Qualifications and Experience Education:Bachelors or Masters degree in Computer Science, Statistics or related Health Sciences field Experience:A minimum of 23 years of SAS programming experience with clinical trial data.General knowledge of regulatory requirements and drug development process.Fair knowledge of Clinical trial domain and good SASprogramming skills.Ability to work independently.
College Preference : no-bar
Min Qualification : ug
Skills : sas, statistics
Location : Bengaluru, Hyderabad, Mumbai, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/statistical-programmer-bangalore-pune-mumbai-hyderabad-2-3-years-of-experience/
Sr. Analyst  Analytics- Gurgaon (3-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Statistical programmer- Bangalore, pune , Mumbai, Hyderabad (2-3 Years Of Experience)|Data Miner- Chennai (3-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Job Description and Responsibilities:-The Sr. Analyst is responsible for delivering actionable, insightful information to the stakeholders. This position requires to poses excellent data interpretation skills to be utilized on databases and generate actionable insights for the business. You will work closely with senior leaders; designing and delivering key metrics and insights.Qualification:Excellent analytical and problem solving skills, including the ability to disaggregate issues, identify root causes and recommend solutions.Should have been involved in report designing, dashboard development in his past experience. Good understanding of KPI in the business and interpret the same.Interprets data, identifies trends, and translates analysis into recommendations to improve effectiveness and support business strategy. Exceptional analytical thinking with experience working in a global environment.Shall be responsible to be creative in data model creation on excel.Creative and action-oriented mindset 3-5 years of professional work experience with a reputed analytics firm in the field of business intelligence and/or Advanced AnalyticsUnderstanding of Sales, marketing & digital data metrics would be added advantage.Requirements: Minimum 3 years of relevant work experience, preferably a Masters Degree.Exceptional analytical thinking with experience working in a global environment.Must have experience in data driven processes and reporting systems. Experience with multiple complex database, reporting and analytic systemOutstanding oral and written communications skills, as well as excellent presentationskills; ability to lead meetings internally and externally and executive briefings Has a thorough understanding of the people, processes, technology issues and a passion regarding customer service (internal and external) and delivery/execution.Ability to leverage technology to solve for reporting needs  Advanced Excel, Office, SQL.Would be preferred if shall have knowledge of Tableau.
College Preference : no-bar
Min Qualification : pg
Skills : Advanced Excel, analytics, business intelligence, data modeling, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/sr-analyst-analytics-gurgaon-3-5-years-of-experience/
Data Miner- Chennai (3-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Analyst  Analytics- Gurgaon (3-5 Years Of Experience)|MongoDB Administrator / Developer-Bangalore/Ahmedabad/Gurgaon (5+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : What You Will Be Doing?Work with clients to understand their business challenges and formulate business problemsManaging & strategizing the analytics side of the businessLead and Deliver solutions to the client individually/as a team and managing all deliverablesDesign the algorithms for product developmentCoordinate with different teams and full-fill the end requirements of clientBuild new analytical products & contribute in product development Requirements3-6 years of experience in leading analytical projects and delivering value to customersStrong understanding gained through experience on advanced analytics (Predictive Modelling (Logistic regression), Segmentation, forecasting, data mining, and optimization) techniquesHands-on experience in using software packages such as SAS, R,Rapidminer for analytical modelling and data managementExperience in using Business Intelligence tools such as: SAS, Microsoft, Tableau for business applicationsStrong aptitude for analytical problem solvingExceptional communication skills. Demonstrable ability to communicate complex concepts to business audiencesAbility to work effectively in a global team
College Preference : no-bar
Min Qualification : ug
Skills : analytics, data mining, forecasting, logistic regression, predictive modeling, r, sas, segmentation, tableau
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/data-miner-chennai-3-6-years-of-experience/
MongoDB Administrator / Developer-Bangalore/Ahmedabad/Gurgaon (5+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Miner- Chennai (3-6 Years Of Experience)|Hadoop Administrator- Ahmedabad (6-10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Job Description:Minimum 5 Years of Total experience.2+ Years of experience into MongoDB Administrator or Developer.Good knowledge of Ops Manager setup in HA mode with load balancer, Backups and restoreGood knowledge of performance tuning of mongo dB instance, Configuration parameter, Schema design, indexing.Query tuning, Performance Troubleshooting skills, locking, profiler, auditing, Knowledge of mtools.Knowledge of ldap/password challenge authentication, ssl setup.Knowledge of Unix scriptingMongoDB Certified preferred
College Preference : no-bar
Min Qualification : ug
Skills : MongoDB, unix
Location : Ahmedabad, Bengaluru, Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/mongodb-administrator-developer-bangaloreahmedabadgurgaon-5-years-of-experience/
Hadoop Administrator- Ahmedabad (6-10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|MongoDB Administrator / Developer-Bangalore/Ahmedabad/Gurgaon (5+ Years Of Experience)|Analytics Consultant- Bangalore (5-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  10 years
Requirements : 
Task Info : Job Responsibilities of a Hadoop Administrator:Responsible for implementation and support of the Enterprise Hadoop environment.Involves designing, capacity arrangement, cluster set up, performance fine-tuning, monitoring, structure planning, scaling and administration.The administrator consultant will work closely with infrastructure, network, database, business intelligence and application teams to ensure business applications are highly available and performing within agreed on service levels.Need to implement concepts of Hadoop eco system such as YARN, MapReduce, HDFS, HBase, Zookeeper, Pig and Hive.In charge of installing, administering, and supporting Windows and Linux operating systems in an enterprise environment.Accountable for storage, performance tuning and volume management of Hadoop clusters and MapReduce routines.In command of setup, configuration and security for Hadoop clusters using Kerberos.Monitor Hadoop cluster connectivity and performance.Manage and analyze Hadoop log files.File system management and monitoring.Develop and document best practices.HDFS support and maintenance.Setting up new Hadoop users.Responsible for the new and existing administration of Hadoop infrastructure.Include DBA Responsibilities like data modelling, design and implementation, software installation and configuration, database backup and recovery, database connectivity and security.
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, database management, hadoop, hbase, hdfs, hive, linux, mapreduce, pig
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/hadoop-administrator-ahmedabad-6-10-years-of-experience/
Analytics Consultant- Bangalore (5-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Administrator- Ahmedabad (6-10 Years Of Experience)|Senior Software Engineer (Big Data)- Bangalore (2-4 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  7 years
Requirements : 
Task Info : Job descriptionWorks effectively both independently and as a member of a cross functional teamAct as an Analytics Consultant for providing solutions to Clients business needs.Collate the data into coherent and logical structures to produce periodic as well as one-off analyses to aid the decision makersSupporting the analytics needs of business by analyzing data from multiple sources including Capillary CRM data & the external data from the clients.Collate the data into coherent and logical structures to produce periodic as well as one-off analyses to aid the decision makers.Effective and persuasive presentations (verbal and written) on the certain objective provided by the Client.SkillsCandidates with 5 -7 years of relevant experience in Analytics consultancyPrior experience in data analytic tools & delivering presentations to the clientExperience in handling Client in USA/Europe/Asia  Pacific markets for analytics deliverable with atleast 2 years of Client exposure will be a plusAptitude for problem solving and quantitative skillsGood written and interpersonal skillsKnowledge of atleast one data analysis platform : SAS / R / SQL and expertize in MS Excel & MS Powerpoint is a must Experience of working on Advanced analytics (Statistical modeling)Able to prioritize and execute tasks in a high-pressure environmentSelf motivated, Inquisitive and curious, Goal-oriented, detail-oriented, ambitious, and strong ability to follow-through on commitments
College Preference : no-bar
Min Qualification : ug
Skills : analytics, Data analytics, excel, Power point, r, sas, sql, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/analytics-consultant-bangalore-5-7-years-of-experience/
Senior Software Engineer (Big Data)- Bangalore (2-4 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Consultant- Bangalore (5-7 Years of Experience)|Predictive Modeling & Optimisation Consultant- Bangalore (3+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : 1. Own development, design, scaling and maintenance of recommender delivery system that serves personalized recommendations to our Cloud CRM core products.Skills:
College Preference : tier1-any
Min Qualification : ug
Skills : api, data structure, hadoop, hive, J2EE, java, MongoDB, spark
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/senior-software-engineer-big-data-bangalore-2-4-years-of-experience/
Predictive Modeling & Optimisation Consultant- Bangalore (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Software Engineer (Big Data)- Bangalore (2-4 Years of Experience)|Machine Learning Lead- Bangalore (5+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Role and Responsibilities Understand business requirements to translate business problems into analytics problems and construct analysis road-map based on the business context Manage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis Analyse big data using statistics, econometrics, mathematics, operations research, and text mining techniques Develop good visualization to communicate business insights from analysis and make actionable recommendations Help deploy analytics solutions and enable tracking of business outcomes to measure return on investment Keep up with cutting edge analytics techniques and tools in the continuously evolving area of decision science Present/Advise/Interpret/Justify on analytics solutions from a project to Client/Internal Stakeholder Accountability & Responsibility for specific roles played in a project Support technical coaching of analysts.Qualification  3+ years of work in statistical modelling and business analysis role Bachelors/Masters in Economics, Engineering, Mathematics, IT, Statistics and MBA/PGDBMSkills Aware of various statistical analysis methods such as Regression, Logistic regression, decision trees, other segmentation methods and technical expertise in one of these techniques Simulation & Mathematical optimization models Time series forecasting methods Hands on experience in statistical modelling software such as SAS or R Strong Microsoft Excel, Access and PowerPoint skills (VBA & SQL experience preferred)Desirables1. Individuals with specialised industry expertise in atleast one of the areas which includes Technology, Financial Services, Insurance, Retail, Consumer Packaged Goods, Education and Hospitality2. Individuals with Specific solutions experience in Marketing Analytics, Sales Analytics, Risk Management, Supply Chain Analytics, Employee Engagement Analytics and Customer Analytics.3. Risk Modelling Responsible for running the risk model, preparing in depth model review, documents and reports of loss estimates of clients portfolio and developing and performing various quality assurance checks Assisting in implementation of various risk models with model, research, analytics and software team.Responsibilities include reviewing the theoretical assumptions Responsible for creation of test plan according to requirement of the model; verify, validate and execute the entire plan Expertise in conceptualizing business problem into a tangible deliverable (an exhaustive report), model documentation and validation report writing, exposure to BASEL norms Responsible for setting up independent benchmarking tools for testing of various scenarios and boundary conditions of complex models and the implementation of the model
College Preference : no-bar
Min Qualification : ug
Skills : decision trees, logistic regression, r, regression, risk modeling, sas, segmentation, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/predictive-modeling-optimisation-consultant-bangalore-3-years-of-experience/
Machine Learning Lead- Bangalore (5+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Predictive Modeling & Optimisation Consultant- Bangalore (3+ Years of Experience)|Analyst- Bangalore (Upto 1 year of experience/ Fresher)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Role and Responsibilities Understand business requirements to translate business problems into analytics problems and construct analysis road-map based on the business context Manage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis Analyse big data using statistics, econometrics, mathematics, operations research, and text mining techniques Develop good visualization to communicate business insights from analysis and make actionable recommendations Help deploy analytics solutions and enable tracking of business outcomes to measure return on investment Keep up with cutting edge analytics techniques and tools in the continuously evolving area of decision science Direct/Lead Project teams consisting of analysts & consultants in terms of planning, execution, motivating to perform and managing quality excellence Effective utilization and resource management by monitoring and assessing project resources, relinquish resources or request additional resources as needed Mitigate the risks associated with successful project plan completion Client/Customer engagement & management Assist in developing/coaching individuals technically as well as on soft skills during the project.Specific Responsibilities Solving unstructured problems by applying knowledge of data mining, natural language processing algorithms and machine learning. Ability to connect data of different types, working with potentially incomplete data sources and cleaning data sets to discover new insights.Qualification 5+ years of experience working with Machine learning / Advanced analytics domain with Python coding proficiencySkills Strong knowledge and proficiency of Python language in some kind of data analytics domain Experience of working on Machine learning projects using R / Python Working knowledge of machine learning implementation in BIG data / Live business scenario Understanding of BIG data technology basics Strong Analytical skills Communication: Strong English communication and ability to articulate both technical and domain matters Deep knowledge of machine learning methods and applications Strong programming skills Desirables Working knowledge of Java / Hadoop / Spark App development exposure Strong academic background in machine learning Hackathon / Competition enthusiasts
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, hadoop, java, machine learning, python, r, spark
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/machine-learning-lead-bangalore-5-years-of-experience/
Analyst- Bangalore (Upto 1 year of experience/ Fresher),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Lead- Bangalore (5+ Years of Experience)|How to read most commonly used file formats in Data Science (using Python)?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  1 years
Requirements : 
Task Info : We are looking for graduates with less than one year of experience for an opportunity in the Analytics Consulting space.Role and Responsibilities
 Understand business requirements to translate business problems into analytics problems and construct analysis road-map based on the business context
 Manage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis Analyse big data using statistics, econometrics, mathematics, operations research, and text mining techniques Develop good visualization to communicate business insights from analysis and make actionable recommendations Help deploy analytics solutions and enable tracking of business outcomes to measure return on investment Keep up with cutting edge analytics techniques and tools in the continuously evolving area of decision scienceDesirable:
 Proficiency in programming (SAS / R or any other data mining language) and databasemanagement skills (SQL programming) Prior work experience in big data management, business reporting, visualization and / orpredictive analytics is a big plus Functional domain experience in marketing, customer relationship management, sales, supplychain, risk management or pricing function  preferably in financial services, technology, retail,CPG, manufacturing, healthcare or telecom industryQualification
 0-1 Years of Analytics Experience
 Bachelors/Masters in Economics, Engineering, Mathematics, IT, Statistics
Skills
 Has a flair for numbers and is excited about solving business challenges using data
 Advanced degree in a quantitative discipline or business with strong foundation in statisticalmethods & / or optimization techniques & / or machine learning algorithms with good conceptual understanding of their applications Very good interpersonal skills, able to influence without authority, adaptable to changes and possess initiative and creative approaches Strong analytical, presentation and communication skills Self-starters, willing to work in a start up with the motivation to build the organization as the best in the industry.
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, data mining, machine learning, predictive modeling, r, sas, sql, visualization
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/03/analyst-bangalore-upto-1-year-of-experience-fresher/
How to read most commonly used file formats in Data Science (using Python)?,"Learn everything about Analytics|Introduction|Table of Contents|1. What is a file format?|2. Why should a data scientist understand different file formats?|3. Different file formats and how to read them in Python|Learn, compete, hack and get hired","3.1 Comma-separated values|3.2 XLSX files|3.3 ZIP files|3.4 Plain Text (txt) file format|3.5 JSON file format|3.6 XML file format|3.7 HTML files|3.8 Image files|3.9 Hierarchical Data Format (HDF)|3.10 PDF file format|3.11 DOCX file format|3.12 MP3 file format|3.13 MP4 file format|End Notes|Share this:|Like this:|Related Articles|Analyst- Bangalore (Upto 1 year of experience/ Fresher)|Introductory guide on Linear Programming for (aspiring) data scientists|
Ankit Gupta
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",What is Spreadsheet File Format?|What is Archive File format?,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you have been part of the data science(or any data!) industry, you would know the challenge of working with different data types. Different formats, different compression, different parsing on different systems  you could be quickly pulling your hair! Oh and I have not talked about the unstructured data or semi-structured data yet.For any data scientist or data engineer, dealing with different formats can becomea tedious task. In real-world, people rarely get neat tabular data. Thus,it is mandatory for any data scientist (or a data engineer) to be aware of different file formats, common challenges in handling them and the best / efficient ways to handle this data in real life.This article provides common formats a data scientist or a data engineer must be aware of.I will first introduceyou todifferent common file formats used in the industry. Later, well see how to read these file formats in Python.P.S. In rest of this article, I will be referring to a data scientist, but the same applies to a data engineer or any data science professional.A file format is a standard way in which information is encoded for storage in a file. First, the file format specifies whether the file is a binary or ASCII file. Second, it shows how the information is organized. For example, comma-separated values (CSV) file format stores tabular data in plain text.To identify a file format, you can usually look at the file extension to get an idea. For example, a file saved with name Data in CSV format will appear as Data.csv. By noticing .csv extension we can clearly identify that it is a CSV file and data is stored in a tabular format.Usually, the files you will come across will depend on the application you are building.For example, in animage processingsystem, you need image files as input and output. So you will mostly see files in jpeg, gif or pngformat.As a data scientist, you need to understand the underlying structure of various file formats, their advantages and dis-advantages. Unless you understand the underlying structure of the data, you will not be able to explore it. Also, at times you need to make decisions about how to store data.Choosing the optimal file format for storing data can improve the performance of your models in data processing.Now, we willlook at the following file formats and how to read them in Python:Comma-separated values file format falls under spreadsheet file format.In spreadsheet file format, data is stored in cells. Each cell is organized in rows and columns. A column in the spreadsheet file can have different types. For example, a column can be of string type, a date type or an integer type. Some of the most popular spreadsheet file formats are Comma Separated Values ( CSV ), Microsoft Excel Spreadsheet (xls ) and Microsoft Excel Open XML Spreadsheet ( xlsx ).Each line in CSV file represents an observation or commonly called a record. Each record may contain one or more fields which are separated by a comma.Sometimes you may come across files where fields are not separated by using a comma but they are separated using tab. This file format is known as TSV (Tab Separated Values) file format.The below image shows a CSV file which is opened in Notepad.Reading the data from CSV in PythonLet us look athow to read a CSV file in Python.For loading the data you can use the pandas library in python.df = pd.read_csv(/home/Loan_Prediction/train.csv)Above code will load the train.csv file in DataFrame df.XLSX is a Microsoft Excel Open XML file format. It also comes under the Spreadsheet file format. It is an XML-based file format created by Microsoft Excel. The XLSX format was introduced with Microsoft Office 2007.In XLSX data is organized under the cells and columns in a sheet. Each XLSX file may contain one or more sheets. So a workbook can contain multiple sheets.The below image shows a xlsx file which is opened in Microsoft Excel.In above image, you can see that there are multiple sheets present (bottom left) in this file, which are Customers, Employees, Invoice, Order. The image shows the data of only one sheet  Invoice.Reading the data from XLSX fileLets load the data from XLSX file and define the sheet name. For loading the data you can use the Pandas library in python.df = pd.read_excel(/home/Loan_Prediction/train.xlsx, sheetname = Invoice)Above code will load the sheet Invoice from train.xlsx file in DataFrame df.ZIP format isan archive file format.In Archive file format, you create a file that contains multiple files along with metadata. An archive file format is used to collect multiple data files together into a single file. This is done for simply compressing the files to use less storage space.There are many popular computer data archive format for creating archive files. Zip, RAR and Tar beingthe most popular archive file format for compressing the data.So, a ZIP file format is a lossless compression format, which means that if you compress the multiple files using ZIP format you can fully recover the data after decompressing the ZIP file. ZIP file format uses many compression algorithms for compressing the documents. You can easily identify a ZIP file by the .zip extension.Reading a .ZIP file in PythonYou can read a zip file by importing the zipfile package. Below is the python code which can read the train.csv file that is inside the T.zip.Here, I have discussed one of the famous archive format and how to open it in python. I am not mentioning other archive formats. If you want to read about different archive formats and their comparisons you can refer this link.In Plain Text file format, everything is written in plain text. Usually, this text is in unstructured form and there is no meta-data associated with it. The txt file format can easily be read by any program. But interpreting this is very difficult by a computer program.Lets take a simple example of a text File.The following example shows text file data that contain text:Suppose the above text written in a file called text.txt and you want to read this so you can refer the below code.JavaScript Object Notation(JSON) is a text-based open standard designed for exchanging the data over web. JSON format is used for transmitting structured data over the web. The JSON file format can be easily read in any programming language because it is language-independent data format.Lets take an example of a JSON fileThe following example shows how a typical JSON file stores information of employees.Reading a JSON fileLets load the data from JSON file. For loading the data you can use the pandas library in python.df = pd.read_json(/home/kunal/Downloads/Loan_Prediction/train.json)XML is also known as Extensible Markup Language. As the name suggests, it is a markup language. It has certain rules for encoding data. XML file format is a human-readable and machine-readable file format. XML is a self-descriptive language designed for sendinginformation over the internet. XML is very similar to HTML, but has some differences. For example, XML does not use predefined tags as HTML.Lets take the simple example of XML File format.The following example shows anxml document that contains the information of an employee.The <?xml version=1.0?> is a XML declaration at the start of the file (it is optional). In this deceleration, version specifies theXML version and encoding specifies the character encoding used in the document. <contact-info> is a tag in this document. Each XML-tag needs to be closed.Reading XML in pythonFor reading the data from XML file you can import xml.etree. ElementTree library.Lets import anxml file called train and print its root tag.HTML stands for Hyper Text Markup Language. It is the standard markup language which is used for creating Web pages. HTML is used to describe structure of web pages using markup. HTML tags are same as XML but these are predefined. You can easily identify HTML document subsection on basis of tags such as <head> represent the heading of HTML document. <p> paragraph paragraph in HTML. HTML is not case sensitive.The following example shows an HTML document.Each tag in HTML is enclosed under the angular bracket(<>). The <!DOCTYPE html> tag defines that document is in HTML format. <html> is the root tag of this document. The <head> element contains heading part of this document. The <title>, <body>, <h1>, <p> represent the title, body, heading and paragraph respectively in the HTML document.Reading the HTML fileFor reading the HTML file, you can use BeautifulSoup library. Please refer to this tutorial, which will guide you how to parse HTML documents.Beginners guide to Web Scraping in Python (using BeautifulSoup)Image files are probably themost fascinating file format used in data science. Anycomputer vision application is based on image processing. So it is necessary to know differentimage file formats.Usual image files are3-Dimensional, having RGB values. But, they can also be 2-Dimensional (grayscale) or 4-Dimensional (having intensity)  an Image consisting of pixels and meta-data associated with it.Each image consists one or more frames of pixels. And each frame is made up of two-dimensional array of pixel values. Pixel values can be of any intensity. Meta-data associated with an image, can be an image type (.png) or pixel dimensions.Lets take the example of an image by loading it.Now, lets check the type of this image and its shape.If you want to read about imageprocessing you can refer this article. This article will teach you image processing with an example  Basics of Image Processing in PythonIn Hierarchical Data Format ( HDF ), you can store a large amount of data easily. It is not only used for storing high volumes or complex data but also used for storing small volumes or simple data.The advantages of using HDF are as mentioned below:There are multipleHDF formats present. But, HDF5 is the latest version which is designed to address some of the limitations of the older HDF file formats. HDF5 format has some similarity with XML. Like XML, HDF5 files are self-describing and allow users to specify complex data relationships and dependencies.Lets take the example of an HDF5 file format which can be identified using .h5 extension.Read the HDF5 fileYou can read the HDF file using pandas. Below is the python code can load the train.h5 data into the t.t = pd.read_hdf(train.h5)PDF (Portable Document Format) is an incredibly useful format used for interpretation and display of text documents along with incorporated graphics. A special feature of a PDF file is that it can be secured by a password.Heres an example of a pdf file.
Reading a PDF fileOn the other hand, reading a PDF format through a program is a complex task. Although there exists a library which do a good job in parsing PDF file, one of them is PDFMiner. To read a PDF file through PDFMiner, you have to:Microsoft word docx file is another file format which is regularly used by organizations for text based data. It has many characteristics, like inline addition of tables, images, hyperlinks, etc. which helps in making docx an incredibly important file format.The advantage of a docx file over a PDF file is that a docx file is editable. You can also change a docx file to any other format.Heres an example of a docx file:Reading a docx fileSimilar to PDF format, python has a community contributed library to parse a docx file. It is called python-docx2txt.Installing this library is easy through pip by:To read a docx file in Python use the following code:MP3 file format comes under the multimedia file formats. Multimedia file formats are similar to image file formats, but they happen to be one the most complex file formats.In multimedia file formats, you can store variety of data such as text image, graphical, video and audio data. For example, A multimedia format can allow text to be stored as Rich Text Format (RTF) data rather than ASCII data which is a plain-text format.MP3is one of the most common audio coding formats for digital audio. A mp3 file format uses the MPEG-1 (Moving Picture Experts Group  1) encoding format which is a standard for lossy compression of video and audio. In lossy compression, once you have compressed the original file, you cannot recover the original data.A mp3 file format compresses the quality of audio by filtering out the audio which can not be heard by humans. MP3 compression commonly achieves 75 to 95% reduction in size, so it saves a lot of space.mp3 File Format StructureAmp3 file is made up of several frames. A frame canbe further divided into a header and data block. We call these sequence of frames an elementary stream.A header in mp3 usually, identify the beginning of a valid frame and a data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. If you want to know more about mp3 file structure you can refer this link.Reading the multimedia files in pythonFor reading or manipulating the multimedia files in Python you can use a library called PyMedia.MP4 file format is used to store videos and movies. It containsmultiple images (called frames), which play in form of a video as per a specific time period. There are two methods for interpreting a mp4 file. One is a closed entity, in which the whole video is considered as a single entity. And other is mosaic of images, where each image in the video is considered as a different entity and these images are sampled from the video.Heres is an example of mp4 video
Reading an mp4 fileMP4 also hasa community built library for reading and editing mp4 files, called MoviePy.You can install the library from thislink. To read a mp4 video clip,in Python use the following code.You can then display this in jupyter notebook as belowIn this article, I have introduced you to some of the basic file formats, which are used by data scientist on a day to day basis. There are many file formats I have not covered. Good thing is that I dont need to cover all of them in one article.I hope you found this article helpful. I would encourage you to explore more file formats. Good luck! If you still have any difficulty in understanding a specific data format, Id like to interact with you in comments. If you have any more doubts or queries feel free to drop in your comments below.",https://www.analyticsvidhya.com/blog/2017/03/read-commonly-used-formats-using-python/
Introductory guide on Linear Programming for (aspiring) data scientists,"Learn everything about Analytics|Introduction|Table of Content|1.What is Linear Programming?|2. Solve Linear Programs by Graphical Method|3. Solve Linear Program Using R|4. Solve Linear Program using OpenSolver|5. Simplex Method|6. Northwest Corner Method and Least Cost Method|7. Applications of Linear Programming|End Notes|Learn, compete, hack and get hired!","Example of a linear programming problem|Formulating a problem Lets manufacture some chocolates|Common terminologies used in Linear Programming||Process to formulate a Linear Programming problem|Output|6.1 Northwest Corner Method|6.2 Least Cost Method|Share this:|Like this:|Related Articles|How to read most commonly used file formats in Data Science (using Python)?|5 More Deep Learning Applications a beginner can build in minutes (using Python)|
Analytics Vidhya Content Team
|24 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",|Formulation of Linear Problem||Solving a LP through Graphical method,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Optimization is the way of life. We all have finite resources and time and we want to make the most of them. From using your time productively to solving supply chain problems for your company  everything uses optimization. Its a especially interesting and relevant topic in data science.It is also a very interesting topic  it starts with simple problems, but can get very complex. For example, sharing a chocolate between siblings is a simple optimization problem. We dont think inmathematical term while solving it. On the other hand devising inventory and warehousing strategy for an e-tailer can be very complex. Millions of SKUs with different popularity in different regions to be delivered in defined time and resources  you see what I mean!Linear programming (LP) is one of the simplest ways to perform optimization. It helps you solve some very complex optimization problems by making a few simplifying assumptions. As an analyst you are bound to come across applications and problems to be solved by Linear Programming.For some reason, LP doesnt get as much attention as it deserves while learning data science. So, I thought let me do justice to this awesome technique.I decided to write anarticle which explains Linear programming in simple English. I have kept the content as simple as possible. The idea is to get you started and excited about Linear Programming.Now, what is linear programming? Linear programming is a simple technique where we depictcomplex relationships through linear functions and then find the optimum points. The important word in previous sentence is depict. The real relationships might be much more complex  but we can simplify them to linear relationships.Applications of linear programming are every where around you. You use linear programming at personal and professional fronts. You are using linear programming when you are driving from home to work and want to take the shortest route. Or when you have a project delivery you make strategies to make your team work efficiently for on time delivery.Lets say a FedEx delivery man has 6 packages to deliver in a day. The warehouse is located at point A. The 6 delivery destinations are given by U, V, W, X, Y and Z. The numbers on the lines indicate the distance between the cities. To save on fuel and time the delivery person wants to take the shortest route.So, the delivery person will calculate different routes for going to all the6 destinations and then come up with the shortest route. This technique of choosing the shortest route is called linear programming.In this case, the objective of the delivery person is to deliver the parcel on time at all 6 destinations. The process of choosing the best route is called Operation Research. Operation research is an approach to decision-making, which involves a set of methods to operate a system. In the above example, my system was the Delivery model.Linear programming is used for obtaining the most optimal solution for a problem with given constraints. In linear programming, we formulate our real life problem into a mathematical model. It involves an objective function, linear inequalities with subject to constraints.Is the linear representation of the 6 points above representative of real world? Yes and No. It is oversimplification as the real route would not be a straight line. It would likely have multiple turns, U turns, signals and traffic jams. But with a simple assumption, we have reduced the complexity of the problem drastically and are creating a solution which should work in most scenarios.Example:Consider a chocolate manufacturing company which produces only two types of chocolate  Aand B. Both the chocolates require Milk and Choco only. To manufacture each unit of Aand B, following quantities are required:The companykitchen has a total of 5 units of Milk and 12 units of Choco. On each sale, the company makes a profit ofNow, the company wishes to maximize its profit. How many units of Aand Bshould it produce respectively?Solution: The first thing Im gonna do is represent the problem in a tabular form for better understanding.Let the total number of units produced of Abe = XLet the total number of units produced of Bbe = YNow, the total profit is represented by ZThe total profit the company makes is given by the total number of units of Aand Bproduced multiplied by its per unit profit Rs 6 and Rs 5 respectively.Profit: Max Z = 6X+5Ywhich means we have to maximize Z.The company will try to produce as many units of Aand Bto maximize the profit. But the resources Milk and Choco are available inlimited amount.As per the above table, each unit of Aand Brequires 1 unit of Milk. The total amount of Milk available is 5 units. To represent this mathematically,X+Y 5Also, each unit of Aand Brequires 3 units & 2 units of Choco respectively. The total amount of Choco available is 12 units. To represent this mathematically,3X+2Y  12Also, the values for units of A canonly beintegers.So we have two more constraints, X  0 & Y  0For the company to make maximum profit, the above inequalities have to be satisfied.This is called formulating a real-world problem into a mathematical model.Let usdefine some terminologies used in Linear Programming using the above example.Let us look at the steps of defining a Linear Programming problem generically:For a problem to be a linear programming problem, the decision variables, objective function and constraints all have to be linear functions.If the all the three conditions are satisfied, it is called a Linear Programming Problem.A linear program can be solved by multiple methods. In this section, we are going to look at the Graphical method for solving a linear program. This method is used to solve a two variable linear program. If you have only two decision variables, you should use the graphical method to find the optimal solution.A graphical method involves formulating a set of linear inequalities subject to the constraints. Then the inequalities are plotted on a X-Y plane. Once we have plotted all the inequalities on a graph the intersecting region gives us a feasible region. The feasible region explains what all values our model can take. And it also gives us the optimal solution.Lets understand this with the help of an example.Example:A farmer has recently acquired an 110 hectares piece of land. He has decided to grow Wheat and barley on that land. Due to the quality of the sun and the regions excellent climate, the entire production of Wheat and Barley can be sold. He wants to know how to plant each variety in the 110 hectares, given the costs, net profits and labor requirements according to the data shown below:The farmer has a budget of US$10,000 and an availability of 1,200 man-days during the planning horizon. Find the optimal solution and the optimal value.Solution:To solve this problem, first we gonna formulate our linear program.Step 1: Identify the decision variablesThe total area for growing Wheat = X (in hectares)The total area for growing Barley = Y (in hectares)X and Y are my decision variables.Step 2: Write the objective functionSince the production from the entire land can be sold in the market. The farmerwould want to maximize the profit for his total produce. We are given net profit for both Wheat and Barley.The farmer earns a net profit of US$50 for each hectare of Wheatand US$120 for eachBarley.Our objective function (given by Z) is, Max Z = 50X + 120YStep 3: Writing the constraints1. It is given that the farmerhas a total budget of US$10,000. The cost of producing Wheat and Barleyper hectare is also given to us. We have an upper cap on the total cost spent by the farmer. So our equation becomes:100X + 200Y 10,0002. The next constraint is, the upper cap on the availability on the total number of man-days for planning horizon. The total number of man-days available are 1200. As per the table, we are given the man-days per hectare forWheat and Barley.10X + 30Y 12003. The third constraint is the total area present for plantation. The total available area is 110 hectares. So the equation becomes,X + Y 110Step 4: The non-negativity restrictionThe values ofX and Y will be greater than or equal to 0. This goes without saying.X 0, Y 0We have formulated our linear program. Its time to solve it.Since we know that X, Y  0. We will consider only the first quadrant.To plot for the graph for the above equations, first I will simplify all the equations.100X + 200Y 10,000 can be simplified to X + 2Y 100 by dividing by 100.10X + 30Y 1200 can be simplified to X + 3Y 120 by dividing by 10.The third equation is in its simplified form, X + Y 110.Plot the first 2 lines on a graph in first quadrant (like shown below)The optimal feasible solution is achieved at the point of intersection where the budget & man-days constraints are active. This means the point at which the equations X + 2Y 100 and X + 3Y 120 intersect gives us the optimal solution.The values for X and Y which gives the optimal solution is at (60,20).To maximize profit the farmer should produce Wheat and Barleyin 60 hectares and 20 hectares of land respectively.The maximum profit the company will gain is,Max Z = 50 * (60) + 120 * (20)= US$5400R is an open source tool which is very popular among the data scientists for essential data science tasks. Performing linear programming is very easy and we can attain an optimum solution in very few steps. Come lets learn.Example: A toy manufacturing organization manufactures two types of toys A and B. Both the toys are sold at Rs.25 and Rs.20 respectively. There are 2000 resource units available every day from which the toy A requires 20 units while toy B requires 12 units. Both of these toys require a production time of 5 minutes. Total working hours are 9 hours a day. What should be the manufacturing quantity for each of the pipes to maximize the profits?Here:The objective function is:
Max.Z=25x+20ywhere x are the units of pipe Ay are the units of pipe BConstraints:
20x+12y<=20005x+5y<=540Lets see the code part now:Therefore from the output, we see that the organization should produce 88 units of toy A and 20 units of toy B and the maximum profit for the organization will be Rs.2600.In reality, a linear program can contain 30 to 1000 variables and solving it either Graphically or Algebraically is next to impossible. Companies generally use OpenSolver to tackle these real-world problems. Here I am gonna take you through steps to solve a linear program using OpenSolver.OpenSolver is an open source linear and optimizer for Microsoft Excel. It is anadvanced version of built-in excel Solver. You can download OpenSolver here and follow the installationmanual.I want you to get a hands-on knowledge on using OpenSolver. So, for clear understanding, I will explain it using an example.Example:Below there is adiet chart which gives me calories, protien, carbohydrate and fat content for 4 food items. Sara wants a diet with minimum cost. Thediet chart is as follows:The chart gives the nutrient content as well as the per-unit cost of each food item. The diet has to be planned in such a way that it should contain at least 500 calories, 6 grams of protien, 10 grams of carbohydrates and 8 grams of fat.Solution: First, Im gonna formulate my linear program in a spreadsheet.In cell B7:E7 we take the reference the number of units. And in cell B8:E8 we put the per-unit cost of each food items.In cell B10, we want the total cost for the diet. The total cost is given by the sumproduct of number of units eaten and per unit cost. Sumproduct is given by = B7*B8+C7*C8+D7*D8+E7*E8. Lets see this in a spreadsheet.Simplex Method is one of the most powerful & popular methods for linear programming.Simplex method is an iterative procedure for getting the most feasible solution. In this method, we keep transforming the value of basic variables to get maximum value for the objective function.A linear programming function is in its standard form if it seeks to maximize the objective function.subject to constraints,.         .         .         .        .       ..         .         .         .        .       .where,and.After adding slack variables, the corresponding system of constraint equation is,.                  .                    .          .where,The variables,.are called slack variables. They are non-negative numbers which are added to remove the inequalities from an equation.The above explanation gives the theoretical explanation of simplex method. Now, I am gonna explain how to use simplex method in real life using Excel.Example: The advertising alternatives for a company include television, newspaper and radio advertisements. The cost for each medium with their audience coverage is given below.The local newspaper limits the number of advertisements from a single company to ten.Moreover, in order to balance the advertising among the three types of media, no more than half of the total number of advertisements should occur on the radio. And at least 10% should occur on television. The weekly advertising budget is $18,200. How many advertisements should be run in each of the three types of media to maximize the total audience?Solution: First I am going to formulate my problem for a clear understanding.Step 1: Identify Decision VariablesLet ,, represent the total number of ads for television, newspaper, and radio respectively.Step 2: Objective FunctionThe objective of the company is to maximize the audience. The objective function is given by:Step 3: Write down the constraintsNow, I will mention each constraint one by one.It is clearly given that we have a budget constraint. The total budget which can be allocated is $18,200. And the individual costs per television, newspaper and radio advertisement is $2000, $600 and $300 respectively. This can be represented by the equation,Since for a newspaper advertisement, there is an upper cap on the number of advertisements to 10. My first constraints is,The next constraint is the number of advertisements on television. The company wants at least 10% of the total advertisements to be on television. So, it can be represented as:The last constraint is the number of advertisements on the radio cannot be more than half of the total number of advertisements. It can be represented asNow, I have formulated my linear programming problem. We are using simplex method to solve this. I will take you through simplex method one by one.To reiterate all the constraints are as follows. I have simplified the last two equations to bring them in standard form.We have a total of 4 equations. To balance out each equation, I am introducing 4 slack variables,,and .So our equations are as follows:I hope now you are available to make sense of the entire advertising problem. All the above equations, are only for your better understanding. Now if you solve these equations, you will get the values for X1= 4, X2= 10 and X3= 14.On solving the objective function you will get the maximum weekly audience as 1,052,000. You can follow the tutorial here to solve the equation. To solve linear program in excel, follow this tutorial.Northwest corner method is a special type method used for transportation problems in linear programming. It is used to calculate the feasible solution for transporting commodities from one place to another. Whenever you are given a real-world problem, which involves supply and demand from one source of different source. The data model includes the following:The model assumes that there is only one commodity. The demand for which can come from different sources. The objective is to fulfill the total demand with minimum transportation cost. The model is based on the hypothesis that the total demand is equal to the total supply, i.e the model is balanced. Lets understand this with the help of an example.Example: Consider there are 3 silos which are required to satisfy the demand from 4 mills. (A silo is a storage area of farm used to store grain and Mill is a grinding factory for grains).Solution: Lets understand what the above table explains.The cost of transportation from Silo i to Mill j is given by the cost in each cell corresponding to the supply from each silo 1 and the demand at each Mill. For example: The cost of transporting from Silo 1 to Mill 1 is $10, from Silo 3 to Mill 5 is $18. It is also given the total demand & supply for mill and silos. The objective is to find the minimal transportation cost such that the demand for all the mills is satisfied.As the name suggests Northwest corner method is a method of allocating the units starting from the topleft cell. The demand for Mill 1 is 5 and Silo 1 has a total supply of 15. So, 5 units can be allocated to Mill1 at a cost of $10 per unit.The demand for Mill1 is met. then we move to top left cell of Mill 2. The demand for Mill 2 is 15 units, which it can get 10 units from Silo 1 at a cost of $2 per unit and 5 units from Silo 2 at a cost of $7 per unit. Then we move onto Mill 3, the northwest cell is S2M3. The demand for Mill 3 is 15 units, which it can get from Silo 2 at a cost of $9 per unit. Moving on to the last Mill, Mill 4 has a demand of 15 units. It will get 5 units from a Silo 2 at a cost of $20 per unit and 10 units from Silo 3 at a cost of $18 per unit.The total cost of transportation is = 5*10+(2*10+7*5)+9*15+(20*5+18*10) = $520Least Cost method is another method to calculate the most feasible solution for a linear programming problem. This method derives more accurate result than Northwest corner method. It is used for transportation and manufacturing problems. To keep it simple I am explaining the above transportationproblem.According to the least cost method, you start from the cell containing the least unit cost for transportation. So, for the above problem, I supply 5 units from Silo 3 at per unit cost of $4. The demand for Mill1 is met. For Mill 2, we supply 15 units from Silo 1 at per unit cost of $2. Then For Mill 3 we supply 15 units from Silo 2 at per unit cost of $9. Then for Mill 4 we supply 10 units from Silo 2 at per unit cost of $20 and 5 units from Silo 3 a $18 per unit. The total transportation costs is $475.Well the above method explains we can optimize our costs further with the best method. Lets check this using Excel Solver. Solver is an in-built add-on in Microsoft Excel. Its an add-in plug available in Excel. Go to file->options->add-ins->select solver->click on manage->select solver->click Ok. Your solver is now added in excel. You can check it under the Data tab.The first thing I am gonna do is enter my data in excel. After entering the data in excel, I have calculated the total of C3:F3. Similarly for others. This is done to take the total demand from Silo 1 and others.After this,I am gonna break my model into two. The first table gives me the units supplied and the second table gives me the unit cost.Now, I am calculating my total cost which will be given by Sumproduct of unit cost and units supplied.Now I am gonna use Solver to compute my model. Similar to the above method. Add the objective function, variable cells, constraints.Now your model is ready to be solved. Click on solve and you will get your optimal cost. The minimum transportation cost is $435.Linear programming and Optimization areused in various industries. Manufacturing and service industry uses linear programming on a regular basis. In this section, we are going to look at the various applications of Linear programming.Well, the applications of Linear programming dont end here. There are many more applications of linear programming in real-world like applied by Shareholders, Sports, Stock Markets, etc.Go on and explore further.I hope you enjoyed reading this article. I have tried to explain all the basic concepts under linear programming. If you have any doubts or questions feel free to post them in the comments section.I have explained each concept with real life example. I want you to try them at your end and get hands-on experience. Let me know what you think!",https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/
5 More Deep Learning Applications a beginner can build in minutes (using Python),"Learn everything about Analytics|Introduction|Table of Contents|1. Deep learning Applications using existing API|2. Open Source Deep Learning Applications|3. Other Notable Resources|End Notes|Learn, compete, hack and get hired!","1.1 Automatic Image Tagging (Clarifai API)|1.2 Apparels Recommender (Indico API)|2.1 Music Generation using Deep Learning|2.2 Detecting Not Safe For Work Images|2.3 Super Resolution|Share this:|Like this:|Related Articles|Introductory guide on Linear Programming for (aspiring) data scientists|Interview with Harish Subramanian, Program Director, PGP- Big Data Analytics by GLIM|
Faizan Shaikh
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning is fundamentally changing everything around us. A lot ofpeople think that you need to be an expert to use power of deep learning in your applications. However, that is not the case.In my previous article, I discussed 6 deep learning applicationswhich a beginner can build in minutes. It was heart-warming for me to see hundreds of readers getting motivated by it. So I thought of following up from my previous article with a few more applications of deep learning. If you missed out on my previous article, I would suggest you should go through it.In this article, wewill learn how to build applications like automatic image tagging, apparel recommendation, music generation using deep learning and many more. And you will be able to build any of these applications with in minutes!P.S. The article assumes, you know thebasics of Python. If not, just follow this tutorial and then you can start from here again.1. Applications using existing APIs2. Open Sourced Applications3. Other Notable ResourcesImage tagging is one of the first applications of deep learning that showed breakthrough results. Unlike textual data, an image is a lot harder to comprehend for a machine. The machine requires a deeper understanding of the pixel data. Therefore we use image tagging to essentially summarize the image and tell us which categories the image and its underlying objects belong to.Thats why we use image tagging to essentially summarize the image. This tells us which categories the image belongs to and what are its underlying objects.Below is an example of predicted tags given to an image which was found through deep learning.Now, let us look at how to build the above tagging feature for every image using an API provided by Clarifai.Requirements and SpecificationsHere, you will be asked to provide your Client ID and Client Secret respectively. You can find these on the developer page itself.Then run the code byYou will get an output as followsThis is a json output showing a response of the prediction. Here, you can find the relevant tags in outputs->data-> concept->name .Recommendation systems are becoming a great asset day by day. As the number of products is increasing, intelligently targeting specific consumers who would be willing to buy the product is essential.Deep Learning can help us in this genre too!I am not a fashion buff, but I have seen people waste so much time choosing which clothes to wear. How great would it be if we could have an artificial agent know our preferences and suggest us the perfect outfit!Fortunately, with deep learning this is possible.You can find a demo of this application here.The official article describes this in more detail. Now lets find out how can you build this recommender system at your end.Requirements and SpecificationsAnd at the end, replace the part if __name__ == __main__ with below codeAnd run this code byYou will get an output like thiswhich shows the probability of match as shown in the example above.Music generation is one of the coolest applications of deep learning. If this application is used meticulously, it can bring breakthroughs in the industry.Music, just like most of the things in nature, is harmonic. It has patterns which our mind catches and makes sense of. These patterns of music can be taught to a computer and then can be used to create new music rhythms. This is the formula behind music generation.This open source application was built keeping in mind this concept. Below is an example of what it could generate.Now lets look at how we can replicate the results!Requirements:First, install Theano. Note that you have to install the bleeding edge version of Theano, which is required for this. You can find the installation instructions here.Then install Keras by the below codeYou would have to change backend of Keras from tensorflow to Theano. Follow the instructions given here.The final dependency is of Music21. For installation, refer this link.Although censorship is indeed a controversial topic, it is still a vital component for filtering out offensive adult content from the viewers. The inventors of this application focused on filtering out one of the main type of NFSW content, identification of pornographic images. A score is returned, which shows the intensity of NFSW, which can be used to filter out images above a certain threshold.Below shows the images and their corresponding NFSW score as given by the application.Lets look at how to build one such applicationRequirements:We often see in movies that when you zoom in to an image, you can view even the finest detail which can be used to catch a criminal or get a crucial evidence.In reality, this is not the case.When you zoom in, the image often gets blurry and not much can be seen. To cope up with this (and to make the dream a reality), we can use deep learning to increase the resolution of an image, which can help us to get clarity on zooming in.This application is the depiction of the same. Here is its sample output.Now lets look at how to build thisRequirementsDeep learning constantly amazes me. With countless applications in the making, the race for making use of this technology is becoming rampant in the industry. Before signing off, I would like to mention some of the resources which might be inspirational to you.You can also look at the video below for some use cases where deep learning can be used to enrich our lives. Enjoy!I hope you enjoyed reading this article. Deep Learning continues to fascinate everyone including top data scientists across the globe. These are few of the interesting applications I wanted to share with you.If you happen to know any other deep learning application, do share it with me in the comments section.If you come across any queries while building these applications at your end. Post your questions below & Ill be happy to answer them.",https://www.analyticsvidhya.com/blog/2017/02/5-deep-learning-applications-beginner-python/
"Interview with Harish Subramanian, Program Director, PGP- Big Data Analytics by GLIM","Learn everything about Analytics|Introduction|Learn, compete, hack and get hired!","Share this:|Like this:|Related Articles|5 More Deep Learning Applications a beginner can build in minutes (using Python)|How to leverage Social Media Analytics for your business?|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Big data is being generated all around us. Every social media exchange, every digital process, every connected device and machine are generating data to be used by various companies.Companies today are using Big Data for deepening customer engagement, optimizing operations, preventing threats & fraud. In the past two years, companies like IBM, GE, Amazon, Uber have come up with hundred of job positions for big data engineers & data scientists.To make sense of this data it requiresoptimal resources & at par analytical skills.Harish SubramanianTo ensure a successful career in Big data it is important to acquire relevant skills. One way to do that is take up a comprehensive program which acquaints you with knowledge & experience. Recently, GLIM (Great Lakes Institute of Management) started offering aBig Data Analyticsprogram in addition to its current program. To know more about the program, I spoke to Harish Subramanian, Program Director.Here are excerpts of my conversation with Harish Subramanian.KJ: Recently you have launched a Post graduate program in Big Data Analytics. Can you tell us briefly about the program?Mr Subramanian: In the last decade, we have seen a quantum leap in storage, processing, computation, and sensing technologies. With this, Big Data technologies have gone from being the tools of a select few researchers to an industry standard essential to understanding anything about the world we live in. The Great Lakes Post  Graduate Program in Big Data Analytics is a rigorous program that will help participants understand and apply the tools and techniques fundamental to handling the challenges of a Big Data world. New tools are developing every day, and data science professionals need to keep up with these trends and create a state-of-the-art solution by navigating the complex maze of tools available to them. At the same time, we help build the ability to make appropriate choices of data sources and possible analysis techniques  a task that is getting to be a larger challenge with each passing day. KJ: Why do you think theres a need for a specialized program in Big Data Analytics?Mr Subramanian: Weve spent years collaborating with a host of industry experts, data scientists and recruiting managers, and the consensus is that there is a burning need for cross-disciplinary technology professionals in the data analytics space  those that are able to not only use the pertinent tools, but also comprehend the trade-offs in model selection, implementation and data capture. And data analysis is certainly a team sport. A good data analysis team is like a well-choreographed dance group made up of data architects, engineers, analysts and data scientists (with ample support from IT and business leaders). In this program, were building just this intersection of skills our emphasis is on developing professionals who can work with disparate data sources, analyze them, draw valuable conclusions and communicate the insight in a compelling way. KJ: Who is the ideal candidate for this program?Mr Subramanian: This program is aimed at technically minded problem solvers who are looking to build a career in big data technologies, data science and advanced analytics. In terms of skills and background, the ideal candidate will have a good understanding of how data is sourced and managed, have programming abilities that will allow them to write and test models effectively, and understand the basics of statistics. More broadly, though, the ideal candidate for the program is a mid-career technology professional who wants to build on their existing expertise with technology environments to build a cross-disciplinary career in Data Analytics in the Big Data environment. KJ: What will the participants gain from this program?Mr Subramanian: Through this program, participants will become conversant in the statistical foundations upon which the field of analytics is built, a variety of big data technologies to handle complex data, the most pertinent machine learning techniques needed to make sense of all this data, and some compelling visualization techniques that help communicate this insight effectively.At the end of this program:KJ: How is this program different from others that candidates might be considering?Mr Subramanian: At the heart of it, we have built a program that will allow participants to see the whole picture when it comes to the use of vast amounts of messy data to draw business insights. We dont focus narrowly on a set of hot tools or on the mathematical models alone because the context is important to a successful data science team. We also want to give our participants enough of a foundation to build the kind of career they want. KJ: Can you tell us more about the curriculum? Mr Subramanian: The program consists of about 200 hours of classroom sessions that include hands-on exploration of the tools and techniques, and an equivalent amount of time that participants will spend on assignments, assessments, projects and other content. The classroom sessions spread over 12-weekend intensive sessions, covering the following areas:Each of these areas will be covered through a series of practical examples, hands-on lab sessions and a structured series of projects. KJ: What kind of practical exposure would be offered to students as part of this program? Mr Subramanian: We believe very strongly in showing, not telling. So, every module is interactive and rich with practical examples, real-world datasets and handy tips. Since industry experts and practitioners play a major part in this program, participants will learn what works and what doesnt in practice. Additionally, all participants will build their own portfolio of mini-projects and end with a capstone project. We intend for everyone who has gone through this program to be ready to hit the ground running from their very first day on the job. KJ: How will this program be delivered/taught? Is there any industry participation in the course?Mr Subramanian: This 1-year blended program gives participants an immersive experience  combining hands-on programming experience on technologies they will encounter in their Big Data Analytics careers with real-world context from leading industry practitioners. The program also hinges on physical classroom sessions as the fundamental learning environment as these give us the opportunity to be as targeted and interactive as necessary. These are supported by online assessments and content. Additionally, participants have unlimited access to our Big Data Lab on the cloud  which has a range of preconfigured Big Data tools including Hadoop, Apache Spark and a host of related functional tools. Industry experts act as faculty, deliver seminars and guide participants through their projects. In fact, theyre involved in every aspect of our program and have been since inception. KJ: Are there any prerequisites for candidates looking to do this program?Mr Subramanian: Yes, we expect that applicants have at least 2 years of professional experience post-graduation and that this experience should be in a technical role. Since this is a hands-on program, we expect that candidates have programmed in languages such as Python, Java, C++ or R. Even if they havent programmed recently, it would help if they are comfortable with programming environments and are willing to put in the work to do some pre-work to get up to speed. We also require that candidates are familiar with college level mathematics and statistics. KJ: When does the next batch begin and how can our community members enroll for this course?Mr Subramanian:The batch begins on the 25th of March. We conduct rolling admissions and decisions are made soon after each deadline, so the earlier someone applies, the greater their chances of securing their spot in the program. Interested candidates can go to our websiteto learn more or to apply.KJ: Thanks, Mr Subramanian for taking out the time for this interview.",https://www.analyticsvidhya.com/blog/2017/02/interview-harish-subramanian-pgp-big-data/
How to leverage Social Media Analytics for your business?,"Learn everything about Analytics|Introduction|Table of Content|1. What is Social Media Analytics?|2. How are different companies using Social media analytics?|3. What are the benefits derived from social media analytics?|4. How can Social media be used for lead generation?|
5. Tonality / Sentiment Analysis|6. Lead Generation|7. What are the tools used for social media analytics?|End Notes","|About The Author|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|Interview with Harish Subramanian, Program Director, PGP- Big Data Analytics by GLIM|Data Scientist|
Guest Blog
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Conventional media, such as television, radio or newspapers transmits information only in one direction. Users can consume the information which the media offers, but they have very little or no ability to share their own views on the subject.Now-a-days, digital mediums has made it possible to have a two-way form of communication that allows individuals to interact with the information being transmitted. This is known as Social media which encompasses a wide variety of online content, from social networking sites like Facebook, Twitter, YouTube etc to interactive encyclopedias.For example, social video sites like YouTube allow users to share video content and interact through video comments etc. The two-way communication through social media has created opportunities for lots of organization to communicate with their end consumers directly.As the name suggests, Social media analytics involves analyzing data on social media to take business decisions. Data is usually gathered from blogs, forums and social media websites (like Facebook, Twitter, Youtube etc.). It is then analyzedby converting the qualitative data to quantitative data with the help of different text mining and NLP techniques.The most common use of social media analytics is to mine customer sentiment in order to support marketing and customer service activities. Typical Social Media Analytics objectives include:Most of the organizations today have dedicated social media practice team to help brands expand on and off the web.Now-a-days,consumer goods manufacturers, personal technology makers and companies that rely heavily on word-of-mouth referrals to generate business find social media analysis tools crucial to their business strategies.Social media provides retailers with a wealth of information about their consumers that they might never get through traditional media. Since social media creates a two-way interaction between the brand and the individual, retailers can quickly get an understanding of which of their products are favored by buyers, what features of the brand customers can relate with them etc.Things get really useful when marrying social media data with internal data of the organization. Displaying custom results for example, for one of the online bookstores based on what a visitor had tweeted about that day might push up sales. Perhaps a large percentage of their Facebook followers under the age of 30 are suddenly searching for particular things on your website, raising the possibility of a targeted campaign.Following are the main use of Social media by different business entities:Social media listening / monitoring process helps in identifying real-time conversations happening on social media about a product, or a brand so you can respond in a timely manner. Since social media has empowered customers to speak about their experience about a product or brand, it is essential for businesses to assess what is going on in the inter webs by carving out a time to listen for brand/product-related conversations. Here are few benefits:In this article, I will focus on how Social media can be deployed for lead generation. Following are the details.With the introduction of social media, humongous amount of unstructured data is produced every second on the internet. This data contains very relevant & useful information about brands, competitors, Industry, consumers perception about different product, brands, services, etc.Now-a-days, social media is the connection between brands and consumers. However, most marketers think of social media as a brand amplification or awareness generation tool (and not sales). But social media is an integral part of todays sales process which helps to know the prospects and establishing relationships.For high-cost product marketers (for example, very high-cost modular kitchens), it is all about lead generation. The sales team is more interested in good and high-quality leads than anything else. The extensive reach of social media grants it potential as one of the most powerful lead generation tools, as social media allows sales people to see what prospects are saying about their brand and competitors. But the enormous size and dis-organized nature of the data makes it very cumbersome to generate actionable insights manually. Luckily, we can overcome this situation easily with the help of Analytics.Social analytics taps and analyzes consumers opinions converting them into insights, which helps businesses & marketers in identifying potential leads, areas of customer satisfaction or any customer grievance for a product etc.Solution Approach
Keyword Generation: To start with one needs primary inputs about the category to get idea about different keywords to be used for data pulling. For example, for premium Modular Kitchen segments we need information like number and name of the brands, features in different brands, prevailing models name etc. One also needs to create a list of noise words so that it is easier to remove the irrelevant conversation. For example, for premium Modular Kitchen, if we use only Kitchen, it can capture post like @XXX  all competition! Brand-YYY @Kitchen_Art #TheLifesWay which is irrelevant for the context of Modular Kitchen.Data Extraction & Data Cleaning:Once the keyword list is finalized, one needs to formulate the query in proper mannerto capture right content. To avoid the problem mentionedin previous example, we need to mention the query in proper order comprising of keywords and noise words (for example Kitchen and not competition or Modular Kitchen & Brand XXX Or Brand YYY etc.).Also, one needs to select the right sources. For example, for premium Modular Kitchen category, forum like Houzz.com, planned5d.commight be more helpful than Generic blogs etc.Converting Qualitative data to Quantitative data:Next, we need to convert qualitative data to quantitative data using text mining as well as Natural Language Processing (NLP) based techniques. Below are the examples of converting Qualitative data to Quantitative data:
The taxonomy needs to be fine-tuned based on test & learn approach. For example, the taxonomy for the purchase intent for premium Modular Kitchen looks like:
Purchase Intent & Basic Listening Taxonomy Creation & Fine Tuning:To analyze the purchase intent, one needs to create an initial taxonomy (the science concerned with classification of texts) based on some secondary researchers or sample data scan.Analyze the tonality to understand consumer expectations as well as pain points. Since Social Media data is more inclined towards Neutral content, predictive models alone will not suffice as a classification technique to classify Positive, Negative and Neutral tonality.An ensemble approach comprising of predictive modeling along with custom classification rules based on Nave Bayes Classifier would help to achieve higher accuracy (>80%).Please find below the description of tonality calculation process:Once the Purchase Intent & tonality analysis are over, we can classify the content as:We can figure out the Author Name corresponds to High Probability Lead & Medium Probability Lead and analyze their needs & pain areas based on the conversation and accordingly design the communication strategy to target them.Operating Model of the SolutionEvery time, there is new data, the data will be automatically classified based on existing rules. It is advisable to validate the rules in every three months.There are a variety of social media analytics tools to help marketing experts do all the analysis. Few of them are Radian6, Sysomos, Poly Analyst (Megaputer), HootSuite, etc. They can be used for multiple channels, while others focus on particular networks such as Twitter/ Facebook etc. All tools are useful for converting the qualitative data into thequantitative format as well as social media monitoring work.There are also statistical tools like R, SPSS Text Miner, SPSS Modeler and SAS which helps in different advanced analytical work like predictive modeling, Naive Bayes Classifier is used to boost-up the accuracy of Sentiment / tonality analysis.With the help of Social Media Analytics, organizations can identify social leads, influencers and advocates on a daily basis. The potential leads can be segregated into different segments based on the conversation themes and tonality. This Persona Analysis helps to understand the demographics and psychographics of the prospects and influencers.Social Media Analytics can capture profiles of prospects based on the well-defined taxonomy. Generally, the social media experts / analysts verify the leads and sort them into different segments as per business requirement and create personalized communication strategies.Social media is a powerful way to grow your followers, gain trust, and increase overall revenue by acquiring more customers. Using Social Media Analytics, one can definitely increase the ability to grow to the maximum potential through social networks.Anjanita is a dynamic results oriented professional with blend of Analytics Consulting, Business Intelligence and project management experience comprising of 11 years from project scoping to entire execution, in several successful shared services organizations in India in technology, CPG & retail sector.",https://www.analyticsvidhya.com/blog/2017/02/social-media-analytics-business/
Data Scientist,Learn everything about Analytics,"Share this:|Like this:|Related Articles|How to leverage Social Media Analytics for your business?|Brace Yourself  DATAFEST 2017 is coming & Call for AV Volunteers!|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  15 years
Requirements : DollarBird is looking to add a Data Scientist (DS) to its Data and Analytics team. The DS will work as a member of the team that describes, reports on, analyzes, predicts, and proscribes all audience and campaign data and related programmatic media trading as well as select client-focused Managed Services on behalf of select supply-side and demand-side clients. The DS will support all managers and teammates in their day-to-day media trading desk activities across a variety of media channels including: display, mobile, video, social, native, and advanced TV/Audio ad products. The Data Scientist will report directly to the Team Leader, Data and Analytics. Dollarbird Inc is seeking qualified candidates to work on-site, full-time in Mysuru, India and who are eligible to travel to the U.S. Qualifications
	Experience with various rigorous statistical, econometric and Big Data methods to identify opportunities/problems, analyze results, and optimize economic outcomes.
	Strong analytical, data modelling, audience and price action data-focused sensibilities and skills.
	Experience with structured and unstructured audience & pricing data and reporting
	Hands-on experience with media and/or marketing data and analytical teams.
	Experience supporting programmatic, audience, CRM, DSP, DMP, and SSP systems is a plus.
	Hands-on knowledge of DSPs, SSPs, DMPs is desired along with understanding of digital advertising and marketing as it relates to creation, implementation, analysis, modeling, yield and optimization of campaigns.
	The ability to manage your own work, multitask and work well with U.S., offshore and local team members is also required to be successful in this position. Responsibilities
The Data Scientists (DS) main responsibility is to be a great team-player working hands-on in a team that:
1.	Manages a variety of data and analytic platforms & develops proprietary data and technology
2.	Analyzes data and optimizes audience composition, audience value, media trading and campaign performance to ensure successful outcomes and margin.
3.	Sources 1st/2nd/3rd party audience, content, contextual, and pricing data
4.	Delivers actionable business and client-focused results on-demand and proactively As a key member of the team, DS is responsible for:
	Data requirements, relationships, design, architecture, models and evolution
	Data collection implementations, quality assurance, validations and optimizations
	Design of data collection, storage and analytics systems, specification, deployment and use of tools for in-depth, complex analysis, predictive models, reporting and visualization builds, deployments, management, maintenance and optimizations 	Key deliverables for this role will be to analyze ad inventory, eCPM, fill rate, audiences, and anomalous trading behaviour as well as opportunities to create tradeable ad products and approaches.
	Work directly with DMPaaS, Traders, Yield, Operations, Finance, Sales, and Business Development team members to insure reliable data quality, completeness, predictions and yield
	Monitor performance of all programmatic and audience informed media campaigns across business partners media channels: Website, Mobile Applications, Social Media platforms and more
	Create and deliver digital media and audience insights and opportunities via advanced predictive modelling and analysis using databases, predictive and regression modelling, SQL, Excel, DOMO, Sigmoid and/or other database, BI and modelling tools with the goal of offering real-time insights to key stakeholders ensuring campaigns perform successfully as well as keeping our internal teams informed of overall company and business partner performance patterns and opportunities
Secondary Responsibilities  Work with business partner teams to assist in their revenue-related content and audience planning
 Support the development of audience extension / development initiatives across display, mobile and video formats
 Responsible for the optimization around data operations efforts: collection and analytics standards, technology, latest innovations in measurement, modelling, analysis, visualizations, process improvement, best practices, etc
 Measure, analyze and optimize direct and programmatic data enhanced media campaigns
 Provide thorough projections and analysis of all performance metrics associated with individual campaigns
 Regularly monitor campaign pacing and data tags for continuous reporting, modelling, analysis and optimization
 Provide ongoing support to DMPaaS Data Management
 Support the Leader, Data & Analytics and DMPaaS with standard and custom audience analysis and reports daily, weekly, monthly, quarterly, ad-hock time periods and time period comparisons, as well as other monetization related efforts specifically tied to DEM opportunity and campaign delivery
 Develop Machine Learning and Deep Learning-based approaches to audience- and non-audience-based ad inventory price-value discovery and optimization. Required/Desired Skills/Experience  At least 5 years in Data and Predictive analytics design, build, modelling, mining, analysis, reporting and optimization with Publisher-side and/or Agency Operations experience at data provider, major media firm and/or Agency and/or Exchange and/or Ad Network.
 3 years Publisher and/or Advertiser Audience Data Analytics and Modelling experience desired
 Bachelor Degree required (preferred Masters in Math/Statistics/Engineering/Science/Business)
 Mastery of data collection, storage, management, mining and modelling, business intelligence, digital analytics and tools (i.e., Python, mldb, Hadoop, SQL, PMML, APIs, Krux, Google Analytics, Charles Proxy, H2O, DOMO, Tableau, SPARK, etc.)
 Experience with AWS for data storage, databases, ETL, analysis, reporting and visualization and cloud-based distributed computing and predictive modelling required including open source tools
 Machine Learning (Logistic regression, Customer segmentation, Profiling, Scoring), Math and Statistical approaches and languages including open source libraries and Python
 Experience building first-party and second-party data strategies including creation of hierarchies and first-party data segments.
 Strong understanding of digital advertising: website and email display advertising, social media, etc
 Experience in programmatic or auction-based media
 Experience managing and creating DMP-based Audience Segments from 1st/2nd/3rd Party Data.
 Experience with Krux, Adobe Audience Manager, Oracle Audience Manager, DoubleClick / DFP, Free Wheel, JW Player, tag creation and tag management, Operative, and AdX / multi-SSP configurations
 Interest and ability to work in a fast-paced operation on the analytics and revenue side of our business If you are interested and your profile matches with the requirements mentioned, please send across your profile along with the following details to [emailprotected] Total & Relevant Experience:
Current & Expected CTC:
Notice Period:
Open for relocation to Mysuru:
Task Info : DollarBird is looking to add a Data Scientist (DS) to its Data and Analytics team.The DS will work as a member of the team that describes, reports on, analyzes, predicts, and proscribes all audience and campaign data and related programmatic media trading as well as select client-focused Managed Services on behalf of select supply-side and demand-side clients.The DS will support all managers and teammates in their day-to-day media trading desk activities across a variety of media channels including: display, mobile, video, social, native, and advanced TV/Audio ad products.The Data Scientist will report directly to the Team Leader, Data and Analytics.Dollarbird Inc is seeking qualified candidates to work on-site, full-time in Mysuru, India and who are eligible to travel to the U.S.Qualifications Experience with various rigorous statistical, econometric and Big Data methods to identify opportunities/problems, analyze results, and optimize economic outcomes. Strong analytical, data modelling, audience and price action data-focused sensibilities and skills. Experience with structured and unstructured audience & pricing data and reporting Hands-on experience with media and/or marketing data and analytical teams. Experience supporting programmatic, audience, CRM, DSP, DMP, and SSP systems is a plus. Hands-on knowledge of DSPs, SSPs, DMPs is desired along with understanding of digital advertising and marketing as it relates to creation, implementation, analysis, modeling, yield and optimization of campaigns. The ability to manage your own work, multitask and work well with U.S., offshore and local team members is also required to be successful in this position.ResponsibilitiesThe Data Scientists (DS) main responsibility is to be a great team-player working hands-on in a team that:1. Manages a variety of data and analytic platforms & develops proprietary data and technology2. Analyzes data and optimizes audience composition, audience value, media trading and campaign performance to ensure successful outcomes and margin.3. Sources 1st/2nd/3rd party audience, content, contextual, and pricing data4. Delivers actionable business and client-focused results on-demand and proactively As a key member of the team, DS is responsible for: Data requirements, relationships, design, architecture, models and evolution Data collection implementations, quality assurance, validations and optimizations Design of data collection, storage and analytics systems, specification, deployment and use of tools for in-depth, complex analysis, predictive models, reporting and visualization builds, deployments, management, maintenance and optimizations Key deliverables for this role will be to analyze ad inventory, eCPM, fill rate, audiences, and anomalous trading behaviour as well as opportunities to create tradeable ad products and approaches. Work directly with DMPaaS, Traders, Yield, Operations, Finance, Sales, and Business Development team members to insure reliable data quality, completeness, predictions and yield Monitor performance of all programmatic and audience informed media campaigns across business partners media channels: Website, Mobile Applications, Social Media platforms and more Create and deliver digital media and audience insights and opportunities via advanced predictive modelling and analysis using databases, predictive and regression modelling, SQL, Excel, DOMO, Sigmoid and/or other database, BI and modelling tools with the goal of offering real-time insights to key stakeholders ensuring campaigns perform successfully as well as keeping our internal teams informed of overall company and business partner performance patterns and opportunitiesSecondary Responsibilities Work with business partner teams to assist in their revenue-related content and audience planning Support the development of audience extension / development initiatives across display, mobile and video formats Responsible for the optimization around data operations efforts: collection and analytics standards, technology, latest innovations in measurement, modelling, analysis, visualizations, process improvement, best practices, etc Measure, analyze and optimize direct and programmatic data enhanced media campaigns Provide thorough projections and analysis of all performance metrics associated with individual campaigns Regularly monitor campaign pacing and data tags for continuous reporting, modelling, analysis and optimization Provide ongoing support to DMPaaS Data Management Support the Leader, Data & Analytics and DMPaaS with standard and custom audience analysis and reports daily, weekly, monthly, quarterly, ad-hock time periods and time period comparisons, as well as other monetization related efforts specifically tied to DEM opportunity and campaign delivery Develop Machine Learning and Deep Learning-based approaches to audience- and non-audience-based ad inventory price-value discovery and optimization.Required/Desired Skills/Experience At least 5 years in Data and Predictive analytics design, build, modelling, mining, analysis, reporting and optimization with Publisher-side and/or Agency Operations experience at data provider, major media firm and/or Agency and/or Exchange and/or Ad Network. 3 years Publisher and/or Advertiser Audience Data Analytics and Modelling experience desired Bachelor Degree required (preferred Masters in Math/Statistics/Engineering/Science/Business) Mastery of data collection, storage, management, mining and modelling, business intelligence, digital analytics and tools (i.e., Python, mldb, Hadoop, SQL, PMML, APIs, Krux, Google Analytics, Charles Proxy, H2O, DOMO, Tableau, SPARK, etc.) Experience with AWS for data storage, databases, ETL, analysis, reporting and visualization and cloud-based distributed computing and predictive modelling required including open source tools Machine Learning (Logistic regression, Customer segmentation, Profiling, Scoring), Math and Statistical approaches and languages including open source libraries and Python Experience building first-party and second-party data strategies including creation of hierarchies and first-party data segments. Strong understanding of digital advertising: website and email display advertising, social media, etc Experience in programmatic or auction-based media Experience managing and creating DMP-based Audience Segments from 1st/2nd/3rd Party Data. Experience with Krux, Adobe Audience Manager, Oracle Audience Manager, DoubleClick / DFP, Free Wheel, JW Player, tag creation and tag management, Operative, and AdX / multi-SSP configurations Interest and ability to work in a fast-paced operation on the analytics and revenue side of our business
College Preference : tier1-any
Min Qualification : ug
Skills : aws, Data analytics, data visualization, etl, google analytics, hadoop, machine learning, predictive modeling, python, spark, sql, tableau
Location : Mysuru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/data-scientist/
Brace Yourself  DATAFEST 2017 is coming & Call for AV Volunteers!,Learn everything about Analytics|The start|The community today|DATAFEST 2017|Brace yourself  we are on our way!|Call for Volunteers|Getting back to work,"What do you get as an AV Volunteer?|What do you need to be an AV Volunteer?|What is expected from you?|How do you register?|Share this:|Like this:|Related Articles|Data Scientist|Top 28 Cheat Sheets for Machine Learning, Data Science, Probability, SQL & Big Data|
Kunal Jain
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Big things often have small beginningsWhat is common between Richard Branson,Pierre Omidyar, Mark Zuckerberg and Colonol Sanders?They all started small, continuously worked on their idea and made it big. They had an interesting idea or a pet project which they enjoyed. None of them would have realized the true potential of their ideas until they started to build a traction.In a lot of ways, these stories are similar to our journey at Analytics Vidhya. Obviously, I am comparing our scale to these companies  but there are many parallels in the journey. When I started AV as a blog, I was not very clear about the shape and form it will take. I had no idea what scale it could reach. I only knew I was enjoying what I was doing.Slowly and steadily things fellin place and we have built one of the largest analytics community across the globe.Today, we help millions of people in their pursuit of finding practical knowledge about analytics and data science. We are blessed with a vibrant and active community ready to help other members. I see many people like me on the portal helping other community members. I am sure we have enabled a few thousand careers in analytics and data science.I see people running their own meetups, whatsapp groups and other ways to take the knowledge forward. The world todaylooks so different from where I started  when I could not find a place or portal to meet other people in the analytics industry.Today, I wanted to announce 2 things which would take us to a different level  DATAFEST 2017 and AV Volunteer Program. Read on!This April, our blog completes 4 yearsof existence. Like last year, we plan to celebrate the occasion with ana grand online festival  DATAFEST 2017. DATAFEST 2017 would not only be bigger and better  it would be a reflection of our growth and presence today. We plan to come out and celebrate this month like there is no tomorrow (you never know what smart machines will do tomorrow!).This DATAFEST, we will bring you multiple hackathons, lots of jobs and career opportunities. We will also provide you numerous ways to interact with industry experts. We will make sure your learning reaches new heights and you mark this celebration with ardor and intensity.I am not spilling too many beans today! Today, I can only tell you this DATAFEST is going to be bigger, better and the grandest of all online events we ever did. We will leave no stone unturned in our efforts to celebrate this occassion.You will hear more details from us shortly. In the meanwhile, make sure you register yourself for event updates by signing up here.We willmake sure you are the first ones to hear our plans.Ok  let me announcesomething special. I could not keep everything to myself. Here is your opportunity to be in middle of the action. We have opened enrollments for our volunteer program. It is a unique program for all the data science evangelists out there.As part of this program, you will be our torch bearers. You will be the ones who will take our efforts and initiatives to the next level. You will help us define how to make Analytics Vidhya bigger, better and more useful. Last date for registration is 28 Feb.Only a few thingsSo, that is it for now. I will get back to my to do list. You make sure that you are registered for DATAFEST 2017 updates and register for a volunteer (if you are interested) . Once you have done that, wait and see the magic unfold in next 40 days. See you around.",https://www.analyticsvidhya.com/blog/2017/02/datafest-2017-call-volunteers/
"Top 28 Cheat Sheets for Machine Learning, Data Science, Probability, SQL & Big Data","Learn everything about Analytics|Overview||Introduction|Python for Data Science Cheat Sheets|R for Data ScienceCheat Sheets|Machine LearningCheat Sheets|ProbabilityCheat Sheets|SQL & MySQLCheat Sheets|Big DataCheat Sheets|Learn, compete, hack and get hired!","1.Quick Guide to learn Python for Data Science|2.Python for Data Science Cheat sheet|3.Python For Data Science Cheat Sheet NumPy|4.Exploratory Data Analysis in Python|5.Data Exploration using Pandas in Python|6.Data Visualisation in Python|7.Python For Data Science Cheat Sheet Bokeh|8.Cheat Sheet: Scikit Learn|9.Steps To Perform Text Data Cleaning in Python |1.R Reference Card|2. Data Import in R|3. Data Transformation with dplyr|4.Cheat sheet  11 Steps for Data Exploration in R (with codes)|5.Data Visualization in R |6.Data Visualization with ggplot2|7.Cheat sheet:Caret Package|8.R Reference Card for Data Mining|9.Guide to quickly learn Cloud Computing in R Programming|1.Cheat sheet  Python & R codes for common Machine Learning Algorithms|2. Scikit Learn algorithm Cheat sheet|3. Microsoft Azure Machine Learning: Algorithm Cheat Sheet|1.Probability Basics Cheat Sheet|2.Probability cheat sheet for distribution|1.SQL Cheat Sheet|2.MySQL & SQL Cheat Sheet|1.HadoopCheat sheet|2.Apache Spark Cheat sheet|3.Hive Function Cheat Sheet|End Notes|Share this:|Like this:|Related Articles|Brace Yourself  DATAFEST 2017 is coming & Call for AV Volunteers!|How to build Ensemble Models in machine learning? (with code in R)|
Analytics Vidhya Content Team
|30 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data Science is an ever-growing field, there are numerous tools & techniques to remember. It is not possible for anyone to remember all the functions, operations and formulas of each concept. Thats why we have cheat sheets. But there are a plethora of cheat sheets available out there, choosing the right cheat sheet is a tough task. So, I decided to write this article.Here I have selected the cheat sheets on the following criteria: comprehensiveness, clarity, and content.After applying these filters, I have collated some 28 cheat sheets on machine learning, data science, probability, SQL and Big Data. For your convenience, I have segregated the cheat sheets separately for each of the above topics. There are cheat sheets on tools & techniques, various libraries & languages.Read on to know which cheat sheet to use for a particular topic.If you are starting to learn Python, then this cheat sheet is the best resource for you. In this cheat sheet, you will find a step-by-step guide to learn Python. It gives out resources to follow, Python libraries you must know and few helpful tips.This cheat sheet by Datacamp covers all the basics of Python required for data science. If you have just started working on Python then keep this as a quick reference. Mug up these cheat codes for variables & data types functions, string operation, type conversion, lists & commonly used NumPy operations. The unique aspect of this cheat sheet is it lists down important Python libraries & gives cheat codes for selecting & importing these libraries.NumPy is a core library for scientific computing in Python. In this cheat sheet from DataCamp you will find cheat codes for creating NumPy arrays, performing mathematics operation on array, subsetting, slicing, indexing & array manipulation. The unique aspect of this cheat sheet is it gives each function has been categorized & explained in simple English.Your best resource to perform data exploration in Python using NumPy, Pandas & Matplotlib. With this cheat sheet you will learn how to load files in python, convert variables, sort data, create plots, create sample datasets, treat missing values & many more. It is one of the simplified cheat sheet on data exploration.Pandas is one of the important libraries in Python. This cheat sheet on data exploration operation in Python using Pandas is your go-to resource to know each step involved in data exploration. You will find cheat codes for reading & writing data, preview of dataframes, rename columns of dataframe, aggregate the data, etc.Be it a data scientist or a non-techie, visualization is easily interpreted by both. In visual graphs & plots, data comes to life & speaks for itself. In this cheat sheet,learn how to perform data visualization in Python. Explore the different ways in which you can plot your data. Find step by step approach to plot histograms, bar charts, line graph, scatter plot, etc.This cheat sheet on Bokeh, an interactive visualization library in Python is especially useful with large datasets. In this cheat sheet by DataCamp, you will get basic steps for plotting, renderers & visual customization, save plots & create statistical charts.Here is a cheat sheet on scikit-learn for each technique in Python. It provides different functions used for pre-processing, regression, classification, clustering, dimensionality reduction, model selection & metric along with their description. The unique aspect of this cheat sheet is it depicts the complete stages of machine learning.Text cleaning can be a cumbersome process.And knowing the right procedures is the key to getting the desired result. Refer this cheat sheet to performtext data cleaning in Python step by step. Follow this cheat sheet to know when you remove stop words, punctuation, expressions, etc. The unique aspect of this cheat sheet is each step has been explained with codes & examples.Use this reference sheet for cheats codes for all functions & operators under R. Understand what the different terms mean under R. It explains all the functions under data creation, data processing, data manipulation, model function, selection and many more.Learn how to import data with readr, tibble and tidyr. Find functions to write & read functions in tibble. It also provides you useful arguments, reshape data, combine cells with tidyr.This cheat sheet from RStudio is a reference material for data transformationwithdplyr. Get short codes& operators for all operations under data transformation. Then be it summarize cases, group case, manipulation, vectorize & combine variables.This cheat sheet gives a step by step guide to data exploration in R. Learn how to load file in R, convert variables to different data types, transpose a dataset, sort dataframe, create plots & many more.Above we saw cheat sheet on data visualization in Python. Here is a data visualization cheat sheet to give the different graphs by which you can plot the data.With a few lines of code, you can create beautiful charts and data stories. R has awesome libraries to createbasic and more evolvedvisualizations like Bar Chart, Histogram, Scatter Plot, Map visualization, Mosaic Plot and various others.This cheat sheet is specifically for creating a visualization in R using ggplot2. ggplot2 works on the grammar of graphics and is built on a set of visual marks that represent data point. Get cheat codes to create one variable & two variablegraphical component. Along with different techniques for creating plots in R.Caret package provides a set of functions that streamlines the process of creating predictive models. The cheat sheet includes functions for data splitting, pre-processing, feature selection, model tuning & visualization.This cheat sheet provides functions for text mining, outlier detection, clustering, classification, social network analysis, big data, parallel computing using R. This cheat sheet gives you all the functions & operators used for data mining in R.Cloud computing has made it very easy for us to access our files & data from anywhere. In this cheat sheet, you will learn about how to use cloud computing in R. Follow this step by step guide to use R programming on AWS.In this cheat sheet,you will get codes in Python & R for various commonly used machine learning algorithms. The algorithms included are Linear regression, logistics regression, decision tree, SVM, Naive Bayes, KNN, K-means, random forest & few others.This cheat sheetis provided from the official makers of scikit-learn. Many people face the problem of choosing a particular machine learning algorithm for different data types & problems. With the help of this cheat sheet, you have the complete flow for solving a machine learning problem.This cheat sheet helps you choose the best Azure Machine Learning Studio algorithm for your predictive analytics solution. Developed by Microsoft Azure team itself cheat sheet gives you a clear path as per the nature of the data.This cheat sheet provides you a comprehensive reference material for probability & statistics. Each concept has been explained marvelously with a diagrammatical explanation. It covers from the basic probability rules to advanced statistical concepts in a very precise & accurate manner. Developed by the University of Pennsylvania, it is one of the most comprehensive cheat sheets you can lay your hands on.Refer this cheat sheet for a quick overview on Poisson Distribution, Normal distribution, Binomial Distribution, Geometric Distribution and many more. It gives notation, formulas & a brief explanation in simple English for each distribution.In this cheat sheet,learn how to perform basic operations in SQL. Get function for inserting data, update data, deleting data, grouping data, order data, etc. If you have started using SQL this the best reference guide.In this cheat sheet, you will find commonly used MySQL & SQL commands. Get cheat codes for MySQL mathematical function, MySQL string function, basic MySQL commands. You will also find SQL commands for modifying & querying.It is rightly said Hadoop has a vast ecosystem & includes various operations. Learn about the various operators,how they work & what operation they are responsible for. Thecheat sheet has been broken down into a respective general function like distributed systems, processing data, getting data in/out & administration.Here is a cheat sheet for Apache Spark for various operations like transformation, actions, persistence methods, additional transformation & actions, extended RDD, streaming transformation, RDD persistence, etc.In this cheat sheet, get commands for Hive functions. It provides cheat codes for data functions, mathematical function, string function, collection function, built-in aggregate function, built-in table generating function, conditional function and functions for text analytics.I hope you enjoyed reading this article. If I have missed out any cheat sheet which you think should be included in the list. Then post them in the comments section. The other reader & I would like to know about them.If you have any suggestions/feedback then dont forget to share itby dropping in your comments. Tell us what more cheat sheets you would like us to publish.",https://www.analyticsvidhya.com/blog/2017/02/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data/
How to build Ensemble Models in machine learning? (with code in R),"Learn everything about Analytics|Introduction|Table of Content|1.What is ensembling?|2. Types of ensembling|3. Advantages and Disadvantages of ensembling|4. Practical guide to implementing ensembling in R|5. Additional resources|Learn, compete, hack and get hired!","3.1 Advantages|3.2 Disadvantages|Step 1: Train the individual base layer models on training data|Step 2: Predict using each base layer model for training data and test data|Step 3: Now train the top layer model again on the predictions of the bottom layer models that has been made on the training data|Step 4: Finally, predict using the top layer model with the predictions of bottom layer models that has been made for testing data|End Notes|Share this:|Like this:|Related Articles|Top 28 Cheat Sheets for Machine Learning, Data Science, Probability, SQL & Big Data|40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution)|
Saurav Kaushik
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Over the last 12 months, I have been participating in a number of machine learning hackathons on Analytics Vidhya and Kaggle competitions. After the competition, I always make sure to go through the winners solution. The winners solution usually provide me critical insights, which have helped me immensely in future competitions.Most of the winners rely on an ensemble of well-tuned individual models along with feature engineering. If you are starting with machine learning, I would advise you to lay emphasis on these two areas asI have found them equally important todo well in a machine learning.Most of the time, I was able to crack the feature engineering part but probably didnt use the ensemble of multiple models.If you are a beginner, its even better to get familiar with ensembling as early as possible. Chances are that you are already applying it without knowing!In this article, Ill take you through the basics of ensemble modeling. Then I will walk you through the advantages of ensembling. Also, to provide youhands-on experience on ensemble modeling, we will use ensembling on a hackathon problem using R.P.S. For this article, we will assume that you can build individual models in R / Python. If not, you can start your journey with our learning path.In general, ensembling is a technique of combining two or more algorithms of similar or dissimilar types called base learners. This is done to make a more robust system which incorporates the predictions from all the base learners. It can be understood as conference room meeting between multiple traders to make a decision on whether the price of a stock will go up or not.Since all of them have a different understanding of the stock market and thus a different mapping function from the problem statement to the desired outcome. Therefore, they are supposed to make varied predictions on the stock price based on their own understandings of the market.Now we can take all of these predictions into account while making the final decision. This will make our final decision more robust, accurate and less likely to be biased. The final decision would have been opposite if one of these traders would have made this decision alone.You can consider another example ofa candidate going through multiple rounds of job interviews. The final decision of candidates ability is generallytaken based on the feedback of all theinterviewers. Although a single interviewer might notbe able to test the candidatefor each required skilland trait. But the combined feedback of multiple interviewersusually helps in better assessment of the candidate.Some of the basic concepts which you should be aware of before we go into further detail are:Practically speaking, there can be a countless number of ways in which you can ensemble different models. But these are some techniques that are mostly used:For bootstrapped sample, we choose one out of these three randomly. Say we chose Row 2.You see that even though Row 2 is chosen from the data to the bootstrap sample, its still present in the data. Now, each of the three:Rows have the same probability of being selected again. Lets say we choose Row 1 this time.Again, each row in the data has the same probability to be chosen for Bootstrapped sample. Lets say we randomly choose Row 1 again.Thus, we can have multiple bootstrapped samples from the same data. Once we have these multiple bootstrapped samples, we can grow trees for each of these bootstrapped samples and use the majority vote or averaging concepts to get the final prediction. This is how bagging works.One important thing to note here is that its done mainly to reduce the variance. Now, random forest actually uses this concept but it goes a step ahead to further reduce the variance by randomly choosing a subset of features as well for each bootstrapped sample to make the splits while training.It relies on creating a series of weak learners each of which might not be good for the entire dataset but is good for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.Its really important to note that boosting is focused on reducing the bias. This makes the boosting algorithms prone to overfitting. Thus, parameter tuning becomes a crucial part of boosting algorithms to make them avoid overfitting.Some examples of boosting are XGBoost, GBM, ADABOOST, etc.Lets understand it with an example:Here, we have two layers of machine learning models:Here, we have used only two layers but it can be any number of layers and any number of models in each layer. Two of the key principles for selecting the models:One thing that you might have realized is that we have used the top layer model which takes as input the predictions of the bottom layer models. This top layer model can also be replaced by many other simpler formulas like:I believe you would have a good grasp on ensembling concepts by now. Well, enough of theory now, lets get down to implementing ensembling and see whether it can help us improve our accuracy for a real machine learning challenge. If you wish to read more about the basics of ensembling, then you can refer to this resource.For the purpose of implementing ensembling, I have chosen Loan Prediction problem. We have to predict whether the bank should approve the loan based on the applicant profile or not. Its a binary classification problem. You can read more about the problem here.Ill be using caret package in R for training various individual models. Its the goto package for modeling in R. Dont worry if you are not familiar with the caret package, you can get through this article to get the comprehensive knowledge of caret package. Lets get done with getting the data and data cleaning part.I have divided the data into two parts which Ill be using to simulate the training and testing operations. We now define the training controls and the predictor and outcome variables:Now lets get started with training a random forest and test its accuracy on the test set that we have created:Well, as you can see, we got 0.81 accuracy with the individual random forest model. Lets see the performance of KNN:Its great since we are able to get 0.86 accuracy with the individual KNN model. Lets see the performance of Logistic regression as well before we go on to create ensemble of these three.And the logistic regression also gives us the accuracy of 0.86.Now, lets try out different ways of forming an ensemble with these models as we have discussed:Before proceeding further, I would like you to recall about two important criteria that we previously discussed on individual model accuracy and inter-model prediction correlation which must be fulfilled. In the above ensembles, I have skipped checking for the correlation between the predictions of the three models. I have randomly chosen these three models for a demonstration of the concepts. If the predictions are highly correlated, then using these three might not give better results than individual models. But you got the point. Right?So far, we have used simple formulas at the top layer. Instead, we can use another machine learning model which is essentially what stacking is. We can use linear regression for making a linear formula for making the predictions in regression problem for mapping bottom layer model predictions to the outcome or logistic regression similarly in case of classification problem.Moreover, we dont need to restrict ourselves here, we can also use more complex models like GBM, neural nets to develop a non-linear mapping from the predictions of bottom layer models to the outcome.On the same example lets try applying logistic regression and GBM as top layer models. Remember, the following steps that well take:One extremely important thing to note in step 2 is that you should always make out of bag predictions for the training data, otherwise the importance of the base layer models will only be a function of how well a base layer model can recall the training data.Even, most of the steps have been already done previously, but Ill walk you through the steps one by one again.First, lets start with the GBM model as the top layer model.Similarly, we can create an ensemble with logistic regression as the top layer model as well.Great! You made your first ensemble.Note its really important to choose the models for the ensemble wisely to get the best out of the ensemble. The two thumb rules that we discussed will greatly help you in that.By now, you might have developed an in-depth conceptual as well as practical knowledge of ensembling. I would like to encourage you to practice this on machine learning hackathons on Analytics Vidhya, which you can find here.Youll probably find this article on top five questions related to ensembling helpful.Also, if you missed out on the skilltest on ensembling, you can check your understanding of ensembling concepts here.Ensembling is a very popular and effective technique that is very frequently used by data scientists for beating the accuracy benchmark of even the best of individual algorithms. More often than not its the winning recipe in hackathons. The more youll use ensembling, the more youll admire its beauty.Did you enjoy reading this article? Do share your views in the comment section below. If you have any doubts / questions feel free to drop them in the comments below.",https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/
40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution),"Learn everything about Analytics|Introduction|Overall Results|Helpful Resources|Questions & Answers|Learn, compete, hack and get hired!","End Notes|Share this:|Like this:|Related Articles|How to build Ensemble Models in machine learning? (with code in R)|Data Scientist- Bangalore (5+ years of experience)|
Ankit Gupta
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ensemble modeling is a powerful way to improve the performance of your machine learning models. If you wish to be on the top of leaderboard in any machine learningcompetition or want to improve models you are working on  ensemble is the way to go. The meme below kind of summarizes the power of ensembling:Given the importance of ensemble modeling, we decided to test our community on ensemble modeling. The test included basics ofensemble modeling and its practical applications.A total of 1411 participants registered for the skill test.If you missedtaking the test, here is your opportunity for you to find out how many questions you could have answered correctly.Read on!Below is thedistribution of scores, this will help you evaluate your performance:You can access your performance here. More than 230 people participated in the skill testand the highest score was 31.Here are a few statistics about the distribution.Overall distributionMean Score: 17.54Median Score: 18Mode Score: 215 Easy questions on Ensemble Modeling everyone should knowPowerful Trick to choose right models in Ensemble LearningBasics of Ensemble Learning Explained in Simple EnglishPowerful Trick to choose right models in Ensemble LearningQ1. Which of the following algorithm is not an example of an ensemble method?A. Extra Tree Regressor
B. Random Forest
C. Gradient Boosting
D. Decision TreeSolution: (D)Option D is correct. In case of decision tree, we build a single tree and no ensembling is required.Q2. What is true about an ensembled classifier?1. Classifiers that are more sure can vote with more conviction
2. Classifiers can be more sure about a particular part of the space
3. Most of the times, it performs better than a single classifierA. 1 and 2
B. 1 and 3
C. 2 and 3
D. All of the aboveSolution: (D)In an ensemble model, we give higher weights to classifiers which have higher accuracies. In other words, these classifiers are voting with higher conviction.On the other hand,weak learners are sure about specific areas of the problem. By ensembling these weak learners, we can aggregate the results of their sure parts of each of them.The final result would have better results than the individual weak models.Q3. Which of the following option is / are correct regarding benefits of ensemble model?1. Better performance
2. Generalized models
3. Better interpretabilityA. 1 and 3
B. 2 and 3
C. 1 and 2
D. 1, 2 and 3Solution: (C)1 and 2 are the benefits of ensemble modeling. Option 3 is incorrect because when we ensemble multiple models, we lose interpretability of the models.Q4) Which of the following can be true for selecting base learners for an ensemble?1. Different learners can come from same algorithm with different hyper parameters
2. Different learners can come from different algorithms
3. Different learners can come from different training spacesA. 1
B. 2
C. 1 and 3
D. 1, 2 and 3Solution: (D)We can create an ensemble by following any / all of the options mentioned above. So option D is correct.Q5. True or False:Ensemble learning can only be applied to supervised learning methods.A. True
B. FalseSolution: (B)Generally, we use ensemble technique for supervised learning algorithms. But, you can use an ensemble for unsupervised learning algorithms also. Refer this link.Q6. True or False: Ensembles will yield bad results when there is significant diversity among the models.Note: All individual models have meaningful and good predictions.A. True
B. FalseSolution: (B)An ensemble is an art of combining adiverse set of learners (individual models) together to improvise on the stability and predictive power of the model. So, creating an ensemble of diverse models is a very important factor to achieve better results.Q7. Which of the following is / are true about weak learners used in ensemble model?1. Theyhave low variance and they dont usually overfit
2. They have high bias, so they can not solve hard learning problems
3. They have high variance and theydont usually overfitA. 1 and 2
B. 1 and 3
C. 2 and 3
D. None of theseSolution: (A)
Weak learners are sure about particular part of a problem. So they usually dont overfit which means that weak learners have low variance and high bias.Q8.True or False: Ensemble of classifiers may or may not be more accurate than any of its individual model.A. TrueB. FalseSolution: (A)Usually, ensemble would improve the model, but it is not necessary. Hence, option A is correct.Q9. If you use an ensemble of different base models, is it necessary to tune the hyper parameters of all base models to improve the ensemble performance?A. Yes
B. No
C. cant saySolution: (B)It is not necessary. Ensemble of weak learners can also yield a good model.Q10.Generally, anensemble method works better, if the individual base models have ____________?Note: Suppose each individual base models have accuracy greater than 50%.A. Less correlation among predictions
B. High correlation among predictions
C. Correlation does not have any impact on ensemble output
D. None of the aboveSolution: (A)A lower correlation among ensemble model members will increase the error-correcting capability of the model. So it is preferred to use models with low correlations when creating ensembles.Context  Question 11In an election, N candidates are competing against each other and people are voting for either of the candidates. Voters dont communicate with each other while casting their votes.Q.11 Which of the following ensemble method works similar to above-discussed election procedure?Hint: Persons are like base models of ensemble method.A. Bagging
B. Boosting
C. A Or B
D. None of theseSolution: (A)In bagged ensemble, the predictions of the individual models wont depend on each other. So option A is correct.Q12. Suppose you are given n predictions on test data by n different models (M1, M2, . Mn) respectively. Which of the following method(s) can be used to combine the predictions of these models?Note: We are working on a regression problem1. Median
2. Product
3. Average
4. Weighted sum
5. Minimum and Maximum
6. Generalized mean ruleA. 1, 3 and 4
B. 1,3 and 6
C. 1,3, 4 and 6
D. All of aboveSolution: (D)All of the above options are valid methods for aggregating results of different models (in case of a regression model).Context: Question 13 -14Suppose, you are working on a binary classification problem. And there are 3 models each with 70% accuracy.Q13.If you want to ensemble these models using majority voting method. What will be the maximum accuracy you can get?A. 100%
B. 78.38 %
C. 44%
D. 70Solution: (A)Refer below table for models M1, M2 and M3.Actual outputM1M2M3Output11011110111101110111101111011111111111011110111101Q14. If you want to ensemble these models using majority voting. What will be the minimum accuracy you can get?A. Always greater than 70%
B. Always greater than and equal to 70%
C. It can be less than 70%
D. None of theseSolution: (C)Refer below table for models M1, M2 and M3.ActualOutputM1M2M3Output11000111111100010100101111001011111111111111111111Q15. How can we assign the weights to output of different models in anensemble?1. Use analgorithm to return the optimal weights
2. Choose the weights using cross validation
3. Give high weights to more accurate modelsA. 1 and 2
B. 1 and 3
C. 2 and 3
D. All of aboveSolution: (D)All of the options are correct to decide weights of individual models in an ensemble.Q16. Which of the following is true about averaging ensemble?A. It can only be used in classification problem
B. It can only be used in regression problem
C. It can be used in both classification as well as regression
D. None of theseSolution: (C)You can use average ensemble on classification as well as regression. In classification, you can apply averaging on prediction probabilities whereas in regression you can directly average the prediction of different models.Context Question 17Suppose you have given predictions on 5 test observations.predictions = [0.2,0.5,0.33,0.8]
Which of the following will be the ranked average output for these predictions?Hint: You are using min-maxscalingA. [ 0., 0.66666667, 0.33333333, 1. ]
B. [ 0.1210, 0.66666667, 0.95,0.33333333 ]
C. [ 0.1210, 0.66666667, 0.33333333, 0.95 ]
D. None of aboveSolution: (A)The following steps can be applied to get the output in options AYou can follow this code in python to get the desired result.Q18.
In above snapshot, line A and B are the predictions for 2 models (M1, M2 respectively). Now, You want to apply an ensemble which aggregates the results of these two models using weighted averaging. Which of the following line will be more likely of the output of this ensemble if you give 0.7, 0.3 weights to models M1 and M2 respectively.A) A
B) B
C) C
D) D
E) ESolution: (C)Q19. Which of the following is true about weighted majority votes?1. We want to give higher weights to better performing models
2. Inferior models can overrule the best model if collective weighted votes for inferior models is higher than best model
3. Voting is special case of weighted votingA. 1 and 3
B. 2 and 3
C. 1 and 2
D. 1, 2 and 3
E. None of aboveSolution: (D)All of the statements are true.Context  Question 20-21Suppose in a classification problem, you have following probabilities for three models: M1, M2, M3 for five observations of test data set.M1M2M3Output.70.80.75.50.64.80.30.20.35.49.51.50.60.80.60Q20.Which of the following will be the predicted category for these observations if you apply probability threshold greater than or equals to 0.5 for category 1 or less than 0.5 for category 0?Note: You are applying the averaging method to ensemble given predictions by three models.A.M1M2M3Output.70.80.751.50.64.801.30.20.350.49.51.500.60.80.601B.M1M2M3Output.70.80.751.50.64.801.30.20.350.49.51.501.60.80.601C.M1M2M3Output.70.80.751.50.64.801.30.20.351.49.51.500.60.80.600D. None of theseSolution: (B)Take the average of predictions of each models for each observation then apply threshold 0.5 you will get B answer.For example, in first observation of the models (M1, M2 and M3) the outputs are 0.70,0.80,0.75 if you take the average of these three you will get 0.75 which is above 0.5 that means this observation will belong to class 1.Q21: Which of the following will be the predicted category for these observations if you apply probability threshold greater than or equals to 0.5 for category 1 or less than 0.5 for category 0?A.M1M2M3Output.70.80.751.50.64.801.30.20.350.49.51.500.60.80.601B.M1M2M3Output.70.80.751.50.64.801.30.20.350.49.51.501.60.80.601C.M1M2M3Output.70.80.751.50.64.801.30.20.351.49.51.500.60.80.600D. None of theseSolution: (B)Take the weighted average of the predictions of each modelfor each observation then apply threshold 0.5 you will get B answer.For example, in first observation of models (M1,M2 and M3) the outputs are 0.70,0.80,0.75 if you take the weighted average of these three predictions, you will get 0.745 output (0.70 * 0.4 + 0.80 *0.3 + 0.75* 0.3) which is above 0.5 that means this observation will belongto class 1.Context: Question 22-23Suppose in binary classification problem, you have given the following predictions of three models (M1, M2, M3) for five observations of test data set.M1M2M3Output110010011101111Q22: Which of the following will be the output ensemble model if we are using majority voting method?A.M1M2M3Output11000101011010101111B.M1M2M3Output11010100011110111111C.M1M2M3Output11010100011110101111D. None of theseSolution: (B)Take the majority voting for the predictions of each modelfor each observation.For example for a first observation of models(M1,M2 and M3) the outputs are 1,1,0 if you take the majority voting of these three models predictions you will get 2 votes for class 1 that means this observation will belong to class 1.Q23.When using theweighted voting method, which of the following will be the output of an ensemble model?Hint: Count the vote of M1, M2, and M3 as 2.5 times, 6.5 times and 3.5 times respectively.A.M1M2M3Output11000101011010101111B.M1M2M3Output11010100011110111111C.M1M2M3Ouput11010101011110101111D. None of theseSolution: (C)Follow the steps in question number 20,21 and 22.Q24. Which of the following are correct statement(s) about stacking?A.1 and 2B. 2 and 3C. 1 and 3D. All of aboveSolution: (C)Q25. Which of the following are advantages of stacking?A. 1 and 2B. 2 and 3C. 1 and 3D. All of the aboveSolution: (A)Option 1 and 2 are advantages of stacking whereas option 3 is not correct asstaking takes higher time.Q26: Which of the following figure represents stacking?A.B.C. None of theseSolution: (A)A is correct because it is aggregating the results of base models by applying a function f (you can say a model) on theoutputs of d1, d2 and dL.Q27. Which of the following can be one of the steps in stacking?1. Divide the training data into k folds
2. Train k models on each k-1 folds and get the out of fold predictions for remaining one fold
3. Divide the test data set in k folds and get individual fold predictions by different algorithmsA. 1 and 2
B. 2 and 3
C. 1 and 3
D. All of aboveSolution: (A)The third optionis not correct because we dont create folds for testdata in stacking.Q28. Which of the following is the difference between stacking and blending?A. Stacking has less stable CV compared to Blending
B. In Blending, you create out of fold prediction
C. Stacking is simpler than Blending
D. None of theseSolution: (D)Only option D is correct.Q29. Suppose you are using stacking with n different machine learning algorithms with k folds on data.Which of the following is true about one level (m base models + 1 stacker) stacking?Note:A. You will have only k features after the first stage
B. You will have only m features after thefirst stage
C. You will have k+m features after thefirst stage
D. You will have k*n features after thefirst stage
E. None of the aboveSolution: (B)If you have m base models in stacking. That will generatem features for second stage models.Q30. Which of the following is true about bagging?1. Bagging can be parallel
2. The aim of bagging is to reduce bias not variance
3. Bagging helps in reducing overfittingA. 1 and 2
B. 2 and 3
C. 1 and 3
D. All of theseSolution: (C)1. In bagging individual learners are not dependent on each otherso they can be parallel
2-3 The bagging is suitable for high variance low bias models or you can say for complex models.Q31.True or False: In boosting, individual base learners can be parallel.A. True
B. FalseSolution: (B)In boosting you always try to add new models that correct previous models weaknesses. So it is sequential.Q32. Below are the two ensemble models:
1. E1(M1, M2, M3) and 
2. E2(M4, M5, M6)Above, Mx is the individual base models.Which of the following are more likely to choose if following conditions for E1 and E2 are given?E1: Individual Models accuracies are high but models are of the same type or in another term less diverse
 E2: Individual Models accuracies are high but they are of different types in another term high diverse in natureA. E1
B. E2
C. Any of E1 and E2
D. None of theseSolution: (B)We have to select E2 because it contains diverse models. So option B is acorrect option.Q33. Suppose, you have 2000 different models with their predictions and want to ensemble predictions of best x models. Now, which of the following can be a possible method to select the best x models for anensemble?A. Step wise forward selection
B. Step wise backward elimination
C. Both
D. None of aboveSolution: (C)You can apply both the algorithms. In step wise forward selection, you will start with empty predictions and will add the predictions of models one at a time if they improvethe accuracy of anensemble. In Step wise backward elimination, you will start with full set of features and remove model predictions one by one if after removing the predictions of model give animprovement in accuracy.Q34. Suppose, you want to apply a stepwise forward selection method for choosing the best models for an ensemble model. Which of the following is the correct order of the steps?Note: You have more than 1000 models predictions1. Add the models predictions (or in another term take the average) one by one in the ensemble which improves the metrics in the validation set.
2. Start with empty ensemble
3. Return the ensemble from the nested set of ensembles that has maximum performance on the validation setA. 1-2-3
B. 1-3-4
C. 2-1-3
D. None of aboveSolution: (C)Option C is correct.Q35.True or False: Dropout is computationally expensive technique w.r.t. baggingA. True
B. FalseSolution: (B)Because in dropout, weights are shared and the ensemble of subnetworks are trained together.Q36.Dropout in a neural network can be considered as an ensemble technique, where multiple sub-networks are trained together by dropping out certain connections between neurons.Suppose, we have a single hidden layer neural network as shown below.How many possible combinations of subnetworks can be used for classification?How many possible combinations of subnetworks can be used for classification?A. 1B. 9C. 12D. 16E. None of the aboveSolution: (B)There are 16 possible combinations, of which only 9are viable. Non-viable are (6, 7, 12, 13, 14, 15, 16).Q37. How is the model capacity affected with dropout rate (where model capacity means the ability of aneural network to approximate complex functions)?A. Model capacity increases in increase in dropout rateB. Model capacity decreases in increase in dropout rateC. Model capacity is not affected on increase in dropout rateD. None of theseSolution: (B)The subnetworks have more number of neurons to work with when dropout rate is low. So They are more complex resulting in anincrease in overall model complexity. Refer chap 11 of DL book.Q38. Which of the following parameters can be tuned for finding good ensemble model in bagging based algorithms?1. Max number of samples
2. Max features
3. Bootstrapping of samples
4. Bootstrapping of featuresA. 1 and 3
B. 2 and 4
C. 1,2 and 3
D. 1,3 and 4
E. All of aboveSolution: (E)All of the techniques given in the options can be applied to get the good ensemble.Q39. In machine learning, an algorithm (or learning algorithm) is said to be unstable if a small change in training data cause thelarge change in thelearned classifiers.True or False: Bagging of unstable classifiers is a good idea.A. True
B. FalseSolution: (A)Refer the introduction part of this paper.Q40. Suppose there are 25 base classifiers. Each classifierhas error rates of e = 0.35.Suppose you are using averaging as ensemble technique. What will be the probabilities that ensemble of above 25 classifiers willmakeawrong prediction?Note: All classifiers are independent of each otherA. 0.05
B. 0.06
C. 0.07
D. 0.09Ans: BSolution:Refer this linkI hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge of ensemble modeling.If you have any doubts in the questions above, let us know through comments below. Also, If you have any suggestions or improvements you think we should make in the next skill test, you can let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2017/02/40-questions-to-ask-a-data-scientist-on-ensemble-modeling-techniques-skilltest-solution/
Data Scientist- Bangalore (5+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|40 Questions to ask a Data Scientist on Ensemble Modeling Techniques (Skilltest Solution)|6 Deep Learning Applications a beginner can build in minutes (using Python)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : Good to have Banking domain experience.
Candidate should have engineering qualification.
Task Info : Skiils Required:
College Preference : no-bar
Min Qualification : ug
Skills : algorithms, banking, data mining, Log Analytics, machine learning, r, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/data-scientist-bangalore-5-years-of-experience/
6 Deep Learning Applications a beginner can build in minutes (using Python),"Learn everything about Analytics|Introduction|Table of Contents|1. Applications using existing APIs|2. Open Source Applications|3. Other notable Resources|End Notes|Learn, compete, hack and get hired!","1.1.1 Advantages of Deep Learning APIs|1.1.2 Disadvantages of Deep Learning APIs|1.2 Colorize photos using Deep Learning (Algorithmia API)|1.3 Building a ChatBot (Watson API)|1.4 News Aggregator based on Sentiment (Aylien API)|2.1.1 Advantages of Open Source applications|2.1.2 Disadvantages of Open Source applications|2.2 Sentence Correction with Deep Learning|2.3 Convert Male portraits to female and vice-versa with Deep Learning|2.4 Build a deep reinforcement learning bot to play Flappy Bird|Share this:|Like this:|Related Articles|Data Scientist- Bangalore (5+ years of experience)|Python Developer- Gurgaon- (3+ Years of Experience)|
Faizan Shaikh
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Deep Learning has been the most researched and talked about topic in data science recently. And it deserves the attentionit gets, as some of the recent breakthroughs in data science are emanating from deep learning. Its predicted that many deep learning applications will affect your life in the near future. Actually, I think they are already making an impact.However, if you have been looking at deep learning from the outside, it might look difficult and intimidating. Terms like TensorFlow, Keras, GPU based computing might scare you. But, let me tell you a secret quietly  it is not difficult! While cutting edge deep learning will take time and effort to follow, applying them in simple day to day problems is very easy.It is alsofun. I kind of re-discovered the fun and curiosity of a child while applying deep learning. Through this article, I will showcase 6 such applications  which might look difficult at the outset, but can be achieved using Deep Learning implementation in less than an hour. This article is written to showcase these ground-breaking works and give you a taste of how they work.Lets start!P.S. We assume that you would know basics of Python. If not, just follow this tutorial and then you can start from here again.APIs are nothing but a software running on the other side of the internet in a remote PC which can be accessed locally. For example, you plug in bluetooth speakers to your laptop even ifyour machinemight have inbuilt speakers. So, we are able to access the speaker remotely while sitting on our laptop.APIs work on similar concept  some one has already done the hard work for you. You can use it to solve the problem at hand quickly. For more details on API, read here.Ill list out some advantages and disadvantages of building apps using API.If you want to know more about what APIs are, check out this blogLets start with our applications!Automated Image Colorization has been a topic of interest among the computer vision community. It seems surreal to get a colorful photo of a black and white image. Imagine a 4-year old picking up a crayon and gets engrossed in his coloring book! Could we teach an artificial agent do the same and just imagine stuff?Of course, this is a hard problem! This is because we as humans get trained each and every day by seeing how things are colored in real life. We might not notice but our brain is capturing each moment of our lives and extracting meaningful information from it, such as sky is blue and grass is green. This is hard to model in an artificial agent.A recent study shows that if we train a neural network enough on a large number of the especially prepared dataset, we can essentially get a model which could hallucinate colors in a grayscale image. Heres a demonstration of an image colorizer:To practically implement this, we use an API developed by Algorithmia.Requirements and Specifications:Step 1: Register on Algorithmiaand get your own API key. You can find your API key in your profileStep 2: Install algorithmia by typingStep 3: Select a photo you want to colorize and upload it to the data folder provided by algorithmia.Step 4: Create a file locally and name it trial1.py . Open it and write the code as below. Notice that you have to insert the location of your image in data folder and API keyStep 5: Open command prompt and run your code by typing python trial.py. The resulting output will be automatically saved in your data folder. Heres what I got:That is it  you have just created a simple application which acts as a child and can fill in colors in images! Exciting stuff.Watson is a great example to show what an artificial agent can achieve. You may have heard the story of Watson beating humans at a Question and Answering game. Although Watson uses an ensemble of many techniques for working, deep learning still is a core part of its learning process, especially in natural language processing. Here we would use one of the many applications of Watson, to build a conversation service, aka chatbot. A chatbot is an agent that respond as humans do on common questions. It can be an excellent point of contact to customers and respond to them in a timely manner.Here we would use one of the many applications of Watson, to build a conversation service, a.k.a chatbot. A chatbot is an agent that respond as humans do on common questions. It can be an excellent point of contact to customers and respond to them in a timely manner.Heres a demonstration of the platform:Requirements and Specifications:Lets see a step-by-step example of how to build a simple chatbot with Watson.Step 1: Register on Bluemix and activate your conservation services to get your credentialsStep 2: Open terminal and run command as below:Step 3: Make a file trial.py and copy the following code in it. Remember to put your own credentials in it.Step 4: Save your file and run it by typing in console python trial.py. You will get an output in the console which would be the response of Watson for your input.Input: Show me whats nearbyOutput: I understand you want me to locate an amenity. I can find restaurants, gas stations and restrooms nearby.If you want to build a full-fledged project of conversation service with animated car dashboard (as shown in the above gif), view this github repository.A chatbot and an application to color images in under a few minutes  not bad Sometimes we want to see only the good in the world. How cool would it be to filter out all the bad news when reading a newspaper and only see good news!With advanced natural language processing techniques (one of which is deep learning), this is becoming increasingly possible. You can now filter out news by sentiment and present it to the readers.We will see and application of this using Ayliens News API. Below are the screenshots of the demo. You can build your own custom query and check out the result for yourself.  Lets see an implementation of this in pythonRequirements and specifications:Step 1: Register for an account on Aylien website.Step 2: Get API_key and App_ID from your profile when you loginStep 3: Install Aylien News API by going in your terminal and typingStep 4: Create a file trial.py and copy the following codeStep 5: Save the file and run it by typing python trial.py. The output will be a json dump as follows:Woah! I can visualize a chatbot at your service and a service like Alexa reading you news of your interest now! I am sure, you will be as excited about deep learning by now!The best thing thats helping the research community right now is its open source mindset. Researchers are ready to share whatever they achieved so that deep learning research would grow, and as a result, its growing leaps and bounds! Here I mention some of the open source contributions and their variants which have been created from research papers.Note: For open sourced applications, I would recommend you to go through their official repository. This is because some of them are still in infancy stage and may break for unknown reasonsLets look at some open source applications!The systems nowadays can easily detect and correct spelling mistakes, but correcting a grammatical error is a bit harder. To improve on this a bit, we can use deep learning to correct these sentences for us. This repository is an attempt especially for that.Heres a sequence predicting neural network was trained on a corpus of grammatically wrong sentences along with its corrected counterpart. The trained model shows promising results for sentence correction. Heres an example below:Input: Kvothe went to market
Output: Kvothe went to the marketYou can check out a demo on the site: http://atpaino.com/dtc.htmlThe model still fails to correct all the sentences, but with more training data and efficient deep learning algorithms, the results could be improved.Requirements:Step 1: Install tensorflow from their official website. Also, download the repository from GitHub and save it locally from https://github.com/atpaino/deep-text-correctorStep 2: Download the dataset (Cornell Movie-Dialogs Corpus) and extract it in your working directoryStep 3: Create the training data by running the commandAnd create train, validation and test files and save them in the current working directoryStep 4: Now train the deep learning model by:Step 5: The model requires some time to train. After training, you can test it by:Before I speak anything on the application, justobserve the following results:Here the first image is converted into second by a deep learning model! This is really a fun application to show what deep learning can do! In its core, the application uses GAN (generative adversarial network), which a type of deep learning which is capable to new examples on its own.Requirements:Just a warning before you implement this. Training a model takes too long if you are not using a GPU. Even with a high-end GPU (Nvidia GeForce GTX 1080), the training takes 2 hours for one image.Step 1: Download the repository and extract it locally https://github.com/david-gpu/deep-makeoverStep 2: Download the Align&Cropped Images from CelebA dataset. Create a datasets folder by name dataset and extract all images in itStep 3: Train the model by:and then test it by passing the image you want to convertYou may have played Flappy Bird sometime in the past. For those who dont know, it was an extremely addictive Android game in which the aim was to keep flying the bird in air by avoiding obstacles.In this application, a flappy bird Bot is created by using advanced reinforcement learning techniques. Heres a demo of a trained bot.Requirements:Implementing this is easy, as most of the nuts and bolts are included.Step 1: Download the official repository.Step 2: Make sure you have all the dependencies installed. Once you have, run the command as below.We have just scratched the surface of what a deep learning model is capable of. There are many research papers being released every day which gives rise to many such applications. Now its a matter of who thinks the idea first!Ill list out some of the links and resources which I found worth looking atI hope you had fun reading this article. I bet these applications would have blown your mind. Some of you might be aware about these applications & some of you might not be. If you have worked on any of these applications, share your experience with us. The other reader and I would definitely want to know about it.If you have come across these applications for the first time, then let me know which one excited you the most. Share your suggestions / feedback with us in the comments section below.Stay ahead this year with complete learning path for Deep Learning",https://www.analyticsvidhya.com/blog/2017/02/6-deep-learning-applications-beginner-python/
Python Developer- Gurgaon- (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|6 Deep Learning Applications a beginner can build in minutes (using Python)|Business Analyst-Bangalore/Chennai-( 2-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Primary Skills:* Fluency in Python: At least 2.5+ year of hands-on experience with programming languages Python.* Proficient in web based technologies, i.e. HTML, JavaScript, CSS, AJAX, HTTP.* Innovativeness; ability to think outside the box.* Creativity and problems solving ability.* Very good communication skills; must be able to gather requirements from potential users and discuss the requirements effectively with a technical team of developers.Description:* Selected candidate will be on permanent roll of company* He/She will be deputed at client side i.e. A top notch Global ISP , Gurgaon for long duration.* Shift Time: 08:00 am  05:00 pm ( 5 days in week) ( Cab & meals facility free of cost).Educational Qualification:Min Qualification: BS / BE / B.Tech / MCA or equivalent.Experience: 3 yrs.Location: Gurgaon
College Preference : no-bar
Min Qualification : ug
Skills : CSS3, HTML5, JavaScript, python
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/python-developer-gurgaon-3-years-of-experience/
Business Analyst-Bangalore/Chennai-( 2-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Python Developer- Gurgaon- (3+ Years of Experience)|DIRECTOR  CREDIT RISK (BFSI/RISK ANALYTICS)- GURGAON -(10-15 YEARS OF EXPERIENCE)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  7 years
Requirements : 
Task Info : Key ResponsibilitiesAs a member of the BI Team the primary focus of this position would be to develop, support and maintain product offerings using the SQL and standalone or enterprise BI platform.The BI Developer will need to intimately understand the product requirements and recommend the most optimal strategy to implement them, provide timing and sizing estimates for the effort, and implement the enhancements.Additionally, this BI Developer will also work closely with organizational functions in the enhancement of existing products.The creation and maintenance of product documentation will also be an important function of the BI Developer.RequirementsUnderstand organizational processes, interfaces, and considerations of existing BI implementationsDetermine optimal design strategy to implement requirementsProvide timing and sizing estimates to implement recommended design, and present it to team for reviewImplement approved solution designConduct data integrity validations / performance tests on implemented solutionSupport enhancement of existing products on an as-needed basisSupport ad hoc reporting projects on an as-needed basisDevelop and maintain product documentationProvide Level 2 product support when neededDesired SkillsMinimum of 2 years of experience in the use of anyone of the standalone or Enterprise tools like SAS / R / MicroStrategy / Qlikview / Pentaho / Tableau..etc, in all phases of the project life cycle : ETL development, Scripting and front-end designProficiency in SQL is a mustHands-on report development, dashboard creation, freeform SQL reports/metric/attribute/custom group/consolidation, and cubes creationWorking with large data setsIntegrating data from more than one sourceDeveloping most optimal scripts for a given solutionData modellingStrong knowledge of star schemasOptimization of data model for query performanceExperience with complex data models involving more than 5 tablesFront-end designIntimate familiarity with all charts and graphs to determine best one to use for a given situationAutomation of user actionsStrong fundamentals in basic graphic design concepts to create aesthetically pleasing interfaces
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, etl, pentaho, qlikview, r, sas, sql, tableau
Location : Bengaluru, Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/business-analyst-bangalorechennai-2-7-years-of-experience/
DIRECTOR  CREDIT RISK (BFSI/RISK ANALYTICS)- GURGAON -(10-15 YEARS OF EXPERIENCE),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst-Bangalore/Chennai-( 2-7 Years of Experience)|AWS ARCHITECT- GURGAON (12 YEARS OF EXPERIENCE)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  15 years
Requirements : 
Task Info : Role & Responsibilities at job:To be a Leader and Manager who can work on multiple project streams for a client and applies Analytics for better business decision making especially in the area of Lifesciences/ Pharmaceutical domain.Key ResponsibilitiesLeadershipLead & guide the team independently or with little support to implement & deliver complex project assignments.Independently drive delivery in Axtrias risk practice on projects related to: quantitative risk modeling (credit, market and operational risk (a good plus)), ICAAP/CCAR Stress testing, Independent model validation, Loss forecasting and Regulatory compliance reportingManage a team of seasoned data professionals.Project ManagementGet accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans.Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others work.Ability to think in advance about potential risks and mitigation plans.Logical ThinkingAble to think analytically, use a systematic and logical approach to analyse data, problems, and situations. Must be able to guide team members in analysis.Handle Client RelationshipManage client relationship and client expectations independently.Should be able to deliver results back to the Client independently.Assist clients in developing and improving quantitative risk management frameworks, model monitoring mechanisms and modeling methodologies (from regulatory perspective)Undertake initiatives for practice development, help carve out service offerings, products and collateral (Webinars, Whitepapers, business analysis decks)Help build risk capabilities, come up with new ideas for practice development and take them to completionWork closely with leadership and offshore teams to build new capabilities in risk (credit, operational, model monitoring)Establish Axtria as a thought leader in risk management and regulatory advisory space through deep point of view blogs, methodology whitepapers, regulatory elaborationsManage client communicationsEligibility CriteriaRequired ExperienceMinimum of 10-15 years of experience in risk analytics in banking and financial services industry.Deep modeling and technical skillsStrong experience of regulatory environment (Basel II / III, CCAR)Advanced usage of at least one statistical package like SASStrong communication skills (oral and written)Consulting experience with client interfacing role is a plus.Ability to participate in selling solutions and managing accounts with hands on approach to delivery of proposed solutionsDesirable qualitiesCustomer Focus  Dedicated to meeting the expectations of internal and external clients.Problem Solving  Uses rigorous logic and methods to solve difficult problems with effective solutions. Probes all fruitful sources for answers. Is excellent at honest analysis. Looks beyond the obvious and doesnt stop at the first answers.Learning on the Fly  Learns quickly when facing new problems. A relentless and versatile learner.Drive for result  Able to set priorities; pursue tasks tenaciously & with a need to finish. Able to overcome setbacks which may occur along the way.QualificationMasters in Quantitative Finance, Statistics, Econometrics, MBA or related fields (PhD preferred) Certification like CFA, FRM is a plus
College Preference : no-bar
Min Qualification : pg
Skills : banking, business analysis, modeling, risk modeling
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/director-credit-risk-bfsirisk-analytics-gurgaon-10-15-years-of-experience/
AWS ARCHITECT- GURGAON (12 YEARS OF EXPERIENCE),Learn everything about Analytics,"Share this:|Like this:|Related Articles|DIRECTOR  CREDIT RISK (BFSI/RISK ANALYTICS)- GURGAON -(10-15 YEARS OF EXPERIENCE)|SR. ASSOCIATE/PROJECT LEAD  BUSINESS INTELLIGENCE (TABLEAU)- San Francisco, CA (7 YEARS OF EXPERIENCE)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 12  years
Requirements : 
Task Info : Responsibilities:Solution DevelopmentArchitecting, Planning and Implementation of cloud infrastructure for end-to-end Data Warehouse and Business Intelligence applications utilizing best practices from Data Architecture, Data Integration and Data Management solutions. (AWS cloud based project experience will be preferred)Define Governance (including Backup and Recovery Policy) for the Enterprise Data Warehouse (EDW) and BI environments using AWSPerform hands-on research on new AWS services, as they are made available through AWS partnershipDesigning, Installing and Configuring relevant AWS partner applications (along with monitoring utilities).Build / Setup new utilities for efficiently managing AWS services for any implementation.Perform Performance Tuning of RDS databases (Oracle, SQL Server, Postgres, etc.) and RedshiftPerform hands-on project implementation using various AWS services like EC2, EBS, S3, SNS, RDS, Redshift, etc.Perform Capacity Management, Sizing and TCO Calculation for Databases and Servers to be procured with AWS.Defining Database and ETL Architecture for systems involving Data Warehouse, ETL, BI Reporting, Data Marts, ODS, Data Governance, Data Quality, Meta Data Management etc. Assist in effort estimation for new projects/proposals.Demonstrate technical leadership; prepare and review cloud architecture blueprint / roadmap for customers.DeliveryEnsure delivery of technical engagements by exceeding customer expectations.Provide expert consultation on cloud based services and best practices to customers and internal teamsPrepare AWS based ETL and BI Infrastructure Estimation and Pricing for customer and internal projectsLead small to medium size team, and manage the execution by planning infrastructure activities, assigning tasks to team, tracking and monitoring the tasks, and working with close coordination with the project manager.TrainingProvide training and support projects; educate teams / customers on the value of Cloud based implementationBusiness DevelopmentNeed to actively contribute to Pre-Sales /BD for custom solution designing as per customer requirementAuthor solutions for current and potential customer concerns, including responding to RFPsQualification:Bachelors degree in computer science or related fieldRelevant AWS Architect Certifications are mandatoryExperience working on Pharma/Life Sciences projects will be a plusA minimum of 6 years of relevant Data Management and DW-BI design and architecture experienceDesirable:Overall 12+ years of experience in technical leadership designing and implementing data warehousing and Business Intelligence solutions. Must have designed and implemented end to end services for DW and BI applications as an AWS Cloud Architect for at least 2 large scale implementations.Should have successfully done end to end production DW implementations (preferably in Pharma or Life Sciences)Good understanding of Cloud Computing concepts, such as, IaaS, PaaS, and SaaSExperience in architecting, optimizing, & maintaining large enterprise databases and applications.Thorough hands-on DBA knowledge using Oracle, Teradata, Netezza or other databases will be preferred.Windows and Linux based Application Server Administration knowledge (hands-on) is mandatory.In-depth hands-on experience on Amazon Web Services public cloud and their various services such as EC2, EBS, S3, SNS, RDS, Redshift, etc.Expertise in executing AWS solution in both public and hybrid cloud setup.Around 4 to 6 years of experience in executing AWS-based cloud implementations.Around 4 to 6 years of experience in cloud based application architecture design and proposals (including pricing).Expert Knowledge of SQL (Query) tuning using specific constructs, optimizing batch process as well tuning various ETL jobs.Knowledge in Data Analysis & Logical Data Modelling (Cloud, MDM and Big Data project knowledge will be a plus)Hands-on experience in designing scalable, failover, deployable architecture and application migration on AWS.Should have experience in developing data dictionaries and data libraryAbility to classify the data as per industry regulations or security compliances (including encryption / decryption)Working knowledge of disaster recovery and business continuity planning of database and application servers.Good to have experience working on ETL and BI Tools like SSIS, Informatica, MicroStrategy, Tableau, QlikView, etc.Hands-on experience of data processing on Redshift using ELT tools like Matillion, Talend, etc will be a big plus.Knowledge of other public cloud providers will be preferred.Behavioral CompetencyProven ability to handle multiple projects while meeting deadlines and documenting progress towards those deadlines.Excellent communication skills (must be able to interface with both technical and business leaders in the organization)Ability to be a self-starter and can provide leadershipStrong analytical skills to solve and model complex business requirements are a plus
College Preference : no-bar
Min Qualification : ug
Skills : aws, business intelligence, Data Warehouse, etl, oracle, RDBMS, saas, sql server, sql server integration service (SSIS), talend
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/aws-architect-gurgaon-12-years-of-experience/
"SR. ASSOCIATE/PROJECT LEAD  BUSINESS INTELLIGENCE (TABLEAU)- San Francisco, CA (7 YEARS OF EXPERIENCE)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|AWS ARCHITECT- GURGAON (12 YEARS OF EXPERIENCE)|MANAGER/SENIOR MANAGER (BIG DATA SOLUTIONS)- GURGAON  (10 YEARS OF EXPERIENCE)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  years
Requirements : 
Task Info : Job Description:Looking for a season professional experienced in managing data management/BI operations, ad-hoc analytics and reporting in a fast paced environment. The person will be a single point of contact to the client across multiple streams and will manage on-going operations as well as to provide analytics and reporting support on new projects.The person should have ability to engage with multiple stakeholders to understand their business needs, translate into analytics requirements and work with Axtrias offshore team to create and deliver high quality output.Desired Skills & Experience2+ years of experience in independently running data mart and reporting operations. Experience in data warehouse/data mart implementation a plus3+ years of experience in Tableau. Should have thorough knowledge of data management and development of visualizations in tableau.End-to-end experience in dashboard development. Should have ability to gather business requirements, design dashboard UI, create BRD and lead development of dashboards.Hands on experience in SAS. Ability to write advanced queries and carry out data transformation + ad-hoc analyticsAt-least 2 years of experience in a client facing role. Should have ability to engage with business stakeholders, understand business needs and translate into analytics requirements.Project management experience desirable. Ability to independently manage projects across multiple work streams.Understanding of the healthcare domain and experience in working with healthcare clients will be a plusExperience in marketing analytics will be a plus.OtherU.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We sponsor work visas.Flexibility to travel and/or relocate within the US as per project requirements.The exact nature of duties as well as the salary and compensation package will be commensurate with experience and salary history.
College Preference : no-bar
Min Qualification : ug
Skills : data management, Data Warehouse, marketing analytics, sas, tableau
Location : California, San Francisco
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-associateproject-lead-business-intelligence-tableau-san-francisco-ca-7-years-of-experience/
MANAGER/SENIOR MANAGER (BIG DATA SOLUTIONS)- GURGAON  (10 YEARS OF EXPERIENCE),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SR. ASSOCIATE/PROJECT LEAD  BUSINESS INTELLIGENCE (TABLEAU)- San Francisco, CA (7 YEARS OF EXPERIENCE)|ASSOCIATE DIRECTOR/DIRECTOR  COMMERCIAL OPERATIONS ANALYTICS- Berkeley Heights, NJ (6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : 
College Preference : no-bar
Min Qualification : ug
Skills : aws, bigdata, data modeling, hadoop, hive, java, MongoDB, pig, PLSQL, python, spark, sql, storm
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/managersenior-manager-big-data-solutions-gurgaon-10-years-of-experience/
"ASSOCIATE DIRECTOR/DIRECTOR  COMMERCIAL OPERATIONS ANALYTICS- Berkeley Heights, NJ (6 Years Of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|MANAGER/SENIOR MANAGER (BIG DATA SOLUTIONS)- GURGAON  (10 YEARS OF EXPERIENCE)|Director- Gurgaon ( 10 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  years
Requirements : 
Task Info : Job Description:
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business intelligence, c++, C, sas, sql server, statistical modeling, VBA
Location : Berkeley, Berkeley Heights, New Jersey
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/associate-directordirector-commercial-operations-analytics-berkeley-heights-nj-6-years-of-experience/
Director- Gurgaon ( 10 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|ASSOCIATE DIRECTOR/DIRECTOR  COMMERCIAL OPERATIONS ANALYTICS- Berkeley Heights, NJ (6 Years Of Experience)|UI- Developer- Gurgaon (3+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : Desirable:-
College Preference : tier1-any
Min Qualification : ug
Skills : analytics, anova, clustering, excel, linear regression, oracle, r, sas, segmentation, sql server, statistical techniques, time series
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/director-gurgaon-10-years-of-experience/
UI- Developer- Gurgaon (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Director- Gurgaon ( 10 Years of Experience)|Lead BI Analyst- Gurgaon (5-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Description:Knowledge of web/graphic design tools and technologies.Pro-actively reviews the application and researches areas of improvements from design perspectiveDocuments and shares design recommendationsCapable of working on Customer related application issuesProblem research and resolutionNeed to assist in multiple projectsRapid learning capacityStrong sense of ownership, urgency and responsibility for the health and well-being of the systemsBuild good relationships with business and IT partnersAbility to provide weekly status on deliverables, issues, etc.Respond quickly and effectively to production issues.Take responsibility for seeing issues through to resolutionMust have good analytical skillsHave an operational mindset  diligent about watching publishes, tuned into our audits/reconciliations that come outCan operate with some urgency when neededAbility and experience to work in cross cultural virtual teamsCommunication Skills  High level of proficiency (read and write)  Excellent communication and stakeholder management skills across multiple remote locationsMulti-tasking  ability to drive multiple deliverables at the same timeProficient in MS office247 support of the production system for break/fix situations.Skills Required: Photoshop, Dreamweaver, HTML 5, JavaScript, JQuery, Bootstrap, Flash,
College Preference : no-bar
Min Qualification : ug
Skills : analytics, excel, HTML5, JavaScript, jquery, Power point, web application
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/ui-developer-gurgaon-3-years-of-experience/
Lead BI Analyst- Gurgaon (5-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|UI- Developer- Gurgaon (3+ Years of Experience)|Consultant/Sr. Consultant (Murex)- Gurgaon (2 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  7 years
Requirements : 
Task Info : Description:Seasoned BI manager having 5-7 years of experience in BI on large scale Enterprise BI ProjectsTechnical skillsExpertise in Business Requirement gathering, Analysis & conceptualizing high- level architectural framework & DesignStrong exposure on BI tools and technologies, Qlikview or Qliksense expertise is a must.Experience in BI performance optimizationExperience on other BI tools like tableau, power BI is preferredAt least 1-2 years of experience in clients engagement managementHave managed BI Projects with teams.Have good DB query skills like sql /PL Sql etc.Strong client communication skills and project management skillsEffective Communication with the client on the approach, solution and insights generatedTake responsibility for technical skill-building within the organization (training, process definition, research of new tools and techniques, etc)
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, PLSQL, qlikview, sql, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/lead-bi-analyst-gurgaon-5-7-years-of-experience/
Consultant/Sr. Consultant (Murex)- Gurgaon (2 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead BI Analyst- Gurgaon (5-7 Years of Experience)|BI Analyst  Gurgaon (2-5 Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Description:Have basic understanding of XML and XSLT.Basic knowledge of SQL and UNIX.Ability to work with or configuration, accounting, workflow, payment, settlement, market data, generators, views and reports.Other Skills Required:Effective verbal as well as written communication skills.Effective time management skills.A commitment to quality and a thorough approach to workShould be able to travel across globe depending upon on the business requirement.Should not be location bound or preference. A commitment to quality and a thorough approach to workMin. Experience Required: 2 yr in MLC & VAR module                 
College Preference : no-bar
Min Qualification : ug
Skills : sql, unix, xml
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/consultantsr-consultant-murex-gurgaon-2-years-of-experience/
BI Analyst  Gurgaon (2-5 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant/Sr. Consultant (Murex)- Gurgaon (2 Years of Experience )|Lead Ops Service Engineer-Gurgaon (5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Description:Seasoned BI developer having 2-5 years of experience in BI on large scale Enterprise BI ProjectsExpertise in Business Requirement gathering, Analysis & conceptualizing high- level architectural framework & DesignShould have good understanding of Big Data and other emerging trends in BI/ DW areaStrong exposure on BI tools and technologies, Qlikview expertise is a must.Should have done scripting in qlik technologies.Exposure of D3 charts. How to embed them in the Qlik dashbosardsShould be able to create access matrixUnderstanding of SQL database and queriesExperience in Qliksense is preferable.Other skillsCommunicate with the client on the approach, solution and insights generatedTake responsibility for technical skill-building within the organization (training, process definition, research of new tools and techniques, etc)Exhibit thought leadership internally and externally by publishing white papers, participating in discussion forums and seminars
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, business intelligence, qlikview, sql
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/bi-analyst-gurgaon-2-5-years-of-experience/
Lead Ops Service Engineer-Gurgaon (5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|BI Analyst  Gurgaon (2-5 Years of experience)|Senior and Chief Data Scientist- Bangalore (5-15 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Description:Minimum Job Description (Purpose of the Position) Proven experience in database design, data modeling, SQL server, T-SQL, query optimization, common DW principles and ETL development a plus including SSAS, SSRS, SSIS. Detail oriented, with ability to produce high quality analytical reports and specifications. Reporting to offshore lead and communication with US based Project Manager (at times clients too). Providing support to clients/customers over conference calls/emails/IM. Documentation of activities performed. Ensure that established standards, processes and best practices are adhered to. Work under supervision executing the responsibilities of the job and ensure that all commitments and deliverables are met. Manage and support SQL Server databases that include but not limited to- writing complex SQL queries, stored procedures, functions, triggers, managing permissions etc.  preferred. In-depth understanding of SSIS based solutions. Open to work in US/UK shift timings.Key Roles and Responsibilities in detail Monitor, troubleshoot & resolve SQL Jobs (Database processing & tabular cube processing) Communicating with teams upon success / failures Engaging with FTE Leads for any clarifications on timely manner Streamlining the job failures so it doesnt fail again with similar reason Will be involved in Development (Sprint activities) depends on bandwidth Feature Analysis, Feature, Code & output validation- Deployment release management (Dev, UAT & Production) Job development Infrastructure Management & Run Management Job Logging & Auditing- Feature Testing Data analysis and profiling acquisition/usage i.e. AX7, CRMOL, any new product.Competencies required other than skills: Advanced Excel Skills (Pivot table modifications, Data Connections, Power Pivot, Power Query & Power BI) Advanced SQL Services Skills (Reading and writing SProcs, Data Models, Data manipulation queries & Joins etc.) and intent to learn further Experience working in VSTF & TFS (Navigating source control explorer & Opening Tabular Models, Managing source code) Experience reading and writing Multidimensional Expressions (MDX) Experience with navigating, modifying and managing SSAS Cubes Good verbal/written communication skills to comprehend assigned work and provide status. Should be able to work in a team of data analysts. Very Good analytical and problem solving skills on given data to provide insights Experience in using MS Office (Excel, Word, Outlook and SharePoint). Perform in-depth statistical/data analysis for reporting purpose. Be able to independently work on multiple tasks under some supervision. Ability to ""think outside the box"" and ability to work productively in a team. Familiarity with the Microsoft system & environment preferred.
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, database, data modeling, etl, excel, sql server, sql server integration service (SSIS), ssrs
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/lead-ops-service-engineer-gurgaon-5-years-of-experience/
Senior and Chief Data Scientist- Bangalore (5-15 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Lead Ops Service Engineer-Gurgaon (5 years of experience)|Analyst- Bangalore (1-2 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  15 years
Requirements : 
Task Info : Roles & Responsibilities: Do you have a passion for creating data-driven solutions for challenging business problems? We needs outgoing, curious, interdisciplinary data savvy specialists to organize and interpret Big Data to inform client decision makers, drive successful operations, and shape technology and resource investments. We are looking for individuals from diverse educational backgrounds to fill the roles of Senior and Chief Data Scientists. As a Data Scientist, you will get to work with advanced hardware, software and techniques to develop computational algorithms and statistical methods that find patterns and relationships in large volumes of data. You will work as a data miner, statistician, and business analyst. You will have the opportunity to work across many areas to analyze operational, digital, voice, mobile data, and consumer behaviour; with a variety of people and technologies to bring about advanced insights for our clients. We would expect you to be passionate about delivering accurate and actionable analyses to business decision-makers and be able to understand and creatively solve problems that span insights, techniques, technology, and business. Successful applicants will have keen technical insight, creativity, initiative and a curious mind to try and solve open-ended problems. A fantastic opportunity to learn communicating conclusions clearly to a lay audience and become experts through continued education, attending and/or presenting at academic and technical conferences, and collaborating with the external community. White paper creation and presentation is highly encouraged and sponsored. Educational Qualifications: Work Experience:Competency Requirements: Technical Competency Requirements: BehaviouralOther:
College Preference : tier1-any
Min Qualification : pg
Skills : bigdata, data mining, decision trees, etl, forecasting, hadoop, hbase, hive, java, linear regression, logistic regression, machine learning, mapreduce, nlp, nosql, pig, predictive modeling, python, r, RDBMS, regression, sas, spss, teradata, text mining, time series, Weka
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/senior-and-chief-data-scientist-bangalore-5-15-years-of-experience/
Analyst- Bangalore (1-2 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior and Chief Data Scientist- Bangalore (5-15 Years of Experience)|40 Questions to test a Data Scientist on Clustering Techniques (Skill test Solution)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : 
Task Info : An analyst will work closely with our clients and internal teams on analytical projects. The idealcandidate is someone who enjoys working in a fast paced dynamic work environment. They will need tobring together analytical and logical thinking to balance the rigor of science with the businessapplications. The broad responsibility of the role includes:- Being a part of the team working with the clients of multiple geographies. Applying analytical tools and techniques and effectively communicating the results to variousinternal/external stakeholders Understand the nature, structure & quality of data, identify data issues and suggest solutions Execute planned analysis to meet the project objectives Ensure on-time & error free delivery, proactively escalate and resolve issues when required Open to travel to client locations as necessaryWhat are we looking for?Essential Qualifications: Masters Degree or higher from a reputed institution, with excellent quantitative skills andbusiness acumen. Bachelors degree (4 years of education, preferably engineering from institutions like IIT, NIT, RITetc. will also qualify) Field of Education: Engineering, Mathematics, Computer Science/Applications, OperationsResearch, Statistics, Econometrics or similar quantitative areas.Experience: 1-2 years of experience in analytics execution and delivery.Skills and Knowledge: Good knowledge of SAS Knowledge of any other statistical package like SPSS, R etc. would be an added advantage. Excellent knowledge of Microsoft office. Knowledge of statistical concepts like Predictive modeling, segmentation, forecasting etc. isdesired.Competency: Results/Output/Attention to Detaili. Drive for success, organized, orientation toward- get it done- problem solving proactiveapproach to meet client deliverables.ii. Ensure that client/internal deliverables meet the standards set for project delivery.Subject Matter Expertise/Skillsi. Candidate should exhibit the ability to absorb and develop domain knowledge in businessverticals/areas they are working or have worked in the past. Critical Thinkingi. Self-starter who can quickly learn and adapt new techniques and apply the same to the workareas.ii. Assumes ownership, requires minimal supervision and consistently delivers excellence. Teamwork/Collaborationi. Exhibit values and contribute by building positive work environment.ii. Contribute to the Knowledge management frameworkiii. Participate actively in various internal discussion forums.iv. Ability to cope with continuous transformations. Communication  Verbal/Writteni. Excellent written and oral communication skills
College Preference : tier1-any
Min Qualification : ug
Skills : analytics, excel, forecasting, Power point, predictive modeling, r, sas, segmentation, spss, statistical techniques
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/analyst-bangalore-1-2-years-of-experience/
40 Questions to test a Data Scientist on Clustering Techniques (Skill test Solution),"Learn everything about Analytics|Introduction|Overall Results|Helpful Resources|Questions & Answers|End Notes|Learn, compete, hack and get hired!","Share this:|Like this:|Related Articles|Analyst- Bangalore (1-2 Years of Experience)|40 must know Questions on Base SAS for Analysts (Skill test Solution)|
Saurav Kaushik
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The idea of creating machines which learn by themselves has been driving humans for decades now. For fulfilling that dream, unsupervised learning and clustering is the key. Unsupervised learning provides more flexibility, but is more challenging as well.Clustering plays an important role to draw insights from unlabeled data. Itclassifies the data in similar groups which improves various business decisions by providing a meta understanding.In this skill test, we tested our community on clustering techniques. A total of 1566 people registeredin this skill test.If you missedtaking the test, here is your opportunity for you to find out how many questions you could have answered correctly.Below is thedistribution of scores, this will help you evaluate your performance:You can access your performance here. More than 390 people participated in the skill testand the highest score was 33.Here are a few statistics about the distribution.Overall distributionMean Score: 15.11Median Score: 15Mode Score: 16An Introduction to Clustering and different methods of clusteringGetting your clustering right (Part I)Getting your clustering right (Part II)Q1. Movie Recommendation systems are an example of:Options:B. A. 2 OnlyC. 1 and 2D. 1 and 3E. 2 and 3F. 1, 2 and 3H. 1, 2, 3 and 4Solution: (E)Generally, movie recommendation systems cluster the users in a finite number of similar groups based on their previous activities and profile. Then, at a fundamental level, people in the same cluster are made similar recommendations.In some scenarios, this can also be approached as a classification problem for assigning the most appropriate movie class to the user of a specific group of users. Also, a movie recommendation system can be viewed as a reinforcement learning problem where it learns by its previous recommendations and improves the future recommendations.Q2. Sentiment Analysis is an example of:Options:A. 1 OnlyB. 1 and 2C. 1 and 3D. 1, 2 and 3E. 1, 2 and 4F. 1, 2, 3 and 4Solution: (E)Sentiment analysis at the fundamental level is the task of classifying the sentiments represented in an image, text or speech into a set of defined sentiment classes like happy, sad, excited, positive, negative, etc. It can also be viewed as a regression problem for assigning a sentiment score of say 1 to 10 for a corresponding image, text or speech.Another way of looking at sentiment analysis is to consider it using a reinforcement learning perspective where the algorithm constantly learns from the accuracy of past sentiment analysis performed to improve the future performance.Q3. Can decision trees be used for performing clustering?A. TrueB. FalseSolution: (A)Decision trees can also be used to for clusters in the data but clustering often generates natural clusters and is not dependent on any objective function.Q4. Which of the following is the most appropriate strategy for data cleaning before performing clustering analysis, given less than desirable number of data points:Options:A. 1 onlyB. 2 onlyC. 1 and 2D. None of the aboveSolution: (A)Removal of outliers is not recommended if the data points are few in number. In this scenario, capping and flouring of variables is the most appropriate strategy.Q5. What is the minimum no. of variables/ features required to perform clustering?A. 0B. 1C. 2D. 3Solution: (B)At least a single variable is required to perform clustering analysis. Clustering analysis with a single variable can be visualized with the help of a histogram.Q6. For two runs of K-Mean clustering is it expected to get same clustering results?A. YesB. NoSolution: (B)K-Means clustering algorithm instead converses on local minima which might also correspond to the global minima in some cases but not always. Therefore, its advised to run the K-Means algorithm multiple times before drawing inferences about the clusters.However, note that its possible to receive same clustering results from K-means by setting the same seed value for each run. But that is done by simply making the algorithm choose the set of same random no. for each run.Q7. Is it possible that Assignment of observations to clusters does not change between successive iterations in K-MeansA. YesB. NoC. Cant sayD. None of theseSolution: (A)When the K-Means algorithm has reached the local or global minima, it will not alter the assignment of data points to clusters for two successive iterations.Q8. Which of the following can act as possible termination conditions in K-Means?Options:A. 1, 3 and 4B. 1, 2 and 3C. 1, 2 and 4D. All of the aboveSolution: (D)All four conditions can be used as possible termination condition in K-Means clustering:Q9. Which of the following clustering algorithms suffers from the problem of convergence at local optima?Options:A. 1 onlyB. 2 and 3C. 2 and 4D. 1 and 3E. 1,2 and 4F. All of the aboveSolution: (D)Out of the options given, only K-Means clustering algorithm and EM clustering algorithm has the drawback of converging at local minima.Q10. Which of the following algorithm is most sensitive to outliers?A. K-means clustering algorithmB. K-medians clustering algorithmC. K-modes clustering algorithmD. K-medoids clustering algorithmSolution: (A)Out of all the options, K-Means clustering algorithm is most sensitive to outliers as it uses the mean of cluster data points to find the cluster center.Q11. After performing K-Means Clustering analysis on a dataset, you observed the following dendrogram. Which of the following conclusion can be drawn from the dendrogram?A. There were 28 data points in clustering analysisB. The best no. of clusters for the analyzed data points is 4C. The proximity function used is Average-link clusteringD. The above dendrogram interpretation is not possible for K-Means clustering analysisSolution: (D)A dendrogram is not possible for K-Means clustering analysis. However, one can create a cluster gram based on K-Means clustering analysis.Q12. How can Clustering (Unsupervised Learning) be used to improve the accuracy of Linear Regression model (Supervised Learning):Options:A. 1 onlyB. 1 and 2C. 1 and 4D. 3 onlyE. 2 and 4F. All of the aboveSolution: (F)Creating an input feature for cluster ids as ordinal variable or creating an input feature for cluster centroids as a continuous variable might not convey any relevant information to the regression model for multidimensional data. But for clustering in a single dimension, all of the given methods are expected to convey meaningful information to the regression model. For example, to cluster people in two groups based on their hair length, storing clustering ID as ordinal variable and cluster centroids as continuous variables will convey meaningful information.Q13. What could be the possible reason(s) for producing two different dendrograms using agglomerative clustering algorithm for the same dataset?A. Proximity function usedB. of data points usedC. of variables usedD. B and c onlyE. All of the aboveSolution: (E)Change in either of Proximity function, no. of data points or no. of variables will lead to different clustering results and hence different dendrograms.Q14. In the figure below, if you draw a horizontal line on y-axis for y=2. What will be the number of clusters formed?A. 1B. 2C. 3D. 4Solution: (B)Since the number of vertical lines intersecting the red horizontal line at y=2 in the dendrogram are 2, therefore, two clusters will be formed.Q15. What is the most appropriate no. of clusters for the data points represented by the following dendrogram:A. 2B. 4C. 6D. 8Solution: (B)The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram. The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.In the above example, the best choice of no. of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB.Q16. In which of the following cases will K-Means clustering fail to give good results?Options:A. 1 and 2B. 2 and 3C. 2 and 4D. 1, 2 and 4E. 1, 2, 3 and 4Solution: (D)K-Means clustering algorithm fails to give good results when the data contains outliers, the density spread of data points across the data space is different and the data points follow non-convex shapes.Q17. Which of the following metrics, do we have for finding dissimilarity between two clusters in hierarchical clustering?Options:A. 1 and 2B. 1 and 3C. 2 and 3D. 1, 2 and 3Solution: (D)All of the three methods i.e. single link, complete link and average link can be used for finding dissimilarity between two clusters in hierarchical clustering.Q18. Which of the following are true?Options:A. 1 onlyB. 2 onlyC. 1 and 2D. None of themSolution: (A)Clustering analysis is not negatively affected by heteroscedasticity but the results are negatively impacted by multicollinearity of features/ variables used in clustering as the correlated feature/ variable will carry extra weight on the distance calculation than desired.Q19. Given, six points with the following attributes:Which of the following clustering representations and dendrogram depicts the use of MIN or Single link proximity function in hierarchical clustering:A.B.C.D.Solution: (A)For the single link or MIN version of hierarchical clustering, the proximity of two clusters is defined to be the minimum of the distance between any two points in the different clusters. For instance, from the table, we see that the distance between points 3 and 6 is 0.11, and that is the height at which they are joined into one cluster in the dendrogram. As another example, the distance between clusters {3, 6} and {2, 5} is given by dist({3, 6}, {2, 5}) = min(dist(3, 2), dist(6, 2), dist(3, 5), dist(6, 5)) = min(0.1483, 0.2540, 0.2843, 0.3921) = 0.1483.Q20 Given, six points with the following attributes:Which of the following clustering representations and dendrogram depicts the use of MAX or Complete link proximity function in hierarchical clustering:A. B. C. D. Solution: (B)For the single link or MAX version of hierarchical clustering, the proximity of two clusters is defined to be the maximum of the distance between any two points in the different clusters. Similarly, here points 3 and 6 are merged first. However, {3, 6} is merged with {4}, instead of {2, 5}. This is because the dist({3, 6}, {4}) = max(dist(3, 4), dist(6, 4)) = max(0.1513, 0.2216) = 0.2216, which is smaller than dist({3, 6}, {2, 5}) = max(dist(3, 2), dist(6, 2), dist(3, 5), dist(6, 5)) = max(0.1483, 0.2540, 0.2843, 0.3921) = 0.3921 and dist({3, 6}, {1}) = max(dist(3, 1), dist(6, 1)) = max(0.2218, 0.2347) = 0.2347.Q21 Given, six points with the following attributes:Which of the following clustering representations and dendrogram depicts the use of Group average proximity function in hierarchical clustering:A. B. 
C.D.Solution: (C)For the group average version of hierarchical clustering, the proximity of two clusters is defined to be the average of the pairwise proximities between all pairs of points in the different clusters. This is an intermediate approach between MIN and MAX. This is expressed by the following equation:Here, the distance between some clusters. dist({3, 6, 4}, {1}) = (0.2218 + 0.3688 + 0.2347)/(3  1) = 0.2751. dist({2, 5}, {1}) = (0.2357 + 0.3421)/(2  1) = 0.2889. dist({3, 6, 4}, {2, 5}) = (0.1483 + 0.2843 + 0.2540 + 0.3921 + 0.2042 + 0.2932)/(61) = 0.2637. Because dist({3, 6, 4}, {2, 5}) is smaller than dist({3, 6, 4}, {1}) and dist({2, 5}, {1}), these two clusters are merged at the fourth stageQ22. Given, six points with the following attributes:Which of the following clustering representations and dendrogram depicts the use of Wards method proximity function in hierarchical clustering:A.B.C. D.Solution: (D)Ward method is a centroid method. Centroid method calculates the proximity between two clusters by calculating the distance between the centroids of clusters. For Wards method, the proximity between two clusters is defined as the increase in the squared error that results when two clusters are merged. The results of applying Wards method to the sample data set of six points. The resulting clustering is somewhat different from those produced by MIN, MAX, and group average.Q23. What should be the best choice of no. of clusters based on the following results:A. 1B. 2C. 3D. 4Solution: (C)The silhouette coefficient is a measure of how similar an object is to its own cluster compared to other clusters. Number of clusters for which silhouette coefficient is highest represents the best choice of the number of clusters.Q24. Which of the following is/are valid iterative strategy for treating missing values before clustering analysis?A. Imputation with meanB. Nearest Neighbor assignmentC. Imputation with Expectation Maximization algorithmD. All of the aboveSolution: (C)All of the mentioned techniques are valid for treating missing values before clustering analysis but only imputation with EM algorithm is iterative in its functioning.Q25. K-Mean algorithm has some limitations. One of the limitation it has is, it makes hard assignments(A point either completely belongs to a cluster or not belongs at all) of points to clusters.Note: Soft assignment can be consider as the probability of being assigned to each cluster: say K = 3 and for some point xn, p1 = 0.7, p2 = 0.2, p3 = 0.1)Which of the following algorithm(s) allows soft assignments?Options:A. 1 onlyB. 2 onlyC. 1 and 2D. None of theseSolution: (C)Both, Gaussian mixture models and Fuzzy K-means allows soft assignments.Q26. Assume, you want to cluster 7 observations into 3 clusters using K-Means clustering algorithm. After first iteration clusters, C1, C2, C3 has following observations:C1: {(2,2), (4,4), (6,6)}C2: {(0,4), (4,0)}C3: {(5,5), (9,9)}What will be the cluster centroids if you want to proceed for second iteration?A. C1: (4,4), C2: (2,2), C3: (7,7)B. C1: (6,6), C2: (4,4), C3: (9,9)C. C1: (2,2), C2: (0,0), C3: (5,5)D. None of theseSolution: (A)Finding centroid for data points in cluster C1 = ((2+4+6)/3, (2+4+6)/3) = (4, 4)Finding centroid for data points in cluster C2 = ((0+4)/2, (4+0)/2) = (2, 2)Finding centroid for data points in cluster C3 = ((5+9)/2, (5+9)/2) = (7, 7)Hence, C1: (4,4), C2: (2,2), C3: (7,7)Q27. Assume, you want to cluster 7 observations into 3 clusters using K-Means clustering algorithm. After first iteration clusters, C1, C2, C3 has following observations:C1: {(2,2), (4,4), (6,6)}C2: {(0,4), (4,0)}C3: {(5,5), (9,9)}What will be the Manhattan distance for observation (9, 9) from cluster centroid C1. In second iteration.A. 10B. 5*sqrt(2)C. 13*sqrt(2)D. None of theseSolution: (A)Manhattan distance between centroid C1 i.e. (4, 4) and (9, 9) = (9-4) + (9-4) = 10Q28. If two variables V1 and V2, are used for clustering. Which of the following are true for K means clustering with k =3? Options:A. 1 onlyB. 2 onlyC. 1 and 2D. None of the aboveSolution: (A)If the correlation between the variables V1 and V2 is 1, then all the data points will be in a straight line. Hence, all the three cluster centroids will form a straight line as well.Q29. Feature scaling is an important step before applying K-Mean algorithm. What is reason behind this?A. In distance calculation it will give the same weights for all featuresB. You always get the same clusters. If you use or dont use feature scalingC. In Manhattan distance it is an important step but in Euclidian it is notD. None of theseSolution; (A)Feature scaling ensures that all the features get same weight in the clustering analysis. Consider a scenario of clustering people based on their weights (in KG) with range 55-110 and height (in inches) with range 5.6 to 6.4. In this case, the clusters produced without scaling can be very misleading as the range of weight is much higher than that of height. Therefore, its necessary to bring them to same scale so that they have equal weightage on the clustering result.Q30. Which of the following method is used for finding optimal of cluster in K-Mean algorithm?A. Elbow methodB. Manhattan methodC. Ecludian mehthodD. All of the aboveE. None of theseSolution: (A)Out of the given options, only elbow method is used for finding the optimal number of clusters. The elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesnt give much better modeling of the data.Q31. What is true about K-Mean Clustering?Options:A. 1 and 3B. 1 and 2C. 2 and 3D. 1, 2 and 3Solution: (D)All three of the given statements are true. K-means is extremely sensitive to cluster center initialization. Also, bad initialization can lead to Poor convergence speed as well as bad overall clustering.Q32. Which of the following can be applied to get good results for K-means algorithm corresponding to global minima?Options:A. 2 and 3B. 1 and 3C. 1 and 2D. All of aboveSolution: (D)All of these are standard practices that are used in order to obtain good clustering results.Q33. What should be the best choice for number of clusters based on the following results:A. 5B. 6C. 14D. Greater than 14Solution: (B)Based on the above results, the best choice of number of clusters using elbow method is 6.Q34. What should be the best choice for number of clusters based on the following results: A. 2B. 4C. 6D. 8Solution: (C)Generally, a higher average silhouette coefficient indicates better clustering quality. In this plot, the optimal clustering number of grid cells in the study area should be 2, at which the value of the average silhouette coefficient is highest. However, the SSE of this clustering solution (k = 2) is too large. At k = 6, the SSE is much lower. In addition, the value of the average silhouette coefficient at k = 6 is also very high, which is just lower than k = 2. Thus, the best choice is k = 6.Q35. Which of the following sequences is correct for a K-Means algorithm using Forgy method of initialization? Options:A. 1, 2, 3, 5, 4B. 1, 3, 2, 4, 5C. 2, 1, 3, 4, 5D. None of theseSolution: (A)The methods used for initialization in K means are Forgy and Random Partition. The Forgy method randomly chooses k observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the clusters randomly assigned points.Q36. If you are using Multinomial mixture models with the expectation-maximization algorithm for clustering a set of data points into two clusters, which of the assumptions are important:A. All the data points follow two Gaussian distributionB. All the data points follow n Gaussian distribution (n >2)C. All the data points follow two multinomial distributionD. All the data points follow n multinomial distribution (n >2)Solution: (C)In EM algorithm for clustering its essential to choose the same no. of clusters to classify the data points into as the no. of different distributions they are expected to be generated from and also the distributions must be of the same type.Q37. Which of the following is/are not true about Centroid based K-Means clustering algorithm and Distribution based expectation-maximization clustering algorithm:Options:A. 1 onlyB. 5 onlyC. 1 and 3D. 6 and 7E. 4, 6 and 7F. None of the aboveSolution: (B)All of the above statements are true except the 5th as instead K-Means is a special case of EM algorithm in which only the centroids of the cluster distributions are calculated at each iteration.Q38. Which of the following is/are not true about DBSCAN clustering algorithm:Options:A. 1 onlyB. 2 onlyC. 4 onlyD. 2 and 3E. 1 and 5F. 1, 3 and 5Solution: (D)Q39. Which of the following are the high and low bounds for the existence of F-Score?A. [0,1]
B. (0,1)C. [-1,1]
D. None of the aboveSolution: (A)The lowest and highest possible values of F score are 0 and 1 with 1 representing that every data point is assigned to the correct cluster and 0 representing that the precession and/ or recall of the clustering analysis are both 0. In clustering analysis, high value of F score is desired.Q40. Following are the results observed for clustering 6000 data points into 3 clusters: A, B and C:What is the F1-Score with respect to cluster B?A. 3B. 4C. 5D. 6Solution: (D)Here,True Positive, TP = 1200True Negative, TN = 600 + 1600 = 2200False Positive, FP = 1000 + 200 = 1200False Negative, FN = 400 + 400 = 800Therefore,Precision = TP / (TP + FP) = 0.5Recall = TP / (TP + FN) = 0.6Hence,F1 = 2 * (Precision * Recall)/ (Precision + recall) = 0.54 ~ 0.5I hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge of clustering fundamentals and its various techniques.I tried to clear all your doubts through this article, but if we have missed out on something then let us know in comments below. Also, If you have any suggestions or improvements you think we should make in the next skilltest, you can let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2017/02/test-data-scientist-clustering/
40 must know Questions on Base SAS for Analysts (Skill test Solution),"Learn everything about Analytics|Introduction|Overall Results|Helpful Resources|Questions and Answers|End Notes|Learn, compete, hack and get hired!","Share this:|Like this:|Related Articles|40 Questions to test a Data Scientist on Clustering Techniques (Skill test Solution)|Machine Learning Scientist- Bangalore (3+ Years of Experience)|
Sunil Ray
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"SAS probably holds the highest market share in analytics solutions for enterprises. With its good data handling and graphical capabilities, SAS is an important tool for a data scientist / analyst. We recently conducted a skill test on SAS.The topic covered in thisskill test was Base Programming for SAS. The skill test tested both theoretical & practical knowledge of Base Programming in SAS. A total of 977 people participated in this skill test.If you are one of those who missed this great opportunity to test yourself against other SAS practitioners. Go through the below questions, and find out how many can you answer correctly.
Below is thedistribution of scores, this will help you evaluate your performance:You can access your performance here. More than 230 people participated in the skill testand the highest score was 35.Here are a few statistics about the distribution.Overall distributionMean Score: 17.73Median Score: 19Mode Score: 22SAS Learning path and resources  Business Analyst in SASComprehensive Introduction to merging in SASComprehensive guide for Data Exploration in SAS (using Data step and Proc SQL)Q1)Which one of the following is the value of the variable c in the output data set? A) 6
B) 9
C) 8
D) None of the aboveSolution: (C)** is an exponential operator.so c= a **b = 2**3 = 8Q2) Which one of the following statement cant be part of PROC FREQ?A) OUTPUTB) WEIGHTC) SETD) TablesE) None of the aboveSolution: (C)Look at the syntax of PROC FREQ, there is not SET statement required.Q3) We have submitted the following PROC SORT step, which generates an output data set.In which library is the output data set stored?A) WorkB) AVC) SASHELPD) SASUSERSolution: (A)If we are not providing library name explicitly then it will automatically refer to temporary library WORK.Question Context Q4  Q7
Below are the two tables:Q4) How many variables would be in table AV after executing the below SAS program?A) 3B) 4C) 5Solution: (B)If we are using any variable name within data step program it will automatically get created in output data set. Here, Three unique variables in both the tables are name, age, salary and one more variable created within dataset totsal.Q5) After executing below SAS program, how many observations would be AV dataset?A) 4B) 2C) 1D) 6Solution: (D)Above you look at input data sets, there is a one-to-many relationship between Employee and Salary.To know more about merging in SAS, click here.Q6) After executing below SAS program, how many observations would be in AV dataset?A) 4B) 2C) 1D) 6Solution: (B)Here, we are talking about in variables and look at the below table to understand the value of in variables:In this program, we are looking for observations where ins = 0 which means that name values not available in table Salary. In above table, you can see that only two records satisfy that criteria.Q7) Which one of the following command will help us to rename the column Salary to Compensation of table Salary?A.B.C.D. None of the aboveSolution: (B)Syntax to rename variable(s) in SAS is:RENAME = (Old_Var1 = NewVar1 Old_Var2=New_Var2 Old_Var3=New_Var3 )Q8) Which of the following statements is not correct about the program shown below?Solution: (B)In above program, we are writing to output dataset before END statement which means it will not write last value 2005 to output dataset so last value would be 2004. If we remove OUTPUT statement, last value would be 2005.Q9) How can you limit the variables written to output dataset in DATA STEP?Solution: (E)Both DROP and KEEP can be used to limit the variables in the dataset.Q10) Which of the following statements are used to read delimited raw data file and create an SAS data set?Solution: (D)SET can not be used to read raw data files. SET is used to read data from one or more SAS dataset.Question context Q11  Q12Below is the data from a csv file Emp.csvEmployee id,Gender,Name,DOB,Location,Salary,ManagerEmp IDThis dataset is about company employee101,M,John,12/1/1995,Delhi,350000,101102,F,Sangeeta,7/4/1980,Delhi,450000,103103,F,Mary,3/5/1973,Mumbai,500000,101104,M,Richard,6/25/1975,Mumbai,750000,101105,M,Fredrick,8/20/1990,Delhi,320000,101And, following code is used to read the filenamed EMP.Q11)What will be the output if we run the below SAS statements to read emp.csv file?A.B.C.D. None of the aboveSolution: (C)INFILE statement start reading a file from first line of CSV and it can be header row also so we need to mention start row explicitly.Q12)Which option will be added to infile statement to read a dataset from the record with employee name John?A. rows=3B. option= 3C. firstobs=2D. start=3E. Start=2F. firstobs=3Solution: (F)FIRSTOBS option can be used to explicity mention the start row to read. In above table, first row is representing header, second row about table and data set is starting with third row.Q13) Below SAS statements are used to read file Emp.csv from third record of csv file.Code:Output:Now, which statement we should add to the above code to read date column DOB correctly?A. Date 360B. In-format and formatC. Both A and BD. None of the aboveSolution: (B)To read date column, we need to explicitly mention the format type of date and that can be done using INFORMAT and FORMAT statements.Question Context 14In the snapshot below, you can see that variable Avg is in character format.Q14) Which of the following statement will help to convert Avg to numeric format?A. Input(Avg, 5.2)B. PUT(Avg,5.2)C. INT(Avg,5.2)D. Both A and CSolution: (A)INPUT() and PUT() are conversion function in SAS. INPUT() is used to convert text to a number whereas PUT() to convert the number to text.Question Context 15  17Q15) The following SAS program is run on the above table EmpHow many records will it print?A. 1B. 2C. 3D. None of the aboveSolution: (D)Like operator acts as case sensitive and in above table there is no-one whose second character of the name is capital R.Q16) Which of the following statement willcalculate the age of each employee as on 05-Feb-2017?A.B.C.D. None of the aboveSolution: (A)In SAS, date string is always followed by d to act as date.Q17) If you submit the following program on above data set, which variables appear in table Emp?A. Employee_Id, Gender, Name, Location, Salary, DOB, Manager_Emp_IDB. Employee_Id, Gender, Name, Location, Salary, DOB, Manager_Emp_ID, AgeC. Employee_IDD. Employee_ID, AgeE. Employee_ID, Age, DOBSolution: (D)We have only three variables from input dataset Manager_EMP_ID,Employee_ID, Salary and two new variables introduced DOB and Age. In Data statement, we have dropped two (Manager_EMP_ID and Salary) out of these five variables. Now variables in output dataset Employee_ID, Age, and DOB.Question context 18Below is the csv file class.csv for marks of students in different subjects:Name,Gender,Location,English,Maths,Hindi,SanskritMohan,M,Banglore,50,60,70,80Ramesh,M,Banglore,45,50,65,89John,M,Washington,68,,,88Kathy,F,Washington,89,55,85,83George,M,Washington,43,45,95,84Lisa,F,Washington,76,85,,86Venkat,M,Banglore,68,90,78,92Srimohan,M,Banglore,59,56,80Preet,F,Banglore,81,95,85,96Lindsy,F,Washington,66,75,78,82Below code is used to read the file class.csv into a SAS dataset table named class.Above code gives the below output:Q18) In the above output, you can see following issues:Which of the following command can be used with infile statement to remove these errors?A. MISSINGB. MISSOVERC. DSDD. Both A and CE. Both B and CSolution: (E)Whenever a read a delimited file using infile statement and if the file has two or more delimiter together (n value between them) or last column data is missing then it takes the next possible value as an input for that column. And, the next possible value can be other column data of same row or next line also.Now, to avoid these reading issues, we use DSD to prevent reading from next column of the same row and MISSOVER for next line or observation.Question context 19Below is the table ClassQ19) Which of the following command will find the number of missing marks in all variables of table Class.A.B.C.D. Both B and CSolution: (B)Options with PROC MEANS:Q20) Which of the following command will help to impute the missing value of column Hindi with average marks of Hindi?A)B)C) Both A and BD) None of the aboveSolution: (A)In the first option, we are creating a variable avg_score in the table temp and then using this table data in data step to input missing values of HINDI whereas in option second, we are using table class as an input data set for data step.Question Context 21  24Q21) Which of the following statements can be used to append the Table-1 and Table-2 having a unique value of Product_ID?A.B.C.D. none of the aboveSolution: (B)To remove duplicate records based on a variable or multiple variables, we use NODUPKEY with PROC SORT or FIRST./ LAST. option to remove duplicate records. For more detail on removing duplicate records, you can refer this link.Q22) With cash crunch (due to demonetization) the company decided to advance the proposed booking date by 2 months (keeping the day intact). Which of the below SAS formula can be used to advance the date?A.B.C.D.Solution: (B)Look, at the syntax of INTNX() function:INTNX ( interval, from, n < , alignment > ) ;The arguments to the INTNX function are as follows:interval:is a character constant or variable that contains an interval namefrom: is a SAS date value (for date intervals) or datetime value (for datetime intervals)n:is the number of intervals to increment from the interval that contains the from valuealignment: controls the alignment of SAS dates, within the interval, used to identify output observations. Allowed values are BEGINNING, MIDDLE, END, and SAMEDAY/S.In the second option, you can see that we have used the similar syntax to advance the date value by 2 months.Q23) If the following code will run, what will be the output?A.B.C.Solution: (C)IN variable does not appear in output dataset. Here, Proposed_Booking_Date and Date are IN variables and we have dropped the variable Location in data step.Q24) In Table-2, Location name Delhi has been wrongly put, need to replace this with Delhi_NCR. Which of the following code will complete this task?A.B.C.D. Both B and CE. Both A and BF. None of the aboveSolution: (D)The length of field Location in table2 is 8 so first we need to change the format of Location. Here in both options B and C, we have changed the length of field Location.Q25) [ True | False] Value of First. BY-variable and Last. By-variable can be same.A. TrueB. FalseSolution: (A)Yes, it is possible. In case of one unique value for BY variable then this record is the first and last record as well.Q26) Which is pointer control used to read multiple records sequentially?A. @nB. +NC. /D. All of the aboveSolution: (C)You can use one or more forward slash (/) line pointer controls in your INPUT statements to tell SAS to advance to a new record before reading the next data value.Question Context 27  30Table 5NOTE: The dataset has been loaded in SAS and table name is table5.Q27) Categorical column may contain more than two distinct values. For example, Married has two values, Yes and No. How will you find all the distinct values present in the column Education?A.B.C. Both A and BD. None of the aboveSolution: (A)Proc Means is used to look at the frequency distribution of categories of a categorical variable whereas PROC Means used to explore continuous variables.Q28) How will you create an extra column Salutation?A.B.C.DESolution: (A)Below is the syntax of function SCAN:SCAN(string, count_words)String: A constant string or variable have a string valueCount:is a nonzero numeric constant, variable, or expression that has an integer value that specifies the number of the word in the character string that you want SCAN to selectIn above question, we need to extract the first word of string so value of count would be 1 and string variable is name.Q29) Which of the following command will help you to create the below table AV (Exactly Similar) based on Table5?AVA.B.C. Both A and BD. None of the aboveSolution: (D)First of all, here we are creating dummy variables for variable Loan_Status (also known as One Hot Encoding). Both Option A and B will create these dummy variables but after execution of both program you will not be able to create exactly similar dataset like AV because it will have more number of variables and the values of dummy variables for Loan_Status_H and Loan_Status_N is swapped in output table AV.Q30) Which of the following SAS program will help you understand the relationship between two variables Education and Loan_Status?A.B.C.D.Solution: (A)Above, we are trying to create a two-way table based on two categorical variables Education and Loan_Status. And to create two-way table, we need to place * in between them. If we will separate the variable name by space then this will create two individual frequency distributions for both the variables.Q31) [True | Flase] The two programs below will return same output.Program1Program2A.  TrueB.  FalseSolution: (B)In thefirst program, we have LoanAmount in input data set so there would be values 0.4*LoanAmount in Charge column whereas, in the second program, we have dropped the variable LoanAmount so the value of column Charge would be missing because we do not have variable LoanAmount.Q32) Which of the following statement can be used to accumulate the value of the variable in a Data Step?A. SETB. RETAINC. UPDATED. SUMSolution: (B)The RETAIN statement simply copies retaining values by telling the SAS not to resetthe variables at the beginning of each iteration of the DATA step. If you would not use retain statement then SAS would reset the variable at the beginning of each iterationQ33) Given the following SAS error logWhich of the following step, you will take to correct it?A. Replace the WHERE statement with an IF statementB. Change the ** in the BMI formula to a single *C. Change bmi to BMI in the WHERE statementSolution: (A)We can not apply WHERE on derived or calculated variable(s) so we should use IF for subsetting.Q34) Which of the following statement can be used to transpose table Base to table Transposed?A.B.C.    Both A and BD.   None of the aboveSolution: (C)Both program can beused to transpose the data set, One is array approach whereas in second method, we are using PROC Transpose.Q35) [True | False] Where and IF always returns the same result.A) TrueB) FalseSolution: (B)One of the scenarios, we have discussed in question 35.Q36) Which of the following PROC can be used to create Bubble, Scatter and Histogram?A. PROC SGPLOTB. PROC UNIVARIATEC. PROC PLOTD. None of the aboveSolution: (A)PROC SGPLOT can be used to create all above-mentioned charts.Question Context 37  38Table6Note: Above table Table6 is stored in WORK libraryQ37) Which of the following command can be used to plot below chart?A.B.C.D. None of the aboveSolution: (A)Above, we are creating three series of line in a single chart and we dont have any Line and BY statements in PROC SGPLOT.Q38) Which of the following command can be used to plot below chart (Below Product1 is represented on x-axis, Product2 on y-axis and Product3 as the size of bubble)?A.B.C.D.Solution: (B)In bubble chart, we have three variables to visualize. One on x-axis, second one on y-axis and last one as size of bubble. We can create Bubble chart in SAS using PROC SGPLOT with Bubble statement.Question Context 39  40Below is the table of product inventory (SAS data set name is Table7)Q39) Which of the following SAS program will remove the duplicate observation(s) of ID and Area_Type. And, remove observation having the lower magnitude of variable Volume?A.B.C.D. Both B and CSolution: (A)The basic problem with Option B and C is, Descending option is appearing after the variable name which is not the right syntax. In option A, we are first sorting the data set based on ID, Area_Type and Volume (Descending) then again writing a PROC SORT to remove duplicate records based on ID and Area_Type.Q40) Which of the following program will help to bin the variable volume (Adding one more variable to Table7, Volume_Bucket)?A.B.C. Both A and BD. None of the aboveSolution: (B)Select statement works with exact value, it does not compare like greater than or less than so here IF statement will do the task.I hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge of Base Programming in SASI tried to clear all your doubts through this article, but if we have missed out on something then let us know in comments below. Also, If you have any suggestions or improvements you think we should make in the next skill test, you can let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2017/02/40-must-know-questions-on-base-sas-for-analysts-data-scientists-out-there-skilltest-solution/
Machine Learning Scientist- Bangalore (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|40 must know Questions on Base SAS for Analysts (Skill test Solution)|Data Engineer- Bangalore (5+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : DESCRIPTIONHundreds of millions of customers, millions of products, billions of purchase transactions, petabytes of complete clickstream history Stop playing in sand boxes! Join us to enjoy the worlds richest collection of e-commerce and in-device data to segment and target customers on site and through email, social, mobile and display  not to mention the gorgeous correlations (median AUC of ~0.85 over a thousand models). Our Consumer Analytics team is looking for Machine Learning Scientists. You will work with distributed machine learning and statistical algorithms across multiple platforms (AWS, Hadoop and the data warehouse) to harness enormous volumes of online data at scale to match customers and products/offers based on probabilistic reasoning. The Machine Learning Scientist will be a technical player in a team working to develop ultra-scale platforms for machine learning, and help develop new, revolutionary approaches that optimize Amazons systems using cutting edge quantitative techniques. You need to be fluid in:  Data warehousing and Hadoop (SQL, Hive, Pig).  Feature extraction, feature engineering and feature selection.  Machine learning, statistical algorithms and recommenders.  Model evaluation, validation and deployment.  Experimental design and testing. We write custom data extractors, target builders, score optimizers, model managers for Hadoop. This is one of the most exciting machine learning job opportunities on the internet today. If you have a deep technical knowhow in machine learning, know how to deliver highly innovative solutions to challenging problems that directly impact the companys bottom-line, we want to talk to you.  BASIC QUALIFICATIONS A PhD in machine learning, information science, engineering, statistics or a highly quantitative field. Masters with equivalent experience will be considered.  Proven ability to relate to and solve business problems through machine learning and statistics.  2+ years of hands-on experience developing and implementing machine learning algorithms and/or statistical models.  Programming, prototyping and scripting skills (Oracle, SQL, Hive, Pig, SAS, R, Weka, Python).  Writing, communication and presentation skills.PREFERRED QUALIFICATIONS A strong track record of innovating through machine learning and statistical algorithms and their applications.  Strong demonstrated skills implementing and deploying large scale machine learning applications and tools.  Strong skills and experience with programming in SQL, Hive, Pig, R, SAS macros and familiarity/experience with AWS.  3 years industry experience.
College Preference : no-bar
Min Qualification : pg
Skills : aws, hive, machine learning, pig, python, r, sas, sql, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/machine-learning-scientist-bangalore-3-years-of-experience/
Data Engineer- Bangalore (5+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning Scientist- Bangalore (3+ Years of Experience)|Sr. Technical Writer, US (5+ years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : The BI Engineer will play a key role in contributing to the success of each focus area, by partnering with respective business owners and leveraging data to identify areas of improvement & optimization. He / She will build deliverable like business process automation, payment behavior analysis, campaign analysis, fingertip metrics, failure prediction etc. that provide edge to business decision making AND can scale with growth. The role sits in the sweet spot between technology and business worlds AND provides opportunity for growth, high business impact and working with seasoned business leaders.An ideal candidate will be someone with sound technical background in data domain  storage / processing / analytics, has solid business acumen and a strong automation / solution oriented thought process. Will be a self-starter who can start with a business problem and work backwards to conceive & devise best possible solution. Is a great communicator and at ease on partnering with business owners and other internal / external teams. Can explore newer technology options, if need be, and has a high sense of ownership over every deliverable by the team. Is constantly obsessed with customer delight & business impact / end result and gets it done in business time.BASIC QUALIFICATIONS Bachelors degree in Computer Science or a related technical field from an accredited institution, and 5+ years of relevant employment experience. 5+ years of developing end-to-end Business Intelligence solutions: data modeling, ETL and reporting. Proven analytical and quantitative ability and a passion for enabling customers to use data and metrics to back up assumptions, develop business cases, and complete root cause analyses. Strong verbal and written communication skills, including an ability to effectively lead and influence interactions with both business and technical teams. Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.) and using databases in a business environment with complex datasets.PREFERRED QUALIFICATIONS Ability to take loosely defined business questions and translate them into clearly defined technical/data specifications for implementation. Ability to deal with ambiguity and work with rapidly changing business data
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, data modeling, etl, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/data-engineer-bangalore-5-years-of-experience/
"Sr. Technical Writer, US (5+ years of experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Engineer- Bangalore (5+ Years of Experience)|Sr Software/lead Engineers  Python/SDN Development- Bangalore (3+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : We are looking for a writer to work on documentation for AWS analytics services. The ideal candidate has both a technical background that enables easy interaction with software developers and a demonstrated ability to delivery high-quality technical documentation that helps customers.BASIC QUALIFICATIONS A minimum of 5 years in a technical writing role Proven experience designing and delivering customer-oriented documentation Strong written and verbal communication skills Experience working directly with development teams Experience with an XML-based authoring systemPREFERRED QUALIFICATIONS Experience with cloud services and related technologies a plus Experience with Big Data technologies (for example, Hadoop) a plus Experience working in an agile environment a plus Strong attention to detail Strong interpersonal skills Ability to thrive in a fast-paced, ever-changing environment Experience using and following the Microsoft Manual of Style for Technical Publications (MSTP)
College Preference : no-bar
Min Qualification : ug
Skills : aws, bigdata, hadoop
Location : United States
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-technical-writer-us-5-years-of-experience/
Sr Software/lead Engineers  Python/SDN Development- Bangalore (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Technical Writer, US (5+ years of experience)|SAS Developer- Bangalore (4+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : DescriptionRoles & Responsibilities:Technical Expertise  Mandatory: (should possess by candidate)Strong Python experience in developing complex modules (scripting work is not acceptable).Good understanding and working experience in JSON, JSON Schema, REST APIUnderstanding of networking basicsGood to have skills:Docker containersKafka messaging or RabbitMQKnowledge of management protocols such as SNMP, NETCONFValue Add:Working experience in EMS/NMS and SDN is a value addResponsibilities:Individual contributorWorking on development of Resource Adapters and the corresponding tools that are part of SDN platform.Participate in discussions with different groups within the project group based in various locations to create/design technical solutions
College Preference : no-bar
Min Qualification : ug
Skills : python
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-softwarelead-engineers-pythonsdn-development-bangalore-3-years-of-experience/
SAS Developer- Bangalore (4+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr Software/lead Engineers  Python/SDN Development- Bangalore (3+ Years of Experience)|Big Data Engineer- Bangalore (5+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : DescriptionRoles & Responsibilities:Design, development, maintenance and enhancement of automated WPS (SAS) applications to create web based reports & analysis.Requirements gathering from internal/external customersRaw data analysis and exploration of new data sourcesCreate the specification and job documentationCode the application using Base SAS or WPS/Macros/Graph/ODSControl-M job schedule specification for automated execution.Troubleshooting of existing jobs running on Linux.Amend WPS (SAS) code, common shell scripts, WEB pages, web tools, Control M-job schedules, etc.Regular maintenance of daily and historical databasesCreation and maintenance of web delivery tools using Apache, Python, Javascript and other open source toolsAd-hoc performance analysis on request from internal and external customersActive involvement in the specification of measurement strategiesValidation of the quality of data sources and data integrity.Know How / SkillsProficient experience in WPS (V3 or later) and/or SAS Base programming (versions 9.1.3 or later) under UNIX/Linux/WindowsWPS/SAS Macro coding and experience with WPS/SAS Graph, ODS, Proc SQL and Hash tablesUNIX/Linux Shell scripting, UNIX/Linux user skills, SSH, FTPBasic PC skills using Windows, MS Office (especially Excel)Familiarity with Open Source tools (e. g. WINSCP, Eclipse, Mercurial, SSH)Java, Python, ODBC, HTML, JavaScriptKnowledge of relational databases such as Oracle, MySQL Relevant Job Experience4+ years experience as Data analyst or Performance analystAt least 2 years of relevant job experience as a SAS or WPS programmer are mandatoryNot essential, but a plus if presentControl-M and WPS/SAS STAT, Share skillsExperience with Apache, JBOSS, other web/Java serversExperience in web-based GUI programmingExperience in performance data analysisNot essential, but a plus if presentControl-M and WPS/SAS STAT, Share skillsExperience with Apache, JBOSS, other web/Java serversExperience in web-based GUI programmingExperience in performance data analysis
College Preference : no-bar
Min Qualification : ug
Skills : excel, HTML5, JavaScript, linux, oracle, python, sas, Unix shell scripting
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sas-developer-bangalore-4-years-of-experience/
Big Data Engineer- Bangalore (5+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|SAS Developer- Bangalore (4+ Years of Experience)|Business Analyst- Pune (0-5 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : 
Task Info : DescriptionSummary of the Role:Collaborate with teams in BLR, NCE and SYD working on product for Airline Operations with cutting edge tech stack.Contribute and support in the development and maintenance of back-end functionalities of server running on AWS.Take-up all project management tasks related to project  update Github etc; scoping, sizing, clarification/specification, development/refactor.Able to contribute technically to the project.Understand development/delivery issues and ability to spot them as they are taking shape.Identify any risk/issue in the delivery across all development streams, infrastructure and resources.Main Responsibilities:Participate in design and development phase using agile development practices.Estimation of issues/tasks for project planning/scoping.Write unit, integration tests and Continuous Integration with CircleCi.Work with team members, Dev Leads, Architects/Experts, Managers, counterparts in BLR/Nice/SYD.Skills & Knowledge  Technical / Functional and Managerial:5-8 years strong experience in Java field. Experience with Scala, AWS a plus.Delivered at least two end-to-end enterprise/SAAS Big data based solutions.Strong working knowledge in the server side programming using JAVA, J2EE and Scala.Experience in concepts of OOPS and Functional Programming, frameworks like JAX-RS, JAX-WS, Play, JMS, Joda, etc.Knowledge of Couchbase is a plus.Should have cracked problems on scale, performance.Experience in working on SAAS multi-tenant products is a major plus.Competency in professional engineering best practices and processes relating to software development lifecycle (scrum, architecture/design/coding standards, design/code reviews, unit testing, automated build processes (gradle), testing, operations, etc).Prefer with knowledge of Airline Operations, Business Intelligence, or involved with certain component development of such platform is a plus.Strong knowledge in JavaScala Play FrameworkCouchbase or any other nosql databaseGit or any other SCMWorking with AWS environment as plusExcellent collaboration as well as written and oral communication skills.Cloud/SaaS (particularly Amazon Web Services) experience desirable.Worked on Core Java and J2EE.Knowledge of Web Services and REST API
College Preference : no-bar
Min Qualification : ug
Skills : aws, J2EE, java, nosql, saas, Scala
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/big-data-engineer-bangalore-5-years-of-experience/
Business Analyst- Pune (0-5 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Engineer- Bangalore (5+ Years of Experience)|Data Scientist Analytics-Text Analytics Expert- Bangalore (4-8 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  5 years
Requirements : 
Task Info : Experience:  0-5 yrs.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, business analysis
Location : Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/business-analyst-pune-0-5-years-of-experience/
Data Scientist Analytics-Text Analytics Expert- Bangalore (4-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Pune (0-5 Years of Experience)|Analytics Analyst- Noida (1-3 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : 
Task Info : Overview: He/she will be part of companys Analytics team that identifies and develops advanced analytics models and solutions for Accenture operations clients to improve various business outcome indicators.  Develop appropriate text analytics techniques to develop appropriate analytic solutions for unstructured data from different sources  Research Software Engineers are expected to build and develop systems which demonstrate and make accessible the underlying research technology to solve the real time NLP problems.  Responsibilities of a Research Software Engineer include participating in basic and applied research on a wide array of text analytics and machine learning projects.  Identify new research opportunities, and develop innovative technology solutions to the research problems.  Leverage creativity and adaptability to better anticipate competition moves, quickly respond to market changes and support ever-evolving customer needs  Develop appropriate methodology for analyzing and interpreting results from product features A/B testingBasic qualifications-Experience: 4  8 years-Job location: Bangalore Education: Candidates should hold an M.Tech or MS or PhD in Computer Science, Mathematics, Applied Statistics, or related disciplines with an excellent academic record Must have 3+ years experience in Text Analytics using NLP. Must have 3+ years experience in Unstructured Data handling preferably text and AI algorithm development Must have hands on experience in handling text data and using NLP concepts for handling the text and support knowledge component of input text Extensive experience solving business problems using text analytics approaches Ability to code in python/Java language Experience in Deploying state-of-the-art, data-driven learning algorithms to solve business problems using the latest technologies in neural networks, NLP, machine learning, statistical modeling, pattern recognition, and artificial intelligence Extensive hands on experience in handling text data and using NLP concepts for handling the text and support knowledge component of input text Extensive experience in machine learning, text classification, information extraction and Noun, name and other Entity Resolution and other concepts of natural language processing.
College Preference : no-bar
Min Qualification : pg
Skills : java, machine learning, nlp, python, statistical modeling, text analytics
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/data-scientist-analytics-text-analytics-expert-bangalore-4-8-years-of-experience/
Analytics Analyst- Noida (1-3 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist Analytics-Text Analytics Expert- Bangalore (4-8 Years of Experience)|Campaign Analytics- Noida (6-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : Objectives of Role  This position will be responsible for procurement financial analysis / market intelligence / business intelligence analytics Main Accountabilities  Analyze Customer spend volume to be sourced  Utilize and analyze all recent and accurate information (including Customer-internal, Supplier-internal, multi-Customer-based grouped data, and other industry and/or professional association and/or academic information sources) to support creation of specific Sourcing Project opportunity assessment/business case as advised by the Customer and/or Supplier Category Manager  Utilize and analyze all recent and accurate information (including Customer-internal, Supplier-internal, multi-Customer-based grouped data, and other industry and/or professional association and/or academic information sources) to identify ad-hoc potential Sourcing Project opportunities  Create Category Sourcing Initiative Reports and submit to Customer and/or Supplier Category Manager Minimum Requirements  Graduation / Post-Graduation with 1-3 years of prior financial analysis / market intelligence / business intelligence analytics experience  Good quantitative and analytical skills  Strong understanding of latest Microsoft Office applications i.e. Word, PowerPoint, Excel  Flexibility to work with different countries (time-zones), groups, and business environment Preferred Background  Good quantitative and analytical skills  Exposure to procurement analytics  Basic understanding of accounting  Working knowledge of Ariba / Millennium / Oracle ERP / SC systems would be an advantage Basic qualificationsGraduation / Post-Graduation
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, excel, financial modeling, oracle, Power point
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/analytics-analyst-noida-1-3-years-of-experience/
Campaign Analytics- Noida (6-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Analyst- Noida (1-3 Years of Experience)|Marketing Analytics-Mumbai (4+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  7 years
Requirements : 
Task Info : Skills Specifications Ability to generate reports and insights and review the work done by team members Responsibillities Tech  SQL, Excel Communication skills Report development skills Other Pointers Knowledge of email and web campaign design, development, execution and analytics is key  Domain  Web/email analytics experience preferred  Tech  SQL, Excel (Must have)  Good communication skills (verbal & written)  Presentation development skills  Exp in campaign management preferred  Analysis of campaign reports and launch of campaign extensive experience in building statistical models a must  Designing of KPI reports & evaluations forms & decks  Ability to create reports efficiently using the input from the models Experience Level 6-7 yearsBasic qualificationGraduate
College Preference : no-bar
Min Qualification : ug
Skills : excel, sql, web analytics
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/campaign-analytics-noida-6-7-years-of-experience/
Marketing Analytics-Mumbai (4+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Campaign Analytics- Noida (6-7 Years of Experience)|Analytics Manager- New York, San Francisco, Chicag (8+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : Job DescriptionKey Competencies or Skills Required to Undertake the Role The ability to work with large and complex data sets with a clear understanding of the full data ecosystem from source to the presentation layer:  Strong analytical skills  Expertise in Python  Expertise in database skills  SQL, SSIS  Strong data analysis skills with the ability to use MS Excel, Access and other desktop products to prototype and present results  Experience developing or assisting the creation of a data warehouse or data mart, including data modelling and architecture  Understanding of Dashboards that are common across most industries, and the range of visualization methods used (e.g. bar chart, stream graph, heat map, tree map etc.) Additional Competencies or Skills that would help Undertake the Role  Proficiency in database skills -Oracle, Teradata,  Experience with statistical analysis and visualization software such as SAS, JMP, R, Tibco Spotfire, Qlikview, Tableau, etc.  Experience with the open-source tools HTML, JS/JQuery and ""D3.js""  Hands-on experience with Data Query Business Intelligence Software (Business Objects, Cognos)  Familiarity with enterprise data systems and databases with experience in analyzing large, complex volume data sets (Big Data)  Knowledge of Flash, Visual Basic and HTML/JavaScript based visualization services and software libraries Key Responsibilities A full listing of Data Visualization responsibilities would include the following:  Providing value-add analysis to Business through use of visualization software to guide analysis, drawing implications from analysis, and synthesizing into clear communicationsBasic qualifications Any graduate or Post Graduate in Engineering
College Preference : no-bar
Min Qualification : ug
Skills : analytics, data architect, data modeling, excel, HTML5, JavaScript, oracle, python, qlikview, r, sql, sql server integration service (SSIS), tableau, teradata
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/marketing-analytics-mumbai-4-years-of-experience/
"Analytics Manager- New York, San Francisco, Chicag (8+ Years of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Marketing Analytics-Mumbai (4+ Years of Experience)|Hadoop Developer- Gurgaon (2-4 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 8  years
Requirements : 
Task Info : Position Summary:We are looking for Manager for the Analytics team with experience of managing marketing analytics projects making significant contributions to design of analytical approach and handling the work plan. The candidate is expected to possess good Analytical, decision making & problem solving skills.Key Responsibilities:Project ManagementPeople ManagementQualifications and Skills:
College Preference : tier1-any
Min Qualification : pg
Skills : analytics, business analytics
Location : Chicago, New York, San Francisco
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/analytics-manager-new-york-san-francisco-chicag-8-years-of-experience/
Hadoop Developer- Gurgaon (2-4 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Manager- New York, San Francisco, Chicag (8+ Years of Experience)|Sr. Programmer/ Team Leader- Tableau- Gurgaon (2-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : Position Summary:We are looking for candidates with hands on experience in Big Data technologies to be based out of our Gurgaon office.Key Responsibilities:Qualifications and Skills:
College Preference : no-bar
Min Qualification : ug
Skills : flume, hadoop, hive, mapreduce, pig, r, spark, sqoop
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/hadoop-developer-gurgaon-2-4-years-of-experience/
Sr. Programmer/ Team Leader- Tableau- Gurgaon (2-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Hadoop Developer- Gurgaon (2-4 Years of Experience)|Technical Architect- Gurgaon (10+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  7 years
Requirements : 
Task Info : Position Summary:Skilled professional with expertise in Tableau and ability to lead projects on Business Intelligence and Data Warehousing both as an individual contributor as well as a team leader with some client interface.Key Responsibilities:Qualifications and Skills:
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, oracle, sql server, tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-programmer-team-leader-tableau-gurgaon-2-7-years-of-experience/
Technical Architect- Gurgaon (10+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Programmer/ Team Leader- Tableau- Gurgaon (2-7 Years of Experience)|Sr. programmer- Qlikview- Gurgaon (2-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : Position Summary:We are looking for a skilled professional with ability to lead projects on software applications, data warehousing and also work as an individual contributor in designing technical architecture of different solutions across multiple domains and internal productsKey Responsibilities:Qualifications and Skills:
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, java, machine learning, nlp, oracle, r, sql server, text mining
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/technical-architect-gurgaon-10-years-of-experience/
Sr. programmer- Qlikview- Gurgaon (2-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Technical Architect- Gurgaon (10+ Years of Experience)|Sr. Programmer- MSBI- Gurgaon (2-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Position Summary:Skilled professional with expertise in Qlikview and ability to lead projects on Business Intelligence and Data Warehousing both as an individual contributor as well as a team leader with some client interface.Key Responsibilities:Qualifications and Skills:
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, oracle, qlikview, RDBMS, sql server
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-programmer-qlikview-gurgaon-2-6-years-of-experience/
Sr. Programmer- MSBI- Gurgaon (2-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. programmer- Qlikview- Gurgaon (2-6 Years of Experience)|Consultant- CRM Analytics- San Francisco, US (4-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Position Summary:2-6 years of experience in BI and delivering end to end Web  based BI applications with good understanding of SQL Server any other BI Dashboarding tool. Understanding of OLTP vs OLAP data models and database optimization techniques.Key Responsibilities:Skilled professional with ability to execute projects on Web based applications in Microsoft BI and SQL Server with some client interfaceProviding services directly to external clients as well as internal clients in integrated projectsDevelop applications on various BI and Dashboarding requirements rich in Data Visualization.Develop, Test and Implement solutions comprising of ETL (SSIS / Custom Code in Dot Net), Data Warehouse Design (SQL Server), Reporting and Dashboarding (SSRS) and / or web-based interface (Dot Net).Execute end to end Projects  from Requirements gathering to final deliveryQualifications and Skills:Strong skills in SQL and Microsoft BI including SQL Server, SSIS, SSRS. Experience in Query Optimization, Procedures and Functions. Understanding of BI concepts  OLTP vs OLAP and deploying the applications on cloud servers. Experience in ExcelExperience in any BI / Reporting tool (preferably Tableau/Qlikview/Microstrategy).B.Tech. / M. Sc (Computer Science) / MCA / M.Tech.
College Preference : no-bar
Min Qualification : ug
Skills : qlikview, sql, sql server, sql server integration service (SSIS), SQL Server Reporting Services (SSRS), tableau
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/sr-programmer-msbi-gurgaon-2-6-years-of-experience/
"Consultant- CRM Analytics- San Francisco, US (4-6 Years of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr. Programmer- MSBI- Gurgaon (2-6 Years of Experience)|Digital Marketing Specialist- Gurgaon (6-7 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Position Summary:We are looking for Consultant with hands on experience in predictive analytics and SAS to be based out of our San Francisco office.Key Responsibilities:Qualifications and Skills:
College Preference : no-bar
Min Qualification : ug
Skills : Data analytics, excel, logistic regression, r, sas, spss, statistical techniques
Location : San Francisco
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/consultant-crm-analytics-san-francisco-us-4-6-years-of-experience/
Digital Marketing Specialist- Gurgaon (6-7 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Consultant- CRM Analytics- San Francisco, US (4-6 Years of Experience)|Basics of Probability for Data Science explained with examples in R|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  7 years
Requirements : 
Task Info : Position Summary:If you have a marketing background with a mix of creativity, technical skills, and analytics DNA, then this just may be the role for you. We are looking for an experienced, hands-on digital marketing manager who is passionate about using the latest marketing technologies to generate demand and leads. Youll be responsible for digital marketing programs and campaigns, as well as managing all our online properties (website, LinkedIn, Twitter, Facebook, etc.) from both a creative and technical side. Hands-on experience with social media marketing, video marketing, content marketing, online advertising and mobile marketing are a must. You will drive the acquisition of new leads, engaging prospects in the funnel, and generating MQLs across all digital marketing channels with a focus on data driven decisions and continuous optimization of programs.Key Responsibilities:Hands on execution and implementation is requiredCampaigns: Develop quarterly digital campaign strategy and calendar based on company objectivesDigital Advertising: Multi channel online campaign development and executionSocial Media: Manage social media presence on a daily basis, conduct integrated campaignsOnline properties: Maintain and update the website, LinkedIn, Twitter, Facebook, YouTubeWebsite: Site maintenance, manage updates, site administration, work with outside developerSEO: Institute best practices; constantly fine tune and optimize our propertiesPaid Search: Recommend strategies that align with quarterly goals, execute programs, report resultsLead Gen: Generate leads and demand for products and servicesProduction: Produce on-brand videos, ads, and content working with marketing managers, functional team leads, and creative services.Analytics: Metrics and reporting on all online properties and campaignsQualifications and Skills:Hands on execution and implementation is requiredFlawless English and familiarity with the US marketMix of creative, technical and analytics skillsExperience managing a website using WordPressSEO strategies, tools and techniquesTools experience in some or most of the following: WordPress, basic HTML, Google Analytics, Google Adwords, Hootsuite (or similar), Excel, Powerpoint, Adobe Illustrator (or similar)Technically skilled in SEM, PPC, lead generation, with some online ad experienceMBA preferred
College Preference : no-bar
Min Qualification : pg
Skills : excel, google analytics, HTML5, Power point
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/02/digital-marketing-specialist-gurgaon-6-7-years-of-experience/
Basics of Probability for Data Science explained with examples in R,Learn everything about Analytics|Introduction|Table of Contents|1. What is Probability?|2. Random Variables|3. Calculating Probability by principle of counting|4. Binomial Distribution||5. Continuous random variables|6. The Central Limit Theorem|7. Area Under the Normal Distribution|8. Z Scores|9. Challenges|End Notes,"|Why do we need probability?||Two throws of a dice|Life of an insect|What is a normal distribution?|Learn, compete, hack and get hired|Share this:|Like this:|Related Articles|Digital Marketing Specialist- Gurgaon (6-7 Years of Experience)|Comprehensive & Practical Inferential Statistics Guide for data science|
Dishashree Gupta
|54 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Statistically, the probability of any one of us being here is so small that youd think the mere fact of existing would keep us all in a contented dazzlement of surprise                                                    Lewis ThomasFor anyone taking first steps in data science, Probability is a must know concept. Concepts of probability theory are the backbone of many importantconcepts in data science like inferential statistics to Bayesian networks. It would not be wrong to say that the journey of mastering statistics begins with probability.In this guide, I will start withbasics of probability. Then Ill introduce binomial distribution, central limit theorem, normal distribution and Z-score. If they sound scary right now  just hold on for a few minutes. I have explained each concept with an example.I have explained each concept ina simplistic manner to avoid overload of mathematical concepts. At the end of the guide, I have given you two fun and exciting challenges. Go though them and post your answer in the comments sections. Lets see how many of you can answer them correctly.Lets explore probability together!Simply put, probability is an intuitive concept. We use it on a daily basis without necessarily realising that we are speaking and applying probability to work.Life is full of uncertainties. We dont know the outcomes of a particular situation until it happens.Will it rain today? Will I pass the next math test? Will my favorite team win the toss? Will I get a promotion in next 6 months? All these questions are examples of uncertain situations we live in. Let us map them to few common terminology which we will use going forward.In an uncertain world, it can be of immense help to know and understand chances of various events. You can plan thingsaccordingly.If its likely to rain, I would carry my umbrella. If I am likely to have diabetes on the basis of my food habits, I would get myself tested. If my customer is unlikely to pay me a renewal premium without a reminder, I would remind him about it.So knowing the likelihood might be very beneficial.To calculate the likelihood of occurence of an event, we need to put a framework to express the outcome in numbers. We can do this by mapping the outcome of an experiment to numbers.Lets define X to be the outcome of a coin toss.X = outcome of a coin tossPossible Outcomes:Lets take another one.Suppose, I win the game if I get a sum of 8 while rolling two fair dice. I can define my random variable Y to be (the sum of the upward face of two fair dice )Y can take values = (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)A few things to note about random variables:Lets say you went to a fair. There is a stall playing the game of spinning wheel. There are two colors evenly spread on the wheel  red and green. If you land on red, you lose, if you land on green you win.So what happens when you spin the wheel? You either win or you lose? There is no third outcome in this case. If the wheel is fair, there is a 50% chance of winning and 50% chance of losing.Next, suppose the organizer decides to increase the prize money and reduce the green area. Now only th area is green and th is red.How likely are you to win now?Only 25%! This 25% or .25 is the probability of winning.The next stall is our favorite dice stall, where we win if we get a sum of 8 in two throws. Lets see if we have more chances to win here.Lets take random variable X to be the sum of two throws. X can take values (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12). Lets see the probability of each number.Lets see the probability of each number.There are 6 possibilities in the first throw (we can get any number) and same 6 in the second. So total number if combinations would be 36.Lets see how:2{(1,1)} => 1/363{(1,2),(2,1)} => 2/364{(2,2),(3,1),(1,3)} => 3/365{(1,4),(4,1),(2,3),(3,2)} => 4/366{(3,3),(1,5),(5,1),(2,4),(4,2)} => 5/367{(1,6),(6,1),(2,5),(5,2),(3,4),(4,3)} => 6/368{(2,6),(6,2),(3,5),(5,3),(4,4)} => 5/369{(3,6),(6,3),(5,4),(4,5)} => 4/3610{(4,6),(6,4),(5,5)} => 3/3611{(5,6),(6,5)} => 2/3612{(6,6)} = > 1/36So, the chance of success here is 5/36 or approximately 1 in 7, while failure is 31/36. So, unless the stall rewards me 7x of the money I bet on winning, it is a bad game to participate in.We can write this as:You can also see that the total probability is 1. There are only these 2 possibilities.Lets see how these probabilities look like. The probability function for a discrete random variable is theprobability mass function. It shows the exact probabilities for a particular value of the random variable.Here is an important thing to note, a sum of 2.5 is not possible on the throw of two dice. So essentially, my random variable is discrete. There are only fixed integer values that it can take and we can see the probabilities of each occurring.Most of the times, the situations we encounter are pass-fail type. The democrats either win or lose the election. I either get a heads or tails on the coin toss. You either win or lose your football game (assuming that there is always a forced outcome). So there are only two outcomes  win and lose or success and failure. The likelihood of the two may or may not be the same.Let us understand this through an interesting example.Lets say your football team is playing a series of 5 games against your opponent. Who ever wins more games (out of 5) wins the title.Let us say, your team might is more skilled and has 75% chances of winning. So, there is a 25% chance of losing it.What is the probability of you winning the series? Is it 75%or is it something else?Let us find out. What are the possible scenarios in playing 5 games?WWWWW, WWWWL, WWWLL, WWLLL, WLLLL, LLLLL, LWWWW and so on.So for the first game, there are two possibilities, you either win or lose, again for the second game we have two possibilities. Assuming that the first game has no effect on the outcome of the second  No one gets tired, no one gets under pressure after losing etc.So lets define our random variable X to be a number of wins in 5 games. Remember probability of winning is 0.75 and losing is 0.25. Assume that a tie doesnt happen.X=Number of wins in 5 gamesSo the first game has 2 outcomes  win and lose, second again has 2 and so on.So total possibilities is 2*2*2*2*2 = 32While we can count each of these possible outcomes, it becomes very exhaustive and intensive exercise. Let us takehelp of combinatorics here. Choose 2 wins out of 5 games = 5C2 ()so, the Probability for getting k successes in n Bernoulli trails is given by:P(X=k) =nCk pk qn-k , [here p is the probability of success and q is the probability of failure]
Lets see how this comes.What we just calculated were discrete probabilities for a Binomial distribution. If we look at these probabilities we get something like:As you can see the probability of winning the series is much higher than 0.75.The general definition of a binomial distribution is the discrete probability distribution of the number of success in a sequence of n independent Bernoulli trials (having only yes/no or true/false outcomes).If the events are equally likely to occur i.e. p = q = 0.5, the probability distribution looks something like the graph below. Here the probability of success and failure is the same.What difference do we see in the two probability distributions? The first one is skewed towards right. Reason being the likelihood to win is more, hence more wins are more likely than more losses.In the second case when wins and losses are equally likely, so the distribution is symmetrical.Lets assume that probability of winning and losing is equal. p=q=0.5Now, What if I increase my number of trials? What if I play 20 games of football with a probability of winning and losing to be 50-50? There are a lot more possibilities and combinations. The bars get thinner and thinner.The bars get thinner and thinner.What if I play an infinite number of times with equal probability for winning and losing?The bars get infinitely small and the probability distribution looks something like a continuous set of bars which are very close, almost continuous. This now becomes a probability density function. Notice that this now becomes a continuous function.Lets point out some interesting things that happened.Lets take an example of Binomial distribution and implement in R.Example:In an entrance examination there are twenty multiple choice questions. Each question has four options, and only one of them is correct. Find the probability of having seven or less than seven correct answers if a student attempts to answer every question at random.Probability of exactly 7 correct answers at random attempts:OutputProbability of having 7 or less than seven correct answers at random:OutputLets see some cases where the random variables are continuous. Lets say the weatherman is trying to measure the amount of rainfall that will happen tomorrow.Lets say the rainfall likely to happen is around 2 cm. But will it be exactly 2 cm?It can be 2.001 or 2.000001 or 2.000000001 and an infinite number of values in between. Its even impossible for us to measure if its exactly 2 cm.So, we calculate the probability of it, being in a range. We calculate the probability of rainfall being in the range of 2 cm to 2.01 cm. It will be the sum of probabilities for all values between 2 and 2.01. The area under the probability density function with limits 2 and 2.01 will give us that.The probability density function may or may not be symmetrical.Suppose there is an insect whose lifespan ranges from 0 to 16 days. Were looking for the probability that it will die in around 5 to 6 days. Again we would need the sum of probabilities for all values between 5 days and 6 days.We look at the probability density function and find the area of the graph under the limits of 5 and 6. We can use definite integration under the desired limits for the probability density to find the area. Were often interested in the probability of a range of values rather than the probability of an exact value.We can now imagine that the probability at a particular point would be the area of the thinnest possible bar we can imagine. To calculate the probability at x, we would need the area from x to x+, where  is very very small.The total probability density function would then be the collection of all such areas / probabilities.The formula of the probability density function can be written as:For a point x,  is the small value right after the point x. We try to calculate the probability from x to x+ , with limit if  tends to 0.So when you have huge amount of data, you can be confused how to make sense of it. It is difficult to know whats happening underneath it. To tackle this problem, what we do is take a small chunk of data & look at it. But we wont be satisfied with just a single chunk. Wed try to look at multiple chunks to be sure of results.Lets say we have the cholesterol levels of all the people in India, we can look at the mean, median and mode of the data. Maybe plot a histogram with sensible ranges and look at the data. Lets assume this is how the data looks like. The mean of this data is 153.2But this huge amount of data is really tough to process. To process it, we take the data of some 50 people and calculate their mean.We again take a sample of some 50 people and calculate the mean and we keep doing that for quite a number of times. We now plot the means of these samples.
We see that these sample means form a frequency distribution which looks very symmetrical. The frequency around the mean of the actual data is the highestand gradually reduces as we move away from the mean on the either side.So when we take means of cholesterol levels of 50 people, again and again, we observe the mean values are around 150-160. Only a few mean values is more than 170 and less than 140. There are very, very few over 190 or less than 110.We can easily convert the frequencies to see probabilities. If we divide the frequency of a bin (range like 110 to 120) by the total number of data points, we get the probabilities of each bin. So, now the frequency distribution becomes a probability distribution of the same shape. The probability distribution approaches more and more towardsThe probability distribution approaches more and more towards symmetry, when the sample size that we use to create those means, is very large. As the sample size approaches infinity, the probability distribution becomes a perfectly symmetrical where the center of the curve is the mean of the population. The curve is known as normal distribution.The normal distribution informally called as a bell curve looks like this:The equation of the normal distribution happens to be:Here  is the mean of the data while  is the standard deviation of the data.The normal distribution is perfectly symmetrical about the mean. The probabilities move similarly in both directions around the mean. The total area under the curve is 1, since summing up all the possible probabilities would give 1.The distribution might vary a bit depending upon how spread the data is. If the data has a very high range and standard deviation, the normally distributed curve would be spread out and flatter, since a large number of values would be sufficiently away from the mean.Also, if a lot of values are away from the mean, the probability for data being around the mean also drops. Similarly, if the standard deviation is low, which means most of the values are near around the mean, there is high probability of the sample mean being around the mean and the distribution is a lot skinnier. The higher the standard deviation, the thicker and flatter the curve.Lets summarise the main points we saw:Lets take an example to implement Normal Distribution in R:Example:Let us assume that the test scores an entrance exam fits a normal distribution where the mean test score is 67, and the standard deviation is 13.7. Calculate the percentage of students scoring 80 or more in the exam?Since, we are interested in the upper tail of distribution we will apply the following codes:Output17.13% students scored 80 or more in the entrance exam.Now, lets say I have a dataset of cholesterol levels of a number of patients and we need to calculate the probability of how many patients are healthy.The mean value () for cholesterol of all the patients is equal to 150 and standard deviation () is equal to 15. The probability density function is a normal distribution given by the above equation.We need to calculate the probability of cholesterol levels to be between 135 (150-15) and 165 (150+15)  the healthy cholesterol range.Can you see that the healthy patients that we are talking about are one standard deviation on either side of the mean? This means we need to calculate the area under the curve with 135 and 165 as limits. Dont worry, this area for normal distribution is already calculated for us and is ~68%.So always for a normally distributed data, around 68% of the data falls within 1 standard deviation of the mean. So probability of the data being within 1 standard deviation if the mean = 0.68Lets also calculate the probability of being 2 standard deviations away from the mean. Lets say we need to warn the patients who are two standard deviations away.This means 150+30 and 150-30.The range of area to be calculated now is 120 to 180.To your surprise, 95% of the values fall in this range.So the major chunk of the data falls within 2 standard deviations of the mean. So, we can safely say, that for the data to be more than 2 standard deviations away from the mean is highly unlikely  only 5% likely.So, 95% of the patients have their cholesterol levels between 120 and 180. And the remaining 5% are really critical and different from the average values.We will encounter a lot of cases, where we would need to know the probability for the data to be less than or more than a particular value. This value will not be equal to 1 or 2 away from the mean.The distance in terms of number of standard deviations, the observed value is away from the mean, is the standard score or the Z score.A positive Z score indicates that the observed value is Z standard deviations above the mean. Negative Z score indicates that the value is below the mean.Observed value = +z [ is the mean and  is the standard deviation]
In our cholesterol example, lets see where 172 falls on the distribution. We will calculate the Z score to find the percentage of people having cholesterol less than 172.172 = 150+Z*15Here, we see that 172 is 1.47 {(142-150)/15} standard deviations more than the mean. This 1.47 is known as the z value.Now, we would need to use these limits to calculate the area under the curve. Remember that the area under the curve is 1. Lets calculate the probability of people having a cholesterol level of less than 172.To your happiness, you will never have to actually calculate the area under the normal curve, we have the z table that can be used to calculate the probabilities for particular z values. The rows of the Z table have the Z score in tens, while the hundredths decimal is given by the columns. The value is the area under the curve less than that Z score.For a particular z score, we can look into the table to find the probability for values to fall less than that particular z value. It can be ve or +ve. If we look out for 1.47, we find that ~93% data falls less than that. Therefore, 93% patients have cholesterol less than 172. Also, we can safely say that 7% have cholesterol more than 172.Come lets learn to calculate the Z-Score in R with the help of an example:Example:A mobile manufacturing company has taken a sample of mobiles of the same model from the previous months data. They want to calculate the z-score for a given value of 9.5 cm screen size of a particular mobile. You can download the data here.The codes are as follows:OutputHope you found the guide simple and useful. I hope some one would have given this early in my career. Here are a few challenges for you to try:Challenge 1: Contrary to the popular expectation, try calculating the probability of getting 50 heads and 50 tails on 100 flips of fair coins? This expectation is known as the gamblers fallacy! An approximate answer would suffice!Challenge 2: Try another one  In the United States, the average IQ is 100, with a standard deviation of 15. What percentage of the population would you expect to have an IQ more than 120?Post your answers in the comments section.I hope you liked reading the article. To summarize, there are numerous mathematical and everyday problems that are solved using probability and the properties of normal distribution. Problems ranging from biology to industrial automation, probability is ubiquitous  Cant live with it, cant do without it.If you have any doubts or questions, post them in the comments below.",https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/
Comprehensive & Practical Inferential Statistics Guide for data science,"Learn everything about Analytics|Introduction|Table of Contents|1. Why do we need Inferential Statistics?|2. Pre-Requisites|3. Sampling Distribution and Central Limit Theorem|4. Hypothesis Testing|5. Types of Errors in Hypothesis Testing|6. T-tests|7. Different types of t-tests|8. ANOVA|9. Chi-square Goodness of Fit Test|10. Regression and ANOVA
|11. Coefficient of Determination (R-Square)|12. Correlation Coefficient","Enter Inferential Statistics|3.1 Central Limit Theorem|3.2 Confidence Interval|3.3 Practical example|7.1 1-sample t-test|7.2 Paired t-test|7.3 2-sample t-test|7.4 Practical example|8.1 Steps to perform ANOVA|8.2 Practical Example|End Notes|Learn, compete, hack and get hired|Share this:|Like this:|Related Articles|Basics of Probability for Data Science explained with examples in R|45 Questions to test a data scientist on basics of Deep Learning (along with solution)|
NSS
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 0: Identifying the type of t-test|Step 1: State the Null and Alternate Hypothesis|Step 2: Calculate the appropriate test statistic|Step 1: Stating the Null and Alternate Hypothesis|Step 2: Calculating the appropriate ANOVA statistic|Step 0: State the Null and Alternate Hypothesis|Step 1:|Step 2:|Step 3:|Step 4:|Step 5:|Step 6:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Statistics is one of the key fundamental skills required for data science. Any expert in data science would surely recommendlearning / upskilling yourself in statistics.However, if you go out and look for resources on statistics, you will see that a lot of them tend to focus on the mathematics. They will focus on derivation of formulas rather than simplifying the concept. I believe, statistics can be understood in very simple and practical manner. That is why I have created this guide.In this guide, I will take you through Inferential Statistics, which isone of the most important concepts in statistics for data science.I will take you through all the related concepts of Inferential Statistics and their practical applications.This guide would actas a comprehensive resource to learn Inferential Statistics. So, go through the guide, section by section. Work through the examples and develop your statistics skills for data science.Read on!Suppose, you want to know the average salary of Data Science professionals in India. Which of the following methods can be used to calculate it?Well, the first method is not impossible but it would require an enormous amount of resources and time. But today, companies want to make decisions swiftly and in a cost-effective way, so the first method doesnt stand a chance.On the other hand, second method seems feasible. But, there is a caveat. What if the population of Gurgaon is not reflective of the entire population of India? There are then good chances of you making a very wrong estimate of the salary of Indian Data Science professionals.Now, what method can be used to estimate the average salary of all data scientists across India?In simple language, Inferential Statistics is used to draw inferences beyond the immediate data available.With the help of inferential statistics, we can answer the following questions:I am sure by now you must have got a gist of why inferential statistics is important. I will take you through the various techniques & concepts involved in Inferential statistics. But first, lets discuss what are the prerequisites for understanding Inferential Statistics.To begin withInferential Statistics, one must have a good grasp over the following concepts:If you are not comfortable with either of the three concepts mentioned above, you must go throughthem before proceeding further.Throughout the entire article, I will be using a few terminologies quite often. So, here is a brief description of them:Suppose, you note down the salary of any 100 random Data Science professionals in Gurgaon, calculate the mean and repeat the procedure for say like 200 times (arbitrarily).When you plot a frequency graph of these 200 means, you are likely to get a curve similar to the one below.This looks very much similar to the normal curve that you studied in the Descriptive Statistics. This is called Sampling Distribution or the graph obtained by plotting sample means. Let us look at a more formal description of a Sampling Distribution.A Sampling Distribution is a probability distribution of a statistic obtained through a large number of samples drawn from a specific population.A Sampling Distribution behaves much like a normal curve and has some interesting properties like :Population DistributionBut how ?This will be explained using a very important theorem in statistics  The Central Limit Theorem.It states that when plotting a sampling distribution of means, the mean of sample means will be equal to the population mean. And the sampling distribution will approach a normal distribution with variance equal to/n where is the standard deviation of population and n is the sample size.Points to note:This seemed too technical isnt it? Lets break this down to understand this point by point.Now, since we have collected the samples and plotted their means, it is important to know where the population mean lies with respect to a particular sample mean and how confident can we be about it. This brings us to our next topic  Confidence Interval.The confidence interval is a type of interval estimate from the sampling distribution which gives a range of values in which the population statistic may lie. Let us understand this with the help of an example.We know that 95% of the values lie within 2 (1.96 to be more accurate) standard deviation of a normal distribution curve. So, for the above curve, the blue shaded portion represents the confidence interval for a sample mean of 0.Formally, Confidence Interval is defined as,whereas,= the sample mean= Z value for desired confidence level = the population standard deviationFor an alpha value of 0.95 i.e 95% confidence interval, z=1.96.Now there is one more term which you should be familiar with,Margin of Error. It is given as {(z.)/n} and defined as the sampling error by the surveyor or the person who collected the samples. That means, if a sample mean lies in the margin of error range then, it might be possible that its actual value is equal to the population mean and the difference is occurring by chance. Anything outside margin of error is considered statistically significant.And it is easy to infer that the error can be both positive and negative side. The whole margin of error on both sides of the sample statistic constitutes the Confidence Interval. Numerically, C.I is twice of Margin of Error.The below image will help you better visualize Margin of Error and Confidence Interval.The shaded portion on horizontal axis represents the Confidence Interval and half of it is Margin of Error which can be in either direction of x (bar).Interesting points to note about Confidence Intervals:Many people do not have right knowledge about confidence interval and often interpret it incorrectly. So, I would like you to take your time visualizing the 4th argument and let it sink in.Calculate the 95% confidence interval for a sample mean of 40 and sample standard deviation of 40 with sample size equal to 100.Solution:We know, z-value for 95% C.I is 1.96. Hence, Confidence Interval (C.I) is calculated as:C.I= [{x(bar)  (z*s/n)},{x(bar)  (z*s/n)}]
C.I = [{40-(1.96*40/10},{ 40+(1.96*40/10)}]
C.I = [32.16, 47.84]
Before I get into the theoretical explanation, let us understand Hypothesis Testing by using a simple example.Example: Class 8th has a mean score of 40 marks out of 100. The principal of the school decided that extra classes are necessary in order to improve the performance of the class. The class scored an average of 45 marks out of 100 after taking extra classes. Can we be sure whether the increase in marks is a result of extra classes or is it just random?Hypothesis testing lets us identify that. It lets a sample statistic to be checked against a population statistic or statistic of another sample to study any intervention etc. Extra classes being the intervention in the above example.Hypothesistesting is defined in two terms  Null Hypothesis and Alternate Hypothesis.Hypothesis Testing is done on different levels of confidence and makes use of z-score to calculate the probability. So for a 95% Confidence Interval, anything above the z-threshold for 95% would reject the null hypothesis.Points to be noted:Now we have defined a basic Hypothesis Testing framework. It is important to look into some of the mistakes that are committed while performing Hypothesis Testing and try to classify those mistakes if possible.Now, look at the Null Hypothesis definition above. What we notice at the first look is that it is a statement subjective to the tester like you and me and not a fact. That means there is a possibility that the Null Hypothesis can be true or false and we may end up committing some mistakes on the same lines.There are two types of errors that are generally encountered while conducting Hypothesis Testing.The below image will summarize the types of error :T-tests are very much similar to the z-scores, the only difference being that instead of the Population Standard Deviation, we now use the Sample Standard Deviation. The rest is same as before, calculating probabilities on basis of t-values.The Sample Standard Deviation is given as:where n-1 is the Bessels correction for estimating the population parameter.Another difference between z-scores and t-values are that t-values are dependent on Degree of Freedom of a sample. Let us define what degree of freedom is for a sample.The Degree of Freedom  It is the number of variables that have the choice of having more than one arbitrary value. For example, in a sample of size 10 with mean 10, 9 values can be arbitrary but the 1oth value is forced by the sample mean.Points to note about the t-tests:This is the same test as we described above. This test is used to:For eg: A pizza delivery manager may perform a 1-sample t-test whether their delivery time is significantly different from that of the advertised time of 30 minutes by their competitors.where, X(bar)= sample mean= population means= sample standard deviationN = sample sizePaired t-test is performed to check whether there is a difference in mean after a treatment on a sample in comparison to before. It checks whether the Null hypothesis: The difference between the means is Zero, can be rejected or not.The above example suggests that the Null Hypothesis should not be rejected and that there is no significant difference in means before and after the intervention since p-value is not less than the alpha value (o.o5) and t stat is not less than t-critical. The excel sheet for the above exercise is available here.where, d (bar) = mean of the case wise difference between before and after,=standard deviation of the differencen = sample size.This test is used to determine:The above formula represents the 2 sample t-test and can be used in situations like to check whether two machines are producing the same output. The points to be noted for this test are:where, X1 (bar) = mean of the first group= represents1st group sample standard deviation= represents the 1st group sample size.We will understand how to identify which t-test to be used and then proceed on to solve it. The other t-tests will follow the same argument.Example: A population has mean weight of 68 kg. A random sample of size 25 has a mean weight of 70 with standard deviation =4. Identify whetherthis sample is representative of the population?Number of samples in question = 1Number of times the sample is in study = 1Any intervention on sample = NoRecommended t-test = 1- sample t-test.Had there been 2 samples, we would have opted for 2-sample t-test and if there would have been 2 observations on the same sample, we would have opted for paired t-test.`Null Hypothesis:The sample mean and population mean are same.Alternate Hypothesis:The sample mean and population mean are different.df = 25-1 =24t= (70-68)/(4/25) = 2.5Now, for a 95% confidence level, t-critical (two-tail) for rejecting Null Hypothesis for 24 d.f is 2.06 . Hence, we can reject the Null Hypothesis and conclude that the two means are different.You can use the t-test calculator here.ANOVA (Analysis of Variance) is used to check if at least one of two or more groups have statistically different means. Now, the question arises  Why do we need another test for checking thedifference of means between independent groups? Why can we not use multiple t-tests to check for the difference in means?The answer is simple. Multiple t-tests will have a compound effect on the error rate of the result. Performing t-test thrice will give an error rate of ~15% which is too high, whereas ANOVA keeps it at 5% for a 95% confidence interval.To perform an ANOVA, you must have a continuous response variable and at least one categorical factor with two or more levels. ANOVA requires data from approximately normally distributed populations with equal variances between factor levels. However, ANOVA procedures work quite well even if the normality assumption has been violatedunless one or more of the distributions are highly skewed or if the variances are quite different.ANOVA is measured using a statistic known as F-Ratio. It is defined as the ratio of Mean Square (between groups) to the Mean Square (within group).Mean Square (between groups) = Sum of Squares (between groups) / degree of freedom (between groups)Mean Square (within group) = Sum of Squares (within group) / degree of freedom (within group)Here, p = represents the number of groupsn =represents the number of observations in a group= represents the mean of a particular groupX (bar) = represents the mean of all the observationsNow, let us understand the degree of freedom for within group and between groups respectively.Between groups : If there are k groups in ANOVA model, then k-1 will be independent. Hence, k-1 degree of freedom.Within groups : If N represents the total observations in ANOVA (n over all groups) and k are the number of groups then, there will be k fixed points. Hence, N-k degree of freedom.There are various other forms of ANOVA too like Two-way ANOVA, MANOVA, ANCOVA etc. but One-Way ANOVA suffices the requirements of this course.Practical applications of ANOVA in modeling are:Suppose there are 3 chocolates in town and their sweetness is quantified by some metric (S). Data is collected on the three chocolates. You are given the task to identify whether the mean sweetness of the 3 chocolates are different. The data is given as below:                                Type A          Type B          Type CNow we will proceed step-wise to calculate the F-Ratio (ANOVA statistic).Null Hypothesis:Mean sweetness of the three chocolates are same.Alternate Hypothesis:Mean sweetness of at least one of the chocolates is different.In this part, we will be calculating SS(B), SS(W), SS(T) and then move on to calculate MS(B) and MS(W). The thing to note is that,Total Sum of Squares [SS(t)] = Between Sum of Squares [SS(B)] + Within Sum of Squares [SS(W)].So, we need to calculate any two of the three parameters using the data table and formulas given above.As, per the formula above, we need one more statistic i.e Grand Mean denoted by X(bar) in the formula above.X bar = (643+655+702+469+427+525+484+456+402)/9 = 529.22SS(B)=[3*(666.67-529.22)^2]+ [3*(473.67-529.22)^2]+[3*(447.33-529.22)^2] = 86049.55SS (W) = [(643-666.67)^2+(655-666.67)^2+(702-666.67)^2] + [(469-473.67)^2+(427-473.67)^2+(525-473.67)^2] + [(484-447.33)^2+(456-447.33)^2+(402-447.33)^2]= 10254MS(B) = SS(B) / df(B) = 86049.55 / (3-1) = 43024.78MS(W) = SS(W) / df(W) = 10254/(9-3) = 1709F-Ratio = MS(B) / MS(W) = 25.17 .Now, for a 95 % confidence level, F-critical to reject Null Hypothesis for degrees of freedom(2,6) is 5.14 but we have 25.17 as our F-Ratio.So, we can confidently reject the Null Hypothesis and come to a conclusion that at least one of the chocolate has a mean sweetness different from the others.You can use the F-calculator here.Note: ANOVA only lets us know the means fordifferent groups are same or not. It doesnt help usidentify which mean is different.To know which group mean is different, we can use another test know as Least Significant Difference Test.Sometimes, the variable under study is not a continuous variable but a categorical variable. Chi-square test is used when we have one single categorical variable from the population.Let us understand this with help of an example. Suppose a company that manufactures chocolates, states that they manufacture 30% dairy milk, 60% temptation and 10% kit-kat. Now suppose a random sample of 100 chocolates has 50 dairy milk, 45 temptation and 5 kitkats. Does this support the claim made by the company?Let us state our Hypothesis first.Null Hypothesis: The claims are TrueAlternate Hypothesis: The claims are False.Chi-Square Test is given by:where, = sample or observed values= population valuesThe summation is taken over all the levels of a categorical variable.=[n * ] Expected value of a level (i) is equal to the product of sample size and percentage of it in the population.Let us now calculate the Expected values of all the levels.E (dairy milk)= 100 * 30% = 30E (temptation) = 100 * 60% =60E (kitkat) = 100 * 10% = 10Calculating chi-square = [(50-30)^2/30+(45-60)^2/60+(5-10)^2/10] =19.58Now, checking for p (chi-square >19.58) using chi-square calculator, we get p=0.0001. This is significantly lower than the alpha(0.05).So we reject the Null Hypothesis.If you have studied some basic Machine Learning Algorithms, the first algorithm that you must have studied is Regression. If we recall those lessons of Regression, what we generally do is calculate the weights for features present in the model to better predict the output variable. But finding the right set of feature weights or features for that matter is not always possible.It is highly likely that that the existing features in the model are not fit for explaining the trend in dependent variable or the feature weights calculated fail at explaining the trend in dependent variable. What is important is knowing the degree to which our model is successful in explaining the trend (variance) in dependent variable.Enter ANOVA.With the help of ANOVA techniques, we can analyse a model performance very much like we analyse samples for being statistically different or not.But with regression things are not easy. We do not have mean of any kind to compare or sample as such but we can find good alternatives in our regression model which can substitute for mean and sample.Sample in case of regression is a regression model itself with pre-defined features and feature weights whereas mean is replaced by variance(of both dependent and independent variables).Through our ANOVA test we would like to know the amount of variance explained by the Independent variables in Dependent Variable VS the amount of variance that was left unexplained.It is intuitive to see that larger the unexplained variance(trend) of the dependent variable smaller will be the ratio and less effective is our regression model. On the other hand, if we have a large explained variance then it is easy to see that our regression model was successful in explaining the variance in the dependent variable and more effective is our model. The ratio of Explained Variance uand Unexplained Variance is called F-Ratio.Let us now define these explained and unexplained variances to find the effectiveness of our model.1. Regression (Explained) Sum of Squares  It is defined as the amount of variation explained by the Regression model in the dependent variable.Mathematically, it is calculated as:where,[hat] = predicted value andy(bar) = mean of the actual y values.Interpreting Regression sum of squares If our model is a good model for the problem at hand then it would produce an output which has distribution as same to the actual dependent variable. i.e it would be able to capture the inherent variation in the dependent variable.2. Residual Sum of Squares  It is defined as the amount of variation independent variable which is not explained by the Regression model.Mathematically, it is calculated as:where, = actual y  valuef(x) = predicted valueInterpretation of Residual Sum of Squares It can be interpreted as the amount by which the predicted values deviated from the actual values. Large deviation would indicate that the model failed at predicting the correct values for the dependent variable.Let us now work out F-ratio step by step. We will be making using of the Hypothesis Testing framework described above to test the significance of the model.While calculating the F-Ratio care has to be taken to incorporate the effect of degree of freedom. Mathematically, F-Ratio is the ratio of [Regression Sum of Squares/df(regression)] and [Residual Sum of Squares/df(residual)].We will be understanding the entire concept using an example and this excel sheet.Null Hypothesis: The model is unable to explain the variance in the dependent variable (Y).Alternate Hypothesis: The model is able to explain the variance in dependent variable (Y)Calculate the regression equation for X and Y using Excels in-built tool.Predict the values of y for each row of data.Calculate y(mean)  mean of the actual y values which in this case turns out to be 0.4293548387.Calculate the Regression Sum of Squares using the above-mentioned formula. It turned out to be 2.1103632473TheDegree of freedom for regression equation is 1, since we have only 1 independent variable.Calculate the Residual Sum of Squares using the above-mentioned formula. It turned out to be 0.672210946.Degree of Freedom for residual = Total degree of freedom  Degree of freedom(regression)=(62-1)  1 = 60F-Ratio = (2.1103632473/1)/(0.672210946/60) = 188.366Now, for 95% confidence, F-critical to reject Null Hypothesis for 1,60 degrees of freedom in 4. But we have F-ratio as 188, so we can safely reject the Null Hypothesis and conclude that model explains variation to a large extent.It is defined as the ratio of the amount of variance explained by the regression model to the total variation in the data. It represents the strength of correlation between two variables.We already calculated the Regression SS and Residual SS. Total SS is the sum of Regression SS and Residual SS.Total SS = 2.1103632473+ 0.672210946 = 2.78257419Co-efficient of Determination = 2.1103632473/2.78257419 = 0.7588This is another useful statistic which is used to determine the correlation between two variables. It is simply the square root of coefficient of Determination and ranges from -1 to 1 where 0 represents no correlation and 1 represents positive strong correlation while -1 represents negative strong correlation.So, this guide comes to an end with explaining all the theory along with practical implementations of various Inferential Statistics concepts. This guide has been created with a Hypothesis Testing framework and I hope this would be one stop solution for a quick Inferential Statistics guide.If you have any doubts or questions, feel free to drop your comments below. And in case if I have missed out any of the concepts,add them below. The rest of the readers and I would definitely like to know.",https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/
45 Questions to test a data scientist on basics of Deep Learning (along with solution),Learn everything about Analytics|Introduction|Overall Results|Helpful Resources|Questions andAnswers|End Notes,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Comprehensive & Practical Inferential Statistics Guide for data science|Infographic  Learning Plan 2017 for beginners in data science|
Faizan Shaikh
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Back in 2009, deep learning was only an emerging field. Only a few people recognised it as a fruitful area of research. Today, itis being used for developing applications which were considered difficult or impossible to do till some time back.Speech recognition, image recognition, finding patterns in a dataset, object classification in photographs, character text generation, self-driving cars and many more are just a few examples. Hence it is important to be familiar with deep learning and its concepts.In this skilltest, we tested our community on basic concepts of Deep Learning. A total of 1070 people participated in this skill test.If you missedtaking the test, here is your opportunity to look at the questions and check your skill level.Below is thedistribution of scores, this will help you evaluate your performance:You can access your performance here. More than 200 people participated in the skill testand the highest score was 35.Here are a few statistics about the distribution.Overall distributionMean Score:16.45Median Score:20Mode Score: 0It seems like a lot of people started the competition very late or didnt take it beyond a few questions. I am not completely sure why, but may be because the subject is advanced for a lot of audience.If you have any insight on why this is so, do let us know.Fundamentals of Deep Learning  Starting with Artificial Neural NetworkPractical Guide to implementing Neural Networks in Python (using Theano)A Complete Guide on Getting Started with Deep Learning in PythonTutorial: Optimizing Neural Networks using Keras (with Image recognition case study)An Introduction to Implementing Neural Networks using TensorFlowQ1.A neural network model is said to be inspired from the human brain.The neural network consists of many neurons, each neuron takes an input, processes it and gives an output. Heres a diagrammatic representation of a real neuron.Which of the following statement(s) correctly represents a real neuron?A. A neuron has a single input and a single output onlyB. A neuron has multiple inputs but a single output onlyC. A neuron has a single input but multiple outputsD. A neuron has multiple inputs and multiple outputsE. All of the above statements are validSolution: (E)A neuron can have a single Input / Output or multiple Inputs / Outputs.Q2.Below is a mathematical representation of a neuron.The different components of the neuron are denoted as:Considering the above notations, will a line equation (y = mx + c) fall into the category of a neuron?A. YesB. NoSolution: (A)A single neuron with no non-linearity can be considered as a linear regression function.Q3. Let us assume we implement an AND function to a single neuron. Below is a tabular representation of an AND function:The activation function of our neuron is denoted as:What would be the weights and bias?(Hint: For which values of w1, w2 and b does our neuron implement an AND function?)A. Bias = -1.5, w1 = 1, w2 = 1B. Bias = 1.5, w1 = 2, w2 = 2C. Bias = 1, w1 = 1.5, w2 = 1.5D. None of theseSolution: (A)A.Therefore option A is correctQ4.A network is created when we multiple neurons stack together. Let us take an example of a neural network simulating an XNOR function.You can see that the last neuron takes input from two neurons before it. The activation function for all the neurons is given by:Suppose X1 is 0 and X2 is 1, what will be the output for the above neural network?A. 0B. 1Solution: (A)Output of a1: f(0.5*1 + -1*0 + -1*1) = f(-0.5) = 0Output of a2: f(-1.5*1 + 1*0 + 1*1) = f(-0.5) = 0Output of a3: f(-0.5*1 + 1*0 + 1*0) = f(-0.5) = 0So the correct answer is AQ5. In a neural network, knowing the weight and bias of each neuron is the most important step. If you can somehow get the correct value of weight and bias for each neuron, you can approximate any function.What would be the best way to approach this?A. Assign random values and pray to God they are correctB. Search every possible combination of weights and biases till you get the best valueC. Iteratively check that after assigning a value how far you are from the best values, and slightly change the assigned values values to make them betterD. None of theseSolution: (C)Option C is the description of gradient descent.Q6.What are the steps for using a gradient descent algorithm?A. 1, 2, 3, 4, 5B. 5, 4, 3, 2, 1C. 3, 2, 1, 5, 4D. 4, 3, 1, 5, 2Solution: (D)Option D is correctQ7. Suppose you have inputs as x, y, and z with values -2, 5, and -4 respectively. You have a neuron q and neuron f with functions:q = x + yf = q * zGraphical representation of the functions is as follows:What is the gradient of F with respect to x, y, and z?(HINT: To calculate gradient, you must find (df/dx), (df/dy) and (df/dz))A. (-3,4,4)B. (4,4,3)C. (-4,-4,3)D. (3,-4,-4)Solution: (C)Option C is correct.Q8. Nowlets revise the previous slides. We have learned that:Given above is a description of a neural network.When does a neural network model become a deep learning model?A. When you add more hidden layers and increase depth of neural networkB. When there is higher dimensionality of dataC. When the problem is an image recognition problemD. None of theseSolution: (A)More depth means the network is deeper. There is no strict rule of how many layers are necessary to make a model deep, but still if there are more than 2 hidden layers, the model is said to be deep.Q9.A neural network can be considered as multiple simple equations stacked together. Suppose we want to replicate the function for the below mentioned decision boundary.
Using two simple inputs h1 and h2What will be the final equation?A. (h1 AND NOT h2) OR (NOT h1 AND h2)B. (h1 OR NOT h2) AND (NOT h1 OR h2)C. (h1 AND h2) OR (h1 OR h2)D. None of theseSolution: (A)As you can see, combining h1 and h2 in an intelligent way can get you a complex equation easily. Refer Chapter 9 of this bookQ10.Convolutional Neural Networks can perform various types of transformation (rotations or scaling) in an input. Is the statement correct True or False?A. TrueB. FalseSolution: (B)Data Preprocessing steps (viz rotation, scaling) is necessary before you give the data to neural network because neural network cannot do it itself.Q11. Which of the following techniques perform similar operations asdropout in a neural network?A. BaggingB. BoostingC. StackingD. None of theseSolution: (A)Q 12. Which of the following gives non-linearity to a neural network?A. Stochastic Gradient DescentB. Rectified Linear UnitC. Convolution functionD. None of the aboveSolution: (B)Rectified Linear unit is a non-linear activation function.Q13.In training a neural network, you notice that the loss does not decrease in the few starting epochs.
The reasons for this could be:What according to you are the probable reasons?A. 1 and 2B. 2 and 3C. 1 and 3D. Any of theseSolution: (D)The problem can occur due to any of the reasons mentioned.Q14.Which of the following is true about model capacity(where model capacity means the ability of neural network to approximate complex functions) ?A. As number of hidden layers increase, model capacity increasesB. As dropout ratio increases, model capacity increasesC. As learning rate increases, model capacity increasesD. None of theseSolution: (A)Only option A is correct.Q15.If you increase the number of hidden layers in a Multi Layer Perceptron, the classification error of test data always decreases. True or False?A. TrueB. FalseSolution: (B)This is not always true. Overfitting may cause the error to increase.Q16.You are building a neural network where it gets input from the previous layer as well as from itself.Which of the following architecture has feedback connections?A. Recurrent Neural networkB. Convolutional Neural NetworkC. Restricted Boltzmann MachineD. None of theseSolution: (A)Option A is correct.Q17.What is the sequence of the following tasks in a perceptron?A. 1, 2, 3, 4B. 4, 3, 2, 1C. 3, 1, 2, 4D. 1, 4, 3, 2Solution: (D)Sequence D is correct.Q18.Suppose that you have to minimize the cost function by changing the parameters.Which of the following technique could be used for this?A. Exhaustive SearchB. Random SearchC. Bayesian OptimizationD. Any of theseSolution: (D)Any of the above mentioned technique can be used to change parameters.Q19.First Order Gradient descent would not work correctly (i.e. may get stuck) in which of the following graphs?A. B. C. D. None of theseSolution: (B)This is a classic example of saddle point problem of gradient descent.Q20.The below graph shows the accuracy of a trained 3-layer convolutional neural network vs the number of parameters (i.e. number of feature kernels).
The trend suggests that as you increase the width of aneural network, the accuracy increases till a certain threshold value, and then starts decreasing.What could be the possible reason for this decrease?A. Even if number of kernels increase, only few of them are used for predictionB. As the number of kernels increase, the predictive power of neural network decreaseC. As the number of kernels increase, they start to correlate with each other which in turn helps overfittingD. None of theseSolution: (C)As mentioned in option C, the possible reason could be kernel correlation.Q21.Suppose we have one hidden layer neural network as shown above. The hidden layer in this network works as a dimensionality reductor. Now instead of using this hidden layer, we replace it with a dimensionality reduction technique such as PCA.Would the network that uses a dimensionality reduction technique always give same output as network with hidden layer?A. YesB. NoSolution: (B)Because PCA works on correlated features, whereas hidden layers work on predictive capacity of features.Q22.Can a neural network model the function(y=1/x)?A. YesB. NoSolution: (A)Option A is true, because activation function can be reciprocal function.Q23.In which neural net architecture, does weight sharing occur?A. Convolutional neural NetworkB. Recurrent Neural NetworkC. Fully Connected Neural NetworkD. Both A and BSolution: (D)Option D is correct.Q24.Batch Normalization is helpful becauseA. It normalizes (changes) all the input before sending it to the next layerB. It returns back the normalized mean and standard deviation of weightsC. It is a very efficient backpropagation techniqueD. None of theseSolution: (A)To read more about batch normalization, see refer this videoQ25.Instead of trying to achieve absolute zero error, we set a metric called bayes error which is the error we hope to achieve.What could be the reason for using bayes error?A. Input variables may not contain complete information about the output variableB. System (that creates input-output mapping) may be stochasticC. Limited training dataD. All the aboveSolution: (D)In reality achieving accurate prediction is a myth. So we should hope to achieve an achievable result.Q26.The number of neurons in the output layer should match the number of classes (Where the number of classes is greater than 2) in a supervised learning task. True or False?A. TrueB. FalseSolution: (B)It depends on output encoding. If it is one-hot encoding, then its true. But you can have two outputs for four classes, and take the binary values as four classes(00,01,10,11).Q27.In a neural network, which of the following techniques is used to deal with overfitting?A. DropoutB. RegularizationC. Batch NormalizationD. All of theseSolution: (D)All of the techniques can be used to deal with overfitting.Q28.Y = ax^2 + bx + c (polynomial equation of degree 2)Can this equation be represented by a neural network of single hidden layer with linear threshold?A. YesB. NoSolution: (B)The answer is no because having a linear threshold restricts your neural network and in simple terms, makes it a consequential linear transformation function.Q29.What is a dead unit in a neural network?A. A unit which doesnt update during training by any of its neighbourB. A unit which does not respond completely to any of the training patternsC. The unit which produces the biggest sum-squared errorD. None of theseSolution: (A)Option A is correct.Q30.Which of the following statement is the best description of early stopping?A. Train the network until a local minimum in the error function is reachedB. Simulate the network on a test dataset after every epoch of training. Stop training when the generalization error starts to increaseC. Add a momentum term to the weight update in the Generalized Delta Rule, so that training converges more quicklyD. A faster version of backpropagation, such as the `Quickprop algorithmSolution: (B)Option B is correct.Q31.What if we use a learning rate thats too large?A. Network will convergeB. Network will not convergeC. Cant SaySolution: BOption B is correct because the error rate would become erratic and explode.Q32.The network shown in Figure 1 is trained to recognize the characters H and T as shown below:What would be the output of the network?Solution: (D)Without knowing what are the weights and biases of a neural network, we cannot comment on what output it would give.Q33.Suppose a convolutional neural network is trained on ImageNet dataset (Object recognition dataset). This trained model is then given a completely white image as an input.The output probabilities for this input would be equal for all classes. True or False?A. TrueB. FalseSolution: (B)There would be some neurons which are do not activate for white pixels as input. So the classes wont be equal.Q34.When pooling layer is added in a convolutional neural network, translation in-varianceis preserved. True or False?A. TrueB. FalseSolution: (A)Translation invariance is induced when you use pooling.Q35.Which gradient technique is more advantageous when the data is too big to handle in RAM simultaneously?A. Full Batch Gradient DescentB. Stochastic Gradient DescentSolution: (B)Option B is correct.Q36.The graph represents gradient flow of a four-hidden layer neural network which is trained using sigmoid activation function per epoch of training. The neural network suffers with the vanishing gradient problem.Which of the following statements is true?A. Hidden layer 1 corresponds to D, Hidden layer 2 corresponds to C, Hidden layer 3 corresponds to B and Hidden layer 4 corresponds to AB. Hidden layer 1 corresponds to A, Hidden layer 2 corresponds to B, Hidden layer 3 corresponds to C and Hidden layer 4 corresponds to DSolution: (A)This is a description of a vanishing gradient problem. As the backprop algorithm goes to starting layers, learning decreases.Q37.For a classification task, instead of random weight initializations in a neural network, we set all the weights to zero. Which of the following statements is true?A. There will not be any problem and the neural network will train properlyB. The neural network will train but all the neurons will end up recognizing the same thingC. The neural network will not train as there is no net gradient changeD. None of theseSolution: (B)Option B is correct.Q38. There is a plateau at the start. This is happening because the neural network gets stuck at local minima before going on to global minima.To avoid this, which of the following strategy should work?A. Increase the number of parameters, as the network would not get stuck at local minimaB. Decrease the learning rate by 10 times at the start and then use momentumC. Jitter the learning rate, i.e. change the learning rate for a few epochsD. None of theseSolution: (C)Option C can be used to take a neural network out of local minima in which it is stuck.Q39.For an image recognition problem (recognizing a cat in a photo), which architecture of neural network would be better suited to solve the problem?A. Multi Layer PerceptronB. Convolutional Neural NetworkC. Recurrent Neural networkD. PerceptronSolution: (B)Convolutional Neural Network would be better suited for image related problems because of its inherent nature for taking into account changes in nearby locations of an imageQ40.Suppose while training, you encounter this issue. The error suddenly increases after a couple of iterations.You determine that there must a problem with the data. You plot the data and find the insight that, original data is somewhat skewed and that may be causing the problem.What will you do to deal with this challenge?A. NormalizeB. Apply PCA and then NormalizeC. Take Log Transform of the dataD. None of theseSolution: (B)First you would remove the correlations of the data and then zero center it.Q41. Which of the following is a decision boundary of Neural Network?A) BB) AC) DD) CE) All of theseSolution: (E)A neural network is said to be a universal function approximator, so it can theoretically represent any decision boundary.Q42. In the graph below, we observe that the error has many ups and downsShould we be worried?A. Yes, because this means there is a problem with the learning rate of neural network.B. No, as long as there is a cumulative decrease in both training and validation error, we dont need to worry.Solution: (B)Option B is correct. In order to decrease these ups and downs try to increase the batch size.Q43.What are the factors to select the depth of neural network?A. 1, 2, 4, 5B. 2, 3, 4, 5C. 1, 3, 4, 5D. All of theseSolution: (D)All of the above factors are important to select the depth of neural networkQ44.Consider the scenario. The problem you are trying to solve has a small amount of data. Fortunately, you have a pre-trained neural network that was trained on a similar problem. Which of the following methodologies would you choose to make use of this pre-trained network?A. Re-train the model for the new datasetB. Assess on every layer how the model performs and only select a few of themC. Fine tune the last couple of layers onlyD. Freeze all the layers except the last, re-train the last layerSolution: (D)If the dataset is mostly similar, the best method would be to train only the last layer, as previous all layers work as feature extractors.Q45.Increase in size of a convolutional kernel would necessarily increase the performance of a convolutional network.A. TrueB. FalseSolution: (B)Increasing kernel size would not necessarily increase performance. This depends heavily on the dataset.I hope you enjoyed taking the test and you found the solutions helpful. The test focused onconceptual knowledge ofDeep Learning.We tried to clear all your doubts through this article but if we have missed out on something then let me know in comments below. If you have any suggestions or improvements you think we should make in the next skilltest, let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/
Infographic  Learning Plan 2017 for beginners in data science,Learn everything about Analytics|Introduction|Access the complete plan here,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|45 Questions to test a data scientist on basics of Deep Learning (along with solution)|Infographic  Learning Plan 2017 for Transitioners in data science|
Kunal Jain
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Through this plan, we aim toremove the confusion in learning data science for beginners. The biggest challenge which beginners face while learning data science is notdearthof learning material  but too much of it. As a beginner, you are not sure where to start learning, what to practice, how much time to spend on a concept, where to get the useful resources etc. For most of the beginners, this becomes overwhelming and they simply drop out before even learning a single skill.This plan takes this confusion out. The path contains both theoretical resources as well practical examples. We have also provided you withresources / teststo applyyour learning and benchmark yourself. As part of this plan, you will apply the concepts you learn on real-world problems and gain hands-on experience.This learning plan is extremely useful to anyone who wants to learn machine learning, deep learning and data science. For the people who have been looking for a comprehensive action plan to help them sort the course of learning this is the best resource, you can lay your hands on. To download the learning plan click here.",https://www.analyticsvidhya.com/blog/2017/01/learning-plan-2017-beginners-data-science/
Infographic  Learning Plan 2017 for Transitioners in data science,"Learn everything about Analytics|Introduction|Complete Plan|Learn, compete, hack and get hired!","Share this:|Like this:|Related Articles|Infographic  Learning Plan 2017 for beginners in data science|Infographic  Learning Plan 2017 for Intermediates in data science|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This plan is for people planning a careershift in analytics and data science this year. Enteringa new field can be overwhelming. What to learn? How to gain the experience required for applying to right jobs? Would you need to give up on your salary? Questions like this will daunt and haunt you for months. This plan aims to resolve this confusion by providing the right resources at your disposal.The plan containstheoretical as well as practical examples.We have also provided you withresources / teststo applyyour learning and benchmark yourself. As part of this plan, you will apply the concepts you learn on real-world problems and gain hands-on experience.To download the infographic, click here.",https://www.analyticsvidhya.com/blog/2017/01/transitioners-data-science-plan-for-2017/
Infographic  Learning Plan 2017 for Intermediates in data science,Learn everything about Analytics|Introduction|Access the complete plan here,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Infographic  Learning Plan 2017 for Transitioners in data science|Introduction to Structuring Customer complaints explained with examples|
Kunal Jain
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We believe, learning should never stop. This plan is for people with basic knowledge of machine learning or deep learning. You can advance your learning this year using this plan.Depending on your skills and learning agendafor the year, you can choose the area you to learn. The plan starts with various skill assessment and makes sure that you are on top of data science domain by end of the year. The plan contains both theoretical as well practical examples. We have also provided you withresources / teststo applyyour learning and benchmark yourself.This learning plan should provide a structured pathto anyone who wants to learn advanced concepts of machine learning, deep learning and data science. To download the learning plan click here.",https://www.analyticsvidhya.com/blog/2017/01/learning-plan-2017-intermediates-data-science/
Introduction to Structuring Customer complaints explained with examples,Learn everything about Analytics|Introduction|Proposed Method,"Input|Extraction of Features|Summarization|Problem Definition|Summary Features|Computation|Summary Generation|Post Processing|End Notes|Share this:|Like this:|Related Articles|Infographic  Learning Plan 2017 for Intermediates in data science|21 Steps to Get Started with Apache Spark using Scala|
Yogesh Kulkarni
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In past, if you were not particularly happy with a service or a product, you would go to the service provider or the shop and lodge a complaint. With services-businesses going online and due to enormous scale, lodging complaints in-person may not be always possible. Electronic ways such as emails, social media and particularly websites like www.consumercomplaints.in focusing on such issues, are widely used platforms to vent out the anger as well as publicizing the issue in expectancy of quick actions.Keeping a close watch on complaints on such sites has become imperative for businesses such as banks. This article looks at ways to structure these unstructured complaints in an actionable form.In a typical case, a bank may be interested in looking at classification of the complaints into various categories such as Loans, Fixed Deposits, Credit Cards, etc. so that they can be forwarded to respective departments. An important feature would be to summarize long complaints so that further actions can be formulated quickly. Sentiment analysis of such complaints is typically not very useful as most of them would be highly negative anyways. This article proposes a way to classify and summarize customer complaints seen on consumer complaints website.The Natural Language Programming pipeline is utilized to structure the text with stages such as:Following sections describe all these stages, with more elaboration of one the core process of feature extraction, the summarization.Various tools libraries can be used to scrape reviews from websites. Python has libraries such as requests, BeautifulSoup to take care of these tasks. Useful tutorials can be found at Tutorial1 and Tutorial2. Output of this stage is a set of text files with one complaint in each. A sample complaint looks as follows (some of the text has been masked, for the sake of confidentiality):XXXXApart from core text of the complaint, useful features are:Subject being the first line of the complaint is easy to extract. It forms a one line gist of the issue.Other features such as Number of reviews, comments, etc. are typically in a fixed format and can be extracted by regular expressions. Tutorials like this can be used to extract them.Extraction of known categories such as Loans, Fixed Deposits, Credit Cards can also be done using matching pre-defined keywords by regular expressions.Customer complaints could be very long and with such large volume, it is manually impossible to read through all of them. Effective summarization is a way of compressing the text into few meaningful lines.Following section elaborates one of the ways of summarization for customer complaints.Text summaries can be Abstractive or Extractive. In Abstractive, the summary is constructed by employing words and phrases which are (typically) NOT in the original text, whereas in Extractive, few of the highly representative sentences are picked from the original text and ordered to form the summary. The proposed method is of Extractive type.Given a document D, having sentences (?0,?1, , ??) return Y, which is a set of K important statements from D. Thus, Extractive text summarization is a binary classification model, where, out of n sentences, K sentences are labelled as True (meaning, they are part of the summary) or False if otherwise. So, the problem boils down the determining if a sentence (??) is labelled as True or False.Decision of labeling depends on various factors, called as Summary features. In the overall process, for each statement, these features are computed. Their weighted sum is ranked. Top K ranked sentences are chosen as set Y, representing the summary.In the current method, following features are incorporated to arrive at the rank of a sentence:2. Length: Number of words in the sentence can dictate the importance of it. Shorter ones are less important as they may not represent the gist of the whole text.3. Position: Sentences occurring initially and towards end carry more meaning than the middle ones. The first sentence is of utmost importance.4. Proper Nouns: The sentences which contain Named Entity called Proper Nouns (NNP) are important ones as they contain names of the places, persons, etc.5. Cue Words: Domain specific words such as Undelivered, Fraud, etc. suggest important sentences. So, sentences having more such words are given more weightage.6. Topic Words: Topic words are arrived as central words of the whole text. It could be words such as Debit, Loan, etc. Sentences aligned more with them are central to the text and thus are more eligible to be part of the summary.Each feature values are normalized to lie in range 0 to 1.Rank is computed for each sentence as weighted sum of the features. Values of the weights can either be derived empirically of by employing Machine/Deep Learning algorithms such as NaveBayes, Logistic Regression, Support Vector Machine, etc. The current method computes Rank as:A data frame is populated with the summary features as below:The data frame is then sorted based on the Rank and fed for summary generation.While collecting K top ranked sentences, care is taken that sentences similar to already selected sentences are not added to the set Y. This avoids getting almost duplicate sentences in the summary. Similarity measure used in the method is based on TF-ISF and similarity as shown below.And the resultant 3-line (K = 3) summary (Y) is:Each complaint is ready with features such as its Summary and classification category for identifying the department to which it can be forwarded. The concerned department can sort complaints based on the amount involved, number of reviews and/or comments and start addressing the issues by reading the summary. If more details are needed, then original complaints can be considered.The current article presents an automatic summarization method. It extracts features from sentences and picks top ranked sentences as the summary. Features such as Cue Words give flexibility for customization specific to the given domain. Topic words used are the centroids of the clusters of words. Sentences having such central words form the gist of the original text. Overall rank of the sentence captures the effect of all these features in relative importance. The proposed method can be further developed to incorporate additional features. Use of machine/deep learning algorithms can derive more accurate weights used for ranking the sentences.This article was contributed by Yogesh H.Kulkarni who is the second rank holder of Blogathon 2. Stay Tuned to read rest of the articles.",https://www.analyticsvidhya.com/blog/2017/01/introduction-to-structuring-customer-complaints/
21 Steps to Get Started with Apache Spark using Scala,"Learn everything about Analytics|Introduction|Table of Contents|1. What is Scala|2. About Scala|3. Installing Scala|4. Prerequisites for Learning Scala|5. Choosing a development environment|6. Scala Basics Terms|7. Things to note about Scala|8. Variable declaration in Scala|9. Operations on variables|10. The if-else expression in Scala|11. Iteration in Scala|12. Declare a simple function in Scala and call it by passing value|13. Few Data Structures in Scala|14. Writing & Running a program in Scala using an editor|15. Advantages of using Scala for Apache Spark|16. Comparing Scala, Java, Python and R APIs in Apache Spark||17. Install Apache Spark & some basic concepts about Apache Spark|18. Working with RDD in Apache Spark using Scala|19.Working with DataFrame in Apache Spark using Scala|20. Building a machine learning model|21.Additional Resources","2.1 Scala is pure Object-Oriented programming language|2.2 Scala is a functional language|2.3 Scala is a compiler based language (and not interpreted)|2.4 Companies using Scala|8.1 Declare using var|8.2 Declare using val|13.1 Arrays in Scala|13.2 List in Scala|14.1 Compile a Scala Program|14.2 Running Scala Program|18.1 Creating a RDD from existing source|18.2 Creating a RDD from External sources|18.3 Transformations and Actions on RDD|19.1 Name of columns|19.2 Number of observations|19.3 Print the columns datatype|19.4 Show first n rows|19.5 Subsetting or select columns|19.6 Filter rows|19.7 Group DataFrame|19.8 Apply SQL queries on DataFrame|20.1 Applying Linear Regression on train|End Notes|Learn, compete, hack and get hired!|Share this:|Related Articles|Introduction to Structuring Customer complaints explained with examples|SAS Admin- Mumbai, Pune, Bangalore (5-6 years of experience)|
Ankit Gupta
|41 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Apply + operator|Declaring Array in Scala|Accessing an array|Declaring List in Scala|Accessing a list|18.3.1. Map Transformations|18.3.2 Count Action|18.3.3 Reduce Action|18.3.4 flatMap transformation and reduceByKey Action|18.3.5filter Transformation,Declare an array by assigning it some values|Declaring an array using new keywords|ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you ask any industry expert what language should you learn for big data, they would definitely suggest you to start with Scala. Scala has gained a lot ofrecognition for itself and is used by a large number of companies. Scala and Spark are being used at Facebook, Pinterest, NetFlix, Conviva, TripAdvisor for Big Data and Machine Learning applications.Still not convinced? Look at this trend of number of job postings for Scala on Indeed.com:But learning a new language can be intimidating. To help you learn Scala from scratch, I have created this comprehensive guide. The guide is aimed at beginners and enables you to write simple codes in Apache Spark using Scala. I have kept the contentsimple to get you started.By the end of this guide, you will have a thorough understanding of working with Apache Spark in Scala. Read on to learn one more language and add more skills to your resume.This guide is broadly divided into 2 parts. The first part is from section 1 to 14 where we discuss language Scala. Section 15 onwards is how we used Scala in Apache Spark.Scala is an acronym for Scalable Language. It is a general-purpose programming language designed for the programmers who want to write programs in a concise, elegant, and type-safe way. Scala enables programmers to be more productive. Scala is developed as an object-oriented and functional programming language.If you write a code in Scala, you will see that the style is similar to a scripting language. Even though Scala is a new language, it has gained enough users and has a wide community support. It is one of the most user-friendly languages.The design of Scala started in 2001 in the programming methods laboratory at EPFL (cole Polytechnique Fdrale de Lausanne). Scala made its first public appearance in January 2004 on the JVM platform and a few months later in June 2004, it was released on the .(dot)NET platform. The .(dot)NET support of Scala was officially dropped in 2012. A few more characteristics of Scala are:Scala is anobject-oriented programming language. Everything in Scala is an object and any operations you perform is a method call. Scala, allow you to add new operations to existing classes with the help of implicit classes.One of the advantages of Scala is that it makes it very easy to interact with Java code. You can also write a Java code inside Scala class. The Scala supports advanced component architectures through classes and traits.Scala is a programming language that has implemented major functional programming concepts. In Functional programming, every computation is treated as a mathematical function which avoids states and mutable data. The functional programming exhibits following characteristics:Scalais not a pure functional language. Haskell is an example of a pure functional language.If you want to read more about functional programming, please refer to this article.Scala is a compiler based language which makes Scala execution very fast if you compare it with Python (which is an interpreted language). The compiler in Scala works in similar fashion as Java compiler. It gets the source code and generates Java byte-code that can be executed independently on any standard JVM (Java Virtual Machine). If you want to know more about thedifference between complied vs interpreted language please refer this article.There are more important points about Scala which I have not covered. Some of them are:Scala is now big name. It is used by many companies to develop the commercial software. These are the following notable big companies which are using Scala as a programming alternative.If you want to read more about how and when these companies started using Scala please refer this blog.Scala can be installed in any Unix or windows based system. Below are the steps to install for Ubuntu (14.04) for scala version 2.11.7. I am showing the steps for installing Scala (2.11.7) with Java version 7. It is necessary to install Java before installing Scala. You can also install latest version of Scala(2.12.1) as well.Step 0: Open the terminalStep 1: Install JavaIf you are asked to accept Java license terms, click onYes and proceed. Once finished, let us check whether Java has installed successfully or not. To check the Java version and installation, you can type:Step 2: Once Java is installed, we need to install ScalaThis will show you the version of Scala installedScala being an easy to learn language has minimal prerequisites. If you are someone with basic knowledge of C/C++, then you will be easily able to get started with Scala. Since Scala is developed on top of Java. Basic programming function in Scala is similar to Java. So, if you have some basic knowledge of Java syntax and OOPs concept, it would be helpful for you to work in Scala.Once you have installed Scala, there are various options for choosing an environment. Here are the 3 most common options:Choosing right environment depends on your preference and use case. I personally prefer writing a program on shell because it provides alot of good features like suggestions for method call and you can also run your code while writing line by line.Warming up: Running your first Scala program in Shell:
Lets write a first program which adds two numbers.Object: An entity that has state and behavior is known as an object. For example: table, person, car etc.Class: A class can be defined as a blueprint or a template for creating different objects which defines its properties and behavior.Method: It is a behavior of a class. A class can contain one or more than one method. For example:deposit can be considered a method of bank class.Closure: Closure is any function that closes over the environment in which its defined. A closure returns value depends on the value of one or more variables which is declared outside this closure.Traits: Traits are used to define object types by specifying the signature of the supported methods. It is like interface in java.In Scala, you can declare a variable using var or val keyword. The decision is based on whether it is a constant or a variable. If you use var keyword, you define a variable as mutable variable. On the other hand, if youuse val, you define it as immutable.Lets first declare a variable using var and then using val.In the above Scala statement, you declare a mutable variable called Var1 which takes a string value. You can also write the above statementwithout specifying the type of variable. Scala will automatically identify it. For example:In the above Scala statement, we have declared an immutable variable Var2 which takes a string Ankit. Try it for without specifying thetype of variable.If you want to read about mutable and immutable please refer this link.You can perform various operations on variables. There are various kinds of operators defined in Scala. For example: Arithmetic Operators, Relational Operators, Logical Operators, Bitwise Operators, Assignment Operators.Lets see + , ==operators on two variables Var4, Var5. But, before that, let us first assign values toVar4 and Var5.Now, let us apply some operations using operators in Scala.Apply == operatorIf you want to know complete list of operators in Scala refer this link:In Scala, if-else expression is used for conditional statements. You can write one or more conditions insideif. Lets declare a variable called Var3 with a value 1 and then compare Var3 using if-else expression.In the above snippet, the condition evaluates to True and hence True will be printed in the output.Like most languages, Scala also has a FOR-loop which is the most widely used method for iteration. It has a simple syntax too.Scala also supports while and do while loops. If you want to know how both work, please refer this link.You can define afunction in Scala using def keyword. Lets define a function called mul2 which will take a number and multiply it by 10. You need to define the return type of function, if a function not returning any value you should use the Unit keyword.In the below example, the function returns aninteger value.Lets define the function mul2:Now lets pass a value 2 into mul2If you want to read more about the function, please refer this tutorial.In Scala, an array is a collection of similar elements. Itcan contain duplicates. Arrays are also immutable in nature. Further, you can access elements of an array using an index:To declare any array in Scala, you can define it either using anew keyword or you can directly assign some values to an array.In the above program, we have defined an array called name with 5 string values.The following is the syntax for declaring an array variable using anew keyword.Hereyou have declared an array of Strings called name that can hold up to three elements. You can also assign values to name by using anindex.Lets printcontents of name array.You can access the element of an array by index. Lets access the first element of array name. By giving index 0. Index in Scala starts from 0.Lists are one of the most versatile data structure in Scala. Lists contain items of different types in Python, but in Scala the items all have the same type. Scala lists are immutable.Here is a quick example to define a list and then access it.You can define list simply by comma separated values inside the List method.You can also define multi dimensional list in Scala. Lets define a two dimensional list:Lets get the third element of the list numbers . The index should 2 because index in Scala start from 0.We have discussed two of the most used data Structures. You can learn more from this link.Let us start witha Hello World! program. It is a good simple way to understandhow to write, compile and run codes in Scala. No prizes for telling the outcome of this code!As mentioned before, if you are familiar with Java, it will be easier for you to understand Scala. If you know Java, youcan easily see that the structure of above HelloWorld program is very similar to Java program.This program contains a method main (not returning any value) which takes an argument  astring array through command line. Next, it calls a predefined method called Println and passes the argument Hello, world!.You can define the main method as static in Java but in Scala, the static methodis no longer available. Scala programmer cant use static methods because they use singleton objects. To read more about singleton object you can refer this article.To run any Scala program, you first need to compile it. Scalac is the compiler which takes source program as an argument and generates object files as output.Lets start compiling your HelloWorld program using the following steps:1. For compiling it, you first need to paste this program into a text file then you need to save this program as HelloWorld.scala
2. Now you need change your working directory to the directory where your program is saved
3. After changing the directory you can compile the program by issuing the command.4. After compiling, you will get Helloworld.class as an output in thesame directory. If you can see the file, you have successfully compiled the above program.After compiling, youcan now run the program using followingcommand:You will get an output if the above command runs successfully. The program will print Hello, world!If you are working with Apache Spark then you would know that it has 4 different APIs support for different languages: Scala, Java, Python and R.Each of these languages have their own unique advantages. But using Scala is more advantageous than other languages. These are the following reasons why Scala is taking over big data world.Lets compare 4 major languages which are supported by Apache Spark API.MetricsScalaJavaPythonRTypeCompiledCompiledInterpretedInterpretedJVM basedYesYesNoNoVerbosityLessMoreLessLessCode LengthLessMoreLessLessProductivityHighLessHighHighScalabilityHighHighLessLessOOPS SupportYesYesYesYesTo know the basics of Apache Spark and installation, please refer to my first article on Pyspark. I have introduced basic terminologies used in Apache Spark likebig data, cluster computing, driver, worker, spark context, In-memory computation, lazy evaluation, DAG, memory hierarchy and Apache Spark architecture in the previous article.As a quick refresher, I will be explaining some of the topics which are very useful to proceed further. If you are a beginner, then I strongly recommend you to go through my first article beforeproceeding further.Spark has three data representations viz RDD, Dataframe, Dataset. To use Apache Spark functionality, we must use one of them for data manipulation. Lets discusseach of them briefly:First step to use RDD functionality is to create a RDD. In Apache Spark, RDD can be created by two different ways. One is from existing Source and second is from anexternal source.So before moving further lets open the Apache Spark Shell with Scala. Type the following command after switching into the home directory of Spark. It will also load the spark context as sc.After typing above command you can start programming of Apache Spark in Scala.When you want to create a RDD from existing storage in driver program (which we would like to be parallelized). For example, converting an array to RDD, which is already created in a driver program.In the above program, I first created an array for 10 elements and then I created a distributed data called RDD from that array using parallelize method. SparkContext has aparallelize method, which is used for creating the Spark RDD from an iterable already present in driver program.To see thecontent of any RDD we can use collect method. Lets see the content of distData.You can create a RDD through external sources such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format. So lets create a RDD from the text file:The name of thetext file is text.txt. and it has only 4 lines given below.
I love solving data mining problems.
I dont like solving data mining problems.
I love solving data science problems.
I dont like solving data science problems.Lets create the RDD by loading it.Now lets see first two lines in it.The output is received is as below:A map transformation is useful when we need to transform a RDD by applying a function to each element. So how can we use map transformation on rdd in our case?
Lets calculate the length (number of characters) of each line in text.txtAfter applying above map operation, we get the following output:Lets count the number of lines in RDD lines.The above action on lines1 will give 4 asthe output.Lets take the sumof total number of characters in text.txt.Lets calculate frequency of each word in text.txtLets filter out the words in text.txt whoselength is more than 5.A DataFrame in Apache Spark can be created in multiple ways:Lets create a DataFrame using acsv file and perform some analysis on that.For reading a csv file in Apache Spark, we need to specify a new library in our Scala shell. To perform this action, first, we need to download Spark-csv package (Latest version) and extract this package into the home directory of Spark. Then, we need to open a PySpark shell and include the package ( I am using spark-csv_2.10:1.3.0).Now lets load the csv file into a DataFrame df. You can download the file(train) from this link.Lets see thename of columns in df by using columns method.To see thenumber of observation in df you can apply count method.You can use printSchema method on df. Lets print the schema of df.You can use show method on DataFrame. Lets print the first 2 rows of df.To select columns you can use select method. Lets apply select on df for Age columns.To filter the rows you can use filter method. Lets apply filter on Purchase column of df and get the purchase which is greater than 10000.To groupby columns, you can use groupBy method on DataFrame. Lets see the distribution on Age columns in df.To apply queries on DataFrame You need to register DataFrame(df) as table. Lets first register df as temporary table called (B_friday).Now you can apply SQL queries on B_friday table using sqlContext.sql. Lets select columns Age from the B_friday using SQL statement.If you have come this far, you are in for a treat! Ill complete this tutorial by building a machine learning model.I will use only three dependent features and the independent variable in df1. Lets create a DataFrame df1 which has only 4 columns (3 dependent and 1 target).In above DataFrame df1 User_ID,Occupation and Marital_Status are features and Purchase is target column.Lets try to create a formula for Machine learning model like we do in R. First, we need to import RFormula. Then we need to specify the dependent and independent column inside this formula. We also have to specify the names for features column and label column.After creating the formula, we need to fit this formula on df1 and transform df1 through this formula. Lets fit this formula.After applying the formula we can see that train dataset has 2 extra columns called features and label. These are the ones we have specified in the formula (featuresCol=features and labelCol=label)After applying the RFormula and transforming the DataFrame, we now need to develop the machine learning model on this data. I want to apply a Linear Regression for this task. Let us import a Linear regression and apply on train. Before fitting the model, I am setting the hyperparameters.You can also makepredictions on unseen data. But I am not showing this here. Lets print the coefficient and intercept for linear regression.Lets summarize the model over the training set and print out some metrics.Now, lets see RMSE on train.Lets repeat above procedure for taking the prediction on cross-validation set. Lets read the train dataset again.Now, randomly divide the train in two part train_cv and test_cvNow, Transform train_cv and test_cv using RFormula.After transforming using RFormula, wecan builda machine learning model and take the predictions. Lets apply Linear Regression on training and testing data.In train_cv_pred and test_cv_pred, you will find anew column for prediction.In this article, I have provided a practical hands on guide for Scala. I introduced you to write basic programs using Scala, some important points about Scala and how companies are using Scala.I thenrefreshed some of the basic concepts of Apache Spark which I have already covered in my PySpark article and built a machine learning model in Apache Spark using Scala. If you have any questions or doubts, feel free to post them in the comments section.",https://www.analyticsvidhya.com/blog/2017/01/scala/
"SAS Admin- Mumbai, Pune, Bangalore (5-6 years of experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|21 Steps to Get Started with Apache Spark using Scala|Comprehensive Guide on t-SNE algorithm with implementation in R & Python|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  6 years
Requirements : 
Task Info : Description:Experience inSASAdministration andSASprogramming (SASEnterprise Guide, Enterprise Miner and DI studio).Experience administeringSASplatform in a multi-node GRID environment.Experience creating and maintaining security model inSAS.Experience troubleshooting GRID problems.Experience monitoring grid and make sure that nodes are balanced withSASgenerated code running efficiently across grid.Experience reading throughSASlogs to work through problems finding root causes and rectifying.Experience with alternative authentication providers such as Active Directory.Analyze compatibility ofSASproducts with the hardware/software.Must have experience in Unix/Linux platform, includes Unix scripting and job scheduling.Familiarity withSASAccess Engines, and setting up connections to Oracle and Microsoft SQL server.Installation, Upgrade, maintenance and support of allSAScomponents including GRID.Security Management  Administration ofSASUsers, Groups.Setup and Maintain Disaster Recovery services forSAS.Respond to and resolve tickets on technical issues related toSASPlatform within defined SLAs.Co-ordinate withSASSupport for any unresolved issues and tool specific issues or hot fixes/patches.Responsible for Monitoring, reporting, and controllingSASserver usage.Creation and maintenance of system documentation related to theSASenvironment.Ability to prioritize and work on multiple tasks simultaneously.Strong communication and organizational skills.Ability to multi-task, prioritize and execute on assigned deliverables.
College Preference : no-bar
Min Qualification : ug
Skills : linux, oracle, sas, SAS administration, SAS DI studio, SAS enterprise guide, SAS enterprise miner, sql server, unix
Location : Bengaluru, Mumbai, Pune
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/sas-admin-mumbai-pune-bangalore-5-6-years-of-experience/
Comprehensive Guide on t-SNE algorithm with implementation in R & Python,Learn everything about Analytics|Introduction|Table of Content|1. What is t-SNE?|2. What is dimensionality reduction?|3. How does t-SNE fit in the dimensionality reduction algorithm space?|4. Algorithmic details of t-SNE (optional read)|5. What does t-SNE actually do?|6. Use cases|7. t-SNE compared to other dimensionality reduction algorithms|8. Example Implementations|9. Where and When to use t-SNE?|10. Common Fallacies,"|What about PCA?|Limitations of PCA|4.1 Algorithm|4.2 Time and Space Complexity|6.1 Facial Expression Recognition|6.2 Identifying Tumor subpopulations (Medical Imaging)|6.3 Text comparison using wordvec|1. In R|2. In Python|Reference|End Notes||Learn, compete, hack and get hired|Share this:|Like this:|Related Articles|SAS Admin- Mumbai, Pune, Bangalore (5-6 years of experience)|MyStory: How I became a Data Science Hacker from being a Delivery Head|
Analytics Vidhya Content Team
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 1|Step 2|Step 3|Step 4|Hyper parameter tuning|Code|Implementation Time|Interpreting Results|Hyper parameter tuning|Code|Implementation Time|9.1 Data Scientist|9.2 Machine Learning Hacker|9.3 Data Science Enthusiasts,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Imagine you get a dataset with hundreds of features (variables) and have little understanding aboutthe domain the data belongs to. You are expected toidentify hidden patterns in the data, explore and analyze the dataset. And not just that, you have to find out if there is a pattern in the data  is it signal or is it just noise?Does that thought make you uncomfortable? It made my hands sweat when I came across this situation for the first time.Do you wonderhow to explore a multidimensional dataset? It is one of the frequently asked question by many data scientists. In this article, I will take you through a very powerful way to exactly do this.By now, some of you would be screamingIll use PCA for dimensionality reduction and visualization. Well, you are right! PCA is definitely a good choice for dimensionality reduction and visualization for datasets with alarge number of features. But, what if you could use something more advanced than PCA? (If you dont know PCA, I would strongly recommend to read this article first)What if you could easily search for a pattern in non-linear style? In this article, I will tell you about a new algorithmcalled t-SNE (2008), which is much more effective than PCA (1933).I will take you through the basics of t-SNE algorithm first and then will walk you through why t-SNE is a good fit for dimensionality reduction algorithms.You will also, get hands-on knowledge for using t-SNE in both R and Python.Read on!(t-SNE) t-Distributed Stochastic Neighbor Embeddingis a non-linear dimensionality reduction algorithm used for exploring high-dimensionaldata. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.In order to understand how t-SNE works, lets first understand what is dimensionality reduction?Well, in simple terms, dimensionality reduction is the technique of representing multi-dimensional data (data with multiple features having acorrelation with each other) in 2 or 3 dimensions. Some of you might question why do we need Dimensionality Reduction when we can plot the data using scatter plots, histograms& boxplots and make sense of the pattern in data using descriptive statistics.Well, even if you can understand the patterns in data and present it on simple charts, it is still difficult for anyone without statistics background to make sense of it. Also, if you have hundreds of features, you have to study thousands of charts before you can make sense of this data. (Read more about dimensionality reduction here)With the help of dimensionality reduction algorithm, you will be able to present the data explicitly. Now that you have an understanding of what is dimensionality reduction, lets look at how wecan use t-SNE algorithm for reducing dimensions.Following are a few dimensionality reduction algorithms that you can check out:The good news is that you need to study only two of the algorithms mentioned above to effectively visualize data in lower dimensions  PCA and t-SNE.PCA is a linear algorithm. It will not be able to interpret complex polynomial relationship between features. On the other hand, t-SNE is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. A major problem with, linear dimensionality reduction algorithms is that they concentrate on placing dissimilar data points far apart in a lower dimension representation. But in order to represent high dimension data on low dimension, non-linear manifold, it is important that similar datapoints must be represented close together, which is not what linear dimensionality reduction algorithms do.Now, you have a brief understanding of what PCA endeavorsto do.Local approaches seek to map nearby points on the manifold to nearby points in the low-dimensional representation. Global approaches on the other hand attempt to preserve geometry at all scales, i.e mapping nearby points to nearby points and far away points to far away points It is important to know that most of the nonlinear techniques other than t-SNE are not capable of retaining both the local and global structure of the data at the same time.This section is for the people interested in understanding the algorithm in depth. You can safely skip this section if you do not want to go through the math in detail.Lets understand why you should know about t-SNE and the algorithmic details of t-SNE. t-SNE is an improvement on the Stochastic Neighbor Embedding (SNE) algorithm.Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities. The similarity of datapoint to datapoint is the conditional probability, , wouldpickas its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at  .For nearby datapoints, is relatively high, whereas for widely separated datapoints, will be almost infinitesimal (for reasonable values of the variance of the Gaussian, ). Mathematically, the conditional probability is given bywhere is the variance of the Gaussian that is centered on datapoint If you are not interested in the math, think about it in this way, the algorithm starts by converting the shortest distance (a straight line) between the points into probability of similarity of points. Where, thesimilarity between points is: the conditional probability that  would pickas its neighborif neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at .For the low-dimensional counterparts and of the high-dimensional datapoints and it is possible to compute a similar conditional probability, which we denote by.Note that, pi|i and pj|j are set to zero as we only want to model pair wise similarity.In simple terms step 1 and step2 calculate the conditional probability of similarity between a pair of points in For the sake of simplicity, try to understand this in detail.Let us map 3D space to 2D space. What step1 and step2 are doing is calculating the probability of similarity of points in 3D space and calculating the probability of similarity of points in the corresponding 2D space. Logically, the conditional probabilities and must be equal for aperfect representation of the similarity of the datapoints in the different dimensional spaces, i.e the difference between and must be zero for the perfect replication of the plot in high and low dimensions.By this logic SNE attempts to minimize this difference of conditional probability.Now here is the difference between the SNE and t-SNE algorithms.To measure the minimization of sum of difference of conditional probability SNE minimizes the sum of Kullback-Leibler divergences overall data points using a gradient descent method. We must know that KL divergences are asymmetric in nature. In other words, the SNE cost function focuses on retaining the local structure of the data in the map (for reasonable values of the variance of the Gaussian in the high-dimensional space,). Additionally, it is very difficult (computationally inefficient) to optimize this cost function.So t-SNE also tries to minimize the sum of thedifference in conditional probabilities. But it does that by using the symmetric version of the SNE cost function, with simple gradients. Also, t-SNE employs a heavy-tailed distribution in the low-dimensional space to alleviate both the crowding problem (the area of the two-dimensional map that is available to accommodate moderately distant data points will not be nearly large enough compared with the area available to accommodate nearby data points) and the optimization problems of SNE.If we see the equation to calculate the conditional probability, we have left out the variance from the discussion as of now. The remaining parameter to be selected is the variance of the students t-distribution that is centered over each high-dimensional datapoint . It is not likely that there is a single value ofthat is optimal for all data points in the data set because the density of the data is likely to vary. In dense regions, a smaller value ofis usually more appropriate than in sparser regions. Any particular value ofinduces a probability distribution, , over all of the other data points. This distribution has anThis distribution has anentropy which increases asincreases. t-SNE performs a binary search for the value ofthat produces a with a fixed perplexity that is specified bythe user. The perplexity is defined aswhere H() is the Shannon entropy of measured in bitsThe perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.The minimization of the cost function is performed using gradient decent. And physically, the gradient may be interpreted as the resultant force created by a set of springs between the map point and all other map points . All springs exert a force along the direction ( ). The spring between and repels or attracts the map points depending on whether the distance between the two in the map is too small or too large to represent the similarities between the two high-dimensional datapoints. The force exerted by the spring between and is proportional to its length, and also proportional to its stiffness, which is the mismatch (pj|i  qj|i + p i| j  q i| j ) between the pairwise similarities of the data points and the map points[1].-Now that we have understood the algorithm, it is time to analyze its performance. As you might have observed, that the algorithm computes pairwise conditional probabilities and tries to minimize the sum of the difference of the probabilities in higher and lower dimensions. This involves a lot of calculations and computations. So the algorithm is quite heavy on the system resources. t-SNE has a quadratic time and space complexity in the number of data points. This makes it particularly slow and resource draining while applying it to data sets comprising of more than 10,000 observations.After we have looked into the mathematical description of how does the algorithms works, to sum up,what we have learned above. Here is a brief explanation of how t-SNE works. Its quite simple actually, t-SNE a non-linear dimensionality reduction algorithm finds patterns in the data by identifying observed clusters based on similarity of data points with multiple features. But it is not a clustering algorithm it is a dimensionality reduction algorithm. This is because it maps the multi-dimensional data to a lower dimensional space, the input features are no longer identifiable. Thus you cannot make any inference based only on the output of t-SNE. So essentially it is mainly a data exploration and visualization technique.But t-SNE can be used in the process of classification and clustering by using its output as the input feature for other classification algorithms.You may ask, what are the use cases of such an algorithm. t-SNE can be used on almost all high dimensional data sets. But it is extensively applied in Image processing, NLP, genomic data and speech processing. It has been utilized for improving the analysis of brain and heart scans. Below are a few examples:A lot of progress has been made on FER and many algorithms like PCA have been studied for FER. But, FER still remains a challenge due to the difficulties of dimension reduction and classification. t-Stochastic Neighbor Embedding (t-SNE) is used for reducing the high-dimensional data into a relatively low-dimensional subspace and then using other algorithms like AdaBoostM2, Random Forests, Logistic Regression, NNs and others as multi-classifier for the expression classification. In one such attemptfor facial recognition based on the Japanese Female Facial Expression (JAFFE) database with t-SNE and AdaBoostM2. Experimental results showed that the proposed new algorithm applied to FER gained the better performance compared with those traditional algorithms, such as PCA, LDA, LLE and SNE.[2]The flowchart for implementing such a combination on the data could be as follows:Preprocessing  normalization  t-SNE classification algorithm  PCA LDA LLE SNE t-SNESVM 73.5% 74.3% 84.7% 89.6% 90.3%AdaboostM2 75.4% 75.9% 87.7% 90.6% 94.5%Mass spectrometry imaging (MSI) is a technology that simultaneously provides the spatial distribution for hundreds of biomolecules directly from tissue. Spatially mapped t-distributed stochastic neighbor embedding (t-SNE), a nonlinear visualization of the data that is able to better resolve the biomolecular intratumor heterogeneity. In an unbiased manner, t-SNE can uncover tumor subpopulations that are statistically linked to patient survival in gastric cancer and metastasis status in primary tumors of breast cancer. Survival analysis performed on each t-SNE clusters will provide significantly useful results.[3]Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like capital city of. Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. This combination of techniques can be used to provide a birds-eye view of different text sources, including text summaries and their source material. This enables users to explore a text source like a geographical map.[4]While comparing the performance of t-SNE with other algorithms, we will compare t-SNE with other algorithms based on the achieved accuracy rather than the time and resource requirements with relation to accuracy.t-SNE outputs provide better results than PCA and other linear dimensionality reduction models. This is because a linear method such as classical scaling is not good at modeling curved manifolds. It focuses on preserving the distances between widely separated data points rather than on preserving the distances between nearby data points.The Gaussian kernel employed in the high-dimensional space by t-SNE defines a soft border between the local and global structure of the data. And for pairs of data points that are close together relative to the standard deviation of the Gaussian, the importance of modeling their separations is almost independent of the magnitudes of those separations. Moreover, t-SNE determines the local neighborhood size for each datapoint separately based on the local density of the data (by forcing each conditional probability distribution to have the same perplexity)[1]. This is because the algorithm defines a soft border between the local and global structure of the data. And unlike other non-linear dimensionality reduction algorithms, it performs better than any of them.Lets implement the t-SNE algorithm on MNIST handwritten digit database. This is one of the most explored dataset for image processing.The Rtsne package has an implementation of t-SNE in R. The Rtsne package can be installed in R using the following command typed in the R console:MNIST data can be downloaded from the MNIST website and can be converted into a csv file with small amount of code.For this example, please download the following preprocessed MNIST data. linkAs can be seen t-SNE takes considerably longer time to execute on the same sample size of data than PCA.  The plots can be used for exploratory analysis. The output x & y co-ordinates and as well as cost can be used as features in classification algorithms.An important thing to note is that the pip install tsne produces an error. Installing tsne package is not recommended. t-SNE algorithm can be accessed from sklearn package. The following code is taken from the sklearn examples on the sklearn website.  Well for the data scientist the main problem while using t-SNE is the black box type nature of the algorithm. This impedes the process of providing inferences and insights based on the results.Also, another problem with the algorithm is that it doesnt always provide asimilar output on successive runs.So then how could you use the algorithm? The best way to used the algorithm is to use it for exploratory data analysis. It will give you a very good sense of patterns hidden inside the data. It can also be used as an input parameter for other classification & clustering algorithms.Reduce the dataset to 2 or 3 dimensions and stack this with a non-linear stacker. Using a holdout set for stacking / blending. Then you can boost the t-SNE vectors using XGboost to get better results.For data science enthusiasts who are beginning to work with data science, this algorithm presents the best opportunities in terms of research and performance enhancements. There have been a few research papers attempting to improve the time complexity of the algorithm by utilizing linear functions. But an optimal solution is still required. Research papers on implementing t-SNE for avariety of NLP problems and image processing applications is an unexplored territory and has enough scope.Following are a fewcommon fallacies to avoid while interpreting the results of t-SNE:[1] L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9(Nov):2579-2605, 2008[2] Jizheng Yi et.al. Facial expression recognition Based on t-SNE and AdaBoostM2.IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber,Physical and Social Computing (2013)[3] Walid M. Abdelmoulaa et.al. Data-driven identification of prognostic tumor subpopulations using spatially mapped t-SNE of mass spectrometry imaging data.1224412249 | PNAS | October 25, 2016 | vol. 113 | no. 43[4] Hendrik Heuer. Text comparison using word vector representations and dimensionality reduction.8th EUR. CONF. ON PYTHON IN SCIENCE (EUROSCIPY 2015)I hope you enjoyed reading this article. In this article, I have tried to explore all the aspects to help you get started with t-SNE. Im sure you must be excited to explore more on t-SNE algorithm and use it at your end.Share your experience of working with t-SNE algorithm and if you think its better than PCA. If you have any doubts or questions, feel free to post it in the comments section.",https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/
MyStory: How I became a Data Science Hacker from being a Delivery Head,Learn everything about Analytics,"My Career (From 1996 till that Sunday afternoon in June 2014)|The Click that Changed My World|Crossing the Chasm  From Conceptual Knowledge to Working Code|Tale of Two Lives|My Future Plans|What motivates me to keep going?|My learnings  Distilled into the Worlds Largest Analytics Mind Map|Advice for beginners in data science|About The Author|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Comprehensive Guide on t-SNE algorithm with implementation in R & Python|Jr Data Analyst/ Data Scientist|
Guest Blog
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It was a hot Sunday afternoon in June 2014. I still remember that day and recalling that day still gives me goosebumps.I was browsing the internet looking for some interesting articles on Data Management, when a website called Coursera caught my attention. They were publicizing free online courses delivered over the internet.I had never heard of them before and was not sure what to expect fromthese free courses. Eventually, I decided to explore the course material. That one click literally changed my life.In this article, I am sharing my story with you, how I stepped out of my comfort zone and pushed myself from a successful Delivery Head into data science & machine learning.First, let me take you through my career history. After graduating from Indian Institute of Technology, Madras (B.Tech in 1994, M.Tech in 1995), I started my IT career developing ERP software. After working for 3 years, I got a chance to pursue an MBA from Indian Institute of Management, Calcutta which I completed in 2000. This is when my tryst with Data Warehousing & Business Intelligence began, first with a consulting company and then with large IT service firms leading me to the position of Assistant Vice President handling a portfolio of 40+ million dollars.The path that lay before me was also fairly clear  handle more accounts, larger teams, higher revenue targets. This could easily lead my way to the so called top management in this IT services business.But deep down, I was not happy. I was rapidly moving away from being a technologist, a subject matter expert into an excel junkie only worried about revenue, cost, attrition, recruitments & so on.Not that these problems are trivial or un-interesting, it was just that I did not want to spend the rest of my career doing that work.Back to that Sunday afternoon. Once I got into Coursera, it felt like a kid in a candy store. There were wonderful courses being taught by great professors from dream universities and to top it all  they were all free!Very soon, I registered for the introductory Machine Learning course taught by Andrew Ng and was soon doing 3 to 4 courses in parallel. I had some background in Statistics but what was being taught as Machine Learning was really fascinating. By mid-2015, I had completed 12 courses in Machine Learning & Data Science across Coursera & edX.Andrew Ngs course was followed by 9 courses in Data Science Specialization from Johns Hopkins, Bill Howes Data Science course, Analytics Edge on edX and I was becoming increasinglyconfident in connecting the dots among various data science and machine learning topics.By June 2015, with the help of MOOCs and by sacrificing my weekends, I was able to relate to data science and machine learning concepts. But the bigger challenge I was facing, I was still not very comfortable writing end to end programs.To overcome this challenge, I started looking for resources that could help. And then I discovered sites like Analytics Vidhya & Machine Learning Mastery which were focused on helping programmers write Machine Learning code.Towards the end of 2015, I was able to write programs in R and started participating in competitions on Analytics Vidhya, Kaggle, Driven Data etc.Though I was not able to make it to the top percentiles in these competitions, I was very happy working through the datasets and making my submissions. An additional advantage of participating in these Hackathons was interacting with top data scientists in these forums. I thoroughly enjoyed it and to improve further I started going through the approach & codes of experts.In early 2016, I was still in my day job that required handling teams working on Data Management & Business Intelligence projects for Banking & Financial Services customers. While I was utilizing late nights & weekends to work on Machine Learning.It was becoming unviable to lead two different lives and by mid-2016, I decided to join a start-up focused on Analytics, Data Science & Machine Learning.I feel that I have finally found my calling in life. By working on use cases for large Fortune 500 organizations, I am beginning to see the power of analytics in action. I only wish I was 10-15 years younger but as they say better late than never.I believe that we are at the cusp of a huge wave of business solutions powered by data & data science related techniques. I want to utilize my experience to formulate appropriate use cases, the solutions to which could help organizations provide better products & services. Also, India with its young workforce can become an analytics superpower by nurturing high-profile, talented data scientists. I would like to contribute by way of providing training & mentoring interested people.My motivation stems from the fact that in afuture going to be driven largely by technology, data science could be our answer to provide a better, healthy world for coming generations. In general, I am a big fan of the Open Source movement. The fact that many talented people spend their priceless hours to create fantastic software to be made available for free is a big inspiration for me to give back something to society.I have spent a considerable portion of my career working on all aspects of Data Management & Business Intelligence. Those concepts along with my recently acquired (2.5 years) data science knowledge puts me in a unique position to appreciate all aspects of data-driven decision making geared towards delivering business value.I have encapsulated this knowledge in a mindmap, which I claim to be the worlds largest analytics mindmap (1800+ nodes) accessible at this link: https://bit.ly/31KArT8. My goal is to make this mindmap as an anchor point for all Data Science practitioners to understand the broad canvas of analytics and appreciate this wonderful field in a holistic manner.These are very exciting times for people entering the analytics & data science industry. There are tons of opportunities to make a name for oneself. Having said that, the basics are not to be forgotten Develop business understanding, Be curious about technology, learn programming and more importantly, never give up!Karthikeyan Sankaran is currently a Director at LatentView Analytics which provides solutions at the intersection of Business, Technology & Math to business problems across a wide range of industries. Karthik has close to two decades of experience in the Information Technology industry having worked in multiple roles across the space of Data Management, Business Intelligence & Analytics.This story was received as part of Whats Your Story? contest on Analytics Vidhya. We found his story inspiring and hope that it inspires more and more people to this fascinating world of Analytics and Data Science.Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/01/delivery-head-to-data-science-hacker/
Jr Data Analyst/ Data Scientist,Learn everything about Analytics,"Share this:|Like this:|Related Articles|MyStory: How I became a Data Science Hacker from being a Delivery Head|Simple Beginners guide to Reinforcement Learning & its implementation|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : Excellent verbal and written skills
Self driven
Detail oriented
Team player
Task Info : The ideal candidate should be eager to join a fast-growing and successful startup with a casual work environment and a fun company culture where they will be not only be challenged but also get to immediately witness and cherish their contributions to the company.This is a very critical and hands-on role as part of the Core Product Development Team. The successful candidate will bring energy and enthusiasm to an Engineering team that takes pride in delivering high-quality software.Responsibilities:Desired skills:** Candidate has to have strong hands-on knowledge on noSQL databases(Cassandra) **Experience:1-2 Years as a Strong Application Developer for Software Products or Large Applications using Relational/NoSQL Databases.
College Preference : no-bar
Min Qualification : ug
Skills : Apache Cassandra, apache spark, Data analytics, data science, nosql, RDBMS
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/jr-data-analyst-data-scientist/
Simple Beginners guide to Reinforcement Learning & its implementation,Learn everything about Analytics|Introduction|Table of Content|1. Formulating a Reinforcement Learning Problem|2. Comparison with other machine learning methodologies|3. Framework for solving Reinforcement Learning Problems||4. An implementation of Reinforcement Learning|5. Increasing the complexity|6. A Peek into Recent Advancements in Reinforcement Learning|7. Additional Resources|End notes,"Re-inforcement Learning||Markov Decision Process:|Shortest Path Problem||Other ways of travelling?|Problem  Towers of Hanoi|Problem  3 x 3 Rubix Cube|Learn, compete, hack and get hired|Share this:|Like this:|Related Articles|Jr Data Analyst/ Data Scientist|The most comprehensive Data Science learning plan for 2017|
Faizan Shaikh
|28 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 1: Install keras-rl library|Step 2: Install dependencies for CartPole environment|Step 3: lets get started!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"One of the most fundamental question for scientists across the globe has been  How to learn a new skill?. The desire to understand the answer is obvious  if we can understand this, we can enable humanspecies to do things we might not have thought before. Alternately, we can train machines to do more human tasks and create true artificial intelligence.While we dont have a complete answer to the above question yet, there are a few things which are clear. Irrespective of the skill, we first learn by interacting with the environment. Whether we are learning to drive a car or whether it an infant learning to walk, the learning is based on the interaction withthe environment. Learning from interaction is the foundational underlying concept for all theories of learning and intelligence.Today, we will explore Reinforcement Learning  a goal-oriented learning based oninteraction with environment.Reinforcement Learning is said to be the hope of true artificial intelligence. And it is rightly said so, because the potential that Reinforcement Learning possesses is immense.Reinforcement Learning is growing rapidly, producing wide variety of learning algorithms for different applications. Hence it is important to be familiar with the techniques of reinforcement learning. If you are not familiarwith reinforcement learning, I will suggest you to go through my previous article on introduction to reinforcement learning and the open source RL platforms.Once you have an understanding of underlying fundamentals, proceed with this article. By the end of this article you will have a thorough understanding of Reinforcement Learning and its practical implementation.P.S. For implementation, we assume that you have basic knowledge of Python. If you dont know Python, you should first go through this tutorialReinforcement Learning is learning what to do and how to map situations to actions. The end result is to maximize the numerical reward signal. The learner is not told which action to take, but instead must discover which action will yield the maximum reward. Lets understand this with a simple example below.Consider an example of a child learning to walk.Here are the steps a child will take while learning to walk:Sounds like a difficult task right? It actually is a bit challenging to get up and start walking, but you have become so useto it that you are not fazed by the task. But now you can get the gist of how difficult it is for a child.Lets formalize the above example, the problem statement of the example is to walk, where the child is an agent trying to manipulate the environment (which is the surface on which it walks) by taking actions (viz walking) and he/she tries to go from one state (viz each stephe/she takes) to another. The child gets a reward (lets say chocolate) when he/she accomplishes a submodule of the task (viz taking couple of steps) and will not receive any chocolate(a.k.a negative reward) when he/she is not able to walk. This is asimplified description of a reinforcement learning problem.Heres a good introductory video on Reinforcement Learning.Reinforcement Learning belongs to a bigger class of machine learning algorithm. Below is the description of types of machine learning methodologies.Lets see a comparison between RL and others:There is also a fourth type of machine learning methodology called semi-supervised learning, which is essentially a combination of supervised and unsupervised learning. It differs from reinforcement learning as similar to supervised and semi-supervised learning has direct mapping whereas reinforcement does not.To understand how to solve a reinforcement learning problem, lets go through a classic example of reinforcement learning problem  Multi-Armed Bandit Problem. First, we would understand the fundamental problem of exploration vs exploitation and then go on to define the framework to solve RL problems.Suppose you have many slot machines with random payouts. A slot machine would look something like this.Now you want to do is get the maximum bonus from the slot machines as fast as possible. What would you do?One naive approach might be to select only one slot machine and keep pulling the lever all day long. Sounds boring, but it may give you some payouts.With this approach, you might hit the jackpot (with a probability close to 0.00000.1) but most of the time you may just be sitting in front of the slot machine losing money. Formally, this can be defined as a pure exploitation approach. Is this the optimal choice? The answer is NO.Lets look at another approach. We could pull alever of each & every slot machine and pray to God that at least one of them would hit the jackpot. This is another naive approach which would keep you pulling levers all day long, but give you sub-optimal payouts. Formally this approach is a pure exploration approach.Both of these approaches are not optimal, and we have to find a proper balance between them to get maximum reward. This is said to be exploration vs exploitation dilemma of reinforcement learning.First, we formally define the framework for reinforcement learning problem and then list down the probable approaches to solve the problem.The mathematical framework for defining a solution in reinforcement learning scenario is called Markov Decision Process. This can be designed as:We have to take an action (A) to transition from our start state to our end state (S). In return getting rewards (R) for each action we take. Our actions can lead toa positive reward or negative reward.The set of actions we took define our policy () and the rewards we get in return defines our value (V). Our task here is to maximize our rewards by choosing the correct policy. So we have to maximize for all possible values of S for a time t.Let me take you through another example to make itclear.This is a representation of a shortest path problem. The task is to go from place A to place F, with as low cost as possible. The numbers at each edge between two places represent the cost taken to traverse the distance. The negative cost are actually some earnings on the way. We define Value is the total cumulative reward when you do a policy.Here,Now suppose you are at place A, the only visible path is your next destination and anything beyond that is not known at this stage (a.k.a observable space).You can take a greedy approachand take the best possible next step, which is going from {A -> D} from a subset of {A -> (B, C, D, E)}. Similarly now you are at place D and want to go to place F, you can choose from {D -> (B, C, F)}. We see that {D -> F} has the lowest cost and hence we take that path.So here, our policy was to take {A -> D -> F} and our Value is -120.Congratulations! You have just implemented a reinforcement learning algorithm. This algorithm is known as epsilon greedy, which is literally a greedy approach to solving the problem. Now if you (the salesman) want to go from place A to place F again, you would always choose the same policy.Can you guess which category does our policy belong to i.e. (pure exploration vs pure exploitation)?Notice that the policy we took is not an optimal policy. We would have to explore a little bit to find the optimal policy. The approach which we took here is policy based learning, and our task is to find the optimal policy among all the possible policies. There are different ways to solve this problem, Ill briefly list down the major categoriesI would try to cover in-depth reinforcement learning algorithms in future articles. Till then, you can refer to this paper on a survey of reinforcement learning algorithms.We will be using Deep Q-learning algorithm. Q-learning is a policy based learning algorithm with the function approximator as a neural network. This algorithm was used by Google to beat humans at Atari games!Lets see a pseudocode of Q-learning:A simple description of Q-learning can be summarized as follows:We will first see what Cartpole problem is then go on to coding up a solutionWhen I was a kid, I remember that I would pick a stick and try to balance it on one hand. Me and my friends used to have this competition where whoever balances it for more time would get a reward, a chocolate!Heres a short video description of a real cart-pole systemLets code it up!To setup our code, we need to first install a few things,From terminal, run the following commands:Assuming you have pip installed, you need to install the following librariesFirst we have to import modules that are necessaryThen set the relevant variablesNext, we build a very simple single hidden layer neural network model.Next, weconfigure and compile our agent. We set our policy as Epsilon Greedy andwe also set our memory as Sequential Memory because we want to store the result of actions we performed and the rewards we get for each action.Now we test our reinforcement learning modelThis will be the output of our model:And Voila! You have just built a reinforcement learning bot!Now that you have seen a basic implementation of Re-inforcement learning, let us start moving towards a few more problems, increasing the complexity little bit every time.For those, who dont know the game  it was invented in 1883 andconsists of 3 rods along witha number of sequentially-sized disks (3 in the figure above) starting atthe leftmost rod. The objective is to move all the disks from the leftmost rod to the rightmost rodwith the least number of moves. (You can read more onwikipedia)If we have to map this problem, let us start with states:All possible states:Here are our 27 possible states:Where (12)3* represents disks 1 and 2 in leftmost rod (top to bottom) 3 in middle rod and * denotes an empty rightmost rodNumerical Reward:Since we want to solve the problem in least number of steps, we can attach a reward of -1 to each step.Policy:Now, without going in any technical details, we can map possible transitions between above states. For example (123)** -> (23)1* with reward -1. It can also go to (23)*1If you can now see a parallel, each of these 27 states mentioned above can represent a graph similar to that of shortest path algorithm above and we can find the most optimal solutions by experimenting various states and paths.While I can solve this for you as well, I would want you to do this by yourself. Follow the same line of thought I used above and you should be good.Start by defining the Starting state and the end state. Next, define all possible states and their transitions along with reward and policy. Finally, you should be able to create a solution for solving a rubix cube using the same approach.As you would realize that the complexity of this Rubix Cube is many folds higher than the Towers of Hanoi. You can also understand how the possible number of options have increased in number. Now, think of number of states and options in a game of Chess and then in Go! Google DeepMind recently created a deep reinforcement learning algorithm which defeated Lee Sedol!With the recent success in Deep Learning, now the focus is slowly shifting to applying deep learning to solve reinforcement learning problems. The news recently has been flooded with the defeat of Lee Sedol by a deep reinforcement learning algorithm developed by Google DeepMind. Similar breakthroughs are being seen in video games, where the algorithms developed are achieving human-level accuracy and beyond. Research is still at par, with both industrial and academic masterminds working together to accomplish the goal of building better self-learning robotsSourceSome major domains where RL has been applied are as follows:There are so many things unexplored and with the current craze of deep learning applied to reinforcement learning, there certainly are breakthroughs incoming!Here is one of the recent news:Excited to share an update on #AlphaGo! pic.twitter.com/IT5HGBmYDr Demis Hassabis (@demishassabis) January 4, 2017I hope now you have in-depth understanding of how reinforcement learning works. Here are some additional resources to help you explore more about reinforcement learningI hope you liked reading this article. If you have any doubts or questions, feel free to post them below.If you have worked with Reinforcement Learning before then share your experience below.Through this article I wanted to provide you an overview of reinforcement learning with its practical implementation. Hope you make found it useful.",https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/
The most comprehensive Data Science learning plan for 2017,Learn everything about Analytics|Overview||Note  Here is The Ultimate Learning Path to Becoming a Data Scientist in 2019|Why create this learning path?|Who should use this learning path?|How can you use this learning path?|Table of Contents|1. A few definitions before we start|2. Setting target and timelines for yourself|3. Ultimate Beginners path for 2017|4. Transitioners path for 2017||The Ultimate Path for transitioners|5. Intermediates path for 2017|End Notes,"3.1: Getting Started and testing the waters|3.2: Basics of Mathematics and Statistics|3.3: Introducing the tool  R / Python|Feature Selection/ Engineering|3.4: Basic & Advanced machine learning tools|3.5: Building your profile|3.6:Apply for Jobs & Internships||5.1: Assess your technical & structured thinking skills Jan 2017|5.2: Few more ML algorithms Feb 2017|5.3: Pick up a data visualization tool (March 2017)|||5.4: Big Data tools and techniques (April 2017)|5.5: Deep Learning Basics & Advanced (May 2017  August 2017)|5.6: Reinforcement Learning (September 2017  October 2017)|5.7: Web frameworks & Cloud Computing (November 2017  December 2017)|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Simple Beginners guide to Reinforcement Learning & its implementation|MyStory: How I became a Data Science Analyst from a Software developer?|
NSS
|160 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Descriptive Statistics  1 week|Probability  2 weeks||Inferential Statistics  2 weeks||Linear Algebra  1 week||Structured Thinking  2 weeks|Tools|1. R|2. Python|Exploration and Visualization|1. R|2. Python|Linear Regression|Logistic Regression|Decision Trees|KNN (K- Nearest Neighbors)|K-Means|Naive Bayes|Dimensionality Reduction|Random Forests|Gradient Boosting Machines|XGBOOST|Support Vector Machines|GitHub Profile Building (mandatory)|Practice via competitions (mandatory)|Discussion Forums (optional)||Structured Thinking|Interactive Visualization using d3.js|Creating Visualizations using QlikView|Creating Visualizations in Tableau|Big Data||Other useful tools:|Deep Learning Basics (May 2017  June 2017)|Deep Learning advanced (June 2017  August 2017)|Web Frameworks|Cloud computing,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I joined Analytics Vidhya as an intern last summer. I had no clue what was in store for me. I had been following the blog for some time and liked the community, but did not know what to expect as an intern.The initial few days were good  all the interns were smart, motivated and fun to be around. We played cricket in office, did internal hackathons over weekends and learnt a lot of data science. But, if there was one defining moment for me in the internship  it was when I realized the impact Analytics Vidhya was having in data science community.I saw thousands of people following Analytics Vidhya religiously. I saw people looking up for guidance in our meetups and hackathons. I saw people transitioning their careers because of the resources we provide them. That is when this good internship transformed into a mind blowing experience.That is the day I decided that this is my calling. It just felt that this is what I would want to do daily.Among various resources on Analytics Vidhya, learning paths are special. The amount of effort and thinking they need is tremendous. The number of drafts they undergo is mind-boggling. But, the kind of impact they create for our audience is HUGE. That is why I decided that I will create a learning plan for 2017 for all our followers.We created a similar plan for 2016 and we saw transitions happening by people following this learning plan. This time we have created a much granular and a more detailed learning plan. The sole aim behind creating this comprehensive plan is to create a much bigger impact for our followers this year.This learning path would be extremely useful for any one who wants to learn machine learning, deep learning or data science in this year. If you plan to wait for a year, we will publish something similar in 2018 as well But, for the people looking for action this year, this framework and plan of action should be extremely useful. Whether you are a complete fresher or a transitioner or you are looking to up-skill yourself, this plan should give you the necessary direction.We published a similar plan in2016 and we saw followers makingtransition by simply following the plan. This years plan is more nuanced than last years one  so if you plan to pick up / improve data science skills  this plan will guide you through the journey.In creating this plan, we have removed the confusion from the process of learning. The biggest challenge which people face while learning is not dearth of learning material  but too much of it. You are not sure where to start learning, what to practice, how much time to spend on a concept, where to get the useful resources etc. For most of the beginners, this becomes overwhelming and they simply drop out before even learning a single skill.This plan takes this confusion out. This path contains both theoretical resources as well practical examples. We have also provided you with resources / tests to applyyour learning and benchmark yourself. As part of this plan, you will apply the concepts you learn on real-world problems and gain hands-on experience.The first thing you need to do is identify which kind of learner are you. Have a look at the definitions / descriptions below and identify which category you belong to.
We have created these guides with the following target in mind:Structure for your 2017 journey:Time suggested: 4 weeks (January 2017)At this stage, it is important to understand why you want to become a data scientist? What are your strengths and weaknesses?Do you know what it takes to be a Data Scientist? You must answers these questions before jumping on the boat of Data Science journey.Watch this excellent video where Tetiana Ivanova describes how she became a Data Scientist without going through a Masters or doctorate program in data science and with help of Meetups.Hereare some additionalresources you can use to answer these questions:Go ahead and think through theseaspects of choosing a career in data science. This decision is going to decide the next 11 months of your life.Time suggested: 8 weeks (February 2017  March 2017)Topics to be covered:Time suggested: 8 weeks (April 2017  May 2017)Topics to be covered:Time suggested: 12 weeks (June2017  August2017)Topics to be covered (June 2017  July 2017):Time suggested: 8 weeks (September2017  October2017)Topics to be covered:It is very important for a Data Scientist to have a GitHub profile to host all the codes of the project he/she has undertaken. Potential employers not only see what you have done, how you have coded and how frequently / how long you have been practicing data science.Also, codes on GitHub open up avenues for open source projects which can highly boost your learning. If you dont know how to use Git,you can learn fromGit and GitHubon Udacity. This is one of the best and easy to learn course to manage the repositories through terminal.Time and again, I have stressed on the fact that practice beats theory. Moreover coding in hackathons brings you closer to developing data products in real life for solving real world problems. Below are most popular platforms to participate in Data Science/ Machine Learning Competitions.Discussions are a great way to learn in a peer-to-peer setup from finding an answer to a question you stuck to providing answers to someone elses questions. Below are some of the discussion rich platforms which you should keep a tab on to clear your doubts.Time suggested: 8 weeks (November2017  December2017)Topics to be covered: Jobs / InternshipsIf you are here after diligently following the above steps, then you can be sure that you are ready for a Job / Internship position at any Data Science / Analytics or Machine Learning firms. But it becomes quite difficult to identify the right jobs. So, for the purpose of saving the trouble, I have created a list of portals which lists down Data Science/ Machine Learning jobs and Internships.In order to prepare for these interviews, you should go through this Damn Good Hiring GuideLet me start by giving you the bad news  it is not going to be easy to transition in data science. Also, the more your work experience, the more difficult your transition would typically be. You would need a strong resolve  there will be times when you might question, whether this is the right domain for you.The good news is that once you get your first break in the industry, there is no looking back. Also, because of the salary differential from other industry, you may not need to compromise on your earnings during transition.To achieve your goal all you have to do is follow this learning path diligently. We have covered all the skills, techniques you need to gain to take your first steps in data science.Simply put, if you are looking for a transition under a year, you will need to learn everything welaid out for the beginner above. Additionally, you will need to carve out additional time to showcase your skills. You will need to overcome the doubts of your potential employers through your projects and work.I am sure you are beginning to understand why transition is not an easy thing.Structure for your 2017 journey:The structure of the path is similar, but you will need to accelerate your learning in the first half of the plan. Start bygoing through this articleand go through a few success stories to understand what a transition would entail. Once you are set for the journey, follow the plan by sticking to these timelines.
If you can build predictive models, but dont necessary know deep learning and some recent development in the domain, this learning path can help you out. Depending on your skills and learning plan for the year, you can pick and choose the areas you want to learn.Structure of intermediate path for 2017:The first step in creating your learning plan is to benchmark yourself on variousskills  both technical and structured thinking. You can go through the skill tests on Analytics Vidhya to judge whether you need to review the old material. If you do well, go ahead with acquiring new skills. Else, go back to practice for some more time.If you feel the need to go through the old material once again, refer to beginners path which contains various usefulresources.Skill tests:There are a few specific machine learning algorithms, which come in handy while solving specific problems. For example, try solving online click prediction on large data sets with out applying online learning algorithms and you would know what I am talking about. Here are a few advanced ML algorithms you should learn this month:Online Machine LearningVowpal WabbitFTRL- Algorithms
Exercise: Practice on one of the old Kaggle competitions or open click through rate data sets as provided by Criteo.Ideally you should pick up D3.js for sure and either one of QlikView and Tableau. While D3.js provides the most flexibility, QlikView and Tableau are both handy for creating dashboards or less complex story creation and narration.Topics to be covered:The reason d3.js is not so much popular among Data Scientist is because it requires an entire different skill test like HTML, CSS, Javascript which is not typical of a Data Scientist.But knowing D3.js can take your story telling capabilities to a different level. You can create non-static Interactive graphs embedded right in a browser for a much richer experience. Below are the list of resources to master d3.jsTopics to be covered: Reinforcement Learning (Theory)Now that youknow machine learning well, you might want to apply it to web products. What you need to learn is a working knowledge about web frameworks. Web frameworks allow you to quickly build and prototype web based products, with out getting intothe complications of coding.Given that you would already have working knowledge of Python, you can choose any of the Python based web frameworks. I would recommend Flask for its simplicity. Flask is a simple and light web framework, which should serve your needs well. If you are looking to build a complex web product, you might want to consider Django as well.Resources for learning Flask:Exercises:Additionally, you should do a side project to merry your machine learning skills and web development skills. You can build a simple web application where users can upload pictures and find which make and model the car is. Or may be tells people about their age.
Now that you know how to build web applications, you should also get your hands dirty on cloud computing. A few popular platforms are Amazon Web Services (AWS), Google Cloud platform and Microsoft Azure.Each of these platform provide extensive documentation for their offering. If you have to pick only one  AWS is the way to go because of its popularity, wide spread use and comprehensive offerings.I hope you found this learning pathhelpful. I have made it as specific and comprehensive as possible. If you think I have missed out on any specific areas or resources, do let me know.If you want to progress in your data science journey all you have to do is choose your category and follow the learning diligently.If you have any questions, doubts or suggestions drop in your comment below and I will be happy to answer them.If you want to make your own learning path share it with me how are you planning to follow your journey of becoming a data scientist.",https://www.analyticsvidhya.com/blog/2017/01/the-most-comprehensive-data-science-learning-plan-for-2017/
MyStory: How I became a Data Science Analyst from a Software developer?,Learn everything about Analytics,"Background|How it all started?|Getting the Flavour|The Struggle is for real and is always worth it|The Win  Win Situation|My Advice to People|Which is more important for a Data Analyst: Intellectual Curiosity/Intuition V.S Deep Statistical Knowledge?|End Notes|About the Author|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|The most comprehensive Data Science learning plan for 2017|Sentiment Analysis of Twitter Posts on Chennai Floods using Python|
Guest Blog
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dont let the noise of others opinions drown out your own inner voice.-Steve JobsTo be honest, my inner voice always told me to believe I am good at numbers & communication, and no matter how many wrong paths I took, my boat sailed all the way to the shore I was meant to be on.Before I reveal how I got introduced to this phenomenal field Data Science & Analytics, I will take you through what other jobs I tried my hands on. Moreover, I wouldnt say I have reached the pinnacle of success, but I am sure I have secured a right track at least for now.Well, my journey till here is filled with lot ups and down. Read my full story to know more.I was in my pre-final year at college, I honestly had no clue what profession I am going to be in after I finish my graduation (like most Indian students). I bagged an internship at a startup as Trainee Android developer.Well to be honest, I did like the idea of creating stuff, but I never had that thing for core software development. The only thing that carried me to take this internship was my passion for creating thebeautiful user interface(s). I did go through some MOOCs on android development before taking up the internship. After 2 months of creating apps, I started to get the feeling that it was not my thing. Well, that definitely doesnt imply that I was not doing well at it, but I just could not see myself doing that for the rest of my life.Quick Take Away: Finding your passion in life is essential.It was early in 2015 when I got a flavor of this amazing field. As I recall, it was after one of those popular questions all aspiring data scientist ask Which programming language for Data Science- R vs Python vs SAS?The best answer to this question could be found nowhere else but on this phenomenal website  Analytics Vidhya (AV). Thats how I got introduced to AV. From that day onwards I have been spending a significant time of my life on AV. But the real challenge was yet to show up.Did I grab a job in analytics yet?NO! So it was still not a merry time for me.Quick Take Away: The journey starts with one thing, no matter how long or short.I started to apply for data science jobs, but most of the positions sought Masters /PhD students (especially in statistics). And besides that, I also started looking for predictive business analytics courses [classroom], as I was out of college by this time.Learning from MOOCs is not easy and is time-consuming especially when you have a job in atotally different field. I must admit perseverance was the key that got me through it. I kept following AV and started attending most of the Data Hackathons they conductedespecially offline ones. It was one great place to meet people of same mindset, which allowed me to learn most tricks for data science.I also picked up some basic courses on Coursera, Excel to MySQL techniques for business is one. Initially, I use to score miserably in competitions but I just kept on trying. Remember, even if you win the competition or not, you are always taking away good amount of knowledge  so keep on participating in competitions. In my case, I realized I was the kind of person who performs the best if there is a deadline associated with it.Meanwhile, I was working as an Analyst under Property & Casualty vertical of Insurance. My nature of work didnt require me to use much of the cutting edge technologies. In spite of that, I always tried looking for ways to use new tools and techniques in order to improve the way I did my job, which of course allowed me to learn a lot.One thing to my readers -No matter how difficult the path to your dream seems, just getting back at it again & again and yet again makes it easier than ever.I am more than thankful to Analytics Vidhya for laying out a path via The Ultimate Plan to Become a Data Scientist in 2016. By sticking to this path it can land you into wonders, trust me.Word of caution: Data Science & Analytics is a research oriented field dont just pursue it because there is a lot of hype behind it. If you dont like exploring your brains off on a particular case / topic in order to yield few but meaningful information, sorry to say this field is not meant for you.Like they say, its 10% luck, 20% skill, 15% concentrated willpower, 5% pleasure, 50% pain and a 100% reason to win the game. Well, this shouldnt be taken literally but yeah a typical success story comprises of these.The few things that I realized defines your success in this field are:1.) Attitude2.) Aptitude3.) Discretionary Efforts4.) Communication Skills (is like bread and butter)5.) Tools and Techniques (must have).Having said that, I choose my way to success by being hell-bent to learn all it takes to be in decision engineering (Data Sciences). In the span of 8 months of my job as an Analyst,I practiced on different data science tools at my workplace.The skills which I learned on my way allowed me to relate the conceptual problems into pragmatic approach of solving the problems. Well, to some extent 10% of my luck also played an important part in landing me a Data Science job.Fortunately, later that year my organization declared a new business vertical- Data Science. After finding out this I was the happiest person in the organization (I bet there were many more though). Every discretionary effort I made until now determined the probability of my inclusion in this Business vertical. I applied for the position with a portfolio of all my projects by my side. In addition to this, I coordinated the introduction session to Data Science vertical which allowed me to closely meet the Business unit head that later led us into further interaction(s).My Portfolio of projects, AV-Data Hack Meetup Winner Certificate, Technical Skills and Discretionary Efforts (like I have mentioned before) made me nail this position. Working as a data science analyst requires one to be a quick learner, every new tool brought into the arsenal should be grasped really quickly. Its quite incumbent to haveIts quite incumbent to have aninsatiable passion for learning new things if you want to be a successful data scientist. I have not achieved success yet, but I am pretty sure I have embarked on the right path.After all, the only thing that determines the ultimate success is nothing but perseverance.Thinking like an entrepreneur really helps, a successful data scientist shouldnt restrict themselves to only building models. One should also find opportunities to get involved in other vital roles of Analysis process.Tools and techniques are important, but only being an adept at using them wont lead to ultimate success. Both hypothesis generation and data preparation are equally important. One who is not able to communicate the insights drawn from the analysis process would never fall in the category of Data Science unicorns.Intellectual curiosity is the most important thing. The understanding of statistics can be gained, but the curiosity is more innate, if youre not naturally into working with data, you can be a fantastic analyst, but you become a specific type of analyst. These analysts may not care enough to build a better model.They lack creativity and the determination to go the extra mile. Theyll need hand holding and will struggle to resolve new problems. But being curious to know more insights about the data you can overcomethe lack of statistical knowledge.People willing to pursue a successful career in this phenomenal field should be inclined towards Using data science than doing data science. Essentially, one should not only be good at providing solutions to problems at hand, but also be able to identify data problems and take accountability of using data science to structurally solvethese problems. An analyst is only as good as their ability to independently work on problems. So, break free from whatever is holding you back.Ankit Dwivedi is data science aficionado & computer science engineer with 3+ years of experience as a Data Analyst in the Data Science & Analytics vertical at Wenso Ltd. (a leading IT firm in UK). Wenso, being a paramount in GDPR consultation in the UK, deals with a lot of EU data, bringing in the need for better Data Science solutions. Here, he is working on a gamut of Data Science problems. In his free time, he likes reading articles & solving problems in competitions at Analytics Vidhya and Kaggle.Ankit is the winner of Whats Your Story. We found his story very inspiring and that has earned him Rank 1 in the competition. To read the stories of other winners. Stay Tuned!Disclaimer: Our stories are published as narrated by the community members. They do not represent Analytics Vidhyas view on any product / services / curriculum.",https://www.analyticsvidhya.com/blog/2017/01/mystory-how-i-became-data-science-analyst-from-software-developer/
Sentiment Analysis of Twitter Posts on Chennai Floods using Python,Learn everything about Analytics|Introduction|Building Corpus|Data Exploration|Text Pre-processing|Text Exploration|Clustering|Topic Modeling|Doc2Vec and K-means,"Latent Dirichlet Allocation (LDA)|End Notes|Got expertise in Business Intelligence / Machine Learning / Big Data / Data Science? Showcase your knowledge and help Analytics Vidhya community byposting your blog.|Share this:|Like this:|Related Articles|MyStory: How I became a Data Science Analyst from a Software developer?|Ultimate Guide to Understand and Implement Natural Language Processing (with codes in Python)|
Yogesh Kulkarni
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The best way to learn data science is to do data science. No second thought about it!One of the ways, I do this is continuously look for interesting work done by other community members. Once I understand the project, I do / improve the project on my own. Honestly, I cant think of a better way to learn data science.As part of my search,I came across a study on sentiment analysis of Chennai Floods on Analytics Vidhya. I decided to perform sentiment analysis of the same study using Python and add it here. Well, what can be better than building onto something great.To get acquainted with the crisis of Chennai Floods, 2015 you can read the complete studyhere. This study wasdone on a set of social interactions limited to the first two days of Chennai Floods in December 2015.The objectives of this article is to understand the different subjects of interactions during the floods using Python.Grouping similar messages together with emphasis on predominant themes (rescue, food, supplies, ambulance calls) can help government and other authorities to act in the right manner during the crisis time.A typical tweet is mostly a text message within limit of 140 characters. #hashtags convey subject of the tweet whereas @user seeks attention of that user. Forwarding is denoted by rt (retweet) and is a measure of its popularity. One can like a tweet by making it favorite.About 6000 twits were collected with #ChennaiFloods hashtag and between 1st and 2nd Dec 2015. Jeffersons GetOldTweets utility (got) was used in Python 2.7 to collect the older tweets. One can store the tweets either in a csv file or to a database like MongoDb to be used for further processing.Tweets stored in MongoDB can be accessed from another python script. Following example shows how the whole db was converted to Pandas dataframe.First few records of the dataframe look as below:Once in dataframe format, it is easier to explore the data. Here are few examples:Top 10 Hashtags trendingAs seen in the study the most used tags were #chennairains, #ICanAccommodate, apart from the original query tag #ChennaiFloods.As seen from the plot, most active users were TMManiac with about 85 tweets, Texx_willer with 60 tweets and so onTop 10 Users tweetingAll tweets are processed to remove unnecessary things like links, non-English words, stopwords, punctuations, etc.The word list generated looks like:[time, history, temple, closed, due, pic, twitter, havoc, incessant, ]The words are plotted again to find the most frequently used terms. A few simple words repeat more often than others: help, people, stay, safe, etc.[(twitter, 1026), (pic, 1005), (help, 569), (people, 429), (safe, 274)]These are immediate reactions and responses to the crisis.Some infrequent terms are [(fit, 1), (bible, 1), (disappear, 1), (regulated, 1), (doom, 1)].Most frequently used wordsCollocations are the words that are found together. They can be bi-grams (two words together) or phrases like trigrams (3 words) or n-grams (n words).Most frequently appearing Bigrams are:[(pic, twitter), (lady, labour), (national, media), (pani, pani), (team, along), (stay, safe), (rescue, team), (beyond, along), (team, beyond), (rescue, along)]These depict the disastrous situation, like stay safe, rescue team, even a commonly used Hindi phrase pani pani (lots of water).In such crisis situations, lots of similar tweets are generated. They can be grouped together in clusters based on closeness or distance amongst them. Artem Lukanin has explained the process in details here. TF-IDF method is used to vectorize the tweets and then cosine distance is measured to assess the similarity.Each tweet is pre-processed and added to a list. The list is fed to TFIDF Vectorizer to convert each tweet into a vector. Each value in the vector depends on how many times a word or a term appears in the tweet (TF) and on how rare it is amongst all tweets/documents (IDF). Below is a visual representation of TFIDF matrix it generates.TF-IDF MatrixBefore using the Vectorizer, the pre-processed tweets are added in the data frame so that each tweets association with other parameters like id, user is maintained.Vectorization is done using 1-3 n-grams, meaning phrases with 1,2,3 words are used to compute frequencies, i.e. TF IDF values. One can get cosine similarity amongst tweets/documents as well.K-means clustering algorithm is used to group tweets into choosen number (say, 3) of groups.The output shows 3 clusters, with following number of tweets in respective clusters.Most of the tweets are clustered around in group Id =1. Remaining are in group id 2 and id 0.
The top words used in each cluster can be computed by as follows:The result is:Finding central subject in the set of documents, tweets in case here. Following are two ways of detecting topics, i.e. clustering the tweetsLDA is commonly used to identify chosen number (say, 6) topics. Refer tutorial for more details.The output gives us following set of words for each topic.It is clear from the words associated with the topics that they represent certain sentiments. Topic 0 is about Caution, Topic 1 is about Help, Topic 2 is about News, etc.Doc2Vec methodology available in gensim package is used to vectorize the tweets, as follows:Once trained model is ready the tweet-vectors available in model can be clustered using K-means.The result is the list of topics and commonly used words in each, respectively.It is clear from the words associated with the topics that they represent certain sentiments. Topic 0 is about Caution, Topic 1 is about Actions, Topic 2 is about Climate, etc.The result is the list of topics and commonly used words in each, respectively.This article shows how to implement Capstone-Chennai Floods study using Python and its libraries. With this tutorial, one can get introduction to various Natural Language Processing (NLP) workflows such as accessing twitter data, pre-processing text, explorations, clustering and topic modeling.",https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/
Ultimate Guide to Understand and Implement Natural Language Processing (with codes in Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Introduction to Natural Language Processing|2. Text Preprocessing|3.Text to Features (Feature Engineering on text data)|4. Important tasks of NLP|5. Important Libraries for NLP (python)|Projects,"2.1 Noise Removal|2.2 Lexicon Normalization|2.3 Object Standardization|3.1 Syntactic Parsing|3.2 Entity Extraction (Entities as features)|3.3 Statistical Features|3.4 Word Embedding (text vectors)|4.1 Text Classification|4.2 Text Matching / Similarity|4.3 Coreference Resolution|4.4 Other NLP problems / tasks|End Notes|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Sentiment Analysis of Twitter Posts on Chennai Floods using Python|Data Scientist- Chennai (4-6 years of experience)|
Shivam Bansal
|57 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",A. Named Entity Recognition (NER)|B. Topic Modeling|C. N-Grams as Features|A. Term Frequency  Inverse Document Frequency (TF  IDF)|B. Count / Density / Readability Features,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"According to industry estimates,only 21% of the available data is present in structured form. Data is being generated as we speak, as we tweet, as we send messages on Whatsapp and in various other activities. Majority of this data exists in the textual form, which is highly unstructured in nature.Few notorious examples include  tweets / posts on social media, user to user chat conversations, news, blogs and articles, product or services reviews and patient records in the healthcare sector. A few more recent ones includes chatbots and other voice driven bots.Despite having high dimension data, the information present in it is not directly accessible unless it is processed (read and understood) manually or analyzed by an automated system.In order to produce significant and actionable insights from text data, it is important to get acquainted with the techniques and principles of Natural Language Processing (NLP).So, if you plan to create chatbots this year, or you want to use the power of unstructured text, this guide is the right starting point. This guide unearths the concepts of natural language processing, its techniques and implementation. The aim of the article is to teach the concepts of natural language processing and apply it on real data set. Moreover, we also have a video based course on NLP with 3 real life projects.NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as  automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.Before moving further, I would like to explain some terms that are used in the article:Steps to install NLTK and its data:Install Pip: run in terminal:Install NLTK: run in terminal :Download NLTK data: run python shell (in terminal) and write the following code:Follow the instructions on screen and download the desired package or collection. Other libraries can be directly installed using pip.Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.It is predominantly comprised of three steps:The following image shows the architecture of text preprocessing pipeline.Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.For example  language stopwords (commonly used words of a language  is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.Following is the python code for the same purpose.Another approach is to use the regular expressions while dealing with special patterns of noise.We have explained regular expressions in detail in one of our previous article.Following python code removes a regex pattern from the input text:Another type of textual noise is about the multiple representations exhibited by single word.For example  play, player, played, plays and playing are the different variations of the word  play, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.The most common lexicon normalization practices are :Below is the sample code that performs lemmatization and stemming using pythons popular library  NLTK.Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.Some of the examples are  acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text.Apart from three steps discussed so far, other types of text preprocessing includes encoding-decoding noise, grammar checker, and spelling correction etc. The detailed article about preprocessing and its methods is given in one of my previous article. To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques  Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail.Syntactical parsing invol ves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.Dependency Trees  Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence  Bills  on ports and immigration were submitted by Senator Brownback, Republican of Kansas. The relationship among the words can be observed in the form of a tree representation as shown: The tree shows that submitted is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as  (Bills <-> ports <by> proposition relation), (ports <-> immigration <by> conjugation relation).This type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees.Part of speech tagging  Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. H ere is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)Part of Speech tagging is used for many important purposes in NLP:A.Word sense disambiguation: Some language words have multiple meanings according to their usage. For example, in the two sentences below:I. Please book my flight for Delhi II. I am going to read this book in the flightBook is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word book is used as v erb, while in II it is used as no un. (Lesk Algorithm is also us ed for similar purposes)B.Improving word-based features: A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:Sentence -book my flight, I will read this book Tokens  (book, 2), (my, 1), (flight, 1), (I, 1), (will, 1), (read, 1), (this, 1) Tokens with POS  (book_VB, 1), (my_PRP$, 1), (flight_NN, 1), (I_PRP, 1), (will_MD, 1), (read_VB, 1), (this_DT, 1), (book_NN, 1)C. Normalization and Lemmatization: POS tags are the basis of lemmatization process for converting a word to its base form (lemma).D.Efficient stopword removal : P OS tags are also useful in efficient removal of stopwords.For example, there are some tags which always define the low frequency / less important words of a language. For example: (IN  within, upon, except), (CD  one,two, hundred), (MD  may, mu st etc)Entities are defined as the most important chunks of a sentence  noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.Topic Modelling & Named Entity Recognition are the two key entity detection methods in NLP.The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :Sentence  Sergey Brin, the manager of Google Inc. is walking in the streets of New York. Named Entities  ( person : Sergey Brin ), (org : Google Inc.), (location : New York)A typical NER model consists of three blocks:Noun phrase identification: This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.Phrase classification:This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.Entity disambiguation:Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are  Google Knowledge Graph, IBM Watson and Wikipedia.Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as a repeating pattern of co-occurring terms in a corpus. A good topic model results in  health, doctor, patient, hospital for a topic  Healthcare, and farm, crops, wheat for a topic  Farming.Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique, Following is the code to implement topic modeling using LDA in python. For a detailed explanation about its working and implementation, check the complete article here. A combinationof N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text.Text data can also be quantified directly into numbers using several techniques described in this section:TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example  let say there is a dataset of N text documents, In any document D, TF and IDF will be defined as Term Frequency (TF)  TF for a term t is defined as the count of a term t in a document DInverse Document Frequency (IDF)  IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.TF . IDF  TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using pythons scikit learn package to convert a text into tf idf vectors:The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i.Count or Density based features can also be used in models and analysis. These features might seem trivial but shows a great impact in learning models. Some of the features are: Word Count, Sentence Count, Punctuation Counts and Industry specific word counts. Other types of measures include readability measures such as syllable counts, smog index and flesch reading ease. Refer to Textstat library to create such features.Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.Word2Vec and GloVe are the two popular models to create word embedding of a text. These models takes a text corpus as input and produces the word vectors as output.Word2Vec model is composed of preprocessing module, a shallow neural network model called Continuous Bag of Words and another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations. Following code using gensim package prepares the word embedding as the vectors.They can be used as feature vectors for ML model, used to measure text similarity using cosine similarity techniques, words clustering and text classification techniques.This section talks about different use cases and problems in the field of natural language processing.Text classification is one of the classical problem of NLP. Notorious examples include  Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines.Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information filtering, and storage purposes.A typical natural language classifier consists of two parts: (a) Training (b) Prediction as shown in image below. Firstly the text input is processes and features are created. The machine learning models then learn these features and is used for predicting against the new text.Here is a code that uses naive bayes classifier using text blob library (built on top of nltk).Scikit.Learn also provides a pipeline framework for text classification:The text classification model are heavily dependent upon the quality and quantity of features, while applying any machine learning model it is always a good practice to include more and more training data. H ere are some tips that I wrote about improving the text classification accuracy in one of my previous article.One of the important areas of NLP is the matching of text objects to find similarities. Important applications of text matching includes automatic spelling correction, data de-duplication and genome analysis etc.A number of text matching techniques are available depending upon the requirement. This section describes the important techniques in detail.A. Levenshtein Distance  The Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with the allowable edit operations being insertion, deletion, or substitution of a single character. Following is the implementation for efficient memory computations.B. Phonetic Matching  A Phonetic matching algorithm takes a keyword as input (persons name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar. It is very useful for searching large text corpuses, correcting spelling errors and matching relevant names. Soundex and Metaphone are two main phonetic algorithms used for this purpose. Pythons module Fuzzy is used to compute soundex strings for different words, for example C. Flexible String Matching  A complete text matching system includes different algorithms pipelined together to compute variety of text variations. Regular expressions are really helpful for this purposes as well. Another common techniques include  exact string matching, lemmatized matching, and compact matching (takes care of spaces, punctuations, slangs etc).D. Cosine Similarity  W hen the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. Following code converts a text to vectors (using term frequency) and applies cosine similarity to provide closeness among two text.Coreference Resolution is a process of finding relational links among the words (or phrases) within the sentences. Consider an example sentence:  Donald went to Johns office to see the new table. He looked at it for an hour.Humans can quickly figure out that he denotes Donald (and not John), and that it denotes the table (and not Johns office). Coreference Resolution is the component of NLPthat does this job automatically. It is used in document summarization, question answering, and information extraction. Stanford CoreNLP provides a python wrapper for commercial purposes.Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:I hope this tutorial will help you maximize your efficiency when starting with natural language processing in Python. I am sure this not only gave you an idea about basic techniques but it also showed you how to implement some of the more sophisticated techniques available today. If you come across any difficulty while practicing Python, or you have any thoughts / suggestions / feedback please feel free to post them in thecomments below.This article was contributed by Shivam Bansal who is the winner of Blogathon 2. We will soon be publishing other top two blogs from the competitionBlogathon 2. So, Stay Tuned!",https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/
Data Scientist- Chennai (4-6 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Ultimate Guide to Understand and Implement Natural Language Processing (with codes in Python)|Director-Data Science & Analytics- Mumbai (3-5 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Data Scientist / Machine Learning Language ChoiceDatabaseSkills Data / ML VisualizationsComputer Vision (CV)Natural Language Processing(NLP)Recommender SystemsReinforcement LearningGraphical ModelsAnalysis:a) Predictive analysis b) Prescriptive analysis c) Feedback analysisRecommendation Engines:Text MiningData MiningSupervised/Unsupervised/Semi -Supervised Learning Algorithms:Deep LearningInstance  based AlgorithmsBayesian AlgorithmsEnsemble AlgorithmsClustering AlgorithmRegularization AlgorithmsDecision Tree AlgorithmsDimensionality reduction AlgorithmsAssociation Rule Learning AlgorithmsRegression AlgorithmsFeature Selection AlgorithmsAlgorithms Accuracy EvaluationsComputational IntelligenceEvolutionary Algorithms
College Preference : no-bar
Min Qualification : ug
Skills : datavisualization, machine learning, MongoDB, nlp, predictive modeling, python, r, recommendation engine, Reinforcement Learning, ruby, supervised learning, unsupervised learning
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-chennai-4-6-years-of-experience/
Director-Data Science & Analytics- Mumbai (3-5 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Chennai (4-6 years of experience)|Vice President/ Associate- Bangalore (4-8 Years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : Curriculum iteration (10% time)
Industry-relevant cases and projects (40% time)
Academic quality assurance and guest faculty (40% time)
Thought-leadership (10% time)
Student experience (post-program launch)
Task Info : We are the Indias hottest online education startup. Well funded with a committed team, our mission is to empower individuals to reach their full professional potential in the most engaging learning environment. We are partnering with universities and corporates to unite academia and industry and bring education in India to the next level. With a lack of more than one million skilled individuals in areas such as Big Data and Digital Business Management, we are creating post-graduate programs for working professionals in new-age, industry relevant areas. Merging the latest technology, pedagogy and services, we are creating an immersive student experience  anytime and anywhere.Key Responsibilities:We have launch a 11-month online Post Graduate Diploma Program in Data Analytics in partnership with a leading education institute. We envision building this credential into a gold standard for Analytics professionals, akin to a CFA in the Finance world. As a Subject Matter Expert (SME)  Data Analytics, we would seek your leadership in the following areas to help deliver a rigorous, industry-relevant program: Curriculum iteration (10% time): Build a rigorous program by informing decisions on curriculum structure, describing various analytics concepts in innovative, simple ways, and time allocation across concepts Industry-relevant cases and projects (40% time): Identify industry-relevant projects / cases and develop them (e.g. data sets, code solutions, evaluation criteria) with industry partners to offer students compelling project experience from a recruiter standpoint Academic quality assurance and guest faculty (40% time): Help create learning material with an in-house team of data science associates and review its technical and pedagogical quality to ensure an engaging delivery on the program. Deliver 1-2 modules to supplement the existing faculty. Thought-leadership (10% time): Leverage our platform to publish original content (e.g. video as guest faculty, articles on our blog) to position yourself, and by association we as a thought leader in the analytics industry Student experience (post-program launch): Lead a team of program associates to assist students with their academic doubts on the our platformWe are an engaging with various partners like Citi, Flipkart, Fractal, Genpact etc. which will give us the opportunity to build brand equity alongside industry thought leaders through our proprietary events, video content database, blog, among others. Join us in the journey of building a world-class program in Data Analytics!Desired Profile: 3-5 years in Data Analytics education and experience Education / hands-on, analytics workplace experience in applying concepts in statistics, descriptive, inferential, predictive analytics and machine learning, using regular and Big Data sets Intermediate to advanced proficiency in R, SQL, and / or Python Experience in multiple verticals will be a plusConsideration: Depending on the deliverables.
College Preference : no-bar
Min Qualification : pg
Skills : bigdata, data analysis, machine learning, MongoDB, python, r, sql, statistics
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/director-data-science-analytics-mumbai-3-5-years-of-experience/
Vice President/ Associate- Bangalore (4-8 Years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Director-Data Science & Analytics- Mumbai (3-5 years of experience)|Data Scientist- Bangalore (2+ years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  8 years
Requirements : Work with entrepreneurs / management teams
Demonstrated project management skills and an ability to work effectively in small teams
Attention to detail, high level of comfort with ambiguity and a sense of curiosity
Exceptional communication skills (written, spoken and presentation skills)
Task Info : We are the human centered venture capital firm, invests in transformative and scalable entrepreneurial ventures focused on low income communities. Our Method of investing has democratized essential services for over 20 million underserved customers and catalyzed billions of dollars of capital into 25+ companies in India and Latin America. We are the founding or first institutional capital in several companies focused on financial services, agriculture, education, healthcare and housing. An entrepreneurial emerging markets team based in India, Colombia and the US, we are widely recognized as a leader in impact investing.We are looking to recruit a candidate (at the Vice President or Associate level) who has a true passion for using her/his skill sets and execution ability to bring about meaningful improvements in the world. We have a strong organisational culture that defines and shapes who we are  and who we want to be as individuals and as an organisation. Everyone here are the part of a shared ambition, loves the power of a small team and is passionate about building alignment between entrepreneurs, customers and investors.PRIMARY / KEY RESPONSIBILITIES Work with entrepreneurs / management teams of our portfolio companies on specific projects and track financial and operating metrics in order to facilitate, and report on, business performance and customer centricity Building and analysing financial models, performing valuation analysis, conducting and coordinating due diligence and preparing investment recommendations Provide analytical support (qualitative and quantitative), research and industry analysis, while evaluating business models and investment pipeline opportunities Work on special projects that are relevant to our portfolioSKILLS/ EXPERIENCE: Team player who loves to work in a challenging and collaborative work environment Willingness to contribute to our unique work culture by sharing their life experience Demonstrated project management skills and an ability to work effectively in small teams Attention to detail, high level of comfort with ambiguity and a sense of curiosity Exceptional communication skills (written, spoken and presentation skills) Ability to build relationships of trust with our portfolio companies Proven ability to work with an international, cross-continental and diverse team Passion for travel into the real India and an understanding / appreciation for the lives of customers in low income communities Strong financial modeling / analytical skills in the context of investments or transactions, built through experience in venture capital, private equity, investment banking, or a combination of finance and business roles in an entrepreneurial company Relevant work experience at top tier organisations in emerging markets(8+ years for Vice President, 4+ years for Associate) Relevant and strong academic record from a top tier educational institutionAPPLYINGThe cover letter should explain why you want to join us and how your skills and experience align with our requirements for this position. The subject of the email should state Position (Vice President/ Associate)- Bangalore. We are an Equal Opportunity Employer
College Preference : tier1-any
Min Qualification : open
Skills : Advanced Excel, Data Warehouse, excel, financial modeling, Investment banking, portfolio monitoring, Scenario modelling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/vice-president-associate-bangalore-4-8-years-of-experience/
Data Scientist- Bangalore (2+ years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Vice President/ Associate- Bangalore (4-8 Years of experience)|46 Questions on SQL to test a data science professional (Skilltest Solution)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : Quick Learner and should be able to build solutions to solve complex business
problems
Task Info : Main Responsibilities:Advocate, evangelize and build data-fuelled products that help our customers growin businessSelecting features, building and optimizing classifiers using machine learningtechniquesProcessing, cleansing, and verifying the integrity of data used for analysisWrite well designed and efficient codesDevelop new analytical methods/tools as per the requirementRequired Skills:Experience in the field of data science, building predictive analytics solutions fordifferent business problemsExperience in handling large datasets (Structured or Unstructured)Advanced knowledge of statistical techniques, machine learning algorithms, datamining and text miningStrong Programming background and expertise in building models in languages likePython, R, Scala etc.Quick Learner and should be able to build solutions to solve complex businessproblemsExperience working with big data platforms, like Hadoop, Hive, HBase, Spark,Elasticsearch etc.
College Preference : tier1-entire
Min Qualification : ug
Skills : data science, hadoop, hbase, hive, machine learning, predictive modeling, python, r, spark, statistical modeling
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-bangalore-2-years-of-experience/
46 Questions on SQL to test a data science professional (Skilltest Solution),Learn everything about Analytics|Introduction|Overall Scores|Helpful Resources for SQL,"Questions & Answers|End Notes|Learn, compete, hack and get hired!|Share this:|Related Articles|Data Scientist- Bangalore (2+ years of experience)|19 MOOCs on Mathematics & Statistics for Data Science & Machine Learning|
Ankit Gupta
|20 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If there is one language every data science professional should know  it is SQL. SQL stands for Structured Query Language. It is a query language used to access data from relational databases and is widely used in data science.We conducted a skilltest to test our community on SQL and it gave 2017 a rocking start. A total of 1,666 participants registered for the skilltest.This test focuses on practical aspects and challenges people encounter while using SQL. In this article, we provide answers to these test questions. If you took the test, check out which areas need improvement. If you did not take the test, here is your opportunity to look at the questions and check your skill level independently.Below are thedistribution of scores, this will help you evaluate your performance:You can assess your performance here. More than 700 people participated in the skilltestand the highest score was 41.Here are a few statistics about the distribution.Overall distributionMean Score:22.32Median Score: 25Mode Score: 27This is an interesting distribution. I think we are seeing 3 different profiles of people here:How much did you score and where do you fit?Basics of SQL and RDBMS  must have skills for data science professionalsSQL commands for Commonly Used Excel Operations1) Which of the following is the correct order of occurrence in a typical SQL statement?A. select, group by, where, havingB. select, where, group by, havingC. select, where, having, group byselect, having, where, group byD. select, having, where, group bySolution: BWhere always comes before group by and having always comes after group by.Question Context: 2 to 12STUDENTENROLLEDQ2) Which of the following is the correct outcome of the SQL query below?Query: SELECT cid FROM ENROLLED WHERE grade = 'C'A. Extract the course ids(cid) where student receive the grade C in the courseB. Extract the unique course ids(cid) where student receive the grade C in the courseC. ErrorD. None of theseSolution: AThe query will extract the course ids where student receive the grade C in the course.Q3) Which of the following is the correct outcome of the SQL query below?Query: SELECT DISTINCT cid FROM ENROLLED WHERE grade = 'C'A. Extract the course ids where student receive the grade C in the courseB. Extract the Distinct course ids where student receive the grade of C in the courseC. ErrorD. None of theseSolution:BBy using DISTINCT keyword you can extract the Distinct course ids where student receive the grade of C in the course.Q4) Which of the following is the correct outcome of the SQL query below?Query: SELECT name, cid FROM student, enrolled WHERE student.sid = enrolled.sid AND enrolled.grade = 'C'A. Returns the name of all students and their corresponding course idsB. Returns the name of students and their corresponding course id where they have received grade CC. ErrorD. None of theseSolution:BThe above query first joined the ENROLLED and STUDENT tables then it will evaluate the WHERE condition and then it will return the name of students and corresponding course id where they received the grade of C.Q5) Which of the following is the correct outcome of the SQL query below?A. Returns the name, grade of the students who took course 15-415 and got a grade A or B in that courseB. Returns the name, grade of the students who took the course 15-415 but didnt get grade A or B in that courseC. ErrorD. None of theseSolution: AThe above query first joined the ENROLLED and STUDENT tables then it will evaluate the where condition and then it will return the name, grade of the students, those took 15-415 and got a grade A or B in the course. But for the given two tables it will give zero records in output.Q6) Which of the following query will find all the unique students who have taken more than one course?A. SELECT DISTINCT e1.sid FROM enrolled AS e1, enrolled AS e2 WHERE e1.sid != e2.sid AND e1.cid != e2.cidB. SELECT DISTINCT e1.sid FROM enrolled AS e1, enrolled AS e2 WHERE e1.sid = e2.sid AND e1.cid = e2.cidC. SELECT DISTINCT e1.sid FROM enrolled AS e1, enrolled AS e2 WHERE e1.sid != e2.sid AND e1.cid != e2.cidD. SELECT DISTINCT e1.sid FROM enrolled AS e1, enrolled AS e2 WHERE e1.sid = e2.sid AND e1.cid != e2.cidSolution: DOption D would be a right option. This query will first apply self join on enrolled table and then it evaluate the condition e1.sid = e2.sid AND e1.cid != e2.cid.Q7) Which of the following statement will add a column F_name to the STUDENT table?A. ALTER TABLE Student add column ( F_name varchar(20));B. ALTER TABLE Student add F_name varchar(20);C. ALTER TABLE Student add (F_name varchar(20));D. ALTER TABLE Student add column (F_name);Solution: BALTER TABLE command allows a user to add a new column to a table. Option B is correct syntax of ALTER to add a column in the table.Q8) Which of the following query(s) will result in asuccessful insertion of a record in the STUDENT table?Query1: INSERT INTO student (sid, name, login, age, gpa) VALUES (53888, Drake, [emailprotected], 29, 3.5)Query2: INSERT INTO student VALUES (53888, Drake, [emailprotected], 29, 3.5)A. Both queries will insert the record successfullyB. Query 1 will insert the record successfully and Query 2 will notC. Query 2 will insert the record successfully and Query 1 will notD. Both queries will not be able to insert the record successfullySolution: ABoth queries will successfully insert a row in table student. The Query 1 is usefulwhen you want to Provide target table, columns, and values for new tuples and Query 2 is a Short-hand version of insert commandQ9) Sid in ENROLLED table is Foreign Key referenced by Sid in STUDENT table. Now you want to insert a record into the ENROLLED table.Which of the following option(s) will insert a row in ENROLLED table successfully?A. 1 and 3B. Only 3C. 2 and 4D. Only 4Solution: COption 2 and 4 will run successfully because in ENROLLED tables Sid column you can insert those values which are present in STUDENTs table Sid columns due to foreign key.Q10) Consider the following queries:Query1: select name from enrolled LEFT OUTER JOIN student on student.sid = enrolled.sid;
 Query2: select name from student LEFT OUTER JOIN enrolled on student.sid = enrolled.sid;Which of the following option is correct?A. Queries 1 and 2 will give the same resultsB. Queries 1 and 2 will give different resultsC. Query 1 will produce an error and Query 2 will run successfullyD. Query 2 will produce an error and Query 1 will run successfullySolution: AIn (LEFT, RIGHT or FULL) OUTER joins, order matters. But both query will give the same results because both are dependent on records present in table and which column in selected.Q11) Which of the following statements will modify the data type of Sid column in ENROLLED table?Note: There is no foreign key relationship between tables STUDENT and ENROLLED.A. ALTER TABLE ENROLLED MODIFY (sid varchar(100));B. ALTER TABLE ENROLLED MODIFY sid varchar(100);C. ALTER TABLE ENROLLED MODIFY column (sid varchar(100));D. ALTER TABLE ENROLLED MODIFY attribute (sid varchar(100));Solution:AThe ALTER TABLE MODIFY is used to modify column definition in a table. So option A is correct.Q12) Which of the following statement will remove the Sid column from the ENROLLED table?Note: There is no foreign key relationship between tables STUDENT and ENROLLED.A. ALTER TABLE ENROLLED DROP (sid varchar(10) );B. ALTER TABLE ENROLLED DROP COLUMN (sid varchar(10) );C. ALTER TABLE ENROLLED DROP COLUMN Sid;D. ALTER TABLE ENROLLED MODIFY (sid);Solution:CThe ALTER TABLE DROP COLUMN can be used to drop a column from the table. So Option C is theright answer.Q13) Which of the following command(s) is / are related to transaction control in SQL?A. ROLLBACKB. COMMITC. SAVEPOINTD. All of the aboveSolution: DAll are related to transaction control in SQL.Q14) Which of the following is true for a primary key?A. It can take a value more than onceB. It can take null valuesC. It cant take null valuesD. None of theseSolution: CIn a relational schema, there exist only one primary key and it cant take null values. So option C is the correct answer.Q15) What is the difference between a primary key and a unique key?A. Primary key cannot be a date variable whereas unique key can beB. You can have only one primary key whereas you can have multiple unique keysC. Primary key can take null values but unique key cannot null valuesD. None of theseSolution: BYou can create a date variable as a primary key in table. In relational schema, you can have only one primary key and there may be multiple unique key present in table. Unique key can take null values.Q16) Which of the following statement(s) is/are true for UPDATE in SQL?Select the correct option:A. 1, 3 and 4B. 2, 3 and 4C. 3 and 4D. 1 onlySolution: AOptions are self-explanatory.Q17) Which of the following is true for TRUNCATE in SQL?A. It is usually slower than DELETE commandB. It is usually faster than DELETE commandC. There is no comparison between DELETE & TRUNCATED. Truncate command can be rolled backE. None of theseSolution: BTRUNCATE is faster than delete bcoz truncate is a ddl command so it does not produce any rollback information and the storage space is released while the delete command is a dml command and it produces rollback information too and space is not deallocated using delete command.Q18) Which of the following statement is correct about CREATE TABLE command while creating a table?A. We need to assign a datatype to each columnB. We have flexibility in SQL. We can assign a datatype to column even after creating a tableC. It is mandatory to insert atleast a single row while creating a tableD. None of theseSolution: AEach column must possess behavioral attributes like data types and precision in order to build the structure of the table.Q19) Which of the following are the synonyms for column and row of a table?Select the correct option:A. 1 and 2B. 3 and 4C. Only 1D. Only 2Solution: AIn DBMS records are also known as tuple and rows. And columns are known as attributes and fields.Q20) Which of the following operator is used for comparing NULL values in SQL?A. EqualB. ISC. IND. None of AboveSolution: BIn SQL if you want to compare a null valueyou need to use IS statement.Q21) Which of the following statement(s) is/are true about HAVING and WHERE clause in SQL?Select the correct option:A. 1 and 3B. 1 and 4C. 2 and 3D. 2 and 4Solution: AHAVING is performed after GROUP BY. If you have to apply some conditions to get results. you need to use WHERE before group by.Q22) Identify, which of the following column A or C given in the below table is a Primary Key or Foreign Key?Note: We have defined Foreign Key and Primary Key in a single tableA. Column A is Foreign Key and Column C is Primary KeyB. Column C is Foreign Key and Column A is Primary KeyC. Both can be Primary KeyD. Based on the above table, we cannot tell which column is Primary Key and which is Foreign KeySolution: BColumn A is taking unique values and column A doesnt have null values. So it can be considered as Primary key of this table. Whereas B is the example of foreign key because all values present in this column are already present in column A.Q23) What are the tuples additionally deleted to preserve reference integritywhen the rows (2,4) are deleted from the below table. Suppose you are using ON DELETE CASCADE.Note: We have defined Foreign Key and Primary Key in single tableA. (5,2) , (7,2), (9,5)B. (5,2) , (7,2)C. (5,2) , (7,2), (9,5), (3,4)D. (5,2) , (7,2), (9,5),(6,4)Solution: AWhen (2,4) is deleted. Since C is a foreign key referring A with delete on cascade, all entries with value 2 in C must be deleted. So (5, 2) and (7, 2) are deleted. As a result of this 5 and 7 are deleted from A which causes (9, 5) to be deleted.Q24) Suppose you are given a table/relation EMPLOYEE which has two columns (Name and Salary). The Salary column in this table has some NULL values. Now, I want to find out the records which have null values.
What will be the output for the following queries?Query 1. SELECT * FROM EMPLOYEE WHERE Salary <> NULL;Query 2. SELECT * FROM EMPLOYEE WHERE Salary = NULL;A. Query 1 will give last 4 rows as output (excluding null value)B. Query 2 will give first row as output (only record containing null value)C. Query 1 and Query 2 both will give the same resultD. Cant saySolution: CIf we compare(<>, =) Salary it will give 0 records. Below are the following reasons:Q25) What is the difference between TRUNCATE, DELETE and DROP? Which of the following statement(s) is/ are correct?Select the correct option:A. 1 and 3B. 2 and 3C. 1 and 4D. 2 and 4E. None of the aboveSolution: AOptions are self-explanatory.Q26) Tables A, B have three columns (namely: id, age, name) each. These tables have no null values and there are 100 records in each of the table.Here are two queries based on these two tables A and B:Query1: SELECT A.id FROM A WHERE A.age > ALL (SELECT B.age FROM B WHERE B.name = 'Ankit')Query2: SELECT A.id FROM A WHERE A.age > ANY (SELECT B.age FROM B WHERE B.name = 'Ankit')Now, which of the following statement is correct for the output of each query?A. The number of tuples in the output of Query 1 will be more than or equal to the output of Query 2B. The number of tuples in the output of Query 1 will be equal to the output of Query 2C. The number of tuples in the output Query 1 will be less than or equal to the output of Query 2D. Cant saySolution: CANY and ALL operate on subqueries that return multiple values. ANY returns true if any of the subquery values meet the condition. But in case of ALL it will returns the records if all conditions are true. So options C is correct.Q27) What is true about relation (table) in different normal forms (1NF, 2NF, 3NF)?Select the correct option:A. 1 and 2B. 2 and 3C. 1 and 3D. 2 and 4Solution: BIf a relation is satisfying higher normal forms, it automatically satisfies lower normal forms also. For example, if a relation is satisfying kNF it will automatically satisfy gNF where g<=k.28) Suppose you want to compare three keys (Primary Key, Super Key and Candidate Key) in a database. Which of the following option(s) is/are correct?Select the correct option:A. 1 and 2B. 2 and 3C. 1 and 3D. 2 and 4E. 1, 2 and 3Solution: AOptions are self-explanatoryQ29) Consider a relation R with the schema R (A, B, C, D, E, F) with a set of functional dependencies F as follows:{AB->C, BC->AD, D->E, CF->B}Which of the following will be the output of DA+?Note: For any X, X+ is closure of X.A) DAB) DAEC) ABCDD) ABCDEFSolution: B(DA)+ = DAE30) Suppose you have a table Loan_Records. 
SELECT Count(*) FROM ( (SELECT Borrower, Bank_Manager FROM Loan_Records) AS S NATURAL JOIN (SELECT Bank_Manager, Loan_Amount FROM Loan_Records) AS T );What is the output of the following SQL query?A. 4B. 5C. 8D. 10Solution: BTemporary table S is given belowTemporary table T is given belowIf you apply natural join on both tables (S and T) and evaluate the condition on Bank_Manager. You will get the following intermediate table after applynatural joinSunderjan appears two times in Bank_Manager column, so their will be four entries with Bank_Manager as Sunderjan. So count(*) will give the 5 output in outer query.Q31) Is SELECT operation in SQL equivalent to PROJECT operation in relational algebra?A. Yes, both are equivalent in all the casesB. No, both are not equivalentSolution: BIn relational algebra PROJECT operation gives the unique record but in case of SELECT operation in SQL you need to use distinct keyword for getting unique records.Table: AV1Questions 32-36 are based on the above table.
Q32) What will be the output of following query?Query 1: Select name from AV1 where name like '%a%'Ans: BSolution: The query will search for records in column Name which will have atleast one a and Like operation is case sensitive so A will not be the answer. Hence B is true.Table: AV1Q33) What will be the output for the below query?Query: SELECT Name from AV1 where Name LIKE '%______%';Note: The above operation contains 6 underscores (_) used with LIKE operator.A. It will return names where number of characters in names are greater than or equals to 6B. It will return names where number of characters in names are greater than 6C. It will return names where number of characters in names are less than or equals to 6D. It will give an errorSolution: AThe query will search for records in column Name where the number of characters in names are greater than or equal to 6.Q34) What will be the output of the below query?Query: SELECT Company, AVG(Salary) FROM AV1 HAVING AVG(Salary) > 1200 GROUP BY Company WHERE Salary > 1000 ;A.B.C.D. None of theseSolution: DThis query will give the error because WHERE is always evaluated before GROUP BY and Having is always evaluated after GROUP BY
Q35) What will be the output for the below Query 1 and Query 2?Query 1: SELECT MAX(Salary) FROM AV1 WHERE Salary < (SELECT MAX(Salary) from AV1);
Query 2: WITH S AS (SELECT Salary, ROW_NUMBER() OVER(ORDER BY Salary DESC) AS RowNum FROM AV1) SELECT Salary FROM S WHERE RowNum = 2;A. Query 1 output = 1200 and Query 2 output =1200B. Query 1 output = 1200 and Query 2 output =1400C. Query 1 output = 1400 and Query 2 output =1200D. Query 1 output = 1400 and Query 2 output =1400Solution: ABoth queries will generate the second-highest salary in AV1 which is 1200. Hence A is right option.Q36) Consider the following relational schema.Students(rollno: integer, sname: string)Courses (courseno: integer, cname: string)Registration (rollno: integer, courseno: integer, percent: real)Now, which of the following query would be able to find the unique names of all students having score more than 90% in the courseno 107?A. SELECT DISTINCT S.sname FROM Students as S, Registration as R WHERE R.rollno=S.rollno AND R.courseno=107 AND R.percent >90B. SELECT UNIQUE S.sname FROM Students as S, Registration as R WHERE R.rollno=S.rollno AND R.courseno=107 AND R.percent >90C. SELECT sname FROM Students as S, Registration as R WHERE R.rollno=S.rollno AND R.courseno=107 AND R.percent >90D. None of theseSolution: AOption A is true, Option B will give the error (UNIQUE is not used in SQL) and in option C unique names will not be the output.Q37) Consider the relation T1 (A, B) in which (A, B) is the primary key and the relation T2 (A, C) where A is the primary key. Assume there are no null values and no foreign keys or integrity constraints.Now, which of the following option is correct related to following queries?Query 1: select A from T1 where A in (select A from T2)Query 2: select A from T2 where A in (select A from T1)A. Both queries will definitely give the same resultB. Both queries may give the same resultC. Both queries will definitely give a different resultD. None of theseSolution: BFor the same values ( values should be unique) for the column A in tables T1 and T2. Query 1 and Query 2 will give the same output. Hence B is trueQ38) Which of the following option is correct about following queries?Query 1. SELECT emp.id, department.id FROM emp NATURAL JOIN departmentQuery 2. SELECT emp.id, department.id FROM department NATURAL JOIN empA. Both queries will give same outputsB. Both queries will give different outputC. Need table structureD. None of theseSolution: AFor Natural joins, the order doesnt matter. The queries will return same results.Q39) Indexing is useful in a database for fast searching. Generally, B-tree is used for indexing in a database. Now, you want to use Binary Search Tree instead of B-tree.Suppose there are numbers between 1 and 100 and you want to search the number 35 using Binary Search Tree algorithm. Which of the following sequences CANNOT be the sequence for the numbers examined?A. 10, 75, 64, 43, 60, 57, 55B. 90, 12, 68, 34, 62, 45, 55C. 9, 85, 47, 68, 43, 57, 55D. 79, 14, 72, 56, 16, 53, 55Solution: CIn BST on right side of parent number should be greater than it, but in C after 47, 43 appears that is wrong.Q40) If anindex scan is replaced by sequential scan in SQL, then what will happen?Note: Number of observations is equal to 1 million.A. Execution will be fasterB. Execution will be slowerC. Execution will not be affectedD. None of theseSolution: BThe addition of the index made the query execution faster since the sequential scan is replaced by the index scan.Q41) Suppose you have a csv file which has 3 columns (User_ID, Gender, Product_ID) and 7,150,884 rows. You have created a table train from this file in SQL.Now, you run Query 1 as given below and get the following output:Query 1: EXPLAIN select * from train where Product_ID = 'P00370853';OUTPUT:QUERY PLAN
--------------------------------------------------------------
Seq Scan on train (cost=0.00..79723.88 rows=16428 width=68)
Filter: ((product_id)::text = 'P00370853'::text)
(2 rows)You have now created Product_ID column as an index in train table using the below SQL query:CREATE INDEX product_ID ON train(Product_ID)And, you run Query 2 (same as Query 1) on train and get the following output.Query 2: EXPLAIN select * from train where Product_ID = 'P00370853';OUTPUT:QUERY PLAN
-------------------------------------------------------------------------------
Bitmap Heap Scan on train (cost=829.53..40738.85 rows=35754 width=68)
Recheck Cond: ((product_id)::text = 'P00370853'::text)
-> Bitmap Index Scan on product_id (cost=0.00..820.59 rows=35754 width=0)
Index Cond: ((product_id)::text = 'P00370853'::text)
(4 rows)Which query will take less time to execute?A. Query 1B. Query 2C. Both queries will take the same timeD. Cant saySolution: BFor Query Plan for Query 1 execution time is 79723.88 and for Query Plan of Query 2 execution time is 40738.85. So Query 2 is taking less time.Q42) Suppose you have a CSV file which has 3 columns (User_ID, Gender, product_ID) and 7150884 rows. You have created a table train from this file in SQL.Now, you run Query 1 (mentioned below):Query1: EXPLAIN SELECT * from train WHERE product_ID like '%7085%';Then, you created product_ID columns as an index in train table using below SQL query:CREATE INDEX product_ID ON train(Product_ID)Suppose, you run Query 2 (same as Query 1) on train table.Query 2: EXPLAIN SELECT * from train WHERE product_ID like '%7085%';Let T1 and T2 be time taken by Query 1 and Query 2 respectively. Which query will take less time to execute?A. T1>T2B. T2>T1C. T1~T2D. Cant saySolution: CThe addition of the index didnt change the query execution plan since the index doesnt help for the LIKE query.Q43) Suppose you have a table Employee. In Employee table, you have a column named Salary. Now, you applied Query1 on Employee table.Query 1: SELECT * FROM Employee where Salary*100 > 5000;After that, you created an index on Salary columns and then you re-run the Query 2 (same as Query 1).Query 2: SELECT * FROM Employee where Salary*100 > 5000;Here, Query 1 is taking T1 time and Query 2 is taking T2 time.Which of the following is true for the queries time?A. T1 > T2B. T2 > T1C. T1 ~ T2D. Cant saySolution: CThe addition of the index didnt change the query execution plan. The index on rating will not work for the query (Salary * 100 > 5000). Theoretically it might work in this case, but obviously the system is not smart enough to work that way; But you can create an index on (Salary * 100) which will help.Q44) Suppose you are given a table words. The table has 2 columns id and word.
What will be the output for the below query?Query: select c1, c2, c3 from ( select id, lag(word) over (order by id) as c1, word as c2, lead(word) over (order by id) as c3 from words ) as t where c2 = Mining or c2 = Problems;A.B. ErrorC.D. None of theseSolution: AQ45) What is true for View in SQL?Select the correct option:A. 1 and 3B. 2 and 4C. 1, 3 and 4D. All of theseSolution: DAll options are correct.Q46) Suppose I created a table called avian using below SQL query:Query : CREATE TABLE avian ( emp_id SERIAL PRIMARY KEY, name varchar);Now, I want to insert some records in the table avian:Query1: INSERT INTO avian (name) VALUES(FRAZY');Query2: INSERT INTO avian (name) VALUES(ANKIT');Query3: INSERT INTO avian (name) VALUES('SUNIL');Query4: INSERT INTO avian (name) VALUES('SAURAV');Which of the following will be the output of the below query?Query: Select * FROM avian;A.
B.C. ErrorD. None of theseSolution: AAt the time of table creation Avian, we have used SERIAL for emp_id which autoincrement emp_id whenever you insert a record in table avian. Hence A is true.I hope you enjoyed taking the test and you found the solutions helpful. The test focused onconceptual knowledge of SQL.We tried to clear all your doubts through this article but if we have missed out on something then let me know in comments below. If you have any suggestions or improvements you think we should make in the next skilltest, let us know by dropping your feedback in the comments section.",https://www.analyticsvidhya.com/blog/2017/01/46-questions-on-sql-to-test-a-data-science-professional-skilltest-solution/
19 MOOCs on Mathematics & Statistics for Data Science & Machine Learning,Learn everything about Analytics|Introduction|Which courseis suitable for you?|Table of Content|Beginners Mathematics & Statistics|Intermediate Mathematics & Statistics|Advanced Mathematics & Statistics|End Notes,"1. Data Science Maths Skills|2.Intro to Descriptive Statistics|3.Intro to Inferential Statistics|4. Introduction to Probability and Data|5.Math is Everywhere: Applications of Finite Math|6.Probability: Basic Concepts & Discrete Random Variables|7.Mathematical Biostatistics Boot Camp 1|8.Applications of Linear Algebra Part 1|9.Introduction to Mathematical Thinking|1.Bayesian Statistics: From Concept to Data Analysis|2.Game Theory 1|3.Game Theory II: Advanced Applications|4.Introduction to Linear Models and Matrix Algebra|5.Advanced Linear Models for Data Science 1: Least Squares|6.Advanced Linear Models for Data Science 2: Statistical Linear Models|7. Maths in Sports|1.Discrete Optimization|2.Statistics for Genomic Data Science|3.Biostatistics for Big Data Applications|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|46 Questions on SQL to test a data science professional (Skilltest Solution)|Java/ Python Developer- Bangalore (2+years of experience)|
Analytics Vidhya Content Team
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Before creation, God did just pure mathematics. Then he thought it would be pleasant change to do some applied                                           -John Edensor LittlewoodMathematics & Statistics are the founding steps for data science and machine learning. Most of the successful data scientists I know of, come fromone of these areas computer science, applied mathematics & statistics or economics. If you wish to excel in data science, you must have a good understanding of basic algebra and statistics.However, learning Maths for people not having background in mathematics can be intimidating. First, you have to identify what to study and what not. The list can includeLinear Algebra, calculus, probability, statistics, discrete mathematics, regression, optimization and many more topics. What do you do? How deep to you want to get in each of these topics? It is very difficult to navigate through this by yourself.If you have faced this situation before  dont worry! You are at the right place now. I have done the hard work for you. Here is a list of popular open courses on Maths for Data science from Coursera, edX, Udemy and Udacity. The list has been carefully curated to give you a structured path to teach you the required concepts of mathematics used in data science.Get started now to learn & explore mathematics for data science.To help you navigate through the courses, I have divided the article into beginners, intermediate and advanced section. Choose your level of expertise in mathematics before delving further. Further, I have added the pre-requisites for each course. You can check if you know these topics before starting the course.Few courses may require you to finish the preceding course for better understanding. So, make sure that you either know the subject or have undergone these courses.Read on to find out the right course for you!Duration: 4 weeksLed by: Duke University (Coursera)If you are abeginner with very minimal knowledge of mathematics, then this course is for you. In this course, you will learn about concepts of algebra like set theory, inequalities, functions, coordinate geometry, logarithms, probability theory and many more. This course will take you through all the basic maths skills required for data science and would provide a strong foundation. The course starts from 9 Jan 2017 and is lead by professors from Duke University.Prerequisites: Basic maths skillsDuration: 8 weeksLed by: UdacityThis course by Udacity is an excellent beginners guide for learning statistics. It is fun, practical and filled with examples.The Descriptive Statistics course will first make you familiar with different terms of statistics and their definition. Then you will learn about statistics concepts like central tendency, variability, standard normal distribution and sampling distribution. This course doesnt require any prior knowledge of statistics and is open for enrollment.Prerequisites: NoneDuration: 8 weeksLed by: UdacityAfter you have gone through the Descriptive Statistics course, it is time for Inferential statistics. The same practical approach to the subject continues in this course.In this course, you will learn concepts of statistics like estimation, hypothesis testing, t-test, chi-square test, one-way Anova, two-way Anova, correlation, and regression. There are problem set and quiz questions after each topic. You will also be able to test your learning on a real-life dataset at the end of the course. The course is open for enrollment.Prerequisites: Completeunderstanding of Descriptive Statistics (the course mentioned above)Alternate Course: You can also look atStatistics: Unlocking the World of Data.It is a6 weeks long course run byUniversity of Edinburgh (edX)Duration: 5 weeksLed by: Duke University(Coursera)It will provide you hands on experience in data visualization and numeric statistics using R and RStudio.The course will first take you through basics of probability and data exploration to give a basic understanding to get started. Then, it will individually explain various concepts under each topic in detail. Atthe end, you will be testedon a data analysis project using a real-world dataset.The course is led by a Professor in Statistics at Duke University and is also a prerequisite for Statistics in R specialization. If you are looking forward to learn R for data science, then you must take this course. The course is open for enrollment.Prerequisites: Basic Statistics and knowledge of RDuration: 1 weekLed by: Davidson College (Udemy)As the name suggests, this course tells you how maths is being used everywhere from Angry birds to Google. It is a fun approach to applied mathematical concepts.In this course, you will learn how equation of lines is used to create computer fonts, how graph theory plays a vital role in angry birds, linear systems model the performance of a sports team and how Google uses probability and simulation to lead the race in search engines.The course is led by themathematics professor at Davidson College and is open for enrollment.Prerequisites:Understanding of linear algebra and programmingDuration: 6 weeksLed by: Purdue University (edX)This course is designed for anyone looking for a career in data science & information science. It covers essentials of mathematical probabilities.In this course, you will learnthe basic concepts of probability, random variables, distributions, Bayes Theorem, probability mass functions and CDFs, joint distributions and expected values.Once you are familiar with the basics, you will learn about advanced concepts Bernoulli and Binomial distributions, Geometric distribution, Negative Binomial distribution, Poisson distribution, Hypergeometric distribution and discrete uniform distribution.After taking this course you will have athorough understanding of how probability is used in everyday life. The course is open for enrollment.Prerequisite: Basics StatisticsDuration: 4 weeksLed by: Johns Hopkins University(Coursera)Honestly, the Bio in Biostatistics is misleading. This course is all about fundamental probability and statisticstechniques for data analysis. The course covers topics on probability, expectations, conditional probabilities, distributions, confidence intervals, bootstrapping, binomial proportions, and logs. A prior knowledge of linear algebra and programming will be advantageous but not mandatory to begin with this course. The course starts from 16 Jan 2017 and is led by biostatistics professor at Johns Hopkins University. A well-paced course with acomplete introduction to mathematical statistics.Prerequisites: Basic Linear algebra, calculus and programming useful but not mandatoryDuration: 5 weeksLed by: Davidson College (edX)This is an interesting course on applications of linear algebra in data science.The course will first take you through fundamentals oflinear algebra. Then, it will introduce you to applications of linear algebra for recognizing handwritten numbers, ranking of sports team along with online codes.The course is open for enrollment.Prerequisite:Basic linear algebraDuration: 8 weeksLed by: Stanford University(Coursera)In this mathematical thinking course from Stanford, you will learn how to develop analytical thinking skills. The course teaches you interesting ways to develop out-of-the-box thinking and helps you remain ahead of the competitive curve.In this course, you will learn about analysis of a language, quantifiers, brief introduction to number theory and real analysis. To make the most of this course one must have familiarity with algebra, number system and elementary set theory.The course starts from 9 Jan 2017 and is led by professors at Stanford. It is open for enrollment.Prerequisites: Basic algebra, number system and elementary set theory.By this time, you knowall the basic concepts a data scientist needs to know. This is the time to take your mathematical knowledge to the next level.Duration: 4 weeksLed by: University of California (Coursera)Bayesian Statistics is an important topic in data science. For some reason, it does not get as much attention.In this course, the first section covers basic topics like probabilitylike conditional probability, probability distribution and Bayes Theorem. Then you will learn about statistical inference for both Frequentist and Bayesian approach, methods for selecting prior distributions and models for discrete data and Bayesian analysis for continuous data.Prior knowledge of statistics concepts is required to take this course. The course starts from 16 Jan 2017.Prerequisite: Basic & Advanced StatisticsDuration: 8 weeksLed by: Stanford University and University of British Columbia (Coursera)Game theory is an important component of data science.In this course, you will learn about basics of game theory and its applications. If you are looking to master Re-inforcement learning this year  this course is a must learn for you.The course provides basic understanding of representing games and strategies, the extensive form (which computer scientists call game trees), Bayesian games (modeling things like auctions), repeated and stochastic games. Each concepthas been explained with the help of examples and applications. The course is led by professors from the Stanford University and The University of British Columbia.The course is open for enrollment.Prerequisite: Basic probability and mathematical thinkingDuration: 5 weeksLed by: Stanford University and The University of British Columbia (Coursera)After going through the basics of Game theory in the previous course, this course is on the advanced applications of game theory. You will learn about how to design interactions between agents in order to achieve good social outcomes. The three main topics covered are social choice theory, mechanism design, and auctions. The course starts from 30 Jan 2017 and is led by professors fromStanford University & The University of British Columbia.The course is open for enrollment.Prerequisite: Basics of Game TheoryDuration: 4 weeksLed By: Harvard University (edX)Matrix algebra is used in various tools for experimental design and analysis of high-dimensional data.For easy understanding, the course has been divided intoseven parts to provide you a step by step approach. You will learn about matrix algebra notation & operations, application of matrix algebra to data analysis, linear models and QR decomposition.The language used throughout the course is R. Feel free to choose which part of the course caters more to your interest and take the course accordingly.The course is conducted by biostatistics professors at Harvard University and is open for enrolment now.Prerequisite: Basic Linear algebra and knowledge of RDuration: 6 weeksLed by: Johns Hopkins University (Coursera)This course is a two part series for advanced linear statistical learning models. For all those who have an understanding of regressions models and are looking to explore this topic further must take this course.In this course, you will learn about one & two parameter regression, linear regression, general least square, least square examples, bases & residuals.Before you proceed further let me clear, to take this course you need abasic understanding of linear algebra & multivariate calculus, statistics & regression models, familiarity with proof based mathematics and working knowledge of R. The course starts from 23 Jan 2017.Prerequisite:Linear Algebra, calculus, statistics and knowledge of RDuration: 6 weeksLed by: Johns Hopkins UniversityThis is the second part of the course on advanced linear statistical learning models. For all those who have anunderstanding of regressions models and are looking to explore this topic further must take this course. In this course, you will learn about basics of statistical modeling multivariate normal distribution, distributional results, and residuals.Before you proceed further let me clear, to take this course you need basic understanding of linear algebra & multivariate calculus, statistics & regression models, familiarity with proof based mathematics and working knowledge of R. The course starts from 23 Jan 2017.Prerequisite:Linear Algebra, calculus, statistics and knowledge of RDuration: 8 weeksLed by: University of Notre Dam (edX)I am someone who is very curious to know how mathematics can be used to drive deeper insights in sports and everyday life.I came across this course, which shows how your favorite sport uses mathematics to analyze data and know the trends, performance ofplayers and their fellow teams.In this course, you will learn how inductive reasoning is used in mathematical analysis, how probability is used to evaluate data, assess the risk and outcomes of any event.All the major team sports, athletic sports, and even extreme sports like mountain climbing have been covered in the course. The course is led by professors of theUniversityof Notre Dam and is currently open for enrolment.Prerequisite: Statistics & Linear AlgebraBravo, by now  you would be on your own. You would have developed a knack for mathematics & statistics and would feel confident about continuous learning  way to go!Duration: 8 weeksLed by: University of Melbourne (Coursera)Every industry & company makes use of optimization. Airlines use optimization to ensure fixed turn-around-time, E-commerce like Amazon uses optimization for on time delivery of products. Macro-level applications of optimization includesdeploying electricity to millions of people, way for new medical drug discoveries and many more.This course provides you a complete understanding ofdiscrete optimization and it is being used in our everyday lives. First, it will take you through fundamental basics of discrete optimization and its various techniques. You will learn about constraint, linear and mixed integer programming.The last section of the course includes advanced topics on optimization.The prerequisites to take this course aregood programming skills, knowledge of fundamental algorithms, and linear algebra. The course starts from 16 Jan 2017 and is conducted by professors at Melbourne University.Prerequisite: Programming, algorithms and linear algebraDuration: 4 weeksLed by: Johns Hopkins UniversityIf you aspire to become a generation sequencing data scientist then you must take this course.In this course, you will learn about exploratory analysis, linear modeling, hypothesis testing & multi-hypothesis testing,different types of processlike RNA-seq, GWAS, ChIP-Seq, and DNA Methylation studies. This course is part of Genomic Data Scientist specialization from Johns Hopkins. The course starts from 16 Jan 2017.This course is part of Genomic Data Scientist specialization from Johns Hopkins. The course starts from 16 Jan 2017.Prerequisite: Advanced Statistics and algorithmsDuration: 8 weeksLed by: utmb Health (edX)This course is an introduction to data analysis using biomedical big data.In this course, you will learn aboutfundamental components of biostatistical methods. Working with biomedical big data can pose various challenges for someone not familiar with statistics.Learn how basic statistics is used in biomedical data types. You will learn about basics of R programming, how to create & interpret graphical summaries of data and inferential statistics for parametric & non-parametric methods. It will provide you hands on experience in R with biomedical problem types.The course is open for enrolment.Prerequisite: Advanced statistics and knowledge RI hope you found this article useful. By now, you would have identified the learning areas for yourself.If you are from mathematics background, you can choose the right courses for yourself. On the other hand, if you do not have a mathematics background, then start from the beginners sections and move ahead.For those of you, whohave taken any of these courses, let us know your feedback about them. Share your opinions with me and other users through comments below. Through this article I wanted to provide you a list of resources available at your disposal in mathematics for data science. Hope you make good use of them.",https://www.analyticsvidhya.com/blog/2017/01/19-mooc-mathematics-statistics-datascience-machine-learning/
Java/ Python Developer- Bangalore (2+years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|19 MOOCs on Mathematics & Statistics for Data Science & Machine Learning|How to create Beautiful, Interactive data visualizations using Plotly in R and Python?|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : Ability to work in a team
Task Info : Main Responsibilities: Responsibilities include designing and developing Java/Python applications in the field ofData Analytics Contribute in all phases of the development lifecycle Write well designed, testable and efficient code Prepare and produce releases of software components Support continuous improvement by investigating alternatives and technologiesRequired Skills: Hands on experience in designing and developing applications in Java/Python Knowledge in Big Data, Machine Learning or any Data Analysis tool will be an addedadvantage Understanding of code versioning tools, such as Git Translate application storyboards and use cases into functional applications Knowledge of Design Patterns Ability to work in a team
College Preference : no-bar
Min Qualification : ug
Skills : bigdata, data analysis, java, machine learning, python
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/java-python-developer-bangalore-2years-of-experience/
"How to create Beautiful, Interactive data visualizations using Plotly in R and Python?",Learn everything about Analytics|Introduction|Table of Contents|1.What is Plotly?|2. Advantages and Disadvantages of Plotly.|3. Steps for creating plots in Plotly.|4. Setting up Data|5. Basic Visualization|6. Advanced Visualization|7. Using plotly with ggplot2||8. Different versions of Plotly.|End Notes,"4.1 Iris Data|4.2 International Airline Passengers Dataset|4.3 Volcano Dataset|5.1 Histograms|5.2 Bar Charts|5.3 Box Plots|5.4 Scatter Plots|5.5 Time Series Plots|6.1 Heat Maps|6.2 3D Scatter Plots|6.3 3D Surfaces|Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|Java/ Python Developer- Bangalore (2+years of experience)|Welcome 2017  Are you prepared for a year of data based disruption?|
Saurav Kaushik
|21 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",In R|In Python|In R:|In Python|In R|In Python|In R|In Python|In R|In Python|In R|In Python|In R|In Python|In R|In Python|In Python|In R|In Python|In R|In Python|In R|In Python|In R|In Python,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The greatest value of a picture is when it forces us to notice what we never expected to see.John TukeyData visualization is an art as well as a science. It takes constant practice and efforts to master the art of data visualization. I always keep exploring how to make my visualizationsmore interesting and informative. My main tool for creating these data visualizations had been ggplots. When I started using ggplots, I was amazed by its power. I felt like I was now an evolved story teller.Then I realized that it is difficult to make interactive charts using ggplots. So, if you want to show something in 3 dimension, you can not look at it from various angles. So my exploration started again. One of the best alternatives, I found after spending hours was to learn D3.js. D3.js is a must know language if you really wish to excel in data visualization. Here, you can find a great resource to realize the power of D3.js and master it.But I realized that D3.js is not as popular in data science community as it should have been, probably because it requires a different skill set (for ex. HTML, CSS and knowledge of JavaScript).Today, I am going to tell you something which will change the way you perform data visualizations in the language / tool of your choice (R, Python, MATLAB, Perl, Julia, Arduino).Plotly is one of the finest data visualization tools available built on top of visualization library D3.js, HTML and CSS. It is created using Pythonand the Djangoframework. One can choose to create interactive datavisualizations online or use the libraries that plotly offers to create these visualizations in the language/ tool of choice. Itis compatible with a number of languages/ tools: R, Python, MATLAB, Perl, Julia, Arduino.Lets have a look at some of the advantages and disadvantages of Plotly:Advantages:Disadvantages:Data visualization is an artwith no hard and fast rules.Onesimply should do what it takes to convey the message to the audience.Hereis a series of typical steps for creating interactive plots using plotlyIn R:In Python:4. Adding the layout fields like plot title axis title/ labels, axis title/ label fonts, etc.In R:In Python:In RIn PythonSince R and Python are two of the most popular languages among data scientists, Ill be focusing on creating interactive visualizations using these two languages.For performing a wide range of interactive data visualizations, Ill be using some of the publicly available datasets. You can follow the following code to get the datasets that Ill be using during the course of this article :You can get International airline passengers dataset here .You can get International airline passengers dataset here.To get a good understanding of when you should use which plot, Ill recommend you to check out this resource.Feel free to play around and explore these plots more. Here are a few things that you can try in the coming plots:;You can view the interactive plot here.You can view the interactive plot here.You can view the interactive plot here.Lets start with a simple scatter plot using iris dataset.You can view the interactive plot here.You can view the interactive plot here.2. We can add another dimension (Petal Length) to the plot by using the size of each data point in the plot.You can view the interactive plot here.You can view the interactive plot here.Till now, we have got a grasp of how plotly can be beneficial for basic visualizations. Now lets shift gears and see plotly in action for advanced visualizations.You can view the interactive plot here.You can view the interactive plot here.You can view the interactive plot here.ggplot2 is one of the best visualization libraries out there. The best part about plotly is that it can add interactivity to ggplotsand also ggplotly() which will further enhance the plots. For learning more about ggplot, you can check out this resource.Lets better understand it with an example in R.You can view the interactive plot here.Plotly offers four different versions, namely:Each of these versions is differentiated based on pricing and features. You can learn more about each of the versions here. The community version is free to get started and also provides decentcapabilities. Butone major drawback of community version is theinability to create private plots that to share online. Ifdata security isa prominent challenge for an individual or organisation, either of personal, professional or on-premise versions should be opted based upon the needs. For the above examples, I have used the community version.After going through this article, you would have got a good grasp of how to create interactive plotly visualizations in R as well as Python. I personally use plotly a lot and find it really useful. Combining plotly with ggplots by using ggplotly() can give you the best visualizations in R or Python. But keep in mind that plotly is not limited to R and Python only, there a lot of other languages/ tools that it supports as well.I believe this article has inspired you to use plotly for data visualization tasks. Did youIf you have any questions / doubts, do let me know in the comments below. If you enjoyed reading this article? Do share your views in the comment section below.",https://www.analyticsvidhya.com/blog/2017/01/beginners-guide-to-create-beautiful-interactive-data-visualizations-using-plotly-in-r-and-python/
Welcome 2017  Are you prepared for a year of data based disruption?,Learn everything about Analytics|The changing landscape|2017 for Analytics Vidhya|Kickstart January 2017|What are you doing in 2017?,"Learn, compete, hack and get hired!|Share this:|Like this:|Related Articles|How to create Beautiful, Interactive data visualizations using Plotly in R and Python?|Data Scientist- Marketing- Bangalore (4-9 Years of Experience)|
Kunal Jain
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are in exciting and challenging times. The pace of change in data science industry is increasing by the day. It is difficult to understand the kind of impact data science would make to our life. I happened to be in my home town on the new years eve and caught up with some old buddies. One of them was a Chartered Accountant (CA), another one had his own clothing business and one of them was in professional education.Surprisingly, none of them realized the impact data science and automation would have on their work. The CA believed that accounting requires too many human interventions to be automated even in next 5  10 years, the friend running clothing business had not heard about Internet of Things and the one in professional education said that professional education can not be moved to Open Courses as they dont make people industry ready. None of them was thinking data either as an opportunity or a threat.You could imagine how the next few hours would have gone with me being in the room!Take a step back and think what all happened in 2016.AlphaGo defeating Lee Sedol, Tesla adding self driving capabilities to its cars, chat bots revolutionising the way we interact with our customers, TensorFlow grew in popularity, early AI assistants in form of Google Assistant, Alexa, Cortana & Siri  these are just a few examples which come totop of the mind.I am sure 2017 would be even more eventful. Data generation and number of connected devices would only increase. We would see better IoT platforms and their mass usage later in the year. We will see increasing number of AI bots coming out which can act as meaningful assistants. We arestill some time away from a generic agent, but there are enough specific applications for an AI assistant. Sales of Amazon Alexa Dot during Christmas goes on to show some early signs of success.The bottom line is irrespective of whether you like itor not, data science, machine learning & artificial intelligence will change your life and 2017 would be the first year when a laymanstart realizing the depth of this impact.This changing landscape means a huge opportunity for Analytics Vidhya. Not only we will see more and more professionals entering in the industry, but we will also see the increased pace of up-skilling. Which means that we will continue to help our community members learn about the new tools and techniques. You will see more guides and tutorials coming on new areas like IoT, Artificial Intelligence and other data related domains.But this year, we will take our engagement with you miles ahead. We will challenge you to participate in various competitions, skill tests and hackathons. And we will continue to find you the best jobsfor you in the industry. We will provide you more opportunities to interact with the industry experts  our meetups, webinars and learnups will continue to do this for you.This year, we will not measure our success on our penetration or the traffic on the website. We will measure our success by the number of users we engaged with and made a difference in their career.We are all set to define the momentum for 2017 this month. We plan to bring you awesome guides, hackathons, jobs, webinars and meetups for you in January. You can check out the contests coming up here.We will start the year with Skilltest on SQL and a Offline workshopin Chennai. SQL is a must have skill for every data professional and the event in Chennai is meant to provide mentorship and guidance to beginners in Chennai.Later in the month, we have Strategic Ball and Deep Learning skill test. If you are a student from India, do check out Data Tales competition from Great Lakes Institute of Management.I am sure we will get you started with your learning and agenda. But, we would want to know, what are your plans for 2017? What areas do you plan to learn? Which tools and techniques will you plan to learn? What kind of competitions are you looking out for?Do let us know through the comments below and we will make sure that we help you out in every possible manner through out the year.Hope you have a rocking year 2017!",https://www.analyticsvidhya.com/blog/2017/01/welcome-2017-what-are-you-doing-this-year/
Data Scientist- Marketing- Bangalore (4-9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Welcome 2017  Are you prepared for a year of data based disruption?|Data Scientist- Mumbai (2-4 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  4 years
Requirements : 
Task Info : Responsibilities:a) Work with the team and business users in analyzing requirements, assisting in design tasks, developing MIS reports & dashboards.b) Identifying reuse opportunities, creating and maintaining standards and best practice guidelines.c) Prepare Unit test plan, test scripts and participate in SIT phase.d) Take active role in presales activities.Primary Skills:a) Handson experience in any of the BI tools.b) Exposure to OpenSource BI tools is advantageous.c) Proficiency in SQL, PL/SQL & scripting.d) Good understanding of DWH concepts.e) Exposure to publishing of reports / dashbaords between environments and scheduling of jobs.Secondary Skills:a) Understanding of SDLC processes.b) Retail & FMCG or BFS domain exposure will be a value add.c) Exposure to any of the ETL tool will be an added advantage.Soft Skills:a) Good verbal & written communication skills.b) Good interpersonal skills.Educational Background:B.E. / B.Tech / MCA / B.ScExperience Range:3 yrs to 4 yrs.
College Preference : no-bar
Min Qualification : ug
Skills : business intelligence, etl, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-marketing-bangalore-4-9-years-of-experience-2/
Data Scientist- Mumbai (2-4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Marketing- Bangalore (4-9 Years of Experience)|Machine Learning engineer- Delhi/NCR (3+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 	Good communication skills
	Education Background  Any graduation / Any post graduate
	Willingness to work with a start up
Task Info : At least two years of hands on experience on Python / JavaExperience in R will be added advantageWillingness to work with a start upLooking for self starters with can do attitudeGood communication skillsEducation BackgroundAny graduation / Any post graduate
College Preference : no-bar
Min Qualification : ug
Skills : java, python, r
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-mumbai-2-4-years-of-experience/
Machine Learning engineer- Delhi/NCR (3+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Mumbai (2-4 years of experience)|Data Scientist- Delhi/NCR (3+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : We are the SaaS based data & analytics firm working with brands to empower them with actionable data driven insights. Our platform, strategic analysis, data libraries & services allows brands to stay ahead of their competition through a data-driven marketing approach. Be it social media (Facebook, Twitter, Youtube, Instagram etc.), digital coverage, sales related metrics (e-commerce, Google Analytics etc.), e-mail Marketing such as MailChimp, we enable brands to connect the various dots across digital. We are currently looking for a qualified candidate to join as a Machine Learning Engineer.You will be responsible for developing core predictive and sentiment analytics for brands in the Marketing Space. Focus will be on using the best of what lies at the forefront of technology and data science to address complex, real-world digital marketing challenges.Responsibilities:Introduce and implement diverse Machine Intelligence and Data Engineering disciplines.Drive product development and ensure timely deployment/deliverables.Ability to work on projects both collaboratively as a part of a multi-disciplinary team and individually.Analyze and understand the disparate sources of industry dataApply data-mining and machine learning techniques to large structured and unstructured datasetsCollaborate closely with the product team and other engineers to translate business objectives into new features and productsRequirements:Experience in implementing Sentiment Analysis tools.M.S. or Ph.D. in a quantitative field (e.g. computer science, mathematics, physics) or 3+ years experience with applied machine learning, algorithm development, and analyticsExperience in dealing with structured, unstructured and real-time dataScala, Java, PythonExperience with NLP is a must; recommendation engines is a plusWorked on NoSQL databases like MongoDB, Elasticsearch, Cassandra etc.Other skills wed appreciate:Experience on working with Digital related data eg. Facebook, Twitter etc.Experience with large scale data analysis (e.g. Spark, Hadoop/MapReduce)Experienced with any one of the following  Go, Java, Python, R, Scala, Hive, Pig etc.Working experience with these is a plus  ElasticSearch, Kibana, Spark, Solr, Hadoop, Hive, Storm, Mahout etc.
College Preference : no-bar
Min Qualification : pg
Skills : hadoop, java, machine learning, mapreduce, nlp, nosql, python
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/machine-learning-engineer-delhincr-3-years-of-experience/
Data Scientist- Delhi/NCR (3+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Machine Learning engineer- Delhi/NCR (3+ Years Of Experience)|Business Analyst/Senior Business Analyst & AM  Gurgaon (2-7 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : We are the SaaS based data & analytics firm working with brands to empower them with actionable data driven insights. Our platform, strategic analysis, data libraries & services allows brands to stay ahead of their competition through a data-driven marketing approach. Be it social media (Facebook, Twitter, Youtube, Instagram etc.), digital coverage, sales related metrics (E-Commerce, Google Analytics etc.), e-mail Marketing such as MailChimp, we enable brands to connect the various dots across digital. We are currently looking for a qualified candidate to join the organization as a Data Scientist.You will be responsible for developing core predictive and decision analytics. Focus will be on using the best of what lies at the forefront of technology and data science to address complex, real-world digital marketing challenges.Responsibilities:Introduce and implement diverse Machine Intelligence and Data Engineering disciplines.Drive product development and ensure timely deployment/deliverables.Ability to work on projects both collaboratively as a part of a multi-disciplinary team and individually.Analyze and understand the disparate sources of industry dataApply data-mining and machine learning techniques to large structured and unstructured datasetsCollaborate closely with the product team and other engineers to translate business objectives into new features and productsRequirements:M.S. or Ph.D. in a quantitative field (e.g. computer science, mathematics, physics) or 3+ years experience with applied machine learning, algorithm development, and analyticsExperience in dealing with structured, unstructured and real-time dataScala, Java, PythonExperience in working with Matlab/R or a similar numerical analysis toolWorked on NoSQL databases like MongoDB, Elasticsearch, Cassandra etc.A deep understanding of predictive analytics and modeling.Other skills wed appreciate:Experience on working with digital related data eg. Facebook metrics, Twitter metrics etc.Experience with large scale data analysis (e.g. Spark, Hadoop/MapReduce)Experienced with any one of the following  Go, Java, Python, R, Scala, Hive, Pig etc.Working experience with these is a plus  ElasticSearch, Kibana, Spark, Solr, Hadoop, Hive, Storm, Mahout etc.Bayesian Networks, SPN, SVM-C|R, HMMs, Logistic Regression
College Preference : no-bar
Min Qualification : pg
Skills : hadoop, java, logistic regression, mapreduce, matlab, nosql, predictive modeling, python, r, spark
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-delhincr-3-years-of-experience/
Business Analyst/Senior Business Analyst & AM  Gurgaon (2-7 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Delhi/NCR (3+ Years Of Experience)|Senior Data Scientist- Bangalore (4-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  7 years
Requirements : Excellent communication skills are required
Should be good in logical thinking
Should be a team player
Task Info : Qualification and Skills Required-Excellent communication skills are requiredExperience: 2yrs to 7yrsShould have experience in SAS Analytics profileShould be good in logical thinkingShould be a team playerAny graduate/ any post graduateShift Window: 11am to 8:30pm
College Preference : no-bar
Min Qualification : ug
Skills : business analysis, sas
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/business-analystsenior-business-analyst-am-gurgaon-2-7-years-of-experience/
Senior Data Scientist- Bangalore (4-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst/Senior Business Analyst & AM  Gurgaon (2-7 years of experience)|Engagement Manager- Bangalore (6-7 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  5 years
Requirements : 
Task Info : 4-5 years of experienceIn-depth experience in analytics and applicationsExpertise in languages like R, Python etc.From top recognizable company brands is a big plus
College Preference : no-bar
Min Qualification : ug
Skills : analytics, python, r
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/senior-data-scientist-bangalore-4-5-years-of-experience/
Engagement Manager- Bangalore (6-7 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Data Scientist- Bangalore (4-5 Years Of Experience)|Analyst- Ahmedabad (1-2 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  7 years
Requirements : 
Task Info : From top consulting/analytics firms like ZS, Mu Sigma, Fractal etc.Min. 6-7 years of experienceShould be a really good problem solver and be able to scope out and execute projects
College Preference : no-bar
Min Qualification : ug
Skills : 
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/engagement-manager-bangalore-6-7-years-of-experience/
Analyst- Ahmedabad (1-2 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Engagement Manager- Bangalore (6-7 Years Of Experience)|Assistant manager- Analytics  Gurgaon & Chennai (2-4 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : 
Task Info : ResponsibilitiesInterpret calling output for data qualityRun DM selection and measurement scenariosCreate power points for customers, internal and externalQualification and Skills RequiredSomeone with SQL and power point knowledge.Engineering degree or MBA with 1-2 year exp.PreferredPrior analysis experience must, in any industry
College Preference : no-bar
Min Qualification : ug
Skills : analytics, sql
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/analyst-ahmedabad-1-2-years-of-experience/
Assistant manager- Analytics  Gurgaon & Chennai (2-4 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analyst- Ahmedabad (1-2 years of experience)|Senior Manager- Analytics- Bangalore (4-10 years of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  4 years
Requirements : 
Task Info : ResponsibilitiesOwn model development & management; Plan, build, and improve modelsUse appropriate data, processes & techniques for building high quality modelsCommunicate & mitigate model risksRegularly conduct model monitoringCreate proper model documentation and get appropriate approvalsPlan strategy scenarios; Plan & run test & controlProactively look at data trends, and share & discuss key findingsDeep dive into data and provide actionable recommendationsScope of WorkBuilding, improving, monitoring various types of modelsConduct high quality business analysis through statistical techniquesProvide actionable recommendations for business problemsKey Results AreasHigh quality of model building, managing and monitoringPertinent & useful recommendations to business problemsKey relationshipsAnalytics teamBusiness and data teamFunctional HeadsQualification and Skills Required2+ years of experienceExperience of building models and conducting complex analyses requiredPost graduate in a Mathematical discipline (Mathematics including Statistics, Physics, Economics)Graduate in a Mathematical/Engineering discipline from tier-I collegeSkills:SAS or RAbility to build and manage models single handedlyIndependently able to deliver resultsAble to get to results with only high level inputsCan solve fairly complex analysis/modelling problemsTimely provide all analysis to meet all deadlinesAble to interpret, share and discuss results in the business contextCommunicate results and issues clearly to seniors and stakeholdersManage and prioritize own work stream
College Preference : tier1-any
Min Qualification : ug
Skills : modeling, r, sas, statistics
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/assistant-manager-analytics-gurgaon-chennai-2-4-years-of-experience/
Senior Manager- Analytics- Bangalore (4-10 years of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Assistant manager- Analytics  Gurgaon & Chennai (2-4 years of experience)|AM- Business Anlaytics- Gurgaon (3-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  10 years
Requirements : 
Task Info : Qualification and Skills RequiredInteract frequently with the onsite team and work out the execution planDecide the approach while thinking through possible deadlocks in advanceTypically ourbusiness problems cannot be solved using a linear/single line of thought.Some days, you would find yourself deeply engrossed in debugging a code, writing a program while on other days you could be designing an algorithm or defining the right architectureIn short, a day would involve hands-on delivery  be it brainstorming on a project, or writing some algorithm in SAS/R/Java or whatever technology it takesExpertise in proposals, directing the team below him for deliveryExtensive experience with analytics delivery across domainsBelief in the fail fast and learn way of doing things rather than sitting out of the processAn open mind and pleasing personality with good comfort level in interacting and motivating people from diverse backgroundsA strong appreciation for analytics, math and technologyWillingness to do an honest and continuous evaluation and development of your leadership skillsQualifications-Strong background in advanced analytics and business domainsExperience of 4-10 yearsMu Sigma/Axtria experience is highly preferred
College Preference : no-bar
Min Qualification : ug
Skills : analytics, java, r, sas
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/senior-manager-analytics-bangalore-4-10-years-of-experience/
AM- Business Anlaytics- Gurgaon (3-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Manager- Analytics- Bangalore (4-10 years of experience)|AM- Fraud Analytics- Gurgaon (3-6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : Qualification and Skills Required-Should be proficient with SAS, at least 2-3 years of experienceAt least 2 years of experience on modelingExcellent communication skillsExperience in any domainExperience  3-6 years
College Preference : no-bar
Min Qualification : ug
Skills : business analytics, modeling, sas
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/am-business-anlaytics-gurgaon-3-6-years-of-experience/
AM- Fraud Analytics- Gurgaon (3-6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|AM- Business Anlaytics- Gurgaon (3-6 Years Of Experience)|Senior Consultant- Ahmadabad (4-8 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  6 years
Requirements : 
Task Info : Qualification and Skills RequiredShould have worked in Banking domain for at least 2 yearsExperience on SAS for at least 2 yearsExcellent communication skillsExperience- 3-6 years
College Preference : no-bar
Min Qualification : ug
Skills : analytics, banking, sas
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/am-fraud-analytics-gurgaon-3-6-years-of-experience/
Senior Consultant- Ahmadabad (4-8 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|AM- Fraud Analytics- Gurgaon (3-6 Years of Experience)|Data Software Engineer- Ahmedabad (3+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  8 years
Requirements : 
Task Info : Responsibilities-Should have the ability to clearly understand business requirements, propose solutions and address organizational needs.Deliver strategic, polished and complete analysis to clients.Coordinate communication between the clients and the internal team of analysts.Project monitoring and provide strategic inputs. Regularly monitor and ensure each project is completed as per defined timelines and without any post production errors / issues.Liaise with Senior Management in key business areas impacted by strategy changes.Cross sell experience and build strong relations with stakeholders.Build & own an IQR Brand.Perform other duties as assignedQualification and Skills Required5  8 years of relevant industry experience is mandatory.Extensive client facing experience of 3  4 yrs. with knowledge on Stat Modelling and familiarity with SQL.Past experience in service delivery of Analytics projects is an advantage.Experience in analytics consulting is must, preferred in banking/credit card/other financial portfoliosExpert knowledge of MS Excel, basic knowledge of MSSQLExpert analytical skills with the ability to think creatively and develop new solutionsExperience in Portfolio Segmentation, Loyalty Analytics, Direct Marketing Campaign Response and Performance tracking, Predictive Modeling.Must have hands on expertise in extensive Data Analysis, Scorecard Development using Logistic/Linear Regression, Segmentation.Experience as a Subject Matter Expert on a few areas of Marketing/Credit Card Portfolio is a plus.Knowledge of statistics and data presentation methodology, hands-on preferableExcellent written and verbal communication skills; ability to communicate with clients and senior management to facilitate accurate business decisions with qualitative analysisAbility to provide excellent customer service and establish and maintain credibility and interpersonal relationships with diverse group of individuals (e.g. business leaders, colleagues, staff at all levels)
College Preference : no-bar
Min Qualification : ug
Skills : banking, data analysis, excel, linear regression, logistic regression, predictive modeling, segmentation, sql, statistical modeling
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/senior-consultant-ahmadabad-4-8-years-of-experience/
Data Software Engineer- Ahmedabad (3+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Consultant- Ahmadabad (4-8 Years Of Experience)|Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.We are the Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.Competencies and Skills.Help in building an enterprise-level data processing and analytics platformResponsibilities Manage data extract and ingest issues from partner data sources including all flavors of database servers and technologies Provide SQL expertise in developing ETL transformations for our internal data model Manage data movement throughout our AWS infrastructure including building data science models and standing up end user applicationsTechnologies ETL Tools Data warehouse design Python Java ScalaExperience: Experience in writing complicated database queries in the SQL language. Expertise in data warehouse development Prior experience working with a globally distributed product and engineering team
College Preference : no-bar
Min Qualification : ug
Skills : aws, etl, java, python, sql
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-software-engineer-ahmedabad-3-years-of-experience/
Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Software Engineer- Ahmedabad (3+ Years Of Experience)|Data Analyst- MSBI- Bangalore (2-3 Years Of experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  years
Requirements : 
Task Info : 
College Preference : tier1-any
Min Qualification : ug
Skills : analytics
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/digital-marketing-strategist-ahmedabad-6-years-of-experience-2/
Data Analyst- MSBI- Bangalore (2-3 Years Of experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience)|Risk Analyst/ Business analyst- Delhi (5+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : 
Task Info : Job responsibilities: Perform complex data analysis, design, support, and documentation. Help translate business problems to analytical frameworks: extract insights from consumer, sales and other data available and provide data analysis, synthesis & presentation support. Interact with end users and line managers to gather requirements, and lead implementation phase. Ensure all deliverables meet client expectations in terms of scope, speed & quality Manage Junior Analysts Essentials Expertise in SQL programming Fine tuning of tables, performance improvements in SQL; Database: RDBMS Oracle or SQL Server or any other database. Hands on Experience in SSIS/SSRS. Also, SSAS will be an added advantage Excellent presentation and communication skills Willing to learn and try new things proactively Entrepreneurial in nature; self-motivated; work with minimal guidance 2-3 years of relevant experience
College Preference : no-bar
Min Qualification : ug
Skills : oracle, sql, sql server, ssrs
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-analyst-msbi-bangalore-2-3-years-of-experience/
Risk Analyst/ Business analyst- Delhi (5+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst- MSBI- Bangalore (2-3 Years Of experience)|Statistical Analyst- Ahmedabad ( 1-2 years of experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : DESCRIPTION-The ideal Candidate will be a Credit Risk Statistician / Modeler to support the analytical, reporting and data needs of a young, exciting and well-funded start up.MINIMUM EDUCATION/EXPERIENCE REQUIRED-Bachelors degree required in a highly technical/mathematical discipline (i.e. statistics, mathematics, quantitative analysis, economics, computer science, etc) . . . plus 5 years experience in credit risk management or other quantitative financial analysis.IDEAL QUALIFICATIONSThe Candidate will be required to use data mining, predictive modeling and statistical techniques to solve business problems.Strong expertise with SAS / R for data manipulation , practical experience with ARIMA, ARIMAX.Knowledge of at least one of the following: credit loss forecasting models (NCO, PD, LGD), PPNR, time series, (OLS) linear regression models and statistical sampling techniques.Familiarity with SR11-07 regulatory guidance for Model Risk Management and previous experience doing model validation.Understanding of CCAR, DFAST (stress testing models), Basel, AML and fraud models, and ALLL models is a plus.Knowledge of financial services.The candidate will be required to develop their statistical modeling skills in areas such as segmentation analysis, logistic regression, decision trees, and multivariate analysis.Ability to use analytics in a collaborative effort across functions to derive optimum solutions to business problems.Demonstrated problem solving skills.Demonstrated ability to effectively communicate (written and verbal) technical and analytical information to a variety of interested parties at all levels.Experience data mining large databasesRESPONSIBILITIESExtensive knowledge of credit risk databases, credit bureau to provide data and analytical support to senior management team.Perform analysis of data using statistical analysis tools including SQL, R, and Excel, and present results and recommendations to management.Identify deviations from forecast/expectations and explain variances.Identify risk and/or opportunities.Identify opportunities to leverage statistical solutions to business problems.The Statistician must be an excellent communicator (both spoken and written forms) and be able to effectively foster interactive collaboration and discussions with other team members.He must be able to present analysis and conclusions in a logical and clear manner.
College Preference : no-bar
Min Qualification : ug
Skills : data mining, decision trees, excel, forecasting, linear regression, logistic regression, predictive modeling, r, sas, sql, statistics, time series
Location : New Delhi
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/risk-analyst-business-analyst-delhi-5-years-of-experience/
Statistical Analyst- Ahmedabad ( 1-2 years of experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Risk Analyst/ Business analyst- Delhi (5+ Years Of Experience)|Junior Data Scientist- Mumbai (0-2 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  2 years
Requirements : 
Task Info : Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.We are the Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies. We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.Duties and Responsibilities- Develop, optimize scoring programs and extract data for analysis and predictive modeling. Execute predictive model scoring in order to support various customers marketing initiatives. Cross-functional support of complex queries for SQL and SAS macros. Explore and analyze customer behavioral data, provide actionable insights and identify business opportunities. Understand customer migration patterns and perform customer profiling and segmentation. Work with client teams to incorporate analysis findings to plan future campaigns. Communicate findings and recommendations to various clients. Stay current with business results, strategies, industry standards and best practices. Support Senior Statistical Analyst in projects Validate various models and articulate results in terms of recommended action stepsQualification & Skills- Highly energized personality with a positive attitude and passion for statistical analysis and predictive modeling. Detail oriented and demonstrated ability to manage multiple projects simultaneously. Demonstrated analytical and problem solving skills. Ability to translate business need into model/analysis specifications. Ability to make strategic recommendations based on analysis results. Effective team member with strong written and verbal communication skills. Demonstrated ability to communicate complicated statistical analysis/modeling concepts to business executives. Ability to effectively prioritize, multi task and work under tight timelines. Ability to understand overall business objectives and make accurate, thoughtful decisions.Education & Experience Masters Degree in Statistics or any related quantitative fields 1-2 years analytical experience in a fast paced and professional environment 1-2 years of hands-on SAS and advanced SQL experience (UNIX environment preferred) Experience in MS Access, Excel and Power point Experience in working with large relational databases (Teradata, Oracle) preferred Experience in Customer Marketing preferred Experience in cluster and factor analysis for customer profiling and segmentation preferred
College Preference : no-bar
Min Qualification : pg
Skills : clustering, database, excel, factor analysis, oracle, sas, segmentation, sql, statistics, teradata
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/statistical-analyst-ahmedabad-1-2-years-of-experience/
Junior Data Scientist- Mumbai (0-2 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Statistical Analyst- Ahmedabad ( 1-2 years of experience )|Data Analyst: Sql+ Tableau- Bangalore (2-5 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 0  2 years
Requirements : 
Task Info : You will collaborate with the brightest technical minds in building futuristic products, researching ground-breaking concepts, and influencing new ideas to transform retail. If you want to make meaningful contributions and reinvent the retail space that will impact millions of guests, Target is for you.About This Opportunity:Responsible for the development of high performance, distributed computing tasks using Big Data technologies such as Hadoop, NoSQL, text mining and other distributed environment technologies based on the needs of the organization. Designs and drives the creation of new standards and best practices in the use of statistical data modelling, big data and optimization tools.Deploy data-science and technology based algorithmic solutions to address business needs for our customersUtilize recommender systems, collaborative filtering techniques, propensity modeling to drive our customers business prioritiesQualifications:Msc  Statistics / Math or PhD in any streamExperience :0 to 2 yearsRequirementsExperience / Strong Knowledge of Regression/Linear AnalysisFamiliarity with any of Deep Learning, Distributed RF, Generalized Linear Model,K-Mean and naive BayesFamiliarity with designing algorithms on Hadoop ecosystemExperience / Strong Knowledge with recommendation systemsA strong passion for empirical research and for answering hard questions with dataExcellent written and verbal communication skills
College Preference : no-bar
Min Qualification : pg
Skills : deep learning, hadoop, linear regression, naive bayes, regression
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/junior-data-scientist-mumbai-0-2-years-of-experience/
Data Analyst: Sql+ Tableau- Bangalore (2-5 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Junior Data Scientist- Mumbai (0-2 Years of Experience)|Data Scientist- Gurgaon (5+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  5 years
Requirements : 
Task Info : Job responsibilities: Perform complex data analysis, design, support, and documentation Help translate business problems to analytical frameworks: extract insights from consumer, sales and other data available and provide data analysis, synthesis & presentation support Interact with end users and line managers to gather requirements, and lead implementation phase. Ensure all deliverables meet client expectations in terms of scope, speed & quality Manage Junior Analysts Essentials Good hands-on expertise in Tableau and ETL tools. Database: RDBMS Oracle or SQL Server or any other database; Fine tuning of tables, performance improvements in SQL, knowledge of advanced SQL queries Providing Insights on the data presented Excellent presentation and communication skills Willing to learn and try new things proactively Entrepreneurial in nature; self-motivated; work with minimal guidance 2-5 years of relevant experience
College Preference : no-bar
Min Qualification : ug
Skills : etl, oracle, sql, sql server, tableau
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-analyst-sql-tableau-bangalore-2-5-years-of-experience/
Data Scientist- Gurgaon (5+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst: Sql+ Tableau- Bangalore (2-5 Years Of Experience)|Big Data Engineer- Bangalore (7-9 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : The data scientist is responsible for solving complex big-data problems in the mobile space using data mining, machine learning, statistical analysis and common sense. The person will work on building and maintaining models. Also would be analyzing patterns to derive insights on various aspects. Build predictive models, work closely with the engineering team to ensure they are correctly deployed, track their performance & optimize the same. Lead day to day activities for at least one project/account Work with cross functional teams to conduct experimentation Optimize the models to show performance and drive incrementality Prepare marketing material on the deployed models along with marketing teams Drive adoption of analytical tools and techniques developedEducation:Bachelors/Master degree in Statistics/Mathematics/Economics/Engineering/CS/OR MBA  MCA P.S.: We really don t care if you can build and run models..Experience: 5+yrsKey Skills: Coding skills Expert in at least one high level language such as R,Pyhton Exposure to Machine Learning techniques, NLP and text mining is a must Predictive Modeling knowledge necessary Understanding SQL, Stored Procedures Perform analytics delivery & business analysis
College Preference : no-bar
Min Qualification : ug
Skills : machine learning, nlp, predictive modeling, python, r, sql, text mining
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-gurgaon-5-years-of-experience/
Big Data Engineer- Bangalore (7-9 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Gurgaon (5+ Years of Experience)|Data Scientist (Product)- Mumbai (1+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 7  9 years
Requirements : 
Task Info : Mandatory:Excellent knowledge of Core Java or NodeJSExpertise in J2EE technologies: Spring, Java, JSP, JSF, JDBC, Hibernate, Struts.Experience in developing UI using JQuery/ Angular JS.Experience in designing database schemas and writing fine-tuned queries.Experience in working on UNIX/Linux environment.Designed and built extremely reliable, scalable and high performing enterprise systems7-9 years of experience in open source Java/J2EE technologiesStrong problem solving and analytical capabilities.Strong scripting skills  Shell/PythonDesired:Understanding of Hadoop  MapReduce/Pig/Hive/SparkExposure to NoSQL databasesExposure to AWSThe role is of an individual contributor. Youll be expected to code atleast 80% of the time. Please attach your resume. If selected, youll need to join in one month.
College Preference : no-bar
Min Qualification : ug
Skills : aws, hadoop, hive, java, jquery, linux, mapreduce, nosql, pig, python, unix
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/big-data-engineer-bangalore-7-9-years-of-experience/
Data Scientist (Product)- Mumbai (1+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Engineer- Bangalore (7-9 Years of Experience)|Sr Eng Manager or Director of Engineering- Mumbai (10+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  years
Requirements : 
Task Info : The Data Scientist (Product) will be responsible for developing and maintaining healthy product analytics environment, predictive systems, creating efficient algorithms and improving data quality. This individual will work closely with the Head of Analytics to identify, evaluate, design and implement statistical analyses of gathered open source, proprietary, and customer data to create analytic metrics and tools suitable for use in Digital Sandbox applications. This individual will have the opportunity to contribute directly to the features and capabilities deployed in our applications.Key Responsibilities: Work with engineering and research teams on designing, building and deploying data analysis systems for large data sets Work with various Product Managers for setting up analytics for their respective products Conduct Event Audits, random data checks, user funnel analysis at regular intervals Work with marketing teams to understand the ROI and reach of various marketing campaigns Design, develop and implement R&D and pre-product prototype solutions and implementations Create algorithms to extract information from large data sets. Establish scalable, efficient, automated processes for model development, model validation, model implementation and large scale data analysis. Develop metrics and prototypes that can be used to drive business decisions. Provide thought-leadership and dependable execution on diverse projects. Identify emergent trends and opportunities for future client growth and development
College Preference : no-bar
Min Qualification : ug
Skills : algorithms, c++, datavisualization, google analytics, java, python, r, sas, sql
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-product-mumbai-1-years-of-experience/
Sr Eng Manager or Director of Engineering- Mumbai (10+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Product)- Mumbai (1+ Years of Experience)|Developer Ops Manager  Mumbai (4-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 10  years
Requirements : 
Task Info : BS or MS in Computer Science or equivalent from a top tier engineering program. 10+ years software development management and operations management experience with a track record of delivering large-scale software products in SaaS environment. Strong leadership skills and an ability to hire and retain strong teams. Ability to draw a long term vision for the group while maintaining a complete grasp of the day to day details. Experience managing teams to design, build, and operate large web systems. Familiarity with networking, monitoring, and deployment systems is a must. Utilization of technology for data gathering Data transformation and statistical modeling using the resulting data within both traditional data-warehousing, business intelligence and statistical environments Big data environments using Hive, Pig, Impala, HBase, Sqoop, Flume, Splunk and Spark In-depth knowledge of end-to-end web services and full stack web development (HTML, JavaScript, Java, Python, REST APIs, and various app servers) Experience managing teams using Agile/SCRUM methodologies Must have experience in building analytics for structured and unstructured data and managing large data ingestion using technologies like Kafka and Flume Must have hands on experience in implementing solutions using SPARK, PIG, Apache Storm Must have a good understanding of Cloud Development and Cloud APIs. Experience on commercial cloud platforms such as Amazon EC2, Rackspace a plus Working knowledge of the differences between batch and real-time steaming processing Working knowledge of Web Services, XML, REST, and JSON Proven track record of execution in a fast paced environment
College Preference : tier1-any
Min Qualification : ug
Skills : bigdata, cloud, hive, java, java script, pig, python, spark, sqoop
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/sr-eng-manager-or-director-of-engineering-mumbai-10-years-of-experience/
Developer Ops Manager  Mumbai (4-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Sr Eng Manager or Director of Engineering- Mumbai (10+ Years of Experience)|Data Scientist (Theoretical)- Delhi/NCR (4+ Year Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  6 years
Requirements : 
Task Info : Experienceworking with Cloud (AWS) operational infrastructure which includes cloud services like Elastic Load balancers, S3 storages, memory & physical instances, DNS servers, analytics, watch services & log monitoring services.Data management skills with SQL, NoSQL & Big Data.Understanding of scaling and resilience technologies such as memcached, Redis, queuing etc.Performance tuning & optimization of webservers like NGNX, apache, tomcat etc.Strong experience in planning & maintenance of BigData technologies.Strong grasp of deployment lifecycle management.(Requirement definition, systems architecture, technical design, development, test, product release, and live operation).Working experience on high traffic services and highly scalable systems, multi-threaded design with a complete overview on applications, systems, Databases & Network.Ability to code and script background maintenance jobs.Ability to use a wide variety of open source technologies and toolsExperience testing and troubleshooting live services.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, aws, bigdata, nosql, sql
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/developer-ops-manager-mumbai-4-6-years-of-experience/
Data Scientist (Theoretical)- Delhi/NCR (4+ Year Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Developer Ops Manager  Mumbai (4-6 Years Of Experience)|Data Scientist (Industry Experience)-Delhi/NCR ( 5+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 4  years
Requirements : 
Task Info : Description:The primary work area will be in the realm of statistics, hypotheses testing, correlations & pattern recognition. The goal of this data scientist will be to look at various situations/scenarios/problems and apply different models to data to understand which model works with maximum efficiency & accuracy. Post testing, these algorithms will be deployed on the platform as one or more of the following: automated reports, automated analytics, custom reports etc.A lack of industry experience in this role should be compensated by a greater understanding of the data science realm: what models are out there currently? What models can be applied to a particular scenario? What direction should reporting & analytics take in the future? Are there more efficient ways of doing certain things we are already doing? Someone with a PhD level who has worked in finance or market research or process optimization or consumer theory etc.Top Line requirements across these roles include:Statistical testing: Regressions, correlations, hypotheses testingClustering analysesMachine Learning techniquesPredictive analytics: forecasting etc.Data miningAlgorithm buildingDecision tress & decision tree forecasting
College Preference : no-bar
Min Qualification : ug
Skills : clustering, data mining, decision trees, forecasting, machine learning, regression, statistics
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-theoretical-delhincr-4-year-of-experience/
Data Scientist (Industry Experience)-Delhi/NCR ( 5+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Theoretical)- Delhi/NCR (4+ Year Of Experience)|Risk Analyst-Gurgaon ( 3+ Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 5  years
Requirements : 
Task Info : Responsibilities:1. Understand client side requests (for ex. what different variables mean in a digital context; are there specific problems (analytical) in the industry that they are aware of.2. Be the bridge between scenario creation (for ex. a broad problem we are trying to solve) and theoretical data science. Essentially, work closely with the theoretical data scientists to decode digital related pain points, translate them into data science work flows and collaborate with the theoretical data scientist to apply models, build algorithms and eventually deploy on the platform.Industry experience in this role compensates for perhaps less theoretical knowledge on the subject. The industry experience is relevant because this individual should know how brands are approaching digital, what are they aware of in terms of current market needs & demands and the general analytical capabilities offered by different firms in our space.Top Line requirements across these roles include:Digital MarketingPredictive analytics: forecasting etc.R/ Python
College Preference : no-bar
Min Qualification : ug
Skills : data science, forecasting, predictive model, python, r
Location : Gurgaon
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-industry-experience-delhincr-5-years-of-experience/
Risk Analyst-Gurgaon ( 3+ Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist (Industry Experience)-Delhi/NCR ( 5+ Years of Experience)|Big Data Engineer- Hyderabad (2+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : Risk analysts, sometimes referred to as risk managers, identifies and analyses potential risks threatening the financial position of commercial and industrial corporations and public/private organizations. They help businesses determine the amount of financial risks involved concerning investments and operational costs.Responsibilities:1.He is responsible for predicting change and future trends, as well as forecasting cost to the organization.2.He should have an understanding of investment Risk Systems & portfolio management & essential business skills- communication & organization3.He should have ability of forecasting and monitoring market trends.4.He should use his analytical skills, along with his knowledge of international business and currency markets, to help clients limit losses.5.He should be able to calculate risks involved in transactions and business proposals.6.He should be able to recommend precautionary or improvement measures.7.Help clients meet financial goals.8.Analyze financial statements such as profit and loss, company budget and employee headcount reports.9.Compile reports showing the proposed plan of action for existing and potential clients10.Monitor and assess the post-period implementation of risk management strategies.11.Strong Analytical Skills with The Competencies To Lead And Conduct Analyses.12.Strong Project Management Skills And Ability To Prioritize And Manage Multiple Initiatives SimultaneouslyQualifications:1.Bachelors degree in finance/ mathematics/ economics2.Masters degree: MBA from Tier A institute/Masters of Economics or Statistics from premier institutesSkills:1.In-depth knowledge of statistical/programming languages is a requisite for this role2.Strong quantitative skills and in particular advanced statistics and/or econometrics are highly desirable.3.Highly proficient in EXCEL, WORD and POWERPOINT, Access4.Proficiency in MatLab/SAS and statistical packages desired5. 2 years of work experience in an analytical role requiring quantitative and qualitative analytics (e.g. SQL, R) in the areas of business intelligence, statistics or scientific analysis.6.Experience with data analysis.7.Knowledge of Big Data and Machine learning algorithms is a plusWork Experience:1. 3 to 8 Years in the financial institutions such as banks & investment firms.
College Preference : tier1-any
Min Qualification : ug
Skills : banking, bigdata, business intelligence, data analysis, excel, machine learning, r, sql, statistics
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/risk-analyst-gurgaon-3-years-of-experience/
Big Data Engineer- Hyderabad (2+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Risk Analyst-Gurgaon ( 3+ Years Of Experience)|Business Analyst- Chennai (2+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : Qualification & Skill Required:1.Minimum experience should be 2+ years.2.Hands on experience in database like MySQL, SQL Server, Oracle, Redis, Cassandra.3.Hands on experience in Hadoop ecosystem, MapReduce4.Strong knowledge of Python, Java.5.Strong knowledge of Machine Learning6.Knowledge of Data Mining, Statistical Data Analysis, Predictive analytics will be added advantage.7.Knowledge of Forecasting, Prediction & Scenario Analysis will be desirable.8.Knowledge of Algorithm design, Graph theory, Core Java, R, Matlab will be plus.9.Knowledge of Scripting Language like perl, Apache Mahout, Stanford NLP, Wordnet, LIWC, Apache Lucene, Neo4j will be an added advantage.
College Preference : no-bar
Min Qualification : ug
Skills : data mining, forecasting, hadoop, java, machine learning, mapreduce, oracle, predictive modeling, python, r, sql server, statistical modeling
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/big-data-engineer-hyderabad-2-years-of-experience/
Business Analyst- Chennai (2+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Big Data Engineer- Hyderabad (2+ Years of Experience)|MIS Executive- Delhi (1-3 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : Business analysts must have excellent verbal & written communication skills and problem solving skills.He should be creative  with the ability to engage with customers to understand and respond to their needs in rapidly changing business environments.Responsibilities:Implement advanced strategies for gathering, reviewing and analyzing data requirementsPrioritize requirements and create conceptual prototypesThe ability to conduct cost/benefit analysisBusiness case developmentModeling techniques and methods, traceability and quality management techniquesApply best practices for effective communication and problem-solvingSkills Required;Written and verbal communication, including technical writing skills18 + months of experience in Analytics.
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, modeling
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/business-analyst-chennai-2-years-of-experience/
MIS Executive- Delhi (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Business Analyst- Chennai (2+ Years of Experience)|Data Scientist- Noida (1-3 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : Functional Area  Analytics & Business IntelligenceBusiness Type  Trader  (Copper & Aluminum)Experience Required  1 to 4 YearsJob Location  Preet Vihar (East Delhi)Responsibility:To analyze the data and identify the variancesTo identifying areas of revenue leakages and betterment of processTo understand the system enhancements required to better the existing processTo generate Periodic & Ad-hoc Reports reports in accurate & timely manner and presentation for managementEssential Skills:A minimum of 1 years of MIS Analytics experienceData Analytical SkillGood Communication SkillsGood hand on MS Excel, comfortable with formulas like Vlookup, Pivot, Sumif, Countif if then else etc.
College Preference : no-bar
Min Qualification : ug
Skills : data analysis, excel
Location : Delhi
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/mis-executive-delhi-1-3-years-of-experience/
Data Scientist- Noida (1-3 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|MIS Executive- Delhi (1-3 Years Of Experience)|Analytics Delivery Manager- Noida (3-5 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science  
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  3 years
Requirements : 
Task Info : Data Science team is responsible for providing decision insights to business users using data mining, predictive analytics and machine learning techniques. Team works closely with business executives across segments and geographies in understanding business objectives and designing analytical solutions to meet business goals. Candidate will be expected to achieve following goals. Understanding project requirements and business goals. Conceptualize & develop detailed analytical framework for business problem at hand. Involves detailing out heuristics/hypothesis to be tested, potential data points (derived/raw) needed, machine learning techniques to be used. Data preparation using tools like SQL, R/Python etc. Candidate is expected to be comfortable with at least 2 programming languages with SQL as mandatory. Develop statistical models for achieving desired business outcomes. This involves trying out multiple algorithms and finding best outcome. Documentation of entire analytical process starting with raw data to final insights.Desired Skills & Experience 4 year Bachelors or Masters degree from reputed University with concentration on marketing, finance, economics or other quantitative field such as statistics or engineering. 1-3 years of experience in advance analytics within consulting environment. Have extensive analytical/statistical knowledge such as modelling, cluster analysis, Logistic regression, neural networks, Linear regression, time series forecasting, Random Forest. Good theoretical and practical knowledge of tools SAS, R, Python, SQL, VBA etc Ability to comprehend intricate and diverse range of business problems and analyze them with limited or complex data and provide a feasible solution framework. Good Analytical & Problem Solving Skills Good Communication Skills: Refers to effective oral, written and presentation skills
College Preference : tier1-any
Min Qualification : ug
Skills : analytics, clustering, forecasting, linear regression, logistic regression, modeling, neural network, python, r, random forest, sas, sql, statistics, time series
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-noida-1-3-years-of-experience/
Analytics Delivery Manager- Noida (3-5 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Noida (1-3 Years Of Experience)|Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : Role & Expectations Propose analytics driven solutions to business problems. Includes defining project approach, technical and functional documentation. Develop detailed project plan with estimate for time and resources involvement. Getting necessary approval from all stakeholders on project approach, milestones and other key aspects. Lead team of big data developers/data engineers/data scientists in delivering analytics solutions. Contribute with technical papers/blogs in analytics field.Technical Skillset R/Python/Scala/Java (Python/Java preferred) MySQL/SQL (Required)  Linux/Windows (Linux hands on preferred) Hadoop/Map-Reduce/Hive/Spark (Preferred but not pre-requisite)Preferred Candidate Bachelors or higher degree in computer science. We expect you to be hands on with coding. 3-5 years of total experience in implementing and delivering analytics solutions across domains. Should have led team of data scientist, programmers and data engineers before. Experienced in engaging with different business stakeholders Should keep abreast of latest developments in Machine Learning, AI, Big Data and propose use of one or other in actual projects. Must be able to articulate views clearly to business team and client. Should be go getter when it comes to experimenting with new tools.
College Preference : no-bar
Min Qualification : ug
Skills : hadoop, hive, java, linux, mapreduce, python, r, spark, sql
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/analytics-delivery-manager-noida-3-5-years-of-experience/
Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Analytics Delivery Manager- Noida (3-5 Years of Experience)|Senior Analyst, Data Science, Risk Analytics & Modelling- Chennai ( 2+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  years
Requirements : 
Task Info : Do you take great pride in your Craft and Skills? Are you someone with a natural sense of curiosity, and desire to improve? Are you looking for a job you can be passionate about and is more than just a paycheck? Does a collaborative environment that rewards and recognizes great contributions excite you? If this sounds like you, inspires you and resonates as the team you want to be a part of come help us transform data into quantifiable results.Role / Job Description:You will be looked up to as a trusted advisor by our client and as a mentor and specialist by the internal consulting team.You will have a minimum of 6 years of work experience in digital marketing domain. Specific work experience in e-commerce or retail is essential. Clickstream analysis, attribution modeling, design of digital marketing campaigns, reporting etc. are what you should have done in the past.You will be proficient in data handling, understanding, analysis and interpretation of results.As a strategist, you will be clear in articulating a problem statement based on your domain expertise, formulating hypotheses, and ensuring data collected and aggregated can test those hypotheses.You will be responsible for delivering presentations and business insights back to our clients. Additionally, you will serve as a project lead on multiple projects. You should be able to break down the work required, create project plans, get buy-in from stakeholders and manage efforts against the plan.You will be hands-on. Weekly updates, frequent client meetings, work product reviews etc. will be part of life in this role. The team will consist of analysts, consultants, modelers and visualization specialists.You will be helping our organization achieve expertise and excellence in digital marketing analytics pertaining to e-commerce and brick-and-mortar retailers. Towards that end, you will bring with you an understanding of common and best practices.You will be responsible for creating collateral, architecting the solution, pre-sales (creating pitch decks and pitching to prospective clients). You will be expected to publish white papers, articles and help create internal training material.The job location will be Ahmedabad, India with regular visits to the client site abroad. The job will require the person to coordinate well between offshore and onsite team members.We are a Data Analytics company providing strategic solutions to difficult business problems in a variety of industries, including Banking, Casino, Media, Finance and others. With accurate and actionable research and analytics we help our clients become analytically mature companies.We have expertise in providing Analytics Solutions, Modeling and Forecasting, Marketing Research, Business Strategy and Consultation.
College Preference : no-bar
Min Qualification : ug
Skills : analytics, data analysis, data management, datavisualization, ecommerce, reporting, retail banking
Location : Ahmedabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/digital-marketing-strategist-ahmedabad-6-years-of-experience/
"Senior Analyst, Data Science, Risk Analytics & Modelling- Chennai ( 2+ Years of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Digital Marketing Strategist- Ahmedabad- ( 6 Years of Experience)|Data Analyst, Risk Analytics & Modelling-Chennai (2+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : We are looking to hire a senior analyst (Data Science) to work with its risk analytics team based in Chennai. The team focusses on developing data driven risk analytics models to gain insights into borrower behaviour across multiple asset classes using advanced statistical learning methods on more than 6 million underlying loan contracts and estimate loss on its structured finance credit and corporate portfolio.Key responsibilitiesDevelop and implement new (/maintain and use existing) statistical/machine learning models to identify performance and risk drivers in the credit portfolioDevelop and implement new (/maintain and use existing) risk and performance assessment models to measure risk and performance in the portfolioManage large loan level and borrower level data used for risk and learning modelsMeasure the risk and performance indicators for debt and structured finance products using existing and new modelsPerform qualitative and quantitative analysis of various performance metrics for structured transactionsDocument the analysis methodology and findings in report and present the analysis to the internal and external stakeholders/platforms (model notes, white papers, working papers, etc.)Contribute towards other risk management work done by the risk management functionTravel not more than 20% of the time to meet partners and understand the lending modelsEssential Skills-At least two years of experience in implementing statistical/machine learning algorithms (regression, decision trees, SVM) and statistical programming tools (R/Matlab/Octave/Weka)Proficiency in programming: R/Python/VBAStrong background in statistics and probabilityExperience in handling large structured and unstructured datasetsPGDM/Masters degree in Maths/Statistics/Econometrics/Economics/Finance/Engineering /other quantitative disciplinesActuaries/FRM/CFA/CQF/PRM certification would be a plusKnowledge and experience in structured finance products (securitization structures) would be a plusExcellent writing, oral communication and presentation skills
College Preference : no-bar
Min Qualification : pg
Skills : data science, decision trees, machine learning, matlab, modeling, python, r, regression, risk, statistics, svm
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/senior-analyst-data-science-risk-analytics-modelling-chennai-2-years-of-experience/
"Data Analyst, Risk Analytics & Modelling-Chennai (2+ Years of Experience)",Learn everything about Analytics,"Share this:|Like this:|Related Articles|Senior Analyst, Data Science, Risk Analytics & Modelling- Chennai ( 2+ Years of Experience)|ETL Developer- Hyderabad (2-3+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  years
Requirements : 
Task Info : We are looking to hire a data analyst to work with its risk analytics team based in Chennai. The team focusses on developing data driven risk analytics models to gain insights into borrower behaviour across multiple asset classes using advanced statistical learning methods on more than 6 million underlying loan contracts and estimate loss on its structured finance credit and corporate portfolio.Key responsibilitiesManage data sources, database and movement of data and data organization across different systemsDesign and implement data validation processes for dataDesign and supervise data and information security processesDocument the data management and information system processesAssist the Risk Analytics team in designing and executing the data analytics projectPrepare standardized reports using the data based on the team requirementsAssist the Risk Analytics team in designing the business intelligence toolsManage large loan level and borrower level data used for risk and learning modelsMeasure the risk and performance indicators for debt and structured finance products using existing and new modelsContribute towards other risk management work done by the risk management functionTravel not more than 20% of the time to meet IFMR Capital partnersEssential SkillsAt least two years of experience in database or project managementShould have hands on experience of designing database schemasProficiency in programming: SQL and at least one of R/Python/VBAExperience in handling large structured and unstructured datasetsBackground in statistics and probability is a plusPGDM/Masters degree in Information Systems/Maths/Statistics//Finance/Engineering /other quantitative disciplinesActuaries/FRM/CFA/CQF/PRM certification would be a plusKnowledge and experience in structured finance products (securitization structures) would be a plusExcellent writing, oral communication and presentation skills
College Preference : no-bar
Min Qualification : pg
Skills : data analysis, database, data management, modeling, python, r, risk, sql, statistics
Location : Chennai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-analyst-risk-analytics-modelling-chennai-2-years-of-experience/
ETL Developer- Hyderabad (2-3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst, Risk Analytics & Modelling-Chennai (2+ Years of Experience)|Data Analyst/Backend Engineer- Noida (2-6 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  3 years
Requirements : 
Task Info : Role Summary:ETL Developer is responsible for Design and Development of ETL Jobs which follow standards, best practices and are maintainable, modular and reusable.ETL Developer will analyze and review complex object and data models and the metadata repository in order to structure the processes and data for better management and efficient access.Working on multiple projects, and delegating work to Junior Analysts to deliver projects on time.Training and mentoring Junior Analysts and building their proficiency in the ETL process.Job Duties:Preparing mapping document to extract, transform, and load data ensuring compatibility with all tables and requirement specifications.Experience in ETL system design and development with Talend / Pentaho PDI is essential.Create quality rules in Talend.Tune Talend jobs for performance optimization.Write relational and multidimensional database queries.Functional Knowledge of Talend Adminstration Center, Job Servers & Load balancing setup, and all its administrative fucntions.Develop, maintain, and enhance unit test suites to verify the accuracy of ETL processes, dimensional data, OLAP cubes and various forms of BI content including reports, dashboards, and analytical models.Exposure in Map Reduce components of Talend / Pentaho PDI.Creating and deploying Talend / Pentaho custom components is an add-on advantage.Job Specification:BE, B.Tech / MS Degree in Computer Science, Engineering or a related subject.Having an experience of 2  3+ years.Comprehensive understanding and working knowledge in Data Warehouse loading, tuning, and maintenance.Proficiency with Talend or related tools like Pentaho Data Integration / Kettle.Working knowledge of relational database theory and dimensional database models.Ability to write complex SQL database queries.Ability to work independently.
College Preference : no-bar
Min Qualification : ug
Skills : database, etl, pentaho, sql
Location : Hyderabad
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/etl-developer-hyderabad-2-3-years-of-experience/
Data Analyst/Backend Engineer- Noida (2-6 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|ETL Developer- Hyderabad (2-3+ Years of Experience)|Data Scientist- Noida (1-5 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 2  6 years
Requirements : 
Task Info : Company:We are a startup fintech company that is using predictive analytics and machine learning to revolutionize the online lending space in India. We are looking for engineers who can build and optimize our data management backend. This is a great opportunity for someone to come in at the start of a new venture and play a pivotal role.Job Description:Able to manage and organize structured and unstructured data from multiple sources. Design and implement data security processes. Integrate systems with multiple apis for data access. Contribute towards implementing analytical models. Candidate should enjoy working in an unstructured environment with minimal supervision but with significant opportunity for personal growthQualificationsBachelors or Master degree in engineering/sciences with strong background in programming of 2-6 yearsStrong experience in databasesAbility to work independentlyGood analytic and problem solving skillsCompensation: Flexible, with eligibility for equityLocation: Located in Noida, UP next to Delhi border
College Preference : no-bar
Min Qualification : ug
Skills : analytics, database, modeling
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-analystbackend-engineer-noida-2-6-years-of-experience/
Data Scientist- Noida (1-5 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Analyst/Backend Engineer- Noida (2-6 Years Of Experience)|Fraud Analyst- Gurugram (6-8 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 1  5 years
Requirements : 
Task Info : Company:We are a startup fintech company that is using predictive analytics and machine learning to revolutionize the online lending space in India. We are looking for Data scientists who can design and optimize our predictive analytics and machine learning models. This is a great opportunity for someone to come in at the start of a new venture and play a pivotal role.Job Description:Candidate should be able to conceptualize and decide on appropriate analytical models for analyzing the available datasets. Try multiple algorithms and models to decide on appropriate combination for optimal performance. They should enjoy working in an unstructured environment with minimal supervision but with significant opportunity for personal growthQualifications-Bachelors or Master degree in engineering/sciences with strong background in statistics/math/computer science1-5 yrs work experience in programming with Python, C, R etc.Ability to work independentlyGood analytic and problem solving skillsCompensation: Flexible, with eligibility for equityLocation: Located in Noida, UP next to Delhi border
College Preference : no-bar
Min Qualification : ug
Skills : analytics, python, r
Location : Noida
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-noida-1-5-years-of-experience/
Fraud Analyst- Gurugram (6-8 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist- Noida (1-5 Years of Experience )|Data Scientist-Bangalore (6-8 Years of Experience )|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  8 years
Requirements : 
Task Info : A fraud analyst is someone who investigates forgery and theft within customers accounts and transactions on behalf of a bank or a financial institution. They track and monitor the banks transactions and activity that comes through the banks customers accounts. It is their job to identify and trace any suspicious or high-risk transactions, and determine if there is improper activity involved and if there is risk to the bank or its customers.Responsibilities:1.He will be responsible for observing any type of customer transactions to flag or identify any suspicious activity.2.He will be responsible for keeping any collected information confidential while working to catch the criminals who may have committed the felony of check fraud or electronic fraud.3.Monitor real time queues and identify high risk transactions within the business portfolio.4.Identify fraudulent transactions and cancel them from further processing.5.Resolve queued transactions within the service level agreements to reduce potential revenue losses.6.Interact with banks and customers to validate information and to confirm or cancel authorizations.7.Resolve customer issues within the scope of existing service level agreements.8.Monitor constantly customer and transactional records to identify unauthorized transactions and fraudulent accounts.9.Maintain fraud analysis models to improve efficiency and effectiveness of company systems.10.Ensure confidentiality of all information collected during investigation.11.Determine existing fraud trends by analyzing accounts and transaction patterns.12.Identify system improvements to prevent fraudulent activities.13.Recommend anti-fraud processes for changing transaction patterns and trends.14.Generate suspicious activity reports and risk management reports for Managers.Qualification and Skills RequiredA bachelors degree is required for this position, with a concentration in finance, business, mathematics, or economics.Masters Degree In Economics, Statistics, MBA Or Related Quantitative Fields From A Reputable InstituteStrong Analytical Skills With The Competencies to Lead And Conduct AnalysesHave any experience in programs or projects that can project a clear problem-solving or investigative ability.Should have worked in Banking domain for at least 2 yearsExperience on SAS for at least 2 yearsExcellent communication skillsExperience- 6-8 yearsInvestigative skills, written reports, evidence preparation, witness interviews, database maintenance, active-listening, critical thinking, telephone skills, familiarity with SAS, SQL, E-mail, telephone and retail banking systems
College Preference : tier1-any
Min Qualification : ug
Skills : banking, database, retail banking, sas, sql
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/fraud-analyst-gurugram-6-8-years-of-experience/
Data Scientist-Bangalore (6-8 Years of Experience ),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Fraud Analyst- Gurugram (6-8 Years of Experience)|Full-Stack Developer- Mumbai (3-5 Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  8 years
Requirements : 
Task Info : 6-8 years of Relevant experience as Data Scientist and ModelingExperience in end to end modeling using R, SAS or Alpine Data Labs.Experience in Python and Hadoop Ecosystem will be plus.Good experience in statistical modeling techniques and analytical techniques in forecasting, predictive analysis, classification, Text mining and segmentation highly desirableGood exposure to data visualization skills using tableau, or equivalent tools (Qlikview etc.)Candidate should have good understanding of the Business to define and propose analyses which would support the business functions. Hi-Tech or Financial industry experience will be a big plus.Have a good understanding of relational databases and SQL. Experience in working with unstructured data will be a plusGood Communication, presentation and summarization skills
College Preference : no-bar
Min Qualification : ug
Skills : classification, data science, datavisualization, forecasting, hadoop, modeling, python, qlikview, r, sas, segmentation, sql, statistical modeling, tableau, text mining
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/data-scientist-bangalore-6-8-years-of-experience/
Full-Stack Developer- Mumbai (3-5 Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Data Scientist-Bangalore (6-8 Years of Experience )|Collection Analyst- Gurgaon (6-10 Years Of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  5 years
Requirements : 
Task Info : We are building a FinTech product for Indian Financial sector.The product is currently in the development phase having a team of 8 passionate Engineers. We are looking for a Full-Stack developer, who would Get a chance to build an Enterprise grade software product from ground up.Have a steep learning curve of Cloud Computing, data Science, big Data, etc.Have a big Say in all the tech decisions  choosing frameworks, data modeling, how to build internal/external web-services.Skills required:Hands-on, programming experience in any Programming language like Python, Ruby, Java/J2EE or Node.js.Good knowledge on RDBMS and any No-SQL/Graph DBs like MongoDB, Redis, Apache Cassandra etc.Experience on developing single page applications is an added advantage.Strong experience in Web Services development using SOAP and RESTGood knowledge of HTML5, JavaScript and CSS3Experience in one of the web frameworks like jQuery, Angular.js or Backbone.jsExperience with AWS EC2, AWS S3 is desired.Experience in building a SAAS tool is a desired.Experience in nonfunctional aspects of software, especially performance and security is an added advantage.Exposure to full product lifecycle and strong experience in working with cross-functional teams like Dev, QA, Product Managers,Build & Release, Documentation, Customer Support etcWillingness to learn and work with new programming languages or frameworks as needed
College Preference : no-bar
Min Qualification : ug
Skills : aws, java, java script, jquery, nosql, python
Location : Mumbai
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/full-stack-developer-mumbai-3-5-years-of-experience/
Collection Analyst- Gurgaon (6-10 Years Of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Full-Stack Developer- Mumbai (3-5 Years of Experience)|Software Engineer  Machine Learning- Bangalore (3+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 6  10 years
Requirements : Team player Confident Have good knowledge on Power point presentation and Reporting
Task Info : Manages:1.Manages Management and Operational Collection reporting.2.Data analysis to monitor delinquency trends.3.Identify key issues in the portfolio and suggest improvements.4.Ensure most cost effective strategies based on segmentation of customersPrimary objective:1.Identify patterns which trigger increasing delinquencies through data analysis. Identify trends and suggests improvement2.Management and Operational analytical reporting and coordinating with various departmentsKey Responsibilities:1.Create segmentation in the portfolio based on the risk attached to the clients for efficient handling.2.Team handling of 8-10 Analyst3.Monitoring portfolio for increasing delinquency patterns based on the portfolio triggers.4.Creating standard and Ad hoc management reporting and analysis for portfolio review and monitoring and ensuring timely & error free publishing.5.Analyze data to identify early warning signals (Productivity, efficiency & effectiveness) and proactively provide feedback in order to take corrective actions.6.Effective trigger based on monitoring to ensure adherence to the process in place and regular improvement ideas for initiating positive changes in the process.7.Document all changes and strategies for ready reference.8.Create regular champion challenges to drive continuous improvement(improved performance, reducing cost)Experience & Expertise:1.At-least 6-10 years of relevant work experience in a Bank/NBFC/Consultancy/KPO/Credit card companies.2.Relevant work experience is defined as an Analyst with high competency in using Analytical tools like SQL, OBI OR SAS and analytical bend of mind in Collections Analytics.Need to have areas:1.SQL advanced working knowledge2.Excel and MS Access advanced working knowledge3.Analytical bend of mind4.Excellent presentation skills.
College Preference : no-bar
Min Qualification : ug
Skills : banking, data analysis, excel, sas, sql
Location : Gurugram
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/collection-analyst-gurgaon-6-10-years-of-experience/
Software Engineer  Machine Learning- Bangalore (3+ Years of Experience),Learn everything about Analytics,"Share this:|Like this:|Related Articles|Collection Analyst- Gurgaon (6-10 Years Of Experience)|Abinitio  Sr profile (The Lead/Architect/Designer)- Bangalore/Gurgaon ( 10+ Years of Experience)|

|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Experience : 3  years
Requirements : 
Task Info : Job Responsibilities:Design & build machine learning algorithms to solve business problems using different techniques like clustering, recommendation, classification.Develop statistical or machine-assisted approaches to problems at massive scale.Build models to solve business problems such as customer propensity to book, propensity to drop-off and dynamic pricing, recommendation engine etc.Experience:At least 3 yrs+ experience speicifically in building/developing machine learning and predictive algorithms.Proficiency in languages such as Python, R and Java.Experience in developing software within a distributed computation framework such as Hadoop, Spark, Mahout etc.Should also have excellent familiarity with SQL.Qualifications:B.E / M.S in Computer Science/Engineering.
College Preference : no-bar
Min Qualification : ug
Skills : hadoop, java, machine learning, predictive model, python, r, spark, sql
Location : Bengaluru
APPLY HERE",https://www.analyticsvidhya.com/blog/2017/01/software-engineer-machine-learning-bangalore-3-years-of-experience/
