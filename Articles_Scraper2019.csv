Header1,Header2,Header3,Header4,Header5,Header6,Text,Source Link
How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy,Learn everything about Analytics|Overview|Introduction|Table of Contents|Introduction to Information Extraction|Semantic Relationships: Get Structured Knowledge from Unstructured Text|Different Approaches to Information Extraction|Information Extraction using Python and spaCy|End Notes,"1. spaCys Rule-Based Matching|2. Subtree Matching for Relation Extraction|Share this:|Related Articles|4 Unique Methods to Optimize your Python Code for Data Science|
Prateek Joshi
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

 How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I rely heavily on search engines (especially Google) in my daily role as a data scientist. My search results span a variety of queries  Python code questions, machine learning algorithms, comparison of Natural Language Processing (NLP) frameworks, among other things.Ive always been curious about how these search engines understand my query and extract the relevant results as if they know what I am thinking.I wanted to understand how the NLP aspect works here  basically, how does the algorithm understand unstructured text data and convert that into structured data and show me relevant results?Lets take an example. I entered two different queries on Google:In the first instance, Google quickly identified the entity (world cup) and the action (won). In the second query, I didnt even finish the sentence before I got the result!How do you think Google understood the context behind these queries? Its a fascinating thought and we are going to unbox it in this article. We will understand the core idea behind how these meaningful and relevant results are generated based on our search queries. And yes, we will even dive into the Python code and take a hands-on approach to this. Lets dive in!Note: I recommend going through this article on Introduction to Computational Linguistics and Dependency Trees in data science to get a better feel for what we will learn here.Information Extraction (IE) is a crucial cog in the field of Natural Language Processing (NLP) and linguistics. Its widely used for tasks such as Question Answering Systems, Machine Translation, Entity Extraction, Event Extraction, Named Entity Linking, Coreference Resolution, Relation Extraction, etc.In information extraction, there is an important concept of triples.A triple represents a couple of entities and a relation between them. For example, (Obama, born, Hawaii) is a triple in which Obama and Hawaii are the related entities, and the relation between them is born.In this article, we will focus on the extraction of these types of triples from a given text.Before moving ahead, lets take a look at the different approaches to Information Extraction. We can broadly divide Information Extraction into two branches as shown below:In Traditional Information Extraction, the relations to be extracted are pre-defined. In this article, we will cover the rule-based methods only.In Open Information Extraction, the relations are not pre-defined. The system is free to extract any relations it comes across while going through the text data.Have a look at the text snippet below:Can you think of any method to extract meaningful information from this text? Lets try to solve this problem sentence by sentence:In the first sentence, we have two entities (Food Tutorials and Wes Anderson). These entities are related by the term Directed. Hence, (Wes Anderson, directed, Food Tutorials) is a triple. Similarly, we can extract relations from the other sentences as well:It turns out that we can get structured information based on the syntactic structure and grammar of the text, as illustrated in the example above.In the previous section, we managed to easily extract triples from a few sentences. However, in the real world, the data size is huge and manual extraction of structured information is not feasible. Therefore, automating this information extraction becomes important.There are multiple approaches to perform information extraction automatically. Lets understand them one-by-one:We have a grasp on the theory here so lets get into the Python code aspect. Im sure youve been itching to get your hands on this section!We will do a small project to extract structured information from unstructured data (text data in our case). Weve already seen that the information in text lies in the form of relations between different entities.Hence, in this section, we will try to discover and extract different entity pairs that are associated with some relation or another.Before we get started, lets talk about Marti Hearst. She is a computational linguistics researcher and a professor in the School of Information at the University of California, Berkeley. How does she fit into this article? I can sense you wondering.Professor Marti has actually done extensive research on the topic of information extraction. One of her most interesting studies focuses on building a set of text-patterns that can be employed to extract meaningful information from text. These patterns are popularly known as Hearst Patterns.Lets look at the example below:We can infer that Gelidium is a part of red algae just by looking at the structure of the sentence.In linguistics terms, we will call red algae as Hypernym and Gelidium as its Hyponym.We can formalize this pattern as X such as Y, where X is the hypernym and Y is the hyponym. This was one of the many patterns from the Hearst Patterns. Heres a list to give you an intuition behind the idea:Now lets try to extract hypernym-hyponym pairs by using these patterns/rules. We will use spaCys rule-based matcher to perform this task.First, we will import the required libraries:Next, load a spaCy model:We are all set to mine information from text based on these Hearst Patterns.To be able to pull out the desired information from the above sentence, it is really important to understand its syntactic structure  things like the subject, object, modifiers, and parts-of-speech (POS) in the sentence.We can easily explore these syntactic details in the sentence by using spaCy:Output:Have a look around the terms such and as . They are followed by a noun (countries). And after them, we have a proper noun (Vietnam) that acts as a hyponym.So, lets create the required pattern using the dependency tags and the POS tags:Lets extract the pattern from the text:Output: countries such as VietnamNice! It works perfectly. However, if we could get developing countries instead of just countries, then the output would make more sense.So, we will now also capture the modifier of the noun just before such as by using the code below:Output: developing countries such as VietnamHere, developing countries is the hypernym and Vietnam is the hyponym. Both of them are semantically related.Note: The key OP: ? in the pattern above means that the modifier (amod) can occur once or not at all.In a similar manner, we can get several pairs from any piece of text:Now lets use some other Hearst Patterns to extract more hypernyms and hyponyms.Output:Output: car and other vehiclesOutput: car and other vehiclesLets try out the same code to capture the X or Y pattern:The rest of the code will remain the same:Output: car or other vehiclesExcellent  it works!Output:Output: Eight people, including two childrenOutput: Eight people, including two childrenOutput:Output: fruits, especially whole fruitsOutput: fruits, especially whole fruitsThe simple rule-based methods work well for information extraction tasks. However, they have a few drawbacks and shortcomings.We have to be extremely creative to come up with new rules to capture different patterns. It is difficult to build patterns that generalize well across different sentences.To enhance the rule-based methods for relation/information extraction, we should try to understand the dependency structure of the sentences at hand.Lets take a sample text and build its dependency graphing tree:Output:Can you find any interesting relation in this sentence?If you look at the entities in the sentence  Tableau and Salesforce  they are related by the term acquired. So, the pattern I can extract from this sentence is either Salesforce acquired Tableau or X acquired Y.Now consider this statement: Careem, a ride-hailing major in the middle east, was acquired by Uber.Its dependency graph will look something like this:Pretty scary, right?Dont worry! All we have to check is which dependency paths are common between multiple sentences. This method is known as Subtree matching.For instance, if we compare this statement with the previous one:We will just consider the common dependency paths and extract the entities and the relation (acquired) between them. Hence, the relations extracted from these sentences are:Lets try to implement this technique in Python. We will again use spaCy as it makes it pretty easy to traverse a dependency tree.We will start by taking a look at the dependency tags and POS tags of the words in the sentence:Output:Here, the dependency tag for Tableau is nsubjpass which stands for a passive subject (as it is a passive sentence). The other entity Salesforce is the object in this sentence and the term acquired is the ROOT of the sentence which means it somehow connects the object and the subject.Lets define a function to perform subtree matching:In this case, we just have to find all those sentences that:We can then capture the subject and the object from the sentences. Lets call the above function:Output: (Salesforce, Tableau)Here, the subject is the acquirer and the object is the entity that is getting acquired. Lets use the same function, subtree_matcher( ), to extract entities related by the same relation (acquired):Output: (Uber, Careem)Did you see what happened here? This sentence had more words and punctuation marks but still, our logic worked and successfully extracted the related entities.But wait  what if I change the sentence from passive to active voice? Will our logic still work?Output: (Tableau,  )Thats not quite what we expected. The function has failed to capture Salesforce and wrongly returned Tableau as the acquirer.So, what could go wrong? Lets understand the dependency tree of this sentence:Output:It turns out that the grammatical functions (subject and object) of the terms Salesforce and Tableau have been interchanged in the active voice. However, now the dependency tag for the subject has changed to nsubj from nsubjpass. This tag indicates that the sentence is in the active voice.We can use this property to modify our subtree matching function. Given below is the new function for subtree matching:Lets try this new function on the active voice sentence:Output: (Salesforce, Tableau)Great! The output is correct. Lets pass the previous passive sentence to this function:Output: (Salesforce, Tableau)Thats exactly what we were looking for. We have made the function slightly more general. I would urge you to deep dive into the grammatical structure of different types of sentences and try to make this function more flexible.In this article, we learned about Information Extraction, the concept of relations and triples, and different methods for relation extraction. Personally, I really enjoyed doing research on this topic and am planning to write a few more articles on more advanced methods for information extraction.Although we have covered a lot of ground, we have just scratched the surface of the field of Information Extraction. The next step is to use the techniques learned in this article on a real-world text dataset and see how effective these methods are.If youre new to the world of NLP, I highly recommend taking our popular course on the subject:",https://www.analyticsvidhya.com/blog/2019/09/introduction-information-extraction-python-spacy/
4 Unique Methods to Optimize your Python Code for Data Science,Learn everything about Analytics|Overview|Introduction|What is Optimization?|1. Pandas.apply()  A Feature Engineering Gem|2. Pandas.DataFrame.loc  A Brilliant Hack for Data Manipulation in Python|3. Vectorize your Functions in Python|4. Multiprocessing in Python|End Notes,"Share this:|Related Articles|How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy|A Beginner-Friendly Guide to PyTorch and How it Works from Scratch|
LAKSHAY ARORA
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a programmer at heart. Ive been doing programming since well before my university days and I continue to be amazed at the sheer number of avenues that open up using simple Python code.But I wasnt always efficient at it. I believe this is a trait most programmers share  especially those who are just starting out. The thrill of writing code always takes precedence over how efficient and neat it is. While this works during our college days, things are wildly different in a professional environment, especially a data science project.Writing optimized Python code is very, very important as a data scientist. There are no two ways about it  a messy, inefficient notebook will cost you time and your project a lot of money. As experienced data scientists and professionals know, this is unacceptable when were working with a client.So in this article, I draw on my years of experience in programming to list down and showcase four methods you can use to optimize Python code for your data science project.If youre new to the world of Python (and Data Science), I recommend going through the below resources:Lets first define what optimization is. And well do this using an intuitive example.Heres our problem statement:Suppose we are given an array where each index represents a city and the value of that index represents the distance between that city and the next city. Lets say we have two indices and we need to calculate the total distance between those two indices. In simple terms, we need to find the total sum between any two given indices.The first thought that comes to mind is that a simple FOR loop will work well here. But what if there are 100,000+ cities and we are receiving 50,000+ queries per second? Do you still think a FOR loop will give us a good enough solution for our problem?Not really. And this is where optimizing our code works wonders.Code optimization, in simple terms, means reducing the number of operations to execute any task while producing the correct results.Lets calculate the number of operations a FOR loop will take to perform this task:We have to figure out the distance between the city with index 1 and index 3 in the above array.What if the array size is 100,000 and the number of queries is 50,000?This is quite a massive number. Our FOR loop will take a lot of time if the size of the array and the number of queries are further increased. Can you think of an optimized method where we can produce the correct results while using a lesser number of solutions?Here, I will talk about a potentially better solution to solve this problem by using the prefix array to calculate the distances. Lets see how it works:Can you understand what we did here? We got the same distance with just one operation! And the best thing about this method is that it will take just one operation to calculate the distance between any two indices, regardless of if the difference between the indices is 1 or 100,000. Isnt that amazing?I have created a sample dataset with an array size of 100,000 and 50,000 queries. Lets compare the time taken by both the methods in the live coding window below.Note: The dataset has a total of 50,000 queries and you can change the parameter execute_queries to execute any number of queries up to 50,000 and see the time taken by each method to perform the task.Pandas is already a highly optimized library but most of us still do not make the best use of it. Think about the common places in a data science project where you use it.One function I can think of is Feature Engineering where we create new features using existing features. One of the most effective ways to do this is using Pandas.apply().Here, we can pass a user-defined function and apply it to every single data point of the Pandas series. It is one of the best add-ons to the Pandas library as this function helps to segregate data according to the conditions required. We can then efficiently use it for data manipulation tasks.Lets use the Twitter sentiment analysis data to calculate the word count for each tweet. We will be using different methods, like the dataframe iterrows method, NumPy array, and the apply method. Well then compare it in the live coding window below. You can download the data set from here.You might have noticed that the apply function is much faster than the iterrows function. Its performance is comparable to the NumPy array but the apply function provides much more flexibility. You can read more about its documentation here.This is one of my favorite hacks of the Pandas library. I feel this is a must-know method for data scientists who deal with data manipulation tasks (so almost everyone then!).Most of the time we are required to update only some values of a particular column in a dataset based upon some condition. Pandas.DataFrame.loc gives us the most optimized solution for these kinds of problems.Lets solve a problem using this loc function. You can download the dataset well be using here.Check the value counts of the City variable:Now, lets say we want only the top 5 cities and want to replace the rest of the cities as Others. So lets do that:See how easy it was to update the values? This is the most optimized way to solve a data manipulation task of this kind.Another way to get rid of slow loops is by vectorizing the function. This means that a newly created function will be applied on a list of inputs and will return an array of results. Vectorizing in Python can speed up your computation by at least two iterations.Lets verify this in the live coding window below on the same Twitter Sentiment Analysis Dataset.",https://www.analyticsvidhya.com/blog/2019/09/4-methods-optimize-python-code-data-science/
A Beginner-Friendly Guide to PyTorch and How it Works from Scratch,Learn everything about Analytics|Overview|Introduction|Table of Contents|Getting Started with PyTorch|Basics of PyTorch|Common PyTorch Modules|Building a Neural Network from Scratch in PyTorch|Solving an Image Classification Problem using PyTorch|End Notes,"TorchScript|Distributed Training|Python Support|Dynamic Computation Graphs|Introduction to Tensors|Mathematical operations|Matrix Initialization|Matrix Operations|Concatenating Tensors|Reshaping Tensors|Autograd Module|Optim Module|nn Module|Loading the data|Training the model|Getting predictions|Share this:|Related Articles|4 Unique Methods to Optimize your Python Code for Data Science|9 Powerful Tips and Tricks for Working with Image Data using skimage in Python|
Pulkit Sharma
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every once in a while, there comes a library or framework that reshapes and reimagines how we look at the field of deep learning. The remarkable progress a single framework can bring about never ceases to amaze me.I can safely say PyTorch is on that list of deep learning libraries. It has helped accelerate the research that goes into deep learning models by making them computationally faster and less expensive (a data scientists dream!).Ive personally found PyTorch really useful for my work. I delve heavily into the arts of computer vision and find myself leaning on PyTorchs flexibility and efficiency quite often.So in this article, I will guide you on how PyTorch works, and how you can get started with it today itself. Well cover everything there is to cover about this game-changing deep learning library and also take up a really cool case study to see PyTorch in action.PyTorch is a Python-based library that provides maximum flexibility and speed.Ive found PyTorch to be as simple as working with NumPy  and trust me, that is not an exaggeration.You will figure this out really soon as we move forward in this article. But before we dive into the nuances of PyTorch, lets look at some of the key features of this library which make it unique and easy to use.PyTorch TorchScript helps to create serializable and optimizable models. Once we train these models in Python, they can be run independently from Python as well. This helps when were in the model deployment stage of a data science project.So, you can train a model in PyTorch using Python and then export the model via TorchScript to a production environment where Python is not available. We will discuss model deployment in more detail in the later articles of this series.PyTorch also supports distributed training which enables researchers as well as practitioners to parallelize their computations. Distributed training makes it possible to use multiple GPUs to process larger batches of input data. This, in turn, reduces the computation time.PyTorch has a very good interaction with Python. In fact, coding in PyTorch is quite similar to Python. So if you are comfortable with Python, you are going to love working with PyTorch.PyTorch has a unique way of building neural networks. It creates dynamic computation graphs meaning that the graph will be created on the fly:And this is just skimming the surface of why PyTorch has become such a beloved framework in the data science community.Right  now its time to get started with understanding the basics of PyTorch. So make sure you install PyTorch on your machine before proceeding. The latest version of PyTorch (PyTorch 1.2) was released on August 08, 2019 and you can see the installation steps for it using this link.Remember how I said PyTorch is quite similar to Numpy earlier? Lets build on that statement now. I will demonstrate basic PyTorch operations and show you how similar they are to NumPy.In the NumPy library, we have multi-dimensional arrays whereas in PyTorch, we have tensors. So, lets first understand what tensors are.Tensors are multidimensional arrays. And PyTorch tensors are similar to NumPys n-dimensional arrays. We can use these tensors on a GPU as well (this is not the case with NumPy arrays). This is a major advantage of using tensors.PyTorch supports multiple types of tensors, including:Now, lets look at the basics of PyTorch along with how it compares against NumPy. Well start by importing both the NumPy and the Torch libraries:Now, lets see how we can assign a variable in NumPy as well as PyTorch:Lets quickly look at the type of both these variables:Type here confirms that the first variable (a) here is a NumPy array whereas the second variable (b) is a torch tensor.Next, we will see how to perform mathematical operations on these tensors and how it is similar to NumPys mathematical operations.Do you remember how to perform mathematical operations on NumPy arrays? If not, let me quickly recap that for you.We will initialize two arrays and then perform mathematical operations like addition, subtraction, multiplication, and division, on them:These are the two NumPy arrays we have initialized. Now lets see how we can perform mathematical operations on these arrays:Lets now see how we can do the same using PyTorch on tensors. So, first, lets initialize two tensors:Next, perform the operations which we saw in NumPy:Did you see the similarities? The codes are exactly the same to perform the above mentioned mathematical operations in both NumPy and PyTorch.Next, lets see how to initialize a matrix as well as perform matrix operations in PyTorch (along with, you guessed it, its NumPy counterpart!).Lets say we want a matrix of shape 3*3 having all zeros. Take a moment to think  how can we do that using NumPy?Fairly straightforward. We just have to use the zeros() function of NumPy and pass the desired shape ((3,3) in our case), and we get a matrix consisting of all zeros. Lets now see how we can do this in PyTorch:Similar to NumPy, PyTorch also has the zeros() function which takes the shape as input and returns a matrix of zeros of a specified shape. Now, while building a neural network, we randomly initialize the weights for the model. So, lets see how we can initialize a matrix with random numbers:We have specified the random seed at the beginning here so that every time we run the above code, the same random number will generate. The random.randn() function returns random numbers that follow a standard normal distribution. But lets not get waylaid by the statistics part of things. Well focus on how we can initialize a similar matrix of random numbers using PyTorch:This is where even more similarities with NumPy crop up. PyTorch also has a function called randn() that returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).Note that we have set the random seed here as well just to reproduce the results every time you run this code. So far, we have seen how to initialize a matrix using PyTorch. Next, lets see how to perform matrix operations in PyTorch.We will first initialize two matrices in NumPy:Next, lets perform basic operations on them using NumPy:Matrix transpose is one technique which is also very useful while creating a neural network from scratch. So lets see how we take the transpose of a matrix in NumPy:The transpose() function of NumPy automatically returns the transpose of a matrix. How does this happen in PyTorch? Lets find out:Note that the .mm() function of PyTorch is similar to the dot product in NumPy. This function will be helpful when we create our model from scratch in PyTorch. Calculating transpose is also similar to NumPy:Next, we will look at some other common operations like concatenating and reshaping tensors. From this point forward, I will not be comparing PyTorch against NumPy as you must have got an idea of how the codes are similar.Lets say we have two tensors as shown below:What if we want to concatenate these tensors vertically? We can use the below code:As you can see, the second tensor has been stacked below the first tensor. We can concatenate the tensors horizontally as well by setting the dim parameter to 1:Lets say we have the following tensor:We can use the .reshape() function and pass the required shape as a parameter. Lets try to convert the above tensor of shape (2,4) to a tensor of shape (1,8):Awesome! PyTorch also provides the functionality to convert NumPy arrays to tensors. You can use the below code to do it:With me so far? Good  lets move on and dive deeper into the various aspects of PyTorch.PyTorch uses a technique called automatic differentiation. It records all the operations that we are performing and replays it backward to compute gradients. This technique helps us to save time on each epoch as we are calculating the gradients on the forward pass itself.Lets look at an example to understand how the gradients are computed:Here, we have initialized a tensor. Specifying requires_grad as True will make sure that the gradients are stored for this particular tensor whenever we perform some operation on it. Lets now perform some operations on the defined tensor:First of all, we added 5 to all the elements of this tensor and then taken the mean of that tensor. We will first manually calculate the gradients and then verify that using PyTorch. We performed the following operations on a:b = a + 5c = mean(b) = (a+5) / 4Now, the derivative of c w.r.t. a will be  and hence the gradient matrix will be 0.25. Lets verify this using PyTorch:As expected, we have the gradients.The autograd module helps us to compute the gradients in the forward pass itself which saves a lot of computation time of an epoch.The Optim module in PyTorch has pre-written codes for most of the optimizers that are used while building a neural network. We just have to import them and then they can be used to build models.Lets see how we can use an optimizer in PyTorch:Above are the examples to get the ADAM and SGD optimizers. Most of the commonly used optimizers are supported in PyTorch and hence we do not have to write them from scratch. Some of them are:The autograd module in PyTorch helps us define computation graphs as we proceed in the model. But, just using the autograd module can be low-level when we are dealing with a complex neural network.In those cases, we can make use of the nn module. This defines a set of functions, similar to the layers of a neural network, which takes the input from the previous state and produces an output.We will use all these modules and define our neural network to solve a case study in the later sections. For now, lets build a neural network from scratch that will help us understand how PyTorch works in a practical way.I hope you are comfortable with building a neural network from scratch using NumPy. If not, I highly recommend you go through this article.Alright  time to get started with neural networks! This is going to be a lot of fun so lets get right down to it. We will first initialize the input and output:Next, we will define the sigmoid function which will act as the activation function and the derivative of the sigmoid function which will help us in the backpropagation step:Next, initialize the parameters for our model including the number of epochs, learning rate, weights, biases, etc.:Here we have randomly initialized the weights and biases using the .randn() function which we saw earlier. Finally, we will create a neural network. I am taking a simple model here just to make things clear. There is a single hidden layer and an input and an output layer in the model:In the forward propagation step, we are calculating the output and finally, in the backward propagation step, we are calculating the error. We will then update the weights and biases using this error.Lets now look at the output from the model:So, the target is 1, 1, 0 and the predicted values from the model are 0.98, 0.97 and 0.03. Not bad at all!This is how we can build and train a neural network from scratch in PyTorch. Lets now take things up a notch and dive into a case study. We will try to solve that case study using the techniques we have learned in this article.Youre going to love this section. This is where all our learning will culminate in a final neural network model on a real-world case study.Our task is to identify the type of apparel by looking at a variety of apparel images. Its a classic image classification problem using computer vision. This dataset, taken from the DataHack Platform, can be downloaded here.There are a total of 10 classes in which we can classify the images of apparels:There are 70,000 images, out of which 60,000 are in the training set and the remaining 10,000 in the test set. All the images are grayscale images of size (28*28).The dataset contains two folders  one each for the training set and the test set. In each folder, there is a .csv file that has the id of the image and its corresponding label and a folder containing the images for that particular set.Lets now get started with the code! We will first import the required libraries:Next, read the .csv file that we downloaded from the competition page.id here represents the name of the image (we just have to add .png as the images are in png format) and the label is the corresponding class of that particular image.Lets now plot an image to get a better understanding of how our data looks. We will randomly select an image and plot it. So, lets first create a random number generator:Plot an image:This is a random image from our dataset and gives us an idea of what all the other images look like. Next, we will load all the training images using the train.csv file. We will use a for loop to read all the images from the training set and finally store them as a NumPy array:So, there are 60,000 images in the training set each of shape 28 x 28. We will be making a simple neural network which takes a one-dimensional input and hence we have to flatten these two-dimensional images into a single dimension:We have reshaped the images to a single dimension. So far, we have created the input set but we also need the target to train the model, right? So, lets go ahead and create that:Lets create a validation set to evaluate how well our model will perform on unseen data:We have taken 10 percent of the training data in the validation set.Now, its time to define our model. We will first import the Torch package and the required modules:Next, define the parameters like the number of neurons in the hidden layer, the number of epochs, and the learning rate:Finally, lets build the model! For now, we will have a single hidden layer and choose the loss function as cross-entropy. We will be using the Adam optimizer here. Remember that there are other parameters of our model and you can change them as well.Lets now train the model for a specified number of epochs and save the training and validation loss for each epoch:Here, I have printed the training losses after every second epoch and we can see that the loss is decreasing. Lets now plot the training and validation loss to check whether they are in sync or not:Perfect! We can see that the training and validation losses are in sync and the model is not overfitting. Lets now check how accurate our model is on predicting the classes for both training and validation sets. Well start by looking at the training accuracy:We got an accuracy of above 65% on the training set. Lets check for the validation set as well:We have an almost similar performance on the validation set. Note that even though we have used a very simple architecture with just one hidden layer, the performance is pretty good.You can try to increase the number of hidden layers or play with other model parameters like the optimizer function, the number of hidden units, etc. and try to improve the performance further.Finally, lets load the test images, make predictions on them and submit the predictions on the competition page. We will have to preprocess the test images in a similar way that we did for the training images:Lets convert these images to a 1-d array now:Finally, we will make predictions for these images:Great, we now have the predictions. We will now save these predictions in the sample submission file:Replace these labels with the predictions that we got from the model for test images:Save this sample submission file and make a submission on the competition page:After submitting these predictions, we get an accuracy of 64.625% on the leaderboard. You can use this accuracy as a benchmark and try to improve on this by playing around with the parameters of the above model.In this article, we understood the basic concepts of PyTorch including how its quite intuitively similar to NumPy. We also saw how to build a neural network from scratch using PyTorch.We then took a case study where we solved an image classification problem and got a benchmark score of around 65% on the leaderboard. I encourage you to try and improve this score by changing different parameters of the model, including the optimizer function, increasing the number of hidden layers, tuning the number of hidden units, etc.This article is the start of our journey with PyTorch. In the upcoming articles, we will learn how to build convolutional neural networks in PyTorch, model checkpointing techniques, and how to deploy trained models using PyTorch. So stay tuned!As always, if you have any doubts related to this article, feel free to post them in the comments section below. And make sure you check out our course if youre interested in computer vision and deep learning:",https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/
9 Powerful Tips and Tricks for Working with Image Data using skimage in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is skimage and Why Should We Use it?|1. Reading Images in Python using skimage|2. Changing Image Format|3. Resizing Images using skimage|4. Rescaling (Upscale/Downscale) Images using skimage|5. Rotate an Image by Different Angles using skimage|6. Flip Images Horizontally and Vertically|7. Crop Images|8. Altering Image Brightness using skimage|9. Using Filters in skimage|End Notes,"Loading Images from skimage|Reading Images from our System using skimage|Share this:|Related Articles|A Beginner-Friendly Guide to PyTorch and How it Works from Scratch|4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know|
Aishwarya Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats the first thing that comes to your mind when you hear image preprocessing? I received a few quizzical looks when I asked this question to a group of data science enthusiasts. If youre wondering what this is, read on!Were pretty familiar with the preprocessing steps for structured (tabular) data. You find and plug in any missing values, detect and deal with outliers, etc. This helps us build better and more robust machine learning models. But how does that work when were working with image data?As it turns out, the preprocessing step is a crucial one in the world of computer vision (images, videos, and so on). skimage, part of the scikit-learn family, is a really helpful library to get us started.In this article, we will look at some simple yet powerful preprocessing techniques for images using skimage in Python. This is a good starting point for your computer vision journey so happy learning!And if computer vision is your career of choice, or you want to learn more about how to work with images, build object detection models and more, check out the below course:There are multiple libraries and frameworks in Python that let us work with image data. So why should we use skimage? Its a fair question so let me answer that here before we dive into the article.Scikit-image, or skimage, is an open source Python package designed for image preprocessing.If you have previously worked with sklearn, getting started with skimage will be a piece of cake. Even if you are completely new to Python, skimage is fairly easy to learn and use.What I really like about skimage is that it has a well-structured documentation that lists down all the modules, sub-modules and functions provided within skimage. Here is the list of all the sub-modules and functions within the skimage package: API Reference.Lets start with the basics. The very first step is learning how to import images in Python using skimage.An image is made up of multiple small square boxes called pixels. The image Ive shown below is a perfect example of this. The small squares you see here are the pixels:We can see that this image has 22 pixels along the vertical line and 16 pixels horizontally. Hence, the size of this image would be 22 x 16.When we read or load an image using scikit-image (or any other package for that matter), we see that the image is stored in the form of numbers. These numbers are called pixel values and they represent the intensity of each pixel in the image.Within the scikit-image package, there are several sample images provided in the data module. Lets say we want to load a single image to perform a few experiments. Instead of using an external image, we can simply load one of the images provided within the package!Here is the Python code to do this:Notice that I have used the imshow function here to view the image in the notebook itself. Simple, right?What if you want to load an image from your machine instead of the ones provided in the package? Im sure thats what youll want to do eventually. For this, we can use the imread function from skimage.We can read images in two formats  colored and grayscale. We will see both of these in action and understand how theyre different.The imread function has a parameter as_gray which is used to specify if the image must be converted into a grayscale image or not. We will start with reading an image in grayscale format, by setting the parameter to true:We are easily able to view the image using the imshow function. But is that really how the image is stored? Let us check what we have in the variable image_gray:The variable stores the image in the form of a matrix of numbers. As you can see, the shape of the matrix is 259 x 195. These numbers are called pixel values and they denote the intensity of the pixels in the images.Now, well load the image in the original color format. For this, we will have to set the parameter as_gray to False:Nice! We have the same image here in a colored format. Now you might be wondering what is the difference between the two and which format should you use? Good questions  so lets address them one by one.Did you notice the shape of the image in this case? It is (258, 195, 3) while previously the shape was (258, 195). The three dimensions here represent the number of channels in the image. For a colored image, the most popular format for storing images is RGB (Red-Green-Blue).But which format should we use? The colored images have more information as compared to the grayscale images but the size of the images is very large. The number of pixels in RGB is 3 times more. This can present quite a challenge when were restricted by low compute power.Hence, grayscale images are often used to reduce the computational complexity. So if the size of your dataset is very large, you can choose to go for grayscale over colored.In the last section we discussed about two important formats in which we can load the images  RGB and grayscale. In this section we will learn how to convert an image from one format to another. To start with, we will read an image in RGB format and convert it into the grayscale format. The function we will use here is rgb2grayOther two popular formats are HSV (hue, saturation, value) and HSL (hue, saturation, lightness) which are alternative representations of the RGB format. Let me briefly explain what each of these terms mean.The image shown below will make your understanding more clear- Source: WikipediaChanging the image to any of these formats will be the same as we did for converting to grayscale. We can use the functions rgb2hsl and rgb2hsv to convert into HSL and HSV format respectively. Here I have demonstrated the conversion of image to HSV format.One of the biggest challenges in computer vision is that we require a huge amount of data for training our model. The data we collect is often from different sources which might result in variation in the size of the images. This might be a problem while extracting features from the images, or using the same for data augmentation.Ideally, the size of the images should be the same when were building our model. If were using a pre-trained model, it is important to resize and normalize the input data to the same format over which the network was originally trained. This is why resizing images is an important image preprocessing step. Here, we are going to use the resize function from skimage. The input to this function will be the image we want to update and the required dimensions for the new image:Rescaling images is another common computer vision technique. This implies scaling the images by a particular factor. For example  reducing the size of each image by half (downscale) or increasing the size of images by a factor of 2 (upscale).You might argue that we can simply use the resize function for this task, what is the difference?If the original size of all the images is the same, say (300, 300), we can directly use the resize function and specify the required dimensions (150, 150). But if the size of the images is different (like the images shown below), the resize function cannot be used. This is because the half of each image would be different.And this is a use case you will encounter a lot in your computer vision journey.So here, we can use the rescale function and specify the scaling factor. All the images will be scaled by this factor, based on the original size of the image. Here is an example:So far we have looked at resizing and rescaling the images. Lets turn our focus and see how we can change the orientation of images. But before we dive into that, we should discuss why we need to change the image orientation in the first place.Consider the below images. The first image is slightly tilted (which may be due to the camera orientation).To fix this orientation problem, we will need to rotate the image by a certain angle. We can use the rotate function of skimage and specify the angle by which we need the image to be rotated:This looks great! The orientation problem is all fixed. But if you look closely, the picture is cropped around the corners. This is because, during the rotation, the size of the image remains the same causing the area around the corner to get cropped.We are not losing any important information in this scenario but that might not always be the case. This obstacle is taken care of by the resize parameter in the rotate function (by default the parameter value is False):We can also use the rotation concept for data augmentation. For those who are not familiar with the term, Data Augmentation is a technique of generating more samples for training the model, using the available data.Say you are building an image classification model to identify images of cats and dogs. Take a look at the sample images shown below. Both the images on the left would be classified as dog and the images on the right would be classified as cat: What did we change here? We simply rotated the images by 180 degrees and generated the new images. Think about it  you can double the size of the training data by simply adding one new image against every image in the existing data!We can flip an image both horizontally and vertically. This creates a mirror image along the horizontal/vertical axis. We can use this technique for both image preprocessing and image augmentation.Although there is no direct function for this in skimage, we can use NumPy to perform this task.NumPy provides functions flipud and fliplr for flipping the images across the horizontal and vertical axis respectively.The internal working of the function is very simple. For a horizontal flip, the rows remain intact while the entries in the columns are reserved. Let us take the same cat/dog example and use the flip function on it:Looking good!You must have used the cropping function on your phone a gazillion times. I do it way too often!You can crop images inside your Python notebook as well using skimage. We crop images to remove the unwanted portion of the image or to focus on a particular part of the image.Lets say we have the below image from a basketball match (left image). In its current form, the shape of the image is 1067 x 1600. Now, I want to remove 100 pixels from all 4 sides of the image. This would mean that we remove 100 pixels from the top, bottom, left and right of the image, thus focusing on the object at the center:There are two ways to achieve this:So, lets crop the above image using the second method:How often do you fiddle around with the brightness of an image you took in a bad light? Despite the recent advances in how cameras function, low light imaging is a headache. skimage will fix that for us.Images with different brightness can be used to make our computer vision model robust to changes in lighting conditions.This is important for systems that work in outdoor lighting, for instance, CCTV cameras on traffic signals.The brightness of images can be changed using the adjust_gamma function in skimage, which uses a method called gamma correlation. For any given image, the pixel values are first normalized between 0  1 and then multiplied by a specified gamma value. The resulting pixel values are scaled back to the range 0-255.For gamma greater than 1, the output image will be darker than the input image. While for gamma less than 1, the output image will be brighter than the input image.We can use Filters to modify or enhance an images features. Youll be pretty familiar with filters if youve ever played around with images on social media platforms.We can use filters for various purposes, such as smoothing and sharpening the image, removing noise, highlighting features and edges in the image, etc.When we apply a filter on an image, every pixel value is replaced by a new value generated using surrounding pixel values. The simplest filter is the median filter, where the pixel values are replaced with the median of neighboring pixels.Another popular filter is the sobel filter. We use this when we want to highlight the edges in an image. If you want to learn in detail about how the filter works, you can refer to this article.3 Beginner-Friendly Techniques to Extract Features from Image Data using PythonCongratulations on taking your first step in computer vision! It can appear to be a daunting field initially, but if you have a structured thinking mindset and a good grasp on how machine learning algorithms work, youll quickly pick up the nuances of working with image and video data.There are other things we can do using skimage, such as extracting the edges from an image, or adding noise to an image, among other things. I want you to take these two up for starters, and try them out in Python. Thats how you learn new concepts!And as I mentioned at the start of the article, you should check out our comprehensive course on computer vision:I look forward to hearing your thoughts and feedback on this article. Connect with me in the comments section below!",https://www.analyticsvidhya.com/blog/2019/09/9-powerful-tricks-for-working-image-data-skimage-python/
4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know,Learn everything about Analytics|Overview|Introduction|Table of Contents|A Quick Recap of what we have Covered in this Data Science Leaders Series|Data Science is a Part of the Whole|Synergies with the User Interface Module and Overall User Experience|The Trade-off between Compute Cost and System Accuracy|Model Interpretability|End Notes,"Search Engine|Word Processor|Share this:|Related Articles|9 Powerful Tips and Tricks for Working with Image Data using skimage in Python|A Data Scientists Guide to 8 Types of Sampling Techniques|
Om Deshmukh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Practical data science is a multi-dimensional field. Machine learning algorithms are essentially one part of the entire end-to-end data-science-driven project. I often come across early-stage data science enthusiasts who dont quite have the full picture in mind during their initial days.There are multiple practical considerations we need to account for while building a data-science-driven solution for real-world situations. And heres the curious part  it isnt just limited to the data side of things!Some of the more crucial cogs in a data-science-driven project include:So in this article, I draw on my own experience to talk about how a leader needs to think about a data-science-driven project. Getting to a data-driven solution isnt a straight and pre-defined path. We need to be aware of the various facets of maintaining and deploying our data-driven solution.This article is the continuation of a four article series that focuses on sharing insights on various components involved in successfully implementing a data science project.I want to quickly summarize the first two articles in this series in case you have not read them yet.In the first article, we spoke about the three key constituents whose goals need to be aligned for the successful development and deployment of data-driven products. These constituents are:These are essentially the key stakeholders in a data science project. You can read the full article, and a detailed breakdown, of each stakeholder here.In the second article, we discussed ways to bridge the gap between qualitative business requirements and quantitative inputs to machine learning models.In particular, we spoke about defining the success criteria of data-driven products in a way that progress can be measured in tangible quantitative terms.The article also provided a framework to capture the appropriate granularity of data with consistent human labels to train accurate machine learning models. Finally, the article reflected on how the right team composition is critical for the end-to-end success.You can read the second article here. And now its time to dive into the third article of this series!Take a moment to think about some of the data-driven products that you use regularly. Theres a good chance you might have thought of one of the following:Two more specialized examples would be:A critical component in the data science module of each of these data-driven products would be able to perform content organization and information retrieval.And yet, the Information Retrieval (IR) component will need to be significantly tailored based on the end-use case. Likewise, the flow of information will need to be modified based on the functioning of the IR component.Lets understand this using the examples we mentioned above:As the data science delivery owner, you have to know the end-to-end use case, evaluate the various constraints that it may impose on your solution and also identify the various degrees of freedom it may give you. We will discuss some of the specifics below.I like to think of the data science component as one piece of a jigsaw puzzle: significant in its own right but needs to fit in snugly with the rest of the puzzle!In all practical applications, the data science component is only an enabling technology and never the complete solution by itself.Users interact with the end application through a User Interface (UI). The User Experience (UX) should be designed in a way that is synergistic with the powers of the underlying data science component while camouflaging its shortcomings.Let me illustrate an optimal way of synergizing UI/UX with the data science component using two different examples:A typical web search engine uses heavy data science machinery to rank and categorize WebPages. It then returns the most relevant ones in response to the users query.If the data science component can interpret the query with high confidence and extract the exact specific answer, the user interface can utilize this confidence to just display the answer as a zero-click-result. This will lead to a seamless UX.Source: searchenginejournal.comGoogle applies this. For queries like prime minister of India, it returns the answers as knowledge panels. On the other hand, when the data science confidence on the exact answer is below a certain threshold, it is safe to let the user interact with the system through a few more clicks to get to the specific answer rather than risking a bad user experience.When we search for where do MPs of India meet, Googles first link has the right answer but because the confidence on the exact answer snippet is lower, it doesnt show a knowledge panel.There is another way to exploit UI/UX synergy with the data science component. The users interactions with the system can also be used to generate indirect labeled data and also as a proxy on the systems performance evaluation.For example, imagine a scenario where a web search engine returns the top 10 results for a given query and the user almost always clicks on either the second or the third link. This implies that the underlying data science component needs to revisit its ranking algorithm so that the second and the third links are ranked higher than the first one.The wisdom-of-crowd also provides the labeled pair of query-and-relevant-Webpage. Admittedly, labeled pairs inferred in such a way will include a variety of user biases. Hence, a nontrivial label normalization process is needed before these labels can be used for training the data science component.Similarly, consider a typical spell-checker in a word processor. The underlying data science machinery is tasked with recognizing when a typed word is likely a spelling mistake, and if so, highlighting the misspelled word and suggesting likely correct words.Thus, the data science team must understand all transformations of the data science-driven output that will go through before reaching the end users hands. And the UI designers and engineers should understand the nature of likely errors that the data science component will make.The data science delivery leader has to drive this collaboration across the teams to provide an optimal end solution. Also, notice that I mentioned high confidence and low confidenceabove, whereas what the machines will need is confidence above 83%. This is the qualitative to quantitative gap that we discussed in the previous article of this series.The next aspect that the teams have to build a common understanding of is about the nature of the users interaction with the end-to-end system.Lets take the example of a speech-to-text system. Here, if the expected setup is such that the user uploads a set of speech files and expects an auto-email-alert when the speech-to-text outputs are available, the data science system can take a considerable amount of time to generate the best quality output.On the other hand, what happens if the user interaction is such that the user speaks a word/phrase and waits for the system to respond? The data science system architecture will have to be such that it trades for a higher compute cost to generate instantaneous results with high accuracy.Knowing the full context in which the data science system will be deployed can also help to make informed trade-offs between the data science systems ability to compute efficiency and overall accuracy.In the above example of speech-to-text, we know that the end-to-end system restricts the user to speak only the names of people in his/her phone-book. So here, the data science component can restrict its search space to the names in the phone-book rather than searching through millions of peoples names.The amount of computing power needed for training and executing the machine learning component typically grows linearly at lower accuracy numbers and then grows exponentially at higher accuracy numbers.The cost of running and maintaining the solution should be a lot lesser than the revenue attributed to the solution for the machine learning solution to be monetarily viable. This can be achieved in a couple of ways:Yet another practical consideration that should be high on our list is model interpretability.By being able to interpret why a given data science model behaved in a particular way helps prioritize changes in the model, changes in the training samples, and/or changes to the architecture to improve the overall performance.In several applications like the loan-eligibility prediction we discussed above, or precision medicine, or forensics, the data science models are, by regulation, required to be interpretable so that human experts can check for unethical biases.Interpretable models also go a long way in building stakeholder trust in the data-driven paradigm of solving business problems. But, the flip side is that often the most accurate models are also the ones that are most abstract and hence least interpretable.Thus, one fundamental issue that the data science delivery leader has to address is the compromise between accuracy and interpretability.Deep Learning-based models fall in the category of higher-abstraction-and-lower-interpretability models. There is a tremendous amount of active research in making deep learning models interpretable (e.g., LIME and Layer wise Relevance Propagation).In summary, a high accuracy data science component by itself may not mean much even if it solves a pressing business need. On one extreme, it could be that the data science solution achieves high accuracy at the cost of high compute power or high turnaround time, neither of which are acceptable by the business. On the other extreme, it could be that the component that the end-user interacts with has minimal sensitivity to the errors of the data science component and thus a relatively simpler model would have sufficed the business needs.A good understanding of how the data science component fits into the overall end-to-end solution will undoubtedly help make the right design and implementation decisions. This, in turn, increases customer acceptance of the solution within a reasonable operational budget.I hope you liked the article. Do post your comments and suggestions below. I will be back with the last article of this series soon.",https://www.analyticsvidhya.com/blog/2019/09/4-key-aspects-data-science-project/
A Data Scientists Guide to 8 Types of Sampling Techniques,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Sampling?|Why do we need Sampling?|Steps involved in Sampling|Different Types of Sampling Techniques|Types of Probability Sampling|Types of Non-Probability Sampling|End Notes,"Step 1|Step 2|Step 3|Step 4|Step 5|Simple Random Sampling|Systematic Sampling|Stratified Sampling|Cluster Sampling|Convenience Sampling|Quota Sampling|Judgment Sampling|Snowball Sampling|Share this:|Related Articles|4 Key Aspects of a Data Science Project Every Data Scientist and Leader Should Know|WNS Analytics Wizard 2019: Top 3 Winners Solutions from our Biggest Data Science Hackathon|
Ronak Gangwal
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Heres a scenario Im sure you are familiar with. You download a relatively big dataset and are excited to get started with analyzing it and building your machine learning model. And snap  your machine gives an out of memory error while trying to load the dataset.Its happened to the best of us. Its one of the biggest hurdles we face in data science  dealing with massive amounts of data on computationally limited machines (not all of us have Googles resource power!).So how can we overcome this perennial problem? Is there a way to pick a subset of the data and analyze that  and that can be a good representation of the entire dataset?Yes! And that method is called sampling. Im sure youve come across this term a lot during your school/university days, and perhaps even in your professional career. Sampling is a great way to pick up a subset of the data and analyze that. But then  should we just pick up any subset randomly?Well, well discuss that in this article. We will talk about eight different types of sampling techniques and where you can use each one. This is a beginner-friendly article but some knowledge about descriptive statistics will serve you well.If youre new to statistics and data science, I encourage you to check out our two popular courses:Note: You can also check out our comprehensive collection of articles on statistics for data science here.Lets start by formally defining what sampling is.Sampling is a method that allows us to get information about the population based on the statistics from a subset of the population (sample), without having to investigate every individual.The above diagram perfectly illustrates what sampling is. Lets understand this at a more intuitive level through an example.We want to find the average height of all adult males in Delhi. The population of Delhi is around 3 crore and males would be roughly around 1.5 crores (these are general assumptions for this example so dont take them at face value!). As you can imagine, it is nearly impossible to find the average height of all males in Delhi. Its also not possible to reach every male so we cant really analyze the entire population. So what can we do instead? We can take multiple samples and calculate the average height of individuals in the selected samples.But then we arrive at another question  how can we take a sample? Should we take a random sample? Or do we have to ask the experts?Lets say we go to a basketball court and take the average height of all the professional basketball players as our sample. This will not be considered a good sample because generally, a basketball player is taller than an average male and it will give us a bad estimate of the average males height.Heres a potential solution  find random people in random situations where our sample would not be skewed based on heights.Im sure you have a solid intuition at this point regarding the question.Sampling is done to draw conclusions about populations from samples, and it enables us to determine a populations characteristics by directly observing only a portion (or sample) of the population.I firmly believe visualizing a concept is a great way to ingrain it in your mind. So heres a step-by-step process of how sampling is typically done, in flowchart form!Lets take an interesting case study and apply these steps to perform sampling. We recently conducted General Elections in India a few months back. You must have seen the public opinion polls every news channel was running at the time:Were these results concluded by considering the views of all 900 million voters of the country or a fraction of these voters? Let us see how it was done.The first stage in the sampling process is to clearly define the target population.So, to carry out opinion polls, polling agencies consider only the people who are above 18 years of age and are eligible to vote in the population.Sampling Frame It is a list of items or people forming a population from which the sample is taken.So, the sampling frame would be the list of all the people whose names appear on the voter list of a constituency.Generally, probability sampling methods are used because every vote has equal value and any person can be included in the sample irrespective of his caste, community, or religion. Different samples are taken from different regions all over the country.Sample Size  It is the number of individuals or items to be taken in a sample that would be enough to make inferences about the population with the desired level of accuracy and precision.Larger the sample size, more accurate our inference about the population would be.For the polls, agencies try to get as many people as possible of diverse backgrounds to be included in the sample as it would help in predicting the number of seats a political party can win.Once the target population, sampling frame, sampling technique, and sample size have been established, the next step is to collect data from the sample.In opinion polls, agencies generally put questions to the people, like which political party are they going to vote for or has the previous party done any work, etc.Based on the answers, agencies try to interpret who the people of a constituency are going to vote for and approximately how many seats is a political party going to win. Pretty exciting work, right?!Here comes another diagrammatic illustration! This one talks about the different types of sampling techniques available to us:For example, lets say our population consists of 20 individuals. Each individual is numbered from 1 to 20 and is represented by a specific color (red, blue, green, or yellow). Each person would have odds of 1 out of 20 of being chosen in probability sampling.With non-probability sampling, these odds are not equal. A person might have a better chance of being chosen than others. So now that we have an idea of these two sampling types, lets dive into each and understand the different types of sampling under each section.This is a type of sampling technique you must have come across at some point. Here, every individual is chosen entirely by chance and each member of the population has an equal chance of being selected.Simple random sampling reduces selection bias.One big advantage of this technique is that it is the most direct method of probability sampling. But it comes with a caveat  it may not select enough individuals with our characteristics of interest. Monte Carlo methods use repeated random sampling for the estimation of unknown parameters.In this type of sampling, the first individual is selected randomly and others are selected using a fixed sampling interval. Lets take a simple example to understand this.Say our population size is x and we have to select a sample size of n. Then, the next individual that we will select would be x/nth intervals away from the first individual. We can select the rest in the same way.Suppose, we began with person number 3, and we want a sample size of 5. So, the next individual that we will select would be at an interval of (20/5) = 4 from the 3rd person, i.e. 7 (3+4), and so on.                                 3,3+4=7, 7+4=11, 11+4=15, 15+4=19  =3, 7, 11, 15, 19Systematic sampling is more convenient than simple random sampling. However, it might also lead to bias if there is an underlying pattern in which we are selecting items from the population (though the chances of that happening are quite rare).In this type of sampling, we divide the population into subgroups (called strata) based on different traits like gender, category, etc. And then we select the sample(s) from these subgroups:Here, we first divided our population into subgroups based on different colors of red, yellow, green and blue. Then, from each color, we selected an individual in the proportion of their numbers in the population.We use this type of sampling when we want representation from all the subgroups of the population. However, stratified sampling requires proper knowledge of the characteristics of the population.In a clustered sample, we use the subgroups of the population as the sampling unit rather than individuals. The population is divided into subgroups, known as clusters, and a whole cluster is randomly selected to be included in the study:In the above example, we have divided our population into 5 clusters. Each cluster consists of 4 individuals and we have taken the 4th cluster in our sample. We can include more clusters as per our sample size.This type of sampling is used when we focus on a specific region or area.This is perhaps the easiest method of sampling because individuals are selected based on their availability and willingness to take part.Here, lets say individuals numbered 4, 7, 12, 15 and 20 want to be part of our sample, and hence, we will include them in the sample.Convenience sampling is prone to significant bias, because the sample may not be the representation of the specific characteristics such as religion or, say the gender, of the population.In this type of sampling, we choose items based on predetermined characteristics of the population. Consider that we have to select individuals having a number in multiples of four for our sample:Therefore, the individuals numbered 4, 8, 12, 16, and 20 are already reserved for our sample.In quota sampling, the chosen sample might not be the best representation of the characteristics of the population that werent considered.It is also known as selective sampling. It depends on the judgment of the experts when choosing whom to ask to participate.Suppose, our experts believe that people numbered 1, 7, 10, 15, and 19 should be considered for our sample as they may help us to infer the population in a better way. As you can imagine, quota samplingis also prone to bias by the experts and may not necessarily be representative.I quite like this sampling technique. Existing people are asked to nominate further people known to them so that the sample increases in size like a rolling snowball. This method of sampling is effective when a sampling frame is difficult to identify. Here, we had randomly chosen person 1 for our sample, and then he/she recommended person 6, and person 6 recommended person 11, and so on.             1->6->11->14->19There is a significant risk of selection bias in snowball sampling, as the referenced individuals will share common traits with the person who recommends them.In this article, we learned about the concept of sampling, steps involved in sampling, and the different types of sampling methods. Sampling has wide applications in the statistical world as well as the real world.
Are there any other types of sampling techniques you feel the community should know? Let me know in the comments section below and well discuss!And as I mentioned earlier, if youre new to data science and statistics, you should really check out the below courses:",https://www.analyticsvidhya.com/blog/2019/09/data-scientists-guide-8-types-of-sampling-techniques/
WNS Analytics Wizard 2019: Top 3 Winners Solutions from our Biggest Data Science Hackathon,Learn everything about Analytics|Overview|Introduction|About the WNS Analytics Wizard 2019 Hackathon|Problem Statement for the WNS Analytics Wizard 2019 Hackathon|Dataset Description|Winners of the WNS Analytics Wizard 2019 Hackathon|End Notes,"Rank 3: Team AK (predictt.ai) (Aakash Kerawat & Akshay Karangale)|Approach|Rank 2: Arefev Sergei|Rank 1: Roman Pyankov|Approach|Share this:|Related Articles|A Data Scientists Guide to 8 Types of Sampling Techniques|Become a Video Analysis Expert: A Simple Approach to Automatically Generating Highlights using Python|
Ankit Choudhary
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","Aakashs approach:|Akshays approach|Validation Scheme|Feature Engineering|Final Model|For such problems, a participant must focus on:",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Hackathons have shaped my data science career in a huge way. They helped me understand the importance of structured thinking and how to use it when working with tight deadlines. This idea is actually the essence that drives the role of a successful data scientist.I get a lot of questions from aspiring data science professionals wondering how to stand out from the competition and land a role in this field. This is a multi-layered question but one of the common elements I always point out  start participating in hackathons and gauge where you stand.And if you can climb up the leaderboard, even better!In this article, I am excited to share the top three winning approaches (and code!) from the WNS Analytics Wizard 2019 hackathon. This was Analytics Vidhyas biggest hackathon yet and there is a LOT to learn from these winners solutions.So bring out a pen and paper, take notes and dont miss out on any other hackathons! Head straight to the DataHack platform and enroll in the upcoming competitions today.The WNS Analytics Wizard 2019 was the biggest hackathon ever hosted by Analytics Vidhya. Heres a summary of the numbers behind this historic hackathon:It was a memorable 9-day hackathon with a wide range of data scientists participating from all over the globe.Lets check out the problem statement for this hackathon.Zbay is an e-commerce website that sells a variety of products on its online platform. Zbay records the user behavior of its customers and stores it in log form. However, most of the time users do not buy the products instantly and there is a time gap during which the customer might surf the internet and perhaps visit competitor websites.Now, to improve the sales of products, Zbay has hired Adiza, an Adtech company that built a system where advertisements are shown for Zbays products on its partner websites.If a user comes to Zbays website and searches for a product, and then visits these partner websites or apps, his/her previously viewed items or their similar items are shown as an advertisement (ad). If the user clicks this ad, he/she will be redirected to Zbays website and might buy the product.In this problem, the task is to predict click probability, i.e., the probability of a user clicking the ad which is shown to them on the partner websites for the next 7 days on the basis of historical view log data, ad impression data and user data.The participants were provided with:The training data contains the impression logs during 2018/11/15  2018/12/13 along with the label which specifies whether the ad is clicked or not. The final model was evaluated on the test data which has impression logs during 2018/12/12  2018/12/18 without the labels.The participants were provided with the below files:train.csv:view_log.csv:item_data.csv:Winning a hackathon is a remarkably challenging task. There are a lot of obstacles to overcome, not to mention the sheer amount of competition from the top data scientists in the world.I loved going through these top solutions and approaches provided by our winners. First, lets look at who won and congratulate them:You can check out the final rankings of all the participants on the Leaderboard.The top three winners have shared their detailed approach from the competition. I am sure you are eager to know their secrets so lets begin.Heres what Team AK shared with us.Our final solution was an ensemble of LightGBM, Neural Networks and CatBoost models.Taking a cursory look at the datasets, it seemed that the features generated from view_logs would play a significant role in improving the score. But soon we discovered that most of the features generated from view_logs were overfitting the training set.This was because a higher percentage of training data had recent view_logs than in the test set. So, we focused more on feature engineering on the training set.The features that worked for us were percentage_clicks_tilldate per user/app_code/user_app_code. These features alone helped us reach 0.73xx on the public leaderboard. Along with these, we used some time-based features and a couple of features from view_logs.We both had slightly different approaches giving similar public leaderboard scores of around 0.75xx.Apart from these common features, we used a few different techniques/features in our individual approaches giving scores of around 0.75xx. Our final submission was ranking the average of our best individual models.My validation strategy was a simple time-based split:You can check out the full code for this approach here.Heres what Arefev shared with us.Overall ApproachLets look at Arafevs step-by-step approach now.Data-preprocessing and Feature EngineeringMy final modelKey TakeawaysThings a participant must focus on while solving such problemsHeres the full code for Arefevs approach.Heres what Roman shared with us.My solution consists mainly of feature engineering.I generated features based on user id and app code characteristics. This is described in more detail below. I trained a LightGBM model on 10 different subsamples. As a final prediction, I averaged these models by rank.For validation, I used StratifiedKFold (from sklearn.model selection) with the below parameters:For each user_id and impression_time, I calculated:I used LightGBM as my final model with the below parameters:I used a StratifiedKFold with 10 folds, so I got 10 models that were used to make predictions on the test data. Rank averaging among these 10 predictions was used as the final prediction. On local validation, I got the following average ROC-AUC value: 0.7383677000.And here is the full code for Romans winning solution!Phew  take a deep breath. Those were some mind-blowing frameworks that won this hackathon. As I mentioned earlier, it is quite a task winning a hackathon and our three winners really stood out with their thought process.I encourage you to head over to the DataHack platform TODAY and participate in the ongoing and upcoming hackathons. It will be an invaluable learning experience (not to mention a good addition to your budding resume!).",https://www.analyticsvidhya.com/blog/2019/09/wns-marketing-analytics-hackathon-top-3-inspiring-winning-solutions/
Become a Video Analysis Expert: A Simple Approach to Automatically Generating Highlights using Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|A Brief Introduction to Sports Video Highlights|Different Approaches to Highlight Generation|My Approach to Automatic Highlight Generation|Understanding the Problem Statement|Automatic Highlight Generation in Python|End Notes,"Natural Language Processing (NLP) based approach|Computer Vision-based approach|What is Short Time Energy?|Share this:|Related Articles|WNS Analytics Wizard 2019: Top 3 Winners Solutions from our Biggest Data Science Hackathon|Everything you Should Know about p-value from Scratch for Data Science|
Aravind Pai
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a huge cricket fan. Ive been hooked to the game since I can remember and it still fuels my day-to-day routine. Im sure a lot of you reading this will be nodding your head!But ever since I started working full-time, keeping up with all the matches has been a tough nut to crack. I can only catch fleeting finishes to matches rather than the whole package. Or I have to follow along with text commentary  either way, it leaves me wanting more.So the data scientist in me decided to do something about this. Was there a way I could put my Python skills to use and cut out all the important parts of a match? I essentially wanted to create my own highlights package using Python.Turns out, I didnt even need to rely on machine learning or deep learning techniques to do it! And now, I want to share my learning (and code) with our community. Check out the below highlight package. This will give you a taste of what we will be building in this article using a simple speech analysis approach:Awesome! l will discuss how I made this automatic highlight generation process so you can learn and apply it to any match (or any sport) that you want. Lets get rolling!If youre new to Python or need a quick refresher, make sure you check out our FREE course here.Weve all seen highlights of sports matches at some point. Even if you dont have an inclination towards sports, youll have come across highlights on the television while sitting in a restaurant, lounging in a hotel, etc.Highlight Generation is the process of extracting the most interesting clips from a sports video. You can think of this as a classic use case of video summarization. In video summarization, the full-length video is converted into a shorter format such that the most important content is preserved.In cricket, the full match video contains actions like fours, sixes, wickets, and so on. The unedited version even captures uninteresting events too like defenses, leaves, wide balls, byes, etc.Highlights, on the other hand, is where the adrenaline-rush kicks in. All the major talking points, like fours, sixes, and wickets  these combine to make the quintessential highlights package.Extracting highlights manually from a full match video requires a lot of human effort. It is a time-consuming task and unless you work for a video company that does this job day in and day out, you need to find a different approach.It is also memory-hogging to store a full match video. So, extracting highlights automatically from a full match video saves a lot of time for the creator as well as the user. And that is what we will discuss in this article.There are different ways we can generate highlights, apart from the manual approach. Two common approaches we can use are  Natural Language Processing (NLP) and Computer Vision. Lets briefly discuss how they work before we jump to my approach.Think about this for a moment before you look at the steps below. How can you use NLP or a text-based approach to extract the important bits from a cricket match?Here is a step-by-step procedure:This computer vision-based approach will come across as quite intuitive. Computer vision is, after all, the field where we train our machines to look at images and videos. So one way to generate highlights using computer vision is to track the scorecard continuously and extract clips only when there is a four, six or a wicket.Can you think of any other approaches using either of these techniques? Let me know in the comments section below  Im very interested in hearing your ideas.AT this point, you might be wondering  we just spoke about two sub-fields of machine learning and deep learning. But the article heading and introduction suggest that we wont be using these two fields. So can we really generate highlights without building models? Yes!Not every problem requires Deep Learning and Machine Learning. Most of the problems can be solved with a thorough understanding of the domain and the data.  Sunil RayI will discuss the concept of automatic highlight generation using Simple Speech Analysis. Lets discuss a few terminologies before moving on to the ultimate approach.An audio signal can be analyzed in the time or frequency domain. In the time domain, an audio signal is analyzed with respect to the time component, whereas in the frequency domain, it is analyzed with respect to the frequency component:The energy or power of an audio signal refers to the loudness of the sound. It is computed by the sum of the square of the amplitude of an audio signal in the time domain. When energy is computed for a chunk of an entire audio signal, then it is known as Short Time Energy.Source: facto-facts.comThe basic idea behind the solution is that in most sports, whenever an interesting event occurs, there is an increase in the commentators voice as well as the spectators. Lets take cricket for example. Whenever a batsman hits a boundary or a bowler takes a wicket, there is a rise in the commentators voice. The ground swells with the sound of the spectators cheering. We can use these changes in audio to capture interesting moments from a video.Here is the step-by-step process:Cricket is the most famous sport in India and played in almost all parts of the country. So, being a die-hard cricket fan, I decided to automate the process of highlights extraction from a full match cricket video. Nevertheless, the same idea can be applied to other sports as well.For this article, I have considered only the first 6 overs (PowerPlay) of the semi-final match between India and Australia at the T20 World Cup in 2007. You can watch the full match on YouTube here and download the video for the first six overs from here.I have extracted the audio from the video with the help of a software called WavePad Audio Editor. You can download the audio clip from here.We can get the duration of the audio clip in minutes using the code below:
Now, we will break the audio into chunks of 5 seconds each since we are interested in finding out whether a particular audio chunk contains a rise in the audio voice:Let us listen to one of the audio chunks:Compute the energy for the chunk:Visualize the chunk in the time-series domain:As we can see, the amplitude of a signal is varying with respect to time. Next, compute the Short Time Energy for every chunk:Let us understand the Short Time Energy distribution of the chunks:The energy distribution is right-skewed as we can see in the above plot. We will choose the extreme value as the threshold since we are interested in the clips only when the commentators speech and spectators cheers are high.Here, I am considering the threshold to be 12,000 as it lies on the tail of the distribution. Feel free to experiment with different values and see what result you get.Merge consecutive time intervals of audio clips into one:Extract the video within a particular time interval to form highlights.Remember Since the commentators speech and spectators cheers increase only after the batsman has played a shot, I am considering only five seconds post every excitement clip:I have used online editors to merge all the extracted clips to form a single video. Here are the highlights generated from the PowerPlay using a simple speech analysis approach:Congratulations on making it this far and generating your own highlight package! Go ahead and apply this technique to any match or sport you want. It might appear straightforward but its such a powerful approach. Follow the code on Github.The key takeaways from the article  have a thorough understanding of the domain as well as the data before getting into the model building process since it drives us to a better solution in most of the problems.In this article, we have seen how to automate the process of highlight extraction from a full match sports video using simple speech analysis. I would recommend you to experiment in different sports too.Liked the article? Want to share a different approach? Feel free to connect with me in the comments section below! And if youre looking to learn Python, heres a FREE course for you:",https://www.analyticsvidhya.com/blog/2019/09/guide-automatic-highlight-generation-python-without-machine-learning/
Everything you Should Know about p-value from Scratch for Data Science,Learn everything about Analytics|Overview|Introduction|How we will Understand p-value from Scratch|What is p-value?|Statistical Significance of the p-value: Enter  Alpha value|Example of p-value in Statistics|Example of p-value in Data Science|Some traditional (mis)interpretations of the p-value|End Notes,"p-value < alpha|p-value > alpha:|A solution to this Problem|Share this:|Related Articles|Become a Video Analysis Expert: A Simple Approach to Automatically Generating Highlights using Python|Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor|
Sharoon Saxena
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 1: Understand the given information|Step 2: Calculating the Z-Score|Step 3: Referring to the Z-table and finding the p-value:|Step 4: Comparing p-value and alpha value:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Does the below scenario look familiar when you talk about p-value to aspiring data scientists?I cannot tell you the number of times data scientists, even established ones, flounder when it comes to explaining how to interpret a p-value. In fact, take a moment to answer these questions:These are crucial questions that every data science professional should be able to answer. And in my experience, most struggle to get past the first question. We cannot expect to convince our clients about the result of a machine learning model if we cant break it down for them, right?The Wikipedia definition of p-value is daunting to anyone who is new to the world of statistics and data science. This is how a typical conversation about p-value goes:And you are left hanging with formulae and conventions about what to do but no clue on how to interpret the p-value. So how do we learn p-value once and for all and indelibly ingrain it in our mind?In this article, we will start building the intuition for the p-value step-by-step from scratch and will also debunk the traditional (mis)interpretations of the p-value. This is what we will cover:So, lets dive right into it.Lets start with the absolute basics. What is p-value? To understand this question, we will pick up the normal distribution:We have the range of values on the x-axis and the frequency of occurrences of different values on the y-axis. If you need a quick refresher on the concept of normal distributions, check out this article.Now, lets say we pick any random value from this distribution. The probability that we will pick values close to the mean is highest as it has the highest peak (due to high occurrence values in that region). We can clearly see that if we move away from the peak, the occurrence of the values decreases rapidly and so does the corresponding probability, towards a very small value close to zero.But this article is about p-values  so why are we looking at a normal distribution? Well, with respect to the normal distribution we discussed above, consider the way we define the p-value.p-value is the cumulative probability (area under the curve) of the values to the right of the red point in the figure above.Or,p-value corresponding to the red point tells us about the total probability of getting any value to the right hand side of the red point, when the values are picked randomly from the population distribution.Now, this might look like a very naive definition, but we will build on it as we go along.P-value does not hold any value by itself. A large p-value implies that sample scores are more aligned or similar to the population score. It is as simple as that.Now, you might have come across the thumb rule of comparing the p-value with the alpha value to draw conclusions. So lets look into the alpha value.Ive mentioned the alpha value, also known as the significance level, a few times so far. This is a value that we know to be 0.05 or 5% for some unknown reason.We are also taught in statistics classes the convention that p-value being less than alpha means that the results obtained are statistically significant. But what in the world is the alpha value?So, lets spend a moment to look at what the alpha value signifies.Alpha value is nothing but a threshold p-value, which the group conducting the test/experiment decides upon before conducting a test of similarity or significance ( Z-test or a T-test).This means that if the likeliness of getting the sample score is less than alpha or the threshold p-value, we consider it significantly different from the population, or even belonging to some new sample distribution.Consider the above normal distribution again. The red point in this distribution represents the alpha value or the threshold p-value. Now, lets say that the green and orange points represent different sample results obtained after an experiment.We can see in the plot that the leftmost green point has a p-value greater than the alpha. As a result, these values can be obtained with fairly high probability and the sample results are regarded as lucky.The point on the rightmost side (orange) has a p-value less than the alpha value (red). As a result, the sample results are a rare outcome and very unlikely to be lucky. Therefore, they are significantly different from the population.The alpha value is decided depending on the test being performed. An alpha value of 0.05 is considered a good convention if we are not sure of what value to consider.But this comes with an asterisk  the smaller the value of alpha we consider, the harder it is to consider the results as significant. Keep in mind that the alpha value will vary from experiment to experiment and there is no alpha value which can be considered as a thumb rule.Lets look at the relationship between the alpha value and the p-value closely.Consider the following population distribution:Here, the red point represents the alpha value. This is basically the threshold p-value. We can clearly see that the area under the curve to the right of the threshold is very low.The orange point represents the p-value using the sample population. In this case, we can clearly see that the p-value is less than the alpha value (the area to the right of the red point is larger than the area to the right of the orange point). This can be interpreted as:The results obtained from the sample is an extremity of the population distribution (an extremely rare event), and hence there is a good chance it may belong to some other distribution (as shown below).Considering our definitions of alpha and the p-value, we consider the sample results obtained as significantly different. We can clearly see that the p-value is far less than the alpha value.Right  I feel you should answer this question before reading further. Now that you know the other side of this coin, you will be able to think of the outcome of this scenario.p-value greater than the alpha means that the results are in favor of the null hypothesis and therefore we fail to reject it. This result is often against the alternate hypothesis (obtained results are from another distribution) and the results obtained are not significant and simply a matter of chance or luck.Again, consider the same population distribution curve with the red point as alpha and the orange point as the calculated p-value from the sample:So, p-value > alpha (considering the area under the curve to the right-hand side of the red and the orange points) can be interpreted as follows:The sample results are just a low probable event of the population distribution and are very likely to be obtained by luck.We can clearly see that the area under the population curve to the right of the orange point is much larger than the alpha value. This means that the obtained results are more likely to be part of the same population distribution than being a part of some other distribution.Now that we have understood the interpretation of the p-value and the alpha value, lets look at a classic example from the world of statistics.In the National Academy of Archery, the head coach intends to improve the performance of the archers ahead of an upcoming competition. What do you think is a good way to improve the performance of the archers?He proposed and implemented the idea that breathing exercises and meditation before the competition could help. The statistics before and after experiments are below:Interesting. The results favor the assumption that the overall score of the archers improved. But the coach wants to make sure that these results are because of the improved ability of the archers and not by luck or chance. So what do you think we should do?This is a classic example of a similarity test (Z-test in this case) where we want to check whether the sample is similar to the population or not. I will not go deep into the similarity test since that is out of the scope of this article.In order to solve this, we will follow a step-by-step approach:We have the population mean and standard deviation with us and the sample size is over 30, which means we will be using the Z-test.According to the problem above, there can be two possible conditions:We will now calculate the Z-Score using the above formula. What do the symbols stand for, you ask? Well, here you go:On plugging in the corresponding values, Z-Score comes out to be  3.87.If we look up the Z-table for 3.87, we get a value of ~0.999. This is the area under the curve or probability under the population distribution. But this is the probability of what?The probability that we obtained is to the left of the Z-score (Red Point) which we calculated. The value 0.999 represents the total probability of getting a result less than the sample score 78, with respect to the population.Here, the red point signifies where the sample mean lies with respect to the population distribution. But we have studied earlier that p value is to the right-hand side of the red point, so what do we do?For this, we will use the fact that the total area under the normal Z distribution is 1. Therefore the area to the right of Z-score (or p-value represented by the unshaded region) can be calculated as:p-value = 1  0.999p-value = 0.0010.001 (p-value) is the unshaded area to the right of the red point. The value 0.001 represents the total probability of getting a result greater than the sample score 78, with respect to the population.We were not given any value for alpha, therefore we can consider alpha = 0.05. According to our understanding, if the likeliness of obtaining the sample (p-value) result is less than the alpha value, we consider the sample results obtained as significantly different.We can clearly see that the p-value is far less than the alpha value:0.001 (red region) << 0.5 (orange region)This says that the likeliness of obtaining the mean as 78 is a rare event with respect to the population distribution. Therefore, it is convenient to say that the increase in the performance of the archers in the sample population is not the result of luck. The sample population belongs to some other (better in this case) distribution of itself.Now, this is the section Im sure youve been waiting for. Using p-value in statistics is understandable and weve even heard of it plenty of times. But where does p-value fit in the data science spectrum?Even though many aspiring data scientists understand what the p-value means, they do not know how to use this knowledge in Data Science. As a result, they miss out on a significantly powerful method of improving their models.P-value is an important metric in the process of feature selection. In feature selection, we try to find out the best subset of the independent variables to build the model.Now you might ask, Why not just throw in all the independent variables?Actually, throwing in redundant and non-contributing variables adds complexity to the model. Moreover, they can reduce the model performance in terms of accuracy, runtime and even memory footprint.Note: If you need a refresher on feature selection, refer to the below tutorial:Lets look at an example. Consider that I have a dataset that contains information about different startups. We have the below variables:Our aim is to predict the profits earned by the startups based on the rest of the independent variables. Now, your intuition might say  use all the independent variables available to build a linear regression model.After preprocessing and OneHotEncoding, the dependent variables have the following mapping:Next, we will build an OLS (ordinary least squares) model using the statsmodelslibrary. Heres what we get:This table displays all the statistics regarding the independent variables. But right now, we are only interested in looking at the column with the p-values in it. We can clearly see that the R&S Spend , Administration and State_California have a p-value over 0.50!But the question is, what does this p-value mean in a regression model? For that, lets understand whats the hypothesis for which these p-values are calculated:Now, the above results show that R&S Spend, Administration and State_California have no significant effect over the Profit earned by the startups. So lets start by removing these three variables from the model.The resultant mapping after removing those two variables is:On again building the OLS model using the statsmodels library, this is what we get:We can see that there is now only one variable left over the value of 0.05  State_Florida. So should we remove it?For starters, we never decided any alpha value. If we were to take the alpha value 0.05, the variable State_Florida would have been eliminated. If I would have selected the alpha as 0.10, the variable would have survived the filtration process.In this case, I will let it stay considering that 0.05 is not a thumb rule to choose for the alpha value.The most important thing to note in this model summary is that although we have reduced two independent variables, the value of the adjusted R-Square value went up.This is a two-fold effect as we discussed previously. With the help of p-value, we not only made a simpler model with fewer variables, but we also improved the models performance.Before wrapping up this article, lets look at different ways p-values are misinterpreted by a lot of data science professionals and statisticians.There are many ways I have seen people misinterpreting the p-value. Here are just a few of the most common mistakes:And there are many more! Keep these in mind and youll do well the next time you encounter p-value in your work.In this article, we followed a step by step procedure to understand p-value thoroughly by introducing one parameter at a time. P-value can be very intriguing to a new statistician or a data scientist, but the way we understood it above with example in statistics and an example in data science, I believe we can now explain p-value confidently to anyone without having to depend upon the complex definitions or conventions set in stone just because no one ever explained it to us.If you want to learn more, check out the following courses:",https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/
Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is a Feature Descriptor?|Introduction to the HOG Feature Descriptor|Process of Calculating the Histogram of Oriented Gradients (HOG)|Different Methods to Create Histograms using Gradients and Orientation|Implementing HOG Feature Descriptor in Python||End Notes,"Step 1: Preprocess the Data (64 x 128)|Step 2: Calculating Gradients (direction x and y)|Step 3: Calculate the Magnitude and Orientation|Method 1:|Method 2:|Method 3: |Method 4:|Step 4: Calculate Histogram of Gradients in 88 cells (91)|Step 5: Normalize gradients in 1616 cell (361)|Step 6: Features for the complete image|Share this:|Related Articles|Everything you Should Know about p-value from Scratch for Data Science|Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model|
Aishwarya Singh
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Feature engineering is a game-changer in the world of machine learning algorithms. Its actually one of my favorite aspects of being a data scientist! This is where we get to experiment the most  to engineer new features from existing ones and improve our models performance.Some of the top data scientists in the world rely on feature engineering to boost their leaderboard score in hackathons. Im sure you would even have used various feature engineering techniques on structured data.
Can we extend this technique to unstructured data, such as images? Its an intriguing riddle for computer vision enthusiasts and one we will solve in this article. Get ready to perform feature engineering in the form of feature extraction on image data!There are actually multiple techniques for feature extraction. I covered three basic ones in my previous article which you should read before proceeding:3 Beginner-Friendly Techniques to Extract Features from Image Data using PythonIn this article, I will introduce you to a popular feature extraction technique for images  Histogram of Oriented Gradients, or HOG as its commonly known. We will understand what is the HOG feature descriptor, how it works (the complete math behind the algorithm), and finally, implement it in Python.You might have had this question since you read the heading. So lets clear that up first before we jump into the HOG part of the article.Take a look at the two images shown below. Can you differentiate between the objects in the image?We can clearly see that the right image here has a dog and the left image has a car. Now, let me make this task slightly more complicated  identify the objects shown in the image below:Still easy, right? Can you guess what was the difference between the first and the second case? The first pair of images had a lot of information, like the shape of the object, its color, the edges, background, etc.On the other hand, the second pair had much less information (only the shape and the edges) but it was still enough to differentiate the two images.Do you see where I am going with this? We were easily able to differentiate the objects in the second case because it had the necessary information we would need to identify the object. And that is exactly what a feature descriptor does:It is a simplified representation of the image that contains only the most important information about the image.There are a number of feature descriptors out there. Here are a few of the most popular ones:In this article, we are going to focus on the HOG feature descriptor and how it works. Lets get started!HOG, or Histogram of Oriented Gradients, is a feature descriptor that is often used to extract features from image data. It is widely used in computer vision tasks for object detection.Lets look at some important aspects of HOG that makes it different from other feature descriptors:To put a formal definition to this:The HOG feature descriptor counts the occurrences of gradient orientation in localized portions of an image.Implementing HOG using tools like OpenCV is extremely simple. Its just a few lines of code since we have a predefined function called hog in the skimage.feature library. Our focus in this article, however, is on how these features are actually calculated.We should now have a basic idea of what a HOG feature descriptor is. Its time to delve into the core idea behind this article. Lets discuss the step-by-step process to calculate HOG.Consider the below image of size (180 x 280). Let us take a detailed look at how the HOG features will be created for this image:This is a step most of you will be pretty familiar with. Preprocessing data is a crucial step in any machine learning project and thats no different when working with images.We need to preprocess the image and bring down the width to height ratio to 1:2. The image size should preferably be 64 x 128. This is because we will be dividing the image into 8*8 and 16*16 patches to extract the features. Having the specified size (64 x 128) will make all our calculations pretty simple. In fact, this is the exact value used in the original paper.Coming back to the example we have, let us take the size 64 x 128 to be the standard image size for now. Here is the resized image:The next step is to calculate the gradient for every pixel in the image. Gradients are the small change in the x and y directions. Here, I am going to take a small patch from the image and calculate the gradients on that:We will get the pixel values for this patch. Lets say we generate the below pixel matrix for the given patch (the matrix shown here is merely used as an example and these are not the original pixel values for the given patch):Source: Applied Machine Learning CourseI have highlighted the pixel value 85. Now, to determine the gradient (or change) in the x-direction, we need to subtract the value on the left from the pixel value on the right. Similarly, to calculate the gradient in the y-direction, we will subtract the pixel value below from the pixel value above the selected pixel.Hence the resultant gradients in the x and y direction for this pixel are:This process will give us two new matrices  one storing gradients in the x-direction and the other storing gradients in the y direction. This is similar to using a Sobel Kernel of size 1. The magnitude would be higher when there is a sharp change in intensity, such as around the edges.We have calculated the gradients in both x and y direction separately. The same process is repeated for all the pixels in the image. The next step would be to find the magnitude and orientation using these values.Using the gradients we calculated in the last step, we will now determine the magnitude and direction for each pixel value. For this step, we will be using the Pythagoras theorem (yes, the same one which you studied back in school!).Take a look at the image below:The gradients are basically the base and perpendicular here. So, for the previous example, we had Gx and Gy as 11 and 8. Lets apply the Pythagoras theorem to calculate the total gradient magnitude:Total Gradient Magnitude = [(Gx)2+(Gy)2]
Total Gradient Magnitude = [(11)2+(8)2] = 13.6Next, calculate the orientation (or direction) for the same pixel. We know that we can write the tan for the angles:tan() = Gy / GxHence, the value of the angle would be: = atan(Gy / Gx)The orientation comes out to be 36 when we plug in the values. So now, for every pixel value, we have the total gradient (magnitude) and the orientation (direction). We need to generate the histogram using these gradients and orientations.But hang on  we need to take a small break before we jump into how histograms are created in the HOG feature descriptor. Consider this a small step in the overall process. And well start this by discussing some simple methods of creating Histograms using the two values that we have  gradients and orientation.A histogram is a plot that shows the frequency distribution of a set of continuous data. We have the variable (in the form of bins) on the x-axis and the frequency on the y-axis. Here, we are going to take the angle or orientation on the x-axis and the frequency on the y-axis.Let us start with the simplest way to generate histograms. We will take each pixel value, find the orientation of the pixel and update the frequency table.Here is the process for the highlighted pixel (85). Since the orientation for this pixel is 36, we will add a number against angle value 36, denoting the frequency:Source: Applied Machine Learning CourseThe same process is repeated for all the pixel values, and we end up with a frequency table that denotes angles and the occurrence of these angles in the image. This frequency table can be used to generate a histogram with angle values on the x-axis and the frequency on the y-axis.Thats one way to create a histogram. Note that here the bin value of the histogram is 1. Hence we get about 180 different buckets, each representing an orientation value. Another method is to create the histogram features for higher bin values.This method is similar to the previous method, except that here we have a bin size of 20. So, the number of buckets we would get here is 9.Again, for each pixel, we will check the orientation, and store the frequency of the orientation values in the form of a 9 x 1 matrix. Plotting this would give us the histogram:Source: Applied Machine Learning CourseThe above two methods use only the orientation values to generate histograms and do not take the gradient value into account. Here is another way in which we can generate the histogram  instead of using the frequency, we can use the gradient magnitude to fill the values in the matrix. Below is an example of this:Source: Applied Machine Learning CourseYou might have noticed that we are using the orientation value of 30, and updating the bin 20 only. Additionally, we should give some weight to the other bin as well.Lets make a small modification to the above method. Here, we will add the contribution of a pixels gradient to the bins on either side of the pixel gradient. Remember, the higher contribution should be to the bin value which is closer to the orientation.Source: Applied Machine Learning CourseThis is exactly how histograms are created in the HOG feature descriptor.The histograms created in the HOG feature descriptor are not generated for the whole image. Instead, the image is divided into 88 cells, and the histogram of oriented gradients is computed for each cell. Why do you think this happens?By doing so, we get the features (or histogram) for the smaller patches which in turn represent the whole image. We can certainly change this value here from 8 x 8 to 16 x 16 or 32 x 32.If we divide the image into 88 cells and generate the histograms, we will get a 9 x 1 matrix for each cell. This matrix is generated using method 4 that we discussed in the previous section.Once we have generated the HOG for the 88 patches in the image, the next step is to normalize the histogram.Before we understand how this is done, its important to understand why this is done in the first place.Although we already have the HOG features created for the 88 cells of the image, the gradients of the image are sensitive to the overall lighting. This means that for a particular picture, some portion of the image would be very bright as compared to the other portions.We cannot completely eliminate this from the image. But we can reduce this lighting variation by normalizing the gradients by taking 1616 blocks. Here is an example that can explain how 1616 blocks are created:Here, we will be combining four 88 cells to create a 1616 block. And we already know that each 88 cell has a 91 matrix for a histogram. So, we would have four 91 matrices or a single 361 matrix. To normalize this matrix, we will divide each of these values by the square root of the sum of squares of the values. Mathematically, for a given vector V:V = [a1, a2, a3, .a36]
We calculate the root of the sum of squares:k = (a1)2+ (a2)2+ (a3)2+ . (a36)2And divide all the values in the vector V with this value k:The resultant would be a normalized vector of size 361.We are now at the final step of generating HOG features for the image. So far, we have created features for 1616 blocks of the image. Now, we will combine all these to get the features for the final image.Can you guess what would be the total number of features that we will have for the given image? We would first need to find out how many such 1616 blocks would we get for a single 64128 image:We would have 105 (715) blocks of 1616. Each of these 105 blocks has a vector of 361 as features. Hence, the total features for the image would be 105 x 361 = 3780 features.We will now generate HOG features for a single image and verify if we get the same number of features at the end.Time to fire up Python! This, Im sure, is the most anticipated section of this article. So lets get rolling.We will see how we can generate HOG features on a single image, and if the same can be applied on a larger dataset. We will first load the required libraries and the image for which we are going to create the HOG features:We can see that the shape of the image is 663 x 459. We will have to resize this image into 64 x 128. Note that we are using skimage which takes the input as height x width.Here, I am going to use the hog function from skimage.features directly. So we dont have to calculate the gradients, magnitude (total gradient) and orientation individually. The hog function would internally calculate it and return the feature matrix.Also, if you set the parameter visualize = True, it will return an image of the HOG.Before going ahead, let me give you a basic idea of what each of these hyperparameters represents. Alternatively, you can check the definitions from the official documentation here.The feature matrix from the function is stored in the variable fd, and the image is stored in hog_image. Let us check the shape of the feature matrix:As expected, we have 3,780 features for the image and this verifies the calculations we did in step 7 earlier. You can choose to change the values of the hyperparameters and that will give you a feature matrix of different sizes.Lets finally look at the HOG image:The idea behind this article was to give you an understanding of what is actually happening behind the HOG feature descriptor and how the features are calculated. The complete process is broken down into 7 simple steps.As a next step, I would encourage you to try using HOG features on a simple computer vision problem and see if the model performance improves. Do share your results in the comment section!",https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/
Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model,Learn everything about Analytics|Overview|Introduction|What well cover in this Video Classification Tutorial|Overview of Video Classification|Steps to build Video Classification model|Exploring the Video Classification dataset|Training the Video Classification Model|Evaluating our Video Classification Model|End Notes,"Reading all the video frames|Creating a validation set|Defining the architecture of the video classification model|Training the video classification model|Defining model architecture and loading weights|Creating the test data|Generating predictions for test videos|Evaluating the model|Share this:|Related Articles|Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor|Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!|
Pulkit Sharma
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have written extensive articles and guides on how to build computer vision models using image data. Detecting objects in images, classifying those objects, generating labels from movie posters  there is so much we can do using computer vision and deep learning.This time, I decided to turn my attention to the less-heralded aspect of computer vision  videos! We are consuming video content at an unprecedented pace. I feel this area of computer vision holds a lot of potential for data scientists.I was curious about applying the same computer vision algorithms to video data. The approach I used for building image classification models  was it generalizable?Videos can be tricky for machines to handle. Their dynamic nature, as opposed to an images static one, can make it complex for a data scientist to build those models.But dont worry, its not that different from working with image data. In this article, we will build our very own video classification model in Python. This is a very hands-on tutorial so fire up your Jupyter notebooks  this is going to a very fun ride.If youre new to the world of deep learning and computer vision, we have the perfect course for you to begin your journey:When you really break it down  how would you define videos?We can say that videos are a collection of a set of images arranged in a specific order. These sets of images are also referred to as frames.Thats why a video classification problem is not that different from an image classification problem. For an image classification task, we take images, use feature extractors (like convolutional neural networks or CNNs) to extract features from images, and then classify that image based on these extracted features. Video classification involves just one extra step.We first extract frames from the given video. We can then follow the same steps as we do for an image classification task. This is the simplest way to deal with video data.There are actually multiple other ways to deal with videos and there is even a niche field of video analytics. I highly recommend going through the below article to understand how to deal with videos and extract frames in Python:Also, we will be using CNNs to extract features from the frames of videos. If you need a quick refresher on what CNNs are and how they work, this is where you should begin:Excited to build a model that is able to classify videos into their respective categories? We will be working on the UCF101  Action Recognition Data Set which consists of 13,320 different video clips belonging to 101 distinct categories.Let me summarize the steps that we will be following to build our video classification model:Lets now start exploring the data!You can download the dataset from the official UCF101 site. The dataset is in a .rar format so we first have to extract the videos from it. Create a new folder, lets say Videos (you can pick any other name as well), and then use the following command to extract all the downloaded videos:The official documentation of UCF101 states that:It is very important to keep the videos belonging to the same group separate in training and testing. Since the videos in a group are obtained from a single long video, sharing videos from the same group in training and testing sets would give high performance.So, we will split the dataset into the train and test set as suggested in the official documentation. You can download the train/test split from here. Keep in mind that since we are dealing with a large dataset, you might require high computation power.We now have the videos in one folder and the train/test splitting file in another folder. Next, we will create the dataset. Open your Jupyter notebook and follow the below code block. We will first import the required libraries:We will now store the name of videos in a dataframe:
This is how the names of videos are given in the .txt file. It is not properly aligned and we will need to preprocess it. Before that, lets create a similar dataframe for test videos as well:Next, we will add the tag of each video (for both training and test sets). Did you notice that the entire part before the / in the video name represents the tag of the video? Hence, we will split the entire string on / and select the tag for all the videos:So whats next? Now, we will extract the frames from the training videos which will be used to train the model. I will be storing all the frames in a folder named train_1.So, first of all, make a new folder and rename it to train_1 and then follow the code given below to extract frames:This will take some time as there are more than 9,500 videos in the training set. Once the frames are extracted, we will save the name of these frames with their corresponding tag in a .csv file. Creating this file will help us to read the frames which we will see in the next section.So far, we have extracted frames from all the training videos and saved them in a .csv file along with their corresponding tags. Its now time to train our model which we will use to predict the tags for videos in the test set.Its finally time to train our video classification model! Im sure this is the most anticipated section of the tutorial. I have divided this step into sub-steps for ease of understanding:So, lets get started with the first step where we will extract the frames. We will import the libraries first:Remember, we created a .csv file that contains the names of each frame and their corresponding tag? Lets read it as well:This is how the first five rows look like. We have the corresponding class or tag for each frame. Now, using this .csv file, we will read the frames that we extracted earlier and then store those frames as a NumPy array:Output: (73844, 224, 224, 3)We have 73,844 images each of size (224, 224, 3). Next, we will create the validation set.To create the validation set, we need to make sure that the distribution of each class is similar in both training and validation sets. We can use the stratify parameter to do that:Here, stratify = y (which is the class or tags of each frame) keeps the similar distribution of classes in both the training as well as the validation set.Remember  there are 101 categories in which a video can be classified. So, we will have to create 101 different columns in the target, one for each category. We will use the get_dummies() function for that:Next step  define the architecture of our video classification model.Since we do not have a very large dataset, creating a model from scratch might not work well. So, we will use a pre-trained model and take its learnings to solve our problem.For this particular dataset, we will be using the VGG-16 pre-trained model. Lets create a base model of the pre-trained model:This model was trained on a dataset that has 1,000 classes. We will fine tune this model as per our requirement. include_top = False will remove the last layer of this model so that we can tune it as per our need.Now, we will extract features from this pre-trained model for our training and validation images:Output: (59075, 7, 7, 512)We have 59,075 images in the training set and the shape has been changed to (7, 7, 512) since we have passed these images through the VGG16 architecture. Similarly, we will extract features for validation frames:Output: (14769, 7, 7, 512)There are 14,769 images in the validation set and the shape of these images has also changed to (7, 7, 512). We will use a fully connected network now to fine-tune the model. This fully connected network takes input in single dimension. So, we will reshape the images into a single dimension:It is always advisable to normalize the pixel values, i.e., keep the pixel values between 0 and 1. This helps the model to converge faster.Next, we will create the architecture of the model. We have to define the input shape for that. So, lets check the shape of our images:Output: (59075, 25088)The input shape will be 25,088. Lets now create the architecture:We have multiple fully connected dense layers. I have added dropout layers as well so that the model will not overfit. The number of neurons in the final layer is equal to the number of classes that we have and hence the number of neurons here is 101.We will now train our model using the training frames and validate the model using validation frames. We will save the weights of the model so that we will not have to retrain the model again and again.So, lets define a function to save the weights of the model:We will decide the optimum model based on the validation loss. Note that the weights will be saved as weights.hdf5. You can rename the file if you wish. Before training the model, we have to compile it:We are using the categorical_crossentropy as the loss function and the optimizer is Adam. Lets train the model:I have trained the model for 200 epochs. To download the weights which I got after training the model, you can use this link.We now have the weights which we will use to make predictions for the new videos. So, in the next section, we will see how well this model will perform on the task of video classification!Lets open a new Jupyter notebook to evaluate the model. The evaluation part can also be split into multiple steps to understand the process more clearly:Youll be familiar with the first step  importing the required libraries:Next, we will define the model architecture which will be similar to what we had while training the model:This is the pre-trained model and we will fine-tune it next:Now, as we have defined the architecture, we will now load the trained weights which we stored as weights.hdf5:Compile the model as well:Make sure that the loss function, optimizer, and the metrics are the same as we used while training the model.You should have downloaded the train/test split files as per the official documentation of the UCF101 dataset. If not, download it from here. In the downloaded folder, there is a file named testlist01.txt which contains the list of test videos. We will make use of that to create the test data:We now have the list of all the videos stored in a dataframe. To map the predicted categories with the actual categories, we will use the train_new.csv file:Now, we will make predictions for the videos in the test set.Let me summarize what we will be doing in this step before looking at the code. The below steps will help you understand the prediction part:Lets code these steps and generate predictions:This step will take some time as there are around 3,800 videos in the test set. Once we have the predictions, we will calculate the performance of the model.Time to evaluate our model and see what all the fuss was about.We have the actual tags as well as the tags predicted by our model. We will make use of these to get the accuracy score. On the official documentation page of UCF101, the current accuracy is 43.90%. Can our model beat that? Lets check!Output: 44.80570975416337Great! Our models accuracy of 44.8% is comparable to what the official documentation states (43.9%).You might be wondering why we are satisfied with a below 50% accuracy. Well, the reason behind this low accuracy is majorly due to lack of data. We only have around 13,000 videos and even those are of a very short duration.In this article, we covered one of the most interesting applications of computer vision  video classification. We first understood how to deal with videos, then we extracted frames, trained a video classification model, and finally got a comparable accuracy of 44.8% on the test videos.We can now try different approaches and aim to improve the performance of the model. Some approaches which I can think of are to use 3D Convolutions which can directly deal with videos.Since videos are a sequence of frames, we can solve it as a sequence problem as well. So, there can be multiple more solutions to this and I suggest you explore them. Feel free to share your findings with the community.As always, if you have any suggestions or doubts related to this article, post them in the comments section below and I will be happy to answer them. And as I mentioned earlier, do check out the computer vision course if youre new to this field.",https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/
Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!,Learn everything about Analytics|Overview|Introduction|Top Data Science GitHub Projects|Machine Learning Projects|pyforest  Importing all Python Data Science Libraries in One Line of Code|HungaBunga  A Different Way of Building Machine Learning Models using sklearn|Deep Learning Projects|Behavior Suite for Reinforcement Learning (bsuite) by DeepMind|DistilBERT  A Lighter and Cheaper Version of Googles BERT|ShuffleNet Series  An Extremely Efficient Convolutional Neural Network for Mobile Devices|RAdam  Improving the Variance of Learning Rates|Programming Projects|ggtext  Improved Text Rendering for ggplot2|End Notes,"Share this:|Related Articles|Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model|3 Beginner-Friendly Techniques to Extract Features from Image Data using Python|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you ready to take that next big step in your machine learning journey? Working on toy datasets and using popular data science libraries and frameworks is a good start. But if you truly want to stand out from the competition, you need to take a leap and differentiate yourself.A brilliant way to do this is to do a project on the latest breakthroughs in data science. Want to become a Computer Vision expert? Learn how the latest object detection algorithm works. If Natural Language Processing (NLP) is your calling, then learn about the various aspects and off-shoots of the Transformer architecture.My point is  always be ready and willing to work on new data science techniques. This is one of the fastest-growing fields in the industry and we as data scientists need to grow along with it.So, lets check out seven data science GitHub projects that were created in August 2019. As always, I have kept the domain broad to include projects from machine learning to reinforcement learning.And if you have come across any library that isnt on this list, let the community know in the comments section below this article!This article is part of the monthly GitHub project series we host on Analytics Vidhya. Heres the full list for 2019 so far in case you missed out on some mind-blowing projects:I have divided these data science projects into three broad categories:I really, really like this Python library. As the above heading suggests, your typical data science libraries are imported using just one library  pyforest. Check out this quick demo Ive taken from the librarys GitHub repository:Excited yet? pyforest currently includes pandas, NumPy, matplotlib, and many more data science libraries.Just use pip install pyforest to install the library on your machine and youre good to go. And you can import all the popular Python libraries for data science in just one line of code:Awesome! Im thoroughly enjoying using this and Im certain you will as well. You should check out the below free course on Python if youre new to the language:How do you pick the best machine learning model from the ones youve built? How do you ensure the right hyperparameter values are in play? These are critical questions a data scientist needs to answer.And the HungaBunga project will help you reach that answer faster than most data science libraries. It runs through all the sklearn models (yes, all!) with all the possible hyperparameters and ranks them using cross-validation.Heres how to import all the models (both classification and regression):You should check out the below comprehensive article on supervised machine learning algorithms:Deepmind has been in the news recently for the huge losses they have posted year-on-year. But lets face it, the company is still clearly ahead in terms of its research in reinforcement learning. They have bet big on this field as the future of artificial intelligence.So here comes their latest open source release  the bsuite. This project is a collection of experiments that aims to understand the core capabilities of a reinforcement learning agent.I like this area of research because it is essentially trying to fulfill two objectives (as per their GitHub repository):The GitHub repository contains a detailed explanation of how to use bsuite in your projects. You can install it using the below code:If youre new to reinforcement learning, here are a couple of articles to get you started:You must have heard of BERT at this point. It is one of the most popular and quickly becoming a widely-adopted Natural Language Processing (NLP) framework. BERT is based on the Transformer architecture.But it comes with one caveat  it can be quite resource-intensive. So how can data scientists work on BERT on their own machines? Step up  DistilBERT!DistilBERT, short for Distillated-BERT, comes from the team behind the popular PyTorch-Transformers framework. It is a small and cheap Transformer model built on the BERT architecture. According to the team, DistilBERT runs 60% faster while preserving over 95% of BERTs performances.This GitHub repository explains how DistilBERT works along with the Python code. You can learn more about PyTorch-Transformers and how to use it in Python here:A computer vision project for you! ShuffleNet is an extremely computation-efficient convolutional neural network (CNN) architecture. It has been designed for mobile devices with very limited computing power.This GitHub repository includes the below ShuffleNet models (yes, there are multiple):So are you looking to understand CNNs? You know I have you covered:RAdam was released less than two weeks ago and it has already accumulated 1200+ stars. That tells you a lot about how well this repository is doing!The developers behind RAdam show in their paper that the convergence issue we face in deep learning techniques is due to the undesirably big variance of the adaptive learning rate in the early stages of model training.RAdam is a new variant of Adam, that rectifies the variance of the adaptive learning rate. This release brings a solid improvement over the vanilla Adam optimizer which does suffer from the issue of variance.Here is the performance of RAdam compared to Adam and SGD with different learning rates (X-axis is the number of epochs):You should definitely check out the below guide on optimization in machine learning (including Adam):This one is for all the R users in our community. And especially all of you who work regularly with the awesome ggplot2 package (which is basically everyone).The ggtext package enables us to produce rich-text rendering for the plots we generate. Here are a few things you can try out using ggtext:The GitHub repository contains a few intuitive examples which you can replicate on your own machine.ggtext is not yet available through CRAN so you can download and install it from GitHub using this command:Want to learn more about ggplot2 and how to work with interactive plots in R? Here you go:I love working on these monthly articles. The amount of research and hence breakthroughs happening in data science are extraordinary. No matter which era or standard you compare it with, the rapid advancement is staggering.Which data science project did you find the most interesting? Will you be trying anything out soon? Let me know in the comments section below and well discuss ideas!",https://www.analyticsvidhya.com/blog/2019/09/7-data-science-projects-github-showcase-your-skills/
3 Beginner-Friendly Techniques to Extract Features from Image Data using Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|How do Machines Store Images?|Reading Image Data in Python|Method #1: Grayscale Pixel Values as Features|Method #2: Mean Pixel Value of Channels||Method #3: Extracting Edge Features|End Notes,"Share this:|Related Articles|Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!|A Complete List of Important Natural Language Processing Frameworks you should Know (NLP Infographic)|
Aishwarya Singh
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you worked with image data before? Perhaps youve wanted to build your own object detection model, or simply want to count the number of people walking into a building. The possibilities of working with images using computer vision techniques are endless.But Ive seen a trend among data scientists recently. Theres a strong belief that when it comes to working with unstructured data, especially image data, deep learning models are the way forward. Deep learning techniques undoubtedly perform extremely well, but is that the only way to work with images?Not all of us have unlimited resources like the big technology behemoths such as Google and Facebook. So how can we work with image data if not through the lens of deep learning?We can leverage the power of machine learning! Thats right  we can use simple machine learning models like decision trees or Support Vector Machines (SVM). If we provide the right data and features, these machine learning models can perform adequately and can even be used as a benchmark solution.So in this beginner-friendly article, we will understand the different ways in which we can generate features from images. You can then use these methods in your favorite machine learning algorithms!Lets start with the basics. Its important to understand how we can read and store images on our machines before we look at anything else. Consider this the pd.read_ function, but for images.Ill kick things off with a simple example. Look at the image below:We have an image of the number 8. Look really closely at the image  youll notice that it is made up of small square boxes. These are called pixels.There is a caveat, however. We see the images as they are  in their visual form. We can easily differentiate the edges and colors to identify what is in the picture. Machines, on the other hand, struggle to do this. They store images in the form of numbers. Have a look at the image below:Machines store images in the form of a matrix of numbers. The size of this matrix depends on the number of pixels we have in any given image.Lets say the dimensions of an image are 180 x 200 or n x m. These dimensions are basically the number of pixels in the image (height x width).These numbers, or the pixel values, denote the intensity or brightness of the pixel. Smaller numbers (closer to zero) represent black, and larger numbers (closer to 255) denote white. Youll understand whatever we have learned so far by analyzing the below image.The dimensions of the below image are 22 x 16, which you can verify by counting the number of pixels:Source: Applied Machine Learning CourseThe example we just discussed is that of a black and white image. What about colored images (which are far more prevalent in the real world)? Do you think colored images also stored in the form of a 2D matrix as well?A colored image is typically composed of multiple colors and almost all colors can be generated from three primary colors  red, green and blue.Hence, in the case of a colored image, there are three Matrices (or channels)  Red, Green, and Blue. Each matrix has values between 0-255 representing the intensity of the color for that pixel. Consider the below image to understand this concept:Source: Applied Machine Learning CourseWe have a colored image on the left (as we humans would see it). On the right, we have three matrices for the three color channels  Red, Green, and Blue. The three channels are superimposed to form a colored image.Note that these are not the original pixel values for the given image as the original matrix would be very large and difficult to visualize. Also, there are various other formats in which the images are stored. RGB is the most popular one and hence I have addressed it here. You can read more about the other popular formats here.Lets put our theoretical knowledge into practice. Well fire up Python and load an image to see what the matrix looks like:(28,28)The matrix has 784 values and this is a very small part of the complete matrix. Heres a LIVE coding window for you to run all the above code and see the result without leaving this article! Go ahead and play around with it:Lets now dive into the core idea behind this article and explore various methods of using pixel values as features.The simplest way to create features from an image is to use these raw pixel values as separate features.Consider the same example for our image above (the number 8)  the dimension of the image is 28 x 28.Can you guess the number of features for this image? The number of features will be the same as the number of pixels! Hence, that number will be 784.Now heres another curious question  how do we arrange these 784 pixels as features? Well, we can simply append every pixel value one after the other to generate a feature vector. This is illustrated in the image below:Let us take an image in Python and create these features for that image:(650, 450The image shape here is 650 x 450. Hence, the number of features should be 297,000. We can generate this using the reshape function from NumPy where we specify the dimension of the image:Here, we have our feature  which is a 1D array of length 297,000. Easy, right? Try your hand at this feature extraction method in the below live coding window:But here, we only had a single channel or a grayscale image. Can we do the same for a colored image? Lets find out!While reading the image in the previous section, we had set the parameter as_gray = True. So we only had one channel in the image and we could easily append the pixel values. Let us remove the parameter and load the image again:This time, the image has a dimension (660, 450, 3), where 3 is the number of channels. We can go ahead and create the features as we did previously. The number of features, in this case, will be 660*450*3 = 891,000.Alternatively, here is another approach we can use:Instead of using the pixel values from the three channels separately, we can generate a new matrix that has the mean value of pixels from all three channels.The image below will give you even more clarity around this idea:By doing so, the number of features remains the same and we also take into account the pixel values from all three channels of the image. Let us code this out in Python. We will create a new matrix with the same size 660 x 450, where all values are initialized to 0. This matrix will store the mean pixel values for the three channels:We have a 3D matrix of dimension (660 x 450 x 3) where 660 is the height, 450 is the width and 3 is the number of channels. To get the average pixel values, we will use a for loop:The new matrix will have the same height and width but only 1 channel. Now we can follow the same steps that we did in the previous section. We append the pixel values one after the other to get a 1D array:Consider that we are given the below image and we need to identify the objects present in it:You must have recognized the objects in an instant  a dog, a car and a cat. What are the features that you considered while differentiating each of these images? The shape could be one important factor, followed by color, or size. What if the machine could also identify the shape as we do?A similar idea is to extract edges as features and use that as the input for the model. I want you to think about this for a moment  how can we identify edges in an image? Edge is basically where there is a sharp change in color. Look at the below image:I have highlighted two edges here. We could identify the edge because there was a change in color from white to brown (in the right image) and brown to black (in the left). And as we know, an image is represented in the form of numbers. So, we will look for pixels around which there is a drastic change in the pixel values.Lets say we have the following matrix for the image:Source: Applied Machine Learning CourseTo identify if a pixel is an edge or not, we will simply subtract the values on either side of the pixel. For this example, we have the highlighted value of 85. We will find the difference between the values 89 and 78. Since this difference is not very large, we can say that there is no edge around this pixel.Now consider the pixel 125 highlighted in the below image:Source: Applied Machine Learning CourseSince the difference between the values on either side of this pixel is large, we can conclude that there is a significant transition at this pixel and hence it is an edge. Now the question is, do we have to do this step manually?No! There are various kernels that can be used to highlight the edges in an image. The method we just discussed can also be achieved using the Prewitt kernel (in the x-direction). Given below is the Prewitt kernel:We take the values surrounding the selected pixel and multiply it with the selected kernel (Prewitt kernel). We can then add the resulting values to get a final value. Since we already have -1 in one column and 1 in the other column, adding the values is equivalent to taking the difference.There are various other kernels and I have mentioned four most popularly used ones below:Source: Applied Machine Learning CourseLets now go back to the notebook and generate edge features for the same image:This was a friendly introduction to getting your hands dirty with image data. I feel this is a very important part of a data scientists toolkit given the rapid rise in the number of images being generated these days.So what can you do once you are acquainted with this topic? We will deep dive into the next steps in my next article  dropping soon! So watch this space and if you have any questions or thoughts on this article, let me know in the comments section below.Edit: Here is an article on advanced feature Extraction Techniques for ImagesFeature Engineering for Images: A Valuable Introduction to the HOG Feature DescriptorAlso, here are two comprehensive courses to get you started with machine learning and deep learning:",https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/
A Complete List of Important Natural Language Processing Frameworks you should Know (NLP Infographic),Learn everything about Analytics|Overview|Introduction,"Share this:|Related Articles|3 Beginner-Friendly Techniques to Extract Features from Image Data using Python|11 Innovative Data Visualizations you Should Learn (in Python, R, Tableau and D3.js)|
Mohd Sanad Zaki Rizvi
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you heard about the latest Natural Language Processing framework that was released recently? I dont blame you if youre still catching up with the superb StanfordNLP library or the PyTorch-Transformers framework!There has been a remarkable rise in the amount of research and breakthroughs happening in NLP in the last couple of years.I can trace this recent rise to one (seismic) paper  Attention is All You Need by Google AI in June 2017. This breakthrough has spawned so many new and exciting NLP libraries that enable us to work with text in ways that were previously limited to our imagination (or Hollywood).Here is the interest in natural language processing according to Google searches in the last 5 years in the US:We can see a similar pattern when we expand the search to include the entire globe!Today, we have State-of-the-Art approaches for Language Modeling, Transfer Learning and many other important and advanced NLP tasks. Most of these involve the application of deep learning, especially the Transformer architecture that was introduced in the above paper.So we decided to collate all the important developments in one place and in one neat timeline. Youll love this infographic and should keep it handy during your own NLP journey.I have listed a few tutorials below to help you get started with these frameworks:Without any further ado, here is the infographic in all its glory! And if you want to download the high-resolution PDF (which you really should), head over here.",https://www.analyticsvidhya.com/blog/2019/08/complete-list-important-frameworks-nlp/
"11 Innovative Data Visualizations you Should Learn (in Python, R, Tableau and D3.js)",Learn everything about Analytics|Overview|Introduction|Data Visualization in R|Create BBC-Style Visualizations in R|Interactive Plots in R|Sankey Diagrams in R|Data Visualization in Tableau|The Worlds Largest Vote  Indias Elections Visualized|Monitor Sales Performance using Tableau|Film Genre Popularity  1910-2018|Data Visualization in D3.js|Concept Map  Relationship Between Concepts|Sequences Sunburst Visualization in D3.js|Visualizing the Interaction Between Game of Thrones Characters|Data Visualization in Python|A Geologic Map of Mars|Plotting Geostationary Satellites in Python|End Notes,"Share this:|Related Articles|A Complete List of Important Natural Language Processing Frameworks you should Know (NLP Infographic)|Decoding the Black Box: An Important Introduction to Interpretable Machine Learning Models in Python|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Visualization gives you answers to questions you didnt know you had.  Ben ShneidermanI have been involved in the data science field for a few years but I come from a non-technical background (learning and development). It took me a while to truly transition to data science. I say truly because the initial few weeks were a whirlwind of change (as I mention in detail in the structured thinking for data science course).One of the biggest changes in my mindset revolved around how I looked at data. Initially, when I was asked by my manager to analyze certain data, I used to come up with run-of-the-mill data visualizations (scatter plots, bar charts, etc.). Little did I imagine the endless possibilities in store for me!I cannot relate more to Ben Shneidermans quote above. Truly mastering data visualization techniques opens doors and opportunities you hadnt dreamed of before. A well thought out visualization peels back the layers surrounding a raw dataset.And that can often be the difference between a successful and a mundane data science project.So in this article, my aim is to show you the incredible power of data visualization. I have put together 11 mind-blowing visualizations that cover a variety of topics. And to show that you can do this in any tool of your choice, we will cover these visualizations in Python, R, Tableau as well as D3.js.The challenge for you? Pick up the visualization (code provided for all of them) and come up with your own version in your tool of choice.The creation of the ggplot2 library has made R the go-to tool for data visualization (for programmers at least!). I started my own data science journey using R and was instantly enthralled by the beauty and power of ggplot.Stylish visualizations, instant insights, unearthing patterns  all of this in just a few lines of code. Its no surprise that even hardcore Python programmers import ggplot2 in their Jupyter notebooks (yes, thats possible now).If youre using R and havent explored ggplot2 yet  make sure you do that TODAY:This isnt strictly one visualization, though that certainly isnt a problem, right? The above dashboard is an amalgamation of visualizations released by the BBC data team. Ive been following the BBC website for years so this was a welcome release.The BBC data team has actually developed and released an R package and an R cookbook for generating visualizations like the one above. The R package is called bbplot. It provides functions for creating and exporting visualizations made in ggplot in the style used by the BBC data team.Here are the key resources to get you on your way:Who doesnt enjoy interactive plots? They are one of the most engaging aspects of a presentation (if used correctly). This visualization shows us how life expectancy has changed over the years in different continents plotted against the GDP per capita.So much information packed into such a small space. The package used to create the above visualization? gganimate! No surprise to see that the power of ggplot extends to yet another awesome visualization type.You can check out our guide to building interactive plots in R:Intrigued? This is a classic example of a Sankey diagram. It essentially shows the flow of information, where the width of the arrows is proportional to the flow quantity. The above visualization shows the relevance of Facebooks custom list advertising.This visualization was created using the ggalluvial package in R. It combines the style and flexibility of the original alluvial package with the power of the tidyverse.The full code, which is just a few lines, can be found here.In good information visualization, there are no rules, no guidelines, no templates, no standard technologies, no stylebooks. You must simply do what it takes.  Edward TufteEdward Tufte is a pioneer in the field of data visualization. I feel this quote really applies to the visualizations we generate using Tableau. The plethora of features and customizations Tableau offers is almost unparalleled.If youre interested in getting started with Tableau, youve come to the right place! Here is a series of articles to help you transition from a Tableau beginner to an expert:This is a truly stunning visualization. I have only taken one part of the full dashboard. The scope of this visualization and the amount of data covered is staggering and really useful for anyone interested in this kind of analysis.Each data point represents details about each seat, including the winners name, state, party, and constituency). Look at how neat this visualization is, despite packing in a bucket load of information. This is something we can all aspire to in our daily/weekly/monthly report, right?Here is the full Tableau dashboard which you can download.I wanted to include a real-world business dashboard. If youre struggling to visualize where you can use these visualizations in the real-world (use your imagination!), you should find this super useful.This is an analysis of sales data to measure the distance from the original quota. I especially liked the first horizontal tab which neatly summarized the key figures a client or stakeholder needs to know.The full Tableau workbook contains five comprehensive dashboards that look at these sales figures from different perspectives. I really feel you should use this as a reference if you work in the sales or marketing field.Im a big movie buff so this visualization instantly drew my attention on the Tableau public gallery. Keep in mind that this is the popularity of film genres over time. Each genre has a different axis range hence look at them from that lens (rather than a one versus one comparison).What stood out for me is that you can consider this as a dashboard with multiple data points presented. Can you think of a similar use case in your professional life where such a dashboard would come in handy?You can download the entire worksheet and play around with it in Tableau.If you want to create jaw-dropping animated visualizations, D3.js should be your go-to tool. It is a powerful library that enables you to build customized visualizations for any kind of storytelling you could imagine for the web.This section is perhaps my favorite out of the four we have covered in this article. You should strongly consider adding D3.js to your skillset, especially if you want to work with data visualization regularly.Here are two popular articles on how to get started with D3.js:I use a concept map quite often. I can easily depict relationships between different concepts or knowledge points. As Wikipedia says, A concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure.Youll find it useful for mapping business decisions, process flow diagrams, information design, knowledge visualization, among other things. Its an under-rated yet useful tool to have in your arsenal.This concept map is very interactive and you can play around with the different nodes as well.Ah, brilliant! This visualization shows how to use the sunburst concept with data that describes the sequence of events.Think about it  you can visualize your customers journey using this. Instead of a static funnel, you can see all possible paths using this visualization. Your marketing team will love you for implementing this. The full D3.js code to generate this sunburst of sequences is here.Are you a Game of Thrones fan? Then youll love this visualization. It represents the influence of each character based on the number of times his/her interaction has come up in the A Storm of Swords book.Note that the nodes represent the characters and the links the interaction between them. The size of the node and name represents the influence of the character. No surprise to see Tyrion having the biggest influence, is there?You can build your own Game of Thrones visualization using this tutorial.We often think of Python as the ultimate programming language for data science. We associate it with cleaning data, building predictive models, and even certain data engineering tasks. But did you know that Python is actually pretty useful for generating data visualizations?Thats right  Python comes with two exclusive libraries for visualization  matplotlib and seaborn. You can check out this article to know more about these libraries and see them in action.This visualization is a thing of beauty. I came across this geologic map of Mars a few days back and Im still astonished that this was created in Python (with a bit of help from Adobe Illustrator). Amazing!The Python libraries used to create this wonderful visualization are:If the font is too small to read or you want to print this out as a poster  get the full high-resolution image here. And you can get the Python code for this visualization here. The GitHub repository has the complete tutorial to get you started.The PyEphem package was used to create this impressive plot in Python. PyEphem basically lets us implement astronomical algorithms in Python.Quite a handful of data science enthusiasts tried their hand on plotting this visualization and you can find all the resources here.I had a lot of fun putting this list together. I work mostly with R and Tableau so it was eye-opening to see the kind of visualizations we can generate using D3.js. Im definitely going to try my hand there.Are there any visualizations youve come across that blew your mind? Go ahead and share them with us in the comments section below. This is the best place to get creative and learn from the community!",https://www.analyticsvidhya.com/blog/2019/08/11-data-visualizations-python-r-tableau-d3js/
Decoding the Black Box: An Important Introduction to Interpretable Machine Learning Models in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Interpretable Machine Learning?|Why do we Need Interpretable Machine Learning?|When can we do Away with Interpretability?|Framework for Interpretable Machine Learning|Lets Talk About Inherently Interpretable Models|Model Agnostic Techniques|LIME (Local Interpretable Model agnostic Explanations)|Python Implementation of Interpretable Machine Learning Techniques|End Notes,"Fairness|Checking causality of features & Debugging models|Regulations|Scope|Model|Linear/Logistic|Decision Trees|An Example of Feature Importance|Tree Ensembles|Global Surrogate Method|Building and Understanding Interpretable Machine Learning Models|Feature Importance||Exercise|Global Surrogate|Implementing LIME in Python to generate local interpretations of black-box models|Exercise 2|Share this:|Related Articles|11 Innovative Data Visualizations you Should Learn (in Python, R, Tableau and D3.js)|How can you Convert a Business Problem into a Data Problem? A Successful Data Science Leaders Guide|
Ankit Choudhary
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Can you interpret a deep neural network? How about a random forest with 500 trees? Building a complex and dense machine learning model has the potential of reaching our desired accuracy, but does it make sense? Can you open up the black-box model and explain how it arrived at the final result?These are critical questions we need to answer as data scientists. A wide variety of businesses are relying on machine learning to drive their strategy and spruce up their bottomline. Building a model that we can explain to our clients and stakeholders is key.Can you imagine building a facial recognition software that misclassifies a person? Or a credit card fraud detection model that raises an alarm for a perfectly legal transaction? And then not being able to explain why thats happening  not ideal.So the question is  how do we build interpretable machine learning models? Thats what we will talk about in this article. Well first understand what interpretable machine learning is and why its important. Then we will understand a simple framework for interpretable ML and use that to build machine learning models.This is a very important topic in machine learning so strap in for a thrilling learning journey!Interpretable machine learning is part of the comprehensive Applied Machine Learning course. The course provides you all the tools and techniques you need to solve business problems using machine learning. It is an end-to-end course for beginners as well as intermediate-level professionals!How do we build trust in machine learning models? Thats essentially what this boils down to.Machine learning-powered applications have become an ever-increasing part of our lives, from image and facial recognition systems to conversational applications, autonomous machines, and personalized systems.The sort of decisions and predictions being made by these machine learning-enabled systems are becoming much more profound, and in many cases, critical to life, death, and personal wellness. The need to trust these AI-based systems is paramount.So, lets first take a step back and ask the question: What involves a predictive modeling lifecycle?All these steps are involved in almost all problems based on structured datasets. However, a key issue that arises especially as model complexity increases is interpretability. Lets start with the formal definition:Interpretation of a machine learning model is the process wherein we try to understand the predictions of a machine learning model.The involvement of humans in a predictive modeling lifecycle is at two important stages:Before we jump into various kinds of techniques for interpreting machine learning models, lets look at why this is important.Lets take a simple example to understand this. Suppose we are trying to predict employees performance in a big company to expedite the appraisal process and identify the best employees.We have data from the last 10 years about the performance reviews of employees. But what if that company tends to promote more men than women?In this case, the model might learn the bias and predict that men tend to perform at a higher scale (and this bias has unfortunately happened in certain real-world scenarios). Now, if there is no way to interpret our model at this stage, the model might end up providing false insights at the cost of compromising on fairness.Lets consider another example. Consider that we are building a model for classifying wolves vs dogs. The data that is available is simply labeled images of dogs and wolves.Now, what if we have wolves and dogs in entirely different backgrounds? This is entirely possible as wolves are mostly found in the wild (in snow, jungles, etc.) while dogs are in completely different backgrounds (households) generally.We built an image classifier and got a really good performance on the validation set.But on using one of the interpretability methods, we see that our model is actually ignoring the dog and wolf while using just the background pixels for doing the classification. This model might give a good performance on the validation set as they contain different backgrounds for the wolf and dog respectively.Having an interpretable model, in this case, enables us to test the causality of the features, test its reliability and ultimately can help us to debug the model appropriately.For example, we can try to alter our data by adding images of the animals in different backgrounds or simply crop the background out of the images to ensure that the right signal is picked up by our machine learning or deep learning model.Based on the latest regulations by the EU, GDPRs article 12 allows individuals to enquire about algorithmic decisions. For example, in the banking and finance industry, questions such as the following can come up and have to be answered by these banks:Not everything requires interpretability. It is also important to understand when we do not need to invest in building interpretable machine learning models.Now that we have an intuition of what ML interpretability is and why its important, lets look at the different ways to classify interpretability techniques:Overall we can think of interpretability under two structures: Whether we are looking to interpret globally for all data points, the importance of each variable, or are we looking to explain a particular prediction which is local?The second way of looking at this is whether we are talking about a technique that works across all types of models (model agnostic) or is tailor-made for a particular class of algorithms (model specific).For linear models such as a linear and logistic regression, we can get the importance from the weights/coefficients of each feature.Lets revisit that quickly. Suppose we are trying to predict an employees salary using linear regression. The independent variables are experience in years and a previous rating out of 5.For normalized data, W1 and W2 can essentially tell us whether the experience is more important towards salary or rating. Here, note that this is a model-specific technique that can be used for both global and local explanations.Learn more about linear and logistic regression in the below articles:A decision tree is another algorithm that is very interpretable as we have access to all the splits for each feature:We can clearly see how the decisions are being taken starting from the root node to the leaf node. We just have to follow the rules on the basis of independent variables and list them down to explain each prediction. Again, this is a model-specific technique that can be used for local explanations.What about global explanations?For a small decision tree, we can use the above diagram. However, if we have a lot of features and we are training a deep decision tree for, lets say, a depth of 8 or 9, there will be too many decision rules to present effectively. In this case, we can use feature importance to interpret the importance of each feature at a global level.Now, lets learn how we can calculate feature importance for a decision tree.Decision trees make splits to maximize the decrease in impurity. We can use this reduction to measure the contribution of each feature.Lets see how this works:Here,Here, again, this is a model-specific technique that can be used for only global explanations. This is because we are looking at the overall importance and not at each prediction.Learn more about decision trees in this superb tutorial.Now, let us try to understand this with an example. This will help you visualize what weve covered so far (and understand its importance).Lets say we have a decision tree with 4 samples and Gini impurity values as shown in the below figure. Each feature here appears only once so we can directly use the formula to calculate feature importance:Since each feature is used once in our case, there is no need to calculate the sum.Take a moment to pause here and calculate this on your own. You will have a much better grasp on the concept once you do it by yourself.Now, for tree ensembles such as random forest and Gradient Boosting Machines, we can use the same feature importance. But this time, we will take its average across all trees. Let us look at the steps involved:The popular sklearn library uses this technique to find feature importance for each feature.Here are two intuitive guides to learn about Random Forest and Ensembling:So far, we have discussed model-specific techniques for linear and logistic regression, as well as decision trees. We also spoke about the feature importance methods that are used for ensemble methods. Im sure youre wondering  what about other models?Now, we know that some models are hard to interpret, such as random forest and gradient boosting.We did use feature importance for these techniques. But it does not tell us whether a particular feature affects the target positively or negatively. And that is VERY important in certain cases.Some machine learning models are even harder to interpret. For example, a deep neural network model can have millions of learned parameters and it essentially ends up being an extreme version of a black-box model.I really like this plot. As the complexity of the machine learning model increases, we get better performance but lose out on interpretability. You should keep this figure handy the next time youre building your own model.So how can we build interpretable machine learning models that dont compromise on accuracy?One idea is to use simpler models. That way you can ensure you have full confidence in the interpretability. However, complex models can provide much better performance. So is there a way we can have some level of interpretability for black-box models as well?Yes! Model agnostic techniques allow us to build and use more complex models without losing all interpretability power.Lets take a high-level look at model-agnostic interpretability. We capture the world by collecting data, and abstract it further by learning to predict the data with a machine learning model. Interpretability is just another layer on top that helps humans understand the black box using a simpler, more interpretable model.The first model agnostic method we will discuss here is the global surrogate method. A global surrogate model is an interpretable model that is trained to approximate the predictions of a black-box model.We can draw conclusions about the black-box model by interpreting the surrogate model. So, we are basically solving machine learning interpretability by using more machine learning!For example, we could interpret a random forest classifier using a simple decision tree to explain its predictions:This is done by training a decision tree on the predictions of the black-box model (which is a random forest in our case). And once it provides good enough accuracy, we can use it to explain the random forest classifier.Overall, we want an interpretable surrogate model that is trained to approximate the predictions of a black-box model and draw conclusions.Here is a step-by-step breakdown to understand how a global surrogate model works:The global surrogate method is good for looking at an interpretable model that can explain predictions for a black-box approach. However, this will not work well if we want to understand how a single prediction was made for a given observation.This is where we use the LIME technique which stands for local interpretable model agnostic explanations. LIME is based on the work presented in this paper. Let us understand how LIME works using an example.Suppose we are working on a binary classification problem. As we can see in the above image, we have a decision boundary for a black-box model with two features.Lets say we want to interpret the contributions of x1 and x2 for the observation in yellow (in the below image). We take the data sampled from a normal distribution to generate fake data around the observation:Next, we assign higher weights to the points that are closer to our observation:We train an interpretable model over the fake data generated from the distribution. Now, we have a new local decision boundary for the locally learned model (in white) that can be used to understand the contributions of x1 and x2 towards the prediction of our observation:Summarising all the steps:My favorite part of the article  building interpretable machine learning models in Python!Here, we will work on the implementation of both the methods we covered above. We will use the big mart sales problem hosted on our Datahack Platform. The problem statement includes predicting sales for different items being sold at different outlets. You can download the dataset from the above link.Note: You can go through this course to fully understand how to build models using this data. Our focus here is to focus on the interpretability part.Let us first look at how to do interpretability for inherently interpretable machine learning models.Importing the Required LibrariesReading DataMissing Value TreatmentFeature EngineeringData PreprocessingTrain-Test SplitTraining a Decision Tree ModelUse the Graphviz library to visualize the decision treeThis visualization of our decision tree clearly displays the rules it is using to make a prediction. Here, Item_MRP & Outlet_Type are the first features that are affecting the sales of various items at each outlet. If you want to look at the complete decision tree, you can easily do that by changing the max_depth parameter using the export_graphviz function.Now, we will have a look at the feature importance for each feature in case of a random forest.The random forest model gives a similar interpretation. Item_MRP still remains the most important feature (exactly as the decision tree model above). Relative importance also helps us compare each feature. For example, Outlet_Type_0 is a much more important feature than other outlet types.As an exercise try calculating the feature importance for the decision tree we fit earlier and compare:Next, we will create a surrogate decision tree model for this random forest model and see what we get.This decision tree performs well on the new target and can be used as a surrogate model to explain the predictions of a random forest model. Similarly, we can use it for any other complex model. Just make sure your decision tree fits well, otherwise, you might get wrong interpretations (a nightmare!).We can implement the LIME technique in both R & Python using the LIME package. Lets jump into implementation for the same to check the local interpretation of a given prediction using LIME:Generate Explanations using LIME
The predicted value for sales is 185.40. Each features contribution to this prediction is shown in the right bar plot. Orange signifies the positive impact and blue signifies the negative impact of that feature on the target. For example, Item_MRP has a positive impact on sales.Using the coding window below, try applying LIME to more complex models such as Xgboost & LightGBM. The code for preprocessing and model building is already inserted here:For more details on LIME and its implementation, you can go through this article.LIME is a powerful technique but has its disadvantages as it relies on locally generated fake data and uses simple linear models to explain predictions. However, it can be used for text and image data as well.As I mentioned earlier, interpretable machine learning is part of our utterly comprehensive end-to-end course:Make sure you check it out. And if you have any questions or feedback regarding this article, let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2019/08/decoding-black-box-step-by-step-guide-interpretable-machine-learning-models-python/
How can you Convert a Business Problem into a Data Problem? A Successful Data Science Leaders Guide,Learn everything about Analytics|Overview|Introduction|Table of Contents|Quick Recap of Managing Different Data Science Stakeholders (Article #1)|Bridging the Qualitative-to-Quantitative Gap in Data Science|Is the Right Data Available with the Right Level of Granularity?|Are We Asking the Right Questions?|Repeatability and Reproducibility: Consistency in Labeled Data for Accurate AI Systems|Active Learning for Efficient and More Accurate AI Systems|Diverse Data Science Team Composition is Critical for Success|End Notes,"Share this:|Related Articles|Decoding the Black Box: An Important Introduction to Interpretable Machine Learning Models in Python|NLP Essentials: Removing Stopwords and Performing Text Normalization using NLTK and spaCy in Python|
Om Deshmukh
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How effectively can you convert a business problem into a data problem?This question holds the key to unlocking the potential of your data science project. There is no one-size-fits-all approach here. This is a nontrivial effort with positive long-term results and hence deserves a great deal of focused collaboration across the product team, the data science team, and the engineering team.Every leader knows that being able to measure progress is an invaluable aspect of any project. This understanding goes to an entirely different level when it comes to data science projects.We discussed how to manage the different stakeholders in data science in my previous article (recap below). In this article, we are going to discussthe journey of translating the broad qualitative business requirements into tangible quantitative data-driven solutions.One of the most tangible advantages of this approach, among many others, is that it establishes a common understanding of what success means and how we can measure it. It also lays a framework for how progress will be tracked and communicated among the various internal and external stakeholders.This is the second article of a four-article series that discusses my learnings from developing data-driven products from scratch and deploying them in real-world environments where their performance influences the clients business/financial decisions. You can read articles one and three here:Let me quickly recap what we covered in the first article of this series. Its important to have this background before reading further as it is essentially the base on which this article will revolve.We discussed the three key stakeholders in a data-driven product ecosystem and how the data-science-delivery leader has to align them with each other. The three main stakeholders are:With that background, lets dive into this article!Consider the following mini-scenarios:Did you find any of these questions artificial? Do re-read the scenarios and take a few seconds to think through. Most of us would find these questions to be perfectly natural!What would certainly be artificial though is asking questions like:In most scenarios, we express our asks in qualitative terms. This is true about business requirements as well.Isnt it more likely that the initial client ask will be Build us a landing page which is aesthetically pleasing yet informative versus we need a landing page which is rated at least 8.5-out-of-10 by 1000 random visitors to our website on visual-appeal, navigability and product-information parameters?On the other hand, systems are built and evaluated based on exact quantitative requirements. For example, the database query has to return in less than 30 milliseconds, the website has to fully load in less than 3 milliseconds on a typical 10mbps connection, and so on.This gap between qualitative business requirements and quantitative machine requirements is exacerbated when it comes to data-driven products.A typical business requirement for a data-driven product could be develop an optimal digital marketing strategy to reach the likely target customer population. Converting this to a quantifiable requirement has several non-trivial challenges. Some of these are:To define customers similar to our target population, we need to agree on a set of N dimensions that will be used for computing this similarity:After that, we need to critically evaluate whether all the relevant data exists in an accessible format. If not, are there ways to infer at least parts of it?Consider a business scenario where a company has a chatbot that handles customer queries automatically. When the chatbot fails to resolve a customer query, the call is transferred to a human expert.It is fair to assume that the cost of a human expert manning a call center is higher than an automated chatbot resolving the customer query. Thus, the business problem can be stated as: Reduce the proportion of calls that reach a human expert.The first barrier to cross is often the HiPPO Effect.Simply put, the HiPPO (Highest Paid Persons Opinion) effect states that the authority figures suggestions are interpreted as the final truth, and promptly implemented, even if the findings from the data are contrary.For instance, in the above example, the HiPPO might be that calls are getting diverted to human experts due to time-out issues related to network connectivity within the chatbots workflow. A more prudent data-driven approach would be to list out all the possible reasons leading to call diversions, one of them being the connectivity issue.Such a list can be derived from a combination of expert knowledge and some initial data log analysis. This step falls under, what we call, the data-discovery phase.The data-discovery phase, which is essentially an iterative process, systematizes the use of insights from the data to guide the experts intuition and to identify the next dimension of data to investigate.The data-discovery phase also identifies if there are any gaps in the ideal-data-needed vs. actual-data-available. For example, we may identify that the last interaction between the chatbot and the customer is not being stored in the database. This lack of data needs to be solved promptly by changing the data storage schema.Source: YseopLets assume that this analysis of possible failure scenarios led to the following findings: Armed with this information, the next step would be to dig deeper. For example:The findings from this step will help rank the problems in terms of their prevalence and also identify systemic issues. If the failure of the speech-to-text component is one of the prevalent problems, the speech-to-text vendor needs to be approached to identify if the speech inputs are not being captured/transferred as per the norms/best-practices or if the speech-to-text system needs more context for better predictions.Moving further along in this journey, translating qualitative data specific questions into quantitative model training strategies is also a nuanced topic, one that can have far-reaching consequences.Continuing the conversation on speech-to-text issues, it may seem prudent to answer who is the caller?. At the surface level, it may seem synonymous to is the caller Miss Y?. But these two questions lead to totally different Machine Learning (ML) models.The who is the caller? question leads to an N-class classification problem (where N is the number of possible callers), whereas is the caller Miss Y? leads to N binary-classifiers!While all of this may seem complex and data science-led, we cannot underestimate the role of the domain expert. While all errors are mathematically equal, some errors can be more damaging to the companys finances and reputation than others.Domain experts play a critical role in understanding the impact of these errors. Domain experts also help layout the best practices in the industry, understand customer expectations and adhere to regulatory requirements.For example, even if the chatbot is 100% confident that the user has asked for a renewal of a relatively inexpensive service, the call may need to be routed to a human for regulatory compliance purposes depending on the nature of the service.One of the final steps is to have a relevant subset of data labeled by human experts in a consistent manner.At the vast scale of Big Data, we are talking about obtaining labels for hundreds of thousands of samples. This will need a huge team of human experts to provide the labels.A more efficient way would be to sample the data in such a manner that only the most diverse set of samples are sent for labeling. One of the best ways to do this is to use stratified sampling. Domain experts will need to analyze which data dimensions get used for the stratification.Consistency in human labels is trickier than it may seem at first. If the existing automated techniques for label generation are 100% accurate, then there is no need for training any newer machine learning algorithms. And hence, there is no need for human-labeled training samples (e.g., we do not need manual transcription of speech if speech-to-text systems are 100% accurate).At the same time, if there is no subjectivity in human labeling, then it is just a matter of tabulating the list of steps that the human expert has followed and automating those steps. Almost all practical machine learning systems need training because they are not able to adequately capture the various nuances that humans apply in coming to a particular decision.Thus, there will be a certain level of inherent subjectivity in the human labels that cant be done away with.The goal, however, should be to design label-capturing systems that minimize avenues for extraneous subjectivity.For example, if we are training a machine learning system to predict emotion from speech, the human labels will be generated by playing the speech signals and asking the human labeler to provide the predominant emotion.One way to minimize extraneous subjectivity is to provide a drop-down of the possible emotion label options instead of letting the human labeler enter his/her inputs in a free flow text format. Similarly, even before the first sample gets labeled, there should be a normalization exercise among the human experts where they agree on the interpretation of each label (e.g., what is the difference between sad and angry).An objective way to check the subjectivity is repeatability and reproducibility (R&R). Repeatability measures the impact of temporal context on human decisions. It is computed as follows:Reproducibility measures how consistently the labels can be replicated across experts. It is computed as follows:Conducting R&R evaluations on even a small scale of data can help identify process improvements as well as help gauge the complexity of the problem.Machine learning is typically passive. This means that the machine doesnt proactively ask for human labels on samples where it is most confusing. Instead, the machines are trained on labeled samples that are fed to the training algorithms.A relatively new branch of machine learning called Active Learning tries to address this. It does so by:The human labels are sought on priority for such confusing samples.For all the pieces to come together, we need an all-rounder data science team:We covered quite a lot of ground here. We discussed the nuances of translating a qualitative business requirement into tangible quantitative business requirements.Reach out to me in the comments section below if you have any questions. I would love to hear your experience on this topic.In the third article of this series, we will discuss various deployment aspects as the data-driven product gets ready for real-world deployment. So watch this space!",https://www.analyticsvidhya.com/blog/2019/08/data-science-leader-how-effectively-transition-business-requirements-data-driven-solutions/
NLP Essentials: Removing Stopwords and Performing Text Normalization using NLTK and spaCy in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|What are Stopwords?|Why do we Need to Remove Stopwords?|When Should we Remove Stopwords?|Different Methods to Remove Stopwords|Introduction to Text Normalization|What are Stemming and Lemmatization?|Why do we need to Perform Stemming or Lemmatization?|Methods to Perform Text Normalization|End Notes,"Remove Stopwords|Avoid Stopword Removal|1. Stopword Removal using NLTK|2. Stopword Removal using spaCy|3. Stopword Removal using Gensim|Stemming|Lemmatization|S0, which one should we prefer?|1. Text Normalization using NLTK|2. Text Normalization using spaCy|3. Text Normalization using TextBlob|Share this:|Related Articles|How can you Convert a Business Problem into a Data Problem? A Successful Data Science Leaders Guide|10 Powerful Python Tricks for Data Science you Need to Try Today|
Shubham Singh
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Dont you love how wonderfully diverse Natural Language Processing (NLP) is? Things we never imagined possible before are now just a few lines of code away. Its delightful!But working with text data brings its own box of challenges. Machines have an almighty struggle dealing with raw text. We need to perform certain steps, called preprocessing, before we can work with text data using NLP techniques.Miss out on these steps, and we are in for a botched model. These are essential NLP techniques you need to incorporate in your code, your framework, and your project.We discussed the first step on how to get started with NLP in this article. Lets take things a little further and take a leap. We will discuss how to remove stopwords and perform text normalization in Python using a few very popular NLP libraries  NLTK, spaCy, Gensim, and TextBlob.Are you a beginner in NLP? Or want to get started with machine learning but arent sure where to begin? We have these two fields comprehensively covered in our end-to-end courses:Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.Generally, the most common words used in a text are the, is, in, for, where, when, to, at etc.Consider this text string  There is a pen on the table. Now, the words is, a, on, and the add no meaning to the statement while parsing it. Whereas words like there, book, and table are the keywords and tell us what the statement is all about.A note here  we need to perform tokenization before removing any stopwords. I encourage you to go through my article below on the different methods to perform tokenization:Heres a basic list of stopwords you might find helpful:Quite an important question and one you must have in mind.Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text.Just like we saw in the above section, words like there, book, and table add more meaning to the text as compared to the words is and on.However, in tasks like machine translation and text summarization, removing stopwords is not advisable.Here are a few key benefits of removing stopwords:Ive summarized this into two parts: when we can remove stopwords and when we should avoid doing so.We can remove stopwords while performing the following tasks:Feel free to add more NLP tasks to this list!NLTK, or the Natural Language Toolkit, is a treasure trove of a library for text preprocessing. Its one of my favorite Python libraries. NLTK has a list of stopwords stored in 16 different languages.You can use the below code to see the list of stopwords in NLTK:Now, to remove stopwords using NLTK, you can use the following code block. This is a LIVE coding window so you can play around with the code and see the results without leaving the article!Here is the list we obtained after tokenization:And the list after removing stopwords:Notice that the size of the text has almost reduced to half! Can you visualize the sheer usefulness of removing stopwords?spaCy is one of the most versatile and widely used libraries in NLP. We can quickly and efficiently remove stopwords from the given text using SpaCy. It has a list of its own stopwords that can be imported as STOP_WORDS from the spacy.lang.en.stop_words class.Heres how you can remove stopwords using spaCy in Python:This is the list we obtained after tokenization:And the list after removing stopwords:An important point to note  stopword removal doesnt take off the punctuation marks or newline characters. We will need to remove them manually.Read more about spaCy in this article with the librarys co-founders:Gensim is a pretty handy library to work with on NLP tasks. While pre-processing, gensim provides methods to remove stopwords as well. We can easily import the remove_stopwords method from the class gensim.parsing.preprocessing. Try your hand on Gensim to remove stopwords in the below live coding window:While using gensim for removing stopwords, we can directly use it on the raw text. Theres no need to perform tokenization before removing stopwords. This can save us a lot of time.In any natural language, words can be written or spoken in more than one form depending on the situation. Thats what makes the language such a thrilling part of our lives, right? For example:In all these sentences, we can see that the word eat has been used in multiple forms. For us, it is easy to understand that eating is the activity here. So it doesnt really matter to us whether it is ate, eat, or eaten  we know what is going on.Unfortunately, that is not the case with machines. They treat these words differently. Therefore, we need to normalize them to their root word, which is eat in our example.Hence, text normalization is a process of transforming a word into a single canonical form. This can be done by two processes, stemming and lemmatization. Lets understand what they are in detail.Stemming and Lemmatization is simply normalization of words, which means reducing a word to its root form.In most natural languages, a root word can have many variants. For example, the word play can be used as playing, played, plays, etc. You can think of similar examples (and there are plenty).Lets first understand stemming:Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).Lets consider the following two sentences:We can easily state that both the sentences are conveying the same meaning, that is, driving activity in the past. A machine will treat both sentences differently. Thus, to make the text understandable for the machine, we need to perform stemming or lemmatization.Another benefit of text normalization is that it reduces the number of unique words in the text data. This helps in bringing down the training time of the machine learning model (and dont we all want that?).Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.Lemmatization returns the lemma, which is the root word of all its inflection forms.We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features.The NLTK library has a lot of amazing methods to perform different steps of data preprocessing. There are methods like PorterStemmer() and WordNetLemmatizer() to perform stemming and lemmatization, respectively.Lets see them in action.StemmingWe can clearly see the difference here. Now, lets perform lemmatization on the same text.LemmatizationHere, v stands for verb, a stands for adjective and n stands for noun. The lemmatizer only lemmatizes those words which match the pos parameter of the lemmatize method.Lemmatization is done on the basis of part-of-speech tagging (POS tagging). Well talk in detail about POS tagging in an upcoming article.spaCy, as we saw earlier, is an amazing NLP library. It provides many industry-level methods to perform lemmatization. Unfortunately, spaCy has no module for stemming. To perform lemmatization, check out the below code:Here -PRON- is the notation for pronoun which could easily be removed using regular expressions. The benefit of spaCy is that we do not have to pass any pos parameter to perform lemmatization.TextBlob is a Python library especially made for preprocessing text data. It is based on the NLTK library. We can use TextBlob to perform lemmatization. However, theres no module for stemming in TextBlob.So lets see how to perform lemmatization using TextBlob in Python:Just like we saw above in the NLTK section, TextBlob also uses POS tagging to perform lemmatization. You can read more about how to use TextBlob in NLP here:Stopwords play an important role in problems like sentiment analysis, question answering systems, etc. Thats why removing stopwords can potentially affect our models accuracy drastically.This, as I mentioned, is part two of my series on How to Get Started with NLP. You can check out part 1 on tokenization here.And if youre looking for a place where you can finally begin your NLP journey, we have the perfect course for you:",https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/
10 Powerful Python Tricks for Data Science you Need to Try Today,Learn everything about Analytics|Overview|Introduction|1. zip: Combine Multiple Lists in Python|2. gmplot: Plot the GPS Coordinates in your Dataset on Google Maps|3. category_encoders: Encode your Categorical Variables using 15 Different Encoding Schemes|4. progress_apply: Monitor the Time you Spend on Data Science Tasks|5. pandas_profiling: Generate a Detailed Report of your Dataset|6. grouper: Grouping Time Series Data|7. unstack: Transform the Index into Columns of your Dataframe|8. %matplotlib Notebook: Interactive Plots in your Jupyter Notebook|9. %%time: Check the Running Time of a Particular Block of Python Code|10: rpy2: R and Python in the Same Jupyter Notebook!|End Notes,"||Share this:|Related Articles|NLP Essentials: Removing Stopwords and Performing Text Normalization using NLTK and spaCy in Python|The Most Comprehensive Guide to K-Means Clustering Youll Ever Need|
LAKSHAY ARORA
|17 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Installing gmplot|Plot the location coordinates on Google Maps|Installing category-encoders|Transform the categorical data into numerical data,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"When was the last time you learned a new Python trick? As data scientists, we are accustomed to working with familiar libraries and calling the same functions every time. Its time to break the old routine!Python isnt just limited to Pandas, NumPy and scikit-learn (though they are absolutely essential in datascience)! There is a whole world of Python tricks we can use to improve our code, speed up our data science tasks, and become much more efficient in writing code.And more importantly  learning new things we can do in Python is simply a whole lot of fun! I love playing around with different packages and functions. And every once in a while, a new trick will catch my eye and I incorporate that into my daily routine.So I decided to collate my favorite Python tricks in one place  this article! This list ranges from speeding up basic data science tasks like pre-processing to getting R and Python code in the same Jupyter Notebook. Theres a whole lot of learning waiting for us so lets dive right in!New to Python and the world of Data Science? Here is a superb and utterly comprehensive course to get you started with both:Quite often we end up writing complex for loops to combine more than one list together. Sounds familiar? Then youll love the zip function. The purpose of this zip function is to make an iterator that aggregates elements from each of the iterables.Lets see how to use the zip function via a simple example and combine multiple lists:Thats it! See how easy it is to combine multiple lists?I love working with Google Maps data. Think about it  it is one of the most data-rich applications youll find anywhere. Thats why I decided to start off with this Python trick.Scatterplots are excellent when we want to see the relationship between two variables. But will you use them if the variables are the latitude and longitude coordinates of a location? Probably not. It would be best to plot these points on a real map so that we can easily visualize and solve a particular problem (such as optimizing routes).gmplot provides an amazing interface to generate HTML and JavaScript to render all the data wed like on top of Google Maps. Lets see how to use gmplot with an example.You can download the dataset for this code here. Right, lets import the libraries and read the data:The above code will generate the HTML file and you can see that latitude and longitude coordinates are plotted on Google Maps. The heatmap shows the areas with a high density of points in red color. Pretty cool, right?One of the biggest obstacles we face in early-stage data science datasets  what in the world should we do about categorical variables? Our machines crunch numbers in the blink of an eye but dealing with categories is a whole different problem.A few Machine Learning algorithms can handle categorical variables on their own. But we are required to convert them into numerical variables and for this, category_encoders is an amazing library that provides 15 different encoding schemes.Lets see how can we make use of this library.category_encoders supports around 15 different encoding methods, such as:All the encoders are fully compatible with sklearn-transformers so they can easily be used in your existing scripts. Also, category_encoders supports NumPy arrays and Pandas dataframes. You can read more about category_encoders here.How much time do you typically spend on cleaning and pre-processing your data? The saying that a data scientist typically spends 60-70% of his/her time just cleaning the data is quite true. And its important for us to track this, right?We dont want to spend days and days cleaning the data and neglecting the other data science steps. This is where the progress_applyfunction makes our life so much easier. Let me demonstrate how it works.Lets calculate the distance of all the points from a particular point and see the progress of completion of this task. You can download the dataset for this here.You can see how easy is to track the progress of our code. Simple, efficient and a lifesaver.We spend a LOT of our time in understanding the data weve been given. And thats fair  we dont want to jump straight to model building without understanding what were working with. This is an essential step in any data science project.pandas_profiling is a Python package that reduces a lot of effort in doing the initial data analysis steps. This package generates a detailed report of our data in just one line of code!We can see that with just one line of code, we got a detailed report of the dataset:Who isnt familiar with Pandas at this point? It is one of the most popular Python libraries around and is widely used for data manipulation and analysis. We know that Pandas has amazing capabilities to manipulate and summarize the data.I was recently working on a time series problem and noticed that Pandas had a Grouper function that I had never used before. I became really curious about its use (the data scientist curse!).As it turns out, this Grouper function is quite an important function for time series data analysis. So, lets try this out and see how it works. You can download the dataset for this code here.Now, the first step to deal with any time series data is to convert the date column into a DateTime format:Suppose our objective is to see the monthly sales for each customer. Most of us try to write something complex here. But this is where Pandas has something more useful for us (got to love Pandas!).Instead of having to play around with reindexing, we can use a simple approach via the groupby syntax. Well add something extra to this function by providing a little more information on how to group the data in the date column. It looks cleaner and works in exactly the same way:We just saw how grouper can be helpful in grouping time series data. Now, heres a challenge  what if we want to see the name column (which is the index in the above example) as the column of our dataframe.This is where the unstack function becomes vital. Lets apply the unstack function on the above code sample and see the results.Quite useful! Note: If the index is not a MultiIndex, the output will be a Series.Im a big fan of the matplotlib library. It is the most common visualization library that we use to generate all kinds of plots in our Jupyter Notebooks.To view these plots, we generally use one line  %matplotlib inline  while importing the matplotlib library. This works well but it renders the static plots within the Jupyter Notebook.Just replace the line %matplotlib inline with %matplotlib notebook and watch the magic unfold. You will get resizable and zoomable plots within your notebook!Brilliant! With just one word change, we can get interactive plots that allow us to resize and zoom within the plots.There can be multiple approaches to solve one problem. We know this pretty well as data scientists. Computational costs matter in the industry  especially if its a small or medium-sized organization. You might want to choose the best approach which completes the task in the minimum amount of time.Its actually very easy to check the running time of a particular block of code in your Jupyter notebook.Just add the %%time command to check the running time of a particular cell:Here, we have the CPU time and the Wall time. The CPU time is the total execution time or runtime for which the CPU was dedicated to a process. Wall time is the time that a clock would have measured as having elapsed between the start of the process and now.R and Python are two of the best and most popular open-source programming languages in the data science world. R is mainly used for statistical analysis while Python provides an easy interface to translate mathematical solutions into code.Heres the good news  we can use both of them in a single Jupyter Notebook! We can make use of both ecosystems and for that, we just need to install rpy2.So, lets shelve the R versus Python debate for now and enjoy plotting ggplot-level charts in our Jupyter Notebook.We can use both the languages together, and even pass variables between them.Here, we created a dataframe df in Python and used that to create a scatterplot using Rs ggplot2 library (the function geom_point). Go ahead and try this out  youre sure to love it.This is my essential Python tricks collection. I love using these packages and functions in my day-to-day tasks. Honestly, my productivity has increased and its made working in Python more fun than ever before.Are there any Python tricks you want me to know apart from these? Let me know in the comments section below and well trade ideas!And if youre a Python beginner and a newcomer in data science, you really should check out our comprehensive and best-selling course:",https://www.analyticsvidhya.com/blog/2019/08/10-powerful-python-tricks-data-science/
The Most Comprehensive Guide to K-Means Clustering Youll Ever Need,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Clustering?|How is Clustering an Unsupervised Learning Problem?|Properties of Clusters|Applications of Clustering in Real-World Scenarios|Understanding the Different Evaluation Metrics for Clustering|Introduction to K-Means Clustering|Implementing K-Means Clustering in Python from Scratch|Challenges with the K-Means Clustering Algorithm|K-Means++ to Choose Initial Cluster Centroids for K-Means Clustering|How to Choose the Right Number of Clusters in K-Means Clustering?|Implementing K-Means Clustering in Python|End Notes,"Property 1|Property 2|Customer Segmentation|Document Clustering|
Image Segmentation|Recommendation Engines|Inertia|Dunn Index|Step 1: Choose the number of clusters k|Step 2: Select k random points from the data as centroids|Step 3: Assign all the points to the closest cluster centroid|Step 4: Recompute the centroids of newly formed clusters|Step 5: Repeat steps 3 and 4|Stopping Criteria for K-Means Clustering|Share this:|Related Articles|10 Powerful Python Tricks for Data Science you Need to Try Today|Innoplexus Sentiment Analysis Hackathon: Top 3 Out-of-the-Box Winning Approaches|
Pulkit Sharma
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Here is a LIVE CODING window for you to play around with the code and see the results for yourself  without leaving this article! Go ahead and start working on it:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love working on recommendation engines. Whenever I come across any recommendation engine on a website, I cant wait to break it down and understand how it works underneath. Its one of the many great things about being a data scientist!What truly fascinates me about these systems is how we can group similar items, products, and users together. This grouping, or segmenting, works across industries. And thats what makes the concept of clustering such an important one in data science.Clustering helps us understand our data in a unique way  by grouping things together into  you guessed it  clusters.In this article, we will cover k-means clustering and its components comprehensively. Well look at clustering, why it matters, its applications and then deep dive into k-means clustering (including how to perform it in Python on a real-world dataset).And if you want to directly work on the Python code, jump straight here. We have a live coding window where you can build your own k-means clustering algorithm without leaving this article!Learn more about clustering and other machine learning algorithms (both supervised and unsupervised) in the comprehensive Applied Machine Learning course.Lets kick things off with a simple example. A bank wants to give credit card offers to its customers. Currently, they look at the details of each customer and based on this information, decide which offer should be given to which customer.Now, the bank can potentially have millions of customers. Does it make sense to look at the details of each customer separately and then make a decision? Certainly not! It is a manual process and will take a huge amount of time.So what can the bank do? One option is to segment its customers into different groups. For instance, the bank can group the customers based on their income:
Can you see where Im going with this? The bank can now make three different strategies or offers, one for each group. Here, instead of creating different strategies for individual customers, they only have to make 3 strategies. This will reduce the effort as well as the time.The groups I have shown above are known as clusters and the process of creating these groups is known as clustering. Formally, we can say that:Clustering is the process of dividing the entire data into groups (also known as clusters) based on the patterns in the data.Can you guess which type of learning problem clustering is? Is it a supervised or unsupervised learning problem?Think about it for a moment and make use of the example we just saw. Got it? Clustering is an unsupervised learning problem!Lets say you are working on a project where you need to predict the sales of a big mart:Or, a project where your task is to predict whether a loan will be approved or not:
We have a fixed target to predict in both of these situations. In the sales prediction problem, we have to predict the Item_Outlet_Sales based on outlet_size, outlet_location_type, etc. and in the loan approval problem, we have to predict the Loan_Status depending on the Gender, marital status, the income of the customers, etc.So, when we have a target variable to predict based on a given set of predictors or independent variables, such problems are called supervised learning problems.Now, there might be situations where we do not have any target variable to predict.Such problems, without any fixed target variable, are known as unsupervised learning problems. In these problems, we only have the independent variables and no target/dependent variable.In clustering, we do not have a target to predict. We look at the data and then try to club similar observations and form different groups. Hence it is an unsupervised learning problem.We now know what are clusters and the concept of clustering. Next, lets look at the properties of these clusters which we must consider while forming the clusters.How about another example? Well take the same bank as before who wants to segment its customers. For simplicity purposes, lets say the bank only wants to use the income and debt to make the segmentation. They collected the customer data and used a scatter plot to visualize it:
On the X-axis, we have the income of the customer and the y-axis represents the amount of debt. Here, we can clearly visualize that these customers can be segmented into 4 different clusters as shown below:
This is how clustering helps to create segments (clusters) from the data. The bank can further use these clusters to make strategies and offer discounts to its customers. So lets look at the properties of these clusters.All the data points in a cluster should be similar to each other.Let me illustrate it using the above example:If the customers in a particular cluster are not similar to each other, then their requirements might vary, right? If the bank gives them the same offer, they might not like it and their interest in the bank might reduce. Not ideal.Having similar data points within the same cluster helps the bank to use targeted marketing. You can think of similar examples from your everyday life and think about how clustering will (or already does) impact the business strategy.The data points from different clusters should be as different as possible. This will intuitively make sense if you grasped the above property. Lets again take the same example to understand this property:Which of these cases do you think will give us the better clusters? If you look at case I:Customers in the red and blue clusters are quite similar to each other. The top four points in the red cluster share similar properties as that of the top two customers in the blue cluster. They have high income and high debt value. Here, we have clustered them differently. Whereas, if you look at case II:Points in the red cluster are completely different from the customers in the blue cluster. All the customers in the red cluster have high income and high debt and customers in the blue cluster have high income and low debt value. Clearly we have a better clustering of customers in this case.Hence, data points from different clusters should be as different from each other as possible to have more meaningful clusters.So far, we have understood what clustering is and the different properties of clusters. But why do we even need clustering? Lets clear this doubt in the next section and look at some applications of clustering.Clustering is a widely used technique in the industry. It is actually being used in almost every domain, ranging from banking to recommendation engines, document clustering to image segmentation.We covered this earlier  one of the most common applications of clustering is customer segmentation. And it isnt just limited to banking. This strategy is across functions, including telecom, e-commerce, sports, advertising, sales, etc.This is another common application of clustering. Lets say you have multiple documents and you need to cluster similar documents together. Clustering helps us group these documents such that similar documents are in the same clusters.We can also use clustering to perform image segmentation. Here, we try to club similar pixels in the image together. We can apply clustering to create clusters having similar pixels in the same group.You can refer to this article to see how we can make use of clustering for image segmentation tasks.Clustering can also be used in recommendation engines. Lets say you want to recommend songs to your friends. You can look at the songs liked by that person and then use clustering to find similar songs and finally recommend the most similar songs.There are many more applications which Im sure you have already thought of. You can share these applications in the comments section below. Next, lets look at how we can evaluate our clusters.The primary aim of clustering is not just to make clusters, but to make good and meaningful ones. We saw this in the below example:Here, we used only two features and hence it was easy for us to visualize and decide which of these clusters is better.Unfortunately, thats not how real-world scenarios work. We will have a ton of features to work with. Lets take the customer segmentation example again  we will have features like customers income, occupation, gender, age, and many more. Visualizing all these features together and deciding better and meaningful clusters would not be possible for us.This is where we can make use of evaluation metrics. Lets discuss a few of them and understand how we can use them to evaluate the quality of our clusters.Recall the first property of clusters we covered above. This is what inertia evaluates. It tells us how far the points within a cluster are. So, inertia actually calculates the sum of all the points within a cluster from the centroid of that cluster.We calculate this for all the clusters and the final inertial value is the sum of all these distances. This distance within the clusters is known as intracluster distance. So, inertia gives us the sum of intracluster distances:Now, what do you think should be the value of inertia for a good cluster? Is a small inertial value good or do we need a larger value? We want the points within the same cluster to be similar to each other, right? Hence, the distance between them should be as low as possible.Keeping this in mind, we can say that the lesser the inertia value, the better our clusters are.We now know that inertia tries to minimize the intracluster distance. It is trying to make more compact clusters.Let me put it this way  if the distance between the centroid of a cluster and the points in that cluster is small, it means that the points are closer to each other. So, inertia makes sure that the first property of clusters is satisfied. But it does not care about the second property  that different clusters should be as different from each other as possible.This is where Dunn index can come into action.
Along with the distance between the centroid and points, the Dunn index also takes into account the distance between two clusters. This distance between the centroids of two different clusters is known as inter-cluster distance. Lets look at the formula of the Dunn index:Dunn index is the ratio of the minimum of inter-cluster distances and maximum of intracluster distances.We want to maximize the Dunn index. The more the value of the Dunn index, the better will be the clusters. Lets understand the intuition behind Dunn index:
In order to maximize the value of the Dunn index, the numerator should be maximum. Here, we are taking the minimum of the inter-cluster distances. So, the distance between even the closest clusters should be more which will eventually make sure that the clusters are far away from each other.Also, the denominator should be minimum to maximize the Dunn index. Here, we are taking the maximum of intracluster distances. Again, the intuition is the same here. The maximum distance between the cluster centroids and the points should be minimum which will eventually make sure that the clusters are compact.We have finally arrived at the meat of this article!Recall the first property of clusters  it states that the points within a cluster should be similar to each other. So, our aim here is to minimize the distance between the points within a cluster.There is an algorithm that tries to minimize the distance of the points in a cluster with their centroid  the k-means clustering technique.K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid.The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.Lets now take an example to understand how K-Means actually works:
We have these 8 points and we want to apply k-means to create clusters for these points. Heres how we can do it.The first step in k-means is to pick the number of clusters, k.Next, we randomly select the centroid for each cluster. Lets say we want to have 2 clusters, so k is equal to 2 here. We then randomly select the centroid:Here, the red and green circles represent the centroid for these clusters.Once we have initialized the centroids, we assign each point to the closest cluster centroid:Here you can see that the points which are closer to the red point are assigned to the red cluster whereas the points which are closer to the green point are assigned to the green cluster.Now, once we have assigned all of the points to either cluster, the next step is to compute the centroids of newly formed clusters:Here, the red and green crosses are the new centroids.We then repeat steps 3 and 4:The step of computing the centroid and assigning all the points to the cluster based on their distance from the centroid is a single iteration. But wait  when should we stop this process? It cant run till eternity, right?There are essentially three stopping criteria that can be adopted to stop the K-means algorithm:We can stop the algorithm if the centroids of newly formed clusters are not changing. Even after multiple iterations, if we are getting the same centroids for all the clusters, we can say that the algorithm is not learning any new pattern and it is a sign to stop the training.Another clear sign that we should stop the training process if the points remain in the same cluster even after training the algorithm for multiple iterations.Finally, we can stop the training if the maximum number of iterations is reached. Suppose if we have set the number of iterations as 100. The process will repeat for 100 iterations before stopping.Time to fire up our Jupyter notebooks (or whichever IDE you use) and get our hands dirty in Python!We will be working on the big mart sales dataset that you can download here. I encourage you to read more about the dataset and the problem statement here. This will help you visualize what we are working on (and why we are doing this). Two pretty important questions in any data science project.First, import all the required libraries:Now, we will read the CSV file and look at the first five rows of the data:For this article, we will be taking only two variables from the data  LoanAmount and ApplicantIncome. This will make it easy to visualize the steps as well. Lets pick these two variables and visualize the data points:
Steps 1 and 2 of K-Means were about choosing the number of clusters (k) and selecting random centroids for each cluster. We will pick 3 clusters and then select random observations from the data as the centroids:Here, the red dots represent the 3 centroids for each cluster. Note that we have chosen these points randomly and hence every time you run this code, you might get different centroids.Next, we will define some conditions to implement the K-Means Clustering algorithm. Lets first look at the code:These values might vary every time we run this. Here, we are stopping the training when the centroids are not changing after two iterations. We have initially defined the diff as 1 and inside the while loop, we are calculating this diff as the difference between the centroids in the previous iteration and the current iteration.When this difference is 0, we are stopping the training. Lets now visualize the clusters we have got:Awesome! Here, we can clearly visualize three clusters. The red dots represent the centroid of each cluster. I hope you now have a clear understanding of how K-Means work.However, there are certain situations where this algorithm might not perform as well. Lets look at some challenges which you can face while working with k-means.One of the common challenges we face while working with K-Means is that the size of clusters is different. Lets say we have the below points:
The left and the rightmost clusters are of smaller size compared to the central cluster. Now, if we apply k-means clustering on these points, the results will be something like this:
Another challenge with k-means is when the densities of the original points are different. Lets say these are the original points:
Here, the points in the red cluster are spread out whereas the points in the remaining clusters are closely packed together. Now, if we apply k-means on these points, we will get clusters like this:
We can see that the compact points have been assigned to a single cluster. Whereas the points that are spread loosely but were in the same cluster, have been assigned to different clusters. Not ideal so what can we do about this?One of the solutions is to use a higher number of clusters. So, in all the above scenarios, instead of using 3 clusters, we can have a bigger number. Perhaps setting k=10 might lead to more meaningful clusters.Remember how we randomly initialize the centroids in k-means clustering? Well, this is also potentially problematic because we might get different clusters every time. So, to solve this problem of random initialization, there is an algorithm called K-Means++ that can be used to choose the initial values, or the initial cluster centroids, for K-Means.In some cases, if the initialization of clusters is not appropriate, K-Means can result in arbitrarily bad clusters. This is where K-Means++ helps. It specifies a procedure to initialize the cluster centers before moving forward with the standard k-means clustering algorithm.Using the K-Means++ algorithm, we optimize the step where we randomly pick the cluster centroid. We are more likely to find a solution that is competitive to the optimal K-Means solution while using the K-Means++ initialization.The steps to initialize the centroids using K-Means++ are:Lets take an example to understand this more clearly. Lets say we have the following points and we want to make 3 clusters here:
Now, the first step is to randomly pick a data point as a cluster centroid:Lets say we pick the green point as the initial centroid. Now, we will calculate the distance (D(x)) of each data point with this centroid:
The next centroid will be the one whose squared distance (D(x)2) is the farthest from the current centroid:
In this case, the red point will be selected as the next centroid. Now, to select the last centroid, we will take the distance of each point from its closest centroid and the point having the largest squared distance will be selected as the next centroid:
We will select the last centroid as:
We can continue with the K-Means algorithm after initializing the centroids. Using K-Means++ to initialize the centroids tends to improve the clusters. Although it is computationally costly relative to random initialization, subsequent K-Means often converge more rapidly.Im sure theres one question which youve been wondering about since the start of this article  how many clusters should we make? Aka, what should be the optimum number of clusters to have while performing K-Means?One of the most common doubts everyone has while working with K-Means is selecting the right number of clusters.So, lets look at a technique that will help us choose the right value of clusters for the K-Means algorithm. Lets take the customer segmentation example which we saw earlier. To recap, the bank wants to segment its customers based on their income and amount of debt:
Here, we can have two clusters which will separate the customers as shown below:
All the customers with low income are in one cluster whereas the customers with high income are in the second cluster. We can also have 4 clusters:
Here, one cluster might represent customers who have low income and low debt, other cluster is where customers have high income and high debt, and so on. There can be 8 clusters as well:
Honestly, we can have any number of clusters. Can you guess what would be the maximum number of possible clusters? One thing which we can do is to assign each point to a separate cluster. Hence, in this case, the number of clusters will be equal to the number of points or observations. So,The maximum possible number of clusters will be equal to the number of observations in the dataset.But then how can we decide the optimum number of clusters? One thing we can do is plot a graph, also known as an elbow curve, where the x-axis will represent the number of clusters and the y-axis will be an evaluation metric. Lets say inertia for now.You can choose any other evaluation metric like the Dunn index as well:Next, we will start with a small cluster value, lets say 2. Train the model using 2 clusters, calculate the inertia for that model, and finally plot it in the above graph. Lets say we got an inertia value of around 1000:
Now, we will increase the number of clusters, train the model again, and plot the inertia value. This is the plot we get:
When we changed the cluster value from 2 to 4, the inertia value reduced very sharply. This decrease in the inertia value reduces and eventually becomes constant as we increase the number of clusters further.So,the cluster value where this decrease in inertia value becomes constant can be chosen as the right cluster value for our data.Here, we can choose any number of clusters between 6 and 10. We can have 7, 8, or even 9 clusters. You must also look at the computation cost while deciding the number of clusters. If we increase the number of clusters, the computation cost will also increase. So, if you do not have high computational resources, my advice is to choose a lesser number of clusters.Lets now implement the K-Means Clustering algorithm in Python. We will also see how to use K-Means++ to initialize the centroids and will also plot this elbow curve to decide what should be the right number of clusters for our dataset.We will be working on a wholesale customer segmentation problem. You can download the dataset using this link. The data is hosted on the UCI Machine Learning repository.The aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc. So, lets start coding!We will first import the required libraries:Next, lets read the data and look at the first five rows:
We have the spending details of customers on different products like Milk, Grocery, Frozen, Detergents, etc. Now, we have to segment the customers based on the provided details. Before doing that, lets pull out some statistics related to the data:
Here, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude whereas variables like Fresh, Milk, Grocery, etc. have a higher magnitude.Since K-Means is a distance-based algorithm, this difference of magnitude can create a problem. So lets first bring all the variables to the same magnitude:
The magnitude looks similar now. Next, lets create a kmeans function and fit it on the data:We have initialized two clusters and pay attention  the initialization is not random here. We have used the k-means++ initialization which generally produces better results as we have discussed in the previous section as well.Lets evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:Output: 2599.38555935614We got an inertia value of almost 2600. Now, lets see how we can use the elbow curve to determine the optimum number of clusters in Python.We will first fit multiple k-means models and in each successive model, we will increase the number of clusters. We will store the inertia value of each model and then plot it to visualize the result:
Can you tell the optimum cluster value from this plot? Looking at the above elbow curve, we can choose any number of clusters between 5 to 8. Lets set the number of clusters as 6 and fit the model:Finally, lets look at the value count of points in each of the above-formed clusters:
So, there are 234 data points belonging to cluster 4 (index 3), then 125 points in cluster 2 (index 1), and so on. This is how we can implement K-Means Clustering in Python.In this article, we discussed one of the most famous clustering algorithms  K-Means. We implemented it from scratch and looked at its step-by-step implementation. We looked at the challenges which we might face while working with K-Means and also saw how K-Means++ can be helpful when initializing the cluster centroids.Finally, we implemented k-means and looked at the elbow curve which helps to find the optimum number of clusters in the K-Means algorithm.If you have any doubts or feedback, feel free to share them in the comments section below. And make sure you check out the comprehensive Applied Machine Learning course that takes you from the basics of machine learning to advanced algorithms (including an entire module on deploying your machine learning models!).",https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/
Innoplexus Sentiment Analysis Hackathon: Top 3 Out-of-the-Box Winning Approaches,Learn everything about Analytics|Overview|Introduction|About the Innoplexus Sentiment Analysis Hackathon|Problem Statement for the Innoplexus Sentiment Analysis Hackathon|Winners of the Innoplexus Sentiment Analysis Hackathon|End Notes,"Rank 3: Mohsin Hasan Khan (ML Engineer @HealthifyMe)|Rank 2: Harini Vengala (Statistical Analyst @WalmartLabs India)|Rank 1: Melwin Babu (Data Scientist @nference)|Share this:|Related Articles|The Most Comprehensive Guide to K-Means Clustering Youll Ever Need|A Detailed Guide to 7 Loss Functions for Machine Learning Algorithms with Python Code|
Ankit Choudhary
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Approach|Approach|Approach,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a big fan of hackathons. Ive learned so much about data science from participating in these hackathons in the past few years. Ill admit it  I have gained a lot of knowledge through this medium and this, in turn, has accelerated my professional career.This comes with a caveat  winning a data science hackathon is really hard. Just think about the number of obstacles in your way:A single decimal point could be the difference between the top 10 and the top 50. Isnt this why we love hackathons in the first place? The thrill of seeing our hard work pay off with a rise in the leaderboard rankings is unparalleled.So, were thrilled to bring to you the top 3 winning approaches from the Innoplexus Sentiment Analysis hackathon! You are going to be awestruck by how these three top data scientists thought through their solutions and came up with their own unique framework.There is a LOT to learn from these approaches. Trust me, take the time to go through the steps and understand where they came from. And then think if you would have done anything differently. And then  go ahead and take part in these hackathons yourself on our DataHack platform!So lets begin, shall we?Its always an exciting prospect, hosting hackathons with our partner Innoplexus. Each time they come up with problem statements that are based on Natural Language Processing (NLP), an immensely popular field right now. We have seen huge developments in NLP thanks to transfer learning models such as BERT, XLNet, GPT-2, etc.And sentiment analysis is one of the most common NLP projects data scientists tend to work on. This Innolpexus hackathon was a 5-day contest with more than 3200 data scientists across the globe competing for job opportunities and exciting prizes offered by Innoplexus.It was a hard-fought contest with a total of 8000+ submissions and a variety of approaches employed by the best in the business to occupy the top spots.For those of you who could not make it to the top, or otherwise could not find time to work on the problem, we have collated the winners approach and solutions to help you appreciate and learn from these. So here goes.There are a lot of components that go into building the narrative of a brand. It isnt just built and controlled by the company that owns the brand. Think about any big brand you are familiar with and youll instantly understand what Im talking about.For this reason, companies are constantly looking out across various platforms, such as blogs, forums, social media, etc. for checking the sentiment around their various products and also competitor products to learn how their brand resonates in the market. This analysis helps them in various aspects of their post-launch market research.This is relevant for a lot of industries, including pharma and their drugs.But this comes with several challenges. Primarily, the language used in this type of content is not strictly grammatically correct. We often come across people using sarcasm. Others cover several topics with different sentiments in one post. Other users post comments to indicate their sentiment around the topic.Broadly speaking, sentiment can be clubbed into 3 major buckets  Positive, Negative and Neutral Sentiments.In the Innoplexus Sentiment Analysis Hackathon, the participants were provided with data containing samples of text. This text could potentially contain one or more drug mentions. Each row contained a unique combination of the text and the drug mention. Note that the same text could also have different sentiments for a different drug.Given the text and drug name, the task was to predict the sentiment for texts contained in the test dataset. Given below is an example of text from the dataset:Example:Stelara is still fairly new to Crohns treatment. This is why you might not get a lot of replies. Ive done some research, but most of the time to work answers are from Psoriasis boards. For Psoriasis, it seems to be about 4-12 weeks to reach a strong therapeutic level. The good news is, Stelara seems to be getting rave reviews from Crohns patients. It seems to be the best med to come along since Remicade. I hope you have good success with it. My daughter was diagnosed Feb. 19/07, (13 yrs. old at the time of diagnosis), with Crohns of the Terminal Illium. Has used Prednisone and Pentasa. Started Imuran (02/09), had an abdominal abscess (12/08). 2cm of Stricture. Started Remicade in Feb. 2014, along with 100mgs. of Imuran.The above text is positive for Stelara and negative for Remicade. Now that we have a solid understanding of what the problem at hand was, lets dive into the winning approaches!As I mentioned earlier, winning a hackathon is extremely difficult. I loved going through these top solutions and approaches provided by our winners. First, lets look at who won and congratulate them:Here are the final rankings of all the participants on the Leaderboard.The top 3 winners have shared their detailed approach from the competition. I am sure you are eager to know their secrets so lets begin.Heres what Mohsin shared with us:My final solution is an ensemble of BERT and XLNet runs.Heres what Harini shared with us:My final model was an ensemble of 3 BERT and 1 AEN.Heres what Melwin shared with us:I noticed pretty early that increasing the max sequence length increased the score sufficiently. This observation more or less dictated my approach. I used a basic XLNet model with hardly any feature engineering.It was great fun interacting with these winners and getting to know their approach during the competition. This is a tightly contest hackathon and as you have already seen, the winning approaches were supremely awesome.I encourage you to head over to the DataHack platform TODAY and participate in the ongoing and upcoming hackathons. It will be an invaluable learning experience!",https://www.analyticsvidhya.com/blog/2019/08/innoplexus-sentiment-analysis-hackathon-top-3-out-of-the-box-winning-approaches/
A Detailed Guide to 7 Loss Functions for Machine Learning Algorithms with Python Code,Learn everything about Analytics|Overview|Introduction|Table of Contents|What are Loss Functions?|Regression Loss Functions|Binary Classification Loss Functions|Multi-Class Classification Loss Functions|End Notes,"Whats the Difference between a Loss Function and a Cost Function?|1. Squared Error Loss|2. Absolute Error Loss|3. Huber Loss|1. Binary Cross Entropy Loss|2. Hinge Loss|1. Multi-Class Cross Entropy Loss|2. KL-Divergence|Share this:|Related Articles|Innoplexus Sentiment Analysis Hackathon: Top 3 Out-of-the-Box Winning Approaches|Everything You Ever Wanted to Know About Setting up Python on Windows, Linux and Mac|
Khyati Mahendru
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picture this  youve trained a machine learning model on a given dataset and are ready to put it in front of your client. But how can you be sure that this model will give the optimum result? Is there a metric or a technique that will help you quickly evaluate your model on the dataset?Yes  and that, in a nutshell, is where loss functions come into play in machine learning.Loss functions are at the heart of the machine learning algorithms we love to use. But Ive seen the majority of beginners and enthusiasts become quite confused regarding how and where to use them.Theyre not difficult to understand and will enhance your understand of machine learning algorithms infinitely. So, what are loss functions and how can you grasp their meaning?In this article, I will discuss 7 common loss functions used in machine learning and explain where each of them is used.We have a lot to cover in this article so lets begin!Loss functions are one part of the entire machine learning journey you will take. Heres the perfect course to help you get started and make you industry-ready:Lets say you are on the top of a hill and need to climb down. How do you decide where to walk towards?Heres what I would do:This intuition that I just judged my decisions against? This is exactly what a loss function provides.A loss function maps decisions to their associated costs.Deciding to go up the slope will cost us energy and time. Deciding to go down will benefit us. Therefore, it has a negative cost.In supervised machine learning algorithms, we want to minimize the error for each training example during the learning process. This is done using some optimization strategies like gradient descent. And this error comes from the loss function.I want to emphasize this here  although cost function and loss function are synonymous and used interchangeably, they are different.A loss function is for a single training example. It is also sometimes called an error function. A cost function, on the other hand, is the average loss over the entire training dataset. The optimization strategies aim at minimizing the cost function.You must be quite familiar with linear regression at this point. It deals with modeling a linear relationship between a dependent variable, Y, and several independent variables, X_is. Thus, we essentially fit a line in space on these variables.We will use the given data points to find the coefficients a0, a1, , an.Source: WikipediaWe will use the famous Boston Housing Dataset for understanding this concept. And to keep things simple, we will use only one feature  the Average number of rooms per dwelling (X)  to predict the dependent variable  Median Value (Y) of houses in $1000 s.We will use Gradient Descent as an optimization strategy to find the regression line. I will not go into the intricate details about Gradient Descent, but here is a reminder of the Weight Update Rule:Source: Hackernoon.comHere, theta_j is the weight to be updated, alpha is the learning rate and J is the cost function. The cost function is parameterized by theta. Our aim is to find the value of theta which yields minimum overall cost.You can get an in-depth explanation of Gradient Descent and how it works here.I have defined the steps that we will follow for each loss function below:Squared Error loss for each training example, also known as L2 Loss, is the square of the difference between the actual and the predicted values:The corresponding cost function is the Mean of these Squared Errors (MSE).I encourage you to try and find the gradient for gradient descent yourself before referring to the code below.I used this code on the Boston data for different values of the learning rate for 500 iterations each:Heres a task for you. Try running the code for a learning rate of 0.1 again for 500 iterations. Let me know your observations and any possible explanations in the comments section.Lets talk a bit more about the MSE loss function. It is a positive quadratic function (of the form ax^2 + bx + c where a > 0). Remember how it looks graphically?A quadratic function only has a global minimum. Since there are no local minima, we will never get stuck in one. Hence, it is always guaranteed that Gradient Descent will converge (if it converges at all) to the global minimum.The MSE loss function penalizes the model for making large errors by squaring them. Squaring a large quantity makes it even larger, right? But theres a caveat. This property makes the MSE cost function less robust to outliers. Therefore, it should not be used if our data is prone to many outliers.Absolute Error for each training example is the distance between the predicted and the actual values, irrespective of the sign. Absolute Error is also known as the L1 loss:As I mentioned before, the cost is the Mean of these Absolute Errors (MAE).The MAE cost is more robust to outliers as compared to MSE. However, handling the absolute or modulus operator in mathematical equations is not easy. Im sure a lot of you must agree with this! We can consider this as a disadvantage of MAE.Here is the code for the update_weight function with MAE cost:We get the below plot after running the code for 500 iterations with different learning rates:The Huber loss combines the best properties of MSE and MAE. It is quadratic for smaller errors and is linear otherwise (and similarly for its gradient). It is identified by its delta parameter:We obtain the below plot for 500 iterations of weight update at a learning rate of 0.0001 for different values of the delta parameter:Huber loss is more robust to outliers than MSE. It is used in Robust Regression, M-estimation and Additive Modelling. A variant of Huber Loss is also used in classification.The name is pretty self-explanatory. Binary Classification refers to assigning an object into one of two classes. This classification is based on a rule applied to the input feature vector. For example, classifying an email as spam or not spam based on, say its subject line, is binary classification.I will illustrate these binary classification loss functions on the Breast Cancer dataset.We want to classify a tumor as Malignant or Benign based on features like average radius, area, perimeter, etc. For simplification, we will use only two input features (X_1 and X_2) namely worst area and mean symmetry for classification. The target value Y can be 0 (Malignant) or 1 (Benign).Here is a scatter plot for our data:
Let us start by understanding the term entropy.Generally, we use entropy to indicate disorder or uncertainty. It is measured for a random variable X with probability distribution p(X):The negative sign is used to make the overall quantity positive.A greater value of entropy for a probability distribution indicates a greater uncertainty in the distribution. Likewise, a smaller value indicates a more certain distribution.This makes binary cross-entropy suitable as a loss function  you want to minimize its value. We use binary cross-entropy loss for classification models which output a probability p.Then, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as:This is also called Log-Loss. To calculate the probability p, we can use the sigmoid function. Here, z is a function of our input features:The range of the sigmoid function is [0, 1] which makes it suitable for calculating probability.Try to find the gradient yourself and then look at the code for the update_weight function below.I got the below plot on using the weight update rule for 1000 iterations with different values of alpha:Hinge loss is primarily used with Support Vector Machine (SVM) Classifiers with class labels -1 and 1. So make sure you change the label of the Malignant class in the dataset from 0 to -1.Hinge Loss not only penalizes the wrong predictions but also the right predictions that are not confident.Hinge loss for an input-output pair (x, y) is given as:After running the update function for 2000 iterations with three different values of alpha, we obtain this plot:Hinge Loss simplifies the mathematics for SVM while maximizing the loss (as compared to Log-Loss). It is used when we want to make real-time decisions with not a laser-sharp focus on accuracy.Emails are not just classified as spam or not spam (this isnt the 90s anymore!). They are classified into various other categories  Work, Home, Social, Promotions, etc. This is a Multi-Class Classification use case.Well use the Iris Dataset for understanding the remaining two loss functions. We will use 2 features X_1, Sepal length and feature X_2, Petal width, to predict the class (Y) of the Iris flower  Setosa, Versicolor or VirginicaOur task is to implement the classifier using a neural network model and the in-built Adam optimizer in Keras. This is because as the number of parameters increases, the math, as well as the code, will become difficult to comprehend.If you are new to Neural Networks, I highly recommend reading this article first.Here is the scatter plot for our data:The multi-class cross-entropy loss is a generalization of the Binary Cross Entropy loss. The loss for input vector X_i and the corresponding one-hot encoded target vector Y_i is:We use the softmax function to find the probabilities p_ij:Source: WikipediaSoftmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer. Google Developers BlogFinally, our output is the class with the maximum probability for the given input.We build a model using an input layer and an output layer and compile it with different learning rates. Specify the loss parameter as categorical_crossentropy in the model.compile() statement:Here are the plots for cost and accuracy respectively after training for 200 epochs: The Kullback-Liebler Divergence is a measure of how a probability distribution differs from another distribution. A KL-divergence of zero indicates that the distributions are identical.Notice that the divergence function is not symmetric.This is why KL-Divergence cannot be used as a distance metric.I will describe the basic approach of using KL-Divergence as a loss function without getting into its math. We want to approximate the true probability distribution P of our target variables with respect to the input features, given some approximate distribution Q. Since KL-Divergence is not symmetric, we can do this in two ways:The first approach is used in Supervised learning, the second in Reinforcement Learning. KL-Divergence is functionally similar to multi-class cross-entropy and is also called relative entropy of P with respect to Q:We specify the kullback_leibler_divergence as the value of the loss parameter in the compile() function as we did before with the multi-class cross-entropy loss. KL-Divergence is used more commonly to approximate complex functions than in multi-class classification. We come across KL-Divergence frequently while playing with deep-generative models like Variational Autoencoders (VAEs).Woah! We have covered a lot of ground here. Give yourself a pat on your back for making it all the way to the end. This was quite a comprehensive list of loss functions we typically use in machine learning.I would suggest going through this article a couple of times more as you proceed with your machine learning journey. This isnt a one-time effort. It will take a few readings and experience to understand how and where these loss functions work.Make sure to experiment with these loss functions and let me know your observations down in the comments. Also, let me know other topics that you would like to read about. I will do my best to cover them in future articles.Meanwhile, make sure you check out our comprehensive beginner-level machine learning course:",https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/
"Everything You Ever Wanted to Know About Setting up Python on Windows, Linux and Mac",Learn everything about Analytics|Overview|Introduction|Table of Contents|Important Tools for Data Science|Steps to Install Python on Linux|Steps to Install Python on macOS|Steps to Install Python on Windows|End Notes,"What are Anaconda and Miniconda?|Share this:|Related Articles|A Detailed Guide to 7 Loss Functions for Machine Learning Algorithms with Python Code|A Comprehensive Guide to Build your own Language Model in Python!|
Mohd Sanad Zaki Rizvi
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you struggling to install Python on your machine? Its actually a pretty common issue Ive seen among beginners in data science. Installation might seem simple in theory, but things can get a bit tricky in reality.I have personally faced various obstacles when trying to set up Python on my Linux and Windows machines. The installation seems to be running smoothly before boom! An issue about compatibility. Or another issue about a certain dependency being missing.If youve ever faced these kinds of niggling issues when attempting to install Python on your machine  this article is for you. I had to visit several forums and websites to figure out where I was going wrong. Its not a great experience. So I decided to collate everything and put it together in one place for you.I have provided a step-by-step breakdown of how you can setup Python through Anaconda on all the three platforms:And if youre new to data science and machine learning and looking to understand how to use Python, make sure you check out our beginner-friendly courses:A Data Scientists toolbox can really surprise you as there can be multiple tools that are needed for different aspects of the job. There are, however, some tools that are more important (or widely used) than others. Here are a few must-have tools that every Data Scientist, beginner or experienced, needs:And the best part is that all of these tools come by default with Miniconda/Anaconda!Anaconda is a very important software to have when you are learning data science. It lets us install almost all the libraries/tools that we would need in our data science journey with Python. It has a very simple interface that lets us accomplish most data science tasks in just a few lines of code.Minoconda is a smaller version of Anaconda. Its a lightweight version and a good choice if you do not have enough disk space on your computer.Lets see how we can set up both Anaconda and Miniconda on our own machines!Linux is a widely loved platform among the data science community. It offers immense flexibility in terms of the data science tasks that we perform. But theres a slight caveat here  it can be quite tricky to install software on Linux! This is especially true if youre a Linux beginner.Here are the steps to set up Python and popular data science tools on Linux.Step 1: Get MinicondaYou can download Miniconda from this link:You can choose the Linuxversion of the installer and the suggested Python version should be any version greater than Python 3.5.Step 2: Install MinicondaNow that you have downloaded the Miniconda file, the next step is to install it in your system. For that, first go to the directory in which the file is downloaded:Then, in order to start the installation script, use the bash command with the Miniconda file name:If asked for confirmation, just press Enter to continue.Once you see the license terms, keep pressing enter until it asks to accept the terms. Then type yes to accept the terms. It will then ask you to select the installation location:You can give a separate location or just press enter to select the default one. I usually prefer the default option unless I have space issues on my main drive and then I give the alternate installation location.After this, the process is fairly straightforward as you just need to say yes and press Enter for everything. Keep in mind that the installation might take some time so feel free to grab a coffee while your machine works hard to install everything!Once you have finished the previous steps, you will be asked to open another terminal to activate the Miniconda installation. Open a new terminal window, and then we can proceed with the next steps.Step 3: Create a new environmentAn environment is basically your workspace. You can set it up as you want. How cool is that?You can choose what version of which Python library should be in your environment and this will basically help you have more control over your data science workflow.Now, the benefit of an environment in Miniconda is that it lets you create multiple such environments. You can manage multiple independent environments, each for a separate task!Let me explain this using an example. Lets say we are working with a State-of-the-Art framework (like PyTorch-Transformers for Natural Language Processing) and we need all the latest versions of the dependent libraries. This is where environments come in handy.We can have that new setup co-exist along with a simpler setup where we have an old legacy project and we are forced to use certain versions of libraries that are needed for our project.You can create an environment using the following command:av is the name of the environment (you can give it any name you want). And python=3 is the version of Python we want to use.To check if the environment has been successfully created, type the following command:And this will give us a list of environments currently installed in our system.Step 4: Activate the new environmentNow, to start working with the new environment that you created, type the following command:To make sure that things are working fine in the active environment, we can see the list of libraries installed in this environment using the following command:The above command should give you an output like this:Once you are done working with an environment and you want to deactivate it, you can do that using:So now that the setup is fully complete, its time to check that everything works out as expected. Lets go to the next step!Step 5: Start a Jupyter NotebookTime to fire up our Jupyter notebook:This will start the Jupyter Notebook in a browser:Next, you can simply click on New and select Python 3 to start a Python 3 notebook to work with:Pretty straightforward, wasnt it?Congratulations! You have now successfully installed Anaconda on your system. And since Anaconda by default comes with Python and all the data science libraries like Pandas, Numpy, Scikit-Learn, etc., you now have all of those in your system too!In case you still have doubts or are stuck at any step, here is a video of the entire installation process:The installation steps for macOS are very similar to the Linux installation steps. Both of them have the same bash terminal. The only difference is the Miniconda installer file that you need to download.You can download Miniconda for macOS from this link:This time you have to choose the macOS bash installer and the suggested Python version should be any version greater than Python 3.5.Once you have downloaded the above file, you just have to follow steps 2 to 5 from the Linux installation steps and that would be enough to get you started.Watch the below video to get a full run-through of installing Python on macOS:Lets have a look at the steps to install Python and other data science libraries on Windows.Step 1: Get AnacondaYou can download Anaconda from this link:You can choose the Windows version of the installer and the suggested Python version should be any version greater than Python 3.5.Now you will see two choices  32 Bit and64 Bitinstaller. Choose the one which is compatible with your system (right-click on My Computer to view this if youre unsure).Step 2: Installing AnacondaOnce you have downloaded the installation file, go to the Downloads folder and double-click on the file. A new window for installation will open:Then click Next and this will take you to the license agreement. Click I Agree to accept it:It will then ask you whether you want to install this software only for this user or all the users of the system. This is totally your choice to make. I usually go with the recommended option:Now you can choose the location where you want this software to be installed:Now, in the next window, you will get a couple of advanced options. You can untick both of them for now and click Install. This step might take some time:Once the installation is complete, you can click Next:You can skip the installation for Microsoft Visual Code:And click finish:And voila! Python is all ready for you to begin analyzing data and building machine learning models.Step 3: Start Jupyter NotebookTo see that everything has been installed correctly, we will open a Jupyter Notebook. To do that, first go the start menu and search for Jupyter:Click on the Jupyter Notebook option and this will open the notebook in a browser:Now you can simply click on New and select Python 3 to start a Python 3 notebook to work with:Heres a video detailing how you can install Python on Windows in case you prefer learning through a visual format:So that was all about installing Python on all the popular platforms out there. My aim here was to acquaint you with the installation process and to clear any doubts you might have.If you still face any issues, let me know in the comments section below and Ill be happy to help you out! And make sure you check out the two courses below to start your journey into data science using Python:",https://www.analyticsvidhya.com/blog/2019/08/everything-know-about-setting-up-python-windows-linux-and-mac/
A Comprehensive Guide to Build your own Language Model in Python!,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is a Language Model in NLP?|Building an N-gram Language Model|Building a Neural Language Model|Natural Language Generation using OpenAIs GPT-2|End Notes,"Types of Language Models|What are N-grams (unigram, bigram, trigrams)?|How do N-gram Language Models work?|Building a Basic Language Model|Limitations of N-gram approach to Language Modeling|Understanding the problem statement|Import the libraries|Read the dataset|Preprocessing the Text Data|Creating Sequences|Encoding Sequences|Create Training and Validation set|Model Building|Inference|Results|About PyTorch-Transformers|Installing PyTorch-Transformers on your Machine|Sentence completion using GPT-2|Conditional Text Generation using GPT-2|Share this:|Related Articles|Everything You Ever Wanted to Know About Setting up Python on Windows, Linux and Mac|A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework|
Mohd Sanad Zaki Rizvi
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We tend to look through language and not realize how much power language has.Language is such a powerful medium of communication. We have the ability to build projects from scratch using the nuances of language. Its what drew me to Natural Language Processing (NLP) in the first place.Im amazed by the vast array of tasks I can perform with NLP  text summarization, generating completely new pieces of text, predicting what word comes next (Googles autofill), among others. Do you know what is common among all these NLP tasks?They are all powered by language models! Honestly, these language models are a crucial first step for most of the advanced NLP tasks.In this article, we will cover the length and breadth of language models. We will begin from basic language models that can be created with a few lines of Python code and move to the State-of-the-Art language models that are trained using humongous data and are being currently used by the likes of Google, Amazon, and Facebook, among others.So, tighten your seatbelts and brush up your linguistic skills  we are heading into the wonderful world of Natural Language Processing!Are you new to NLP? Confused about where to begin? You should check out this comprehensive course designed by experts with decades of industry experience:You shall know the nature of a word by the company it keeps.  John Rupert FirthA language model learns to predict the probability of a sequence of words. But why do we need to learn the probability of words? Lets understand that with an example.Im sure you have used Google Translate at some point. We all use it to translate one language to another for varying reasons. This is an example of a popular NLP application called Machine Translation.In Machine Translation, you take in a bunch of words from a language and convert these words into another language. Now, there can be many potential translations that a system might give you and you will want to compute the probability of each of these translations to understand which one is the most accurate.In the above example, we know that the probability of the first sentence will be more than the second, right? Thats how we arrive at the right translation.This ability to model the rules of a language as a probability gives great power for NLP related tasks. Language models are used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval, and many other daily tasks.There are primarily two types of Language Models:Now that you have a pretty good idea about Language Models, lets start building one!An N-gram is a sequence of N tokens (or words).Lets understand N-gram with an example. Consider the following sentence:I love reading blogs about data science on Analytics Vidhya.A 1-gram (or unigram) is a one-word sequence. For the above sentence, the unigrams would simply be: I, love, reading, blogs, about, data, science, on, Analytics, Vidhya.A 2-gram (or bigram) is a two-word sequence of words, like I love, love reading, or Analytics Vidhya. And a 3-gram (or trigram) is a three-word sequence of words like I love reading, about data science or on Analytics Vidhya.Fairly straightforward stuff!An N-gram language model predicts the probability of a given N-gram within any sequence of words in the language. If we have a good N-gram model, we can predict p(w | h)  what is the probability of seeing the word w given a history of previous words h  where the history contains n-1 words.We must estimate this probability to construct an N-gram model.We compute this probability in two steps:The chain rule of probability is:So what is the chain rule? It tells us how to compute the joint probability of a sequence by using the conditional probability of a word given previous words.But we do not have access to these conditional probabilities with complex conditions of up to n-1 words. So how do we proceed?This is where we introduce a simplification assumption. We can assume for all conditions, that:Here, we approximatethe history (the context) of the word wk by looking only at the last word of the context. This assumption is called the Markov assumption. (We used it here with a simplified context of length 1  which corresponds to a bigram model  we could use larger fixed-sized histories in general).Now that we understand what an N-gram is, lets build a basic language model using trigrams of the Reuters corpus. Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words. We can build a language model in a few lines of code using the NLTK package:The code above is pretty straightforward. We first split our text into trigrams with the help of NLTK and then calculate the frequency in which each combination of the trigrams occurs in the dataset.We then use it to calculate probabilities of a word, given the previous two words. Thats essentially what gives us our Language Model!Lets make simple predictions with this language model. We will start with two simple words  today the. We want our model to tell us what will be the next word:So we get predictions of all the possible words that can come next with their respective probabilities. Now, if we pick up the word price and again make a prediction for the words the and price:If we keep following this process iteratively, we will soon have a coherent sentence! Here is a script to play around with generating a random piece of text using our n-gram model:And here is some of the text generated by our model:Pretty impressive! Even though the sentences feel slightly off (maybe because the Reuters dataset is mostly news), they are very coherent given the fact that we just created a model in 17 lines of Python code and a really small dataset.This is the same underlying principle which the likes of Google, Alexa, and Apple use for language modeling.N-gram based language models do have a few drawbacks:Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences.  Dr. Christopher D. ManningDeep Learning has been shown to perform really well on many NLP tasks like Text Summarization, Machine Translation, etc. and since these tasks are essentially built upon Language Modeling, there has been a tremendous research effort with great results to use Neural Networks for Language Modeling.We can essentially build two kinds of language models  character level and word level. And even under each category, we can have many subcategories based on the simple fact of how we are framing the learning problem. We will be taking the most straightforward approach  building a character-level language model.Does the above text seem familiar? Its the US Declaration of Independence! The dataset we will use is the text from this Declaration.This is a historically important document because it was signed when the United States of America got independence from the British. I used this document as it covers a lot of different topics in a single space. Its also the right size to experiment with because we are training a character-level language model which is comparatively more intensive to run as compared to a word-level language model.The problem statement is to train a language model on the given text and then generate text given an input text in such a way that it looks straight out of this document and is grammatically correct and legible to read.You can download the dataset from here. Lets begin!You can directly read the dataset as a string in Python:We perform basic text preprocessing since this data does not have much noise. We lower case all the words to maintain uniformity and remove words with length less than 3:Once the preprocessing is complete, it is time to create training sequences for the model.The way this problem is modeled is we take in 30 characters as context and ask the model to predict the next character. Now, 30 is a number which I got by trial and error and you can experiment with it too. You essentially need enough characters in the input sequence that your model is able to get the context.Lets see how our training sequences look like:Once the sequences are generated, the next step is to encode each character. This would give us a sequence of numbers.So now, we have sequences like this:Once we are ready with our sequences, we split the data into training and validation splits. This is because while training, I want to keep a track of how good my language model is working with unseen data.Time to build our language model!I have used the embedding layer of Keras to learn a 50 dimension embedding for each character. This helps the model in understanding complex relationships between characters. I have also used a GRU layer as the base model, which has 150 timesteps. Finally, a Dense layer is used with a softmax activation for prediction.Once the model has finished training, we can generate text from the model given an input sequence using the below code:Lets put our model to the test. In the video below, I have given different inputs to the model. Lets see how it performs:Additionally, when we do not give space, it tries to predict a word that will have these as starting characters (like for can mean foreign).Also, note that almost none of the combinations predicted by the model exist in the original training data. So our model is actually building words based on its understanding of the rules of the English language and the vocabulary it has seen during training.We have so far trained our own models to generate text, be it predicting the next word or generating some text with starting words. But that is just scratching the surface of what language models are capable of!Leading research labs have trained much more complex language models on humongous datasets that have led to some of the biggest breakthroughs in the field of Natural Language Processing.In February 2019, OpenAI started quite a storm through its release of a new transformer-based language model calledGPT-2.GPT-2 is a transformer-based generative language model that wastrained on 40GB of curated text from the internet.You can read more about GPT-2 here:So, lets see GPT-2 in action!Before we can start using GPT-2, lets know a bit about the PyTorch-Transformers library. We will be using this library we will use to load the pre-trained models.PyTorch-Transformers provides state-of-the-art pre-trained models for Natural Language Processing (NLP).Most of the State-of-the-Art models require tons of training data and days of training on expensive GPU hardware which is something only the big technology companies and research labs can afford. But by using PyTorch-Transformers, now anyone can utilize the power of State-of-the-Art models!Installing Pytorch-Transformers is pretty straightforward in Python. You can simply use pip install:or if you are working on Colab:Since most of these models are GPU-heavy, I would suggest working with Google Colab for this part of the article.Lets build our own sentence completion model using GPT-2.Well try to predict the next word in the sentence:what is the fastest car in the _________I chose this example because this is the first suggestion that Googles text completion gives. Here is the code for doing the same:Here, we tokenize and index the text as a sequence of numbers and pass it to the GPT2LMHeadModel. This is the GPT2 model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).Awesome! The model successfully predicts the next word asworld. This is pretty amazing as this is what Google was suggesting. I recommend you try this model with different input sentences and see how it performs while predicting the next word in a sentence.Now, we have played around by predicting the next word and the next character so far. Lets take text generation to the next level by generating an entire paragraph from an input piece of text!Lets see what our models generate for the following input text:This is the first paragraph of the poem The Road Not Taken by Robert Frost. Lets put GPT-2 to work and generate the next paragraph of the poem.We will be using the readymade script that PyTorch-Transformers provides for this task. Lets clone their repository first:Now, we just need a single command to start the model!Lets see what output our GPT-2 model gives for the input text:Isnt that crazy?! The output almost perfectly fits in the context of the poem and appears as a good continuation of the first paragraph of the poem.Quite a comprehensive journey, wasnt it? We discussed what language models are and how we can use them using the latest state-of-the-art NLP frameworks. And the end result was so impressive!You should consider this as the beginning of your ride into language models. I encourage you to play around with the code Ive showcased here. This will really help you build your own knowledge and skillset while expanding your opportunities in NLP.And if youre new to NLP and looking for a place to start, here is the perfect starting point:Let me know if you have any queries or feedback related to this article in the comments section below. Happy learning!",https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/
A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Object Detection?|Applications of Object Detection|Why Real-Time Object Detection?|Challenges of Performing Real-Time Object Detection|Introduction to SlimYOLOv3|Understanding the Architecture of SlimYOLOv3|End Notes,"Self-Driving Cars|Face Detection and Face Recognition|Action Recognition|Object Counting|Why YOLOv3?|Sparsity training|SlimYOLOv3|Fine-tuning|Share this:|Related Articles|A Comprehensive Guide to Build your own Language Model in Python!|11 Important Model Evaluation Metrics for Machine Learning Everyone should know|
Pulkit Sharma
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Humans can pick out objects in our line of vision in a matter of milliseconds. In fact  just look around you right now. Youll have taken in the surroundings, quickly detected the objects present, and are now looking at this article. How long did that take?That is real-time object detection. How cool would be it if we could get machines to do that? Now we can! Thanks primarily to the recent surge of breakthroughs in deep learning and computer vision, we can lean on object detection algorithms to not only detect objects in an image  but to do that with the speed and accuracy of humans.We will first look at the various nuances of object detection (including the potential challenges you might face). Then, I will introduce the SlimYOLOv3 framework and deep dive into how it works underneath to detect objects in real-time. Time to get excited!If youre new to the wonderful world of computer vision, we have designed the perfect course for you! Make sure you check it out here:Before we dive into how to detect objects in real-time, lets cover our basics first. This is especially important if youre relatively new to the world of computer vision.Object detection is a technique we use to identify the location of objects in an image. If there is a single object in the image and we want to detect that object, it is known as image localization. What if there are multiple objects in an image? Well, thats what object detection is!Let me explain this using an example:The image on the left has a single object (a dog) and hence detecting this object will be an image localization problem. The image on the right has two objects (a cat and a dog). Detecting both these objects would come under object detection.If you wish to get an in-depth introduction to object detection, feel free to refer to my comprehensive guide:Now, you might be wondering  why is object detection required? ANd more to the point, why do we need to perform real-time object detection? Well answer these questions in the next section.Object Detection is being widely used in the industry right now. Anyone harboring ambitions of working in computer vision should know these applications by heart.The use cases of object detection range from personal security to automated vehicle systems. Lets discuss some of these current and ubiquitous applications.This is one of the most interesting and recent applications of Object detection. Honestly, its one I am truly fascinated by.Self-driving cars (also known as autonomous cars) are vehicles that are capable of moving by themselves with little or no human guidance. Now, in order for a car to decide its next step, i.e. either to move forward or to apply breaks, or to turn, it must know the location of all the objects around it. Using Object Detection techniques, the car can detect objects like other cars, pedestrians, traffic signals, etc.Face detection and recognition are perhaps the most widely used applications of computer vision. Every time you upload a picture on Facebook, Instagram or Google Photos, it automatically detects the people in the images. This is the power of computer vision at work.Youll love this one. The aim is to identify the activity or the actions of one or more series of images. Object Detection is the core concept behind this which detects the activity and then recognizes the action. Heres a cool example:
We can use Object Detection algorithms for counting the number of objects in an image or even in real-time videos. Counting the number of objects is helpful in a variety of ways, including analyzing the performance of a store, or estimating the number of people in a crowd.
These are just a few popular object detection applications. There are a whole host of them springing up in the industry so if you know any that are worth mentioning, give me a shout in the comments section below!Now, heres the thing  most of the applications require real-time analysis. The dynamic nature of our industry leans heavily towards instant results and thats where real-time object detection comes into the picture.Lets take the example of self-driving cars. Consider that we have trained an object detection model which takes a few seconds (say 2 seconds per image) to detect objects in an image and we finally deployed this model in a self-driving car.Do you think this model will be good? Will the car be able to detect objects in front of it and take action accordingly?Certainly not! The inference time here is too much. The car will take a lot of time to make decisions which might lead to serious situations like accidents as well. Hence, in such scenarios, we need a model that will give us real-time results. The model should be able to detect objects and make inferences within microseconds.Some of the commonly used algorithms for object detection include RCNN, Fast RCNN, Faster RCNN, and YOLO.The aim of this article is not to deep dive into these techniques but to understand the SlimYOLOv3 architecture for real-time object detection. If you wish to learn more about these techniques, check out the below tutorials:These techniques work really well when we do not need real-time detection. Unfortunately, they tend to stumble and fall when faced with the prospect of real-time analysis. Lets look at some of the challenges you might encounter when trying to build your own real-time object detection model.Real-time object detection models should be able to sense the environment, parse the scene and finally react accordingly. The model should be able to identify what all types of objects are present in the scene. Once the type of objects have been identified, the model should locate the position of these objects by defining a bounding box around each object.So, there are two functions here. First, classifying the objects in the image (image classification), and then locating the objects with a bounding box (object detection).We can potentially face multiple challenges when we are working on a real-time problem:So how can we overcome these challenges? Well  this is where the crux of the article begins- the SlimYOLOv3 framework! SlimYOLOv3 aims to deal with these limitations and perform real-time object detection with incredible precision.Lets first understand what SlimYOLOv3 is and then we will look at the architecture details to have a better understanding of the framework.Can you guess how a deep learning pipeline works? Heres a quick summary of a typical process:There are multiple components or connections in the model. Some of these connections, after a few iterations, become redundant and hence we can remove these connections from the model. Removing these connections is referred to as pruning.Pruning will not significantly impact the performance of the model and the computation power will reduce significantly. Hence, in SlimYOLOv3, pruning is performed on convolutional layers. We will learn more about how this pruning is done in the next section of this article.After pruning, we fine-tune the model to compensate for the degradation in the models performance.A pruned model results in fewer trainable parameters and lower computation requirements in comparison to the original YOLOv3 and hence it is more convenient for real-time object detection.Lets now discuss the architecture of SlimYOLOv3 to get a better and clearer understanding of how this framework works underneath.The below image illustrates how SlimYOLOv3 works:SlimYOLOv3 is the modified version of YOLOv3. The convolutional layers of YOLOv3 are pruned to achieve a slim and faster version. But wait  why are we using YOLOv3 in the first place? Why not other object detection algorithms like RCNN, Faster RCNN?There are basically two types (or two categories) of deep object detection models:The next step is the sparsity training of this YOLOv3 model:Here, we prune the YOLOv3 model using the following steps:The removed components can either be an individual neural connection or the network structures. To define the importance of each component, we rank each neuron of the network based on their contribution. There are multiple ways to do it:In SlimYOLOv3, the importance is calculated based on the L1 regularized means of neuron weights which are considered as the scaling factor. The absolute value of these scaling factors is the importance of a channel. To accelerate the convergence and improve the generalization of the YOLOv3 model, the batch normalization layer is used after every convolutional layer.We then define a global threshold, lets say , and discard any channel that has a scaling factor less than this threshold. In this way, we prune the YOLOv3 architecture and get the SlimYOLOv3 architecture:While evaluating the scaling factor, the maxpool layers and the upsample layers of the YOLOv3 architecture have not been considered since they have nothing to do with the channel number of the layer number.We now have the SlimYOLOv3 model, so whats next?We fine-tune it so as to compensate for the degradation in performance and finally evaluate the fine-tuned model to determine whether the pruned model is suitable for deployment.Sparsity training is actually quite effective in reducing the scaling factor and hence making the feature channels of convolutional layers sparse. Training with a larger penalty factor of  = 0.01, leads to aggressive decay of the scaling factor and the model starts to overfit.In SlimYOLOv3, a penalty factor of  = 0.0001 is used to perform channel pruning.Weve covered a lot of ground in this article. We saw the different object detection algorithms like RCNN, Fast RCNN, Faster RCNN, as well as the current state-of-the-art for object detection YOLO. Then, we looked at the SlimYOLOv3 architecture which is the pruned version of YOLO and can be used for real-time object detection.Im excited to get my hands on the code for SlimYOLOv3! I will try to implement SlimYOLOv3 and will share my learning with you guys.If you have any questions, doubts or feedback related to this article, feel free to discuss with me in the comments section below.",https://www.analyticsvidhya.com/blog/2019/08/introduction-slimyolov3-real-time-object-detection/
11 Important Model Evaluation Metrics for Machine Learning Everyone should know,"Learn everything about Analytics|Overview|Introduction|Table of Contents|Warming up: Types of Predictive models|1. Confusion Matrix|2. F1 Score|3. Gain and Lift charts
|4. Kolomogorov Smirnov chart
|5. Area Under the ROC curve (AUC  ROC)|6. Log Loss|7. Gini Coefficient|8. Concordant  Discordant ratio|9. Root Mean Squared Error (RMSE)|10. Root Mean Squared Logarithmic Error|11. R-Squared/Adjusted R-Squared|12. Cross Validation|End Notes","|Advantages of using ROC|Adjusted R-Squared|Here is an example of scoring on Kaggle!|The concept : Cross Validation|k-fold Cross validation|How does this help to find best (non over-fit) model?|How do we implementk-fold with any model?|But how do we choose k?|You want to apply your analytical skills and test your potential? Thenparticipate in our Hackathonsand compete with TopData Scientists from all over the world.|Share this:|Related Articles|A Friendly Introduction to Real-Time Object Detection using the Powerful SlimYOLOv3 Framework|Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science|
Tavish Srivastava
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was originally published in February 2016 and updated in August 2019. with four new evaluation metrics.The idea of building machine learning models works on a constructive feedback principle. You build a model, get feedback from metrics, make improvements and continue until you achieve a desirable accuracy. Evaluation metrics explain the performance of a model. An important aspect of evaluation metrics is their capability to discriminate among model results.I have seen plenty of analysts and aspiring data scientists not even bothering to check how robust their model is. Once they are finished building a model, they hurriedly map predicted values on unseen data. This is an incorrect approach.Simply building a predictive model is not your motive. Its about creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check the accuracy of your model prior to computing predicted values.In our industry, we consider different kinds of metrics to evaluate our models. The choice of metric completely depends on the type of model and the implementation plan of the model.After you are finished building your model, these 11 metrics will help you in evaluating your models accuracy. Considering the rising popularity and importance of cross-validation, Ive also mentioned its principles in this article.And if youre starting out your machine learning journey, you should check out the comprehensive and popular Applied Machine Learning course which covers this concept in a lot of detail along with the various algorithms and components of machine learning.When we talk about predictive models, we are talking either about a regression model (continuous output) or a classification model (nominal or binary output). The evaluation metrics used in each of these models are different.In classification problems, we use two types of algorithms (dependent on the kind of output it creates):In regression problems, we do not have such inconsistencies in output. The output is always continuous in nature and requires no further treatment.Illustrative ExampleFor a classification model evaluation metric discussion, I have used my predictions for the problem BCI challenge on Kaggle. The solution of the problem is out of the scope of our discussion here. However the final predictions on the training set have been used for this article. The predictions made for this problem were probability outputs which have been converted to class outputs assuming a threshold of 0.5.A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For the problem in hand, we have N=2, and hence we get a 2 X 2 matrix. Here are a few definitions, you need to remember for a confusion matrix :The accuracy for the problem in hand comes out to be 88%. As you can see from the above two tables, the Positive predictive Value is high, but negative predictive value is quite low. Same holds for Sensitivity and Specificity. This is primarily driven by the threshold value we have chosen. If we decrease our threshold value, the two pairs of starkly different numbers will come closer.In general we are concerned with one of the above defined metric. For instance, in a pharmaceutical company, they will be more concerned with minimal wrong positive diagnosis. Hence, they will be more concerned about high Specificity. On the other hand an attrition model will be more concerned with Sensitivity. Confusion matrix are generally used only with class output models.In the last section, we discussed precision and recall for classification problems and also highlighted the importance of choosing precision/recall basis our use case. What if for a use case, we are trying to get the best precision and recall at the same time? F1-Score is the harmonic mean of precision and recall values for a classification problem. The formula for F1-Score is as follows:Now, an obvious question that comes to mind is why are taking a harmonic mean and not an arithmetic mean. This is because HM punishes extreme values more. Let us understand this with an example. We have a binary classification model with the following results:Precision: 0, Recall: 1Here, if we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a dumb classifier which just ignores the input and just predicts one of the classes as output. Now, if we were to take HM, we will get 0 which is accurate as this model is useless for all purposes.This seems simple. There are situations however for which a data scientist would like to give a percentage more importance/weight to either precision or recall. Altering the above expression a bit such that we can include an adjustable parameter beta for this purpose, we get:Fbetameasures the effectiveness of a model with respect to a user who attaches  times as much importance to recall as precision.Gain and Lift chart are mainly concerned to check the rank ordering of the probabilities. Here are the steps to build a Lift/Gain chart:Step 1 : Calculate probability for each observationStep 2 : Rank these probabilities in decreasing order.Step 3 : Build deciles with each group having almost 10% of the observations.Step 4 : Calculate the response rate at each deciles for Good (Responders) ,Bad (Non-responders) and total.You will get following table from which you need to plot Gain/Lift charts:This is a very informative table. Cumulative Gain chart is the graph between Cumulative %Right and Cummulative %Population. For the case in hand here is the graph :This graph tells you how well is your model segregating responders from non-responders. For example, the first decile however has 10% of the population, has 14% of responders. This means we have a 140% lift at first decile.What is the maximum lift we could have reached in first decile? From the first table of this article, we know that the total number of responders are 3850. Also the first decile will contains 543 observations. Hence, the maximum lift at first decile could have been 543/3850 ~ 14.1%. Hence, we are quite close to perfection with this model.Lets now plot the lift curve. Lift curve is the plot between total lift and %population. Note that for a random model, this always stays flat at 100%. Here is the plot for the case in hand :You can also plot decile wise lift with decile number :What does this graph tell you? It tells you that our model does well till the 7th decile. Post which every decile will be skewed towards non-responders. Any model with lift @ decile above 100% till minimum 3rd decile and maximum 7th decile is a good model. Else you might consider over sampling first.Lift / Gain charts are widely used in campaign targeting problems. This tells us till which decile can we target customers for an specific campaign. Also, it tells you how much response do you expect from the new target base.K-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100, if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives.On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.For the case in hand, following is the table :We can also plot the %Cumulative Good and Bad to see the maximum separation. Following is a sample plot :The metrics covered till hereare mostly used in classification problems. Till here, we learnt about confusion matrix, lift and gain chart and kolmogorov-smirnov chart.Lets proceed and learn fewmore important metrics.This is again one of the popular metrics used in the industry. The biggest advantage of using ROC curve is that it is independent of the change in proportion of responders. This statement will get clearer in the following sections.Lets first try to understand what is ROC (Receiver operating characteristic) curve. If we look at the confusion matrix below, we observe that for a probabilistic model, we get different value for each metric.Hence, for each sensitivity, we get a different specificity.The two vary as follows:The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as false positive rate and sensitivity is also known as True Positive rate. Following is the ROC curve for the case in hand.Lets take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion matrix :As you can see, the sensitivity at this threshold is 99.6% and the (1-specificity) is ~60%. This coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we find the area under this curve (AUC).Note that the area of entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb rules:We see that we fall under the excellent band for the current model. But this might simply be over-fitting. In such cases it becomes very important to to in-time and out-of-time validations.Points to Remember:1.For a model which gives class as output, will be represented as a single point in ROC plot.2. Such models cannot be compared with each other as the judgement needs to be taken on a single metric and not using multiple metrics. For instance, model with parameters (0.2,0.8) and model with parameter (0.8,0.2) can be coming out of the same model, hence these metrics should not be directly compared.3. In case of probabilistic model, we were fortunate enough to get a single number which was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is also possible that one model performs better in some region and other performs better in other.Why should you use ROC and not metrics like lift curve?Lift is dependent ontotal response rate of the population. Hence, if the response rate of the population changes, the same model will give a different lift chart. A solution to this concern can be true lift chart (finding the ratio of lift and perfect model lift at each decile). But such ratio rarely makes sense for the business.ROC curve on the other hand is almost independent of the response rate. This is because it has the two axis coming out from columnar calculations of confusion matrix. The numerator and denominator of both x and y axis will change on similar scale in case of response rate shift.AUC ROC considers the predicted probabilities for determining our models performance. However, there is an issue with AUC ROC, it only takes into account the order of probabilities and hence it does not take into account the models capability to predict higher probability for samples more likely to be positive. In that case, we could us the log loss which is nothing butnegative average of the log of corrected predicted probabilities for each instance.Let us calculate log loss for a few random values to get the gist of the above mathematical function:Logloss(1, 0.1) = 2.303Logloss(1, 0.5) = 0.693Logloss(1, 0.9) = 0.105If we plot this relationship, we will get a curve as follows:Its apparent from the gentle downward slope towards the right that the Log Loss gradually declines as the predicted probability improves. Moving in the opposite direction though, the Log Loss ramps up very rapidly as the predicted probability approaches 0.So, lower the log loss, better the model. However, there is no absolute measure on a good log loss and it is use-case/application dependent.Whereas the AUC is computed with regards to binary classification with a varying decision threshold, log loss actually takes certainty of classification into account.Gini coefficient is sometimes used in classification problems. Gini coefficient can be straigh away derived from the AUC ROC number. Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. Following is the formulae used :Gini = 2*AUC  1Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.This is again one of the most important metric for any classification predictions problem. To understand this lets assume we have 3 students who have some likelihood to pass this year. Following are our predictions :A  0.9B  0.5C  0.3Nowpicture this.if we were to fetch pairs of two from these three student, how many pairs will we have? We will have 3 pairs : AB , BC, CA. Now, after the year ends we saw that A and C passed this year while B failed. No, we choose all the pairs where we will find one responder and other non-responder. How many such pairs do we have?We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the probability of responder was higher than non-responder. Whereas discordant pair is where the vice-versa holds true. In case both the probabilities were equal, we say its a tie. Lets see what happens in our case :AB  ConcordantBC  DiscordantHence, we have 50% of concordant cases in this example. Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the models predictive power. For decisions like how many to target are again taken by KS / Lift charts.RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:RMSE metric is given by:where, N is Total Number of Observations.In case of Root mean squared logarithmic error, we take the log of the predictions and actual values. So basically, what changes are the variance that we are measuring. RMSLE is usually used when we dont want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.We learned that when the RMSE decreases, the models performance will improve. But these values alone are not intuitive.In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how good our model is against a random model, which has an accuracy of 0.5. So the random model can be treated as a benchmark.But when we talk about the RMSE metrics, we do not have a benchmark to compare.This is where we can use R-Squared metric. The formula for R-Squared is as follows:MSE(model): Mean Squared Error of the predictions against the actual valuesMSE(baseline): Mean Squared Error of mean prediction against the actual valuesIn other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared. The formulafor adjusted R-Squared is given by:k: number of featuresn: number of samplesAs you can see, this metric takes the number of features into account. When we add more features, the term in the denominator n-(k +1) decreases, so the whole expression increases.If R-Squared does not increase, that means the feature added isnt valuable for our model. So overall we subtract a greater value from 1 and adjusted r2, in turn, would decrease.Beyond these 11 metrics, there is another method to check the model performance. These 7 methods are statistically prominent in data science. But, with arrival of machine learning, we are now blessedwith more robust methods of model selection. Yes! Im talking about Cross Validation.Though, cross validation isnt a really an evaluation metric which is used openly tocommunicate model accuracy.But, the result of cross validation provides good enough intuitive result to generalize the performance of a model.Lets now understand cross validation in detail.Lets first understand the importance of cross validation. Due to busy schedules, these days I dont get much time to participate in data science competitions. Long time back, I participated in TFI Competition on Kaggle. Without delving into my competition performance, I would like to show you the dissimilarity between my public and private leaderboard score.For TFI competition, following were three of my solution and scores (Lesser the better) :You will notice that the third entry which has the worstPublic score turnedto be the best model on Private ranking. There were more than 20 models above the submission_all.csv, but I still chose submission_all.csvas my final entry (which really worked out well). What caused this phenomenon ? The dissimilarity in my public and private leaderboard is caused by over-fitting.Over-fitting is nothing but when you model become highly complex that it starts capturing noise also. This noise adds no value to model, but only inaccuracy.In the following section, I will discuss how you can know if a solution is an over-fit or not before we actually know the test results.Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.Above diagram shows how to validate model with in-time sample. We simply divide the population into 2 samples, and build model on one sample. Rest of the population is used for in-time validation.Could there be anegativeside of the above approach?I believe, a negative side of this approach is that we loose a good amount of data from training the model. Hence, the model is very high bias. And this wont give best estimate for the coefficients. So whats the next best option?What if, we make a 50:50 split of training population and the train on first 50 and validate on rest 50. Then, we train on the other 50, test on first 50. This way we train the model on the entire population, however on 50% in one go. This reduces bias because of sample selection to some extent but gives a smaller sample to train the model on. This approach is known as 2-fold cross validation.Lets extrapolate the last example to k-fold from2-fold cross validation. Now, we will try to visualize how does a k-fold validation work.This is a 7-fold cross validation.Heres what goes on behind the scene : we divide the entire population into 7 equal samples. Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). Then, at the second iteration we train the model with a different sample held as validation. In 7 iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power. Once we have all the 7 models, we take average of the error terms to find which of the models is best.k-fold cross validation is widely used to check whether a model is an overfit or not. If the performance metrics at each of the k times modelling are close to each other and the mean of metric is highest. In a Kaggle competition, you might rely more on the cross validation score and not on the Kaggle public score. This way you will be sure that the Public score is not just by chance.Coding k-fold in R and Python are very similar. Here is how you code a k-fold in Python :This is the tricky part. We have a trade off to choose k.For a small k, we have a higher selection bias but low variance in the performances.For a largek, we have a smallselection bias but highvariance in the performances.Think of extreme cases :k = 2 : We have only 2 samples similar to our 50-50 example. Here we build model only on 50% of the population each time. But as the validation is a significant population, the variance of validation performance is minimal.k = number of observations(n) : This is also known as Leave one out. We have nsamples and modelling repeated n number of times leaving only one observation out for cross validation. Hence, the selection bias is minimal but the variance of validation performance is very large.Generally a value of k = 10 is recommended for most purpose.Measuring the performance on training sample is point less. And leaving a in-time validation batch aside is a waste of data. K-Fold gives us a way to use every singe datapoint which can reduce this selection bias to a good extent. Also, K-fold cross validation can be used with any modelling technique.In addition, the metrics covered in this article are some of the most used metrics of evaluation in a classification and regression problems.Which metric do you often use in classification and regression problem ? Have you used k-fold cross validationbefore for any kind of analysis? Did you see any significant benefits against using a batch validation? Do let us know your thoughts about this guide in the comments section below.",https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/
Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science,Learn everything about Analytics|Overview|Introduction|Table of Contents|Applications of Singular Value Decomposition (SVD)|What is Singular Value Decomposition (SVD)?|3 Ways to Perform SVD in Python|End Notes,"SVD for Image Compression|SVD for Image Recovery|SVD for Eigenfaces|SVD for Spectral Clustering|SVD for Removing Background from Videos|Rank of a Matrix|Singular Value Decomposition (SVD)|Why is SVD used in Dimensionality Reduction?|SVD in NumPy|Truncated SVD in scikit-learn|Randomized SVD in scikit-learn|Share this:|Related Articles|11 Important Model Evaluation Metrics for Machine Learning Everyone should know|7 Innovative Machine Learning GitHub Projects you Should Try Out in Python|
Khyati Mahendru
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Another day has passed, and I still havent used y = mx + b.Sounds familiar? I often hear my school and college acquaintances complain that the algebra equations they spent so much time on are essentially useless in the real world.Well  I can assure you thats simply not true. Especially if you want to carve out a career in data science.Linear algebra bridges the gap between theory and practical implementation of concepts. A healthy understanding of linear algebra opens doors to machine learning algorithms we thought were impossible to understand. And one such use of linear algebra is in Singular Value Decomposition (SVD) for dimensionality reduction.You must have come across SVD a lot in data science. Its everywhere, especially when were dealing with dimensionality reduction. But what is it? How does it work? And what are SVDs applications?I briefly mentioned SVD and its applications in my article on the Applications of Linear Algebra in Data Science. In fact, SVD is the foundation of Recommendation Systems that are at the heart of huge companies like Google, YouTube, Amazon, Facebook and many more.We will look at five super useful applications of SVD in this article. But we wont stop there  we will explore how we can use SVD in Python in three different ways as well.And if youre looking for a one-stop-shop to learn all machine learning concepts, we have put together one of the most comprehensive courses available anywhere. Make sure you check it out (and yes, SVD is in there as part of the dimensionality reduction module).We are going to follow a top-down approach here and discuss the applications first. I have explained the math behind SVD after the applications for those interested in how it works underneath.You just need to know four things to understand the applications:In most of the applications, the basic principle of Dimensionality Reduction is used. You want to reduce a high-rank matrix to a low-rank matrix while preserving important information.How many times have we faced this issue? We love clicking images with our smartphone cameras and saving random photos off the web. And then one day  no space! Image compression helps deal with that headache.It minimizes the size of an image in bytes to an acceptable level of quality. This means that you are able to store more images in the same disk space as compared to before.Image compression takes advantage of the fact that only a few of the singular values obtained after SVD are large. You can trim the three matrices based on the first few singular values and obtain a compressed approximation of the original image. Some of the compressed images are nearly indistinguishable from the original by the human eye.Heres how you can code this in Python:Output:If you ask me, even the last image (with n_components = 100) is quite impressive. I would not have guessed that it was compressed if I did not have the other images for comparison.Ever clicked an image in low light? Or had an old image become corrupt? We assume that we cannot get that image back anymore. Its surely lost to the past. Well  not anymore!Well understand image recovery through the concept of matrix completion (and a cool Netflix example).Matrix Completion is the process of filling in the missing entries in a partially observed matrix. The Netflix problem is a common example of this.Given a ratings-matrix in which each entry (i,j) represents the rating of movie j by customer i, if customer i has watched movie j and is otherwise missing, we would like to predict the remaining entries in order to make good recommendations to customers on what to watch next.The basic fact that helps to solve this problem is that most users have a pattern in the movies they watch and in the ratings they give to these movies. So, the ratings-matrix has little unique information. This means that a low-rank matrix would be able to provide a good enough approximation for the matrix.This is what we achieve with the help of SVD.Where else do you see this property? Yes, in matrices of images! Since an image is contiguous, the values of most pixels depend on the pixels around them. So a low-rank matrix can be a good approximation of these images.Here is a snapshot of the results:Chen, Zihan. Singular Value Decomposition and its Applications in Image Processing. ACM, 2018The entire formulation of the problem can be complex to comprehend and requires knowledge of other advanced concepts as well. You can read the paper that I referred to here.The original paper Eigenfaces for Recognition came out in 1991. Before this, most of the approaches for facial recognition dealt with identifying individual features such as the eyes or the nose and developing a face model by the position, size, and relationships among these features.The Eigenface approach sought to extract the relevant information in a face image, encode it as efficiently as possible, and compare one face encoding with a database of models encoded similarly.The encoding is obtained by expressing each face as a linear combination of the selected eigenfaces in the new face space.Let me break the approach down into five steps:You can find these eigenfaces using both PCA and SVD. Here is the first of several eigenfaces I obtained after performing SVD on the Labelled Faces in the Wild dataset:As we can see, only the images in the first few rows look like actual faces. Others look noisy and hence I discarded them. I preserved a total of 120 eigenfaces and transformed the data into the new face space. Then I used the k-nearest neighbors classifier to predict the names based on the faces.You can see the classification report below. Clearly, there is scope for improvement. You can try adjusting the number of eigenfaces to preserve and experiment with different classifiers:Have a look at some of the predictions and their true labels:You can find my attempt at Facial Recognition using Eigenfaces here.Clustering is the task of grouping similar objects together. It is an unsupervised machine learning technique. For most of us, clustering is synonymous with K-Means Clustering  a simple but powerful algorithm. However, it is not always the most accurate.Consider the below case:Clearly, there are 2 clusters in concentric circles. But KMeans with n_clusters = 2 gives the following clusters:K-Means is definitely not the appropriate algorithm to use here. Spectral clustering is a technique that combats this. It has roots in Graph theory. These are the basic steps:You can read about the complete algorithm and its math here. The implementation of Spectral Clustering in scikit-learn is similar to KMeans:You will obtain the below perfectly clustered data from the above code:I have always been curious how all those TV commercials and programs manage to get a cool background behind the actors. While this can be done manually, why put in that much manual effort when you have machine learning?Think of how you would distinguish the background of a video from its foreground. The background of a video is essentially static  it does not see a lot of movement. All the movement is seen in the foreground. This is the property that we exploit to separate the background from the foreground.Here are the steps we can follow for implementing this approach:What do you think these horizontal and wavy lines represent? Take a moment to think about this.The horizontal lines represent the pixel values that do not change throughout the video. So essentially, these represent the background in the video. The wavy lines show movement and represent the foreground.Here is a frame of the video after removing the background:Pretty impressive, right?We have discussed five very useful applications of SVD so far. But how does the math behind SVD actually work? And how useful is it for us as data scientists? Lets understand these points in the next section.I have used the term rank a lot in this article. In fact, through all the literature on SVD and its applications, you will encounter the term rank of a matrix very frequently. So let us start by understanding what this is.The rank of a matrix is the maximum number of linearly independent row (or column) vectors in the matrix. A vector r is said to be linearly independent of vectors r1 and r2 if it cannot be expressed as a linear combination of r1 and r2.Consider the three matrices below:The rank of a matrix can be thought of as a representative of the amount of unique information represented by the matrix. Higher the rank, higher the information.So where does SVD fit into the overall picture? SVD deals with decomposing a matrix into a product of 3 matrices as shown:If the dimensions of A are m x n:You might be wondering why we should go through with this seemingly painstaking decomposition. The reason can be understood by an alternate representation of the decomposition. See the figure below:The decomposition allows us to express our original matrix as a linear combination of low-rank matrices.In a practical application, you will observe that only the first few, say k, singular values are large. The rest of the singular values approach zero. As a result, terms except the first few can be ignored without losing much of the information. See how the matrices are truncated in the figure below:To summarize:We know what SVD is, how it works, and where it is used in the real world. But how can we implement SVD on our own?The concept of SVD sounds complex enough. You might be wondering how to find the 3 matrices U, S, and V. It is a long process if we were to calculate these by hand.Fortunately, we do not need to perform these calculations manually. We can implement SVD in Python in three simple ways.NumPy is the fundamental package for Scientific Computing in Python. It has useful Linear Algebra capabilities along with other applications.You can obtain the complete matrices U, S, and V using SVD in numpy.linalg. Note that S is a diagonal matrix which means that most of its entries are zeros. This is called a sparse matrix. To save space, S is returned as a 1D array of singular values instead of the complete 2D matrix.In most common applications, we do not want to find the complete matrices U, S and V. We saw this in dimensionality reduction and Latent Semantic Analysis, remember?We are ultimately going to trim our matrices, so why find the complete matrices in the first place?In such cases, it is better to use TruncatedSVD from sklearn.decomposition. You specify the number of features you want in the output as the n_components parameter. n_components should be strictly less than the number of features in the input matrix:Randomized SVD gives the same results as Truncated SVD and has a faster computation time. While Truncated SVD uses an exact solver ARPACK, Randomized SVD uses approximation techniques.I really feel Singular Value Decomposition is underrated. It is an important fundamental concept of Linear Algebra and its applications are so cool! Trust me, what we saw is just a fraction of SVDs numerous uses.I encourage you to check out this Comprehensive Guide to build Recommendation Engine from scratch to realize the power of SVD for yourself. Building this project will surely add value to your resume (and enhance your own skillset!).Which SVD application(s) impressed you the most? Use the comments section below to let the community know.",https://www.analyticsvidhya.com/blog/2019/08/5-applications-singular-value-decomposition-svd-data-science/
7 Innovative Machine Learning GitHub Projects you Should Try Out in Python,Learn everything about Analytics|Overview|Introduction|Top Machine Learning GitHub Projects|PyTorch-Transformers (NLP)|NeuralClassifier (NLP)|TDEngine (Big Data)|Video Object Removal (Computer Vision)|Python Autocomplete (Programming)|tfpyth  TensorFlow to PyTorch to TensorFlow (Programming)|MedicalNet|End Notes,"Share this:|Related Articles|Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science|A Data Science Leaders Guide to Managing Stakeholders|
Pranav Dar
|23 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have conducted tons of interviews for data science positions in the last couple of years. One thing has stood out  aspiring machine learning professionals dont focus enough on projects that will make them stand out.And no, I dont mean online competitions and hackathons (though that is always a plus point to showcase). Im talking about off-the-cuff experiments you should do using libraries and frameworks that have just been released. This shows the interviewer two broad things:And guess which platform has the latest machine learning developments and code? Thats right  GitHub!So lets look at the top seven machine learning GitHub projects that were released last month. These projects span the length and breadth of machine learning, including projects related to Natural Language Processing (NLP), Computer Vision, Big Data and more.This is part of our monthly Machine Learning GitHub series we have been running since January 2018. Here are the links for this year so you can catch up quickly:
Ill be honest  the power of Natural Language Processing (NLP) blows my mind. I started working in data science a few years back and the sheer scale at which NLP has grown and transformed the way we work with text  it almost defies description.PyTorch-Transformers is the latest in a long line of state-of-the-art NLP libraries. It has beaten all previous benchmarks in various NLP tasks. What I really like about PyTorch Transformers is that is contains PyTorch implementations, pretrained models weights and other important components to get you started quickly.You might have been frustrated previously at the ridiculous amount of computation power required to run state-of-the-art models. I know I was (not everyone has Googles resources!). PyTorch-Transformers eradicates the issue to a large degree and enables folks like us to build state-of-the-art NLP models.Here are a few in-depth articles to get you started with PyTorch-Transformers (and the concept of pre-trained models in NLP):Multi-label classification on text data is quite a challenge in the real world. We typically work on single label tasks when were dealing with early stage NLP problems. The level goes up several notches on real-world data.In a multi-label classification problem, an instance/record can have multiple labels and the number of labels per instance is not fixed.NeuralClassifier enables us to quickly implement neural models for hierarchical multi-label classification tasks. What I personally like about NeuralClassifier is that it provides a wide variety of text encoders we are familiar with, such as FastText, RCNN, Transformer encoder and so on.We can perform the below classification tasks using NeuralClassifier:Here are two excellent articles to read up on what exactly multi-label classification is and how to perform it in Python:This TDEngine repository received the most stars of any new project on GitHub last month. Close to 10,000 stars in less than a month. Let that sink in for a second.TDEngine is an open-source Big Data platform designed for:TDEngine essentially provides a whole suit of tasks that we associate with data engineering. And we get to do all this at super quick speed (10x speed on processing queries and 1/5th computational usage).Theres a caveat (for now)  TDEngine only supports execution on Linux. This GitHub repository includes the full documentation and starters guide with code.I suggest checking out our comprehensive resource guide for data engineers:Have you worked with any image data yet? Computer Vision techniques for manipulating and dealing with images are quite advanced. Object detection for images is considered a basic step to becoming a computer vision expert.What about videos, though? The difficult level goes up several notches when were asked to simply draw bounding boxes around objects in videos. The dynamic aspect of objects makes the entire concept more complex.So, imagine my delight when I came across this GitHub repository. We just need to draw a bounding box around the object in the video to remove it. It really is that easy! Here are a couple of examples of how this project works:If youre new to the world of computer vision, here are a few resources to get you up and running:Youll love this machine learning GitHub project. As data scientists, our entire role revolves around experimenting with algorithms (well, most of us). This project is about how a simple LSTM model can autocomplete Python code.The code highlighted in grey below is what the LSTM model filled in (and the results are at the bottom of the image):As the developers put it:We train and predict on after cleaning comments, strings and blank lines in the python code. The model is trained after tokenizing python code. It seems more efficient than character level prediction with byte-pair encoding.If youve ever spent (wasted) time on writing out mundane Python lines, this might be exactly what youre looking for. Its still in the very early stages so be open to a few issues.And if youre wondering what in the world LSTM is, you should read this introductory article:TensorFlow and PyTorch both have strong user communities. But the incredible adoption rate of PyTorch should see it leapfrog TensorFlow in the next year or two. Note: This isnt a knock on TensorFlow which is pretty solid.So if you have written any code in TensorFlow and a separate one in PyTorch and want to combine the two to train a model  the tfpyth framework is for you. The best part about tfpyth is that we dont need to rewrite the earlier code.This GitHub repository includes a well structured example of how you can use tfpyth. Its definitely a refreshing look at the TensorFlow vs. PyTorch debate, isnt it?Installing tfpyth is this easy:Here are a couple of in-depth articles to learn how TensorFlow and PyTorch work:I associate transfer learning with NLP. Thats my fault  I am so absorbed with the new developments that I did not imagine where else transfer learning could be applied. So I was thrilled when I came across this wonderful MedicalNet project.This GitHub repository contains a PyTorch implementation of the Med3D: Transfer Learning for 3D Medical Image Analysis paper. This machine learning project aggregates the medical dataset with diverse modalities, target organs, and pathologies to build relatively large datasets.And as we well know, our deep learning models do (usually) require a large amount of training data. So MedicalNet, released by TenCent, is a brilliant open source project I hope a lot of folks work on.The developers behind MedicalNet have released four pretrained models based on 23 datasets. And here is an intuitive introduction to transfer learning if you needed one:Quite a mix of machine learning projects we have here. I have provided tutorials, guides and resources after each GitHub project.I have one ask  pick the project that interests you, go through the tutorial, and then apply that particular library to solve the problem. For example, you could take up the NeuralClassifier repository and use that to solve a multi-label classification problem.This will help you broaden your understanding of the topic and expand your current skillset. A win-win scenario! Let me know your thoughts, your favorite project from the above list, and your feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2019/08/7-innovative-machine-learning-github-projects-in-python/
A Data Science Leaders Guide to Managing Stakeholders,Learn everything about Analytics|Overview|Introduction|What we Plan to Cover in this Series|Table of Contents|The Sachin-Tendulkar-Expectation Syndrome|Understanding the Stakeholder|Customer Expectations from Data-Driven Products are Fluid|Address the Illusion of 100% Accuracy|Success of AI Solutions Needs a Paradigm Shift in Product Strategy and Execution|Strike a Fine Balance between Data Science Coolness/Style and Delivery Substance|End Notes,"About the Author|Share this:|Related Articles|7 Innovative Machine Learning GitHub Projects you Should Try Out in Python|Building a Recommendation System using Word2vec: A Unique Tutorial with Case Study in Python|
Om Deshmukh
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Managing stakeholders in the world of data science projects is a tricky prospect. I have seen a lot of executives and professionals get swept up in the hype around data science without properly understanding what a full-blown project entails.And I dont say this lightly  my career has been at the very cusp of machine learning and delivery. I hold a Ph.D. in Data Science and Machine Learning from one of the best institutions in the world and have several years of experience working with some of the top industry research labs.I moved to Yodlee, a FinTech organization, in 2016 to run the data sciences product delivery division. The 18 months that it took me and my team to deliver the first truly data-driven solution were full of learnings. Learnings that no industry research would have taught me.These learnings have made me appreciate the immensely non-trivial efforts that go into converting a research prototype into a finished data-driven product that can be monetized. Here are a couple of my key learnings:I have penned down my experience and learning in a series of articles that will help other data science thought leaders and managers understand the scale of the task at hand.In this four-article series, I will share some of these learnings in the hope that others benefit from our findings.I request you to share your learnings, be it similar or opposite to ours, in the comments section. You can read articles 2 and 3 of this series here:Lets begin with part one!Just about everybody believes that data-science/AI/machine learning is the panacea that will instantly and permanently solve every problem that plagues (or is perceived to plague) their organization. This isirrespective of where they are in the corporate hierarchy and irrespective of the degree of their interaction with the delivery group,
If you think this is an exaggeration, a cursory web-search for quotes on AI by various corporations or the number of organizations with ai in their URL should convince you! I call this the Sachin-Tendulkar-Expectation-Syndrome (STES). Sachin Tendulkar is one of the best cricketers the world has seen. In the late 1990s and the early 2000s, the crowd expected Sachin to hit a century in every match he played.This kind of expectation virtually guaranteed that he would rarely wow the crowd. If he did manage to hit a century, it was met expectations and if he didnt then that was a huge letdown.A data science delivery leader is thus tasked with the dual responsibility of:In this article, I will talk about who the main stakeholders are and share my attempts at working with them.Broadly speaking, there are three main stakeholders. Each stakeholder has priorities which can, at times, be orthogonal to those of the others. As the data-science-delivery-leader, the first responsibility is to manage all these three stakeholders and ensure that they see the perspective of the other two while keeping the end-user of the solution at the centre of the picture. Only then can an optimal and practical solution emerge.The three stakeholders are:While the first two are typically outside of the delivery leaders immediate control, the third one is within and that makes it that much more complicated.The customer-facing teams have a first-hand view of customers perceptions of the existing product as well as what the competition is offering. They can thus be a great litmus test to establish the superiority of the new and improved data-driven solution. This is easier said than done. The customer-facing teams are always torn in the dilemmas of dont fix if it aint broke vs. usher in the change before competition makes you obsolete. When it comes to data-driven solutions, it is not always easy to identify where the solution is with respect to these two extremes.Specifically, data-driven products, if left static, will see a gradual deterioration in performance over time making it difficult to identify when the system is broken.More importantly, if we wait for the solution to be perfect, we may end up with a solution which is perfect but too late. Or worse, perfect at solving a problem which is only of marginal extra value.The customer-facing teams have to collectively work with the data science team and the customer to agree on when to deploy trial runs of the solution and set the expectations of continuous improvements. Even the mighty Google search engine is a lot more accurate today than it was last year. And it will surely be even more accurate a year from now, largely because the underlying data-driven techniques learn from user behavior.Customer-facing teams also have to be vary of the illusion of 100% accuracy. The definition of accuracy in a data-driven solution can vary from user to user. Often, the end user is forgiving of errors if their essential needs are met.For example, lets say we search for jaguar on an image-search engine. Our perception of the quality of the search engine is going to be shaped broadly by two aspects:Yet another category of users may have come to the search engine looking for jaquar, the company that makes sanitary fittings and will be wowed if the search engine shows these images alongside the animal/vehicle images with a did-you-make-a-spelling-mistake suggestion.Similarly, a typical user of a web search engine is going to be satisfied even if the first few search results were not relevant but, say, the fifth search result was exactly what they were looking for.Such educational sessions with the customer-facing teams and the customers themselves are quite imperative to manage their expectations as the new solution is rolled out.Figure 1: Different results of an image search engine for a search term jaguar. Note that all the results can be perceived to be correct.A third aspect that the teams have to be made aware of is the seemingly non-deterministic nature of data-driven solutions. These solutions typically go through a training phase where they are trained on a subset of the universe of all possible data samples.In real-world deployments, there will often be cases where the input sample, even though similar to one of the training samples, leads to an incorrect output. A good data-driven algorithm should provide generalizability (i.e., correct output on unseen input samples). But it can almost never guarantee a 100% accuracy.This is a fundamental shift from how pure software-driven solutions work. An example of this is clicking on the green launch icon on a website that will deterministically launch the new page. The customer-facing teams have to be educated about this behavior.In one of the later articles in this series, I will discuss how the data science team should work closely with the customer-facing teams to plan for post-deployment support and hot fixes for what I call the publicity-hungry bugs. So keep an eye out for that!
The second set of stakeholders is the executive team. AI/machine learning has caught the collective imagination of the mainstream media in general, but in particular also of the media that primarily caters to the technical executives.The executives, thus, need tangible assurances that their organization is utilizing the AI revolution. At the same time, they need to be educated that the AI solutions have their own unique development, deployment and maintenance cycle. This, therefore, calls for a different timeline expectation.I consider myself extremely lucky to have senior executives who are willing to get hands-on learning of various data-science/AI/ML concepts. They genuinely believe in the power of AI and are patient in terms of giving the data science team a long rope to build foundational data-driven solutions. These learning sessions go a long way in clearly articulating the business value of AI/ML.It is critical in such conversations with the executive teams to distinguish between what I call the consumer-AI and the enterprise-AI. Source: Fabrik BrandsThe popular image of AI-can-solve-every-problem-imaginable is largely built based on problems that are low-stakes. Some typical examples of low-stakes consumer-AI problems are things like:In the worst case, the recommended movie may actually be of the horror genre while it was recommended as a romantic comedy.Contrast this with enterprise-AI, where, for example, an AI-driven solution is used to detect the number plates of cars zipping by on the freeway to flag cars which were part of criminal activities. Imagine the AI solution misfires a 5 for a 6 and an innocent driver is pulled over by armed patrol vehicles.Such mistakes have the potential to lead to multimillion-dollar lawsuits. I call these situations as the high-stakes enterprise-AI scenarios.The third set of stakeholders is the data science team. This is by far the most important stakeholder with respect to the timely delivery of the data-driven solution. This group is also typically most excited about simply using the latest coolest technology to the problem at hand, without necessarily always worrying about the appropriateness of the technology.The delivery leader has to periodically remind the data science team that the AI is only a part of the whole puzzle and that the end consumer almost never interacts directly with the data science component. Also, the end consumer is not going to be more forgiving of the errors just because the latest coolest technology is used.The other important role of the delivery leader is to provide enough air cover to the team from day-to-day delivery pressures. This helps them to explore deep and systematic ways to solve the problem at hand. This air cover is particularly important for the data science team (as compared to the software development team) to experiment.This is because there exists a substantial gap between the theory of data science and its practice. Moreover, facilitating a work culture that is conducive to working on machine learning problems will also lead to higher job satisfaction. And that in turns leads to healthier employee retention!Now, the stakeholders are aligned on the advantages (and limitations) of the data science approach. The next step is to translate the business problem into a data-science problem. Our learnings on this step will be discussed in the next article.Managing stakeholders is a key aspect most data science professionals are still not fully aware of. How has your experience been with this side of data science? I would love to hear your thoughts and experience. Lets discuss in the comments section below.Dr. Om Deshmukh  Senior Director of Data Sciences and Innovation, Envestnet | YodleeDr. Om Deshmukh is the senior director of data sciences and innovation at Envestnet|Yodlee. Om is a Ph.D. in Machine Learning from University of Maryland, College Park.He has made significant contributions to the field of data sciences for close to two decades now, which include 50+ patents (filed/granted) 50+ international publications and multi-million dollar top-line / bottom line impact across various business verticals. Om was recognised as one of the top 10 data scientists in India in 2017. In 2019, his team was recognised as one of the 10 best data sciences teams in India.At Envestnet|Yodlee, Om leads a team of data scientists who drive foundational data science initiatives to mine actionable insights from transactional data. His team is also responsible for building end-to-end data science driven solutions and delivering those to the clients.Prior to Yodlee, he was at IBM Research and Xerox Research driving technical strategic and research initiatives around data sciences and big data analytics. Om has strong ties with the academia in India and in the US. He is co-advising a Ph.D. student at IISc and serves as an industry expert for several BTech/MTech project evaluations.",https://www.analyticsvidhya.com/blog/2019/08/data-science-leader-guide-managing-stakeholders/
Building a Recommendation System using Word2vec: A Unique Tutorial with Case Study in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|Introduction to word2vec  Vector Representation of Words|How are word2vec Embeddings Obtained?|Applying word2vec Model on Non-Textual Data|Case Study: Using word2vec in Python for Online Product Recommendation|End Notes,"Training Data Preparation|Obtaining word2vec Embeddings|Treat Missing Data|Data Preparation|Build word2vec Embeddings for Products|Visualize word2vec Embeddings|Start Recommending Products|Share this:|Related Articles|A Data Science Leaders Guide to Managing Stakeholders|OpenAIs GPT-2: A Simple Guide to Build the Worlds Most Advanced Text Generator in Python|
Prateek Joshi
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Be honest  how many times have you used the Recommended for you section on Amazon? Ever since I found out a few years back that machine learning powers this section  I have been hooked. I keep an eye on that section each time I log into Amazon.Theres a reason companies like Netflix, Google, Amazon, Flipkart, etc. spend millions perfecting their recommendation engine. It is a powerful acquisition channel and enhances the customers experience.Let me use a recent example to showcase their power. I went to a popular online marketplace looking for a recliner. There were hundreds of them. From Traditional two-position recliners to Push-Back Liners; from Power Lift Recliner to the Wall Hugger one. I liked most of them and I clicked on a leatherette manual recliner:Notice the different kinds of information presented on this page. The left half of the image contains the pictures of the product from different angles. The right half contains a few details about the product and a section of similar products.This is my favorite part of the image. The website is recommending me similar products and it saves me the effort to manually go and browse similar armchairs.In this article, we are going to build our own recommendation system. But well approach this from a unique perspective. We will use Word2vec, an NLP concept, to recommend products to users. Its a very exciting tutorial so lets dive straight in.I have covered a few concepts in this article that you should be aware of. I recommend taking a look at these two articles to get a quick refresher:We know that machines struggle to deal with raw text data. In fact, its almost impossible for machines to deal with anything except for numerical data. So representing text in the form of vectors has always been the most important step in almost all NLP tasks.One of the most significant steps in this direction has been the use of word2vec embeddings, introduced to the NLP community in 2013. It completely changed the entire landscape of NLP.These embeddings proved to be state-of-the-art for tasks like word analogies and word similarities. word2vec embeddings were also able to achieve tasks like King  man +woman ~= Queen, which was considered an almost magical result.Now, there are two variants of a word2vec model  Continuous Bag of Words and Skip-Gram model. In this article, we will use the Skip-Gram model.Lets first understand how word2vec vectors or embeddings are calculated.A word2vec model is a simple neural network model with a single hidden layer. The task of this model is to predict the nearby words for each and every word in a sentence. However, our objective has nothing to with this task. All we want are the weights learned by the hidden layer of the model once the model is trained. These weights can then be used as the word embeddings.Let me give you an example to understand how a word2vec model works. Consider the sentence below:Lets say the word teleport (highlighted in yellow) is our input word. It has a context window of size 2. This means we are considering only the 2 adjacent words on either side of the input word as the nearby words.Note: The size of the context window is not fixed, it can be changed as per our requirement.Now, the task is to pick the nearby words (words in the context window) one-by-one and find the probability of every word in the vocabulary of being the selected nearby word. Sounds straightforward, right?Lets take another example to understand the entire process in detail.We need a labeled dataset to train a neural network model. This means the dataset should have a set of inputs and an output for every input. You might have a few pressing questions at this point:Well  I have good news for you! We can easily create our own labeled data to train a word2vec model. Below I will illustrate how to generate this dataset from any text. Lets use a sentence and create training data from it.Step 1: The yellow highlighted word will be our input and the words highlighted in green are going to be the output words. We will use a window size of 2 words. Lets start with the first word as the input word.So, the training samples with respect to this input word will be as follows:Step 2: Next, we will take the second word as the input word. The context window will also shift along with it. Now, the nearby words are we, become, and what.The new training samples will get appended to the previous ones as given below:We will continue with these steps until the last word of the sentence. In the end, the complete training data from this sentence will look like this:We have extracted 27 training samples out of a single sentence. Got to love it! This is one of the many things that I like about working with unstructured data  creating a labeled dataset out of thin air.Now, lets say we have a bunch of sentences and we extract training samples from them in the same manner. We will end up with training data of considerable size.Suppose the number of unique words in this dataset is 5,000 and we wish to create word vectors of size 100 each. Then, with respect to the word2vec architecture given below:The inputs would be the one-hot-encoded vectors and the output layer would give the probability of being the nearby word for every word in the vocabulary.Once this model is trained, we can easily extract the learned weight matrix WV x N and use it to extract the word vectors:As you can see above, the weight matrix has a shape of 5000 x 100. The first row of this matrix corresponds to the first word in the vocabulary, the second to the second, and so on.That is how we get the fixed size word vectors or embeddings by word2vec. Similar words in this dataset would have similar vectors, i.e. vectors pointing towards the same direction. For example, the terms car and jeep would have similar vectors as these words:This was a high-level overview of how word2vec is used in NLP.Before we proceed to the implementation part  let me ask you a question. How can we use word2vec for a non-NLP task such as product recommendation? Im sure youve been wondering that since you read this articles topic. So lets finally solve the puzzle.Can you guess the fundamental property of a natural language that word2vec exploits to create vector representations of text?It is the sequential nature of the text. Every sentence or phrase has a sequence of words. In the absence of this sequence, we would have a hard time understanding the text. Just try to interpret the sentence below:these most been languages deciphered written of have alreadyThere is no sequence in this sentence. It becomes difficult for us to grasp it and thats why the sequence of words is so important in any natural language. This very property got me thinking about data other than text that has a sequential nature as well.One such data is the purchases made by the consumers on E-commerce websites. Most of the time there is a pattern in the buying behavior of the consumers. For example, a person involved in sports-related activities might have an online buying pattern similar to this:Purchase history of the consumerIf we can represent each of these products by a vector, then we can easily find similar products. So, if a user is checking out a product online, then we can easily recommend him/her similar products by using the vector similarity score between the products.But how do we get a vector representation of these products? Can we use the word2vec model to get these vectors?We surely can! Just imagine the buying history of a consumer as a sentence and the products as its words:Taking this idea further, lets work on online retail data and build a recommendation system using word2vec embeddings.Lets set up and understand our problem statement.We are asked to create a system that automatically recommends a certain number of products to the consumers on an E-commerce website based on the past purchase behavior of the consumers.We are going to use an Online Retail Dataset that you can download from this link.Lets fire up our Jupyter Notebook and quickly import the required libraries and load the dataset.Here is the description of the fields in this dataset:Output: (541909, 8)The dataset contains 541,909 transactions. That is a pretty good number for us to build our model.Lets convert the StockCode to string datatype:There are 4,372 customers in our dataset. For each of these customers, we will extract their buying history. In other words, we can have 4,372 sequences of purchases.It is a good practice to set aside a small part of the dataset for validation purposes. Therefore, I will use the data of 90% of the customers to create word2vec embeddings. Lets split the data.We will create sequences of purchases made by the customers in the dataset for both the train and validation set.Since we are not planning to train the model any further, we are calling init_sims( ) here. This will make the model much more memory-efficient:Lets check out the summary of model:Output: Word2Vec(vocab=3151, size=100, alpha=0.03)Our model has a vocabulary of 3,151 unique words and their vectors of size 100 each. Next, we will extract the vectors of all the words in our vocabulary and store it in one place for easy access.Output: (3151, 100)
It is always quite helpful to visualize the embeddings that you have created. Over here, we have 100-dimensional embeddings. We cant even visualize 4 dimensions let alone 100. What in the world can we do?We are going to reduce the dimensions of the product embeddings from 100 to 2 by using the UMAP algorithm. It is popularly used for dimensionality reduction.Every dot in this plot is a product. As you can see, there are several tiny clusters of these data points. These are groups of similar products.Congratulations! We are finally ready with the word2vec embeddings for every product in our online retail dataset. Now, our next step is to suggest similar products for a certain product or a products vector.Lets first create a product-ID and product-description dictionary to easily map a products description to its ID and vice versa.Output: [RED WOOLLY HOTTIE WHITE HEART.]I have defined the function below. It will take a products vector (n) as input and return top 6 similar products:Lets try out our function by passing the vector of the product 90019A (SILVER M.O.P ORBIT BRACELET):Output:[(SILVER M.O.P ORBIT DROP EARRINGS, 0.766798734664917),
(PINK HEART OF GLASS BRACELET, 0.7607438564300537),
(AMBER DROP EARRINGS W LONG BEADS, 0.7573930025100708),
(GOLD/M.O.P PENDANT ORBIT NECKLACE, 0.7413625121116638),
(ANT COPPER RED BOUDICCA BRACELET, 0.7289256453514099),
(WHITE VINT ART DECO CRYSTAL NECKLAC, 0.7265784740447998)]Cool! The results are pretty relevant and match well with the input product. However, this output is based on the vector of a single product only. What if we want to recommend products based on the multiple purchases he or she has made in the past?One simple solution is to take the average of all the vectors of the products the user has bought so far and use this resultant vector to find similar products. We will use the function below that takes in a list of product IDs and gives out a 100-dimensional vector which is a mean of vectors of the products in the input list:Recall that we have already created a separate list of purchase sequences for validation purposes. Now lets make use of that.As it turns out, our system has recommended 6 products based on the entire purchase history of a user. Moreover, if you want to get product suggestions based on the last few purchases, only then also you can use the same set of functions.Below I am giving only the last 10 products purchased as input:Output:[(PARISIENNE KEY CABINET , 0.6296610832214355),
(FRENCH ENAMEL CANDLEHOLDER, 0.6204789876937866),
(VINTAGE ZINC WATERING CAN, 0.5855435729026794),
(CREAM HANGING HEART T-LIGHT HOLDER, 0.5839680433273315),
(ENAMEL FLOWER JUG CREAM, 0.5806118845939636)]Feel free to play around this code and try to get product recommendations for more sequences from the validation set. I would be thrilled if you can further optimize this code or make it better.Full code is available here.I had a great time writing this article and sharing my experience of working with word2vec for making product recommendations. You can try to implement this code on similar non-textual sequence data. Music recommendation can be a good use case, for example.This experiment has inspired me to try other NLP techniques and algorithms to solve more non-NLP tasks. Feel free to use the comments section below if you have any doubts or want to share your feedback.",https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/
OpenAIs GPT-2: A Simple Guide to Build the Worlds Most Advanced Text Generator in Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|Whats New in OpenAIs GPT-2 Framework?|How to Setup the Environment for GPT-2|Implementing GPT-2 in Python to Build our own Text Generator|A Note on the Potential Misuse of GPT-2|End Notes,"The Architecture|Share this:|Related Articles|Building a Recommendation System using Word2vec: A Unique Tutorial with Case Study in Python|Introduction to Bayesian Adjustment Rating: The Incredible Concept Behind Online Ratings!|
Shubham Singh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The worlds best economies are directly linked to a culture of encouragement and positive feedback.Can you guess who said that? It wasnt a President or Prime Minister. It certainly wasnt a leading economist like Raghuram Rajan. All out of guesses?This quote was generated by a machine! Thats right  a Natural Language Processing (NLP) model trained on OpenAIs GPT-2 framework came up with that very real quote. The state of machine learning right now is on another level entirely, isnt it?In the midst of what is truly a golden era in NLP, OpenAIs GPT-2 has remoulded the way we work with text data. Where ULMFiT and Googles BERT eased open the door for NLP enthusiasts, GPT-2 has smashed it in and made it so much easier to work on NLP tasks  primarily text generation.We are going to use GPT-2 in this article to build our own text generator. Heres a taste of what well be building:Excited? Then lets get into the article. Well first understand the intuition behind GPT-2 and then dive straight into Python to build our text generation model.If youre an avid NLP follower, youll love the below guides and tutorials on the latest developments in NLP:Natural Language Processing (NLP) has evolved at a remarkable pace in the past couple of years. Machines are now able to understand the context behind sentences  a truly monumental achievement when you think about it.Developed by OpenAI, GPT-2 is a pre-trained language model which we can use for various NLP tasks, such as:Language Modelling(LM) is one of the most important tasks of modern Natural Language Processing (NLP). A language model is a probabilistic model which predicts the next word or character in a document.GPT-2 is a successor of GPT, the original NLP framework by OpenAI. The full GPT-2 model has 1.5 billion parameters, which is almost 10 times the parameters of GPT. GPT-2 give State-of-the Art results as you might have surmised already (and will soon see when we get into Python).The pre-trained model contains data from 8 million web pages collected from outbound links from Reddit. Lets take a minute to understand how GPT-2 works under the hood.The architecture of GPT-2 is based on the very famous Transformers concept that was proposed by Google in their paper Attention is all you need. The Transformer provides a mechanism based on encoder-decoders to detect input-output dependencies.At each step, the model consumes the previously generated symbols as additional input when generating the next output.GPT-2 has only a few architecture modification besides having many more parameters and Transformers layers:GPT-2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the zero-shot setting. GPT-2 outperforms models trained on domain-specific data sets (e.g. Wikipedia, news, books) when evaluated on those same data sets.  Open AI team.Four models with different parameters are trained to cater different scenarios:GPT-2 has the ability to generate a whole article based on small input sentences. This is in stark contrast to earlier NLP models that could only generate the next word, or find the missing word in a sentence. Essentially, we are dealing in a whole new league.Heres how GPT-2 squares up against other similar NLP models:Well be working with a medium-sized model with 345 million parameters. You can download the pre-trained model from the official OpenAI GitHub repository.First, we need to clone the repository by typing the below statement (I recommend using a Colab notebook instead of your local machine for faster computation):Note that we will need to change our directory. To do that, well use chdir() of class os:Next, choose between the models which we want to use. In this case, well be using a medium-sized model with 345 million parameters.This model requires TensorFlow with GPU support to make it run faster. So lets go ahead and install TensorFlow in our notebook:We want to fulfill some essential requirements before diving into the modeling part. In the cloned folder, youll find a file  requirements.txt. This contains the following four libraries which are mandatory for this model to work:Install all of these libraries using just one line of code:Thats it  were all set with our environment. One last step before we dive into our text generator  downloading the medum-size pretrained model! Again  we can do this with just one line of code:Itll take a few moments depending on your internet bandwidth. Once we are done with that, we need to do encoding with the following code:Are you ready? Because this is the moment youve been waiting for. Time to build our very own advanced text generator in Python using GPT-2! Lets begin.First, move into the src folder by using the chdir() just like we did before:Then, import the required libraries:Note: model, sample and encoder are the Python files present in the src subfolder of the main GPT-2 folder:Lets understand the parameters we just saw one-by-one:Note: To generate more than one sample, you need to change the values of both nsamples and batch_size and also have to keep them equal.Now, the time has come to witness the result produced by the most advanced language model. Lets run this function and generate some text (be prepared to be stunned):You will now be asked to enter a string. This is what I went with:I went to a lounge to celebrate my birthday andAnd heres what my GPT-2 text generator came up with:I called Donna and told her I had just adopted her. She thought my disclosure was a donation, but Im not sure if Donna met the criteria. Donna was a genuinely sweet, talented woman who put her life here as a love story. I know she thanked me because I saw her from the photo gallery and she appreciated my outrage. It was most definitely not a gift. I appreciate that I was letting her care about kids, and that she saw something in me. I also didnt have much choice but to let her know about her new engagement, although this doesnt mean I wasnt concerned, I am extremely thankful for all that shes done to this country. When I saw it, I said, Why havent you become like Betty or Linda? Its our countrys baby and I cant take this decision lightly. But dont tell me youre too impatient. Donna wept and hugged me. She never expresses milk, otherwise Id think sorry for her but sometimes they immediately see how much its meant to her. She apologized publicly and raised flagrant error of judgment in front of the society of hard choices to act which is appalling and didnt grant my request for a birth certificate. Donna was highly emotional. I forgot that she is a scout. She literally didnt do anything and she basically was her own surrogate owner. August 11, 2017 at 12:11 PM Anonymous saidIncredible! I was speechless the first time I saw this result. The incredible level of detail and the impressive grammar  its almost impossible to tell that it was generated completely by a machine. Pretty impressive, right?Go ahead and play around with the input string and share your results in the comments section below.GPT-2 has been in the news for its possible malicious use. You can imagine how powerful this NLP framework is. It could easily be used to generate fake news or frankly any fake text without humans being able to realize the difference.Keeping these things in mind, OpenAI didnt release the full model. Instead, they have released a much smaller model. The original model is trained on 40 GB of internet data and has 1.5 billion parameters. The two sample models OpenAI have released have 117 million and 345 million parameters.In this article, weve used the medium-sized model with 345M million parameters. If these smaller models are capable of generating such impressive results, imagine what the complete model of 1.5 billion parameters could generate. Scary and exciting at the same time.Whats next for NLP? I have a feeling we wont have to wait too long to find out.In the meantime, try out this GPT-2 framework and let me know your experience below. Besides that, I also encourage you to use this model only for the purpose of research and to gain knowledge. Keep learning!",https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/
Introduction to Bayesian Adjustment Rating: The Incredible Concept Behind Online Ratings!,"Learn everything about Analytics|Overview|Introduction|Table of Contents|Setting up the Problem Statement for our Rating System|The Ubiquitousness of Social Influence|In the Star Ratings, I trust|Sorting the Star ratings using Bayesian Adjusted Rating|Summarizing what we learned here","What is our concern?|Whats the fix?|About the Author|Share this:|Related Articles|OpenAIs GPT-2: A Simple Guide to Build the Worlds Most Advanced Text Generator in Python|DATAMIN  Unveiling the Worlds Biggest Online Data Science Quizzing Platform|
Abir Mukherjee
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you always look at the rating and the number of ratings of a product before you buy it? I honestly rely a lot on these factors when Im scouring e-commerce sites or other similar product portals. It is my default go-to option to evaluate the product before I go all in.And these ratings arent just limited to e-commerce portals. We now see them across the internet on various channels, like:Ive always been curious about how these rating systems work. The data science professional in me wanted to figure out if it was a simple average calculation or was there more data science involved here?The results of my research are quite fascinating, as we will see in this article. So lets dive into it.Note: Heres an intuitive article on Bayesian Statistics if youre looking to understand the topic (or need a quick refresher):Suppose you are a blogger and you want to put your top three pieces in popularity order at some easy to view area on your beautiful web page. You want to use the number of likes and shares your subscribers have provided on your blogs in order to sort them. And you wonder if a simple average is what you should use as some of your newer blogs have barely reached sufficient views.The fact is, a lot of websites are grappling with exactly the same perplexity but on a much larger scale. And getting this sorting right is equally important for your brand as a blogger, as much as it is incrementally useful for your subscriber community.Online social networks are not just limited to a few dedicated websites or applications that enable users to create and share content but are present everywhere on the internet. These collaborative virtual communities expose persuasive effects people have on each other in the form of social influence. It can take many forms and can be seen in conformity, compliance, peer pressure, obedience, persuasion, as well as sales and marketing.Participating in online customer ratings and reviews is very much a social network of people with a similar product or service interest.End-users of products or services on the web communicate within themselves using the language of stars and likes buttons and occasional text reviews.It is unquestionably at par or more actionable than any other network we may think of.People constitute both the core and the clientele for any Rating or Reputation system no matter what the overarching platform it is meant for. It can simply be a review forum (Yelp, IMDb or Beer Advocate), a sharing economy (like Airbnb or Uber), a marketplace (Amazon, Flipkart or Swiggy), or even Chinas upcoming Social Credit System.Ratings are so much built into our psyches that the trust is not an obstacle with them anymore. We believe in them subconsciously, we take trade decisions based on them, and we genuinely invest in them with our personal choices.So why are ratings so fundamental to our choices? Why do we trust them with our time, money and emotion? We can fairly assume that these ratings create a sense of transparency and validation of our own decisions and serve the same efficiently through one convenient scalar value.Condensing hundreds of ratings from respective user communities into one exclusive rating value which can epitomize the essence of diversity is as much a field of art as it is science. There is a skill, a mastery of how certain businesses have used them so effectively.For them, its a secret sauce, kept such so as to eliminate and reduce attempts by people more interested in changing the current rating than giving their true opinion of it.The philosophy to rank competing products of the same category on a marketplace like Amazon is to make the ranking appear natural; And for that to happen, the sorting metrics (i.e. our scalar rating value for each product) should inherit the essence of the number of reviews it has received relative to the overall category.Thus, we start with how that specific category, as a whole, has been rated by the users. And then we change our strategy from the Simple Average Rating to Bayesian Adjusted Rating in order to incorporate the variation in the number of reviews to our product ranking.This shift to the Bayesian approach provides the comfort of staying consistent even when we have fewer observations. This is because then it is very likely to obtain extreme values just by chance.Lets say we want to find the best product out of 3 options in a category. Heres how I have calculated it by hand:Eventually, what any new user would care about is how intuitive the ranking is based on what they can observe (i.e. the rating and number of reviews for each product of that category) and now that we can see all the individual data it will make us trust the website more if we see the best product with the smallest probability to being poor quality.And this intuition is what Bayesian Adjusted Rating formalizes.Going back to ranking the blogs using some kind of rating derived from the combination of views, likes, and shares, we observe that the simple ranking gives weight to the % of likes and % of shares.For interested viewers, my article ratings are weighted averages of likes, shares, and views.Where,We wont dive into the details of the calculation as my aim here is to introduce you to the Bayesian Adjusted Rating concept. If youre interested in the math behind it, I encourage you to check out resources available online (and there are many).Here, we will jump right to the core of the Bayesian Adjustment to our Rating System:We can then use the new Bayesian Adjusted Ratings to calculate the new ranking. This gives us a more intuitive ranking of the articles compared to the simple average rating.At this point, I would encourage you to pick up a small dataset and try out this concept on your own. Learning the theoretical aspect of data science is good but you wont truly appreciate its value till you actually see it in action. Let me know how your experiments go in the comments section below this article. Would love to discuss further there!So, lets quickly summarize what we have covered here.When we have one or more products with very few ratings, how do they compare to products whose ratings are known with a high level of certainty?Knowing that some products have a higher number of reviews and ratings compared to others, we may think about it as a sliding ruler where, if a product has a lot of reviews, I should trust its own average. But if it got too few reviews relative to others, then I should pretend as if it is an average product across the entire category.And this is exactly what the Bayesian Adjusted Rating does, it shifts the rating somewhere between the simple average of that product and the overall category depending on how much we can trust its own rating.My top two blogs should be my Article #s 3 and 5 respectively based on the pseudo counts of the likes and shares I have received from their views.What are your thoughts on this rating system? Have you seen it in action yourself? I look forward to hearing your thoughts and feedback below!Abir MukherjeeAbir Mukherjee is a research analyst with over a decade of experience in managing credit card marketing, customer servicing, and business development projects. He has worked on multiple analytical solution deliveries for the US market as a part of both onshore and offshore teams. He is a strong believer in simple, contextual and value-driven solutions.Abir has a Masters degree in Industrial Engineering from IIT Kanpur. He earned his Bachelors in Information Technology. Currently based out of Bangalore and is originally from Singrauli, Madhya Pradesh.Outside of his work Abir enjoys painting and graphic design; And fond of traveling to explore new cuisines and cultures.",https://www.analyticsvidhya.com/blog/2019/07/introduction-online-rating-systems-bayesian-adjusted-rating/
DATAMIN  Unveiling the Worlds Biggest Online Data Science Quizzing Platform,"Learn everything about Analytics|So, how do we overcome these 2 challenges?|Datamin  Your Daily Dose of Data Science|Rules of the Game|Gear up for the launch of Datamin","We are thrilled to announce the launch of the worlds biggest online data science quizzing platform: Datamin!||Share this:|Related Articles|Introduction to Bayesian Adjustment Rating: The Incredible Concept Behind Online Ratings!|DataHack Radio #24: Exploring and Designing Chatbots with RASAs Justina Petraityt|
Tavish Srivastava
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you feel some of your peers just do better in their data science career for reasons you cant explain?With all the interest of industries, venture capitalists, governments and various other functions, data science has expanded to a very wide domain. With this expansion, data scientists today need to demonstrate a huge array of skills to excel in the role.A person would need to be:The list keeps on getting bigger as the domain expands. This informative image captures this multi-faceted role:Theres both good news and bad news here.The Good News is that you can pick up any skill you want if you have the right mindset, the discipline and the determination to practice it daily. The Bad News? Well there are broadly two main challenges aspiring data scientists face:The second challenge, of a person coming from a non-data science background, is all too common. I have seen people give up halfway to their goal because they feel they wont ever improve their existing skillset. For instance, a person with no engineering background might feel he/she might never become an awesome data scientist because he might never become good at programming.Sounds like a familiar situation?Here are a couple of solutions for these challenges that I will explain with relatable analogies:Everyone knows Vitamins are essential for our body. If you struggle with any physical/mental illness, do you simply start taking Vitamin pills? No, right? You get your diagnostic tests for various Vitamins and then start the cure.Similarly, a data scientist needs to master a set of skill elements. To understand which element is impeding his/her growth, the data scientist needs to go through a rigorous diagnostic test on various data science aspects.Once you identify your knowledge gap, now what? What if the skill you need to master looks unachievable or simply takes too much time that you dont have?Turns out that the time to learn a new skill OR improve on an existing one is mainly dependent on our state of mind.Looks too abstract? Let me illustrate. In your lifetime, when do you think you had the steepest learning curve? Probably you would have guessed right  when you were a toddler. Can you think of a reason? The main reason was the state of mind. At that age, every new lesson was a game. And learning through a game turns out to be the fastest way you can learn any new skill regardless of your age.We at Analytics Vidhya, combined these concepts and leveraged our expertise in data science to create a new product calledDatamin  Your Daily Dose of Data Science. We are now ready to test the waters and launch the Beta version of the product.Heres Analytics Vidhyas Founder and CEO with the launch announcement:Datamin is a unique data science quizzing platform that has been designed by industry experts. Datamin will:These quizzes will be a week long with almost no limits on the number of questions you attempt. If this was not enough, every competition ends with prizes worth INR 200,000 (USD 3000).The questions you will face will test you broadly on three main categories that are most relevant for data scientists:The beta platform comes with a lot of community engagement levers and modes of giving us LIVE feedback.DATAMIN goes LIVE at 3 pm IST on 25 July 2019 (Thursday). Wear your data science hat, get your gaming self together and get ready for a super fast learning drive.Make sure you register as soon as possible to allow yourself maximum time and maximum attempts. Note that there are no limits on the total number of questions you can attempt, so the only way to maximize your score is to maximize your attempts with high accuracy. Best of luck and happy learning!",https://www.analyticsvidhya.com/blog/2019/07/datamin-unveiling-the-worlds-biggest-online-data-science-quizzing-platform/
DataHack Radio #24: Exploring and Designing Chatbots with RASAs Justina Petraityt,"Learn everything about Analytics|Introduction|Justinas Background, First Role in Data Science & Start with Rasa NLP|The Fascinating Features of Data Science and NLP in Gaming|Introduction to Rasa|Keeping up with the Latest Trends and Developments in Machine Learning and NLP|Interesting Applications of Chatbots you Might Not have Thought of|The Future of Chatbots|End Notes","Share this:|Related Articles|DATAMIN  Unveiling the Worlds Biggest Online Data Science Quizzing Platform|10 Powerful Applications of Linear Algebra in Data Science (with Multiple Resources)|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Chatbots are the most common application of Natural Language Processing (NLP). Organizations are scrambling to integrate chatbots into their daily functions to enhance and personalize our experience. As a data science professional, Im always curious about how these chatbots are built.Rasa is one such open source framework that we can leverage to build our own chatbots. So we are delighted to have Rasas data scientist and Head of Developer Relations, Justina Petraityt, on our DataHack Radio podcast!Justina brings a diverse set of skills and expertise in the NLP domain. In this episode, Kunal and Justina discuss a broad range of topics, including:If youre curious to learn more about how Rasa works in Python, we have the perfect tutorial for you:And subscribe to Analytics Vidhyas DataHack Radio podcast on the below platforms:Im a data scientist at heart.  Justina PetraitytThat quote by Justina essentially encapsulates her career in data science and NLP till date. She completed her Bachelors in Econometrics from Lithuania (her home country). This four-year stint gave her an introduction to data analytics, time series forecasting, and the other core components of data science.Justina was exposed to the data world quite early on in her career (as early as her under-grad days). She worked in digital banking and Fintech as an intern before her first role as a Senior Data Analyst at a UK-based video game company. If that sounds like a dream job  you arent alone!Working at a video game company was like a data scientists dream come true because you have a really, really interesting dataset to work with.She worked on a wide spectrum of machine learning and data analytics related projects there. Take a moment to think about what you would do as a data scientist working with video game data. Now double check that with what Justina worked on:Im sure theres a lot of overlap with what you imagined. Honestly, the incredible far-reaching impact and use of machine learning never ceases to amaze me.This is where Justinas journey with conversational AI (chatbots) started. She worked on a project where she built a bot to automate the analytical reports she used to make for the different department roles in her company (deployed on Slack). So, she improved her knowledge of machine learning and made her company more data-driven. Quite inspiring!This, as you might have pieced together already, is where Justina came across Rasa. She started delving deeper into Rasa, wrote tutorials, attended meetups and continuously worked to improve her already blossoming skillset.That was how she ended up with her current role at Rasa itself. Take note all you budding NLP folks.What were some of the challenging aspects Justina worked on in her gaming AI role? A question all of us were curious to know more about:For those curious about it  Justina worked mostly with supervised machine learning techniques to build her models.The Rasa Stack is a set of open-source NLP tools focused primarily on chatbots. Its one of the most effective and time efficient tools to build complex chatbots in minutes. We love using Rasa at Analytics Vidhya.Justina, as we saw above, started working on Rasa since it was open source and she was looking for a tool to start her chatbot learning. She was quickly won over by the incredible flexibility Rasa offered along with a very supportive user community.I was initially sceptical if Rasa would work as well as Google DialogFlow or wit.ai but when I tried it, it did work as well if not better than these tools!Interesting note here  we can use Rasa to build chatbots in regional languages as well. It isnt just limited to English like the majority of chatbot platforms out there. A very welcome feature indeed!Most approaches use pre-trained vectors to build chatbots. So the limitation of using only those word embeddings resticts the chatbots language to English only. Rasa tackles those challenges by using their own model (called TensorFlow embeddings model).We have a couple of intuitive tutorials which you can use to start your own Rasa journey:Justinas role as the head of developer relations at Rasa means she needs to be on top of the latest developments in machine learning. And that, of course, extends to the NLP field.My personal rule is to learn something new all the time. Whether thats a new technique, or brushing up my programming skills.You can see why Justina is such a successful data scientist. Continuously learning new things and upskiling yourself is a primary quality you must have. Combine that with a never-ending quest for knowledge? You have the perfect combination!The folks at Rasa follow the same approach. They hold regular meetings to discuss the latest developments in NLP (and the field as a whole) where they tackle things like which features to prioritize first, etc.Chatbots are now ubiqiotous. Businesses around the world are relying heavily on these conversational AI agents to propel them into todays digital era. Im sure most of you must have come across these chatbots in your daily routines:But there are other applications of chatbots that might have espaced our attention. Justina provided two excellent examples of these:In general, the perception of what an AI assistant is and what it should be able to do is changing.Justina believes the current go-to approach for building chatbots is quite limited. You can design a chatbot using a set of rules to make it converse in a natural and polite way. But theres scope of expanding beyond that. Essentially, theres no intelligence aspect in play.Making machines understand the context of a sentence is tricky. But we are starting to break down that barrier with breakthroughs like Googles BERT, OpenAIs GPT-2 and Transformer-XL.Justina firmly believes we are soon going to see chatbots evolve to become more contextual and personalized. The fine folks at Rasa are already working on this. They want the chatbot to:Another great addition to the DataHack Radio podcast series. I really liked the diverse set of skills Justina brought to this episode. She is clearly an NLP master and is an eloquent speaker. Her ability to break down seemingly complex topics into non-technical terms is something I really appreciated.What was your favorite aspect on this podcast? And is there any area of guest you would love to hear from on DataHack Radio? Let me know in the comments section below and lets get talking!",https://www.analyticsvidhya.com/blog/2019/07/exploring-designing-chatbots-rasa-justina-petraityte/
10 Powerful Applications of Linear Algebra in Data Science (with Multiple Resources),Learn everything about Analytics|Overview|Introduction|Table of Contents|Why Study Linear Algebra?|Linear Algebra in Machine Learning|Dimensionality Reduction|Natural Language Processing (NLP)|Computer Vision|End Notes,"1. Loss Functions|2. Regularization|3. Covariance Matrix|4. Support Vector Machine Classification|5. Principal Component Analysis (PCA)|6. Singular Value Decomposition|7. Word Embeddings||8. Latent Semantic Analysis (LSA)|9. Image Representation as Tensors|10. Convolution and Image Processing|Share this:|Related Articles|DataHack Radio #24: Exploring and Designing Chatbots with RASAs Justina Petraityt|Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code)|
Khyati Mahendru
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If Data Science was Batman, Linear Algebra would be Robin. This faithful sidekick is often ignored. But in reality, it powers major areas of Data Science including the hot fields of Natural Language Processing and Computer Vision.I have personally seen a LOT of data science enthusiasts skip this subject because they find the math too difficult to understand. When the programming languages for data science offer a plethora of packages for working with data, people dont bother much with linear algebra.Thats a mistake. Linear algebra is behind all the powerful machine learning algorithms we are so familiar with. It is a vital cog in a data scientists skillset. As we will soon see, you should consider linear algebra as a must-know subject in data science.And trust me, Linear Algebra really is all-pervasive! It will open up possibilities of working and manipulating data you would not have imagined before.In this article, I have explained in detail ten awesome applications of Linear Algebra in Data Science. I have broadly categorized the applications into four fields for your reference:I have also provided resources for each application so you can deep dive further into the one(s) which grabs your attention.Note: Before you read on, I recommend going through this superb article  Linear Algebra for Data Science. Its not mandatory for understanding what we will cover here but its a valuable article for your budding skillset.I have come across this question way too many times. Why should you spend time learning Linear Algebra when you can simply import a package in Python and build your model? Its a fair question. So, let me present my point of view regarding this.I consider Linear Algebra as one of the foundational blocks of Data Science. You cannot build a skyscraper without a strong foundation, can you? Think of this scenario:You want to reduce the dimensions of your data using Principal Component Analysis (PCA). How would you decide how many Principal Components to preserve if you did not know how it would affect your data? Clearly, you need to know the mechanics of the algorithm to make this decision.With an understanding of Linear Algebra, you will be able to develop a better intuition for machine learning and deep learning algorithms and not treat them as black boxes. This would allow you to choose proper hyperparameters and develop a better model.You would also be able to code algorithms from scratch and make your own variations to them as well. Isnt this why we love data science in the first place? The ability to experiment and play around with our models? Consider linear algebra as the key to unlock a whole new world.The big question  where does linear algebra fit in machine learning? Lets look at four applications you will all be quite familiar with.You must be quite familiar with how a model, say a Linear Regression model, fits a given data:But wait  how can you calculate how different your prediction is from the expected output? Loss Functions, of course.A loss function is an application of the Vector Norm in Linear Algebra. The norm of a vector can simply be its magnitude. There are many types of vector norms. I will quickly explain two of them: In this 2D space, you could reach the vector (3, 4) by traveling 3 units along the x-axis and then 4 units parallel to the y-axis (as shown). Or you could travel 4 units along the y-axis first and then 3 units parallel to the x-axis. In either case, you will travel a total of 7 units. This distance is calculated using the Pythagoras Theorem (I can see the old math concepts flickering on in your mind!). It is the square root of (3^2 + 4^2), which is equal to 5.But how is the norm used to find the difference between the predicted values and the expected values? Lets say the predicted values are stored in a vector P and the expected values are stored in a vector E. Then P-E is the difference vector. And the norm of P-E is the total loss for the prediction.Regularization is a very important concept in data science. Its a technique we use to prevent models from overfitting. Regularization is actually another application of the Norm.A model is said to overfit when it fits the training data too well. Such a model does not perform well with new data because it has learned even the noise in the training data. It will not be able to generalize on data that it has not seen before. The below illustration sums up this idea really well:Regularization penalizes overly complex models by adding the norm of the weight vector to the cost function. Since we want to minimize the cost function, we will need to minimize this norm. This causes unrequired components of the weight vector to reduce to zero and prevents the prediction function from being overly complex.You can read the below article to learn about the complete mathematics behind regularization:The L1 and L2 norms we discussed above are used in two types of regularization:Refer to our complete tutorial on Ridge and Lasso Regression in Python to know more about these concepts.Bivariate analysis is an important step in data exploration. We want to study the relationship between pairs of variables. Covariance or Correlation are measures used to study relationships between two continuous variables.Covariance indicates the direction of the linear relationship between the variables. A positive covariance indicates that an increase or decrease in one variable is accompanied by the same in another. A negative covariance indicates that an increase or decrease in one is accompanied by the opposite in the other.On the other hand, correlation is the standardized value of Covariance. A correlation value tells us both the strength and direction of the linear relationship and has the range from -1 to 1.Now, you might be thinking that this is a concept of Statistics and not Linear Algebra. Well, remember I told you Linear Algebra is all-pervasive? Using the concepts of transpose and matrix multiplication in Linear Algebra, we have a pretty neat expression for the covariance matrix:Here, X is the standardized data matrix containing all numerical features.I encourage you to read our Complete Tutorial on Data Exploration to know more about the Covariance Matrix, Bivariate Analysis and the other steps involved in Exploratory Data Analysis.Ah yes, support vector machines. One of the most common classification algorithms that regularly produces impressive results. It is an application of the concept of Vector Spaces in Linear Algebra.Support Vector Machine, or SVM, is a discriminative classifier that works by finding a decision surface. It is a supervised machine learning algorithm.In this algorithm, we plot each data item as a point in an n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyperplane that differentiates the two classes very well i.e. with the maximum margin, which is C is this case.A hyperplane is a subspace whose dimensions are one less than its corresponding vector space, so it would be a straight line for a 2D vector space, a 2D plane for a 3D vector space and so on. Again Vector Norm is used to calculate the margin.But what if the data is not linearly separable like the case below?Our intuition says that the decision surface has to be a circle or an ellipse, right? But how do you find it? Here, the concept of Kernel Transformations comes into play. The idea of transformation from one space to another is very common in Linear Algebra.Lets introduce a variable z = x^2 + y^2. This is how the data looks if we plot it along the z and x-axes:Now, this is clearly linearly separable by a line z = a, where a is some positive constant. On transforming back to the original space, we get x^2 + y^2 = a as the decision surface, which is a circle!And the best part? We do not need to add additional features on our own. SVM has a technique called the kernel trick. Read this article on Support Vector Machines to learn about SVM, the kernel trick and how to implement it in Python.You will often work with datasets that have hundreds and even thousands of variables. Thats just how the industry functions. Is it practical to look at each variable and decide which one is more important?That doesnt really make sense. We need to bring down the number of variables to perform any sort of coherent analysis. This is what dimensionality reduction is. Now, lets look at two commonly used dimensionality reduction methods here.Principal Component Analysis, or PCA, is an unsupervised dimensionality reduction technique. PCA finds the directions of maximum variance and projects the data along them to reduce the dimensions.Without going into the math, these directions are the eigenvectors of the covariance matrix of the data.Eigenvectors for a square matrix are special non-zero vectors whose direction does not change even after applying linear transformation (which means multiplying) with the matrix. They are shown as the red-colored vectors in the figure below:You can easily implement PCA in Python using the PCA class in the scikit-learn package:I applied PCA on the Digits dataset from sklearn  a collection of 88 images of handwritten digits. The plot I obtained is rather impressive. The digits appear nicely clustered:Head on to our Comprehensive Guide to 12 Dimensionality Reduction techniques with code in Python for a deeper insight into PCA and 11 other Dimensionality Reduction techniques. It is honestly one of the best articles on this topic you will find anywhere.In my opinion, Singular Value Decomposition (SVD) is underrated and not discussed enough. It is an amazing technique of matrix decomposition with diverse applications. I will try and cover a few of them in a future article.For now, let us talk about SVD in Dimensionality Reduction. Specifically, this is known as Truncated SVD.Source: hadrienj.github.ioSource: researchgate.netHere is the code to implement truncated SVD in Python (its quite similar to PCA):On applying truncated SVD to the Digits data, I got the below plot. Youll notice that its not as well clustered as we obtained after PCA:Natural Language Processing (NLP) is the hottest field in data science right now. This is primarily down to major breakthroughs in the last 18 months. If you were still undecided on which branch to opt for  you should strongly consider NLP.So lets see a couple of interesting applications of linear algebra in NLP. This should help swing your decision!Machine learning algorithms cannot work with raw textual data. We need to convert the text into some numerical and statistical features to create model inputs. There are many ways for engineering features from text data, such as:Word Embeddings is a way of representing words as low dimensional vectors of numbers while preserving their context in the document. These representations are obtained by training different neural networks on a large amount of text which is called a corpus. They also help in analyzing syntactic similarity among words:Word2Vec and GloVe are two popular models to create Word Embeddings.I trained my model on the Shakespeare corpus after some light preprocessing using Word2Vec and obtained the word embedding for the word world:Pretty cool! But whats even more awesome is the below plot I obtained for the vocabulary. Observe that syntactically similar words are closer together. I have highlighted a few such clusters of words. The results are not perfect but they are still quite amazing:There are several other methods to obtain Word Embeddings. Read our article for An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec.What is your first thought when you hear this group of words  prince, royal, king, noble? These very different words are almost synonymous.Now, consider the following sentences:The word pitcher has different meanings based on the other words in the two sentences. It means a baseball player in the first sentence and a jug of juice in the second.Both these sets of words are easy for us humans to interpret with years of experience with the language. But what about machines? Here, the NLP concept of Topic Modeling comes into play:Topic Modeling is an unsupervised technique to find topics across various text documents. These topics are nothing but clusters of related words. Each document can have multiple topics. The topic model outputs the various topics, their distributions in each document, and the frequency of different words it contains.Latent Semantic Analysis (LSA), or Latent Semantic Indexing, is one of the techniques of Topic Modeling. It is another application of Singular Value Decomposition.Latent means hidden. True to its name, LSA attempts to capture the hidden themes or topics from the documents by leveraging the context around the words.I will describe the steps in LSA in short so make sure you check out this Simple Introduction to Topic Modeling using Latent Semantic Analysis with code in Python for a proper and in-depth understanding.For a hands-on experience with Natural Language Processing, you can check out our course on NLP using Python. The course is beginner-friendly and you get to build 5 real-life projects!Another field of deep learning that is creating waves  Computer Vision. If youre looking to expand your skillset beyond tabular data (and you should), then learn how to work with images.This will broaden your current understanding of machine learning and also help you crack interviews quickly.How do you account for the vision in Computer Vision? Obviously, a computer does not process images as humans do. Like I mentioned earlier, machine learning algorithms need numerical features to work with.A digital image is made up of small indivisible units called pixels. Consider the figure below:This grayscale image of the digit zero is made of 8 x 8 = 64 pixels. Each pixel has a value in the range 0 to 255. A value of 0 represents a black pixel and 255 represents a white pixel.Conveniently, an m x n grayscale image can be represented as a 2D matrix with m rows and n columns with the cells containing the respective pixel values:But what about a colored image? A colored image is generally stored in the RGB system. Each image can be thought of as being represented by three 2D matrices, one for each R, G and B channel. A pixel value of 0 in the R channel represents zero intensity of the Red color and of 255 represents the full intensity of the Red color.Each pixel value is then a combination of the corresponding values in the three channels:In reality, instead of using 3 matrices to represent an image, a tensor is used. A tensor is a generalized n-dimensional matrix. For an RGB image, a 3rd ordered tensor is used. Imagine it as three 2D matrices stacked one behind another:Source: slidesharecdn2D Convolution is a very important operation in image processing. It consists of the below steps:The function can seem a bit complex but its widely used for performing various image processing operations like sharpening and blurring the images and edge detection. We just need to know the right kernel for the task we are trying to accomplish. Here are a few kernels you can use:You can download the image I used and try these image processing operations for yourself using the code and the kernels above. Also, try this Computer Vision tutorial on Image Segmentation techniques!Amazing, right? This is by far my most favorite application of Linear Algebra in Data Science.Now that you are acquainted with the basics of Computer Vision, it is time to start your Computer Vision journey with 16 awesome OpenCV functions. We also have a comprehensive course on Computer Vision using Deep Learning in which you can work on real-life Computer Vision case studies!My aim here was to make Linear Algebra a bit more interesting than you might have imagined previously. Personally for me, learning about applications of a subject motivates me to learn more about it.I am sure you are as impressed with these applications as I am. Or perhaps you know of some other applications that I could add to the list? Let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/
Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code),Learn everything about Analytics|Overview|Introduction|Table of Contents|A Brief Overview of Image Segmentation|Understanding Mask R-CNN|Steps to implement Mask R-CNN|Implementing Mask R-CNN in Python|End Notes,"Step 1: Clone the repository|Step 2: Install the dependencies|Step 3: Download the pre-trained weights (trained on MS COCO)|Step 4: Predicting for our image|Share this:|Related Articles|10 Powerful Applications of Linear Algebra in Data Science (with Multiple Resources)|Introduction to PyTorch-Transformers: An Incredible Library for State-of-the-Art NLP (with Python code)|
Pulkit Sharma
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
","Backbone Model|Region Proposal Network (RPN)
|Region of Interest (RoI)
|Segmentation Mask
|Loading Weights|Making Predictions|Inferences",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I am fascinated by self-driving cars. The sheer complexity and mix of different computer vision techniques that go into building a self-driving car system is a dream for a data scientist like me.So, I set about trying to understand the computer vision technique behind how a self-driving car potentially detects objects. A simple object detection framework might not work because it simply detects an object and draws a fixed shape around it.Thats a risky proposition in a real-world scenario. Imagine if theres a sharp turn in the road ahead and our system draws a rectangular box around the road. The car might not be able to understand whether to turn or go straight. Thats a potential disaster!Instead, we need a technique that can detect the exact shape of the road so our self-driving car system can safely navigate the sharp turns as well.The latest state-of-the-art framework that we can use to build such a system? Thats Mask R-CNN!So, in this article, we will first quickly look at what image segmentation is. Then well look at the core of this article  the Mask R-CNN framework. Finally, we will dive into implementing our own Mask R-CNN model in Python. Lets begin!We learned the concept of image segmentation in part 1 of this series in a lot of detail. We discussed what is image segmentation and its different techniques, like region-based segmentation, edge detection segmentation, and segmentation based on clustering.I would recommend checking out that article first if you need a quick refresher (or want to learn image segmentation from scratch).Ill quickly recap that article here. Image segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image. The image shown below will help you to understand what image segmentation is:Here, you can see that each object (which are the cells in this particular image) has been segmented. This is how image segmentation works.We also discussed the two types of image segmentation: Semantic Segmentation and Instance Segmentation. Again, lets take an example to understand both of these types:All 5 objects in the left image are people. Hence, semantic segmentation will classify all the people as a single instance. Now, the image on the right also has 5 objects (all of them are people). But here, different objects of the same class have been assigned as different instances. This is an example of instance segmentation.Part one covered different techniques and their implementation in Python to solve such image segmentation problems. In this article, we will be implementing a state-of-the-art image segmentation technique called Mask R-CNN to solve an instance segmentation problem.Mask R-CNN is basically an extension of Faster R-CNN. Faster R-CNN is widely used for object detection tasks. For a given image, it returns the class label and bounding box coordinates for each object in the image. So, lets say you pass the following image:The Fast R-CNN model will return something like this:The Mask R-CNN framework is built on top of Faster R-CNN. So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask.Lets first quickly understand how Faster R-CNN works. This will help us grasp the intuition behind Mask R-CNN as well.Once you understand how Faster R-CNN works, understanding Mask R-CNN will be very easy. So, lets understand it step-by-step starting from the input to predicting the class label, bounding box, and object mask.Similar to the ConvNet that we use in Faster R-CNN to extract feature maps from the image, we use the ResNet 101 architecture to extract features from the images in Mask R-CNN. So, the first step is to take an image and extract features using the ResNet 101 architecture. These features act as an input for the next layer.Now, we take the feature maps obtained in the previous step and apply a region proposal network (RPM). This basically predicts if an object is present in that region (or not). In this step, we get those regions or feature maps which the model predicts contain some object.The regions obtained from the RPN might be of different shapes, right? Hence, we apply a pooling layer and convert all the regions to the same shape. Next, these regions are passed through a fully connected network so that the class label and bounding boxes are predicted.Till this point, the steps are almost similar to how Faster R-CNN works. Now comes the difference between the two frameworks. In addition to this, Mask R-CNN also generates the segmentation mask.For that, we first compute the region of interest so that the computation time can be reduced. For all the predicted regions, we compute the Intersection over Union (IoU) with the ground truth boxes. We can computer IoU like this:IoU = Area of the intersection / Area of the unionNow, only if the IoU is greater than or equal to 0.5, we consider that as a region of interest. Otherwise, we neglect that particular region. We do this for all the regions and then select only a set of regions for which the IoU is greater than 0.5.Lets understand it using an example. Consider this image:Here, the red box is the ground truth box for this image. Now, lets say we got 4 regions from the RPN as shown below:Here, the IoU of Box 1 and Box 2 is possibly less than 0.5, whereas the IoU of Box 3 and Box 4 is approximately greater than 0.5. Hence. we can say that Box 3 and Box 4 are the region of interest for this particular image whereas Box 1 and Box 2 will be neglected.Next, lets see the final step of Mask R-CNN.Once we have the RoIs based on the IoU values, we can add a mask branch to the existing architecture. This returns the segmentation mask for each region that contains an object. It returns a mask of size 28 X 28 for each region which is then scaled up for inference.Again, lets understand this visually. Consider the following image:The segmentation mask for this image would look something like this:Here, our model has segmented all the objects in the image. This is the final step in Mask R-CNN where we predict the masks for all the objects in the image.Keep in mind that the training time for Mask R-CNN is quite high. It took me somewhere around 1 to 2 days to train the Mask R-CNN on the famous COCO dataset. So, for the scope of this article, we will not be training our own Mask R-CNN model.We will instead use the pretrained weights of the Mask R-CNN model trained on the COCO dataset. Now, before we dive into the Python code, lets look at the steps to use the Mask R-CNN model to perform instance segmentation.Its time to perform some image segmentation tasks! We will be using the mask rcnn framework created by the Data scientists and researchers at Facebook AI Research (FAIR).Lets have a look at the steps which we will follow to perform image segmentation using Mask R-CNN.First, we will clone the mask rcnn repository which has the architecture for Mask R-CNN. Use the following command to clone the repository:Once this is done, we need to install the dependencies required by Mask R-CNN.Here is a list of all the dependencies for Mask R-CNN:You must install all these dependencies before using the Mask R-CNN framework.Next, we need to download the pretrained weights. You can use this link to download the pre-trained weights. These weights are obtained from a model that was trained on the MS COCO dataset. Once you have downloaded the weights, paste this file in the samples folder of the Mask_RCNN repository that we cloned in step 1.Finally, we will use the Mask R-CNN architecture and the pretrained weights to generate predictions for our own images.Once youre done with these four steps, its time to jump into your Jupyter Notebook! We will implement all these things in Python and then generate the masks along with the classes and bounding boxes for objects in our images.Sp, are you ready to dive into Python and code your own image segmentation model? Lets begin!To execute all the code blocks which I will be covering in this section, create a new Python notebook inside the samples folder of the cloned Mask_RCNN repository.Lets start by importing the required libraries:Next, we will define the path for the pretrained weights and the images on which we would like to perform segmentation:If you have not placed the weights in the samples folder, this will again download the weights. Now we will create an inference class which will be used to infer the Mask R-CNN model:What can you infer from the above summary? We can see the multiple specifications of the Mask R-CNN model that we will be using.So, the backbone is resnet101 as we have discussed earlier as well. The mask shape that will be returned by the model is 28X28, as it is trained on the COCO dataset. And we have a total of 81 classes (including the background).We can also see various other statistics as well, like:You should spend a few moments and understand these specifications. If you have any doubts regarding these specifications, feel free to ask me in the comments section below.Next, we will create our model and load the pretrained weights which we downloaded earlier. Make sure that the pretrained weights are in the same folder as that of the notebook otherwise you have to give the location of the weights file:Now, we will define the classes of the COCO dataset which will help us in the prediction phase:Lets load an image and try to see how the model performs. You can use any of your images to test the model.This is the image we will work with. You can clearly identify that there are a couple of cars (one in the front and one in the back) along with a bicycle.Its prediction time! We will use the Mask R-CNN model along with the pretrained weights and see how well it segments the objects in the image. We will first take the predictions from the model and then plot the results to visualize them:Interesting. The model has done pretty well to segment both the cars as well as the bicycle in the image. We can look at each mask or the segmented objects separately as well. Lets see how we can do that.I will first take all the masks predicted by our model and store them in the mask variable. Now, these masks are in the boolean form (True and False) and hence we need to convert them to numbers (1 and 0). Lets do that first:Output:This will give us an array of 0s and 1s, where 0 means that there is no object at that particular pixel and 1 means that there is an object at that pixel. Note that the shape of the mask is similar to that of the original image (you can verify that by printing the shape of the original image).However, the 3 here in the shape of the mask does not represent the channels. Instead, it represents the number of objects segmented by our model. Since the model has identified 3 objects in the above sample image, the shape of the mask is (480, 640, 3). Had there been 5 objects, this shape would have been (480, 640, 5).We now have the original image and the array of masks. To print or get each segment from the image, we will create a for loop and multiply each mask with the original image to get each segment:This is how we can plot each mask or object from the image. This can have a lot of interesting as well as useful use cases. Getting the segments from the entire image can reduce the computation cost as we do not have to preprocess the entire image now, but only the segments.Below are a few more results which I got using our Mask R-CNN model:Looks awesome! You have just built your own image segmentation model using Mask R-CNN  well done.I love working with this awesome Mask R-CNN framework. Perhaps I will now try to integrate that into a self-driving car system. Image segmentation has a wide range of applications, ranging from the healthcare industry to the manufacturing industry. I would suggest you try this framework on different images and see how well it performs. Feel free to share your results with the community.In case you have any questions, doubts or feedback regarding the article, do post them in the comments section below.",https://www.analyticsvidhya.com/blog/2019/07/computer-vision-implementing-mask-r-cnn-image-segmentation/
Introduction to PyTorch-Transformers: An Incredible Library for State-of-the-Art NLP (with Python code),"Learn everything about Analytics|Overview|Introduction|Table of Contents|Demystifying State-of-the-Art in NLP|What is PyTorch-Transformers?|Installing PyTorch-Transformers on your Machine|Predicting the next word using GPT-2|Natural Language Generation using GPT-2, Transformer-XL and XLNet|Training a Masked Language Model for BERT|Analytics Vidhyas take on PyTorch-Transformers","GPT-2|XLNet|Transformer-XL|Share this:|Related Articles|Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code)|How to Get Started with NLP  6 Unique Methods to Perform Tokenization|
Mohd Sanad Zaki Rizvi
|14 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Problem Definition,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"NLPs ImageNet moment has arrived.  Sebastian RuderImagine having the power to build the Natural Language Processing (NLP) model that powers Google Translate. What if I told you this can be done using just a few lines of code in Python? Sounds like an incredibly exciting opportunity.Well  we can now do this sitting in front of our own machines! The latest state-of-the-art NLP release is called PyTorch-Transformers by the folks at HuggingFace. This PyTorch-Transformers library was actually released just yesterday and Im thrilled to present my first impressions along with the Python code.The ability to harness this research would have taken a combination of years, some of the best minds, as well as extensive resources to be created. And we get to simply import it in Python and experiment with it. What a time to be alive!I am truly astonished at the speed of research and development in NLP nowadays. Every new paper/framework/library just pushes the boundary of this incredibly powerful field. And due to the open culture of research around AI and large amounts of freely available text data, there is almost nothing that we cant do today.Now, I cant stress enough the impact that PyTorch-Transformers will have on the research community as well as the NLP industry. I believe this has the potential to revolutionize the landscape of NLP as we know it.Essentially, Natural Language Processing is about teaching computers to understand the intricacies of human language.Before we get into the technical details of PyTorch-Transformers, lets quickly revisit the very concept on which the library is built  NLP. Well also understand what state-of-the-art means as that will set the context for the article.Here are a few things that you need to know before we start with PyTorch-Transformers:Note:This article is going to be full of Transformers so Id highly recommend that you read the below guide in case you need a quick refresher:PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).I have taken this section from PyTorch-Transformers documentation. This library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:All of the above models are the best in class for various NLP tasks. Some of these models are as recent as the previous month!Most of the State-of-the-Art models require tons of training data and days of training on expensive GPU hardware which is something only the big technology companies and research labs can afford. But with the launch of PyTorch-Transformers, now anyone can utilize the power of State-of-the-Art models!Installing Pytorch-Transformers is pretty straightforward in Python. You can just use pip install:or if you are working on Colab:Since most of these models are GPU heavy, I would suggest working with Google Colab for this article.Note:The code in this article is written using thePyTorch framework.Because PyTorch-Transformers supports many NLP models that are trained for Language Modelling, it easily allows for natural language generation tasks like sentence completion.In February 2019, OpenAI created quite the storm through their release of a new transformer-based language model called GPT-2. GPT-2 is a transformer-based generative language model that wastrained on 40GB of curated text from the internet.Being trained in an unsupervised manner, it simply learns to predict a sequence of most likely tokens (i.e. words) that follow a given prompt, based on the patterns it learned to recognize through its training.Lets build our own sentence completion model using GPT-2. Well try to predict the next word in the sentence:I chose this example because this is the first suggestion that Googles text completion gives. Here is the code for doing the same:The code is straightforward. We tokenize and index the text as a sequence of numbers and pass it to the GPT2LMHeadModel. This is nothing but the GPT2 model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).Awesome! The model successfully predicts the next word as world. This is pretty amazing as this is what Google was suggesting. I recommend you try this model with different input sentences and see how it performs while predicting the next word in a sentence.Lets take Text Generation to the next level now. Instead of predicting only the next word, we will generate a paragraph of text based on the given input. Lets see what output our models give for the following input text:We will be using the readymade script that PyTorch-Transformers provides for this task. Lets clone their repository first:Now, you just need a single command to start the model!Lets see what output our GPT-2 model gives for the input text:Isnt that crazy? The text that the model generated is very cohesive and actually can be mistaken as a real news article.XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin. XLNet achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.You can use the following code for the same:This is the output that XLNet gives:Interesting. While the GPT-2 model focussed directly on the scientific angle of the news about unicorns, XLNet actually nicely built up the context and subtly introduced the topic of unicorns. Lets see how does Transformer-XL performs!Transformer networks are limited by a fixed-length context and thus can be improved through learning longer-term dependency. Thats why Google proposed a novel method called Transformer-XL (meaning extra long) for language modeling, which enables a Transformer architecture to learn longer-term dependency.Transformer-XL is up to 1800 times faster than a typical Transformer.You can use the below code to run Transformer-XL:Heres the text generated:Now, this is awesome. It is interesting to see how different models focus on different aspects of the input text to generate further. This variation is due to a lot of factors but mostly can be attributed to different training data and model architectures.But theres a caveat. Neural text generation has been facing a bit of backlash in recent times as people worry it can increase problems related to fake news. But think about the positive side of it! We can use it for many positive applications like- helping writers/creatives with new ideas, and so on.The BERT framework, a new language representation model from Google AI, uses pre-training and fine-tuning to create state-of-the-art NLP models for a wide range of tasks. These tasks include question answering systems, sentiment analysis, and language inference.BERT is pre-trained using the following two unsupervised prediction tasks:And you can implement both of these using PyTorch-Transformers. In fact, you can build your own BERT model from scratch or fine-tune a pre-trained version. So, lets see how can we implement the Masked Language Model for BERT.Lets formally define our problem statement:Given an input sequence, we will randomly mask some words. The model then should predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.So why are we doing this? The model learns the rules of the language during the training process. And well soon see how effective this process is.First, lets prepare a tokenized input from a text string using BertTokenizer:This is how our text looks like after tokenization:The next step would be to convert this into a sequence of integers and create PyTorch tensors of them so that we can use them directly for computation:Notice that we have set [MASK] at the 8th index in the sentence which is the word Hensen. This is what our model will try to predict.Now that our data is rightly pre-processed for BERT, we will create a Masked Language Model. Lets now use BertForMaskedLMto predict a masked token:Lets see what is the output of our model:Thats quite impressive.This was a small demo of training a Masked Language Model on a single input sequence. Nevertheless, it is a very important part of the training process for many Transformer-based architectures. This is because it allows bidirectional training in models  which was previously impossible.Congratulations! Youve just implemented your first Masked Language Model! If you were trying to train BERT, you just finished half your work. This example will have given you a good idea of how to use PyTorch-Transformers to work with the BERT model.In this article, we implemented and explored various State-of-the-Art NLP models like BERT, GPT-2, Transformer-XL, and XLNet using PyTorch-Transformers. This was more like a firest impressions expertiment that I did to give you a good intuition on how to work with this amazing library.Here are 6 compelling reasons why I think you would love this library:Have you ever implemented State-of-the-Art models like BERT and GPT-2? Whats your first take on PyTorch-Transformers? Lets discuss in the comments section below.",https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/
How to Get Started with NLP  6 Unique Methods to Perform Tokenization|,Learn everything about Analytics|Overview|Introduction|Table of Contents|What is Tokenization in NLP?|Why is Tokenization required in NLP?|Methods to Perform Tokenization in Python|End Notes,"1. Tokenization using Pythons split() function|2. Tokenization using Regular Expressions (RegEx)|3. Tokenization using NLTK|4. Tokenization using the spaCy library|5. Tokenization using Keras|6. Tokenization using Gensim|Share this:|Related Articles|Introduction to PyTorch-Transformers: An Incredible Library for State-of-the-Art NLP (with Python code)|Heroes of Machine Learning  Top Experts and Researchers you should follow|
Shubham Singh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but arent sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.So how can we manipulate and clean this text data to build a model? The answer lies in the wonderful world of Natural Language Processing (NLP).Solving an NLP problem is a multi-stage process. We need to clean the unstructured text data first before we can even think about getting to the modeling stage. Cleaning the data consists of a few key steps:In this article, we will talk about the very first step  tokenization. We will first see what tokenization is and why its required in NLP. We will then look at six unique ways to perform tokenization in Python.This article has no prerequisites. Anyone with an interest in NLP or data science will be able to follow along. If youre looking for an end-to-end resource for learning NLP, you should check out our comprehensive course:Tokenization is one of the most common tasks when it comes to working with text data. But what does the term tokenization actually mean?Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.Check out the below image to visualize this definition:The tokens could be words, numbers or punctuation marks. In tokenization, smaller units are created by locating word boundaries. Wait  what are word boundaries?These are the ending point of a word and the beginning of the next word. These tokens are considered as a first step for stemming and lemmatization (the next stage in text preprocessing which we will cover in the next article).I want you to think about the English language here. Pick up any sentence you can think of and hold that in your mind as you read this section. This will help you understand the importance of tokenization in a much easier manner.Before processing a natural language, we need to identify the words that constitute a string of characters. Thats why tokenization is the most basic step to proceed with NLP (text data). This is important because the meaning of the text could easily be interpreted by analyzing the words present in the text. Lets take an example. Consider the below string: This is a cat.What do you think will happen after we perform tokenization on this string? We get [This, is, a, cat].There are numerous uses of doing this. We can use this tokenized form to:And so on. We can extract a lot more information which well discuss in detail in future articles. For now, its time to dive into the meat of this article  the different methods of performing tokenization in NLP.
We are going to look at six unique ways we can perform tokenization on text data. I have provided the Python code for each method so you can follow along on your own machine.Lets start with the split() method as it is the most basic one. It returns a list of strings after breaking the given string by the specified separator. By default, split() breaks a string at each space. We can change the separator to anything. Lets check it out.Word TokenizationSentence TokenizationThis is similar to word tokenization. Here, we study the structure of sentences in the analysis. A sentence usually ends with a full stop (.), so we can use . as a separator to break the string:One major drawback of using Pythons split() method is that we can use only one separator at a time. Another thing to note  in word tokenization, split() did not consider punctuation as a separate token.First, lets understand what a regular expression is. It is basically a special character sequence that helps you match or find other strings or sets of strings using that sequence as a pattern.We can use the re library in Python to work with regular expression. This library comes preinstalled with the Python installation package.Now, lets perform word tokenization and sentence tokenization keeping RegEx in mind.Word TokenizationHere, we have an edge over the split() method as we can pass multiple separators at the same time. In the above code, we used the re.compile() function wherein we passed [.?!]. This means that sentences will split as soon as any of these characters are encountered.Interested in reading more about RegEx? The below resources will get you started with Regular Expressions in NLP:Now, this is a library you will appreciate the more you work with text data. NLTK, short for Natural Language ToolKit, is a library written in Python for symbolic and statistical Natural Language Processing.You can install NLTK using the below code:NLTK contains a module called tokenize() which further classifies into two sub-categories:Lets see both of these one-by-one.Word TokenizationSentence TokenizationI love the spaCy library. I cant remember the last time I didnt use it when I was working on an NLP project. It is just that useful.spaCy is an open-source library for advanced Natural Language Processing (NLP). It supports over 49+ languages and provides state-of-the-art computation speed.To install Spacy in Linux:To install it on other operating systems, go through this link.So, lets see how we can utilize the awesomeness of spaCy to perform tokenization. We will use spacy.lang.en which supports the English language.Word TokenizationSentence TokenizationspaCy is quite fast as compared to other libraries while performing NLP tasks (yes, even NLTK). I encourage you to listen to the below DataHack Radio podcast to know the story behind how spaCy was created and where you can use it:And heres an in-depth tutorial to get you started with spaCy:Keras! One of the hottest deep learning frameworks in the industry right now. It is an open-source neural network library for Python. Keras is super easy to use and can also run on top of TensorFlow.In the NLP context, we can use Keras for cleaning the unstructured text data that we typically collect.You can install Keras on your machine using just one line of code:Lets get cracking. To perform word tokenization using Keras, we use the text_to_word_sequence method from the keras.preprocessing.text class.Lets see Keras in action.Word TokenizationKeras lowers the case of all the alphabets before tokenizing them. That saves us quite a lot of time as you can imagine!The final tokenization method we will cover here is using the Gensim library. It is an open-source library for unsupervised topic modeling and natural language processing and is designed to automatically extract semantic topics from a given document.Heres how you can install Gensim:We can use the gensim.utils class to import the tokenize method for performing word tokenization.Word TokenizationSentence TokenizationTo perform sentence tokenization, we use the split_sentences method from the gensim.summerization.texttcleaner class:You might have noticed that Gensim is quite strict with punctuation. It splits whenever a punctuation is encountered. In sentence splitting as well, Gensim tokenized the text on encountering \n while other libraries ignored it.Tokenization is a critical step in the overall NLP pipeline. We cannot simply jump into the model building part without cleaning the text first.In this article, we saw six different methods of tokenization (word as well as a sentence) from a given text. There are other ways as well but these are good enough to get you started on the topic.Ill be covering other text cleaning steps like removing stopwords, part-of-speech tagging, and recognizing named entities in my future posts. Till then, keep learning!",https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/
Heroes of Machine Learning  Top Experts and Researchers you should follow,Learn everything about Analytics|Overview|Introduction|Our Framework for picking these Machine Learning Experts|1. Geoffrey Hinton|2. Michael I Jordan|3. Andrew Ng|4. Yann LeCun|5. Yoshua Bengio|6. Jrgen Schmidhuber|7. Terry Sejnowski|8. David M. Blei|9. Daphne Koller|10. Zoubin Ghahramani|11. Sebastian Thrun|12. Yaser S. Abu-Mostafa|13. Peter Norvig|14. Trevor Hastie|15. Robert Tibshirani|16. Anil K. Jain|17. Jitendra Malik|18. Vladimir Vapnik|19. Ian Goodfellow|20. Andrej Karpathy|21. Fei-Fei Li|22. Jeremy Howard|23. Rachel Thomas|End Notes,"Why should you follow these experts?|Share this:|Related Articles|How to Get Started with NLP  6 Unique Methods to Perform Tokenization|Popular Machine Learning Applications and Use Cases in our Daily Life|
Analytics Vidhya Content Team
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What a time this is to be working in the machine learning field! The last few years have been a dream run for anyone associated with machine learning as there have been a slew of developments and breakthroughs at an unprecedented pace.Theres just one thing to keep in mind here  these breakthroughs did not happen overnight. It took years and in some cases, decades, of hard work and persistence.We are used to working with established machine learning algorithms like neural networks and random forest (and so on). We tend to lose sight of the effort it took to make these algorithms mainstream. To actually create them from scratch. The people who lay the groundwork for us  those are the true heroes of machine learning.We at Analytics Vidhya salute these heroes who have blazed a trail for this modern era of machine learning. Come join us as we celebrate these experts and their groundbreaking achievements!This isnt your typical influencers list. Our selection isnt based on who has more followers on social media, or some similar banal metric. Heres our simple framework:Machine Learning and Deep Learning is a fast evolving area. We expect this to continue in the coming years. If you want to learn about the latest developments in the field, gain perspective from the best brains and be future proof  there cannot be a better way but to follow these experts!In order to make it easy for you  we have provided links to their profiles which you should follow! Just click on each name to head over to their profiles.Now, coming to the Hall of Fame..Who else would be top of any machine learning list? Geoffrey Hinton is an Emeritus Distinguished Professor at the University of Toronto and a Google Brain researcher.He is best known for his work on artificial neural networks (ANNs). His contributions in the field of deep learning are a key reason behind the success of the field and he is often called the Godfather of Deep Learning (with good reason). His research on the backpropagation algorithm brought about a drastic change in the performance of deep learning models.Mr. Hintons other notable research works are Boltzmann machines and Capsule neural networks. Both major breakthroughs in our field.Hinton recently won the 2018 Turing Award for his groundbreaking work around deep neural networks, along with Yann LeCun and Yoshua Bengio. He has also won the BBVA Foundation Frontiers of Knowledge Award (2016) and IEEE/RSE Wolfson James Clerk Maxwell Award.Michael Jordan is a professor at the University of California, Berkeley. His areas of research are machine learning, statistics, and deep learning. He has been a major advocate of Bayesian networks and has made a significant contribution towards probabilistic graphical models, spectral methods, natural language processing (NLP), and much more.He has won many well-known awards, including the IEEE Neural Networks Pioneer Award, the best paper award (with R. Jacobs) at the American Control Conference (ACC 1991) and the ACM  AAAI Allen Newell Award. He has also been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics.The below talk he gave at SysML is a MUST-WATCH for anyone in machine learning. It gives a good overview of the field and puts the recent hype into perspective.Andrew Ng is probably the most recognizable name in this list, at least to machine learning enthusiasts. He is considered as one of the most significant researchers in Machine Learning and Deep Learning in todays time.He is the co-founder of Coursera and deeplearning.ai and an Adjunct Professor of Computer Science at Stanford University. Professor Andrew also co-founded the Google Brain project and was previously the Chief Scientist at Baidu.His aim is to democratize deep learning and give everyone in the world access to high-quality education for free. His online courses on machine learning and deep learning are highly sought after.Andrew has an exceptional track record as an academic researcher  he has over 300 published papers in machine learning and robotics! He is also a recipient of prestigious awards like IJCAI Computers and Thought award, ICML Best Paper Award, ACL Best Paper Award and many, many more.Yann LeCun is another iconic name in machine learning. He is a professor, researcher, and R&D manager with academic and industry experience in machine learning, deep learning, computer vision, and robotics.Mr. LeCun is currently the Chief AI Scientist and VP at Facebook.Yann LeCun is the founding father of convolutional nets. He made convolutional neural networks work with backpropagation, which is widely used in computer vision applications. And thats just scraping the surface of what this expert is capable of.He has over 150 papers published under his name and has received a number of awards for his contributions. He has won the 2014 IEEE Neural Network Pioneer Award and the 2015 PAMI Distinguished Researcher Award. LeCun also won the 2018 Turing award, along with Geoffrey Hinton and Yoshua Bengio.Yoshua Bengio is a professor at the Department of Computer Science and Operations Research at the Universit de Montral. He is also the co-founder of Element AI, a Montreal-based business incubator that seeks to transform AI research into real-world business applications.Yoshua is well known for his work on artificial neural networks and deep learning in the 1980s and 1990s. He co-created the prestigious ICLR conference with Yann LeCun. He is one of the most-cited computer scientists in the areas of deep learning, recurrent networks, probabilistic learning, and natural language.There is probably no topic in deep learning that Yoshua hasnt touched and thats why his contribution to the field of deep learning is quite diverse as compared to his contemporaries.He has received the prestigious award of Canada Research Chair in Statistical Learning Algorithms and also won the 2018 Turing award. Do check out his talk on Deep Learning below:If you havent heard of Jrgen Schmidhuber yet  rectify that immediately! He is a computer scientist, known for his work around artificial neural networks and deep learning. His lifetime goal is to build a self-improving Artificial Intelligence smarter than himself.He, along with some of his students, published sophisticated versions of long short-term memory (LSTM), an improved version of recurrent neural networks. His research work also included the speeding up of convolutional neural networks using GPUs.Mr. Schmidhuber is the recipient of numerous awards, author of over 350 peer-reviewed papers, and Chief Scientist of the company NNAISENSE, which aims at building the first practical general-purpose AI. He is also advising various governments on AI strategies.Terry is a professor at the Salk Institute for Biological Studies and the author of The Deep Learning Revolution(MIT Press). He is one of the pioneers of neural networks back in the 1980s. Along with Geoffrey Hinton, he demonstrated that simple neural networks could be useful and made to learn certain tasks.He is the co-inventor of the Boltzmann machine along with Geoffery Hinton and contributed immensely in solving problems related to speech and vision. Terry is also the co-creator of the algorithm for Independent Component Analysis that has been widely used in machine learning and signal processing.He received the Hebb Prize for his contributions to learning algorithms by the International Neural Network Society in 1999. He also received IEEEs Neural Network Pioneer Award in 2002. In 2017, he was elected to the National Academy of Inventors.David is a professor of Statistics and Computer Science at Columbia University. His research interests lie in topic models, probabilistic modeling, and approximate Bayesian inference.He was one of the original developers of the popular topic modeling technique, Latent Dirichlet Allocation (LDA), along with Andrew Ng and Michael I. Jordan. His research work revolves around recommendation systems, neuroscience, computational social sciences, and natural language.He is the recipient of the ICML Test of Time Award (for Dynamic Topic Models) 2016, the Presidential Award for Outstanding Teaching, and many more. Apart from that, he has published over 100 papers.This lecture by Blei on Topic Models is a gem. I have already bookmarked it.Daphne Koller is a Professor in the Department of Computer Science at Stanford University and one of the founders of Coursera. She received both her bachelors degree and a masters degree from the Hebrew University of Jerusalem. Then she went on to complete her Ph.D. at Stanford in 1993.Her areas of interest are computer vision and computational biology. She even co-authored a book on probabilistic graphical models along with Nir Friedman. After leaving Coursera in 2016, she founded a drug discovery startup called InsitroThe online education model of Stanford was her idea that she initiated in 2010. It has led to the formation of the open-for-all online courses that are being offered by Stanford.She was awarded the Arthur Samuel Thesis Award in 1994, Presidential Early Career Award for Scientists and Engineers (PECASE) in 1999 and in 2011 was elected as a member of the National Academy of Engineering, an American non-governmental organization.Zoubin is a professor of Information Engineering at the University of Cambridge. His research interests include Bayesian approaches to machine learning, statistics, information retrieval, bioinformatics, and artificial intelligence.He completed his Ph.D. from the Department of Brain and Cognitive Sciences at the Massachusetts Institute of Technology, under Michael I. Jordan and Tomaso Poggio. In 2014, he co-founded a startup, Geometric Intelligence that focusses on object or scenario recognition.Later, Uber acquired Geometric Intelligence and Zoubin joined Ubers A.I. Labs in 2016. He has published over 250 research papers and was elected Fellow of the Royal Society (FRS) in 2015.Another very popular name on our list. Sebastian Thrun is currently the CEO of Kitty Hawk Corporation and the co-founder of Udacity. But were sure youve heard his name before all these things.Sebastian founded the Google X Lab and Googles self-driving team. He led the project from the start and is widely considered a leader when it comes to autonomous vehicles. He has developed multiple autonomous robotic systems in his career.As you might expect from a person of Sebastians stature, he is deeply integrated into the academic side of machine learning as well. He is the Adjunct Professor at Stanford University and at Georgia Tech.Sebastian was named one of Brilliant 5 by Popular Science magazine in 2005. He has also been awarded the Max-Planck-Research Award (2011).If youve gone through Andrew Ngs videos, theres a good chance you would have come across Yaser Abu-Mostafas lectures too. His ability to break down complex topics into easy-to-understand bytes is really incredible. Theres a lot each of us could learn from him.Professor Yaser is a Professor of Electrical Engineering and Computer Science at the California Institute of Technology. He co-founded the most renowned machine learning conference for researchers  NIPS, or the Conference on Neural Information Processing Systems.He was awarded the Richard Feynman award prize for excellence in teaching (which is no surprise to anyone who has seen his talk about machine learning) and has numerous technical publications.Peter Norvig is among the godfathers of modern-day AI. There are no two ways about it  he has inspired the current work that is happening around the world in machine learning. We owe him a huge debt of gratitude.He is currently the Director of Research at Google. Before his current role, Peter was head of Googles core search algorithms group, and of NASA Amess Computational Sciences Division. He won the NASA Exceptional Achievement Award in 2001.Peter is also a bestselling author and has written numerous books on the field of artificial intelligence. We loved his article titled Teach Yourself Programming in Ten Years where he put forth an impassioned argument against introductory books that promised to teach you programming in one go.A cool fact about Peter Norvig  he was employee #8 at Junglee!Does the name sound familiar? Trevor Hastie is the co-author of the popular books Introduction to Statistical Learning and Elements of Statistical Learning. Professor Trevor is well known for his contributions to the field of applied statistics (published over 200 articles and written over 5 books in this field).He is currently the Professor of Mathematical Sciences and the Professor of Statistics at Stanford University. He has a wonderful way of engaging with the audience and making statistics and machine learning concepts fun to learn.Professor Trevor is a member of some of the most highly distinguished societies in academia, such as the Royal Statistical Society, American Statistical Association, National Academy of Sciences, among others.Have you heard of LASSO regression? Well  you should. Its an integral part of a data scientists toolbox. The most influential person involved in creating and developing this LASSO method? Robert Tibshirani!He is currently a Professor in the Departments of Statistics, Health Research, and Policy at Stanford University. He has recently been working extensively in the healthcare field, developing statistical tools to analyze complex genomic datasets.Professor Robert is also a popular author. In fact, he is the other co-author of the two books we mentioned under Traveor Hasties profile  Introduction to Statistical Learning and Elements of Statistics Learning.Like Trevor Hastie, he is also a member of prestigious academic societies, such as the Institute of Mathematical Statistics, the American Statistical Association, Royal Society of Canada, among others.Anil K. Jain is a University Distinguished Professor in the Computer Science and Engineering Department at Michigan State University. He is an IIT-Kanpur graduate in electrical engineering.Professor Anil is known for his contributions in the fields of computer vision, pattern recognition, and biometric recognition and is a highly cited machine learning Google Scholar profile.He has been awarded a plethora of awards based on his work in computer science and machine learning. He received the W. Wallace McDowell Award in 2007 from the IEEE Computer Society, the Humboldt Research Award, among various others. He also received the best paper awards from the IEEE Transactions on Neural Networks(1996) and thePattern Recognitionjournal (1987, 1991, and 2005).Jitendra Malik is currently a Professor of Electrical Engineering and Computer Sciences at the University of California, Berkeley. He also plays a pivotal role at Facebook as part of their AI Research division.Jitendra is another pioneer in the computer vision field. He has mentored over 60 Ph.D. students and has been a part of well-known algorithms and concepts in machine learning, such as high dynamic range imaging,shape context, and R-CNN. The latter, R-CNN, is a popular type of neural network.Per Wikipedia, he was awarded the Longuet-Higgins Prize in 2007 and 2008 and the Helmholtz Prize twice in 2015 for contributions that have stood the test of time (awarded to papers after 10 years of publication).Vladimir Vapnik is one of the primary developers of the Vapnik-Chervonenkis theory of statistical learning. But hes made a name of himself in the machine learning community for co-creating one of the most popular classification algorithms.Support Vector Machines (SVMs)!Vladimir is currently involved at Facebook AI Research where he is working with, you guessed it, Yann LeCun. His publications have been cited close to 180,000 times according to Wikipedia, an astonishing number.Vladimir is also the co-creator of the support vector clustering algorithm. The number of awards he was won is staggering and too long to list here. Some notable ones are the Gabor Award in 2005, the Neural Networks Pioneer Award in 2010, and the Benjamin Franklin Medal in Computer an Cognitive Science in 2012.If youre remotely interested in computer vision, you should know the name  Ian Goodfellow. He is best known for inventing Generative Adversarial Networks (GANs). GANs have become ubiquitous in deep learning and are popularly used at companies like Facebook and Google.Ian is currently a Director of Machine Learning at Apple. He is a researcher at heart and has previously worked as a research scientist at Google Brain and OpenAI.Ians list of mentors is enviable. He completed his MS in computer science under Andrew Ng and his Ph.D. under Yoshua Bengio and Aaron Courville.Andre Karpathy is already a legend in the AI community. He is currently working as the Director of AI at Tesla but has long been involved in the machine learning domain. His interest and specialization lies in deep learning, computer vision and image recognition.He completed his Ph.D. from Stanford University under the supervision of the great Dr. Fei-Fei Li. He has previously worked at OpenAI as a research scientist as well. Talk about working in elite company!Dr. Fei-Fei Li is an iconic name in the machine learning community. Her resume speaks for itself:The list goes on. She is an expert and thought leader in the fields of machine learning, computer vision, artificial intelligence, and cognitive neuroscience. She has published 170+ peer-reviewed research papers and continues to be a shining light for women in tech, data science and frankly, for all data scientists.If youre a programmer by profession or at heart, youll love the work Jeremy Howard does. His Twitter timeline is a treasure trove of information and resources for programmers and developers interested in machine learning.He is a founding researcher at fast.ai along with Rachel Thomas (profiled below). Howard started his professional career in management consulting before he jumped into entrepreneurship. He is a big advocate of open source library and packages and ahs contributed to several of them over the years.Jeremy has mentored and advised many startups and is a Young Global Leader with the World Economic Forum.Rachel Thomas is the other co-founder of fast.ai. She is a mathematics whiz and a strong advocate for using AI for good. She was actually one of the earliest engineers at Uber during its foundational days.She regularly features ain the top AI conference in the world and was selected by Forbes in their 20 Incredible Women in AI list.We personally love the way Rachel breaks down complex math equations into simple terms so that programmers are able to follow along. Her talk on why AI needs everyone is riveting and important in equal measure:This is by no means an exhaustive list. Thats primarily the reason we put a framework in place before we created the list. We have been inspired by these heroes of machine learning and continue to look up to their work every day we work with their algorithms!Who is your favorite expert from this list? Or is there anyone we should have included? Lets discuss in the comments section below!",https://www.analyticsvidhya.com/blog/2019/07/heroes-of-machine-learning-experts-researchers/
Popular Machine Learning Applications and Use Cases in our Daily Life,Learn everything about Analytics|Overview|Introduction|Popular Machine Learning Use Cases Well Cover|1. Machine Learning Use Cases in Smartphones|2. Machine Learning Use Cases in Transportation|3. Machine Learning Use Cases in Popular Web Services|4. Machine Learning Use Cases in Sales and Marketing|6. Machine Learning Use Cases in Security|7. Machine Learning Use Cases in the Financial Domain|8. Other Popular Machine Learning Use Cases|End Notes,"Voice Assistants|Smartphone Cameras|App Store and Play Store Recommendations|Face Unlock  Smartphones|Dynamic Pricing in Travel|Transportation and Commuting  Uber|Google Maps|Email filtering|Google Search|Google Translate|LinkedIn and Facebook recommendations and ads|Recommendation Engines|Personalized Marketing|Customer Support Queries (and Chatbots)|Video Surveillance|Cyber Security (Captchas)|Catching Fraud in Banking|Personalized Banking|Self-Driving Cars|Share this:|Related Articles|Heroes of Machine Learning  Top Experts and Researchers you should follow|Learn how to Build your own Speech-to-Text Model (using Python)|
Pranav Dar
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Picture this  you have an interview tomorrow for a machine learning role you have been aspiring to for a long time. Everything needs to go as per schedule otherwise plans might get messed up.So, you tell your virtual assistant to:The beauty about all of this? You didnt need to move or spend time typing all of this. All you did was speak to your virtual assistant and the machine learning algorithms powering that system went to work!This isnt some futuristic scenario where machines have a human-level psyche  this is now! We are living in the midst of a truly global revolution thanks to advancements in computational power and thus machine learning applications.So, lets look at the most common use cases of machine learning we deal with in our day-to-day lives (sometimes without even realizing its machine learning at play).Did you know that machine learning powers most of the features on your smartphone?Thats right! From the voice assistant that sets your alarm and find you the best restaurants to the simple use case of unlocking your phone via facial recognition  machine learning is truly embedded in our favorite devices.That example we saw in the introduction about talking to our virtual assistant? That was all about the concept of speech recognition  a budding topic in machine learning right now.Voice assistants are ubiquitous right now. You must have used (or at least heard about) the below popular voice assistants:And so on. The common thread between all of these voice assistants? They are powered by machine learning algorithms! These voice assistants recognize speech (the words we say) using Natural Language Processing (NLP), convert them into numbers using machine learning, and formulate a response accordingly.The field is ripe for these assistants becoming smarter in the future as machine learning techniques become more advanced. And you can learn to build your own speech recognition system with our awesome tutorial:Wait  what in the world does machine learning have to do with my smartphone camera? Quite a lot, as it turns out.The incredible images we are able to click these days and the depth of these images  all of that is thanks to machine learning algorithms. They analyze every pixel in a given image to detect objects, blur the background, and a whole host of tricks.These machine learning algorithms do several things to improve and enhance the smartphones camera:Interested in reading more about how you can use machine learning to build your own smartphone camera software? Your wait is over! Heres the perfect tutorial to get you on your way:I love this feature of both Googles Play Store and Apples App Store. The Recommended for you section is based on the applications I have installed on my phone (or previously used).For example, if I have a few sports and food-related applications  so my recommended for you section is usually filled with applications that are similar to these apps. I appreciate that the Play Store is personalized to my taste and shows me apps I have a higher chance of downloading.Source: LiveMintCheck it out right now if you havent noticed this.How does Apple or Google do this? Two words  recommendation engines. This is a very popular concept in machine learning right now. There are various ways of building a recommendation engine and you can get started with your own right here:Most of us have are quite familiar with this. We pick up our smartphone and it unlocks itself by detecting our face. Its smart, efficient, time-saving and frankly superb.What a lot of people dont know about this is that our smartphones use a technique called facial recognition to do this. And the core idea behind facial recognition is powered by  you guessed it  machine learning.The applications of facial recognition are vast and businesses around the world are already reaping the benefits:The usage of face recognition models is only going to increase in the next few years so why not teach yourself how to build one from scratch?The application of machine learning in the transport industry has gone to an entirely different level in the last decade. This coincides with the rise of ride-hailing apps like Uber, Lyft, Ola, etc.These companies use machine learning throughout their many products, from planning optimal routes to deciding prices for the rise we take. So lets look at a few popular use cases in transportation which use machine learning heavily.Do you often get frustrated by the surge pricing cab-hailing companies use? I encounter it on a daily basis for my commute to and from work. Prices seem to be perpetually hiked. Why is this happening?!So I dug into this a bit more and came across the concept of dynamic pricing  an excellent machine learning use case. To understand this, lets take a simple example.Imagine youre starting a ride-hailing business. You need to plan the ride prices for each route in the city in a way that would attract customers but also improve your bottomline. One way to do it is to manually map prices to each route. Not an ideal solution.This is where dynamic pricing plays a vital role. It means adjusting your prices to changing market conditions. So prices vary depending on factors like location, time of day, weather, overall customer demand, etc. Thats the underlying idea behind why surge pricing was introduced.Dynamic pricing is a thriving practice in various industries, such as travel, hospitality, transportation, and logistics, among others.Dynamic pricing isnt the only machine learning use case ride-hailing companies like Uber use. They rely heavily on machine learning to identify the most optimal route to get the passenger from point A to B.For us, it appears to be a rather simple solution. Put your location, the destination and the nearest driver will come to pick us up. But what appears to be straightforward is actually a complex web of architectures and services on Ubers backend.There are multiple machine learning techniques at play that aim to optimize the route we take.Check out this article by Uber where they talk more about using machine learning to identify efficient routes:You must have guessed this one by now. Google Maps is a prime example of a machine learning use case. In fact, I would recommend opening up Google Maps right now and picking out the different features it offers.Here are some that I can see (and have used extensively):Google uses a ton of machine learning algorithms to produce all these features. Machine learning is deeply embedded in Google Maps and thats why the routes are getting smarter with each update.The estimated travel time feature works almost perfectly. If it shows 40 minutes to reach your destination, you can be sure your travel time will be approximately around that timeline. Got to love machine learning!Youll love this section. We interact with certain applications every day multiple times. What we perhaps did not realize until recently  most of these applications work thanks to the power and flexibility of machine learning.Here are four use cases you are ultra familiar with. Now, look at them from a machine learning perspective.Dealing with way too emails at work? Or is your personal email inbox bursting with utterly random and spam emails? Weve all been there. My inbox count once read 11,000+ unread emails!Wouldnt it be easy if we could write a rule that would filter emails according to their subject? A marketing mail would go to that folder. An email about work would come into my primary inbox (and so on). This would make life so much easier.As it turns out, this is exactly what most email services are now doing! Theyre using machine learning to parse through the emails subject line and categorize it accordingly. Take Gmail for example. The machine learning algorithm Google uses has been trained on millions of emails so it can work seamlessly for the end-user (us).While Gmail allows us to customize labels, the service offers default labels:The machine learning algorithms immediately categorize the email into one of these three labels as soon as you receive an email. We get an instant alert if Gmail deems it a Primary email.Of course, Gmail also uses machine learning to figure out if the email is spam or not. A feature we are all truly grateful for. Googles algorithm has become a lot smarter over the years in deciding if an email is spam or not. This is where getting more data for a machine learning algorithm is so helpful  something Google has in abundance.The most popular machine learning use case in this (or any) list. Everyone has used Google Search and most of us use it multiple times on a daily basis. I would venture to say we take it for granted that Google will serve us the best results up front.But how does Google Search work?Google Search has become an impenetrable behemoth that mortals cannot crack. How it works underneath is something only those folks who have designed Google Search know. One thing we can say for certain  Google uses machine learning to power its Search engine.The amount of data Google has to constantly train and refine its algorithms is a number we cannot fathom. No calculator in the world will tell us the number of queries Google has processed in the last two decades. It is a trasure trove for data scientists!Now  imagine you were asked to build your own Google search. What rules would you use? What kind of content would you include? How would you rank sites? Heres an article that will get you started:Im fluent in Google Translate. Ive picked up bits and pieces of foreign languages like German, Spanish, and Italian thanks to this wonderful service by Google. Anytime I come across a bit of text in a foreign language, Google Translate immediately offers me the answer.It wont surprise you to know that Google uses machine learning to understand the sentence(s) sent by the user, convert them to the requested language, and show the output. Machine learning is deeply embedded in Googles ecosystem and we are all benefitting from that.Fortunately, we have a sense of how Google uses machine learning to power its Translate engine. This article will help you understand and get started with the topic:Social media platforms are classic use cases of machine learning. Like Google, these platforms have integrated machine learning into their very fabric. From your home feed to the kind of ads you see, all of these features work thanks to machine learning.A feature which we regularly see if People you may know. This is a common feature across all social media platforms, Twitter, Facebook, LinkedIn, etc. These companies use machine learning algorithms to look at your profile, your interests, your current friends, their friends, and a whole host of other variables.The algorithm then generates a list of people that match a certain pattern. These people are then recommended to you with the expectation that you might know them (or at least have profiles very similar to yours).I have personally connected with a lot of my professional colleagues and college friends thanks to LinkedIns system. Its a use case of machine learning benefitting everyone involved in the process.The ads that we see work in a similar fashion. They are tailored to your tastes, interests and especially your recent browsing or purchase history. If you are a part of a lot of data science groups, Facebook or LinkedIns machine learning algorithm might suggest machine learning courses.Pay attention to this next time youre using social media. Its all machine learning behind the curtains!Top companies in the world are using machine learning to transform their strategies from top to bottom. The two most impacted functions? Marketing and Sales!These days if youre working in the marketing an sales field, you need to know at least one Business Intelligence tool (like Tableau or Power BI). Additionally, marketers are expected to know how to leverage machine learning in their day-to-day role to increase brand awareness, improve the bottomline, etc.So, here are three popular use cases in marketing and sales where machine learning is changing the way things work.We briefly spoke about recommendation engines earlier. I mentioned that these systems are ubiquitous. But where are they used in the marketing and sales field? And how?Lets take a simple example to understand this. Before the advent of IMDb (and Netflix), we all used to go to DVD stores or rely on Google to search for movies to watch. The store clerk would offer suggestions on what to watch and we took a hail mary pass by picking up movies we had no idea about.That world is almost completely in the past now thanks to recommendation engines. We can log on to a site and it recommends products and services to me based on my taste and previous browsing history. Some popular examples of recommendation engines:The list is long. Recommendation engines are everywhere around us and marketing and Sales departers are leaning on them more than ever before to attract (and retain) new customers.I encourage you to read this beginner-friendly tutorial on how to build your own recommendation engine:Recommendation engines are part of an overall umbrella concept called personalized marketing. The meaning of this concept is in the name itself  it is a type of marketing technique tailored to an individuals need.Think about this. How many calls do you get from credit card or loan companies offering their services for free? These calls offer the same services without understanding what you want (or dont want). Its traditional marketing that is now outdated and well behind the digital revolution.Now imagine if these calls or emails came highly personalized to your interests. If youre a big shopaholic and that reflects in your purchase history, perhaps the message could be about a new service offering to extend your credit line. Or if youre a machine learning enthusiast, the email could offer courses suited to your taste.Honestly, the potential for personalized marketing is HUGE. Machine learning helps to identify customer segments and tailor your marketing campaigns for those segments. You can regularly check how your campaign is doing through metrics like open rates, clickthrough rates, and so on.I strongly recommend reading the below guide which will help you rebrand your digital marketing strategy:You will understand this at a very personal level if youve ever dealt with customer support (and who hasnt?). Those dreaded phone calls, the interminable wait, the unresolved query  it all adds up to a very frustrating user experience.Machine learning is helping remove all these obstacles. Using concepts of Natural Language Processing (NLP) and sentiment analysis, machine learning algorithms are able to understand what were saying and the tone which we are saying it in.We can broadly divide these queries into two categories:For the former, machine learning algorithms detect the message and the sentiment to redirect the query to the appropriate customer support person. They can then deal with the user accordingly.Text-based queries, on the other hand, are now almost exclusively being handled by chatbots. Almost all businesses are now leveraging these chatbots on their sites. They remove the impediment of waiting and immediately provide answers  hence, a super useful end-user experience.We have put together two intuitive articles on how to build your own chatbot which you should check out:Machine learning is disrupting the security industry as well! The days of traditional security, where security guards used to sit for hours on end noting down vehicle numbers and stopping suspicious folks  its slowly being phased out.Businesses are using machine learning to better analyze threats and respond to adversarial attacks. These use cases extend to both offline threats as well as online (bank frauds, financial threats, etc.).Im certain you must have heard or read about a certain country using video surveillance to track its citizens (a quick Google search will tell you anyway). Organizations globally are using video surveillance for various tasks, like detecting intruders, identifying threats of violence, catching criminals, etc.All of this is not being done manually, however. That would be immensely time taking. So instead, machine learning algorithms are being used for the software that is put inside these surveillance cameras.These machine learning algorithms use various computer vision techniques (like object detection) to identify potential threats and nab offenders.Heres a quite unique use case of machine learning for security:Im not a robot  does this sentence seem familiar? We often encounter this button when a website suspects it is dealing with a machine rather than a human.These tests are called CAPTCHA, short for Completely Automated Public Turing test. We are asked to identify traffic lights, trees, crosswalks, and all sorts of objects to prove that we are, indeed, human.The traffic lights and trees are getting covered by other objects, cars are getting obscured, crosswalks are more distant, and all sorts of complications. Why are websites making life more difficult for us? The answer to this lies with machine learning.The website Verge put it best:Because CAPTCHA is such an elegant tool for training AI, any given test could only ever be temporary, something its inventors acknowledged at the outset. With all those researchers, scammers, and ordinary humans solving billions of puzzles just at the threshold of what AI can do, at some point the machines were going to pass us by.So Google is using machine learning to make CAPTCHA even more complex to decipher. The researchers are using image recognition techniques to crack these CAPTCHAS and consequently enhance their security at the backend.Most of the jobs in machine learning are geared towards the financial domain. And that makes sense  this is the ultimate numbers field. A lot of banking institutions till recently used to lean on logistic regression (a simple machine learning algorithm) to crunch these numbers.There are tons of use cases of machine learning in finance. Lets look at two very common ones you (most likely) have come across.Have you ever been a victim of credit card fraud? Its a painful experience to go through. The shock of the fraud is exacerbated by the amount of paperwork the bank asks you to fill out.Thankfully, machine learning is solving different layers of this process. From fraud detection to fraud prevention, machine learning algorithms are changing the way banks work to improve the customers experience.The challenge with this is keeping up with the level of cyber threats. These adversaries are two steps ahead of the curve at each stage. As soon as the latest machine learning solution comes up, these attachers perfect it and build on top of it.Having said that, machine learning has definitely helped streamline the process. These algorithms are able to identify fraudulent transactions and flag them so the bank can connect with the customers ASAP to check if they made the transaction.A good example is to look at the spending patterns of consumers. If a purchase does not fit in with this pattern (the amount is too high, or from a different country, etc.), then the algorithms alert the bank and put the transaction on hold.The below two articles by a cybersecurity and machine learning expert explain how to build robust malware detection models:Another use case of recommendation engines! This one is targeted specifically for the banking domain. You must be quite familiar with personalization at this point  so think about what personalized banking could mean before you read further.We have read about banks targeting customer microsegments and tailoring offers to them. Personalized banking takes this concept to an entirely new level.The ideal personalization scenario is using machine learning to anticipate the users need and targeting segments of each individual. As a report by BCG states:Personalization in banking is not primarily about selling. Its about providing service, information, and advice, often on a daily basis or even several times a day. Such interactions, as opposed to infrequent sales communications, form the crux of the customers banking experience.Read the full BCG article here:I wanted to include this section of machine learning use cases that did not quite fit in our above categories. I will constantly update this section so lets start by looking at a very interesting use case  self-driving cars!Out of all the use cases we have covered in this article, self-driving cars fascinate me the most. It is a crowning achievement of what we have been able to accomplish using hardware and machine learning.The beauty of self-driving cars is that all the three main aspects of machine learning  supervised, unsupervised and reinforcement learning  are used throughout the cars design.Here are just a few features of self-driving cars where machine learning is used:We are living in a golden age of machine learning. You must have imagined the vast and endless possibilities of this wonderful field.Are there any applications you feel we should include here? Which applications did you find the most intriguing? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/07/ultimate-list-popular-machine-learning-use-cases/
Learn how to Build your own Speech-to-Text Model (using Python),Learn everything about Analytics|Overview|Introduction|Table of Contents|A Brief History of Speech Recognition through the Decades|Introduction to Signal Processing|Different Feature Extraction Techniques for an Audio Signal|Understanding the Problem Statement for our Speech-to-Text Project|Implementing the Speech-to-Text Model in Python|Code|End Notes,"What is an Audio Signal?|Parameters of an audio signal|Different types of signals|What is sampling the signal and why is it required?|Time-domain|Frequency domain|Spectrogram|Import the libraries|Data Exploration and Visualization|Duration of recordings|Share this:|Related Articles|Popular Machine Learning Applications and Use Cases in our Daily Life|21 Must-Know Open Source Tools for Machine Learning you Probably Arent Using (but should!)|
Aravind Pai
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Digital signal|Analog signal||Sampling rate|Resampling|Preprocessing the audio waves|Split into train and validation set|Model Architecture for this problem|Model building,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Hey Google. Whats the weather like today?This will sound familiar to anyone who has owned a smartphone in the last decade. I cant remember the last time I took the time to type out the entire query on Google Search. I simply ask the question  and Google lays out the entire weather pattern for me.It saves me a ton of time and I can quickly glance at my screen and get back to work. A win-win for everyone! But how does Google understand what Im saying? And how does Googles system convert my query into text on my phones screen?This is where the beauty of speech-to-text models comes in. Google uses a mix of deep learning and Natural Language Processing (NLP) techniques to parse through our query, retrieve the answer and present it in the form of both audio and text.The same speech-to-text concept is used in all the other popular speech recognition technologies out there, such as Amazons Alexa, Apples Siri, and so on. The semantics might vary from company to company, but the overall idea remains the same.I have personally researched quite a bit on this topic as I wanted to understand how I could build my own speech-to-text model using my Python and deep learning skills. Its a fascinating concept and one I wanted to share with all of you.So in this article, I will walk you through the basics of speech recognition systems (AKA an introduction to signal processing). We will then use this as the core when we implement our own speech-to-text model from scratch in Python.Looking for a place to start your deep learning and/or NLP journey? Weve got the perfect resources for you:You must be quite familiar with speech recognition systems. They are ubiquitous these days  from Apples Siri to Google Assistant. These are all new advents though brought about by rapid advancements in technology.Did you know that the exploration of speech recognition goes way back to the 1950s? Thats right  these systems have been around for over 50 years! We have prepared a neat illustrated timeline for you to quickly understand how Speech Recognition systems have evolved over the decades:Wouldnt it be great if we can also work on such great use cases using our machine learning skills? Thats exactly what we will be doing in this tutorial!Before we dive into the practical aspect of speech-to-text systems, I strongly recommend reading up on the basics of signal processing first. This will enable you to understand how the Python code works and make you a better NLP and deep learning professional!So, let us first understand some common terms and parameters of a signal.This is pretty intuitive  any object that vibrates produces sound waves. Have you ever thought of how we are able to hear someones voice? It is due to the audio waves. Lets quickly understand the process behind it.When an object vibrates, the air molecules oscillate to and fro from their rest position and transmits its energy to neighboring molecules. This results in the transmission of energy from one molecule to another which in turn produces a sound wave.The below GIF wonderfully depicts the difference between a high and low-frequency signal:In the next section, I will discuss different types of signals that we encounter in our daily life.We come across broadly two different types of signals in our day-to-day life  Digital and Analog.A digital signal is a discrete representation of a signal over a period of time. Here, the finite number of samples exists between any two-time intervals.For example, the batting average of top and middle-order batsmen year-wise forms a digital signal since it results in a finite number of samples.scroll.inAn analog signal is a continuous representation of a signal over a period of time. In an analog signal, an infinite number of samples exist between any two-time intervals.For example, an audio signal is an analog one since it is a continuous representation of the signal.Wondering how we are going to store the audio signal since it has an infinite number of samples?Sit back and relax! We will touch on that concept in the next section.An audio signal is a continuous representation of amplitude as it varies with time. Here, time can even be in picoseconds. That is why an audio signal is an analog signal.Analog signals are memory hogging since they have an infinite number of samples and processing them is highly computationally demanding. Therefore, we need a technique to convert analog signals to digital signals so that we can work with them easily.Sampling the signal is a process of converting an analog signal to a digital signal by selecting a certain number of samples per second from the analog signal. Can you see what we are doing here? We are converting an audio signal to a discrete signal through sampling so that it can be stored and processed efficiently in memory.I really like the below illustration. It depicts how the analog audio signal is discretized and stored in the memory:The key thing to take away from the above figure is that we are able to reconstruct an almost similar audio wave even after sampling the analog signal since I have chosen a high sampling rate. The sampling rate or sampling frequency is defined as the number of samples selected per second.The first step in speech recognition is to extract the features from an audio signal which we will input to our model later. So now, l will walk you through the different ways of extracting features from the audio signal.Here, the audio signal is represented by the amplitude as a function of time. In simple words, it is a plot between amplitude and time. The features are the amplitudes which are recorded at different time intervals.The limitation of the time-domain analysis is that it completely ignores the information about the rate of the signal which is addressed by the frequency domain analysis. So lets discuss that in the next section.In the frequency domain, the audio signal is represented by amplitude as a function of frequency. Simply put  it is a plot between frequency and amplitude. The features are the amplitudes recorded at different frequencies.The limitation of this frequency domain analysis is that it completely ignores the order or sequence of the signal which is addressed by time-domain analysis.Remember:Time-domain analysis completely ignores the frequency component whereas frequency domain analysis pays no attention to the time component.We can get the time-dependent frequencies with the help of a spectrogram.Ever heard of a spectrogram? Its a 2D plot between time and frequency where each point in the plot represents the amplitude of a particular frequency at a particular time in terms of intensity of color. In simple terms, the spectrogram is a spectrum (broad range of colors) of frequencies as it varies with time.The right features to extract from audio depends on the use case we are working with. Its finally time to get our hands dirty and fire up our Jupyter Notebook!Lets understand the problem statement of our project before we move into the implementation part.We might be on the verge of having too many screens around us. It seems like every day, new versions of common objects are re-invented with built-in wifi and bright touchscreens. A promising antidote to our screen addiction is voice interfaces.TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people. Well build a speech recognition system that understands simple spoken commands.You can download the dataset from here.The wait is over! Its time to build our own Speech-to-Text model from scratch.First, import all the necessary libraries into our notebook. LibROSA and SciPy are the Python libraries used for processing audio signals.Data Exploration and Visualization helps us to understand the data as well as pre-processing steps in a better way.Visualization of Audio signal in time series domainNow, well visualize the audio signal in the time series domain:Let us now look at the sampling rate of the audio signals:From the above, we can understand that the sampling rate of the signal is 16,000 Hz. Let us re-sample it to 8000 Hz since most of the speech-related frequencies are present at 8000 Hz:Now, lets understand the number of recordings for each voice command:Whats next? A look at the distribution of the duration of recordings:In the data exploration part earlier, we have seen that the duration of a few recordings is less than 1 second and the sampling rate is too high. So, let us read the audio waves and use the below-preprocessing steps to deal with this.Here are the two steps well follow:Let us define these preprocessing steps in the below code snippet:Convert the output labels to integer encoded:Now, convert the integer encoded labels to a one-hot vector since it is a multi-classification problem:Reshape the 2D array to 3D since the input to the conv1d must be a 3D array:Next, we will train the model on 80% of the data and validate on the remaining 20%:We will build the speech-to-text model using conv1d. Conv1d is a convolutional neural network which performs the convolution along only one dimension.Here is the model architecture:Let us implement the model using Keras functional API.Define the loss function to be categorical cross-entropy since it is a multi-classification problem:Early stopping and model checkpoints are the callbacks to stop training the neural network at the right time and to save the best model after every epoch:Let us train the model on a batch size of 32 and evaluate the performance on the holdout set:Diagnostic plotIm going to lean on visualization again to understand the performance of the model over a period of time:Loading the best modelDefine the function that predicts text for the given audio:Prediction time! Make predictions on the validation data:The best part is yet to come! Here is a script that prompts a user to record voice commands. Record your own voice commands and test it on the model:Let us now read the saved voice command and convert it to text:Here is an awesome video that I tested on one of my colleagues voice commands:Congratulations! You have just built your very own speech-to-text model!Find the notebook hereGot to love the power of deep learning and NLP. This is a microcosm of the things we can do with deep learning. I encourage you to try it out and share the results with our community. In this article, we covered all the concepts and implemented our own speech recognition system from scratch in Python.I hope you have learned something new today. I will see you in the next article. If you have any queries/feedback, please free to share in the below comments section!",https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/
21 Must-Know Open Source Tools for Machine Learning you Probably Arent Using (but should!),"Learn everything about Analytics|Overview|Introduction|We have divided the Open Source Machine Learning Tools into 5 Categories:|1. Open Source Machine Learning Tools for Non-Programmers|2. Open Source Machine Learning Tools for Model Deployment|3. Open Source Machine Learning Tools for Big Data|4. Open Source Machine Learning Tools for Computer Vision, NLP, and Audio|Open Source Tools for Reinforcement Learning|End Notes","Share this:|Related Articles|Learn how to Build your own Speech-to-Text Model (using Python)|Heading for a Data Science Interview? 6 Things to Do 1 Day Before your Interview|
Mohd Sanad Zaki Rizvi
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love the open-source machine learning community. The majority of my learning as an aspiring and then as an established data scientist came from open-source resources and tools.If you havent yet embraced the beauty of open-source tools in machine learning  youre missing out! The open-source community is massive and has an incredibly supportive attitude towards new tools and embracing the concept of democratizing machine learning.You must already know the popular open-source tools like R, Python, Jupyter notebooks, and so on. But there is a world beyond these popular tools  a place where under-the-radar machine learning tools exist. These arent as eminent as their counterparts but can be a lifesaver for many machine learning tasks.In this article, we will look at 21 such open-source tools for machine learning. I strongly encourage you to spend some time going through each category I have mentioned. There is a LOT to learn beyond what we typically learn in courses and videos.Note that many of these are Python-based libraries/tools because lets face it  Python is as versatile a programming language as we could get!Machine learning can appear complex to people coming from a non-programming and non-technical background. Its a vast field and I can imagine how daunting that first step can appear. Can a person with no programming experience ever succeed in machine learning?As it turns out, you can! Here are some tools that can help you cross the chasm and enter the famed machine learning world:There are a lot more interesting free and open-source software that provide great accessibility to do machine learning without writing (a lot of) code.On the other side of the coin, there are some paid out-of-the-box services you can consider, such as Google AutoML, Azure Studio, Deep Cognition, and Data Robot.Deploying machine learning models is one of the most overlooked yet important tasks you should be aware of. It will almost certainly come up in interviews so you might as well be well-versed with the topic.Here are some frameworks that can make it easier to deploy that pet project of yours to a real-world device.Big Data is a field that treats ways to analyze, systematically extract information from, or otherwise, deal with datasets that are too large or complex to be dealt with by traditional data processing application software. Imagine processing millions of tweets in a day for sentiment analysis. This feels like a humongous task, doesnt it?Dont worry! Here are some tools that can help you work with Big Data.If we want machines to think, we need to teach them to see. Dr. Fei-Fei Li on Computer VisionRL is the new talk of the town when it comes to Machine Learning. The goal ofreinforcement learning(RL) is to train smart agents that can interact with their environment and solve complex tasks, with real-world applications towardsrobotics,self-driving cars, andmore.The rapid progress in this field has been fueled by making agents play games such as the iconicAtari console games, the ancientgame of Go, or professionally played video games likeDota 2orStarcraft 2, all of which provide challenging environments where new algorithms and ideas can be quickly tested in a safe and reproducible manner. Here are some of the most useful training environments for RL:As it must have been evident by the above set of tools that open source is the way to go when we consider data science and AI-related projects. I have probably just scratched the tip of the iceberg but there are numerous tools available for a variety of tasks that make life easier for you as a data scientist, you just need to know where to look.In this article, we have covered 5 interesting areas of data science that no one really talks much about ML without code, ML deployment, Big data, Vision/NLP/Sound and Reinforcement learning. These 5 areas, I personally feel have the most impact when the real-world value of AI is taken into account.What are the tools that you think should have been on this list? Write your favorites below for the community to know!",https://www.analyticsvidhya.com/blog/2019/07/21-open-source-machine-learning-tools/
Heading for a Data Science Interview? 6 Things to Do 1 Day Before your Interview,Learn everything about Analytics|Introduction|Be Thorough with your Data Science Resume|Study up on your Data Science Projects|Practice Solving Puzzles  A Key Data Science Skill|Prepare to Face Case Studies|Research the Job Profile and the Organization|Review confusing terms|End Notes,"Share this:|Related Articles|21 Must-Know Open Source Tools for Machine Learning you Probably Arent Using (but should!)|8 Simple and Unique Data Science Projects to Create Art, Generate Music and Much More!|
Khyati Mahendru
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Youve finally done it! You have landed an interview for a data science role. Now, a day before your interview, youre not sure what to study. The day is almost here but there is so much to cover!Sound familiar?Interviews can be daunting  I completely get that. Add in Data Science, and youve got yourself a nerve-racking cocktail. Data Science professionals need to combine their technical skills with their soft skills. Its a tough landscape to navigate.Landing the interview is great  but cracking it? Thats where things get really interesting. What should you study? What should you leave out? Is there any cheat code you can apply and simply plug and play it in during the interview?If you are in a similar situation  youve come to the right place!In this article, I will focus on 6 key things to do a day before your big Data Science Interview, apart from the obvious revision of concepts, to make sure you absolutely nail this opportunity. I will not cover the entire preparation process which should ideally begin months in advance of the actual interview.Personally, I have often felt under-prepared for interviews because I did not know what to expect out of them. If you have ever felt similarly, the Ace Data Science Interviews course can help. The course breaks the complete Interview Process into 7 steps by taking into account the experiences of taking hundreds of interviews and guides on how to excel at each of these steps.The absolute basic of any interview, and especially a data science one. You should be able to explain everything listed on your resume. Anything that you could possibly reference, you should be able to speak about it.If youve listed an NLP project for example, and are unable to explain the details  thats a MAJOR red flag for the interviewer.Use the day before the interview to edit and revise your resume. Cut details that are not required and add new ones if required. Think about each experience and project that you list  does it add something relevant?That means your experience with a marketing firm as a non-technical person might not be very relevant for a data science role. You should consider keeping details like that off your resume. Mentioning it will just give the interviewer a sense that you are not clear about what you want from the job.Also, think of how you will go about explaining your work experience. Your account should depict your skills and how they led to progress. Consider the following statements:Doesnt the second statement sound way more impressive than the first?Make sure to make your achievements are measurable and quantifiable. This will leave a better impression on the minds of your data science interviewer(s).I recommend reading our guide to building an effective Data Science Resume. It mentions the 4 key aspects that will make or break your data science application.Much like the other details on your resume, deciding what projects to talk about in your interview is also crucial. If there are any projects irrelevant to the role youve applied to, adding it in anyway isnt a great practice. This just shows your interviewer that you cannot prioritize well.Source: DataOptimalShortlist 3 to 4 projects which showcase your best work and prepare yourself to talk about them. These projects could be from your current organization, internships, from some coursework or even independent projects using datasets from Analytics Vidhya or Kaggle. Also, keep in mind that these projects should be relevant to your job profile.I keep reiterating this because it is THAT important.Let me give you my own example. I had listed a research project on my resume which I had done two years back. In hindsight, I should have left it off since it had nothing relevant to the internship role I was interviewing for  a data analytics intern.As I went on explaining what I did in this project, I made the mistake of mentioning the term cubic splines. The interviewer immediately wanted me to elaborate on cubic splines and I realized that I had dug myself into a hole. And no, I did not get the internship.Theres a lesson there for all of you data science enthusiasts! If you are looking for projects, refer to our list of 24 Ultimate Data Science Projects to boost your Knowledge and Skills.Puzzles are a fairly popular way of evaluating a candidates quick thinking and analytical acumen. You need to be logical, creative and good with numbers to solve puzzles.Many organizations use puzzles for testing their candidates on their problem-solving skills. They want to know about your thought process and how you approach a problem.I cannot give you a complete guide to solving each puzzle, but I do have a few tips for you to proceed towards puzzle-solving:Try solving the puzzles in our list of 20 Hard Data Science Interview Puzzles that every analyst should solve at least once.Organizations use case studies as a means of evaluating candidates on how they approach real-life problems. Case studies are the closest thing to the problems that you would be encountering in your role later on. I have seen freshers struggle the most with this part of the data science interview process.The tricky aspect of a case study is that it might not be directly related to data science. For example, I got a case study around how to predict the number of black cars in Delhi NCR right now. Its a tricky one  but if you have a structured mindset  youll knock it out of the park!Approaching a case study can appear hard since there is no fixed formula to solve them. But you can use the below points to guide yourself through them:Have a look at some of the case studies on Analytics Vidhya (practice each of them and youll be interview-ready in a jiffy):Researching the job profile has obvious benefits. You would be able to streamline your preparation based on what is required from the role.Sometimes, employers may even ask candidates a question or use a keyword to make sure they read the job description carefully:These questions will be dreadful if you didnt read up on the company and the role.I highly recommend spending some time reading about the companys mission, vision and core values. Find out about their key achievements. Try and find the data science set up that they have and what kind of projects they work on. If possible, find out about the hierarchy of the organization and how the data science team fits into it.Studying the organization and its structure will help you frame better questions for your interviewers. This shows your enthusiasm and curiosity towards the organization and leaves your interviewers impressed.Are there any data science terms that have bamboozled you before? Im sure there are a few  this is true for even experienced data scientists.A few confusing terms or concepts that I encourage you to read up on a day before your interview:I frequently have to look up the difference between these terms and I am sure most of you do as well. These can stump you if asked in an interview. You know the answer, but the slight differences just arent coming to you.Source: AB TastyMake sure to revise such terms a day before the interview. Refer to our glossary of common machine learning and data science terms for a quick idea around these concepts.These are just some last-minute tips. The entire data science interview preparation is a long process. You need to start months in advance and build your profile. There are also multiple rounds in a data science hiring process, including:The Ace Data Science Interviews course covers all of these rounds in detail. The course also has a rich collection of Interview Questions along with many helpful tips and tricks. This could significantly increase your chances of acing your next Data Science Interview. So make sure to check it out!",https://www.analyticsvidhya.com/blog/2019/07/6-essential-tips-should-know-day-before-data-science-interview/
"8 Simple and Unique Data Science Projects to Create Art, Generate Music and Much More!","Learn everything about Analytics|Overview|Introduction|Projects covered in this article|Data Science Projects for Art|Googles Quick, Draw!|Intricate Art Generation|The Belamy Family|Data Science Projects for Music|Googles Magenta Project|Clara|The AI DJ Project|Miscellaneous Projects|IBMs Project Debater|MITs Generate Almost Anything Series|End Notes","|Share this:|Related Articles|Heading for a Data Science Interview? 6 Things to Do 1 Day Before your Interview|How to Build an Effective Data Science Resume? 4 Key Aspects that Will Make or Break your Application|
Purva Huilgol
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have always had a fascination with the creative arts. There is something quite pleasing about taking a blank slate (or paper) and letting my imagination fuel my creativity.But taking out time these days to practice my art? Simply not possible. Im sure anyone who has a hobby outside their working environment will relate to my situation! So when I came across a few Google projects about using data scienceto generate art, I was thrilled beyond measure.I associate data science with your run-of-the-mill industry domains  finance, marketing, healthcare, etc. But data science for creative work, such as art, writing, music or even debating? It sounds too futuristic to be relevant for data scientists right now.I can assure you this future scenario is now! We are living in the midst of a wonderful AI art revolution. So if youve ever wanted to become an artist, or even have an inkling of creativity that you want to channel  this is the article for you.I have put together a list of some of the most interesting and creative projects I have either worked on or come across. I have categorized them into Art (which involves painting/drawing, etc.), Music (composition, instrumental, etc.) and finally some miscellaneous projects that include a nifty machine debating at a human-level intelligence.This is machine learning for the creative arts!Drawing new things and filling them up with color is something most of us did during our childhood. I could never have imagined that a machine would be able to conjure up art as good as anything we have produced. But here we are.A simple neural network trained on a certain art or set of images can now generate stunning visual imagery. Imagine the fun Leonardo da Vinci would have had with this back in the Renaissance era!Of course we start with Google. Who else would rank top of the most creative list when it comes to artificial intelligence? Googles Creative Lab and Experiments with Google came together to create this simple tool that guesses what you are trying to draw.Douglas Eck and David Ha came up with Sketch-RNN, a Recurrent Neural Network which generates drawings of common objects.The model is trained on human-drawn sketches of everyday objects represented as a sequence. The sequence is then fed to a sequence-to-sequence autoencoder. This, in turn, trains the neural network.Additionally, the team has also maintained a dataset of 50 million drawings contributed by players of the Quick, Draw! game. Here are a few resources to get you started with Quick, Draw!:Have you ever heard of Zentangles? Chances are that you have without ever realizing it. The intricate patterns some of us draw at the corner of pages, which we used to call just doodles, are actually Zentangles.They have recently become extremely popular in coloring books and pop art as well. Zentagles, however, are bound by some visual rules and recurring patterns. Here are a few examples of various Zentangles:Kalai Ramea, a researcher at the Palo Alto Research Center (previously Xerox PARC), believed that such art was a good domain to apply style transfer algorithms (Neural Style Transfer). The generated designs she came up with are very unique and colorful too.The project involves using a Style Transfer Algorithm and applying it to an image. The content image is a silhouette of an image we would like to apply the Zentangle style to while the Style Image is any pattern (black-and-white, or colorful). The algorithm basically transfers the style of the Style Image to the Content Image. A brief explanation of neural style transfer is provided below:The weights used are from a pre-trained network called VGGNet, a deep convolutional network for object recognition developed and trained by the University of Oxfords Visual Geometry Group.A sample of Kalais work created from a quilt and Darth Vader image:Awesome, right? Kalai also presented this research at the Self-Organizing Conference on Machine Learning (SOCML) 2017. You can start learning more about intricate art generation here:Generative Adversarial Networks (GANs) are the flavor of the month among the deep learning community nowadays. GANs are being used to generate photographs of people who dont exist and even draw up landscapes and portraits.The trio of Gauthier Vernier, Pierre Fautrel and Hugo Caselles-Dupr took the applications of GANs a step further. As a part of Obvious, a collective of artists and machine learning researchers based in Paris, they created portraits of an entirely fictional Belamy Family from GANs.The Family is a collection of 11 portraits of different family members with the crowning glory being the portrait of Edmond de Belamy, which fetched $432,500 at the world-famous auction house, Christies.The classical nature of the portraits stems from the fact the training data comprised of 15,000 portraits from the 14th to the 20th Century.The best part? Belamy is derived from Bel ami which translates to I, Goodfellow (from I. Goodfellow, creator of GANs) and each portrait is signed off with the loss function formula of the GAN model.Using AI algorithms to generate music seems like a natural choice at first glance. Music is essentially a collection of notes  and AI thrives on that kind of data. So, its not surprising to see the kind of progress researchers have made with AI for music.Ah yes, Google again. Launched in the summer of 2016, Googles Magenta project was initially widely known among research and AI enthusiasts, but its claim to fame among the populace was their Bach doodle. Created to celebrate J.S. Bachs 334th birthday, the model will harmonize any user given input in Bachs style.The AI model is called Coconet, a convolutional neural network that fills in missing pieces of music. To train this model, the team used 306 chorale harmonies written by Bach. The model erases some random notes from the training set and regenerates new notes to fill in the blanks.More on the Bach doodle explained by the team behind it:Clara, created by Christine McLeavey Payne (Pianist and Fellow at OpenAI), is a neural network that composes piano and chamber music. Based on the concept that music is also a language, Christine has developed an LSTM based neural network that predicts what notes or chords we should play next.To accomplish this, Christine first obtained a dataset of MIDI files and converted them into text format. The text file was then divided into notewise or chordwise levels, which is similar to character-level and word-level language models.Christine shows us a demo of Clara:Christine also created a Music Critic model that classifies music into human-generated and machine-generated.You knew this one was coming  the mashup of AI and DJs! This was actually the first image I visualized when I heard about AI and music. A good DJ can completely transform the mood of a live audience  something AI does to us all the time!Qosmo, a Japanese company focusing exclusively on Computational Creativity, created the AI DJ Project. This is a combination of both human and algorithmic DJs creating new music.The project consists of 3 phases:Check out the project page explaining these three steps in detail.AIs use cases in the creative arts extend beyond art and music. Curious to see what else AI can do that you might not have imagined yet? Lets find out!Natural Language Processing (NLP) is never far away from any AI list these days. And this project by IBM is by far the most complex project on this list. Every component which went in the making of this AI debater explores the concepts of Machine Learning.Get a hold of this  the Argument Mining Component detects claims and evidence in a corpus and assesses the quality of the arguments. The Stance Classification and Sentiment Analysis modules deal with classifying expert opinions on a stance, analyzing the sentiments of sentences and idioms, and even identifying a stance based on a claim.Next, the Deep Neural Nets, along with weak supervision, predict phase-breaks, score an argument and further enhance the argument mining. Finally, the Text-to-Speech Systems tell which words and phrases emphasize and generate effective speech patterns.Join the debate on subsidizing pre-schools with Project Debater debating against Champion debater Harish Natarajan:Students of MITs most popular class by Neil Gershenfeld, How to Make (Almost) Anything, are breaking new barriers and applying AI models in the most ingenious ways.They have released a series of 6 episodes, with each episode featuring an expert human collaborator working with the team to explore different ways AI can be used to further their domain. The list includes AI-generated music, AI-generated Fashion, Graffiti, and by far my favorite: AI-generated Pizza.Make sure you browse through the project site.Other honorable mentions include the AI-generated short film Eclipse:Also, heres a really cool repository of faces of people who dont exist that were created using GANs. Incredible!Truly incredible what Data Science can do these days. This further illustrates that the boundaries of Data Science are as yet unexplored. This is particularly useful for transitioners who would like to apply AI in their domain of expertise.Personally, I also found these projects extremely useful for learning concepts which I previously considered difficult. It is much easier to understand an algorithm when you see it being applied to something fun and interesting.Can you think of other ways where Data Science can be applied (but currently isnt)? Lets discuss in the comments section below!",https://www.analyticsvidhya.com/blog/2019/07/8-impressive-data-science-projects-create-art-music-debates/
How to Build an Effective Data Science Resume? 4 Key Aspects that Will Make or Break your Application,Learn everything about Analytics|Overview|Introduction|Table of Contents|A good way to think about your resume is to look at it as a real estate|1. Structure of your Data Science Resume|2. Adding Content and Information to the Resume|3. Get Feedback from Industry Experts|4. Build your Digital Presence|End Notes,"What is the right length of the resume?|Create Differentiated Areas|Information Prioritisation|Make your Content Crisp and Clear|Share this:|Related Articles|8 Simple and Unique Data Science Projects to Create Art, Generate Music and Much More!|Dont Miss out on these 24 Amazing Python Libraries for Data Science|
Prateek Joshi
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

 4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you applying to data science jobs but not receiving any phone calls for the position you want? This is among the biggest gripes aspiring data science professionals have. Regardless of the role youre applying for (data scientist, data engineer, data analyst, etc.)  clearing that first hurdle is a significant obstacle.If you find yourself in a similar position, theres a good chance recruiters are passing over your resume. It is the single most important aspect of landing a data science interview. A poorly crafted resume or too many irrelevant details will land your resume in the rejection pile.Heres the good news  crafting the perfect data science resume is a skill you can learn! Once you know how to expertly update your resume, youll be able to effectively market your skills when applying for your next data science job.Now, I will take you through a step-by-step process to build an awesome data science resume. If you follow this process sincerely, theres a good chance your resume would leave a good impression on potential recruiters.If youre struggling to land or clear data science interviews, we have curated the perfect course for you. The Ace Data Science Interviews course is an amalgamation of our combined experience of taking hundreds of interviews to help you land your dream data science role.This is a very intuitive way of crafting your resume. Let me explain. In any house, you have a fixed area and a floor plan to work with. You need to make sure that things fit in neatly in whatever space that is available to you.Similarly, your resume has limited space which you should use judiciously and tell your story effectively.Keep this analogy in mind as we walk through the steps below.First, we should consider the overall structure of the resume. This will help us plan the different sections we should include and how lengthy (or short) those sections should be.While building a resume, one of the most common dilemmas is what should be the length of the resume? Ideally, a single page is sufficient. If you keep your resume crisp and to the point, it would ensure that the interviewer/recruiter is reading what you want them to read.A one page resume is recommended, while a two page one is also acceptable. Anything beyond two pages increases the chances of getting your resume rejected. In my experience, crafting a multiple page resume is usually skimmed over by the recruiter  not an ideal situation to be in.Have a look at the below resume. It stretches all the way up to 3 pages. This is highly undesirable and it would leave a bad impression on the recruiter or the interviewer:Now, take a look at the below resume. Its exactly what a data science recruiter would look for. It becomes really easy to skim through it and get an idea about your skills and capabilities. And given that recruiters are parsing through hundreds of resumes a week  this is what they keep an eye out for.So, try to keep only the relevant information on your resume. For example, if youre applying for an NLP role, there isnt much need of mentioning how you took accounts as a subject back in college. Space is absolutely vital  use it wisely.But what if all youve achieved and done is simply too much for a single page? Heres my advice  do not hesitate to cut down the content and keep only those details that would be in-line with the job you are applying to.Once you have selected the information that you would like to display on your resume, it is time to identify the right sections or areas on the resume where you would put your experience and information.Here are a few key points you should consider while preparing your data science resume:Here comes the meat of your resume  your experience and projects in data science. Again, your ability to fit everything into a single page will come in handy. But how can you do that? Lets take a look!Lets pay attention to what you should put in each of the sections discussed above. This exercise is crucial because you want to tell as much about yourself as possible without including any irrelevant information. On top of that, due to space constraints on the resume, you might even have to sacrifice some of your relevant and important information.Hence, prioritising what to display and what not to on the resume is a critical step. It depends not just on your knowledge and work experience but also on the nature of the job you wish to apply for.For example, lets say you have worked on a Natural Language Processing (NLP) problem and you go ahead and apply for an NLP Data Scientist role. However, most of the projects mentioned in your resume are related to basic machine learning challenges. This will be a very risky scenario for you as the recruiter might well reject your resume because he would not come to know that you can even handle NLP tasks.I completely understand that its difficult to leave information out. But thats the price we have to pay when we want to land our dream role. So, lets see what kind of information we should include and what to exclude in different sections of the resume:Awesome! The basic template of your resume is ready. So, whats next? Your resume should pack a punch so lets see how to make it impactful.To make your resume stand out from the rest of the candidates, incorporate the below points Ive mentioned:Now that your resume is all done and ready, there is still one final step left for you to execute  getting practical and experienced feedback on your resume. This is important because when we work on something with full dedication and sincerity, we often tend to overlook its flaws and drawbacks. It is a human tendency. The only way to rectify this problem is by getting your work reviewed by the right people.For example, at Analytics Vidhya, once I have written a blog article on a project, I get it cross-checked and reviewed by my teammates. This adds a fresh perspective to my thoughts and the feedback that I receive helps me a lot in further improving the article for the community.Hence, to build a solid data science resume, it is imperative to get it reviewed by industry experts, data scientists, subject matter experts, etc. This is where your networking skills would be useful. Share your resume with the people in the industry and take their feedback. Ask specific questions to these people. For example, you can ask which three out of five of your projects should be there on your resume. Or how you can quantify a certain task you performed in college or in a previous organization.
We have so far seen the essential ingredients to build a great data science resume. However, in todays world, having a good resume alone might not be enough to land that coveted interview call. Especially if you are applying for the Data Scientist role.Your resume should be supplemented with your digital profile as well.We are living and thriving in the midst of a digital revolution. It stands to reason that the recruiting process would incorporate that as well, right?Let me give you my example. I take quite a few data science interviews every week. Before I get on the call or enter the interview room, I always check two things:I additionally look at the projects mentioned on both these platforms. Are the projects relevant to the current role? This helps me visualize the candidates profile so I can structure my questions in a certain manner. I can also gauge whether the skills mentioned by the candidate in his/her resume are reflecting in their GitHub profile.To build an impressive and powerful digital profile, you can take cues from the following ideas:This is not an exhaustive list. There could be other means and tools as well to enhance your digital presence. However, keep in mind that you are building this digital profile to make sure that when you go for your interviews, it should reflect your expertise.It is also not feasible to maintain your profile at every major platform. Therefore, you have to be selective when it comes to shaping up your digital profile.But lets settle this now  LinkedIn and GitHub profiles are mandatory to have. Absolutely no question around that. Apart from these two, you can have a presence on blogs, podcasts, or YouTube as well (but these are good to have profile rather than mandatory ones).This article should be a good starting point for your data science job application process. As I mentioned earlier, having a solid, relevant and impactful resume is mandatory if you want to land that dream data science job.But this is one step in the entire data science interview process. There are multiple rounds you should be aware of (and be prepared for), such as:And so on. Weve covered each of these steps in our comprehensive Ace Data Science Interviews course. The course has several handouts as well to supplement your learning including a comprehensive interview guide with over 240 questions. So start your interview preparation today!",https://www.analyticsvidhya.com/blog/2019/07/how-to-build-effective-data-science-resume-4-key-aspects/
Dont Miss out on these 24 Amazing Python Libraries for Data Science,"Learn everything about Analytics|Overview|Introduction|Python libraries for different data science tasks:|Python Libraries for Data Collection|Beautiful Soup|Scrapy|Selenium|Python Libraries for Data Cleaning and Manipulation|Pandas|PyOD|NumPy||SpaCy|Python Libraries for Data Visualization|Matplotlib|Seaborn|||Bokeh|Python Libraries for Modeling|Scikit-learn||TensorFlow|PyTorch|Python Libraries for Data Interpretability|LIME|H2O|Python Libraries for Audio Processing|LibROSA|Madmom||pyAudioAnalysis|Python Libraries for Image Processing|OpenCV-Python|Scikit-image|Pillow||Python Libraries for Database|psycopg|SQLAlchemy|Python Libraries for Deployment|Flask
||End Notes","Share this:|Related Articles|How to Build an Effective Data Science Resume? 4 Key Aspects that Will Make or Break your Application|11 Superb Data Science Videos Every Data Scientist Must Watch|
Shubham Singh
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Array creation|Basic operations|Histogram|3D Graph,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a massive fan of the Python language. It was the first programming language I learned for data science and it has been a constant companion ever since. Three things stand out about Python for me:In fact, there are so many Python libraries out there that it can become overwhelming to keep abreast of whats out there. Thats why I decided to take away that pain and compile this list of 24 awesome Python libraries covering the end-to-end data science lifecycle.Thats right  Ive categorized these libraries by their respective roles in data science. So Ive mentioned libraries for data cleaning, data manipulation, visualization, building models and even model deployment (among others). This is quite a comprehensive list to get you started on your data science journey using Python.Have you ever faced a situation where you just didnt have enough data for a problem you wanted to solve? Its an eternal issue in data science. Thats why learning how to extract and collect data is a very crucial skill for a data scientist. It opens up avenues that were not previously possible.So here are three useful Python libraries for extracting and collection data.One of the best ways of collecting data is by scraping websites (ethically and legally of course!). Doing it manually takes way too much manual effort and time. Beautiful Soup is your savior here.Beautiful Soup is an HTML and XML parser which creates parse trees for parsed pages which is used to extract data from webpages. This process of extracting data from web pages is called web scraping.Use the following code to install BeautifulSoup:Heres a simple code to implement Beautiful Soup for extracting all the anchor tags from HTML:I recommend going through the below article to learn how to use Beautiful Soup in Python:Scrapy is another super useful Python library for web scraping. It is an open source and collaborative framework for extracting the data you require from websites. It is fast and simple to use.Heres the code to install Scrapy:It is a framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format.Heres a simple code to implement Scrapy:Heres the perfect tutorial to learn Scrapy and implement it in Python:Selenium is a popular tool for automating browsers. Its primarily used for testing in the industry but is also very handy for web scraping. Selenium is actually becoming quite popular in the IT field so Im sure a lot of you would have at least heard about it.We can easily program a Python script to automate a web browser using Selenium. It gives us the freedom we need to efficiently extract the data and store it in our preferred format for future use.I wrote an article recently about scraping YouTube video data using Python and Selenium:Alright  so youve collected your data and are ready to dive in. Now its time to clean any messy data we might be faced with and learn how to manipulate it so our data is ready for modeling.Here are four Python libraries that will help you do just that. Remember, well be dealing with both structured (numerical) as well as text data (unstructured) in the real-world  and this list of libraries covers it all.When it comes to data manipulation and analysis, nothing beats Pandas. It is the most popular Python library, period. Pandas is written in the Python language especially for manipulation and analysis tasks.The name is derived from the term panel data, an econometrics term for datasets that include observations over multiple time periods for the same individuals.  WikipediaPandas come pre-installed with Python or Anaconda but heres the code in case required:Pandas provides features like:Here is an article and an awesome cheatsheet to get your Pandas skills right up to scratch:Struggling with detecting outliers? Youre not alone. Its a common problem among aspiring (and even established) data scientists. How do you define outliers in the first place?Dont worry, the PyOD library is here to your rescue.PyOD is a comprehensive and scalable Python toolkit for detecting outlying objects. Outlier detection is basically identifying rare items or observations which are different significantly from the majority of data.You can download pyOD by using the below code:How does PyOD work and how can you implement it on your own? Well, the below guide will answer all your PyOD questions:NumPy, like Pandas, is an incredibly popular Python library. NumPy brings in functions to support large multi-dimensional arrays and matrices. It also brings in high-level mathematical functions to work with these arrays and matrices.NumPy is an open-source library and has multiple contributors. It comes pre-installed with Anaconda and Python but heres the code to install it in case you need it at some point:Below are some of the basic functions you can perform using NumPy:And a whole lot more!Weve discussed how to clean and manipulate numerical data so far. But what if youre working on text data? The libraries weve seen so far might not cut it.Step up, spaCy. It is a super useful and flexible Natural Language Processing (NLP) library and frameworkto clean text documents for model creation. SpaCy is fast as compared to other libraries which are used for similar tasks.To install Spacy in Linux:To install it on other operating systems, go through this link.Of course we have you covered for learning spaCy:So whats next? My favorite aspect of the entire data science pipeline  data visualization! This is where our hypotheses are checked, hidden insights are unearthed and patterns are found.Here are three awesome Python libraries for data visualization.Matplotlib is the most popular data visualization library in Python. It allows us to generate and build plots of all kinds. This is my go-to library for exploring data visually along with Seaborn (more of that later).You can install matplotlib through the following code:Below are a few examples of different kind of plots we can build using matplotlib:Since weve covered Pandas, NumPy and now matplotlib, check out the below tutorial meshing all these three Python libraries:Seaborn is another plotting library based on matplotlib. It is a python library that provides high level interface for drawing attractive graphs. What matplotlib can do, Seaborn just does it in a more visually appealing manner.Some of the features of Seaborn are:You can install Seaborn using just one line of code:Lets go through some cool graphs to see what seaborn can do:Heres another example:Got to love Seaborn!Bokeh is an interactive visualization library that targets modern web browsers for presentation. It provides elegant construction of versatile graphics for a large number of datasets.Bokeh can be used to create interactive plots, dashboards and data applications. Youll be pretty familiar with the installation process by now:Feel free to go through the following article to learn more about Bokeh and see it in action:And weve arrived at the most anticipated section of this article  building models! Thats the reason most of us got into data science in the first place, isnt it?Lets explore model building through these three Python libraries.Like Pandas for data manipulation and matplotlib for visualization, scikit-learn is the Python leader for building models. There is just nothing else that compares to it.In fact, scikit-learn is built on NumPy, SciPy and matplotlib. It is open source and accessible to everyone and reusable in various contexts.Heres how you can install it:Scikit-learn supports different operations that are performed in machine learning like classification, regression, clustering, model selection, etc. You name it  and scikit-learn has a module for that.Id also recommend going through the following link to learn more about scikit-learn:Developed by Google, TensorFlow is a popular deep learning library that helps you build and train different models. It is an open source end-to-end platform. TensorFlow provides easy model building, robust machine learning production, and powerful experimentation tools and libraries.An entire ecosystem to help you solve challenging, real-world problems with Machine Learning  GoogleTensorFlow provides multiple levels of abstraction for you to choose from according to your need. It is used for building and training models by using the high-level Keras API, which makes getting started with TensorFlow and machine learning easy.Go through this link to see the installation processes. And get started with TensorFlow using these articles:What is PyTorch? Well, its a Python-based scientific computing package that can be used as:Go here to check out the installation process for different operating systems.PyTorch offers the below features:Here are two incredibly detailed and simple-to-understand articles on PyTorch:Do you truly understand how your model is working? Can you explain why your model came up with the results that it did? These are questions every data scientist should be able to answer. Building a black box model is of no use in the industry.So, Ive mentioned two Python libraries that will help you interpret your models performance.LIME is an algorithm (and library) that can explain the predictions of any classifier or regressor. How does LIME do this? By approximating it locally with an interpretable model. Inspired from the paper Why Should I Trust You?: Explaining the Predictions of Any Classifier, this model interpreter can be used to generate explanations of any classification algorithm.Installing LIME is this easy:This article will help build an intuition behind LIME and model interpretability in general:Im sure a lot of you will have heard of H2O.ai. They are market leaders in automated machine learning. But did you know they also have a model interpretability library in Python?H2Os driverless AI offers simple data visualization techniques for representing high-degree feature interactions and nonlinear model behavior. It provides Machine Learning Interpretability (MLI) through visualizations that clarify modeling results and the effect of features in a model.Go through the following link to read more about H2Os Driverless AI perform MLI.Audio processing or audio analysis refers to the extraction of information and meaning from audio signals for analysis or classification or any other task. Its becoming a popular function in deep learning so keep an ear out for that.LibROSA is a Python library for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems.Click this link to check out the installation details.Heres an in-depth article on audio processing and how it works:The name might sound funny, but Madmom is a pretty nifty audio data analysis Python library. It is an audio signal processing library written in Python with a strong focus on music information retrieval (MIR) tasks.You need the following prerequisites to install Madmom:And you need the below packages to test the installation:The code to install Madmom:We even have an article to learn how Madmom works for music information retrieval:pyAudioAnalysis is a Python library for audio feature extraction, classification, and segmentation. It covers a wide range of audio analysis tasks, such as:You can install it by using the following code:You must learn how to work with image data if youre looking for a role in the data science industry. Image processing is becoming ubiquitous as organizations are able to collect more and more data (thanks largely to advancements in computational resources).So make sure youre comfortable with at least one of the below three Python libraries.When it comes to image processing, OpenCV is the first name that comes to my mind. OpenCV-Python is the Python API for image processing, combining the best qualities of the OpenCV C++ API and the Python language.It is mainly designed to solve computer vision problems.OpenCV-Python makes use of NumPy, which weve seen above. All the OpenCV array structures are converted to and from NumPy arrays. This also makes it easier to integrate with other libraries that use NumPy such as SciPy and Matplotlib.Install OpenCV-Python in your system:Here are two popular tutorials on how to use OpenCV in Python:Another python dependency for image processing is Scikit-image. It is a collection of algorithms for performing multiple and diverse image processing tasks.You can use it to perform image segmentation, geometric transformations, color space manipulation, analysis, filtering, morphology, feature detection, and much more.We need to have the below packages before installing scikit-image:And this is how you can install scikit-image on your machine:Pillow is the newer version of PIL (Python Imaging Library). It is forked from PIL and has been adopted as a replacement for the original PIL in some Linux distributions like Ubuntu.Pillow offers several standard procedures for performing image manipulation:How to install Pillow? Its this simple:Check out the following AI comic illustrating the use of Pillow in computer vision:Learning how to store, access and retrieve data from a database is a must-have skill for any data scientist. You simply cannot escape from this aspect of the role. Building models is great but how would you do that without first retrieving the data?Ive picked out two Python libraries related to SQL that you might find useful.Psycopg is the most popular PostgreSQL (an advanced open source relational database ) adapter for the Python programming language. At its core, Psycopg fully implements the Python DB API 2.0 specifications.The current psycopg2 implementation supports:Heres how you can install psycopg2:Ah, SQL. The most popular database language. SQLAlchemy, a Python SQL toolkit and Object Relational Mapper, gives application developers the full power and flexibility of SQL.It is designed for efficient and high-performing database access.SQLAlchemy considers the database to be a relational algebra engine, not just a collection of tables.To install SQLAlchemy, you can use the following line of code:Do you know what model deployment is? If not, you should learn this ASAP. Deploying a model means putting your final model into the final application (or the production environment as its technically called).Flask is a web framework written in Python that is popularly used for deploying data science models. Flask has two components:Check out the example below to print Hello world:The below article is a good starting point to learn Flask:In this article, we saw a huge bundle of python libraries that are commonly used while doing a data science project. There are a LOT more libraries that are out there but these are the core ones every data scientist should know.Any Python libraries I missed? Or any library from our list which you particularly found useful? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/07/dont-miss-out-24-amazing-python-libraries-data-science/
11 Superb Data Science Videos Every Data Scientist Must Watch,"Learn everything about Analytics|Overview|Introduction|Without any further ado, here are 11 awesome Data Science Videos:|Natural Language Processing (NLP)|XLNet Explained|How does Google Duplex work?|Googles POEMPORTRAITS: Combining Art and AI|Generative Models|Dive into Variational Autoencoders!|Create Facial Animation from Audio|MuseNet Learned to Compose Mozart, Bon Jovi, and More|Reinforcement Learning|Teaching a Computer to Drive|Learn how Google DeepMinds AlphaGo Zero works|Google DeepMinds AI learns to Walk|AI learns to play 2048|BONUS: Adobe develops AI to detect Photoshopped Faces|End Notes","Share this:|Related Articles|Dont Miss out on these 24 Amazing Python Libraries for Data Science|10 Artificial Intelligence (AI) Startups in India You Should Know|
Khyati Mahendru
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love learning and understanding data science concepts through videos. I simply do not have the time to pour through books and pages of text to understand different ideas and topics. Instead, I get a much better overview of concepts via videos and then pick and choose the topics I want to learn more about.The sheer quality and diversity of topics available on platforms like YouTube never ceases to amaze. I recently learned about the amazing XLNet framework for NLP from a video (which I have mentioned below for your consumption). This helped me grasp the concept so I could explore more about XLNet!I strongly believe structure is very necessary when were learning any concept or topic. I follow that approach each time I write an article as well. Thats why Ive categorized these videos into their respective domains, primarily Natural Language Processing (NLP), Generative Models and Reinforcement Learning.So are you ready to dive in and explore the length and breadth of data science through these fascinating videos?MuseNet Learned to Compose Mozart, Bon Jovi, and MoreXLNet is the hottest framework in NLP right now. You simply must be aware of what it is and how it works if you want to carve out a career in this field. I came across this video recently and wanted to share it with the community as soon as possible.XLNet is the latest state-of-the-art NLP framework. It has outperformed Googles BERT on 20 NLP tasks and achieved state-of-the-art results on 18 of them. That is very, very impressive.Make sure you check out our article covering XLNet and its powerful ability here.The below video provides a clear explanation of the original XLNet research paper. Note: You might need to know a few NLP concepts beforehand to truly grasp the inner workings of XLNet.Remember when Sundar Pichai went on stage and sent the whole world into a frenzy when he unveiled Google Duplex in his keynote at Google I/O 2018? I remember listening in complete awe to the super-realistic calls that the AI made.It took a bit of time for the data science and NLP community to come up with an explanation as to how Google Duplex actually works. Its pretty powerful and has the potential to change how we interact with machines.So the million dollar question  did Google Duplex pass the Turing Test!? You decide after watching this video:I am an artist and the prospect of combining any art form with Artificial Intelligence is extremely enticing. In a world where there is so much fear around AI, such applications are more than welcome.Googles POEMPORTRAITS AI has been trained on nineteenth-century poetry using NLP techniques. You can contribute and donate a word to generate your own POEMPORTRAIT. Check out how this awesome concept works:Heres one of our favorite reinforcement learning experts Xander Streenbrugge from his wonderful ArxivInsights channel.Variational Autoencoders (VAEs) are powerful generative models with diverse applications. You can generate human faces or synthesize your own music or use VAEs for removing noise from images.I like this video a lot. Xander begins with an introduction to basic autoencoders and then goes into VAEs and disentangled beta-VAEs. Quite technical, but explained beautifully and concisely, in typical Xander style.Xander is coming back to DataHack Summit this year so you can hear from him and meet him in person!I was immediately drawn to the video when I read the title. This is Generative Models at their best! You can not only generate facial animation from audio but also generate different emotions for the same audio. And the facial expressions look incredibly natural.If you arent following Two Minute Papers, youre missing out. They regularly churn out videos breaking down the latest developments in easy-to-understand fashion. Its a gem of a channel.Another entry from the Two Minute Papers archive.OpenAIs MuseNet is a deep neural network that generates musical compositions with different instruments and combines different styles. It uses the same general-purpose unsupervised technology as GPT-2 and the results are amazing.Never heard of GPT-2? Its an NLP framework on par with XLNet. Check out how MustNet works here:Self-driving cars have always fascinated me. The sheer scale of an autonomous vehicles project is staggering. There are so many components, both on the hardware side as well as the data science side, that need to align for this project to work.This is a perfect video for beginners to learn about Genetic Programming and Reinforcement Learning and how they are used to create powerful applications. Simons personality kept me hooked until the very end.And I am definitely trying the project on my own.Another great video by Xander. He explains Google DeepMinds popular paper on AlphaGo Zero.AlphaGo Zero is a new version of the original AlphaGo program that beat human champion Lee Sedol comprehensively. I recommend reading our article on Monte Carlo Tree Search, the algorithm behind AlphaGo before proceeding to learn about AlphaGo Zero.AlphaGo Zero uses Reinforcement Learning to beat the worlds leading Go players without using data from human games.AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.Source: WikipediaThis video is both hilarious and informative. Exactly the type of video I like when Im learning new things! It was funny to watch the AI learn to walk. But at the same time, it left me marveling at the power of Reinforcement Learning.The video discusses 3 papers to try and explain how the AI learned to walk and it is surprisingly simple to understand.Have you ever played the 2048 game? It is super addictive once you get the hang of it. I used to easily finish games earlier but not anymore. Being a data science enthusiast, I am going to train my computer to play it with the help of this awesome video.This is another example of the use of Genetic Programming and Evolutionary Algorithms.Adobe is a market leader in image and video manipulation software. Other companies have tried, but not many have even gotten close to Adobes level.Last month, Adobe announced its research efforts to detect manipulated images. High time someone did that! It will soon be impossible to tell real from fake given how quickly GANs have taken over the world.Imagine Donald Trump challenging Kim Jong Un to a nuclear war and then claiming that it was a deepfake and shrugging off all responsibility! We need to avoid those situations turning into reality. This video shows how Adobes algorithm works and tried to combat fake images:I love knowing about the latest research in data science, machine learning and AI. But I find it hard to read papers. It takes a lot of time and effort  something not every data science professional has. I am sure many of you struggle with the same. Consuming videos is the ideal way to get an overview of these concepts.You can then pick and choose where your interests lie and try to spin up a project or blog post on it. Trust me, its a wonderful way to learn and ingrain new data science concepts.What are some of your favorite channels or videos on data science? Lets discuss in the comments below.",https://www.analyticsvidhya.com/blog/2019/07/11-data-science-videos-every-data-scientist-must-watch/
10 Artificial Intelligence (AI) Startups in India You Should Know,Learn everything about Analytics|Overview|Introduction|Framework to shortlist the startups|Our Pick of the Best AI Startups in India:|Conversational AI Startups|Haptik.ai|Avaamo|Healthcare Startups|Niramai|Doxper|Logistics Startup|LogiNext|Locus.sh|Fintech Startups|Rubique|LendingKart|Other Awesome AI Startups|CropIn|Niki.ai|End Notes,"Share this:|Related Articles|11 Superb Data Science Videos Every Data Scientist Must Watch|6 Powerful Open Source Machine Learning GitHub Repositories for Data Scientists|
Shubham Singh
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"AI is the new electricity.  Andrew NgWhen Andrew Ng speaks, you drop everything and pay attention. Thats what I (and thousands of others) did when Andrew Ng compared our age of AI to the discovery of electricity.We are truly living in the age of artificial intelligence. Companies are spending billions of Dollars just to stay relevant in todays ever-changing environment. If youre not AI-compatible, the consensus is that you will soon be an also-ran in the industry.This got me thinking  could I bring out the AI startups in India truly bringing about a global revolution? It was an interesting quest. I put down a framework or criteria to filter the top AI startups in India (which we will see soon).So, this article highlights the top Indian AI startups, the top initiators, that are using AI to build a better future for us. Ready to dive in and look at the framework I used to create this list?If youre entirely new to artificial intelligence, here are a few articles to get you started:This list has been curated based on certain parameters which act as indicators for the success of startups. The parameters on which startups have been evaluated are:Chatbots are ubiquitous these days. From businesses to the research lab, they have become an integral part of an organizations strategy. Learning how to create a chatbot from scratch is a much-vaunted skill in data science.Haptik.ai is the worlds largest conversational AI platform. Im a huge Haptik fan. The company was launched back in 2013 as a one-stop app for all everyday tasks and is now undoubtedly a market leader in its field.Haptik provides 16 different channels of texts and voices for the users to build, deploy and manage a conversational application. Haptik only focuses on use cases to enable end consumer engagement on the back of real conversational data of a billion+ messages.With the help of AI, Haptik helps millions in their day to day tasks including me. Its quick response and query solving capability is quite remarkable. Its indeed a personal assistant.Avaamo is another conversational startup that is currently serving six industries: Insurance, Financial Services, Healthcare, Telecommunications, Retail.Founded in 2014, Avaamo has created a name for itself in around 40 countries now. Pretty impressive!Avaamo specializes in conversational interfaces to solve specific, high impact problems in the enterprise. It uses AI to make conversational computing for the enterprise a reality.Unlike the first generation conversational AI where a user could only pass commands like play music or open camera, Avaamo brings in second generation conversational AI experience that executes rich multi-turn conversations capable of handlingqueries in customer service, generating quotes in insurance,or answering claims inquiries in healthcare.This AI startup will resonate with a lot of you. Healthcare is one domain where AI needs to make its mark. Progress has been slow due to various reasons but things have been looking promising in the last couple of years.NIRAMAI stands for Non-Invasive Risk Assessment with Machine Intelligence. In Sanskrit, Niramai means being free from illness. It is a novel breast cancer screening solution.Breast cancer is the leading cause of cancer death today in women.According to WHO, one in every 12 women have the risk of a breast abnormality. Indian women have only about 50% chance of survival.Niramai provides a cost-efficient way to detect breast cancer and is a better alternative to the existing method of mammography which requires a high capital cost. The major drawback of mammography is that it can be used only for women above 45 years of age because it cannot identify tumors effectively for younger women.The core of the NIRAMAI solution is Thermalytix, a computer-aided diagnostic engine that is powered by Artificial Intelligence. Go through this link to know more about how Niramai uses AI for the early detection of cancer.Doxper is another Indian AI startup working in the healthcare sector.Have you ever seen hospital records? Keeping the records of patients is a hectic task  it is truly a formidable function. Doxper helps in simplifying the way healthcare data is recorded.Doxper helps doctors, hospitals, and patients as well. It improves the doctor-patient interaction. Doxper aims to provide easy storage & retrieval, automated patient follow-ups, and auto data transcription.When information is written using Doxper, it gets stored in the cloud and is automatically shared with the patient immediately. This helps in keeping the records safe and makes communicating with the patient really simple.Hospitals can use it to immediately digitize Casualty/Emergency, OPD, and ICU. This can also be used in existing HIS systems through intuitive APIs.This is one remarkable product helping the healthcare sector to digitize the records of their patients.AI has left no stone untouched. It has found its niche in almost every sector, even the previously gigantic and manual logistics field. There are quite a few AI-powered logistics startups springing up and LogiNext is definitely among the leaders right now.LogiNext helps in managing field services. It tracks and optimizes field agent movements in real time on a single map interface.LogiNext helps organizations plan and manage the dispatch schedule, delivery routes and capacity in the most cost-optimized way.It empowers its users by providing them the power to track (real-time) the shipment every single minute. It provides insights and visualizations based on predictive and big data analytics. Logistics analytics helps the user to accurately predict the future with algorithm-enabled location intelligence and optimize the logistics and field service management operations.It provides every single detail right from pickup to delivery. It also provides complete field service management analytics.The logistics and field workforce management solutions are fully automated, effective, secure and can be seamlessly integrated with multiple platforms providing complete logistics automation.Another logistics startup in the list is Locus. Started back in 2015, the company provides facilities like route planning and optimizing, real-time fleet tracking, insights and analytics, and automated shipment sorting and rider allocation.Locus.sh offers a number of features, including:AI in finance just intuitively makes sense. Finance is all about number crunching (well, almost!) and machines are well equipped at this point to work with numbers. It is a perfect match. So its no surprise that the FinTech sector has seen a massive surge in AI applications.Rubique is one such FinTech startup that is contributing to making finance simple by using AI.Rubique wants to revolutionize the finance industry by introducing predictability. This helps its users to find the best match to his/her credit requirements with the help of an AI-based recommendation engine and Rubiques financial matchmaking platform.Rubiques multi-sided lending platform provides features like e-KYC, bank statement analysis, credit bureau check, credit memo generation & MCA integration along with real-time application tracking to make it a paperless experience.Rubique is doing a great job of providing top-notch solutions to the entire lending spectrum.LendingKart is another brilliant startup tackling the financial sector by providing loans to small businesses. Spread over 1300+ cities, LendingKart is on its way to becoming one of the leading FinTech companies in the world. Heres their official statement:LENDINGKART Group aims to make working capital finance available at the fingertips of entrepreneurs, so that they can focus on business instead of worrying about the gaps in their cash-flows.LendingKart has developed technology tools based on big data analysis which facilitates lenders to evaluate borrowers creditworthiness. They dont focus on past records of the vendor. The main aim of LendingKart is to make capital funds available to help entrepreneurs focus on their business instead of worrying about cash-flow flaws.The agriculture sector is synonymous with Indian values. It is an integral part of this country and what we are all about. So what can AI do to accelerate progress in this field? CropIn, a Bengaluru-based startup, provides a glimpse into the future of agriculture.CropIn is a smart farming app that provides future-ready farming solutions to the entire sector. It is an intuitive, intelligent and self-evolving system. The company provides real-time solutions to predict trends, archive patterns and to make a blueprint for future businesses.CropIn has the capability of live reporting, and it also provides geographical analysis, interpretation, and insight. Data gathering through a smartphone app ensures efficient operations, lower costs and better visibility for your field agents at all times.CropIns vision is to maximize per acre value and the mission is to make every farm traceable.Real-time insights help to take planned and responsive business decisions. The predictability of quantity & quality of yield combined with a reduced cost of operations results in higher productivity for businesses.Imagine an AI helping you perform online transactions end-to-end. Sounds too futurustic to be true. But heres the good news  its already here!Niki.ai is a virtual agent that does end-to-end online transactions for you in various domains. Everything from postpaid recharges to hotel bookings, Niki covers the length and breadth of it.Niki uses AI to book and make payments after taking a few inputs from the users. This reduces the end-to-end interaction time of the customer and service provider and helps in providing fast and responsive services.Booking movie tickets, household bills, bus booking, event booking, etc.  you name and it and Niki does it.In this article, we saw some of the most amazing AI startups in India that are changing the way we live. This is in no way an exhaustive list. There are a LOT of AI startups springing up right now that bring their own unique brand to the table.If you come across any other Indian AI-based startup which holds the capacity to make an impact and affect millions in a positive way, let me know in the comments section. Till then  keep learning!",https://www.analyticsvidhya.com/blog/2019/07/10-ai-startups-india/
6 Powerful Open Source Machine Learning GitHub Repositories for Data Scientists,Learn everything about Analytics|Overview|Introduction|Top Machine Learning GitHub Repositories|XLNet: The Next Big NLP Framework|Implementation of XLNet in PyTorch|Google Research Football  A Unique Reinforcement Learning Environment|Implementation of the CRAFT Text Detector|MMAction  Open Source Toolbox for Action Understanding in Videos|TRAINS  Auto-Magical Experiment Manager & Version Control for AI|End Notes,"Share this:|Related Articles|10 Artificial Intelligence (AI) Startups in India You Should Know|Top 5 Must-Read Answers  What does a Data Scientist do on a Daily Basis?|
Pranav Dar
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you sometimes feel that machine learning is too broad and vast to keep up? I certainly feel that way. Just check out the list of major developments in Natural Language Processing (NLP) in the last year:It can become overwhelming as a data scientist to simply keep track of all thats happening in machine learning. My aim of running this GitHub series since January 2018 has been to take that pain away for our community.We trawl through every open source machine learning release each month and pick out the top developments we feel you should absolutely know. This is an ever-evolving field  and data scientists should always be on top of these breakthroughs. Otherwise, we risk being left behind.This months machine learning GitHub collection is quite broad in its scope. Ive covered one of the biggest NLP releases in recent times (XLNet), a unique approach to reinforcement learning by Google, understanding actions in videos, among other repositories.Fun times ahead so lets get rolling!You can also go through the GitHub repositories and Reddit discussions weve covered so far this year:Of course we are starting with NLP. It is the hottest field in machine learning right now. If you thought 2018 was a big year (and it was), 2019 has taken up the mantle now.The latest state-of-the-art NLP framework is XLNet. It has taken the NLP (and machine learning) community by storm. XLNet uses Transformer-XL at its core. The developers have released a pretrained model as well to help you get started with XLNet.XLNet has so far outperformed Googles BERT on 20 NLP tasks and achieved state-of-the-art performance on 18 such tasks. Here are a few results on popular NLP benchmarks for reading comprehensions:Want more? Here are the results for text classification:XLNet is, to put it mildly, very impressive. You can read the full research paper here.Wait  were you wondering how you can implement XLNet on your machine? Look no further  this repository will get you started in no time.If youre well versed with NLP features this will be pretty simple to understand. But if youre new to this field, take a few moments to go through the documentation I mentioned above and then try this out.The developer(s) has also provided the entire code in Google Colab so you can leverage GPU power for free! This is a framework you DONT want to miss out on.Im a huge football fan so the title of the repository instantly had my attention. Google Research and football  what in the world do these two have to do with each other?Well, this repository contains a reinforcement learning environment based on the open-source game Gameplay Football. This environment was created exclusively for research purposes by the Google Research team. Here are a few scenarios produced within the environment:Agents are trained to play football in an advanced, physics-based 3D simulator. Ive seen a few RL environments in the last couple of years but this one takes the cake.The research paper makes for interesting reading, especially if youre a football or reinforcement learning enthusiast (or both!). Check it out here.This is a fascinating concept. CRAFT stands for Character Region Awareness for Text Detection. This should be on your to-read list if youre interested in computer vision. Just check out this GIF:Can you figure out how the algorithm is working? CRAFT detects the text area by exploring each character region present in the image. And the bounding box of the text? That is obtained by simply finding minimum bounding rectangles on a binary map.Youll grasp CRAFT in a jiffy if youre familiar with the concept of object detection. This repository includes a pretrained model so you dont have to code this algorithm from scratch!You can find more details and an in-depth explanation of CRAFT in this paper.Ever worked with video data before? Its a really challenging but rewarding experience. Just imagine the sheer amount of things we can do and extract from a video.How about understanding the action being performed in a particular video frame? Thats what the MMAction repository does. It is an open source toolbox for action understanding based on PyTorch. MMAction can perform the below tasks, as per the repository:MMActions developers have also provided tools to deal with different kinds of video datasets. The repository contains a healthy number of steps to at least get you up and running.Here is the getting started guide for MMAction.One of the most crucial, and yet overlooked, aspects of a data scientists skillset  software engineering. It is an intrinsic part of the job. Knowing how to build models is great, but its equally important to understand the software side of your project.If youve never heard of version control before, rectify that immediately. TRAINS records and manages various deep learning research workloads and does so with practically zero integration costs.The best part about TRAINS (and there are many) is that its free and open source. You only need to write two lines of code to fully integrate TRAINS into your environment. It currently integrates with PyTorch, TensorFlow, and Keras and also supports Jupyter notebooks.The developers have set up a demo server here. Go ahead and try out TRAINS using whatever code you want to test.My pick for this month is surely XLNet. It has opened up endless opportunities for NLP scientists. Theres only one caveat though  it requires strong computational power. Will Google Colab come to the rescue? Let me know if youve tried it out yet.On a relevant note, NLP is THE field to get into right now. Developments are happening at breakneck speed and I can easily predict theres a lot more coming this year. If you havent already, start delving into this as soon as you can.Are there any other machine learning GitHub repositories I should include in this list? Which one did you like from this months collection? Lets discuss in the comments section below.",https://www.analyticsvidhya.com/blog/2019/07/6-powerful-open-source-machine-learning-github-repositories-data-scientists/
Top 5 Must-Read Answers  What does a Data Scientist do on a Daily Basis?,Learn everything about Analytics|Overview|Introduction|Machine Learning is Very Process Oriented  Mike West|A Percentage-wise Breakdown of a Data Scientists Day-to-Day Role  Vinita Silaparasetty|Data Scientist Perspective from a Small-Sized Company  Justin Fister|The Data Scientist is a bit of a Myth  Tim Kiely|Machine Learning Engineer Working on NLP Tasks  Evan Pete Walsh|End Notes,"Share this:|Related Articles|6 Powerful Open Source Machine Learning GitHub Repositories for Data Scientists|Announcing DataHack Summit 2019  The Biggest Artificial Intelligence and Machine Learning Conference Yet|
Shubham Singh
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",|||,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a curious person by nature. Whenever I come across a concept I havent heard of before, I cant wait to dig in and find out how it works. This has come in quite handy in my own data science journey.But before I landed my first break in data science, I was always curious about what data scientists actually did every day. Was I supposed to simply build models all the time? Or was the oft-quoted saying about spending 70-80% of our time cleaning data actually true?Im sure you have asked (or at least wondered) about this too. The role of a data scientist might be the sexiest job of the 21st century, but what does that entail on a day-to-day basis?I decided to research this. I wanted to expand my horizons and understand how data scientists look at their role in different domains (such as NLP). This helped me gain a broader understanding of our role and why we should always read different perspectives when it comes to data science.So, here is a list of the top 5 answers to help you get a sense of what the typical routine of a data scientist is. Prepare to be surprised  building models isnt the primary (and only) function in a data scientists day-to-day tasks!I also encourage you to take part in a discussion on this question here. This will enrich your current understanding of what a data scientist does and your thoughts will foster a discussion among our community!Note: I have taken the answers verbatim from Quora and added my thoughts right at the beginning of each answer. This will help you get a good perspective of what the answer covers without diluting the authors thoughts. Enjoy!I like this answer because its crisp, to-the-point and simple. The author has even designed a flow diagram and explained his thought process in a wonderfully illustrated way. Here is his answer in full:Machine learning is very process oriented. Therefore, Im always somewhere in one of the pictures below:Machine learning engineers spend a ton of time in the first two pictures (or stages). The fun part is really in the third stage but its only a small part of what happens in the real world.Some key things to keep in mind about data science in the real world:I really like the use of visualization by Vinita. The percentage-wise description of each data science task is helpful and insightful. Vinita has also leaned on her experience to explain the step-by-step work a data scientist does. Its a must-read answer!Contrary to popular belief, Data Science is not all glamour. The following survey results by CrowdFlower accurately sum up a typical day for a Data Scientist:There is a lot of backtracking involved. Sometimes you even need to be able to predict what consequences removing/adding a variable might have.This is a superb answer and one I can relate to. Note that machine learning, the most anticipated aspect of a data scientists job, only occupies 5% of the total time! Just like Vinita, he has also explained his tasks in terms of percentage. Here is Justins view:The author, Tim Kiely, uses a Venn diagram to explain what data science is. Just take a look at this Venn diagram below  it will blow your mind. Tim additionally talks about what data scientists are supposed to be by taking a somewhat contradictory view of the general definition. Here is Tims answer:The Data Scientist is a bit of a myth, in my opinion. Not to say they arent out there but they are far rarer than is popularly understood and are more of the exception than the rule.I liken it to the Web Master title of the dot-com bubble  these supposed people who could do full stack programming, front end development, marketing, everything. All of those roles/skills were always specialized and remain so today.Data Scientists are supposed to be database architects, understand distributed computing, have a deep understanding of statistics AND some area of business or field expertise. Thats asking a lot when any one of those skill sets can take a career to build.

The Data Scientists Ive worked with typically have a Ph.D. in A.I. or Machine learning and are effective communicators, which gives them the ability to direct the analysts, DevOps people, programmers and DBAs at their disposal to solve problems with data-driven solutions. They outline the desired solution and leave it to their teams to fill in the gaps.Lets drill down into a particular specialization of machine learning. One of my favorites  Natural Language Processing (NLP)! I wanted to bring out a machine learning engineers view here (a role every data scientist should become familiar with). Check out Evans full response:Currently working on NLP, for the most part, including intent classification and entity extraction. Heres a typical day for me:The data scientist role is truly multi-faceted, isnt it? A LOT of aspiring data scientists assume that they will primarily be building models all day long but that simply isnt the case.There are all sorts of tasks involved in a typical data science project which youll find yourself working on day-to-day. I quite like that because it opens up avenues to learn new concepts and apply them in the real world.Ill be posting some more career-related articles on Analytics Vidhya, so stay tuned and keep learning!",https://www.analyticsvidhya.com/blog/2019/06/what-does-data-scientist-do-daily-basis-top-5-quora-answers/
Announcing DataHack Summit 2019  The Biggest Artificial Intelligence and Machine Learning Conference Yet,Learn everything about Analytics|DataHack Summit 2019|Bringing Together Futurists to Achieve Super Intelligence|Speakers at DHS 2019|DataHack Summit 2019 by the Numbers|Wait..A Day Full of Exclusively Hack Sessions?|What kind of content can you expect at DHS 2019?|How can you attend DataHack Summit 2019?,"Yes, we are about to unleash the era of Super AI at DataHack Summit 2019, 13-16 November in Bengaluru  the largest Artificial Intelligence and Machine Learning conference yet!|Click here to book your ticket|Book your DataHack Summit 2019 Tickets here|Share this:|Related Articles|Top 5 Must-Read Answers  What does a Data Scientist do on a Daily Basis?|6 Useful Programming Languages for Data Science You Should Learn (that are not R and Python)|
Pranav Dar
","Mark the date in your calendar  November 15th, 2019  is an exclusive Hack Day.",ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"DataHack Summit 2018 was a grand success with more than 1,000 attendees from various diverse industries and domains. Our community loved the content, knowledge sharing, and everything about the event. Check out the highlights from DHS 2018:So what do you do when you scale such heights?You continue scaling up!Analytics Vidhyas mission to bring together people, machines and their collaborative experience is about to be unleashed to a whole new level. A level that hasnt been seen before.Records are made to be broken  and DHS 2019 will break them again. There will be more Power Talks, more Hack Sessions, more Workshops, and more content than ever before. And we are adding an extra day to the conference  just for hack sessions (more on this below)!If you are a data professional, or someone who wants to see machine intelligence unleashing a new era in our world  DataHack Summit 2019 is the place you want to be. This years conference is between 13th  16th November in Bengaluru, India.Tickets are selling at an unprecedented pace. The Early Bird discount ends soon but the tickets might be sold out before that  so make sure you grab a seat today:We bring you the top thought leaders in Artificial Intelligence and Machine Learning, and the top Data Scientists and Machine Learning Engineers from across the globe. Heres a list of the top speakers confirmed for the conference:And many more which you can check out the Speakers page. Are you interested in submitting a speaker proposal? Then fill in this form and we will get in touch with you accordingly.The sheer scale of DataHack Summit 2019 is staggering in its scope. The learning will increase multifold, records will tumble, and we will witness artificial intelligence and machine learning in action like never before.Thats right! We are thrilled to present Hack Day at DHS 2019.Hack Sessions are one of the most in-demand and popular features of DataHack Summit. We introduced the concept of hack sessions at DHS 2017, expanded on that at DHS 2018, and are ready to truly unleash their power this year at DHS 2019.Hack sessions are an integral part of what we bring to you through this conference and we want to make sure you get full access to all the latest AI and machine learning trends.  Kunal JainHack sessions are essentially hour-long live interactive coding sessions presented by the top data scientists from around the globe  a dream for all machine learning professionals!The content we showcase at DataHack Summit has always been lauded for the breadth and depth of topics we cover. We want to continue that trend and are leaving no stone unturned this year as we unleash the latest developments, breakthroughs, and happenings in AI and machine learning.Heres an overview of what you can expect at DataHack Summit 2019:The Applied ML workshop from DHS 2018  a successful day full of learningTickets are ON SALE right now at an incredible price. Our Early Bird discount is still valid but the tickets are selling out quickly so make sure you book yours today before you miss out.Have you attended DataHack Summit before? What are you most looking forward to seeing this year? And what else do you feel we should add? We would love to hear from you  let us know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/06/announcing-datahack-summit-2019-biggest-ai-machine-learning-conference/
6 Useful Programming Languages for Data Science You Should Learn (that are not R and Python),Learn everything about Analytics|Overview|Introduction|Scala|Julia|JavaScript|Swift|Go (Golang)|Spark|End Notes,"Top Scala Libraries for Data Science|Top Julia Libraries for Data Science|Top JavaScript Libraries for Data Science|Top Swift Libraries for Data Science|Top Go Libraries for Data Science|Top Spark Libraries for Data Science|Share this:|Related Articles|Announcing DataHack Summit 2019  The Biggest Artificial Intelligence and Machine Learning Conference Yet|Simplifying Google AIs Best Paper at ICML 2019 on Unsupervised Learning|
Analytics Vidhya Content Team
|32 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Which programming language should I pick up to start my data science journey?If I started handing out nickels for each time I saw this question  there would be a lot of millionaires! Its easily the most popular question asked by data science enthusiasts. The answer, Im sure youve seen, usually hovers between Python and R.But heres my question  why should we limit ourselves to these two languages? There is a whole world of programming languages we can pick up and apply in this field. And therein lies the beauty of data science  it transcends programming languages.My aim here is to introduce a world beyond Python and R while keeping the core idea behind them. We will cover 6 powerful and useful programming languages for data science that I feel every data scientist should learn (or at least be aware of). All of these languages are open source.And lets face it  we love comparisons. Whether its Apple v Samsung, iOS v Android, MacOS v Windows (or Linux), these comparisons lead to intense discussions. So if this article sparks a debate among our community  thats even better!So, what are these languages and how they are used in the field of data science? Lets find out!Note: I have also provided open source libraries and free tutorials wherever possible to help you get started with each programming language.Scala is a fairly common programming language. Chances are youve either worked on it or come across it at some point (especially if youve worked in IT).Scala is an open source modern multi-paradigm programming language created by Martin Odersky in 2003. Scala stands for Scalable Language. It is designed to express common programming standards in a brief, elegant and type-safe way.Lets put it this way  if you are aware of Javas syntax, youll pick up Scala in a jiffy. In fact, learning Scala will be pretty smooth if you know programming languages like C, C++ or Python. I can already see your enthusiasm starting to light up!So, why Scala? Well, the code we write in Scala is compiled and executed much faster as compared to pure Python (and not specialized libraries like NumPy). I love Scala because of its stability, flexibility, high speed, and scalability. You can use Scala to develop useful products that work with Big Data.Interested in learning Scala? We have the perfect article for you:Julia is coming up big right now in the data science world. If you didnt know this already, its time to get on board. A few experts are already claiming it as a rival to Python! It might be a little too soon for that but it gives us an idea of how useful Julia is.Julia is a refreshingly modern, meaningful and high-performance programming language created by a group of computer scientists and mathematicians at MIT. It is open source and is commonly used for scientific calculations and data manipulations.Youll pick up Julia quickly if youve worked on R, Python or Matlab before. There even exists a scikit-learn library in Julia to help your transition. What else could a data scientist ask for?Again the question comes up  why Julia for data science? There are multiple reasons but the primary one is that the execution speed of Julia is 10x-30x than that of Python and R.You can refer to the below article to learn Julia for data science from scratch:Calling all developers! If you were looking for a way into data science without wanting to learn a new language  JavaScript is your pathway to the jackpot.JavaScript is a powerful, lightweight, and easy-to-implement programming language. It was first launched in Netscape 2.0 in 1995 under the moniker LiveScript.Its good to have some basic knowledge of HTML and prior exposure to object-oriented programming concepts if you want to pick up JavaScript. This will give you a basic idea of creating online applications. This comes in especially handy when youre deploying your machine learning models in mobile apps or in the browser.Apart from this, JavaScript has some excellent libraries for data visualization and creating dashboards. Various machine learning techniques like gesture recognition, object recognition, music composition, etc. can be executed using TensorFlow.js, a powerful JavaScript library for data science.You can get started with machine learning in the browser by following the steps mentioned in the below article:Are you an Apple fan? Do you love using their various devices and their tightly-knit iOS? Well, then youll love Swift.Swift is an open source, easy, and flexible programming language developed by Apple for iOS and OS X apps. Swift builds on the best of C and Objective-C, without the constraints of C compatibility. Its actually a friendly programming language for freshers because of its concise yet expressive syntax and lightning speed to run the apps.Swift has recently started gaining traction among the data science community. It is highly endorsed by Jeremy Howard (fast.ais co-founder). There are various libraries for performing tasks like numerical computation, high-performance functions for matrix math, digital signal processing, applying deep learning methods, building machine learning models, etc.Refer to the below article to learn more about Swift for TensorFlow:How could Google ever stay out of any data science related discussion?Go, as the name suggests, is a programming language created by Google. Simple, reliable, and efficient software  thats Go in a nutshell. What I like about Go is its singular focus. It keeps conflicts at bay by focusing on one method at a time (as opposed to other languages where there are multiple ways to solve a problem).There are a great number of open source tools, packages, and resources for performing data science tasks using Go. This includes data gathering, data organization, data parsing, arithmetic and statistical computations, EDA and building machine learning models, etc.Check out the below discussion to learn more about the important libraries in Go:Spark is more of a framework than a language but youll soon see why its on my list. It is very popular among data engineers and data scientists.Spark provides:It is an open source, fast cluster computing framework which is used for processing, querying and analyzing Big Data. The advantage of Spark over other big data frameworks is that it is based on in-memory computation. This enables computations to run up to a hundred times faster.Basic knowledge of Python is good enough for you to pick up Spark quickly.Spark can perform various data science and data engineering tasks, such as:Heres the perfect article to learn Apache Spark:Dont you love how vast the field is for data science languages? Python and R are wonderful in their own right. But my aim here was to bring out other languages that we can use to perform data science tasks.Some of these languages you might even know right now (Im sure all you developers are aware of JavaScript!)  you just didnt realize you could use it for building awesome visualizations and designing models. Well, now you do!Any language(s) you feel I should have included in the article? Connect with me in the comments section below. I look forward to hearing your thoughts, suggestions, and feedback!",https://www.analyticsvidhya.com/blog/2019/06/6-useful-programming-languages-data-science-r-python/
Simplifying Google AIs Best Paper at ICML 2019 on Unsupervised Learning,Learn everything about Analytics|Overview|Introduction|The Best Paper Award at ICML 2019 Goes to:|Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations|End Notes,"Objective of this Paper|Current State-of-the-Art Approach|Contribution of this Paper to the Field|Experimental Design Proposed by Google AI|Key Experimental Results|Share this:|Related Articles|6 Useful Programming Languages for Data Science You Should Learn (that are not R and Python)|How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models|
Shubham Singh
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"There are only a handful of machine learning conferences in the world that attract the top brains in this field. One such conference, which I am an avid follower of, is the International Conference on Machine Learning (ICML).Folks from top machine learning research companies, like Google AI, Facebook, Uber, etc. come together and present their latest research. Its a conference any data scientist would not want to miss.ICML 2019, held last week in Southern California, USA, saw records tumble in astounding fashion. The number of papers received and the number of papers accepted at the conference  both broke all previous records. Check out the numbers:Source: MediumA panel of hand-picked judges is charged with picking out the best papers from this list. Receiving this best paper award is quite a prestigious achievement  everyone in the research community strives for it!And decrypting these best papers from ICML 2019 has been an eye-opener for me. I love going through these papers and breaking them down so our community can also partake in the hottest happenings in machine learning.In this article, well look at Google AIs best paper from the ICML 2019 conference. There is a heavy focus on unsupervised learning so theres a lot to unpack. Lets dive right in.You can also check out my articles on the best papers from ICLR 2019 here.Our main focus is on the first paper from the Google AI team. So lets check out what Google has put forward for our community.Note: There are certain unsupervised deep learning concepts you should be aware of before diving into this article. I suggest going through the below guides first in case you need a quick refresher:Lets first understand what disentangled representations are. Here is Google AIs succinct and simple definition of the concept:The ability to understand high-dimensional data, and to distill that knowledge into useful representations in an unsupervised manner, remains a key challenge in deep learning. One approach to solving these challenges is through disentangled representations, models that capture the independent features of a given scene in such a way that if one feature changes, the others remain unaffected.  Google AIAs the paper says, in representation learning, it is often assumed that real-world observations x, like images or videos, are generated by a two-step generative process:In other words, a lower dimensional entity, which is mapped to the higher-dimensional space of observation, could be used to explain a high-dimension observation.The objective of this research is to point out the areas of improvement for future work to make unsupervised disentangled methods better.The authors have released a reproducible large-scale experimental study on seven different datasets, including 12,000 models that were trained covering the most prominent methods and evaluation metrics.There is currently no single formalized notion of disentanglement which is widely accepted. So, the key intuition is that a disentangled representation should separate the distinct, informative factors of variations in the data.The current state-of-the-art approaches for unsupervised disentanglement learning are largely based on Variational Autoencoders (VAEs). A specific distribution P(z) is assumed on a latent space and then a deep neural network is used to parameterize the conditional probability P(x|z).Similarly, the distribution P(z|x) is approximated using a variational distribution Q(z|x). The model is then trained by minimizing a suitable approximation to the negative log-likelihood.Google AI researchers have challenged the commonly held assumptions in this field. I have summarized their contributions below:Visualization of the ground-truth factors of the Shapes3D data set: Floor color (upper left), wall color (upper middle), object color (upper right), object size (bottom left), object shape (bottom middle), and camera angle (bottom right)I have taken this section from within the paper itself. If you have any queries, you can reach out to me in the comments section below the article and Ill be happy to clarify them.Considered methods:All the considered methods augment the VAE (Variational Autoencoders) loss with some regularizer.Considered metrics:Datasets:The Scream PaintingThis is the part that will get every data scientist out of their seats! The researchers have showcased their results by answering a set of questions.Total correlation based on a fitted Gaussian of the sampled (left) and the mean representation (right) plotted against regularization strength for Color-dSprites and approaches (except AnnealedVAE). The total correlation of the sampled representation decreases while the total correlation of the mean representation increases as the regularization strength is increased(left) FactorVAE score for each method on Cars3D. Models are abbreviated (0=- VAE, 1=FactorVAE, 2=-TCVAE, 3=DIP-VAE-I, 4=DIP-VAE-II, 5=AnnealedVAE). The scores are heavily overlapping. (right) Distribution of FactorVAE scores for FactorVAE model for different regularization strengths on Cars3D.Statistical efficiency of the FactorVAE Score for learning a GBT downstream task on dSprites.The Google AI team continues to nail its machine learning research. They continue to be on top of the latest advacements, this years International Conference of Machine Learning.The second paper selected is based on how the results could be made better in Gaussian Process Regression, you can check out the paper through the link provided in this article.Let me know about your views on the Google AI research paper in the comments section below. Keep learning!",https://www.analyticsvidhya.com/blog/2019/06/simplifying-google-ai-best-paper-icml-2019/
How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models,Learn everything about Analytics|Overview|Introduction|Table of Contents|Sequence-to-Sequence Models  A Backdrop|Introduction to the Transformer|Understanding Transformer-XL|The New Sensation in NLP: Googles BERT (Bidirectional Encoder Representations from Transformers)|End Notes,"RNN based Sequence-to-Sequence Model|Challenges|Understanding Transformers Model Architecture|Getting a Hang of Self-Attention|Calculating Self-Attention|Limitations of the Transformer|Using Transformer for Language Modeling
|Using Transformer-XL for Language Modeling|BERTs Model Architecture|BERT Pre-Training Tasks|Share this:|Related Articles|Simplifying Google AIs Best Paper at ICML 2019 on Unsupervised Learning|The AI Comic: Z.A.I.N  Issue #2: Facial Recognition using Computer Vision|
Prateek Joshi
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",1. Masked Language Modeling (MLM)|2. Next Sentence Prediction,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love being a data scientist working in Natural Language Processing (NLP) right now. The breakthroughs and developments are occurring at an unprecedented pace. From the super-efficient ULMFiT framework to Googles BERT, NLP is truly in the midst of a golden era.And at the heart of this revolution is the concept of the Transformer. This has transformed the way we data scientists work with text data  and youll soon see how in this article.Want an example of how useful Transformer is? Take a look at the paragraph below:The highlighted words refer to the same person  Griezmann, a popular football player. Its not that difficult for us to figure out the relationships among such words spread across the text. However, it is quite an uphill task for a machine.Capturing such relationships and sequence of words in sentences is vital for a machine to understand a natural language. This is where the Transformer concept plays a major role.Note: This article assumes a basic understanding of a few deep learning concepts:Sequence-to-sequence (seq2seq) models in NLP are used to convert sequences of Type A to sequences of Type B. For example, translation of English sentences to German sentences is a sequence-to-sequence task.Recurrent Neural Network (RNN) based sequence-to-sequence models have garnered a lot of traction ever since they were introduced in 2014. Most of the data in the current world are in the form of sequences  it can be a number sequence, text sequence, a video frame sequence or an audio sequence.The performance of these seq2seq models was further enhanced with the addition of the Attention Mechanism in 2015. How quickly advancements in NLP have been happening in the last 5 years  incredible!These sequence-to-sequence models are pretty versatile and they are used in a variety of NLP tasks, such as:Lets take a simple example of a sequence-to-sequence model. Check out the below illustration:German to English Translation using seq2seqThe above seq2seq model is converting a German phrase to its English counterpart. Lets break it down:Despite being so good at what it does, there are certain limitations of seq-2-seq models with attention:The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. The Transformer was proposed in the paper Attention Is All You Need. It is recommended reading for anyone interested in NLP.Quoting from the paper:The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.Here, transduction means the conversion of input sequences into output sequences. The idea behind Transformer is to handle the dependencies between input and output with attention and recurrence completely.Lets take a look at the architecture of the Transformer below. It might look intimidating but dont worry, we will break it down and understand it block by block.The Transformer  Model Architecture(Source: https://arxiv.org/abs/1706.03762)The above image is a superb illustration of Transformers architecture. Lets first focus on the Encoder and Decoder parts only.Now focus on the below image. The Encoder block has 1 layer of a Multi-Head Attention followed by another layer of Feed Forward Neural Network. The decoder, on the other hand, has an extra Masked Multi-Head Attention.The encoder and decoder blocks are actually multiple identical encoders and decoders stacked on top of each other. Both the encoder stack and the decoder stack have the same number of units.The number of encoder and decoder units is a hyperparameter. In the paper, 6 encoders and decoders have been used.Lets see how this setup of the encoder and the decoder stack works:An important thing to note here  in addition to the self-attention and feed-forward layers, the decoders also have one more layer of Encoder-Decoder Attention layer. This helps the decoder focus on the appropriate parts of the input sequence.You might be thinking  what exactly does this Self-Attention layer do in the Transformer? Excellent question! This is arguably the most crucial component in the entire setup so lets understand this concept.According to the paper:Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.Take a look at the above image. Can you figure out what the term it in this sentence refers to?Is it referring to the street or to the animal? Its a simple question for us but not for an algorithm. When the model is processing the word it, self-attention tries to associate it with animal in the same sentence.Self-attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence. Now, lets see how we can calculate self-attention.I have divided this section into various steps for ease of understanding.1. First, we need to create three vectors from each of the encoders input vectors:These vectors are trained and updated during the training process. Well know more about their roles once we are done with this section2. Next, we will calculate self-attention for every word in the input sequence3. Consider this phrase  Action gets results. To calculate the self-attention for the first word Action, we will calculate scores for all the words in the phrase with respect to Action. This score determines the importance of other words when we are encoding a certain word in an input sequenceSo, z1 is the self-attention vector for the first word of the input sequence Action gets results. We can get the vectors for the rest of the words in the input sequence in the same fashion:Self-attention is computed not once but multiple times in the Transformers architecture, in parallel and independently. It is therefore referred to as Multi-head Attention. The outputs are concatenated and linearly transformed as shown in the figure below:According to the paper Attention Is All You Need:Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.You can access the code to implement Transformer here.Transformer is undoubtedly a huge improvement over the RNN based seq2seq models. But it comes with its own share of limitations:So how do we deal with these pretty major issues? Thats the question folks who worked with Transformer asked. And out of this came Transformer-XL.Transformer architectures can learn longer-term dependency. However, they cant stretch beyond a certain level due to the use of fixed-length context (input text segments). A new architecture was proposed to overcome this shortcoming in the paper  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.In this architecture, the hidden states obtained in previous segments are reused as a source of information for the current segment. It enables modeling longer-term dependency as the information can flow from one segment to the next.Think of language modeling as a process of estimating the probability of the next word given the previous words.Al-Rfou et al. (2018) proposed the idea of applying the Transformer model for language modeling. As per the paper, the entire corpus can be split into fixed-length segments of manageable sizes. Then, we train the Transformer model on the segments independently, ignoring all contextual information from previous segments:Transformer Model with a segment length of 4 (Source: https://arxiv.org/abs/1901.02860)This architecture doesnt suffer from the problem of vanishing gradients. But the context fragmentation limits its longer-term dependency learning. During the evaluation phase, the segment is shifted to the right by only one position. The new segment has to be processed entirely from scratch. This evaluation method is unfortunately quite compute-intensive.During the training phase in Transformer-XL, the hidden state computed for the previous state is used as an additional context for the current segment. This recurrence mechanism of Transformer-XL takes care of the limitations of using a fixed-length context.Transformer XL Model with a segment length of 4During the evaluation phase, the representations from the previous segments can be reused instead of being computed from scratch (as is the case of the Transformer model). This, of course, increases the computation speed manifold.You can access the code to implement Transformer-XL here.We all know how significant transfer learning has been in the field of computer vision. For instance, a pre-trained deep learning model could be fine-tuned for a new task on the ImageNet dataset and still give decent results on a relatively small labeled dataset.Language model pre-training similarly has been quite effective for improving many natural language processing tasks: (https://paperswithcode.com/paper/transformer-xl-attentive-language-models and https://paperswithcode.com/paper/transformer-xl-attentive-language-models).The BERT framework, a new language representation model from Google AI, uses pre-training and fine-tuning to create state-of-the-art models for a wide range of tasks. These tasks include question answering systems, sentiment analysis, and language inference.BERT uses a multi-layer bidirectional Transformer encoder. Its self-attention layer performs self-attention in both directions. Google has released two variants of the model:BERT uses bidirectionality by pre-training on a couple of tasks  Masked Language Model and Next Sentence Prediction. Lets discuss these two tasks in detail.BERT is pre-trained using the following two unsupervised prediction tasks.According to the paper:The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer.The Google AI researchers masked 15% of the words in each sequence at random. The task? To predict these masked words. A caveat here  the masked words were not always replaced by the masked tokens [MASK] because the [MASK] token would never appear during fine-tuning.So, the researchers used the below technique:Generally, language models do not capture the relationship between consecutive sentences. BERT was pre-trained on this task as well.For language model pre-training, BERT uses pairs of sentences as its training data. The selection of sentences for each pair is quite interesting. Lets try to understand it with the help of an example.Imagine we have a text dataset of 100,000 sentences and we want to pre-train a BERT language model using this dataset. So, there will be 50,000 training examples or pairs of sentences as the training data.Architectures like BERT demonstrate that unsupervised learning (pre-training and fine-tuning) is going to be a key element in many language understanding systems. Low resource tasks especially can reap huge benefits from these deep bidirectional architectures.Below is a snapshot of a few NLP tasks where BERT plays an important role:Source: https://arxiv.org/abs/1810.04805We should really consider ourselves lucky as so many state-of-the-art advancements are happening in NLP at such a rapid pace. Architectures like Transformers and BERT are paving the way for even more advanced breakthroughs to happen in the coming years.I encourage you to implement these models and share your work in the comments section below. And if you have any feedback on this article or any doubts/queries, then do let me know and I will get back to you.You can also take the below course to learn or brush up your NLP skills:",https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/
The AI Comic: Z.A.I.N  Issue #2: Facial Recognition using Computer Vision,Learn everything about Analytics|Introduction|Getting to Know this AI Comics Chief Character Z.A.I.N|Python Code and Explanation Behind Z.A.I.Ns Facial Recognition Model|End Notes,"I recommend reading the previous issue of the AI comic first  Issue#1: No Attendance, No Problem. This will help you understand why we are leaning on the concept of facial recognition in this issue and also build your computer vision foundations.|Share this:|Related Articles|How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models|Build a Machine Learning Model in your Browser using TensorFlow.js and Python|
Prerit Rathi
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Note: Use the right and left arrow keys to navigate through the below slider (or simply swipe if youre using a touchscreen!).,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"First  a HUGE thank you to our community for the wonderful response to the first issue of our exclusive AI comic  Z.A.I.N! Our aim of helping millions of kids and AI enthusiasts understand the wonderful world of AI and machine learning is off to a dream start  I am truly overwhelmed by the positive feedback.So, I am delighted to announce that we will continue to create, design and publish new issues of our AI comic on a regular basis! This weeks issue is all about the wide and complex world of facial recognition using computer vision.This AI comic series successfully merges technical and complex Artificial Intelligence implementations with the fun of reading comic books. Learning has never been more fun!In the debut issue of our AI comic, we introduced Z.A.I.N, the chief protagonist and an Artificial Intelligence whiz. He built a computer vision system using Python to solve the problem of attendance tracking at his school. I like to believe that Z.A.I.N is more than a character  he is a way of thinking, a new way by which we can make this world a better place to live in.But there is a catch in the model Z.A.I.N built. The model is able to count the number of students in the class, sure. But it doesnt recognize faces. What if a student sent a substitute? Z.A.I.Ns original model wouldnt be able to detect that yet.This is where Z.A.I.N and we will discover the awesome concept of facial recognition. We will even build a facial recognition model in Python once we see what Z.A.I.N does in this issue!Who is Z.A.I.N? And whats the plot for issue #2 of this AI comic? Here is an illustrated summary of all that you need to know before pouring into this weeks issue:You can download the full comic here!Enjoyed reading Issue #2? Now lets see how ZAIN came up with that extraordinary feat! Thats right  we are going to dive deep into the Python code behind ZAINs facial recognition model.First, we need data to train our own model. This comes with a caveat  we wont be using a pre-defined dataset. This model has to be trained on our own customized data. How else will facial recognition work for our situation?We use the below tools to curate our dataset:We need to first initialize our camera to capture the video. Then, we will use the frontal face classifier to make bounding boxes around the face. Please note that since we have used frontal-face-default specifically, it will only detect specific faces.So, you can choose which classifier to use according to your requirements. After the bounding boxes have been created, we:Here, we store the grayscale version of our picture in a variable gray.Then the variable faces contain the faces detected by our detectMultiScale function. Next, we have a for loop with parameters that are our four coordinates of the top left and bottom right corner of the face detected. After we are in the for loop, we append the sampleNum variable by 1.Then we save that image to our folder with the name in the format: (str + label + sampleNum)Prefer learning through video examples? I have created the below video just for you to understand what the above code does:And thats it  our dataset is ready for action! So whats next? Well, we will now build and train our model on those images!Well use two tools specifically to do that:Here, we will convert the images into NumPy arrays and train the model. We will then save the trained model as Recogniser.yml:the variable detector stores the classifier (harrcascade_frontal_face_default), and recognizer stores the LBPHFaceRecognizer.Here we are defining a function: getImagesAndLables which does exactly like its name. It gets the images with their respective labels, but how? here it is:first, declare a variable: imagePaths which has the path to the folder/directory in which the images/dataset is stored. Next, we have two empty lists: faceSamples, IDs.Then again we have a for loop which basically uses the pillow library we imported to read the image in our dataset and convert into grayscale at the same time. Now, we have our images but how is a computer supposed to read those, for this, we convert the images into numpy array.you may realize that our images are stored in the format: str +label+sampleNum.So, we just need to assign the image with one of the two labels/classes. But, the challenge here is to split the file name such that only the label is left. This is exactly what the ID variable contains.the basic process of splitting and assigning the images with there respective labels is complete. This below code block just uses recognizer.train to train our model on the images and there labels, A classic example of supervised learning. then we save our trained model as Recogniser.yml.The below code essentially does three things for us:our model is already trained so we just import the trained model by using recognizer.read(). Then a simple if-else code block does its work and classifies the faces in the feed given by our webcam into one of the two classes and if the face does not belong to any of them, It classifies as unknownHeres another intuitive video to show you what the above code block does:This second issue of Analytics Vidhyas AI comic, Z.A.I.N, covered how computer vision can change our day-to-day lives for the better. And guess what? There are many more adventures that await him and all of us. Strap in because things are just getting started!Issue #3 is dropping very soon. The AI adventures of ZAIN will continue to reach new heights as we tackle real-world issues and continue to help you on your machine learning journey.Thank you for reading and I encourage you to build on the model we created in this article. Feel free to reach me with your thoughts and precious feedback in the comments section below!",https://www.analyticsvidhya.com/blog/2019/06/ai-comic-zain-issue-2-facial-recognition-computer-vision/
Build a Machine Learning Model in your Browser using TensorFlow.js and Python,Learn everything about Analytics|Overview|Introduction|Table of Contents|1. Why should you use TensorFlow.js?|Machine Learning in the Browser|Utilizing Googles Pretrained models: PoseNet|End Notes,"Building an Image Classification Model in your Browser using a Webcam|Features of TensorFlow.js|Core API: Working with Tensors|Layers API: Building models like Keras|How does PoseNet work?|Share this:|Related Articles|The AI Comic: Z.A.I.N  Issue #2: Facial Recognition using Computer Vision|An Introduction to the Powerful Bayes Theorem for Data Science Professionals|
Mohd Sanad Zaki Rizvi
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats your favourite tool to code machine learning models? This eternal question prompts all sorts of different answers from data scientists. Some prefer RStudio, others have a special affinity towards Jupyter Notebooks. Im definitely in the latter category.So, when I first came across TensorFlow.js (previously deeplearn.js), my mind was blown. Building a machine learning model in my browser? And using JavaScript? Sounded too good to be true!More than 4.3 billion people use a web browser  around 55% of the worlds population.  Wikipedia (March 2019)Not only has Googles TensorFlow.js democratized machine learning for the masses by bringing it to the browser, But it is also the perfect gateway to machine learning for developers who work regularly with JavaScript.Our web browsers are one of the most easily accessible platforms. And thats why it makes sense to be able to build applications that are able to not only train machine learning models but are also able to learn or transfer learn in the browser itself.In this article, well first understand the importance of using TensorFlow.js and its different components. Well then deep dive straight into building our own machine learning model in the browser using TensorFlow.js. Then we will build an application that will detect your body pose using your computers webcam!If youre new to TensorFlow, you can learn more about it here:Ill answer this question using a unique approach. I wont delve into the theoretical aspect of TensorFlow.js and list down pointers on why its such an incredible tool.Instead, I will simply show you what you will miss out on if you do not use TensorFlow.js. So, lets build an application to classify images using your webcam in under 5 minutes. Thats right  we will jump right into the code!And heres the best part  you do not need to install anything to do this! Just a text editor and a web browser is enough. The below video shows the application well be building:How cool is that? I literally built that in a matter of minutes in my browser. So lets look at the steps and code to help you build your own image classification model in your web browser.Key points to note in this example:
I love the fact that we didnt need to install anything in our machine. This example should work on any modern system irrespective of whether it is Linux, Windows or MacOS  this is the power of building models on the web using JavaScript.Now, lets see the awesome features TensorFlow.js provides and how you can utilize them for deploying machine learning models in your browser.TensorFlow.js is a library for developing and training ML models in JavaScript, and deploying in the browser or on Node.js.TensorFlow.js offers a plethora of features to leverage and play around with.It is an extension of TensorFlow in JavaScript, the programming language behind the logic of almost every website, browser or application that we use on the internet. JavaScript is as versatile as Python so using it to develop machine learning models gives us a lot of advantages:Deploying with TensorFlow.js is a lot easier than the conventional approachTensorFlow.js provides the below major functionalities in its current form:In this article, we will focus on the first two features. Well discuss transfer learning and deploying our model in Python in the second part of this series (coming soon!).TensorFlow.js provides two ways to train models (quite similar to what TensorFlow does):Lets understand both the approaches through the lens of a few examples. After all, the best way to learn a concept is by putting it into practice!First, set up your HTML file:Create a newindex.htmlfile in your computer and write the following code in it:We have created a basic HTML page and loaded Tensorflow.js (line 7) from a cloud URL.A note about installing TensorFlow.js (deeplearn.js)Since TensorFlow.js is made for the browser, the easiest method to install and use TensorFlow.js is to not install it at all. You can simply load it from a URL in your HTML.What if you want to work locally? Well, you can actually use TensorFlow.js inside Jupyter Notebook like you normally do in case of Python or R. Theres a solution in this for everyone!This local approach is slightly longer and takes time so we wont be using it in this article. If you do want to learn how to do it, you can start by installing ijavascript kernel for Jupyter. Here is a screenshot of how it looks in my Jupyter notebook:Now, the recommended approach to use TensorFlow.js is to load it directly by using the official URL of the library. You just have to add the following line to your HTML file:And done! It really is that straightforward.The Core API is very similar to the TensorFlow Core where we can define models using low-level tensor operations and linear algebra.This is very useful if we want to build custom models or want to build neural networks from scratch. Lets take up an example of working with tensors in the browser.Start by adding the below code between the <script></script>tags in your index.html file:The<script> tags basically denote JavaScript. Anything we write between these tags would be executed as JavaScript code. Here is how your index.html should look now:In the above code, we are performing basic addition and multiplication operations on two tensors a and b and printing the result in the browser. Now, go to your terminal, open your project folder, and start a Python server by using the below command:Then go to your browser and open the following address:Once you see a page saying Tensorflow.js Core API, open the consoleusing the keysCtrl+Shift+I. This should work on both Chrome and Firefox. We get the output of the above operations in the console:If you want to read more about Core API in depth, then I recommend going through the official CoreAPI documentation.The Layers API is very similar to Keras in Python. Just like Keras, you can create a model using both sequentialandfunctionalapproaches.Lets take a closer look at the sequential approach through an example. We will train a regression model on these data points:Here, X and Y have a linear relationship  each Y is corresponding to X + i (where i is 0, 1, 2, 3n+1). Lets train a basic regression model on this dataset. You can write the following code between the <script></script>tags in your index.html file:The keen-eyed among you must have noticed how the above syntax is very similar to the Keras syntax for building a sequential model in Python. Well get the prediction when we go back to our browser console.Our simple regression model predicts7.556, which is very close to the expected value of8. This was a basic example but we can clearly see how easy and useful it is to build machine learning models straight in our browser itself.TensorFlow.js is capable of building both machine learning and deep learning models in the browser. It also automatically takes advantage of the power of GPU(s), if available in your system during model training.Here are a few examples of deep learning models trained using TensorFlow.js on some standard datasets:You can explore these examples in the tfjs-examples repository.TensorFlow.js provides tons of pretrained models from Google for many useful tasks like object detection, voice recognition, image segmentation etc. The advantage of pre-trained models is that we can use them without any major dependencies or installation and right out of the box.Google is widely expected to add even more models in the coming months. You can take a look at the available pre-trained models here:Tensorflow.js comes with a lot of pre-trained models from GoogleWe will work withPoseNet in this article. PoseNet is a vision model that can be used to estimate the pose of a person in an image or video by estimating where key body joints are located.This is a fascinating concept. Pose estimation refers to computer vision techniques that detect human figures in images and videos. This helps us determine, for example, where someones elbow shows up in an image.Just to be clear  pose estimation is not about recognizing who is in an image. The algorithm is simply estimating where key body joints are located.The key points detected are indexed by Part and ID, with a confidence score between 0.0 and 1.0 (1.0 being the highest.Keypoints detected in PoseNetHere is an example of the kind of output PoseNet gives:Incredible, right?! We will be using the ml5.js library in order to work with PoseNet. ml5.js is a library built on top of TensorFlow.js along with p5.js, another library that makes it easier to access the webcam in the browser.ml5.js aims to make machine learning approachable for a broad audience of artists, creative coders, and students. The library provides access to machine learning algorithms and models in the browser with a simple syntax, building on top of TensorFlow.js.For example, you can create an image classification model with MobileNet using ml5.js in under 5 lines of code like this:Its this simplicity of Ml5.js that makes it so good for quick prototyping in the browser and that is why we are also using it for our project.Lets get back to PoseNet. Create a new file index.html and add the below code:This will create a basic HTML web page and load the necessary files:Now, we will write JavaScript code for working with PoseNet. Create a new file posenet.jsin the same folder as index.html. Here are the steps needed to make this work:Lets start with step 1.Step 1:Load the PoseNet model and capture video from your webcamWe will load PoseNet using ml5.js. At the same time, p5.js enables us to capture video from webcam using just a few lines of code:The most important things to note in the above code block are:Step 2: Detect key points in body jointsThe next step is to detect the poses. You might have noticed in the previous step that we are saving every detected pose in the posesvariable by calling poseNet.on(). This function runs in the background continuously. Whenever a new pose is found, it gives the location of body joints in the following format:We do not have to write code for this part since it is automatically generated.Step 3:Display the detected body jointsWe know the detected body joints and their x and y location. Now, we just need to draw them over the video to display the detected body joints. Weve seen that PoseNet gives us a list of body joints detected with a confidence score for each joint and its x and y locations.We will use a threshold value of 20% (keypoint.score > 0.2) confidence score in order to draw a key point. Here is the code to do this:Step 4: Draw the estimated skeleton of the bodyAlong with the key points or body joints, PoseNet also detects the estimated skeleton of the body. We can use the posesvariable to draw the skeleton:Here, we looped over the detected skeleton and created lines joining the key points. The code is fairly straightforward again.Now, the last step is to call the drawSkeleton()anddrawKeypoints()functions repeatedly along with the video feed that we are capturing from the webcam. We can do that using the draw()function of p5.js which is called directly aftersetup()and executes repeatedly:Next, go to your terminal window, into your project folder, and start a Python server:Then go to your browser and open the following address:Voila! Your PoseNet should be nicely detecting your body pose (if you have followed all the steps correctly). Here is how my model looks:You can see why I love TensorFlow.js. It is incredibly effective and doesnt even require you to worry about complex installation steps while building your models.TensorFlow.js shows a lot of promise for making machine learning more accessible by bringing it to the browser. And at the same time, it has advantages like data privacy, interactivity etc. This combination makes it a very powerful tool to keep in a data scientists toolbox, especially if you want to deploy your machine learning applications.In the next article, we will explore how to apply transfer learning in the browser and deploy your machine learning or deep learning models using TensorFlow.js.The project that we did with PoseNet can be taken even further to build a pose recognition application by just training another classifier over it. I encourage you to go ahead and try that! Post in the comments below if you have built something interesting All the code for this article is available on Github.",https://www.analyticsvidhya.com/blog/2019/06/build-machine-learning-model-in-your-browser-tensorflow-js-deeplearn-js/
An Introduction to the Powerful Bayes Theorem for Data Science Professionals,Learn everything about Analytics|Overview|Introduction|Table of Contents|Prerequisites for Bayes Theorem|What is Bayes Theorem?|An Illustration of Bayes Theorem|Applications of Bayes Theorem|End Notes,"1. Experiment|2. Sample Space|3. Event|4. Random Variable|5. Exhaustive Events|6. Independent Events|7. Conditional Probability|8. Marginal Probability|Naive Bayes Classifiers|Discriminant Functions and Surfaces||Bayesian Parameter Estimation|Demonstration of Bayesian Parameter Estimation  Univariate Gaussian Case|Share this:|Related Articles|Build a Machine Learning Model in your Browser using TensorFlow.js and Python|Comprehensive Guide to Text Summarization using Deep Learning in Python|
Khyati Mahendru
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Key points you should be aware of:,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Probability is at the very core of a lot ofdata science algorithms. In fact, the solutions to so many data science problems are probabilistic in nature  hence I always advice focusing on learning statistics and probability before jumping into the algorithms.But Ive seen a lot of aspiring data scientists shunning statistics, especially Bayesian statistics. It remains incomprehensible to a lot of analysts and data scientists. Im sure a lot of you are nodding your head at this!Bayes Theorem, a major aspect of Bayesian Statistics, was created by Thomas Bayes, a monk who lived during the eighteenth century. The very fact that were still learning about it shows how influential his work has been across centuries! Bayes Theorem enables us to work on complex data science problems and is still taught at leading universities worldwide.In this article, we will explore Bayes Theorem in detail along with its applications, including in Naive Bayes Classifiers and Discriminant Functions, among others. Theres a lot to unpack in this article so lets get going!We need to understand a few concepts before diving into the world of Bayes Theorem. These concepts are essentially the prerequisites for understanding Bayes Theorem.Whats the first image that comes to your mind when you hear the word experiment? Most people, including me, imagine a chemical laboratory filled with test tubes and beakers. The concept of an experiment in probability theory is actually quite similar:An experiment is a planned operation carried out under controlled conditions.Tossing a coin, rolling a die, and drawing a card out of a well-shuffled pack of cards are all examples of experiments.The result of an experiment is called an outcome. The set of all possible outcomes of an event is called the sample space. For example, if our experiment is throwing dice and recording its outcome, the sample space will be:What will be the sample when were tossing a coin? Think about it before you see the answer below:An event is a set of outcomes (i.e. a subset of the sample space) of an experiment.Lets get back to the experiment of rolling a dice and define events E and F as:The probability of these events:The basic operations in set theory, union and intersection of events, are possible because an event is a set.Now consider an event G = An odd number is obtained:Such events are called disjoint events. These are also called mutually exclusive events because only one out of the two events can occur at a time:A Random Variable is exactly what it sounds like  a variable taking on random values with each value having some probability (which can be zero). It is a real-valued function defined on the sample space of an experiment:Source: WikipediaLets take a simple example (refer to the above image as we go along). Define a random variable X on the sample space of the experiment of tossing a coin. It takes a value +1 if Heads is obtained and -1 if Tails is obtained. Then, X takes on values +1 and -1 with equal probability of 1/2.Consider that Y is the observed temperature (in Celsius) of a given place on a given day. So, we can say that Y is a continuous random variable defined on the same space, S = [0, 100] (Celsius Scale is defined from zero degree Celsius to 100 degrees Celsius).A set of events is said to be exhaustive if at least one of the events must occur at any time. Thus, two events A and B are said to be exhaustive if A  B = S, the sample space.For example, lets say that A is the event that a card drawn out of a pack is red and B is the event that the card drawn is black. Here, A and B are exhaustive because the sample space S = {red, black}. Pretty straightforward stuff, right?If the occurrence of one event does not have any effect on the occurrence of another, then the two events are said to be independent. Mathematically, two events A and B are said to be independent if:P(A  B) = P(AB) = P(A)*P(B)For example, if A is obtaining a 5 on throwing a die and B is drawing a king of hearts from a well-shuffled pack of cards, then A and B are independent just by their definition. Its usually not as easy to identify independent events, hence we use the formula I mentioned above.Consider that were drawing a card from a given deck. What is the probability that it is a black card? Thats easy  1/2, right? However, what if we know it was a black card  then what would be the probability that it was a king?The approach to this question is not as simple.This is where the concept of conditional probability comes into play. Conditional probability is defined as the probability of an event A, given that another event B has already occurred (i.e. A conditional B). This is represented by P(A|B) and we can define it as:Let event A represent picking a king, and event B, picking a black card. Then, we find P(A|B) using the above formula:Thus, P(A|B) = 4/52. Try this out on an example of your choice. This will help you grasp the entire idea really well.It is the probability of an event A occurring, independent of any other event B, i.e. marginalizing the event B.This is just a fancy way of saying:where ~B represents the event that B does not occur.Lets check if this concept of marginal probability holds true. Here, we need to calculate the probability that a random card drawn out of a pack is red (event A). The answer is obviously 1/2. Lets calculate the same through marginal probability with event B as drawing a king.Source: Itembrowser.comPerfect! So this is good enough to cover our basics of Bayes Theorem. Lets now take a few moments to understand what exactly Bayes Theorem is and how it works.Have you ever seen the popular TV show Sherlock (or any crime thriller show)? Think about it  our beliefs about the culprit change throughout the episode. We process new evidence and refine our hypothesis at each step. This is Bayes Theorem in real life!Now, lets understand this mathematically. This will be pretty simple now that our basics are clear.Consider that A and B are any two events from a sample space S where P(B)  0. Using our understanding of conditional probability, we have:This is the Bayes Theorem.Here, P(A) and P(B) are probabilities of observing A and B independently of each other. Thats why we can say that they are marginal probabilities. P(B|A) and P(A|B) are conditional probabilities.P(A) is called Prior probability and P(B) is called Evidence.P(B|A) is called Likelihood and P(A|B) is called Posterior probability.Equivalently, Bayes Theorem can be written as:These words might sound fancy but the underlying idea behind them is really simple. You can always refer back to this section whenever you have any doubts or reach out to me directly in the comments section below.Lets solve a problem using Bayes Theorem. This will help you understand and visualize where you can apply it. Well take an example which Im sure almost all of us have seen in school.There are 3 boxes labeled A, B, and C:The three boxes are identical and have an equal probability of getting picked. Consider that a red ball is chosen. Then what is the probability that this red ball was picked out of box A?Source: i.ytimg.comLet E denote the event that a red ball is chosen and A, B, and C denote that the respective box is picked. We are required to calculate the conditional probability P(A|E).There are plenty of applications of the Bayes Theorem in the real world. Dont worry if you do not understand all the mathematics involved right away. Just getting a sense of how it works is good enough to start off.Bayesian Decision Theory is a statistical approach to the problem of pattern classification. Under this theory, it is assumed that the underlying probability distribution for the categories is known. Thus, we obtain an ideal Bayes Classifier against which all other classifiers are judged for performance.We will discuss the three main applications of Bayes Theorem:Lets look at each application in detail.This is probably the most famous application of Bayes Theorem, probably even the most powerful. Youll come across the Naive Bayes algorithm a lot in machine learning.Naive Bayes Classifiers are a set of probabilistic classifiers based on the Bayes Theorem. The underlying assumption of these classifiers is that all the features used for classification are independent of each other. Thats where the name naive comes in since it is rare that we obtain a set of totally independent features.The way these classifiers work is exactly how we solved in the illustration, just with a lot more features assumed to be independent of each other.Here, we need to find the probability P(Y|X) where X is an n-dimensional random variable whose component random variables X_1, X_2, ., X_n are independent of each other:Finally, the Y for which P(Y|X) is maximum is our predicted class.Lets talk about the famous Titanic dataset. We have the following features:Remember the problem statement? We need to calculate the probability of survival conditional to all the other variables available in the dataset. Then, based on this probability, we predict if the person survived or not, i.e, class 1 or 0.This is where I pass the buck to you. Refer to our popular article to learn about these Naive Bayes classifiers in detail along with the relevant code in both Python and R and try solving the Titanic dataset yourself. If you get stuck anywhere, you can find me in the comments section below!The name is pretty self-explanatory. A discriminant function is used to discriminate its argument into its relevant class. Want an example? Lets take one!You might have come across Support Vector Machines (SVM) if you have explored classification problems in machine learning. The SVM algorithm classifies the vectors by finding the differentiating hyperplane which best segregates our training examples. This hyperplane can be linear or non-linear: These hyperplanes are our decision surfaces and the equation of this hyperplane is our discriminant function. Make sure you check out our article on Support Vector Machine. It is thorough and includes code both in R and Python.Alright  now lets discuss the topic formally.Let w_1, w_2, .., w_c denote the c classes that our data vector X can be classified into. Then the decision rule becomes:These functions g_i(X), i = 1, 2, ., c, are known as Discriminant functions. These functions separate the vector space into c decision regions R_1, R_2, ., R_c corresponding to each of the c classes. The boundaries of these regions are called decision surfaces or boundaries.If g_i(X) = g_j(X) is the largest value out of the c discriminant functions, then the classification of vector X into class w_i and w_j is ambiguous. So, X is said to lie on a decision boundary or surface.Check out the below figure:Source: Duda, R. O., Hart, P. E., & Stork, D. G. (2012).Pattern classification. John Wiley & Sons.Its a pretty cool concept, right? The 2-dimensional vector space is separated into two decision regions, R_1 and R_2, separated by two hyperbolas.Note that any function f(g_i(X)) can also be used as a discriminant function if f(.) is a monotonically increasing function. The logarithm function is a popular choice for f(.).Now, consider a two-category case with classes w _1 and w_2. The minimum error-rate classification decision rule becomes:P(w_i|X) is a conditional probability and can be calculated using Bayes Theorem. So, we can restate the decision rule in terms of likelihoods and priors to get:Notice that the evidence on the denominator is merely used for scaling and hence we can eliminate it from the decision rule.Thus, an obvious choice for the discriminant functions is:The 2-category case can generally be classified using a single discriminant function.In the figure above, g(X) is a linear function in a 2-dimensional vector X. However, more complicated decision boundaries are also possible:This is the third application of Bayes Theorem. Well use univariate Gaussian Distribution and a bit of mathematics to understand this. Dont worry if it looks complicated  Ive broken it down into easy-to-understand terms.You must have heard about the ultra-popular IMDb Top 250. This is a list of 250 top-rated movies of all time. Shawshank Redemption is #1 on the list with a rating of 9.2/10.How do you think these ratings are calculated? The original formula used by IMDb claimed to use a true Bayesian estimate. The formula has since changed and is not publicly disclosed. Nonetheless, here is the previous formula:Source: WikipediaThe final rating W is a weighted average of R and C with weights v and m respectively. m is a prior estimate.We generally do not have complete information about the probabilistic nature of a classification problem. Instead, we have a vague idea of the situation along with a number of training examples. We then use this information to design a classifier.The basic idea is that the underlying probability distribution has a known form. We can, therefore, describe it using a parameter vector . For example, a Gaussian distribution can be described by  = [, ].Source: WikipediaThen, we need to estimate this vector. This is generally achieved in two ways:I recommend reading this article to get an intuitive and in-depth explanation of maximum likelihood estimation along with a case study in R.We can write it informally as:Let me demonstrate how Bayesian Parameter Estimation works. This will provide further clarity on the theory we just covered.First, let p(X) be normally distributed with a mean  and variance , where  is the only unknown parameter we wish to estimate. Then:Well ease up on the math here. So, let prior probability density p() also be normally distributed with mean  and variance  (which are both known).Here, p(|data) = p(|data) is called the reproducing density and p() = p() is called the conjugate prior.Since this argument in exp() is quadratic in , it represents a normal density. Thus, if we have n training examples, we can say that p(|data) is normally distributed with mean _n and variance _n, whereMy observations:Source: spcforexcel.comHeres a superb and practical implementation of Bayesian Statistics and estimation:The beauty and power of Bayes Theorem never cease to amaze me. A simple concept, given by a monk who died more than 250 years ago, has its use in some of the most prominent machine learning techniques today.You might have a few questions on the various mathematical aspects we explored here. Feel free to connect with me in the comments section below and let me know your feedback on the article as well.",https://www.analyticsvidhya.com/blog/2019/06/introduction-powerful-bayes-theorem-data-science/
Comprehensive Guide to Text Summarization using Deep Learning in Python,Learn everything about Analytics|Introduction|Table of Contents|What is Text Summarization in NLP?|Introduction to Sequence-to-Sequence (Seq2Seq) Modeling|Understanding the Encoder-Decoder Architecture||Limitations of the Encoder  Decoder Architecture||The Intuition behind the Attention Mechanism|Understanding the Problem Statement|Implementing Text Summarization in Python using Keras|How can we Improve the Models Performance Even Further?|How does the Attention Mechanism Work?||Code|End Notes,"Extractive Summarization|Abstractive Summarization|Training phase|Inference Phase|Global Attention|Local Attention|Custom Attention Layer|Import the Libraries|Read the dataset|Drop Duplicates and NA values|Preprocessing|a) Text Cleaning|b) Summary Cleaning|Understanding the distribution of the sequences|Preparing the Tokenizer|Model building|Understanding the Diagnostic plot|Share this:|Related Articles|An Introduction to the Powerful Bayes Theorem for Data Science Professionals|Top 7 Machine Learning Github Repositories for Data Scientists|
Aravind Pai
|39 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I dont want a full report, just give me a summary of the results. I have often found myself in this situation  both in college as well as my professional life. We prepare a comprehensive report and the teacher/supervisor only has time to read the summary.Sounds familiar? Well, I decided to do something about it. Manually converting the report to a summarized version is too time taking, right? Could I lean on Natural Language Processing (NLP) techniques to help me out?This is where the awesome concept of Text Summarization using Deep Learning really helped me out. It solves the one issue which kept bothering me before  now our model can understand the context of the entire text. Its a dream come true for all of us who need to come up with a quick summary of a document!And the results we achieve using text summarization in deep learning? Remarkable. So in this article, we will walk through a step-by-step process for building aText Summarizer using Deep Learning by covering all the concepts required to build it. And then we will implement our first text summarization model in Python!Note: This article requires a basic understanding of a few deep learning concepts. I recommend going through the below articles.Ive kept the how does the attention mechanism work? section at the bottom of this article. Its a math-heavy section and is not mandatory to understand how the Python code works. However, I encourage you to go through it because it will give you a solid idea of this awesome NLP concept.Lets first understand what text summarization is before we look at how it works. Here is a succinct definition to get us started:Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning                  -Text Summarization Techniques: A Brief Survey, 2017There are broadly two different approaches that are used for text summarization:Lets look at these two types in a bit more detail.The name gives away what this approach does. We identify the important sentences or phrases from the original text and extract only those from the text. Those extracted sentences would be our summary. The below diagram illustrates extractive summarization:I recommend going through the below article for building an extractive text summarizer using the TextRank algorithm:This is a very interesting approach. Here, we generate new sentences from the original text. This is in contrast to the extractive approach we saw earlier where we used only the sentences that were present. The sentences generated through abstractive summarization might not be present in the original text:You might have guessed it  we are going to build an Abstractive Text Summarizer using Deep Learning in this article! Lets first understand the concepts necessary for building a Text Summarizer model before diving into the implementation part.Exciting times ahead! We can build a Seq2Seq model on any problem which involves sequential information. This includes Sentiment classification, Neural Machine Translation, and Named Entity Recognition  some very common applications of sequential information.In the case of Neural Machine Translation, the input is a text in one language and the output is also a text in another language:In the Named Entity Recognition, the input is a sequence of words and the output is a sequence of tags for every word in the input sequence:Our objective is to build a text summarizer where the input is a long sequence of words (in a text body), and the output is a short summary (which is a sequence as well). So, we can model this as a Many-to-Many Seq2Seq problem. Below is a typical Seq2Seq model architecture:There are two major components of a Seq2Seq model:Lets understand these two in detail. These are essential to understand how text summarization works underneath the code. You can also check out this tutorial to understand sequence-to-sequence modeling in more detail.
The Encoder-Decoder architecture is mainly used to solve the sequence-to-sequence (Seq2Seq) problems where the input and output sequences are of different lengths.Lets understand this from the perspective of text summarization. The input is a long sequence of words and the output will be a short version of the input sequence.Generally, variants of Recurrent Neural Networks (RNNs), i.e. Gated Recurrent Neural Network (GRU) or Long Short Term Memory (LSTM), are preferred as the encoder and decoder components. This is because they are capable of capturing long term dependencies by overcoming the problem of vanishing gradient.We can set up the Encoder-Decoder in 2 phases:Lets understand these concepts through the lens of an LSTM model.In the training phase, we will first set up the encoder and decoder. We will then train the model to predict the target sequence offset by one timestep. Let us see in detail on how to set up the encoder and decoder.Encoder An Encoder Long Short Term Memory model (LSTM) reads the entire input sequence wherein, at each timestep, one word is fed into the encoder. It then processes the information at every timestep and captures the contextual information present in the input sequence.Ive put together the below diagram which illustrates this process:The hidden state (hi) and cell state (ci) of the last time step are used to initialize the decoder. Remember, this is because the encoder and decoder are two different sets of the LSTM architecture.DecoderThe decoder is also an LSTM network which reads the entire target sequence word-by-word and predicts the same sequence offset by one timestep. The decoder is trained to predict the next word in the sequence given the previous word. <start> and <end> are the special tokens which are added to the target sequence before feeding it into the decoder. The target sequence is unknown while decoding the test sequence. So, we start predicting the target sequence by passing the first word into the decoder which would be always the <start> token. And the <end> token signals the end of the sentence.Pretty intuitive so far.After training, the model is tested on new source sequences for which the target sequence is unknown. So, we need to set up the inference architecture to decode a test sequence:How does the inference process work?Here are the steps to decode the test sequence:Lets take an example where the test sequence is given by [x1, x2, x3, x4]. How will the inference process work for this test sequence? I want you to think about it before you look at my thoughts below.Timestep: t=1Timestep: t=2And, Timestep: t=3As useful as this encoder-decoder architecture is, there are certain limitations that come with it.A potential issue with this encoder-decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences. The performance of a basic encoder-decoder deteriorates rapidly as the length of an input sentence increases.    -Neural Machine Translation by Jointly Learning to Align and TranslateSo how do we overcome this problem of long sequences? This is where the concept of attention mechanism comes into the picture. It aims to predict a word by looking at a few specific parts of the sequence only, rather than the entire sequence. It really is as awesome as it sounds!How much attention do we need to pay to every word in the input sequence for generating a word at timestep t? Thats the key intuition behind this attention mechanism concept.Lets consider a simple example to understand how Attention Mechanism works:The first word I in the target sequence is connected to the fourth word you in the source sequence, right? Similarly, the second-word love in the target sequence is associated with the fifth word like in the source sequence. So, instead of looking at all the words in the source sequence, we can increase the importance of specific parts of the source sequence that result in the target sequence. This is the basic idea behind the attention mechanism.There are 2 different classes of attention mechanism depending on the way the attended context vector is derived:Lets briefly touch on these classes.Here, the attention is placed on all the source positions. In other words, all the hidden states of the encoder are considered for deriving the attended context vector:Source: Effective Approaches to Attention-based Neural Machine Translation  2015Here, the attention is placed on only a few source positions. Only a few hidden states of the encoder are considered for deriving the attended context vector:Source: Effective Approaches to Attention-based Neural Machine Translation  2015We will be using the Global Attention mechanism in this article.Customer reviews can often be long and descriptive. Analyzing these reviews manually, as you can imagine, is really time-consuming. This is where the brilliance of Natural Language Processing can be applied to generate a summary for long reviews.We will be working on a really cool dataset. Our objective here is to generate a summary for the Amazon Fine Food reviews using the abstraction-based approach we learned about above.You can download the dataset from here.Its time to fire up our Jupyter notebooks! Lets dive into the implementation details right away.Keras does not officially support attention layer. So, we can either implement our own attention layer or use a third-party implementation. We will go with the latter option for this article. You can download the attention layer from here and copy it in a different file called attention.py.Lets import it into our environment:This dataset consists of reviews of fine foods from Amazon. The data spans a period of more than 10 years, including all ~500,000 reviews up to October 2012. These reviews include product and user information, ratings, plain text review, and summary. It also includes reviews from all other Amazon categories.Well take a sample of 100,000 reviews to reduce the training time of our model. Feel free to use the entire dataset for training your model if your machine has that kind of computational power.Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.Here is the dictionary that we will use for expanding the contractions:We need to define two different functions for preprocessing the reviews and generating the summary since the preprocessing steps involved in text and summary differ slightly.Lets look at the first 10 reviews in our dataset to get an idea of the text preprocessing steps:Output:We will perform the below preprocessing tasks for our data:Lets define the function:And now well look at the first 10 rows of the reviews to an idea of the preprocessing steps for the summary column:Output:Define the function for this task:Remember to add the START and END special tokens at the beginning and end of the summary:Now, lets take a look at the top 5 reviews and their summary:Output:Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:Output:Interesting. We can fix the maximum length of the reviews to 80 since that seems to be the majority review length. Similarly, we can set the maximum summary length to 10:We are getting closer to the model building part. Before that, we need to split our dataset into a training and validation set. Well use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Go ahead and build tokenizers for text and summary:a) Text Tokenizerb) Summary TokenizerWe are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.Here, we are building a 3 stacked LSTM for the encoder:Output:I am using sparse categorical cross-entropy as the loss function since it converts the integer sequence to a one-hot vector on the fly. This overcomes any memory issues.Remember the concept of early stopping? It is used to stop training the neural network at the right time by monitoring a user-specified metric. Here, I am monitoring the validation loss (val_loss). Our model will stop training once the validation loss increases:Well train the model on a batch size of 512 and validate it on the holdout set (which is 10% of our dataset):Now, we will plot a few diagnostic plots to understand the behavior of the model over time:Output:We can infer that there is a slight increase in the validation loss after epoch 10. So, we will stop training the model after this epoch.Next, lets build the dictionary to convert the index to word for target and source vocabulary:InferenceSet up the inference for the encoder and decoder:We are defining a function below which is the implementation of the inference process (which we covered in the above section):Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:Here are a few summaries generated by the model:This is really cool stuff. Even though the actual summary and the summary generated by our model do not match in terms of words, both of them are conveying the same meaning. Our model is able to generate a legible summary based on the context present in the text.This is how we can perform text summarization using deep learning concepts in Python.Your learning doesnt stop here! Theres a lot more you can do to play around and experiment with the model:Now, lets talk about the inner workings of the attention mechanism. As I mentioned at the start of the article, this is a math-heavy section so consider this as optional learning. I still highly recommend reading through this to truly grasp how attention mechanism works.eij= score (si, hj )     where eij denotes the alignment score for the target timestep i and source time step j.There are different types of attention mechanisms depending on the type of score function used. Ive mentioned a few popular attention mechanisms below:Si= concatenate([si; Ci])yi= dense(Si)Lets understand the above attention mechanism steps with the help of an example. Consider the source sequence to be [x1, x2, x3, x4] and target sequence to be [y1, y2].Target timestep i=1Target timestep i=2We can perform similar steps for target timestep i=3 to produce y3.I know this was a heavy dosage of math and theory but understanding this will now help you to grasp the underlying idea behind attention mechanism. This has spawned so many recent developments in NLP and now you are ready to make your own mark!Find the entire notebook here.Take a deep breath  weve covered a lot of ground in this article. And congratulations on building your first text summarization model using deep learning! We have seen how to build our own text summarizer using Seq2Seq modeling in Python.If you have any feedback on this article or any doubts/queries, kindly share them in the comments section below and I will get back to you. And make sure you experiment with the model we built here and share your results with the community!You can also take the below courses to learn or brush up your NLP skills:",https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/
Top 7 Machine Learning Github Repositories for Data Scientists,Learn everything about Analytics|Introduction|Top GitHub Repositories (May 2019)|InterpretML by Microsoft  Machine Learning Interpretability|Tensor2Robot (T2R) by Google Research|Generative Models in TensorFlow 2|STUMPY  Time Series Data Mining|MeshCNN in PyTorch|Awesome Decision Tree Research Papers|TensorWatch by Microsoft Research|Reddit Discussions|Which Skills should a PhD student have if he/she wants to work in the industry?|Neural nets typically contain smaller sub networks that can often learn faster  MIT|Does anybody else feel overwhelmed looking at how much there is to learn?|End Notes,"Share this:|Related Articles|Comprehensive Guide to Text Summarization using Deep Learning in Python|The AI Comic: Z.A.I.N  Issue #1: Automating Attendance using Computer Vision|
Shubham Singh
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If I had to pick one platform that has single-handedly kept me up-to-date with the latest developments in data science and machine learning  it would be GitHub. The sheer scale of GitHub, combined with the power of super data scientists from all over the globe, make it a must-use platform for anyone interested in this field.Can you imagine a world where machine learning libraries and frameworks like BERT, StanfordNLP, TensorFlow, PyTorch, etc. werent open sourced? Its unthinkable! GitHub has democratized machine learning for the masses  exactly in line with what we at Analytics Vidhya believe in.This was one of the primary reasons we started this GitHub series covering the most useful machine learning libraries and packages back in January 2018.Along with that, we have also been covering Reddit discussions we feel are relevant for all data science professionals. This month is no different. I have curated the top five discussions from May which focus on two things  machine learning techniques and career advice from expert data scientists.You can also go through the GitHub repositories and Reddit discussions weve covered throughout this year:Interpretability is a HUGE thing in machine learning right now. Being able to understand how a model produced the output that it did  a critical aspect of any machine learning project. In fact, we even did a podcast with Christoph Molar on interpretable ML that you should check out.InterpretML is an open-source package by Microsoft for training interpretable models and explaining black-box systems. Microsoft put it best when they explained why interpretability is essential:Interpreting the inner working of a machine learning model becomes tougher as the complexity increases. Have you ever tried to take apart and understand a multiple model ensemble? It takes a lot of time and effort to do it.We cant simply go to our client or leadership with a complex model without being able to explain how it produced a good score/accuracy. Thats a one-way ticket back to the drawing board for us.The folks at Microsoft Research have developed the Explainable Boosting Machine (EBM) algorithm to help with interpretability. This EBM technique has both high accuracy and intelligibility  the holy grail.Interpret ML isnt limited to just using EBM. It also supports algorithms like LIME, linear models, decision trees, among others. Comparing models and picking the best one for our project has never been this easy!You can install InterpretML using the below code:Google Research makes another appearance in our monthly Github series. No surprises  they have the most computational power in the business and theyre putting it to good use in machine learning.Their latest open source released, called Tensor2Robot (T2R) is pretty awesome. T2R is a library for training, evaluation and inference of large-scale deep neural networks. But wait  it has been developed with a specific goal in mind. It is tailored for neural networks related to robotic perception and controlNo prizes for guessing the deep learning framework on which Tensor2Robot is built. Thats right  TensorFlow. Tensor2Robot is used within Alphabet, Googles parent organization.Here are a couple of projects implemented using Tensor2Robot:TensorFlow 2.0, the most awaited TensorFlow (TF) version this year, officially launched last month. And I couldnt wait to get my hands on it!This repository contains TF implementations of multiple generative models, including:All these models are implemented on two datasets youll be pretty familiar with  Fashion MNIST and NSYNTH.The best part? All of these implementation are available in a Jupyter Notebook! So you can download it and run it on your own machine or export it to Google Colab. The choice is yours and TensorFlow 2.0 is right here for you to understand and use.A time series repository! I havent come across a new time series development in quite a while.STUMPY is a powerful and scalable library that helps us perform time series data mining tasks. STUMPY is designed to compute a matrix profile. I can see you wondering  what in the world is a matrix profile? Well, this matrix profile is a vector that stores the z-normalized Euclidean distance between any subsequence within a time series and its nearest neighbor.Below are a few time series data mining tasks this matrix profile helps us perform:Use the below code to install it directly via pip:MeshCNN is a general-purpose deep neural network for 3D triangular meshes. These meshes can be used for tasks such as 3D-shape classification or segmentation. A superb application of computer vision.The MeshCNN framework includes convolution, pooling and unpooling layers which are applied directly on the mesh edges:Convolutional Neural Networks (CNNs) are perfect for working with image and visual data. CNNs have become all the rage in recent times with a boom of image related tasks springing up from them. Object detection, image segmentation, image classification, etc.  these are all possible thanks to the advancement in CNNs.3D deep learning is attracting interest in the industry, including fields like robotics and autonomous driving. The problem with 3D shapes is that they are inherently irregular. This makes operations like convolutions difficult an challenging.This is where MeshCNN comes into play. From the repository:Meshes are a list of vertices, edges and faces, which together define the shape of the 3D object. The problem is that every vertex has a different # of neighbors, and there is no order.If youre a fan of computer vision and are keen to learn or apply CNNs, this is the perfect repository for you. You can learn more about CNNs through our articles:Decision Tree algorithms are among the first advanced techniques we learn in machine learning. Honestly, I truly appreciate this technique after logistic regression. I could use it on bigger datasets, understand how it worked, how the splits happened, etc.I personally love this repository. It is a treasure trove for data scientists. The repository contains a collection of papers on tree based algorithms, including decision, regression and classification trees. The repository also contains the implementation of each paper. What more could we ask for?Have you ever wondered how your machine learning algorithms training process works? We write the code, some complication happens behind the scenes (the joy of programming!), and we get the results.Microsoft Research have come up with a tool called TensorWatch that enables us to see real-time visualizations of our machine learning models training process. Incredible! Check out a snippet of how TensorWatch works:TensorWatch, in simple terms, is a debugging and visualization tool for deep learning and reinforcement learning. It works in Jupyter notebooks and enables us to perform many other customized visualizations of our data and our models.Lets spend a few moments checking out the most awesome Reddit discussions related to data science and machine learning from May, 2019. Theres something here for everyone, whether youre a data science enthusiast or practitioner. So lets dig in!This is a tough nut to crack. The first question is whether you should actually opt for a Ph.D ahead of an industry role. And then if you did opt for one, then what skills should you pick up to make your industry transition easier?I believe this discussion could be helpful in decoding one of the biggest enigmas in our career  how do we make a transition from one field or line of work to another? Dont just look at this from the point of view of a Ph.D student. This is very relevant for most of us wanting to get that first break in machine learning.I strongly encourage you to go through this thread as so many experienced data scientists have shared their personal experiences and learning.Recently, a research paper was released expanding on the headline of this thread. The paper explained the Lottery Ticket Hypothesis in which a smaller sub-network, also known as a winning ticket, could be trained faster as compared to a larger network.This discussion focuses on this paper. To read more about the Lottery Ticket Hypothesis and how it works, you can refer to my article where I break down this concept for even beginners to understand:Decoding the Best Papers from ICLR 2019  Neural Networks are Here to RuleI picked this discussion because I can totally relate to it. I used to think  Ive learned so much, and yet there is so much more left. Will I ever become an expert? I made the mistake of looking just at the quantity and not the quality of what I was learning.With the continuous and rapid advancement technology, there will always be a LOT to learn. This thread has some solid advice on how you can set priorities, stick to them, and focus on the task at hand rather than trying to become a jack of all trades.I had a lot of fun (and learning) putting together this months machine learning GitHub collection! I highly recommend bookmarking both these platforms and regularly checking them. Its a great way to stay up to date with all thats new in machine learning.Or, you can always come back each month and check out our top picks. If you think Ive missed any repository or any discussion, comment below and Ill be happy to have a discussion on it!",https://www.analyticsvidhya.com/blog/2019/06/top-7-machine-learning-github-repositories-data-scientists/
The AI Comic: Z.A.I.N  Issue #1: Automating Attendance using Computer Vision,Learn everything about Analytics|Welcome to Issue #1 of Analytics Vidhyas A.I. Comic!||About Z.A.I.N.|Indulge in the World of Comics through Z.A.I.N|Python Code and Explanation|End Notes,"Share this:|Related Articles|Top 7 Machine Learning Github Repositories for Data Scientists|DataHack Radio #23: Ines Montani and Matthew Honnibal  The Brains behind spaCy|
Prerit Rathi
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Sound does not travel in a vaccum.The above concept might just be a simple fact for you  but I had a tough time learning this in the manner it was taught in one of my classes. I actually learnt it when this was implemented in a comic and one of the Super Heroes I followed used this to solve an otherwise difficult challenge.I have been in awe with the power of comics to help us understand concepts in a simple and visual manner. The ebb and flow of stories, the build up and the climax usually hold up people like me for hours.Now, think of a technical article. Do you remember a blog you read about a specific technique from a few months back? Id guess you dont. I certainly dont!Thats why we at Analytics Vidhya came up with the idea of the worlds first Artificial Intelligence (AI) comic! We believe comics are an awesome way of learning a concept through interactive storytelling. Simply put, its a brilliant way to learn complex data science topics.With this AI comic series, I aim to bring out my imagination for the benefit of millions of kids and friends. I aim to help them understand how AI & ML will change our world. And I hope, a few years down the line, they remember Z.A.I.N as the super hero who helped them understand the super power of machines!We will combine advanced blog content and code on machine learning and AI with the storytelling style of a comic. Are you ready to dive into the world of AI comics? Lets begin  and welcome to issue #1!Issue #2 of AI-COMIC: Z.A.I.N (Keep Calm and Optimise) is out now!check it out hereZ.A.I.N. is the first issue of Analytics Vidhyas Artificial Intelligence (AI) comic series that successfully merges technical and complex Artificial Intelligence implementations with the fun of reading comic books.The comic book series combines both of these elements:Heres Kunal Jain, Analytics Vidhyas Founder and CEO, along with me, to give you a glimpse into this AI comic series:Here is all you need to know about the chief character of this comic book series:Note: Use the right and left arrow keys to navigate through the below slider.Had fun reading the A.I. comic? Now, lets understand the code behind the attendance system. Thats right  we are going to implement the Python code we just saw!Well be using the below libraries in this section:Next, we read the .jpg file and plot it with the matplotlib library. In this case, we are using a photo of a single face (Tony Stark) which is saved in here as  TS3.jpg :After reading and plotting the image, we come to the main code block:Here, we use Face and Eye cascade Classifiers to get the coordinates of top and bottom corners of both the face and eyes respectively. Those coordinates/points are stored in variables:Then, we use these coordinates to plot rectangles around the face and eyes. The basic thought process is to count the number of rectangles around the faces, thus counting the number of faces:Lets implement the same code-block on a picture of the classroom. Our aim, remember, was to count the number of faces in the class. Thats exactly how Z.A.I.N saved the day! And now its our turn.The image of the classroom is saved as face-detection.jpg. You can download the image from here and follow along with the code well soon see. You can even play around with an image of your choice. I strongly believe the best way to learn a concept is by experimenting!Lets get back to our classroom. The below code makes rectangles around most of the faces in the classroom:Awesome! We have successfully built the automated attendance system using computer vision! The adventures of Z.A.I.N have only just begun.This first issue of AVs AI comic, Z.A.I.N, was the story about how computer vision can change our day-to-day lives for the better. And guess what? There are many such adventures that await him and all of us.Issue #2 is coming soon. Grab your popcorn and get ready to partake in another awesome AI adventure because the next issue is going to take our learning to a whole new level.Thank you for reading. Feel free to leave your thoughts, your experience, and precious feedback in the comments section below!",https://www.analyticsvidhya.com/blog/2019/06/ai-comic-zain-issue-1-automating-computer-vision/
DataHack Radio #23: Ines Montani and Matthew Honnibal  The Brains behind spaCy,Learn everything about Analytics|Introduction|The Brains behind the spaCy Library|The Motivation and Idea for Developing spaCy|The Business Side of spaCy and Prodigy|The Evolution of spaCy (from v1 to v2.1)|A few Surprising Use Cases of spaCy|Future Trends in NLP and spaCy|Whats the one problem you would want to solve in NLP over the next 3-4 years?|Advice for NLP Enthusiasts and Aspiring Data Scientists|End Notes and Resources,"Share this:|Related Articles|The AI Comic: Z.A.I.N  Issue #1: Automating Attendance using Computer Vision|Exclusive Interview with Sonny Laskar  Kaggle Master and Analytics Vidhya Hackathon Expert|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What would you do if you had the chance to pick the brains behind one of the most popular Natural Language Processing (NLP) libraries of our era? A library that has helped usher in the current boom in NLP applications and nurtured tons of NLP scientists?Well  you invite the creators on our popular DataHack Radio podcast and let them do the talking! We are delighted to welcome Ines Montani and Matt Honnibal, the developers of spaCy  a powerful and advanced library for NLP.Thats right  everything youve ever wanted to know about the wonderful spaCy library is right here in the latest DataHack Radio podcast.This podcast is a 40 minute+ bonanza for NLP enthusiasts and practitioners. Ines and Matt spoke about all things spaCy and NLP, including:And much, much more!Ive put together the key takeaways and highlights in this article. Enjoy the episode and dont forget to subscribe to Analytics Vidhyas DataHack Radio podcast on any of the below platforms:The story behind spaCy goes like this. During his graduation days, Matt had written code for a specific Natural Language Processing (NLP) task. He had been working in NLP for a long time and there were a few companies who wanted to use his research code.The code, however, wasnt ready to be used for general purposes (it was geared more towards performing a specific task). Matt wanted to build something that could be used for a broader set of NLP functions. And from that, the concept of spaCy was born.Matt met up with Ines shortly after the first alpha version of spaCy was released. They have since collaborated on spaCy and have founded their company  explosion.ai. Explosion AI is a digital studio specializing in Artificial Intelligence and Natural Language Processing. Their three primary offerings are:I really liked the story behind Ines and Matts initial days working on spaCy. Ines was intrigued by the computational linguistics aspect of working on spaCy and by the thought that companies could use it to build really significant systems.Theres a funny anecdote about Ines first reaction to understanding the algorithm behind NLP tasks back then. I wont spoil it here  so make sure you listen to this section!The NLTK library existed before spaCy was developed. So why create spaCy in the first place? What was the motivation behind creating a different NLP library? Im sure most of you must have asked this question back when spaCy came out.As Matt put it so well, its important to understand the NLTK library before we answer these questions. NLTK was developed with a different code base and is more from the point of view of teaching NLP topics.The need for commercial NLP comes from a different set of priorities. One of the most important of those priorities is efficiency. And the best way to get efficient Python usable things is to write C extensions with Cython  the language spaCy is ultimately implemented in.  Matt HonnibalLinear models used to be quite popular when Matt started working on spaCy because:There was no possible avenue for writing these C extensions for NLTK. Additionally, Matt wanted to develop a different NLP library that took a different approach. And hence spaCy was designed to fill in these gaps and give a different perspective to folks working in NLP.There was always a scope for creating a business aspect out of spaCy right from the beginning. The question was whether it had to be from the consulting side or something else.Ines and Matt settled on annotation tools at the beginning of their journey. And this kept coming up, according to Ines. So the question was  what were people using and what actually worked for them? Two features stood out:Im sure most of you working in NLP can relate to these! Ines and Matt actually came up with the concept behind Prodigy thanks to these features. Prodigy, for those who havent seen it before, is an annotation tool that data scientists can use to do the annotation themselves, enabling a new level of rapid iteration. Whether youre working on entity recognition, intent detection or image classification, Prodigy can help you train and evaluate your models faster. And who doesnt want that?This is influenced to a large degree by the research and development around NLP.The first version of spaCy was built with the linear model technology we saw above. This was back when neural networks were still in their infancy stage, not quite ready to take the machine learning world by storm.Once the revolution came, when neural networks became more and more mainstream, spaCy made the switch from version 1 to 2. Many of the key features in spaCy 2.0 were around the various ML pipeline components (such as plug-and-play architectures), inspired and influenced by the ever-evolving machine learning community.spaCy 2.1, the current version, is geared more towards stability and performance. One of its stand-out features is dealing with transfer learning and language models  the two concepts that have accelerated research and progress in the NLP field.So whats next for spaCy? What kind of new features can we expect from future updates? Heres Matt:One of the core uses of spaCy is in information extraction. Basically, going from unstructured text to structured knowledge. What we have in the works is a new component for entity linking  resolving names to knowledge-based entries. We have a develop working on this currently and should have it ready by July, when the spaCy conference comes around.They are further working on integrating various regional languages into the spaCy environment. Exciting times ahead!The beauty of machine learning is in its sheer scope and complexity. Even the creators of a library or package can sometimes be taken by surprise looking at its use cases. Here are a few such use cases of spaCy which Ines and Matt did not see coming:I would love to hear from our community here  are there any unique spaCy use cases you have come across? Or perhaps youve experimented with yourself? Let me know in the comments section below!NLP has come leaps and bounds in the last 12-18 months with the release of breakthrough frameworks like OpenAIs GPT-2, Googles BERT, fast.ais ULMFiT, among others. So what do the next 2-5 years hold for NLP and spaCy?Ines and Matt provided quite an in-depth and insightful answer to this. Here are my key takeaways:I loved this question by Kunal and heres Matts answer in full:A better process for information extraction that can be trained on custom problems and that supports entity linking as well. And with an integrated process for annotation.People end up making ad-hoc systems at the moment. spaCy has some useful components for it but we can do better for that.And here is Ines weighing in with her thoughts:Building out a set of best practices and putting everything together that weve learned and seen. At the moment a lot of it is trial and error. Its very difficult to recommend something because every use case is different.Over the next 203 years, Im hoping we can have a better sense of best practices, workflows, how to build end-to-end systems that generalize well to different use cases.Heres a summary of Ines and Matts golden advice for NLP enthusiasts and aspiring data scientists in general:Our data scientists here at Analytics Vidhya are huge fans and avid users of spaCy. Weve been using it since it was launched and have integrated it into our research and our courses as well. It is a truly superb library for NLP tasks.To wrap things up, here are a few resources you might find useful:",https://www.analyticsvidhya.com/blog/2019/06/datahack-radio-ines-montani-matthew-honnibal-brains-behind-spacy/
Exclusive Interview with Sonny Laskar  Kaggle Master and Analytics Vidhya Hackathon Expert,Learn everything about Analytics|Introduction|We covered a variety of data science topics during our conversation:|Sonny Laskars Background and First Role in Data Science|Industry Experience versus Data Science Competitions|Data Science Hackathons and Competitions|Data Science Industry-Related|Rapid Fire Questions: Sonnys Take on Various Data Science Aspects|End Notes,"Pranav Dar: You are currently the Associate Director of Automation and Analytics at Microland, finished 4 times in the top 3 in AVs hackathons, and hold a runner-up finish in a Kaggle competition. Its been quite a ride!|How and where did your data science journey begin?|PD: Your professional career didnt start off in data science. The first 6 years or so were spent on data warehousing and infrastructure.|So what kind of challenges did you face when you were getting into data science? How did you overcome them?|PD: We often hear from hiring managers how aspiring data scientists participate in hackathons and competitions and struggle to bridge the gap during their transition into an industry role.|You have been on both sides of this  you hold rich experience in data science and have excelled in hackathons. What has been your experience in the industry vs. hackathon debate?|PD: Ever since data science started becoming mainstream in the last 5 years, multiple competitions keep happening across platforms simultaneously. How do you pick and choose which data science hackathon or competition youll participate in?|PD: How should a beginner go about participating in these data science hackathons? Which kind of competition should they first dip their toes into?|PD: How should aspiring data scientists approach a competition?|PD: What are 3 critical aspects of a data science project which you feel are often overlooked by newcomers?|PD:AutoML is coming up huge in the industry. What are some other trends in data science we can expect to see in the next 2-3 years?|PD: Tell us 3 things you have learned working in data science.|PD: Which is your favorite machine learning/deep learning algorithm and why?|PD:Which data science professional would you pick to take part in a high-stakes data science competition?|PD:Whats your advice to people trying to get their first data science role?|Share this:|Related Articles|DataHack Radio #23: Ines Montani and Matthew Honnibal  The Brains behind spaCy|Decoding the Best Papers from ICLR 2019  Neural Networks are Here to Rule|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch  
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats the key to cracking data science competitions? How do you use this experience to break into the data science industry? We regularly come across these questions from aspiring data scientists wondering how to make a name for themselves in data science.Who better to answer these questions and provide an in-depth insight into the data science world than a Kaggle Master and a Analytics Vidhya hackathon expert? Ladies and gentlemen, Im delighted to present Sonny Laskar!Sonny is a MBA post-graduate from IIM Indore, the place he credits for starting his data science journey. So for any of you wondering if its possible to make a career transition to data science from a non-data science field  this article is for you.I found Sonny to be a very approachable person and his answers, as youll soon see, are very interesting, knowledgeable and rich with experience. Despite holding a senior role in the industry, Sonny loves taking part in data science competitions and hackathons and regularly scales the top echelons of competition leaderboards.Sonny also holds a lot of experience in the data engineering side of this field. As you can imagine, there is a LOT we can learn from him. I had the opportunity to pick his brain about various data science topics and bring this article to you.And a whole lot more! There is SO much to learn from Sonnys knowledge and thought process. Enjoy the discussion!Sonny Laskar: My Data Science journey started when I was pursuing my MBA from IIM Indore. Analytics was the go-to area for every aspirant. One of the early topics of discussions was based on how Target figured out a teen girl was pregnant before her father did. This made me very curious and I started to deep dive into the world of Data Science.I had already worked extensively with data but mostly around engineering problems and business intelligence. No serious machine learning stuff was popular back then with organizations in India.I spent two months at the University of Texas, Austin in early 2014 and was surprised by the level of maturity they had with data. My visit to Dells headquarters in Austin and how they used social media data to enhance their product positioning was amazing. By the end of this, I was completely convinced that I needed to work on this.SL: I started my career in 2007 in the world of IT Infrastructure. In the initial six years, I was primarily working on building massive scale data warehousing applications (processing ~10TB data every). The focus was more on ETL and BI. Dashboards and Data marts were the primary output of all these efforts. This was what we called Descriptive Analytics.By 2014-15, Predictive Analytics was already getting a lot of attention and adoption in the US. It was then that many organizations in India started looking at Predictive Analytics with significant focus. We were already processing Terabytes of data and were very well versed with the engineering side of things.I was able to understand the fundamentals of Data Science very well since my Mathematics and Statistics concepts are strong and I had a fair exposure to programming.I started with R since that was the programming language popular in academics and improved my understanding by practicing writing code and replicating other work.During my MBA, I got a birds eye view of many statistical and Data Science approaches. Since the focus during MBA was more on business, it didnt allow me to master the technical skills as much as the industry needs. Post my MBA, I started spending roughly 4-5 hours every day writing code and building on top of it.I have already written enough code in the past in Bash, Javascript, PHP & Perl. So, the learning curve was not very steep for me. I also invested in getting access to cloud subscriptions so that I could play with large volumes of data. I think its worth investing that money when you believe it is going to be helpful in the long term.Patience, Perseverance & Practice has been my thumb rule for everything in life, which was what I applied here as well.SL: Data Science is getting a lot of attention from the workforce in the market. It is in fact very easy to get some training to understand the basic concepts (thanks to MOOCs). This leads to excessive supply and recruiters then need some ways to filter. One of the best ways that work is establishing credibility by participating in data science competitions.Just like most things in life, competitions have their pros & cons. There is a lot of preparatory work that gets done before a competition is published. That work is at times extremely complex, time-taking and needs multi-domain understanding.Similarly, the competition ends with a leaderboard score without any view on what was done with the winners solutions. These are grey areas for many first-timers into Data Science which creates a lot of issues when they join the industry.I have conducted at least 100 in-person interviews in the last year and I can see this struggle very prominently. Data Scientists are not expected to just design a machine learning model to predict something. In many organizations, discussions in meeting rooms end up with a task for the Data Scientist such as Let us build a model to predict X.A good Data Scientist might end up concluding that many such X use cases should not be solved at all with machine learning! A Data Science team is not expected to be very large in the real world. They might get involved in many tasks which are either not valuable or can be easily solved without using Machine Learning.If they feel it can be solved with Machine Learning, then there must be a series of discussions to understand what data would help them address that.Unlike competitions, nobody gives you two .csv files called train and test and a nicely written evaluation metric. Almost 80% of the efforts go into defining the problem and getting and processing data. Remaining 20% effort goes into pure modeling and deployment.Exposure to competitions helps address a few parts of this:These are very significant activities and hence recruiters use competitions as a good filter to focus on a smaller set of candidates.To summarize, below are the key issues which competition focused people face when they join the industry:SL: I was hooked to data science competitions back in 2016. I used to participate in as many competitions as I could! Lately, my personal interest has kind of plateaued as incremental learning has diminished. Now I participate only if I have time and a very interesting problem.I also try to participate in offline hackathons along with my Kaggle Grandmaster friend Sudalai Rajkumar (SRK). I usually participate based on three factors:SL: As a beginner, it is important for folks to know the basic building blocks.I would strictly advise that they should not participate in any competition where the data set is large, and the problem statement is complex.They should start with relatively easy data science competitions. Below is what aspiring data scientists should do in the initial few weeks:SL:As we participate in many competitions, we realize that there are a common set of steps that we always follow. We should try to create a template out of it which we can easily modify in every competition. This makes life simpler.I follow the below process:SL: Interesting question. Here is what I would recommend focusing on:SL:AutoML will eventually automate most of the model building & model deployment part of the work. This will include dealing and working with feature engineering (to quite an extent).Importance of domain knowledge, logical reasoning, and having a problem-solving attitude is all that Data Scientist would be expected to excel at.Other key trends that I see:SL: There are too many to list down! But here are my top 3 picks:SL: I use Xgboost & Lightgbm for most of my tasks. They work almost every time. For deep learning, Keras with TensorFlow seems perfect to me.SL: Sudalai Rajkumar (SRK) any day!SL: Here are a few tips from my experience:I thoroughly enjoyed interacting with Sonny Laskar for this interview. His knowledge, his thought process and the way he articulates and structures his thoughts is something we can all learn from.What did you learn from this interview? Are there other data science leaders you would want us to interview? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/05/exclusive-interview-sonny-laskar-kaggle-master-analytics-vidhya-hackathon-expert/
Decoding the Best Papers from ICLR 2019  Neural Networks are Here to Rule,"Learn everything about Analytics|Introduction|And the Best Paper Award at ICLR 2019 Goes To:|Ordered Neurons: Integrating Tree Structures Into Recurrent Neural Networks|The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","Objective of this Paper|Whats the Previous State-of-the-Art?|New Method Proposed in this Paper|Activation Function: cumax()|Structured Gating Mechanism|Experiment and Results|Summarizing the Paper|Objective of this Paper|Lottery Ticket Hypothesis|Identifying Winning Tickets|Applications|The Importance of Winning Ticket Initialization|The Importance of Winning Ticket Structure|Limitation and Future Work|End Notes|Share this:|Related Articles|Exclusive Interview with Sonny Laskar  Kaggle Master and Analytics Vidhya Hackathon Expert|A Beginners Guide to Hierarchical Clustering and how to Perform it in Python|
Shubham Singh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love reading and decoding machine learning research papers. There is so much incredible information to parse through  a goldmine for us data scientists! I was thrilled when the best papers from the peerless ICLR 2019 (International Conference on Learning Representations) conference were announced.I couldnt wait to get my hands on them.However, the majority of research papers are quite difficult to understand. They are written with a specific audience in mind (fellow researchers) and hence they assume a certain level of knowledge.I faced the same issue when I first dabbled into these research papers. I struggled mightily to parse through them and grasp what the underlying technique was. Thats why I decided to help my fellow data scientists in understanding these research papers.There are so many incredible academic conferences happening these days and we need to keep ourselves updated with the latest machine learning developments. This article is my way of giving back to the community that has given me so much!In this article, well look at the two best papers from the ICLR 2019 conference. There is a heavy emphasis on neural networks here so in case you need a refresher or an introduction, check out the below resources:Lets break down these two incredible papers and understand their approaches.The structure of a natural language is hierarchical. This means that larger units or constituents comprise of smaller units or constituents (phrases). This structure is usually tree-like.While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of units. This paper proposes to add such an inductive bias by ordering the neurons.The researchers aim to integrate a tree structure into a neural network language model. The reason behind doing this is to improve generalization via better inductive bias and at the same time, potentially reduce the need for a large amount of training data.If you want to know more about LSTM applications, you can read our guides:And if you prefer learning via video, you can take the below course which has a healthy dose of LSTM:This is where things become really interesting (and really cool for all you nerds out there!).This paper proposes Ordered Neurons. It is a new inductive bias for RNN that forces neurons to represent information at different time-scales.This inductive bias helps to store long-term information in high-term neurons. The short-term information (that can be rapidly forgotten) is kept in low ranking neurons.A new RNN unit  ON-LSTM  is proposed. The new model uses an architecture similar to the standard LSTM:The difference is that the update function for the cell state ct is replaced with a new function cumax().It might be difficult to discern a hierarchy of information between the neurons since the gates in an LSTM act independently on each neuron. So, the researchers have proposed to make the gate for each neuron dependent on the others by enforcing the order in which neurons should be updated.Pretty fascinating, right?ON-LSTM includes a new gating mechanism and a new activation function cumax(). The cumax() function and LSTM are combined together to create a new model ON-LSTM. That explains why this model is biased towards performing tree-like composition operations.I wanted to spend a few moments talking about the cumax() function. It is the key to unlocking the approach presented in this awesome paper.This cumax() activation function is introduced to enforce an order to the update frequency:g^= cumax() = cumsum(softmax()),Here, cumsum denotes the cumulative sum. g^ can be seen as an expectation of a binary gate, g, which splits the cell state into two segments:Thus, the model can apply different update rules on each segment to differentiate long/short information.The paper also introduces a new master forget gate ft and a new master input gate it. These entities are also based on the cumax() function.The values in the master forget gate monotonically increase from 0 to 1 by following the properties of the cumax() function. A similar thing happens in the master input gate where the values monotonically decrease from 1 to 0.These gates serve as high-level control for the update operations of cell states. We can define a new update rule using the master gates:The researchers evaluated their model on four tasks:Here are the final results:The horizontal axis indicates the length of the sequence, and the vertical axis indicates the accuracy of models performance on the corresponding test set. The ON-LSTM model gives an impressive performance on sequences longer than 3.The ON-LSTM model shows better generalization while facing structured data with various lengths. A tree-structured model can achieve quite a strong performance on this dataset.This is one of my favorite papers of 2019. Lets break it down into easy-to-digest sections!Pruning is the process of removing unnecessary weights from neural networks. This process potentially reduces the parameter-counts by more than 90% without affecting the accuracy. It also decreases the size and energy consumption of a trained network, making our inference more efficient.However, if a network can be reduced in size, why dont we train this smaller architecture instead to make training more efficient?This is because the architectures uncovered by pruning are harder to train from the beginning and bring down the accuracy significantly.This paper aims to show that there exist smaller sub-networks that train from the start. These networks learn at least as fast as their larger counterpart while achieving similar test accuracy.For instance, we randomly sample and train sub-networks from a fully connected network for MNIST and convolutional networks for CIFAR10:The dashed lines trace the iteration of minimum validation loss and the test accuracy at that iteration across various levels of sparsity. The sparser the network, the slower the learning and the lower the eventual test accuracy.This is where the researchers have stated their lottery ticket hypothesis.A randomly-initialized, dense neural network contains a sub-network, labeled as winning tickets. This is initialized such that, when trained in isolation, it can match the test accuracy of the original network after training for at most the same number of iterations.Here is a superb illustration of the lottery ticket hypothesis concept:We identify a ticket by training its network and pruning its smallest-magnitude weight. The remaining, unpruned connections constitute the architecture of the winning ticket.Each unpruned connections value is then reset to its initialization from the original network before it was trained.The process for doing this involves an iterative process of smart training and pruning. I have summarized it in five steps:The pruning is one shot, which means it is done only once.But in this paper, the researchers focus on iterative pruning, which repeatedly trains, prunes, and resets the network overground. Each round prunes p^(1/n) % of the weights that survive the previous round.
As a result, this iterative pruning finds winning tickets that match the accuracy of the original network at smaller sizes as compared to one-shot pruning.A question that comes to everyones mind when reading these research papers  where in the world can we apply this? Its all well and good experimenting and coming up trumps with a new approach. But the jackpot is in converting it into a real-world application.This paper can be super useful for figuring out the winning tickets. Lottery Ticket Hypothesis could be applied on fully-connected networks trained on MNIST and on convolutional networks on CIFAR10, increasing both the complexity of the learning problem and the size of the network.Existing work on neural network pruning demonstrates that the functions learned by a neural network can often be represented with fewer parameters. Pruning typically proceeds by training the original network, removing connections, and further fine-tuning.In effect, the initial training initializes the weights of the pruned network so that it can learn in isolation during fine-tuning.A winning ticket learns more slowly and achieves lower test accuracy when it is randomly reinitialized. This suggests that initialization is important to its success.The initialization that gives rise to a winning ticket is arranged in a particular sparse architecture. Since we uncover winning tickets through heavy use of training data, we hypothesize that the structure of our winning tickets encodes an inductive bias customized to the learning task at hand.The researchers are aware that this isnt the finished product yet. The current approach has a few limitations which could be addressed in the future:In this article, we thoroughly discussed the two best research papers published in ICLR. I learned so much while going through these papers and understanding the thought process of these research experts. I encourage you to go through these papers yourself once you finish this article.There are a couple of more research focused conferences coming up soon. The International Conference on Machine Learning (ICML) and the Computer Vision and Pattern Recognition (CVPR) conferences are lined up in the coming months. Stay tuned!",https://www.analyticsvidhya.com/blog/2019/05/best-papers-iclr-2019/
A Beginners Guide to Hierarchical Clustering and how to Perform it in Python,Learn everything about Analytics|Introduction|Table of Contents|Supervised vs Unsupervised Learning|Why Hierarchical Clustering?|What is Hierarchical Clustering?|Types of Hierarchical Clustering|Steps to Perform Hierarchical Clustering|How should we Choose the Number of Clusters in Hierarchical Clustering?|Solving the Wholesale Customer Segmentation problem using Hierarchical Clustering|End Notes,"Agglomerative Hierarchical Clustering|Divisive Hierarchical Clustering|Setting up the Example|Creating a Proximity Matrix|Steps to Perform Hierarchical Clustering|Share this:|Related Articles|Decoding the Best Papers from ICLR 2019  Neural Networks are Here to Rule|Using the Power of Deep Learning for Cyber Security (Part 2)  Must-Read for All Data Scientists|
Pulkit Sharma
|16 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is crucial to understand customer behavior in any industry. I realized this last year when my chief marketing officer asked me  Can you tell me which existing customers should we target for our new product?That was quite a learning curve for me. I quickly realized as a data scientist how important it is to segment customers so my organization can tailor and build targeted strategies. This is where the concept of clustering came in ever so handy!Problems like segmenting customers are often deceptively tricky because we are not working with any target variable in mind. We are officially in the land of unsupervised learning where we need to figure out patterns and structures without a set outcome in mind. Its both challenging and thrilling as a data scientist.Now, there are a few different ways to perform clustering (as youll see below). I will introduce you to one such type in this article  hierarchical clustering.We will learn what hierarchical clustering is, its advantage over the other clustering algorithms, the different types of hierarchical clustering and the steps to perform it. We will finally take up a customer segmentation dataset and then implement hierarchical clustering in Python. I love this technique and Im sure you will too after this article!Note: As mentioned, there are multiple ways to perform clustering. I encourage you to check out our awesome guide to the different types of clustering:Its important to understand the difference between supervised and unsupervised learningunsupervised learning before we dive into hierarchical clustering. Let me explain this difference using a simple example.Supposewe want to estimate the count of bikes that will be rented in a city every day:Or, lets say we want to predict whether a person on board the Titanic survived or not:We have a fixed target to achieve in both these examples:So, when we are given a target variable (count and Survival in the above two cases) which we have to predict based on a given set of predictors or independent variables (season, holiday, Sex, Age, etc.), such problems are called supervised learning problems.Lets look at the figure below to understand this visually:Here, y is our dependent or target variable, and X represents the independent variables. The target variable is dependent on X and hence it is also called a dependent variable. We train our model using the independent variables in the supervision of the target variable and hence the name supervised learning.Our aim, when training the model, is to generate a function that maps the independent variables to the desired target. Once the model is trained, we can pass new sets of observations and the model will predict the target for them. This, in a nutshell, is supervised learning.There might be situations when we do not have any target variable to predict. Such problems, without any explicit target variable, are known as unsupervised learning problems. We only have the independent variables and no target/dependent variable in these problems.We try to divide the entire data into a set of groups in these cases. These groups are known as clusters and the process of making these clusters is known as clustering.This technique is generally used for clustering a population into different groups. A few common examples include segmenting customers, clustering similar documents together, recommending similar songs or movies, etc.There are a LOT more applications of unsupervised learning. If you come across any interesting application, feel free to share them in the comments section below!Now, there are various algorithms that help us to make these clusters. The most commonly used clustering algorithms are K-means and Hierarchical clustering.We should first know how K-means works before we dive into hierarchical clustering. Trust me, it will make the concept of hierarchical clustering all the more easier.Heres a brief overview of how K-means works:It is an iterative process. It will keep on running until the centroids of newly formed clusters do not change or the maximum number of iterations are reached.But there are certain challenges with K-means. It always tries to make clusters of the same size. Also, we have to decide the number of clusters at the beginning of the algorithm. Ideally, we would not know how many clusters should we have, in the beginning of the algorithm and hence it a challenge with K-means.This is a gap hierarchical clustering bridges with aplomb. It takes away the problem of having to pre-define the number of clusters. Sounds like a dream! So, lets see what hierarchical clustering is and how it improves on K-means.Lets say we have the below points and we want to cluster them into groups:We can assign each of these points to a separate cluster:Now, based on the similarity of these clusters, we can combine the most similar clusters together and repeat this process until only a single cluster is left:We are essentially building a hierarchy of clusters. Thats why this algorithm is called hierarchical clustering. I will discuss how to decide the number of clusters in a later section. For now, lets look at the different types of hierarchical clustering.There are mainly two types of hierarchical clustering:Lets understand each type in detail.We assign each point to an individual cluster in this technique. Suppose there are 4 data points. We will assign each of these points to a cluster and hence will have 4 clusters in the beginning:Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left:We are merging (or adding) the clusters at each step, right? Hence, this type of clustering is also known as additive hierarchical clustering.Divisive hierarchical clustering works in the opposite way. Instead of starting with n clusters (in case of n observations), we start with a single cluster and assign all the points to that cluster.So, it doesnt matter if we have 10 or 1000 data points. All these points will belong to the same cluster at the beginning:Now, at each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point:We are splitting (or dividing) the clusters at each step, hence the name divisive hierarchical clustering.Agglomerative Clustering is widely used in the industry and that will be the focus in this article. Divisive hierarchical clustering will be a piece of cake once we have a handle on the agglomerative type.We merge the most similar points or clusters in hierarchical clustering  we know this. Now the question is  how do we decide which points are similar and which are not? Its one of the most important questions in clustering!Heres one way to calculate similarity  Take the distance between the centroids of these clusters. The points having the least distance are referred to as similar points and we can merge them. We can refer to this as a distance-based algorithm as well (since we are calculating the distances between the clusters).In hierarchical clustering, we have a concept called a proximity matrix. This stores the distances between each point. Lets take an example to understand this matrix as well as the steps to perform hierarchical clustering.Suppose a teacher wants to divide her students into different groups. She has the marks scored by each student in an assignment and based on these marks, she wants to segment them into groups. Theres no fixed target here as to how many groups to have. Since the teacher does not know what type of students should be assigned to which group, it cannot be solved as a supervised learning problem. So, we will try to apply hierarchical clustering here and segment the students into different groups.Lets take a sample of 5 students:First, we will create a proximity matrix which will tell us the distance between each of these points. Since we are calculating the distance of each point from each of the other points, we will get a square matrix of shape n X n (where n is the number of observations).Lets make the 5 x 5 proximity matrix for our example:The diagonal elements of this matrix will always be 0 as the distance of a point with itself is always 0. We will use the Euclidean distance formula to calculate the rest of the distances. So, lets say we want to calculate the distance between point 1 and 2:(10-7)^2 = 9 = 3Similarly, we can calculate all the distances and fill the proximity matrix.Step 1:First, we assign all the points to an individual cluster:Different colors here represent different clusters. You can see that we have 5 different clusters for the 5 points in our data.Step 2:Next, we will look at the smallest distance in the proximity matrix and merge the points with the smallest distance. We then update the proximity matrix:Here, the smallest distance is 3 and hence we will merge point 1 and 2:Lets look at the updated clusters and accordingly update the proximity matrix:Here, we have taken the maximum of the two marks (7, 10) to replace the marks for this cluster. Instead of the maximum, we can also take the minimum value or the average values as well. Now, we will again calculate the proximity matrix for these clusters:Step 3:We will repeat step 2 until only a single cluster is left.So, we will first look at the minimum distance in the proximity matrix and then merge the closest pair of clusters. We will get the merged clusters as shown below after repeating these steps:We started with 5 clusters and finally have a single cluster. This is how agglomerative hierarchical clustering works. But the burning question still remains  how do we decide the number of clusters? Lets understand that in the next section.Ready to finally answer this question thats been hanging around since we started learning? To get the number of clusters for hierarchical clustering, we make use of an awesome concept called a Dendrogram.A dendrogram is a tree-like diagram that records the sequences of merges or splits.Lets get back to our teacher-student example. Whenever we merge two clusters, a dendrogram will record the distance between these clusters and represent it in graph form. Lets see how a dendrogram looks like:We have the samples of the dataset on the x-axis and the distance on the y-axis. Whenever two clusters are merged, we will join them in this dendrogram and the height of the join will be the distance between these points. Lets build the dendrogram for our example:Take a moment to process the above image. We started by merging sample 1 and 2 and the distance between these two samples was 3 (refer to the first proximity matrix in the previous section). Lets plot this in the dendrogram:Here, we can see that we have merged sample 1 and 2. The vertical line represents the distance between these samples. Similarly, we plot all the steps where we merged the clusters and finally, we get a dendrogram like this:We can clearly visualize the steps of hierarchical clustering. More the distance of the vertical lines in the dendrogram, more the distance between those clusters.Now, we can set a threshold distance and draw a horizontal line (Generally, we try to set the threshold in such a way that it cuts the tallest vertical line). Lets set this threshold as 12 and draw a horizontal line:The number of clusters will be the number of vertical lines which are being intersected by the line drawn using the threshold. In the above example, since the red line intersects 2 vertical lines, we will have 2 clusters. One cluster will have a sample (1,2,4) and the other will have a sample (3,5). Pretty straightforward, right?This is how we can decide the number of clusters using a dendrogram in Hierarchical Clustering. In the next section, we will implement hierarchical clustering which will help you to understand all the concepts that we have learned in this article.Time to get our hands dirty in Python!We will be working on a wholesale customer segmentation problem. You can download the dataset using this link. The data is hosted on the UCI Machine Learning repository. The aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.Lets explore the data first and then apply Hierarchical Clustering to segment the clients.We will first import the required libraries:Load the data and look at the first few rows:There are multiple product categories  Fresh, Milk, Grocery, etc. The values represent the number of units purchased by each client for each product. Our aim is to make clusters from this data that can segment similar clients together. Wewill, of course, use Hierarchical Clustering for this problem.But before applying Hierarchical Clustering, we have to normalize the data so that the scale of each variable is the same. Why is this important? Well, if the scale of the variables is not the same, the model might become biased towards the variables with a higher magnitude like Fresh or Milk (refer to the above table).So, lets first normalize the data and bring all the variables to the same scale:Here, we can see that the scale of all the variables is almost similar. Now, we are good to go. Lets first draw the dendrogram to help us decide the number of clusters for this particular problem:
The x-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence we can decide a threshold of 6 and cut the dendrogram:We have two clusters as this line cuts the dendrogram at two points. Lets now apply hierarchical clustering for 2 clusters:We can see the values of 0s and 1s in the output since we defined 2 clusters. 0 represents the points that belong to the first cluster and 1 represents points in the second cluster. Lets now visualize the two clusters:Awesome! We can clearly visualize the two clusters here. This is how we can implement hierarchical clustering in Python.Hierarchical clustering is a super useful way of segmenting observations. The advantage of not having to pre-define the number of clusters gives it quite an edge over k-Means.If you are still relatively new to data science, I highly recommend taking the Applied Machine Learning course. It is one of the most comprehensive end-to-end machine learning courses you will find anywhere. Hierarchical clustering is just one of a diverse range of topics we cover in the course.What are your thoughts on hierarchical clustering? Do you feel theres a better way to create clusters using less computational resources? Connect with me in the comments section below and lets discuss!",https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/
Using the Power of Deep Learning for Cyber Security (Part 2)  Must-Read for All Data Scientists,Learn everything about Analytics|Introduction|Table of Contents|What is PowerShell?|Understanding the Problem|Gathering and Building the PowerShell Scripts Dataset|Data Experiments|End Notes,"References|About the Authors|Share this:|Related Articles|A Beginners Guide to Hierarchical Clustering and how to Perform it in Python|Data Science Project: Scraping YouTube Data using Python and Selenium to Classify Videos|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are in the midst of a deep learning revolution. Unprecedented success is being achieved in designing deep neural network models for building computer vision and Natural Language Processing (NLP) applications.State-of-the-art benchmarks are disrupted and updated on a regular basis in tasks like object detection, language translation, and sentiment analysis. Its a great time to work with deep learning!But theres still one field that isnt quite riding this success wave. The application of deep learning in Information Security (InfoSec) is still very much in its nascent stages. The flashy nature of other applications attracts newcomers and investors  but InfoSec is one of the most crucial fields every data scientist should pay attention to.Heres the good news  Malware detection and network intrusion detection are two areas where deep learning has shown significant improvements over the rule-based and classic machine learning-based solutions [3].This article is the second part of our deep learning for cyber security series. We will demonstratethe power of deep neural networks using Tensorflow and Keras to detect obfuscated PowerShell scripts. As we mentioned  this is a must-read for anyone interested in this field.We highly recommend the below reads to get the most out of this article:PowerShell is a task automation and configuration management framework consisting of a robust command line shell. Microsoft open sourced and made it cross-platform compatible in August 2016.PowerShell has been a heavily exploited tool in various cyber attacks scenarios. According to a research study by Symantec, nearly 95.4% of all scripts analyzed by Symantec Blue Coat Sandbox were malicious[4].The Odinaff hacker group leveraged malicious PowerShell scripts as part of its attacks on banks and other financial institutions [5]. We can find many tools like PowerShell Empire [6] and PowerSploit [7] on the internet that can be used for reconnaissance, privilege escalation, lateral movement, persistence, defense evasion, and exfiltration.Adversaries typically use two techniques to evade detection:Obfuscation of PowerShell scripts for malicious intent is on the rise. The task of analyzing them is made even more difficult due to the high flexibility of its syntax. In Acalvio high interaction decoys, we can monitor PowerShell logs, commands, and scripts that the attacker tried to execute in the decoy. We collect these logs and analyze them in real time and detect whether the script is obfuscated or not.Microsoft PowerShell is the ideal attackers tool in a Windows operating system. There are two primary reasons behind this:Microsoft has enhanced PowerShell logging considerably since they launched PowerShell 3.0. If Script Block Logging is enabled, we can capture commands and scripts executed through PowerShell in the event logs. These logs can be analyzed to detect and block malicious scripts.Obfuscation is typically used to evade detection. Daniel and Holmes address this problem of detecting obfuscated scripts in their Blackhat paper [8]. They used a Logistic Regression classifier with Gradient Descent method to achieve a reasonable classification accuracy to separate the obfuscated script from clean scripts.However, a deep feed-forward neural network (FNN) might enhance other performance metrics, such as precision and recall. Hence in this blog, we decided to use a deep neural network and compare the performance metrics with different machine learning (ML) classifiers.We have used the PowerShellCorpus dataset published and open sourced by Daniel [9] for our data experiments. The dataset consists of around ~300k PowerShell scripts scraped from various sources on the internet like Github, PowerShell Gallery, and Technet.We also scraped PowerShell scripts from Poshcode [10] and added them to the corpus. Finally, we had nearly 3 GB of script data consisting of 300k clean scripts. We used the Invoke-Obfuscation [11] tool to obfuscate the scripts. Once we obfuscated all scripts using this tool, we labeled the dataset consisting of class labels as clean or obfuscated script.All the activities performed by an attacker in a high interaction decoy are malicious. However, obfuscation detection asserts the presence of an advanced attacker. Here is a simple PowerShell command to get a list of processes:This command may be obfuscated as:This looks suspicious and noisy. Here is another example of a subtle obfuscation for the same command:This obfuscation makes it hard to detect the intent of the PowerShell command/script. Most of the malicious PowerShell scripts in the wild have these kinds of subtle variations that help them to evade anti-virus software easily.It is nearly impossible for a security analyst to review every PowerShell script to determine whether it is obfuscated or not. Therefore, we need to automate the obfuscation detection. We can use a rule-based approach for obfuscation detection; however, it may not detect a lot of obfuscation types, and a large number of rules need to be manually written by a domain expert. Therefore, a machine learning/deep learning-based solution is an ideal answer for this problem.Typically, the first step of machine learning is data cleaning and preprocessing. For the obfuscation detection dataset, the data preprocessing task is done to remove Unicode characters from a script. Obfuscated scripts look different from normal scripts. Some combination of characters used in obfuscated scripts are not used in normal scripts. So, we use character-level representation for all PowerShell scripts instead of word-based representation.Also, in the case of PowerShell scripting, sophisticated obfuscation can sometimes completely blur the boundary between words/tokens/identifiers, rendering it useless for any word-based tokenization. In fact, character-based tokenization is also used by security researchers to detect PowerShell obfuscated scripts.Lee Holmes from Microsoft has explored character frequency-based representation and cosine similarity to detect obfuscated scripts in his blog [12].There are multiple ways in which characters can be vectorized. One-hot encoding of characters represents every character by a bit. The bit is set to 0 or 1 depending upon whether that character is present in the script or not. The classifiers trained with a single character one-hot encoding perform well.However, this can be improved by capturing the sequence of characters. For example, a command like New-Object may be obfuscated as (Ne+w-+Objec+t). The character plus (+) operator is common for any PowerShell script. However, plus (+) followed by a single () or double quote () may not be as common. Therefore, we use tf-idf character bigrams to represent as the features input to the classifiers. Here are 20 bigrams with top tf-idf score from the training dataset:Clean script:Obfuscated script:Each script is represented using the character bigrams. We process all these features using a deep Feed Forward Neural Network (FFN) with N hidden layers using Keras and Tensorflow.Figure 1: Obfuscation Detection data flow diagram using deep FFNThe data flow diagram above shows the training and prediction flow for obfuscation detection. We have varied the value of hidden layers in the deep FNN and found N=6 to be optimal.The RELU activation function is used for all the hidden layers. Each hidden layer is dense in nature with a dimension of 1000 and a dropout rate of 0.5. For the last layer, sigmoid is used as the activation function. Figure 2 below shows the deep FFN network representation for obfuscation detection:Figure 2: FFN Network Representation for Obfuscation DetectionWe see a validation accuracy of nearly 92% that indicates the model has generalized well outside the training set.Next, we test our model on the test set. We get an accuracy of 93% with 0.99 recall for the obfuscated class. Figure 3 below shows the classification accuracy and classification loss plots for training and validation data for each epoch:Figure 3: Classification Accuracy and Loss plots for Training and Validation PhaseCheck out the results of our deep FNN as compared to other ML models in the table below. Precision and recall were used to measure the efficacy of the various models:Our objective is to correctly detect most of the obfuscated scripts as the obfuscated script. In other words, we would like to minimize the false negative rate for the obfuscated class. The Recall metric seems to be the appropriate measure in this case.Table 1 shows that the deep FNN model achieves more recall as compared to other classifiers. The dataset used in our experiments is of medium scale. The datasets used in the wild are typically quite big, and deep FNN performs even better as compared to the other ML classifiers.PowerShell obfuscation is a smart way to bypass existing antivirus software and hide the attackers intent. Many adversaries use this technique.In this blog, we demonstrated the power of deep learning to detect obfuscated PowerShell scripts. In our next blog of this series, we will share some more use cases where AI and deception can be leveraged for information security.Santosh Kosgi, Data Scientist Acalvio TechnologiesSantosh is a member of Data Science team at Acalvio Technologies. He holds a Masters in Computer Science from IIIT Hyderabad. He previously worked at 247.ai. He is interested in solving real world problems using Machine Learning.Arunabha Choudhury, Data Scientist  Acalvio TechnologiesArunabha is a member of the Data Science team at Acalvio Technologies. He holds a Masters Degree in CS from University of Kansas with minor in Machine Learning. In his 6+ years of experience as Data Scientists, he has worked with companies like Tresata (now considered a Big Data Unicorn based out of Charlotte, NC) and Samsung R&D. To his credit he has 3 patents and 4 conference publications. He is primarily interested in Machine Learning at Scale.Waseem Mohd., Data Scientist  Acalvio TechnologiesWaseem is a member of the Data Science team at Acalvio. He is a Computer Science graduate from IIT Delhi and has previously worked with companies like Samsung and Microsoft.He is interested in real world applications of Deep Learning.Dr. Satnam Singh, Chief Data Scientist  Acalvio TechnologiesDr. Satnam Singh is currently leading security data science development at Acalvio Technologies. He has more than a decade of work experience in successfully building data products from concept to production in multiple domains. He was named as one of the top 10 data scientists in India in 2015. He has 25+ patents and 30+ journal and conference publications to his credit.Apart from holding a PhD degree in ECE from University of Connecticut, Satnam also holds a Masters in ECE from University of Wyoming. Satnam is a senior IEEE member and a regular speaker in various Big Data and Data Science conferences.",https://www.analyticsvidhya.com/blog/2019/05/using-power-deep-learning-cyber-security-2/
Data Science Project: Scraping YouTube Data using Python and Selenium to Classify Videos,Learn everything about Analytics|Introduction|Table of Contents|Overview of Selenium|Prerequisites for our Web Scraping Project|Setting up the Python Environment|Scraping Data from YouTube|Cleaning the Scraped Data using the NLTK Library|Building our Model to Classify YouTube Videos|Analyzing the Results|End Notes,"Share this:|Related Articles|Using the Power of Deep Learning for Cyber Security (Part 2)  Must-Read for All Data Scientists|Statistics for Data Science: Introduction to t-test and its Different Types (with Implementation in R)|
Shubham Singh
|29 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was submitted as part of Analytics Vidhyas Internship Challenge.Im an avid YouTube user. The sheer amount of content I can watch on a single platform is staggering. In fact, a lot of my data science learning has happened through YouTube videos!So, I was browsing YouTube a few weeks ago searching for a certain category to watch. Thats when my data scientist thought process kicked in. Given my love for web scraping and machine learning, could I extract data about YouTube videos and build a model to classify them into their respective categories?I was intrigued! This sounded like the perfect opportunity to combine my existing Python and data science knowledge with my curiosity to learn something new. And Analytics Vidhyas internship challenge offered me the chance to pen down my learning in article form.Web scraping is a skill I feel every data science enthusiast should know. It is immensely helpful when were looking for data for our project or want to analyze specific data present only on a website. Keep in mind though, web scraping should not cross ethical and legal boundaries.In this article, well learn how to use web scraping to extract YouTube video data using Selenium and Python. We will then use the NLTK library to clean the data and then build a model to classify these videos based on specific categories.You can also check out the below tutorials on web scraping using different libraries:Selenium is a popular tool for automating browsers. Its primarily used for testing in the industry but is also very handy for web scraping. You must have come across Selenium if youve worked in the IT field.We can easily program a Python script to automate a web browser using Selenium. It gives us the freedom we need to efficiently extract the data and store it in our preferred format for future use. Selenium requires a driver to interface with our chosen browser. Chrome, for example, requires ChromeDriver, which needs to be installed before we start scraping. The Selenium web driver speaks directly to the browser using the browsers own engine to control it. This makes it incredibly fast.There are a few things we must know before jumping into web scraping:Time to power up your favorite Python IDE (thats Jupyter notebooks for me)! Lets get our hands dirty and start coding.Step 1: Install Python binding:Step 2:Download Chrome WebDriver:Step 3:Move the driver file to a PATH:Go to the downloads directory, unzip the file, and move it to usr/local/bin PATH.Were all set to begin web scraping now.In this article, well be scraping the video ID, video title, and video description of a particular category from YouTube. The categories well be scraping are:So lets begin!With me so far? Now, write the below code to start fetching the links from the page and run the cell. This should fetch all the links present on the web page and store it in a list. Note: Traverse all the way down to load all the videos on that page.The above code will fetch the href attribute of the anchor tag we searched for. Now, we need to create a dataframe with 4 columns  link, title, description, and category. We will store the details of videos for different categories in these columns:We are all set to scrape the video details from YouTube. Heres the Python code to do it:Lets breakdown this code block to understand what we just did:During each iteration, our code saves the extracted data inside the dataframe we created earlier.We have to follow the aforementioned steps for the remaining five categories. We should have six different dataframes once we are done with this. Now, its time to merge them together into a single dataframe:Voila! We have our final dataframe containing all the desired details of a video from all the categories mentioned above. In this section, well use the popular NLTK library to clean the data present in the title and description columns. NLP enthusiasts will love this section!Before we start cleaning the data, we need to store all the columns separately so that we can perform different operations quickly and easily:Import the required libraries first:Now, create a list in which we can store our cleaned data. We will store this data in a dataframe later. Write the following code to create a list and do some data cleaning on the title column from df_title:Did you see what we did here? We removed all the punctuation from the titles and only kept the English root words. After all these iterations, we are ready with our list full of data.We need to follow the same steps to clean the description column from df_description:Note: The range is selected as per the rows in our dataset.Now, convert these lists into dataframes:Next, we need to label encode the categories. The LabelEncoder() function encodes labels with a value between 0 and n_classes  1 where n is the number of distinct labels.Here, we have applied label encoding on df_category and stored the result into dfcategory.We can store our cleaned and encoded data in into a new dataframe:Were not quite all the way done with our cleaning and transformation part.We should create a bag-of-words so that our model can understand the keywords from that bag to classify videos accordingly. Heres the code to do create a bag-of-words:Note: Here, we created 1500 features from data stored in the lists  corpus and corpus1. X stores all the features and y stores our encoded data.We are all set for the most anticipated part of a data scientists role  model building!Before we build our model, we need to divide the data into training set and test set:Make sure that your test set meets the following two conditions:We can use the following code to split the data:Time to train the model! We will use the random forest algorithm here. So lets go ahead and train the model using theRandomForestClassifier() function:Parameters:Note: These parameters are tree-specific.We can now check the performance of our model on the test set:We get an impressive 96.05% accuracy. Our entire process went pretty smoothly! But were not done yet  we need to analyze our results as well to fully understand what we achieved.Lets check the classification report:The result will give the following attributes:We can check our results by creating aconfusion matrix as well:The confusion matrix will be a 66 matrix since we have six classes in our dataset.Ive always wanted to combine my interest in scraping and extracting data with NLP and machine learning. So I loved immersing myself in this project and penning down my approach.In this article, we just witnessed Seleniums potential as a web scraping tool.All the code used in this article israndom forest algorithm Congratulations on successfully scraping and creating a dataset to classify videos!I look forward to hearing your thoughts and feedback on this article.",https://www.analyticsvidhya.com/blog/2019/05/scraping-classifying-youtube-video-data-python-selenium/
Statistics for Data Science: Introduction to t-test and its Different Types (with Implementation in R),Learn everything about Analytics|Introduction|Table of Contents|When should we Perform a t-test?|Assumptions for Performing a t-test|Types of t-tests (with Solved Examples in R)|End Notes,"One-Sample t-test|Implementing the One-Sample t-test in R|Independent Two-Sample t-test|Implementing the Two-Sample t-test in R|Paired Sample t-test|Implementing the Paired t-test in R|Share this:|Related Articles|Data Science Project: Scraping YouTube Data using Python and Selenium to Classify Videos|10 Useful Data Analysis Expressions (DAX) Functions for Power BI Beginners|
Analytics Vidhya Content Team
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You cant prove a hypothesis; you can only improve or disprove it.  Christopher MoncktonEvery day we find ourselves testing new ideas, finding the fastest route to the office, the quickest way to finish our work, or simply finding a better way to do something we love. The critical question, then, is whether our idea is significantly better than what we tried previously.These ideas that we come up with on such a regular basis  thats essentially what a hypothesis is. And testing these ideas to figure out which one works and which one is best left behind, is called hypothesis testing.Hypothesis testing is one of the most fascinating things we do as data scientists. No idea is off-limits at this stage of our project. I have personally seen so many insights coming out of hypothesis testing  insights most of us would have missed if not for this stage!One of the most popular ways to test a hypothesis is a concept called the t-test. There are different types of t-tests, as well soon see, and each one has its own unique application. If youre an aspiring data scientist, you should be aware of what a t-test is and when you can leverage it.So in this article, we will learn about the various nuances of a t-test and then look at the three different t-test types. The icing on the cake? We will implement each type of t-test in R to visualize how they work in practical scenarios. Lets get going!Note: You should go through the below article if you need to brush up on your hypothesis testing concepts:Lets first understand where a t-test can be used before we dive into its different types and their implementations. I strongly believe the best way to learn a concept is by visualizing it through an example. So lets take a simple example to see where a t-test comes in handy.Consider a telecom company that has two service centers in the city. The company wants to find whether the average time required to service a customer is the same in both stores.Source: ShopworksThe company measures the average time taken by 50 random customers in each store. Store A takes 22 minutes while Store B averages 25 minutes. Can we say that Store A is more efficient than Store B in terms of customer service?It does seem that way, doesnt it? However, we have only looked at 50 random customers out of the many people who visit the stores. Simply looking at the average sample time might not be representative of all the customers who visit both the stores.This is where the t-test comes into play. It helps us understand if the difference between two sample means is actually real or simply due to chance.There are certain assumptions we need to heed before performing a t-test:So what are the different types of t-tests? When should we perform each type? Well answer these questions in the next section and see how we can perform each t-test type in R.There are three types of t-tests we can perform based on the data at hand:In this section, we will look at each of these types in detail. I have also provided the R code for each t-test type so you can follow along as we implement them. Its a great way to learn and see how useful these t-tests are!In a one-sample t-test, we compare the average (or mean) of one group against the set average (or mean). This set average can be any theoretical value (or it can be the population mean).Consider the following example  A research scholar wants to determine if the average eating time for a (standard size) burger differs from a set value. Lets say this value is 10 minutes. How do you think the research scholar can go about determining this?He/she can broadly follow the below steps:That, in a nutshell, is how we can perform a one-sample t-test. Heres the formula to calculate this:where,Note: As mentioned earlier in the assumptions that large sample size should be taken for the data to approach a normal distribution. (Although t-test is essential for small samples as their distributions are non-normal).Once we have calculated the t-statistic value, the next task is to compare it with the critical value of the t-test. We can find this in the below t-test table against the degree of freedom (n-1) and the level of significance:This method helps us check whether the difference between the means is statistically significant or not. Lets further solidify our understanding of a one-sample t-test by performing it in R.A mobile manufacturing company has taken a sample of mobiles of the same model from the previous months data. They want to check whether the average screen size of the sample differs from the desired length of 10 cm. You can download the data here.Step 1:First, import the data.Step 2: Validate it for correctness in R:Output:Step 3: Remember the assumptions we discussed earlier? We need to check them:We get the below Q-Q plot:Almost all the values lie on the red line. We can confidently say that the data follows a normal distribution.Step 4:Conduct a one-sample t-test:Output:The t-statistic comes out to be -0.39548. Note that we can treatnegative values as their positive counterpart here. Now, refer to the table mentioned earlier for the t-critical value. The degree of freedom here is 999 and the confidence interval is 95%.The t-critical value is 1.962.Since thet-statistic is less than the t-critical value, we fail to reject the null hypothesis and can conclude that the average screen size of the sample does not differ from 10 cm.We can also verify this from the p-value, which is greater than 0.05. Therefore, we fail to reject the null hypothesis at a 95% confidence interval.The two-sample t-test is used to compare the means of two different samples.Lets say we want to compare the average height of the male employees to the average height of the females. Of course, the number of males and females should be equal for this comparison. This is where a two-sample t-test is used.Heres the formula to calculate the t-statistic for a two-sample t-test:where,Here, the degree of freedom is nA + nB  2.We will follow the same logic we saw in a one-sample t-test to checkif the average of one group is significantly different from another group. Thats right  we will compare the calculated t-statistic with the t-critical value.Lets take an example of an independent two-sample t-test and solve it in R.For this section, we will work with data about two samples of the various models of a mobile phone. We want to check whether the mean screen size of sample 1 differs from the mean screen size of sample 2. You can download the data here.Step 1:Again, first import the data.Step 2: Validate it for correctness in R:Step 3:We need to check the assumptions as we did above. I will leave that exercise up to you now.Also, in this case, we will check the homogeneity of variance:Output:Great, the variances are equal. We can move ahead.Step 4:Conduct the independent two-sample t-test:Note: Rewrite the above code with var.equal = F if you getunequal or unknown variances. This will be a case of Welchs t-test which is used to compare the means of two samples with unequal variances.Output:What can you infer from the above output? We can confirm that the t-statistic is again less than the t-critical value so we fail to reject the null hypothesis. Hence, we can conclude that there is no difference between the mean screen size of both samples.We can verify this again using the p-value. It comes out to be greater than 0.05, therefore we fail to reject the null hypothesis at a 95% confidence interval. There is no difference between the mean of the two samples.The paired sample t-test is quite intriguing. Here, we measure one group at two different times. We compare separate means for a group at two different times or under two different conditions. Confused? Let me explain.A certain manager realized that the productivity level of his employees was trending significantly downwards. This manager decided to conduct a training program for all his employees with the aim of increasing their productivity levels.How will the manager measure if the productivity levels increased? Its simple  just comparethe productivity level of the employees before versus after the training program.Here, we are comparing the same sample (the employees) at two different times (before and after the training). This is an example of a paired t-test. The formula to calculate the t-statistic for a paired t-test is:where,We can take the degree of freedom in this test as n  1 since only one group is involved. Now, lets solve an example in R.The manager of a tyre manufacturing company wants to compare the rubber material for two lots of tyres. One way to do this  check the difference between average kilometers covered by one lot of tyres until they wear out.You can download the data fromhere. Lets do this!Step 1:First, import the data.Step 2: Validate it for correctness in R:Step 3:We now check the assumptions just as we did in a one-sample t-test. Again, I will leave this to you.Step 4:Conduct the paired t-test:Output:You must be a pro at deciphering this output by now! The p-value is less than 0.05. We can reject the null hypothesis at a 95% confidence interval and conclude that there is a significant differencebetween the means of tyres before and after the rubber material replacement.The negative mean in the difference depicts that the average kilometers covered by tyre 2 are more than the average kilometers covered by tyre 1.In this article, we learned about the concept of t-test, its assumptions, and also the three different types of t-tests with their implementations in R. The t-test has both statistical significance as well as practical applications in the real world.If you are new to statistics, want to cover your basics, and also want to get a start in data science, I recommend taking theIntroduction to Data Science course. It gives you a comprehensive overview of both descriptive and inferential statistics before diving into data science techniques.Did you find this article useful? Can you think of any other applications of the t-test? Let me know in the comments section below and we can come up with more ideas!",https://www.analyticsvidhya.com/blog/2019/05/statistics-t-test-introduction-r-implementation/
10 Useful Data Analysis Expressions (DAX) Functions for Power BI Beginners,Learn everything about Analytics|Introduction|Table of Contents|Introduction to Business Intelligence (BI)|The Role of a Business Intelligence Professional|What is Power BI?|Data Analytics Expressions (DAX): What are they and why are they needed?|Implementation of DAX Functions in Power BI|End Notes,"LOOKUP( )|FILTER( ) & CALCULATE( )|Nested IF Condition|Conditional Formatting|Splitting a String Based on Delimiters|Fetching a Particular Letter from a Word|Concatenating Strings|WEEKDAY()|DATE Split Up|Complex Aggregations Based on DATE|About the Authors|Share this:|Related Articles|Statistics for Data Science: Introduction to t-test and its Different Types (with Implementation in R)|A Beginners Guide to Tidyverse  The Most Powerful Collection of R Packages for Data Science|
Guest Blog
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We have worked on plenty of drag-and-drop tools in our business intelligence (BI) journey. But none has come close to matching the Swiss army knife nature of Microsofts Power BI. It truly simplifies the task of crunching numbers, analyzing data and visualizing patterns.It is one of the best drag-and-drop tools in the industry. You pick a field you want to analyze, drop it into the Power BI dashboard, and voila! Insights are right in front of you.But what if we want to dive deeper into our analysis? What should we do if we want to customize certain variables or generate new ones? Just doing a quick drag and drop wont solve our problem. Thats where the power of Data Analysis Expressions (DAX) in Power BI comes into play.This is a handy tool to learn for any data science professional, not just an aspiring business intelligence one. It saves us a ton of time we would otherwise be spending in churning out the code.In this article, we will first understand what BI is and the typical role of a BI industry professional. We will then introduce you to the powerful Microsoft Power BI tool and then deep dive into ten really useful data analysis expressions (DAX) functions we can use in Power BI.Business Intelligence has a broad scope but we can define it as the process of tracking and reviewing business metrics.Lets take an example to understand this.Consider that were running a shipping company which delivers products to our customers in different parts of the country. As with any business, we want to improve our customer experience. There are several metrics we can measure for gauging customer satisfaction, including:This data is processed and displayed in the form of lucid reports which makes it easier to bring home the insights. So, if the chart for the rate of delivery displays a lower rating than expected, we can try to find the root cause and improve the delivery speed.Now, what does a business intelligence professional do? Im sure this must have crossed your mind while going through the above scenario. Below are a few common tasks a typical BI person does:The skillset required to become a BI professional varies from project to project. But broadly, you would need to have a solid knowledge of a BI tool, such as Tableau, PowerBI, Qlik, along with some experience in a programming language like Python, R or SQL. Knowledge of the domain and structured thinking are also sought after.Power BI is one of the most popular and powerful BI tools out there. That will be our focus in this article. Before we get there, heres a wonderful illustration of a typical business intelligence architecture:Sample Business Intelligence architecturePower BI is a popular and incredibly powerful business intelligence tool developed by Microsoft. It has different flavors and offerings. The most basic version is free but it has the ability to perform mid-sized business intelligence exercises easily.In broad terms, Power BI is a cloud-based business analytics solution suite that provides the necessary tools to turn vast volumes of data across silos into accessible information. It has been consistently ranked in the Gartner BI Magic Quadrant.Power BI is popular for its versatility, interactivity, aesthetic designs, extensive connectivity to databases and ease of creating low-effort dashboards. It also leverages support from other tools from its Microsoft ecosystem  Azure, Cortana, SQL Server, Azure Active Directory, Azure Blob Storage, etc.Here are a couple of cool things Power BI can do:We can read data into Power BI from CSV files or from a database. We can also merge tables in Power BI. Quite often, the data required for plotting is readily available in table columns (though thats not always the case).Consider a situation where we need to modify or change the data to improve our dashboards. Suppose those custom modifications we need are not readily available in Power BI. What do we do then? How can we perform this kind of modification and data analysis?The answer is DAX (short for Data Analysis Expressions). DAX is used to bring some meaningful information hidden inside the raw data. In simple words, DAX is used for data manipulation.There are certain rules we have to follow for using DAX. Youll understand these rules quickly if you are familiar with Excel functions. DAX is just an advanced form of that.There are two places where we write DAX:Lets see what both of these stand for:We will work on a few useful DAX commands and their functions in this section. We will be using the Sample Super Store dataset. You can download the dataset hereand start experimenting on your own as well!The dataset contains three tables  Orders, Returns and Users. Go ahead and load the Sample Superstore dataset Excel file into Power BI.Open Power BI and look for Get data in the Home tab. Select Excel and then browse to the dataset file present in your local machine. Load the entire file into your BI window.Now, lets get going!The LOOKUP function is pretty similar to Vlookup in Microsoft Excel.The third table in our dataset contains the details of all the managers per region. Now here is where LOOKUP comes in hand. We can perform a lookup for the Manager column in the users table against the corresponding Region column in the orders table.So how do we perform a lookup in Power BI? There are two things we need to remember:Putting this syntax using our dataset variables:The DAX displayed below is similar to the group by function. It dynamically aggregates a column based on the filter. This is helpful when we are creating a table in Power BI dashboards and need to filter only one column (while the remaining column remains unaffected by the filter).This DAX comes in handy where every column used in the table can have its own filter. Lets take an example to understand how this works.We want to calculate the sum of sales by region. So first, the filter function divides the region column into north, south, east and west. Then, it calculates the sum of sales according to the segregation. We are using a measure here since a particular region can have any number of rows involved within it.Here, we are declaring a variable reg which acts as a key for the filter. We can declare a variable using the keyword VAR. The RETURN keyword gives us the result of the calculation (sum of sales, in our example). The result we get from the calculated DAX is:Consider the orders table in our dataset. The Order Priority column has five values under it. Lets assume we need some integer values instead of the original values present in that column. The Nested IF statement is our friend here:Conditional formatting is one of the most commonly used features of Microsoft Excel. And we can leverage that inside Power BI as well!Conditional formatting, for those who havent used it before, is the ability to change the font color of a column based on a condition from another column. This can be done by creating a new column as per our condition and then using that column to set rules in the conditional formatting tab.Lets try this with an example from our Sample Superstore dataset.We want to change the color of the values displayed in the Order priority column of the orders table. For example, all Critical values should be in red color, all High values should be in green color, etc.We can use the Nested IF column we created above since the conditions are already specified. Set rules in the conditional formatting tab  if the value of the column created above is 1, then the font color should be red If the value is 2, then it should orange, and so on.Heres an image to show how you can do it in Power BI:Another common Excel function we can use in Power BI:The above PATHITEM function returns the resulting string. The SUBSTITUTEfunction replaces the delimiter specified with a particular character and fetches the corresponding word based on the value mentioned.For example, lets perform a split on the Customer Name field. Here, the   space will be the delimiter and we have replaced it with a pipeline |. We need only the first name of a customer, so we have specified 1. You can change this value and see what results you get.Now lets perform three split ups like this and name them as customer_split_1, customer_split_2, customer_split_3 respectively.What if we wanted to extract only a particular letter from a word? It sounds tricky, but its actually quite easy in Power BI:We did it in just one line of code!Lets say a particular customers name is Helen Stein and the split is divided into two parts. We use the above DAX to get the first letter from the first split word (Helen). The LEFT function returns the number of characters by positioning to the start of the string. If we give 2 instead of 1, the above DAX will return He instead of H.We often find ourselves in situations where we need to combine two words together. To see how this works in Power BI, lets assume we have multiple initials in our data.We need to specify only the customer initials in the visualization rather than their whole name. This will help keep the table neat and compact:The concatenate function joins strings together. Here, we have performed a concatenation of three columns:The WEEKDAY function returns an integer number giving us the current day. Lets figure out the day when a few orders were dispatched:The integer number displayed above specifies the start day:There are times when we need to un-pivot the date for certain projects. Heres the good news  we can do this with the help of DAX and the query editor in Power BI.Now, we have two columns  order date and shipping date. We want to delist the dates lying between the two intervals. Lets take the order date as 01-01-2015 and the shipping date as 03-01-2015. Since theres a 3-day difference between them, this row will be listed three times.We can do this through the query editor in Power BI. Select the Custom Column option in the Add Column Tab. The below window pops up and we can add a column name and use the DAX query:Next, find the DATE column in our dataset. Click on the small box inside the DATE column header and select the Expand to new rows option:Notice how the values are appearing in the form of whole numbers? Now, right-click on the column and change the data type to Date format:Working with date data is quite a complicated challenge. There is a lot more to it than just splitting up the different days, months, etc.In this section, we will work on an example of complex aggregation based on dates. For instance, what is the total number of hours listed for the next 2 weeks or the past 2 weeks?Well switch our dataset for this section. You can download the new dataset, called Weekcal,from Forecast. This dataset is modeled on team time-allocation and a planning tool called Float. Float helps us assign tasks and calculate the estimated hours for team members.Theres one caveat in Float we should be aware of. A week always starts from Monday in the Float tool. For example, if you want to calculate the assigned hours for next week starting from Thursday, you will find hours assigned from the most recent Monday to Friday and not from this Thursday to next Thursday.Now, the dataset contains the below features:Lets assume we are interested in finding the number of hours allocated for next week. We can do this using the below logic:Check out the table below to see the estimated hours for each task for the following week:Power BI is a powerful and easy-to-use tool to create quick dashboards with a variety of data sources. DAX queries are used to map data, create new variables and business metrics, and manipulate data. This article gives a brief and quick guide to start your DAX journey.You should also check out theOfficial Microsoft Tutorials for Power BI a great resource!We hope you found this article useful. We look forward to hearing your feedback and comments. Happy Dashboarding, or should we say Happy DAXing!Kirthi Tej YendlooriKirthi Tej is a data analyst at Indium software. He has a year of experience in data visualization and is extremely interested in machine learning. He loves to travel.Sandhya SwaroopSandhya is a young and enthusiastic data analyst willing to learn new and upcoming technologies. She loves data visualization and is passionate about writing.",https://www.analyticsvidhya.com/blog/2019/05/10-useful-data-analysis-expressions-dax-functions-power-bi/
A Beginners Guide to Tidyverse  The Most Powerful Collection of R Packages for Data Science,Learn everything about Analytics|Introduction|Table of contents|What is Tidyverse?|Core R Packages in Tidyverse|Data Wrangling and Transformation|dplyr|tidyr|stringr|forcats|Data Import and Management|readr|tibble|Functional Programming|purrr|Data Visualization and Exploration|ggplot2|Some More Tidyverse Packages|End Notes,"Importing Data|Data Wrangling|Share this:|Related Articles|10 Useful Data Analysis Expressions (DAX) Functions for Power BI Beginners|A Practical Introduction to Prescriptive Analytics (with Case Study in R)|
Analytics Vidhya Content Team
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data scientists spend close to 70% (if not more) of their time cleaning, massaging and preparing data. Thats no secret  multiple surveys have confirmed that number. I can attest to it as well  it is simply the most time-taking aspect in a data science project.Unfortunately, it is also among the least interesting things we do as data scientists. There is no getting around it, though. It is an inevitable part of our role. We simply cannot build powerful and accurate models without ensuring our data is well prepared.So how can we make this phase of our job interesting?Welcome to the wonderful world of Tidyverse! It is the most powerful collection of R packages for preparing, wrangling and visualizing data. Tidyverse has completely changed the way I work with messy data  it has actually made data cleaning and massaging fun!Source: tidyverse.orgIf youre a data scientist and have not yet come across Tidyverse, this article will blow your mind. I will show you the top R packages bundled with in Tidyverse that make data preparation an enjoyable experience. Well also look at code snippets for each package to help you get started.You can also check out my pick of the top eight useful R packages you should incorporate into your data science work.Tidyverse is a collection of essential R packages for data science. The packages under the tidyverse umbrella help us in performing and interacting with the data. There are a whole host of things you can do with your data, such as subsetting, transforming, visualizing, etc.Tidyverse was created by the great Hadley Wickham and his team with the aim of providing all these utilities to clean and work with data.Lets now look at some versatile Tidyverse libraries that the majority of data scientists use to manage and streamline their data workflows.Ready to explore the tidyverse? Go ahead and install it directly from within RStudio:Well be working on the food demand forecasting challenge in this article. I have taken a random 10% sample from the train file for faster computation. You can take the entire dataset if you want (and if your machine can support it!).Lets begin!dplyr is one of my all-time favorite packages. It is simply the most useful package in R for data manipulation. One of the greatest advantages of this package is you can use the pipe function %>%to combine different functions in R. From filtering to grouping the data, this package does it all.Here is the complete list of functions dplyr offers:Lets look at an example to understand how to use these different functions in R.Open up the food forecasting dataset we downloaded earlier. We have 2 other files apart from the training set. We can join them with our train file to add more features. Lets usedplyrand merge all the files. Again, Im just using 10% of the overall data to make the computation faster.Output:Note: We see a lot of NAs here. This is because we randomly chose samples from each of the three files and then merged them. If you use the whole dataset, you will not observe this amount of missing values.Next, lets use three dplyr functions simultaneously to summarise the data. Here, well select TYPE_A from the center_type variable and calculate the mean of the num_orders variable at this particular center:Here,%>% is called the piping operator. This comes in handy when we want to use one or more functions together.Output:Go ahead and try out the other functions. Trust me, they will completely change the way you do data preparation.The tidyr package complements dplyr perfectly. Itboosts the power ofdplyrfor data manipulation and pre-processing. Below is the list of functions tidyroffers:Lets see a quick example of how to use tidyr. Well unite two binary variables and create only one column for both:Output:Heres another example of how tidyr works:Output:We easily converted the factor variables into a table that can be swiftly interpreted without much pre-processing.Dealing with string variables is a tricky challenge. They can often trip up to our final analysis because we skipped over those variables initially thinking they wont affect our model. Thats a mistake.stringris my go-to package in R for such situations. It plays a big role in processing raw data into a cleaner and an easily understandable format. stringr contains a variety of functions that make working with string data really easy.Some basic functions that you can perform with the stringr package are:There are many more functions inside the stringr package. Lets look at a couple of functions:Output:Combine two strings:The forcats package is dedicated to dealing with categorical variables or factors. Anyone who has worked with categorical data knows what a nightmare they can be. forcats feels like a godsend.It is quite frustrating when a factor appears in a place where we least expect it. If were using the tibble format, we dont need to worry about this issue.The aim is to fill in those missing pieces so we can access the power of factors with minimum effort.Use the following example to experiment with factors in your data:Output:Source: effiasoft.comWe have plenty of ways to read data in R. So why use thereadrpackage? The readr package solves the problem of parsing a flat file into a tibble. This provides an improvement over the standard file importing methods and significantly improves the computation speed.You can easily read a .CSV file in the following way:Use this function and youll automatically see the difference in the time RStudio takes to read in huge data files.We work with dataframes in R. Its one of the first things we learn about R  convert your data into a dataframe before we can proceed with any sort of data science steps.Tibble is a type of dataframe in R. It truly stands out when were trying to detect anomalies in our dataset. How? Tibble does not change variable names or types. It certainly doesnt throw up errors when a variable does not exist or a value is missing.Along with the print() function, the Tibble package helps in easy handling of big datasets containing complex objects. Such features enable us to treat the inherent data issues early on, hence producing cleaner code and data.Notice how the data type is mentioned along with the column names. This is a very useful way to present data. Using the above example we can easily see how R gives a tibble output to the users:Output:The train file that we converted to the tibbleformat now gives us a more clear look at the data types and number of variables. Looks pretty neat and tidy, right?The purrr package in R provides a complete toolkit for enhancing Rs functional programming. We can use the functions provided by purrr to avoid many loops with just one line of code.Which function do you typically use to check the mean of every column in your data? Most data scientists using R tend to lean on the summary() function. It gives us the descriptive statistics for each column.An even better way to just deduce the mean value, without using any ugly loops, is to use the map function. Lets see how we can do that using our training set:Output:Im sure you must have heard of ggplot2. It is far and away from the best visualization package I have ever used. Data scientists universally love using ggplot2 to produce their charts and visualizations. Its such a useful and popular package that theyve integrated it into the Python language!There is so much we can do with this package. Whether its building box plots, density plots, violin plots, tile plots, time series plots  you name it and ggplot2 has a function for it.Lets see a few examples of how to create some really interactive plots with ggplot2 in R.num_orders is the target variable in our food forecasting dataset. Lets look at its distribution by generating a density chart:As you can see above, the dependent variable is right-skewed.Now, how about drawing up a violin plot? Its a nice alternative to boxplots for detecting outliers:Woah. There are plenty of outliers in our data. Dont you love how a simple visualization offers up so many insights?Next, plot a scatterplot to check the relationship between the checkout price and the base price:Interestingly, there seems to be a pretty strong linear relationship between the two variables. We can certainly dig deeper into this when were working on this challenge to understand how these variables affect our overall model building strategy.The power of visualization never ceases to amaze me.These packages are not included directly in the tidyverse bundle. So you wont be able to load them through the function library(tidyverse).Hence, I have provided the installation commands for each package in this section.Output:Output:Pretty awesome!Tidyverse is the most popular collection of R packages. Which isnt all that surprising given how useful and easy to use they are.Youre definitely missing out on saving time and making your work much more efficient if you arent using the Tidyverse packages.Have you used these R packages before? Are there any other packages you feel should be incorporated into Tidyverse? I want to hear hear your thoughts, feedback, and experience with Tidyverse. Let me know in the comments section below!And if you get stuck at any point while using these packages, Ill be happy to help you out.We have summarised the use of every package under tidyverse in this amazing cheatsheet, you can access it here.",https://www.analyticsvidhya.com/blog/2019/05/beginner-guide-tidyverse-most-powerful-collection-r-packages-data-science/
A Practical Introduction to Prescriptive Analytics (with Case Study in R),Learn everything about Analytics|Introduction|Table of Contents|What is Prescriptive Analytics?|Setting up our Problem Statement|Getting the Dataset for our Problem|Hypothesis Generation|Laying Down our Model Building Approach|Data Visualization and Data PreparationDescriptive Analytics|Prediction of Customer BehaviorPredictive Analytics|Recommendations to improve performancePrescriptive Analytics|Recommendations|End Notes,"Recommend rate plan migration as a proactive retention strategy|Proactive retention strategy for customers|About the Author|Share this:|Related Articles|A Beginners Guide to Tidyverse  The Most Powerful Collection of R Packages for Data Science|Extracting and Analyzing 1000 Basketball Games using Pandas and Chartify|
Pranov Mishra
|10 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article was submitted as part of Analytics Vidhyas Internship Challenge.What are the different branches of analytics? Most of us, when were starting out on our analytics journey, are taught that there are two types  descriptive analytics and predictive analytics. Theres actually a third branch which is often overlooked  prescriptive analytics.Prescriptive analytics is the most powerful branch among the three. Let me show you how with an example.Recently, a deadly cyclone hit Odisha, India, but thankfully most people had already been evacuated. The Odisha meteorological department had already predicted the arrival of the monstrous cyclone and made the life-saving decision to evacuate the potentially prone regions.Contrast that with 1999, when more than 10,000 people died because of a similar cyclone. They were caught unaware since there was no prediction about the coming storm. So what changed?The government of Odisha was a beneficiary of prescriptive analytics. They were able to utilize the services of the meteorological departments accurate prediction of cyclones  their path, strength, and timing. They used this to make decisions about when and what needs to be done to prevent any loss of life.So in this article, we will first understand what the term prescriptive analytics means. We will then solidify our learning by taking up a case study and implementing the branches of analytics -descriptive, predictive and prescriptive. Lets go!We can broadly classify analytics into three distinct segments  Descriptive, Predictive and Prescriptive Analytics. Lets take a look at each of these:The below image does a nice job of illustrating the components under the prescriptive analytics umbrella:Source: WikipediaIve found the best way of learning a topic is by practicing it. So, lets understand prescriptive analytics by taking up a case study and implementing each analytics segment we discussed above.The senior management in a telecom provider organization is worried about the rising customer attrition levels. Additionally, a recent independent survey has suggested that the industry as a whole will face increasing churn rates and decreasing ARPU (average revenue per unit). The effort to retain customers so far has been very reactive. Only when the customer calls to close their account is when we take action. Thats not a great strategy, is it? The management team is keen to take more proactive measures on this front.We as data scientists are tasked with analyzing their data, deriving insights, predicting the potential behavior of customers, and then recommending steps to improve performance.You can download the dataset from here. I have also provided the full code on my Github repository.There are three R files and you should use them in the below order:Generating a hypothesis is the key to unlocking any data science or analytics project. We should first list down what it is we are trying to achieve through our approach and then proceed from there.Customer churn is being driven by the below factors (according the the independent industry survey):We would like to test the same for our telecom provider. Typically, we encourage the company to come up with an exhaustive set of hypotheses so as not to leave out any variables or major points. However, well narrow our focus down to one for the scope of this article:Are the variables related to cost, billing, network, and service quality making a significant contribution towards a customers decision to stay with or leave the service provider?Now that we have the data set, the problem statement and the hypothesis to test, its time to get our hands dirty. Lets tear into the data and see what insights can be drawn.I have summarized my approach in the below illustration. Typically, any model building exercise will go through similar steps. Note that this is my approach  you can change things up and play around with the data on your end. For instance, we are removing variables with more than 30% missing values but you can take your own call on this.Heres the code to find the variables with more than 30% missing values:As you can see in the above illustration, we removed all variables with more than 30% missing values. Heres the summary of our dataset:We have reduced the number of variables from 82 to 69.Lets do a univariate, bivariate and multivariate analysis of various independent variables along with the target variable. This should give us an idea of the effects of churn. I have shared a few visualizations below. You can find the entire exploratory analysis on the GitHub repository.Lets start by drawing up three plots (output is below the code block):First, we will analyze the mean minutes of usage, revenue range, mean total monthly recurring charge and the mean number of dropped or blocked calls against the target variable  churn:Similarly, we shall analyze the mean number of dropped (failed) voice calls, the total number of calls over the life of the customer, the range of the number of outbound wireless to wireless voice calls and the mean number of call waiting against the churn variable:Lets change things up a bit. Well use the faceting functionality in the awesome ggplot2 package to plot the months of usage, credit class code, call drops and the number of days of current equipment against the churn variable:We will analyze the numeric variable separately to see if there are any features that have high degrees of collinearity. This is because the presence of collinear variables always reduces the models performance since they introduce bias into the model. We should handle the collinearity problem. Now, there are many ways of dealing with it, such as variable transformation and reduction using principal component analysis (PCA). I have removed the highly correlated variables:This is the part most of you will be familiar with  building models on the training data. Well build a number of models so we can compare their performance across the spectrum.It is generally a good practice to train multiple models starting from simple linear models to complex non-parametric and non-linear ones. The performance of models varies depending on how the dependent and independent variables are related. If the relationship is linear, the simpler models give good results (plus theyre easier to interpret).Alternatively, if the relationship is non-linear, complex models generally give better results. As the complexity of the model increases, the bias introduced by the model reduces and the variance increases. For our problem, we will build around ten models on the training set and validate them on unseen test data.The models well build are:Heres the code to the logistic regression model (you can try out the rest using the code provided in my GitHub repository):Below is a comparison of the evaluation of our models:Logistic regression seems to give the best result when compared with the other models. LG_26 is a logistic regression model with a threshold of 26%. Let me know if you improved on this score  I would love to hear your thoughts on how you approached this problem.And now comes the part weve been waiting for  prescriptive analytics! Lets see what recommendations we can come up with to improve the performance of our model.In the image below, weve listed the variables that have more than 50% probability of changing the decision of the customer for every 1 unit change in the respective independent variable. This insight was generated from the logistic regression model we saw above. That is essentially a relationship between the log of odds of the dependent variable with the independent variables.
So, if we calculate the exponential of coefficients of the dependent variable, we get the odds and from that, we get the probability (using formula Probability = Odds/(1+Odds)) of customer behavior changing for one unit change in the independent variable.The below image will give you a better idea of what Im talking about:Remember the hypothesis we generated using the independent survey earlier? This has also come out to be true. The below summary statistics from the logistic model proves that:Heres a quick summary of what we can conclude from our analysis:Lets pen down our recommendations based on what weve understood.Mou_Mean (minutes of usage) is one of the most highly significant variables. Hence, it makes sense to work towards proactively working with customers to increase their MOU so that they are retained for a longer period. Additionally,mouR_Factor is highly significant. This, remember, is a derived variable of mou_Range.Changes in MOU are also highly significant. Change_mF is a derived variable of change_mou.To complement the above, we also see that ovrmou_Mean is also a highly significant variable with an odds ratio of more than 1. The variable has a positive estimate of the coefficient indicating an increase in overage churn.It would help if our company is able to work with the customers. Based on their usage, we can migrate them to optimal plan rates to avoid overage charges.Identify customers who have the highest probability of churn and develop a proactive retention strategy for them. What if the budget is limited? Then the company can build a lift chart and optimize its retention efforts by reaching out to targeted customers:Here, with 30% of the total customer pool, the model accurately provides 33% of total potential churn candidates:The lift achieved will help us to reach out to churn candidates by targeting much fewer of the total customer pool with the company.Also notice how the first 30 deciles gives us the highest gain. This can give us around 33% of the customers who are likely to terminate the services. In simple words, the company selects 30% of the entire customer database which covers 33% of the people who are likely to leave. This is much better than randomly calling customers which would have given perhaps a 15% hit rate from all potential churn candidates.You can use the below code to test the model by identifying 20% of customers who need to be proactively worked with to prevent churn: They are the customers whose probability of churn is greater than 32.24% and less than 84.7%. The ModelBuilding.r code will help you with the logical flow of the above code block.Prescriptive analytics is a truly awesome thing if companies are able to utilize it properly. Its still under the radar as far as the three branches of analytics are concerned.But as we keep moving up in the hierarchy of analytics, prescriptive analytics is the most favored area as it can help organizations to plan and prepare as they can foresee the future with a fair degree of confidence.Prescriptive analytics seeks to determine the best solution or outcome among various choices. Just keep in mind that we cannot separate the three branches of analytics. We need to do descriptive and predictive before jumping into prescriptive.Pranov MishraPranov is aData Science enthusiast with about 11 years of professional experience in the Financial Servicesindustry.Pranov is working as a Vice President in a Multinational Bank and has exposure toStrategic Planning, Intelligent Automation, Data Science, Risk & Controls, Predictive Data Modelling, and People Management. He also mentors analytics (PGPBABI) studentsenrolled with Great Learning and Great Lakes.",https://www.analyticsvidhya.com/blog/2019/05/practical-introduction-prescriptive-analytics/
Extracting and Analyzing 1000 Basketball Games using Pandas and Chartify,Learn everything about Analytics|Introduction|The Approach well Take in this Project|Researching the Data Source|Ensuring Ethical Guidelines are being Followed|Inspecting the Website|Understanding the Different Data Fields|Designing our Database|Fetching and Filtering the Data|Analyzing the Data and Generating Reports|||Comparing two teams|||Biggest blowouts||Most points in one gameday||Most exciting games (Volume 1)|Most exciting games (Volume 2)||Prolific referees||End Notes,"Tools you should have for this article|Inspecting Requests|1. Iterating over the score pages|2. Collecting GameIds and storing them|3. Iterating over game data responses and parsing JSON|4. Saving the specified fields into the Database|5. Cleaning the data|Installing the required libraries|Overall reports about the dataset|Home court advantage|Scored points distribution|Game points by date|Biggest Comebacks|About the Author|Share this:|Related Articles|A Practical Introduction to Prescriptive Analytics (with Case Study in R)|A Guide to Understanding Convolutional Neural Networks (CNNs) using Visualization|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love descriptive statistics. Visualizing data and analyzing trends is one of the most exciting aspects of any data science project. But what if we dont have proper data? Or the data we have is just not sufficient to draw conclusions?Thats where (ethical) web scraping comes in handy. We can source all kinds of data from around the internet  tabular, images, videos, etc. We just need to know a few specific techniques to extract that data.Well focus on extracting data from the NBA.com website in this article. Im a huge basketball fan so I thought why not put my knowledge of web scraping and website creation into sports analysis?Youll find this article useful even if youre not an NBA or sports fan. You will get an overall picture of how to gather, store and analyze public and unstructured data and how to go about planning and implementing a web data science project. Whether you want to learn how to do data analysis or youre interested in sports statistics, you will enjoy the next few minutes for sure.Were going to focus on descriptive statistics because thats always a key element of any data science project.The tools we will work with throughout this article are:Weve already seen this in the article heading! We are going to use the official NBA Stats site as a data source. Im a regular user of that site  it contains a treasure trove of data for NBA fans (especially us data science folks). Additionally, the site is superbly formatted, which makes it ideal for scraping.But we are not going to scrape it. Trust me, theres an easier and better way to reach the data were looking for which I will describe later in the article. We need to first ensure we are not breaking any protocol.We need to make sure that we can use the chosen website ethically as our data source. Why? Because we want to be good website citizens and dont want to do anything that hampers or messes the websites servers. How do we do this? Answering the below questions should help:We are good to go as our goals align with these ethical guidelines.As a side note, I encourage you to answer these questions before you scrape any website. Our approach should not breach or mess up other peoples work.This is the part where a little knowledge about HTTP and how websites work helps you save a ton of hours. Im not thoroughly familiar with the site we are trying to get data from, so I need to properly inspect it first to see whats going on. This is the starting page we get if we open stats.nba.com:We get a lot of player stats here which we can put aside for now. We want to get data about games  not specific players or teams. Hence, we need to find a page where games and results are displayed.Moving on to the scores page:Its getting better  here the full game results and quarter-by-quarter points are displayed. But we still do not have enough details to build a sufficient dataset. What were really looking for is this page:This is the game page. One game per page, in full detail. Here, we can find a bunch of different data fields. This is a good base for building our future database.Now that weve identified where we need to go, its time to do some real technical inspection. Were going to figure out what happens in the background when we request this specific page. In order to do that (Im using FireFox but should be the same or similar for other browsers): Now, we should see all the requests that have been made in the background:The site made close to 83 requests just displaying one page! Now, we have to filter out the ones that arent relevant for us and see only the data requests. To do that, toggle the XHR button inside the network tab on the top-right:We mostly see requests that have got JSON response after toggling the XHR button. Thats good for us because JSON is a popular format to transfer data from the backend to the frontend. Theres a high chance well find our data inside one of these JSON responses. Going through some of the JSON endpoints, I found the one which contains the kind of data we are after.This URL returns a JSON which contains all the data points about a game.  Thats why I said earlier that properly inspecting a website before writing a web scraper can save you a ton of hours. Theres already an API we can use so we dont need to do web scraping to collect data.Now, the URL request needs one parameter GameID. Note that each game has a unique GameID. So we have to find a way to collect these Game IDs as well.Earlier, we were looking at the scores page. This page has each game and a unique GameID for the given day. One possible solution, which we will implement, is to iterate over each day (from the scores pages), collect all GameIDs and then insert these IDs into a database.We will go through these GameIDs and parse JSONs containing game details. Now figure out what data fields we want to collect from the JSON:A basketball game has many kinds of data. This data is about teams, points, players, etc.  there are so many numbers and stats we could collect, its mind-blowing! We will narrow our scope to some specific fields for this project:One record stores data about one game.Generally, when designing a database, the tables and their normalization always depends on the kind of insights we want to gain from the project. For example, you could calculate the winner by looking at the points scored by both teams. Whichever teams got more points is the winner. But in our case, Im creating a separate column for the winner. Because I feel like its not gonna be a problem for us to have a somewhat redundant field, like this, stored.With that said, I dont create a separate column for the points a team scored in the whole game. I just store the quarterly points by both teams. If we will need to know this data well need to always sum up the quarterly points by one team. I think thats not a big sacrifice considering that this way we can analyze specifically the quarters of each game.We will follow the below steps for fetching and filtering our data:Lets understand each step in a bit more detail.`Inspecting even one score page gives us a hint that this page uses a JSON file to get data as well. An example URL of this kind of request:https://stats.nba.com/stats/scoreboardV2?DayOffset=0&LeagueID=00&gameDate=03/03/2019Again, rather than scraping data from the page, we use this endpoint to get GameIDs.We collect GameIDs from the JSON:In this code, data is the parsed JSON we requested in the previous step. Were collecting the GameIDs in a list called game_ids.Storing this in a database:In this step, were using the previously collected GameIDs:After storing data about each game played this season, I recognized some outliers in the dataset. I removed the NBA All-Star game from the database because it was a huge outlier with regards to the points total.It shouldnt be lumped together with the regular season games.I also had to remove some games that were played in the preseason in early October. So now, we only have regular season data.Finally, the fun part: querying the database to generate insightful reports and interesting stats. But first, we need to figure out what reports we want to create:These are ad-hoc reports that might be interesting to go through. There are a bunch of other ways to analyze this dataset  I encourage you to come up with more advanced dashboards.Before we start generating reports, we need to install some libraries were going to use. First, install pandas to handle data tables:Next, instead of matplotlib, were going to use a relatively new but easy-to-use plotting library called chartify:As a warm-up for our data visualization journey, lets start off with some simple descriptive reports about our fresh dataset:Now lets jump into the real stuff. Well generate a pie chart which tells us if theres any home court advantage, aka, is there more chance to win if the team plays at home, based on statistics?(Chartify doesnt yet support pie charts so were using the pandas wrapper function for this task, which is essentially matplotlib.)Interesting. Similar to soccer, NBA teams also have a reasonable advantage of playing at home. The home team won 57% of the games. Considering only regulation time results, home wins: 511, away wins: 338, OT: 47.Lets talk about points. Well use the chartify library from here on. Generate a distribution chart of the scored points per game:The majority of the games are in the 200-240 range point-wise. That is 100-120 points per team per game. Theres a huge drop in the number of games that are outside of this range.Now Im interested to see if theres any correlation between the date of a game and the number of points scored. For example in soccer, teams score more goals when the season is ending soon.It seems the date of the game doesnt make any difference to the number of points scored. At least not at a high-level.See that gap on the right side of our plot? It seems to be falling somewhere in mid-February. As it turns out, no games were played between Feb 15-20. This was the time for the all-star game which we intentionally excluded from our database earlier. Incredible what a simple visualization can reveal, right?Its always a fun exercise comparing teams to see how they are doing relative to each other.For our study, I chose a high performing team and an underperformer:These two teams have pretty different point distributions. For Cleveland, its very rare to reach 120 points in a game. They usually score between 90 and 110. For Milwaukee, they are usually on the edge or over 120 points. Based on this chart, its not surprising to learn that Bucks are the 1st in their conference while the Cavaliers are second-to-last. It would be interesting to see this chart with Kyrie and Lebron back in the team, but thats for another time!We want to see some comebacks. Who doesnt love a rip-roaring comeback by a team most consider to be out of the game? Well take the cases where a team was down in the first half by a lot but managed to win the game:The biggest 1st half deficit that one team was able to overcome was 22 points. The winning team scored 70 points in a half in 4 out of these 5 matches.I want to point out the defensive performance of the Denver Nuggets against Memphis Grizzlies. They restricted the Grizzlies to 32 points in the entire 2nd half. They must have figured something out in the defense in the break.Its this kind of analysis that I love doing through visualizations!If we see the biggest comebacks, we need to check out the biggest blowouts as well. Blowouts are essentials games where one team won by a handsome margin:The biggest blowout was between the Celtics and the Bulls. Boston won 133-77, a ridiculous 56 points win. The surprising thing is that the game was played in Chicago, so Boston was actually the visiting team. Utah Jazz only scored 68 points which are 17 per quarter per team on average. Thats way below the league average for quarterly points per team (28).Now lets look at things from a different angle. Which gamedays saw teams scoring points that were way above the league average?Keep in mind that the average points in an NBA game are 220. So the five days were seeing in the above table truly exceeded that average. February 23 is also in this list averaging 235.5 points per game which is outstanding considering that there were 12 games that day.This might be subjective according to what each of us consider exciting. For the purpose of this article, we will take the number of lead changes during a game. You can set your own metric and generate a new report as well.There were 32 lead changes in the Golden State Warriors v Utah Jazz game! The lead changed every 1.5 minutes on average  that sounds like a pulsating affair. Eventually, GSW won the game 124-123. Weve got two San Antonio Spurs games on the list, maybe the Spurs tend to play give-and-take type of games more often than others?Another way to statistically define exciting games would be based on the number of ties during a game. Interestingly, we get totally different matchups in the top 5 compared to the previous list. 3 of the 5 games went into overtime. There were 26 ties during the Suns v Wizards game which means one team tied the game every 108 seconds on average.Yes, we will look at a few essential referee stats as well. Love them or hate them, they are a huge part of the game.The number of referees in the league (who officiated any games): 68.Most prolific referees: Karl Lane, Tyler Ford, Pat Fraher, Scott Foster, and Josh Tiven. Each of them officiated 48 games. There were 124 game days in our dataset. That means you cannot watch 3 game days in a row without any of them being on or near the court. Impressive!This article is intended to inspire you on how to make use of web data or other kinds of data. There are more and more tools available that you can use to draw insights from public data. I hope this walkthrough gives you some ideas about how to make data work for you.You can also use this analysis to build machine learning models. Weve done the data cleaning and exploration part  take it forward and use your favorite algorithms to predict a teams chances of winning. The possibilities are endless.If you have any questions or suggestions, feel free to leave them in the comments section below. Thanks for reading!Attila TthAttila is the Founder of ScrapingAuthority.com where he teaches web scraping and data engineering. He has expertise in designing and implementing web data extraction and processing solutions. You can check out his YouTube channel here.",https://www.analyticsvidhya.com/blog/2019/05/scraping-nba-data-analyze-1000-basketball-games-python/
A Guide to Understanding Convolutional Neural Networks (CNNs) using Visualization,Learn everything about Analytics|Introduction|Table Of Contents|Why Should we use Visualization to Decode Neural Networks?|Setting up the Model Architecture|Accessing Individual Layers|Visualizing the Building Blocks of CNNs  Filters|Visualizing what a Model Expects  Activation Maximization|Visualizing whats Important in the Input- Occlusion Maps|Visualizing the Contribution of Input Features- Saliency Maps|Class Activation Maps (Gradient Weighted)|Visualizing the Process  Layerwise Output Visualization|End Notes,"Share this:|Related Articles|Extracting and Analyzing 1000 Basketball Games using Pandas and Chartify|Statistics for Data Science: Introduction to the Central Limit Theorem (with implementation in R)|
saurabh pal
|6 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How did your neural network produce this result? This question has sent many data scientists into a tizzy. Its easy to explain how a simple neural network works, but what happens when you increase the layers 1000x in a computer vision project?Our clients or end users require interpretability  they want to know how our model got to the final result.We cant take a pen and paper to explain how a deep neural network works. So how do we shed this black box image of neural networks?By visualizing them! The clarity that comes with visualizing the different features of a neural network is unparalleled. This is especially true when were dealing with a convolutional neural network (CNN) trained on thousands and millions of images.In this article, we will look at different techniques for visualizing convolutional neural networks. Additionally, we will also work on extracting insights from these visualizations for tuning our CNN model.Note: This article assumes you have a basic understanding of Neural Networks and Convolutional Neural Networks. Below are three helpful articles to brush up or get started with this topic:Its a fair question. There are a number of ways to understand how a neural network works, so why turn to the off-beaten path of visualization?Lets answer this question through an example. Consider a project where we need to classify images of animals, like snow leopards and Arabian leopards. Intuitively, we can differentiate between these animals using the image background, right?Both animals live in starkly contrasting habitats. The majority of the snow leopard images will have snow in the background while most of the Arabian leopard images will have a sprawling desert.Heres the problem the model will start classifying snow versus desert images. So, how do we make sure our model has correctly learned the distinguishing features between these two leopard types? The answer lies in the form of visualization.Visualization helps us see what features are guiding the models decision for classifying an image.There are multiple ways to visualize a model, and we will try to implement some of them in this article.I believe the best way of learning is by coding the concept. Hence, this is a very hands-on guide and Im going to dive into the Python code straight away.We will be using the VGG16 architecture with pretrained weights on the ImageNet dataset in this article. Lets first import the model into our program and understand its architecture.We will visualize the model architecture using themodel.summary() functionin Keras. This is a very important step before we get to the model building part. We need to make sure the input and output shapes match our problem statement, hence we visualize the model summary.Below is the model summary generated by the above code:We have a detailed architecture of the model along with the number of trainable parameters at every layer. I want you to spend a few moments going through the above output to understand what we have at hand.This is important when we are training only a subset of the model layers (feature extraction). We can generate the model summary and ensure that the number of non-trainable parameters matches the layers that we do not want to train.Also, we can use the total number of trainable parameters to check whether our GPU will be able to allocate sufficient memory for training the model. Thats a familiar challenge for most of us working on our personal machines!Now that we know how to get the overall architecture of a model, lets dive deeper and try to explore individual layers.Its actually fairly easy to access the individual layers of a Keras model and extract the parameters associated with each layer. This includes the layer weights and other information like the number of filters.Now, we will create dictionaries that map the layer name to its corresponding characteristics and layer weights:The above code gives the following output which consists of different parameters of the block5_conv1 layer:Did you notice that the trainable parameter for our layer block5_conv1 is true? This means that we can update the layer weights by training the model further.Filters are the basic building blocks of any Convolutional Neural Network. Different filters extract different kinds of features from an image. The below GIF illustrates this point really well:As you can see, every convolutional layer is composed of multiple filters.Check out the output we generated in the previous section  the block5_conv1 layer consists of 512 filters. Makes sense, right?Lets plot the first filter of the first convolutional layer of every VGG16 block:We can see the filters of different layers in the above output. All the filters are of the same shape since VGG16 uses only 33 filters.Lets use the image below to understand the concept of activation maximization:Which features do you feel will be important for the model to identify the elephant? Some major ones I can think of:Thats how we instinctively identify elephants, right? Now, lets see what we get when we try to optimize a random image to be classified as that of an elephant.We know that every convolutional layer in a CNN looks for similar patterns in the output of the previous layer. The activation of a convolutional layer is maximized when the input consists of the pattern that it is looking for.In the activation maximization technique, we update the input to each layer so that the activation maximization loss is minimized.How do we do this? We calculate the gradient of the activation loss with respect to the input, and then update the input accordingly:Heres the code for doing this:Our model generated the below output using a random input for the class corresponding to Indian Elephant:From the above image, we can observe that the model expects structures like a tusk, large eyes, and trunk. Now, this information is very important for us to check the sanity of our dataset. For example, lets say that the model was focussing on features like trees or long grass in the background because Indian elephants are generally found in such habitats.Then, using activation maximization, we can figure out that our dataset is probably not sufficient for the task and we need to add images of elephants in different habitats to our training set.Activation maximization is used to visualize what the model expects in an image. Occlusion maps, on the other hand, help us find out which part of the image is important for the model.Now, to understand how occlusion maps work, we consider a model that classifies cars according to their manufacturers, like Toyota, Audi etc.:Can you figure out which company manufactured the above car? Probably not because the part where the company logo is placed has been occluded in the image. That part of the image is clearly important for our classification purposes.Similarly, for generating an occlusion map, we occlude some part of the image and then calculate its probability of belonging to a class. If the probability decreases, then it means that occluded part of the image is important for the class. Otherwise, it is not important.Here, we assign the probability as pixel values for every part of the image and then standardize them to generate a heatmap:The above code defines a function iter_occlusion that returns an image with different masked portions.Now, lets import the image and plot it:Original ImageNow, well follow three steps:HeatmapReally interesting. We will now create a mask using the standardized heatmap probabilities and plot it:MaskFinally, we will impose the mask on our input image and plot that as well:Masked CarCan you guess why were seeing only certain parts? Thats right  only those parts of the input image that had a significant contribution to its output class probability are visible. That, in a nutshell, is what occlusion maps are all about.Saliency maps are another visualization technique based on gradients. These maps were introduced in the paper Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.Saliency maps calculate the effect of every pixel on the output of the model. This involves calculating the gradient of the output with respect to every pixel of the input image.This tells us how to output category changes with respect to small changes in the input image pixels. All the positive values of gradients mean that small changes to the pixel value will increase the output value:These gradients, which are of the same shape as the image (gradient is calculated with respect to every pixel), provide us with the intuition of attention.Lets see how to generate saliency maps for any image. First, we will read the input image using the below code segment.Input ImageNow, we will generate the saliency map for the image using the VGG16 model:We see that the model focuses more on the facial part of the dog. Now, lets look at the results with guided backpropagation:Guided backpropogation truncates all the negative gradients to 0, which means that only the pixels which have a positive influence on the class probability are updated.Class activation maps are also a neural network visualization technique based on the idea of weighing the activation maps according to their gradients or their contribution to the output.The following excerpt from the Grad-CAM paper gives the gist of the technique:Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for dog or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept.In essence, we take the feature map of the final convolutional layer and weigh (multiply) every filter with the gradient of the output with respect to the feature map. Grad-CAM involves the following steps:We can see the input image and its corresponding Class Activation Map below:Now lets generate the Class activation map for the above image.The starting layers of a CNN generally look for low-level features like edges. The features change as we go deeper into the model.Visualizing the output at different layers of the model helps us see what features of the image are highlighted at the respective layer. This step is particularly important to fine-tune an architecture for our problems. Why? Because we can see which layers give what kind of features and then decide which layers we want to use in our model.For example, visualizing layer outputs can help us compare the performance of different layers in the neural style transfer problem.Lets see how we can get the output at different layers of a VGG16 model:The above image shows the different features that are extracted from the image by every layer of VGG16 (except block 5). We can see that the starting layers correspond to low-level features like edges, whereas the later layers look at features like the roof, exhaust, etc. of the car.Visualization never ceases to amaze me. There are multiple ways to understand how a technique works, but visualizing it makes it a whole lot more fun. Here are a couple of resources you should check out:Let me know if you have any questions or feedback on this article. Ill be happy to get into a discussion!",https://www.analyticsvidhya.com/blog/2019/05/understanding-visualizing-neural-networks/
Statistics for Data Science: Introduction to the Central Limit Theorem (with implementation in R),Learn everything about Analytics|Introduction|Table of Contents|What is the Central Limit Theorem (CLT)?|Significance of the Central Limit Theorem|Assumptions Behind the Central Limit Theorem|Implementing the Central Limit Theorem in R|End Notes,"Formally Defining the Central Limit Theorem|Statistical Significance of CLT|Practical Applications of CLT|Understanding the Problem Statement|Solution Methodology|Import the CSV Dataset and Validate it|Share this:|Related Articles|A Guide to Understanding Convolutional Neural Networks (CNNs) using Visualization|Dont Miss these 5 Data Science GitHub Projects and Reddit Discussions (April Edition)|
Analytics Vidhya Content Team
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"What is one of the most important and core concepts of statistics that enables us to do predictive modeling, and yet it often confuses aspiring data scientists? Yes, Im talking about the central limit theorem.It is a powerful statistical concept that every data scientist MUST know. Now, why is that?Well, the central limit theorem (CLT) is at the heart of hypothesis testing  a critical component of the data science lifecycle.Thats right, the idea that lets us explore the vast possibilities of the data we are given springs from CLT. Its actually a simple notion to understand, yet most data scientists flounder at this question during interviews.We will understand the concept of Central Limit Theorem (CLT) in this article. Well see why its important, where its used and then learn how to apply it in R.I recommend going through the below article if you need a quick refresher on distribution and its various types: Lets understand the central limit theorem with the help of an example. This will help you intuitively grasp how CLT works underneath.Consider that there are 15 sections in the science department of a university and each section hosts around 100 students. Our task is to calculate the average weight of students in the science department. Sounds simple, right?The approach I get from aspiring data scientists is to simply calculate the average:But what if the size of the data is humongous? Does this approach make sense? Not really  measuring the weight of all the students will be a very tiresome and long process. So, what can we do instead? Lets look at an alternate approach.Source: http://www.123rf.comThis, in a nutshell, is what the central limit theorem is all about. If you take your learning through videos, check out the below introduction to the central limit theorem. This is part of the comprehensive statistics module in the Introduction to Data Science course:Lets put a formal definition to CLT:Given a dataset with unknown distribution (it could be uniform, binomial or completely random), the sample means will approximate the normal distribution.These samples should be sufficient in size. The distribution of sample means, calculated from repeated sampling, will tend to normality as the size of your samples gets larger.Source: corporatefinanceinstitute.comThe central limit theorem has a wide variety of applications in many fields. Let us look at them in the next section.The central limit theorem has both statistical significance as well as practical applications. Isnt that the sweet spot we aim for when were learning a new concept?Well look at both aspects to gauge where we can use them.Source: http://srjcstaff.santarosa.eduSource: projects.fivethirtyeight.comThe central limit theorem has many applications in different fields. Can you think of more examples? Let me know in the comments section below the article  I will include them here.Before we dive into the implementation of the central limit theorem, its important to understand the assumptions behind this technique:In general, a sample size of 30 is considered sufficient when the population is symmetric.The mean of the sample means is denoted as: X = where,And, the standard deviation of the sample mean is denoted as: X = /sqrt(n)where,And thats it for the concept behind central limit theorem. Time to fire up RStudio and dig into CLTs implementation!Excited to see how we can code the central limit theorem in R? Lets dig in then.A pipe manufacturing organization produces different kinds of pipes. We are given the monthly data of the wall thickness of certain types of pipes. You can download the data here.The organization wants to analyze the data by performing hypothesis testing and constructing confidence intervals to implement some strategies in the future.The challenge is that the distribution of the data is not normal.Note: This analysis works on a few assumptions and one of them is that the data should be normally distributed.The central limit theorem will help us get around the problem of this data where the population is not normal. Therefore, we will simulate the central limit theorem on the given dataset in R step-by-step. So, lets get started.First, import the CSV file in R and then validate the data for correctness:Output:Next,calculate the population mean and plot all the observations of the data:Output:See the red vertical line above? Thats the population mean. Wecan also see from the above plot that the population is not normal, right? Therefore, we need to draw sufficient samples of different sizes and compute their means (known as sample means). We will then plot those sample means to get a normal distribution.In our example, we will draw sufficient samples of size 10, calculate their means, and plot them in R. I know that the minimum sample size taken should be 30 but lets just see what happens when we draw 10:Now, we know that well get a very nice bell-shaped curve as the sample sizes increase. Let us now increase our sample size and see what we get:Here, we get a good bell-shaped curve and the sampling distribution approaches normal distribution as the sample sizes increase. Therefore, we can consider the sampling distributions as normal and the pipe manufacturing organization can use these distributions for further analysis.You can also play around by taking different sample sizes and drawing a different number of samples. Let me know how it works out for you!Central limit theorem is quite an important concept in statistics, and consequently data science. I cannot stress enough on how critical it is that you brush up on your statistics knowledge before getting into data science or even sitting for a data science interview.I recommend taking the Introduction to Data Science course  its a comprehensive look at statistics before introducing data science.If you have any doubts or feedback, do let me know in the comments section below.",https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/
Dont Miss these 5 Data Science GitHub Projects and Reddit Discussions (April Edition),Learn everything about Analytics|Introduction|Data Science GitHub Repositories|Sparse Transformer by OpenAI  A Superb NLP Framework|OpenAIs GPT-2 in a Few Lines of Code|NeuronBlocks  Impressive NLP Deep Learning Toolkit by Microsoft|CenterNet  Computer Vision using Center Point Detection|BentoML  Toolkit for Deploying Models!|Data Science Reddit Discussions|What Role do Tools like Tableau and Alteryx Play in a Data Science Organization?|Lessons Learned During Move from Masters Degree to the Industry|When ML and Data Science are the Death of a Good Company: A Cautionary Tale|Have we hit the Limits of Deep Reinforcement Learning?|What do Data Scientists do on a Day-to-Day Basis?|End Notes,"Share this:|Related Articles|Statistics for Data Science: Introduction to the Central Limit Theorem (with implementation in R)|Last Chance to Register on our Flagship Program  AI & ML BlackBelt|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Data science is an ever-evolving field. As data scientists, we need to have our finger on the pulse of the latest algorithms and frameworks coming up in the community.Ive found GitHub to be an excellent source of knowledge in that regard. The platform helps me stay current with trending data science topics. I can also look up and download code from leading data scientists and companies  what more could a data scientist ask for? So, if youre a:or any mix of the above, this article is for you. Ive taken away the pain of having to browse through multiple repositories by picking the top data science ones here. This months collection has a heavy emphasis on Natural Language Processing (NLP).I have also picked out five in-depth data science-related Reddit discussions for you. Picking the brains of data science experts is a rare opportunity, but Reddit allows us to dive into their thought process. I strongly recommend going through these discussions to improve your knowledge and industry understanding.Want to check out the top repositories from the first three months of 2019? Weve got you covered:Lets get into it!What a year this is turning out to be for OpenAIs NLP research. They captured our attention with the release of GPT-2 in February (more on that later) and have now come up with an NLP framework that builds on top of the popular Transformer architecture.The Sparse Transformer is a deep neural network that predicts the next item in a sequence. This includes text, images and even audio! The initial results have been record-breaking. The algorithm uses the attention mechanism (quite popular in deep learning) to extract patterns from sequences 30 times longer than what was previously possible.Got your attention, didnt it? This repository contains the sparse attention components of this framework. You can clone or download the repository and start working on an NLP sequence prediction problem right now. Just make sure you use Google Colab and the free GPU they offer.Read more about the Sparse Transformer on the below links:Ah yes. OpenAIs GPT-2. I havent seen such hype around a data science library release before. They only released a very small sample of their original model (owing to fear of malicious misuse), but even that mini version of the algorithm has shown us how powerful GPT-2 is for NLP tasks.There have been many attempts to replicate GPT-2s approach but most of them are too complex or long-winded. Thats why this repository caught my eye. Its a simple Python package that allows us to retrain GPT-2s text-generating model on any unseen text. Check out the below-generated text using the gpt2.generate() command:You can install gpt-2-simple directly via pip (youll also need TensorFlow installed):Another NLP entry this month. It just goes to show the mind-boggling pace at which advancements in NLP are happening right now.NeuronBlocks is an NLP toolkit developed by Microsoft that helps data science teams build end-to-end pipelines for neural networks. The idea behind NeuronBlocks is to reduce the cost it takes to build deep neural network models for NLP tasks.There are two major components that makeup NeuronBlocks (use the above image as a reference):You know how costly applying deep learning solutions can get. So make sure you check out NeuronBlocks and see if it works for you or your organization. The full paper describing NeuronBlocks can be read here.I really like this approach to object detection. Generally, detection algorithms identify objects as axis-aligned boxes in the given image. These methods look at multiple object points and locations and classify each. This sounds fair  thats how everyone does it, right?Well, this approach, called CenterNet, models an object as a single point. Basically, it identifies the central point of any bounding box using keypoint estimation. CenterNet has proven to be much faster and more accurate than the bounding box techniques we are familiar with.Try it out next time youre working on an object detection problem  youll love it! You can read the paper explaining CenterNet here.Understanding and learning how to deploy machine learning models is a MUST for any data scientist. In fact, more and more recruiters are starting to ask deployment-related questions during data scientist interviews. If you dont know what it is, you need to brush up right now.BentoML is a Python library that helps you package and deploy machine learning models. You can take your model from your notebook to the production API service in 5 minutes (approximately!). The BentoML service can easily be deployed with your favorite platforms, such as Kubernetes, Docker, Airflow, AWS, Azure, etc.Its a flexible library. It supports popular frameworks like TensorFlow, PyTorch, Sci-kit Learn, XGBoost, etc. You can even deploy custom frameworks using BentoML. Sounds like too good an opportunity to pass up!This GitHub repository contains the code to get you started, plus installation instructions and a couple of examples.Are you working in a Business Intelligence/MIS/Reporting role? Do you often find yourself working with drag-and-drop tools like Tableau, Alteryx, Power BI? If youre reading this article, Im assuming you are interested in transitioning to data science.This discussion thread, started by a slightly frustrated data analyst, dives into the role a data analyst can play in a data science project. The discussion focuses on the skills a data analyst/BI professional needs to pick up to stand any chance of switching to data science.Hint: Learning how to code well is the #1 advice.Also, check out our comprehensive and example-filled article on the 11 steps you should follow to transition into data science.Source: jobs.ieThe biggest gripe hiring data science managers have is the lack of industry experience candidates bring. Bridging the gap between academia and industry has proven to be elusive for most data science enthusiasts. MOOCs, books, articles  all of these are excellent sources of knowledge  but they dont provide industry exposure.This discussion, starting from the authors post, is gold fodder for us. I like that the author has posted an exhaustive description of his interview experience. The comments include on-point questions that probe out more information on this transition.The consensus these days is you can use machine learning and artificial intelligence to improve your organizations bottom line. Thats what management feed leadership and that brings in investment.But what happens when management doesnt know how to build AI and ML solutions? And doesnt invest in first setting up the infrastructure before even thinking about machine learning? That part is often overlooked during discussions and is often fatal to a company.This discussion is about how a company, chugging along using older programming languages and tools, suddenly decides to replace its old architecture with flashy data science scripts and tools. A cautionary tale and one you should pay heed to as you enter this industry.Ive seen this question being asked on multiple forums recently. Its an understandable thought. Apart from a few breakthroughs by a tech giant every few months, we havent seen a lot of progress in deep reinforcement learning.But is this true? Is this really the limit? Weve barely started to scratch the surface and are we already done? Most of us believe theres a lot more to come. This discussion hits the right point between the technical aspect and the overall grand scheme of things.You can apply the lessons learned from this discussion to deep learning as well. Youll see the similarities when the talk turns to deep neural networks.Ever wondered what a data scientist spends most of their day on? Most aspiring professionals think theyll be building model after model. Thats a trap you need to avoid at any cost.I like the first comment in this discussion. The person equates being a data scientist to being a lawyer. That is, there are different kinds of roles depending on which domain youre in. So theres no straight answer to this question.The other comments offer a nice perspective of what data scientists are doing these days. In short, theres a broad range of tasks that will depend entirely on what kind of project you have and the size of your team. Theres some well-intentioned sarcasm as well  I always enjoy that!I loved putting together this months edition given the sheer scope of topics we have covered. Where computer vision techniques have hit a ceiling (relatively speaking), NLP continues to break through barricades. Sparse Transformer by OpenAI seems like a great NLP project to try out next.What did you think of this months collection? Any data science libraries or discussions I missed out on? Hit me up in the comments section below and lets discuss!",https://www.analyticsvidhya.com/blog/2019/05/5-data-science-github-reddit-april/
Last Chance to Register on our Flagship Program  AI & ML BlackBelt,Learn everything about Analytics|Introduction|Philosophy Behind Certified Program  AI & ML Blackbelt (Beginners to Masters)|What all does Certified Program  AI & ML Blackbelt (Beginners to Masters) cover?|Benefits of Certified Program  AI & ML Blackbelt (Beginners to Masters)|Last Chance to Enroll,"Share this:|Related Articles|Dont Miss these 5 Data Science GitHub Projects and Reddit Discussions (April Edition)|Winning Solutions and Approaches from the Machine Learning Hikeathon: Feature Engineering Special!|
Kunal Jain
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"In the last 6 years  we have helped millions of users in learning data science, we helped hundreds of companies across the globe hire top data science talent and we conducted the biggest data science conference for practitioners in India.Over the last 9 months  we took all our experience in helping people learn and helping companies hire talent, and created our flagship program out of it  Certified AI & ML Blackbelt (Beginners to Masters).The program was launched last month and we have received raving feedback from the community. This program comes with 10+ courses, more than 20 projects and one-on-one mentorship for 18 months. There are nopre-requisites to take this course. Everything required to learn data science, machine learning, NLP and Computer Vision is included in the program.Since this is a very intensive program  we will only be offering limited seats. The enrolments for this certified program will start on 31st August 2019 and close on September 3rd, 2019. So, if you are interested in joining the program  this is your last chance!For people new to this, let me share a few details about this program and how we built it.We have been creating short duration courses for almost a year now. When we analyzed the response from our users and our community  one thing stood out  Our users loved the courses and wanted more from us.Quite a few users bought multiple courses from Analytics Vidhya. This prompted us to think of creating a comprehensive end-to-end program for our users. The idea was to start from the absolute basics  Basics of Python, Statistics and anything else you need to make you industry-ready by the end of this program.By the time you finish this comprehensive program, you should have worked on cutting-edge machine learning problems in Natural Language Processing (NLP) and Computer Vision. That is the precise idea with which we created the BlackBelt program.In addition  we wanted to make sure we provide all the support to our users. So, along with our mail support and live Q&A sessions, the BlackBelt program comes with one-on-one mentorship calls. Once you sign up for the course  we make sure that we are there whenever you need us.Heres the list of all the courses in the AI & ML BlackBelt program.Start by building your base for Data Science:Learn Visualization and Manipulation of Data:The mandatory SQL course:Advanced Machine Learning:Interview Preparation:And as I mentioned, we have 2 niche, in-depth specialization courses as well:Below is an awesome graphic illustrating the roadmap you should take:As mentioned above, we will only be offering limited seats on this program. The enrolments will close on September 3rd 2019. So, if you are interested in joining the program  this would be the last chance for now. If you are committed to building a career in data science  I will see you on the other side of the portal. ",https://www.analyticsvidhya.com/blog/2019/05/last-chance-register-course-ai-ml-blackbelt/
Winning Solutions and Approaches from the Machine Learning Hikeathon: Feature Engineering Special!,Learn everything about Analytics|Introduction|About the ML Hikeathon|Understanding the ML Hikeathon Problem Statement|Datasets Provided for the Hikeathon|Winners & their Approaches|Rank 3:Sourabh Jha and Datageek|Rank 2: Tezdhar|Rank 1: Kamakal Vectors|Key Learnings or Takeaways|End Notes,"Share this:|Related Articles|Last Chance to Register on our Flagship Program  AI & ML BlackBelt|8 Awesome Data Science Capstone Projects from Praxis Business School|
Analytics Vidhya Content Team
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"90-95% of the times we find ourselves working with tabular data in machine learning. That number jumps up even more in machine learning hackathons. Can you remember the last time you worked on a challenge that had you thinking  I havent seen this kind of data before!Working with graph data is a unique challenge. Thats why we were thrilled to host the ML Hikeathon in partnership with Hike last month. And our community loved it too  more than 5300 data scientists from all over the world participated in the 9-day event!There was a lot to appreciate and gain by participating in this hackathon, including:If you didnt participate in the ML Hikeathon, you missed out on a lot of fun! But dont worry  head over to our DataHack platform and enrol in upcoming hackathons and practice problems today.The ML Hikeathon was a marathon competition spanning a full 9 days! The hackathon went live on the midnight of March 30th, 2019 and closed on 7th April 2019.Given the nature of the problem statement, it would definitely have taken data scientists some time to understand the requirements and frame their solution. Thats what makes a 9-day hackathon so unique  you get time to think through and experiment with your approach  but the level of the challenge goes up several notches.Here, I have to tip my hat to the data scientists who came up with the top solutions. The creativity and knowledge displayed by these data scientists were sublime.Hike is a popular social platform.Predicting links in their network forms the basis for recommending new friends to their users. And in turn, high-quality recommendations help in the creation of new social connections between existing users.So, the problem statement involved building a model for Link Prediction on Hikes Social Network.This link prediction modelwould increase the retention of new users by helping them find friends as they join the platform. In other words, the aim was to develop an algorithm that can predict whether a Hike user will chat up another Hike user who is a part of his/her phone contact list.The evaluation metric used was Area under the Curve (AUC).The data for this competition was a subset of Hikes social graph and the anonymized features of users. The participants were provided with three files:Alright  lets put our hands together for the winners! These winnersused different and unique approaches to rise up the leaderboard. Below are the top 3 winners on the hackathon leaderboard:Lets look at each of their winning solutions. We have penned down the approaches in the winners own words. Theres a lot to learn from them so take notes!We strongly recommend going through these approaches as they will help shape your mindset for future hackathons. Look at these approaches through a dataset-agnostic lens. Understand how a winner frames their thinking to approach a hackathon as tricky as this one.Feature engineering was the name of the game for Sourabh and Datageek.Validation strategy:Feature engineering:Graph-based features (Directed graph):Graph-based features (Undirected graph):Features using all the pairs:Some more feature ideas they shared were:Models:They used a larger computer machine with 64GB RAM and 16 cores to train due to the large size of the data.Validation strategy:Feature engineering:Models: This provided him a score of 0.938988506 on the private leaderboard. A brilliant result!Heres a brief overview of their approach:Validation strategy:Feature engineering:Feature creation played a very important role in our climb up the leaderboard. We used three major sets of features in our final model:Graph Features:
We created three graphs from the data set:Some of the metrics used include:User Social Circle FeaturesFinal ModelThis provided them with the first rank and an AUC score of 0.940911. Congratulations again to our winners!Below are two key takeaways from the ML Hikeathon:It was great interacting with these winners andunderstanding their approach during thecompetition. We see a heavy emphasis on feature engineering again. Its no coincidence that creating new features is often the difference between several positions in the leaderboard.Hopefully, you will be able to evaluate what you missed out. Check out all the upcoming competitionshere.Participate in them and improve your knowledge as well as expertise in the field. See you in the next hackathon!",https://www.analyticsvidhya.com/blog/2019/04/ml-hikeathon-winning-solution-approaches/
8 Awesome Data Science Capstone Projects from Praxis Business School,Learn everything about Analytics|Introduction|Program Details and Capstone Projects|How has the Program Evolved over the Years?|Capstone Projects by Current Passing out batch at Praxis Business School|Details of the Capstone Projects|How relevant were these projects for the Industry?|Key Takeaways from the day|End Notes,"How Much has the Program Evolved? In which Direction?|What are the key takeaways for the students undergoing the program?|Project 1  Detection of Spam Reviews|Project 2  Opinion Mining on Mobile Phone Features|Project 3  Drowsiness Detection using Computer Vision|Project 4  Gesture Recognition using Computer Vision|Project 5  Team Selection using Computer Vision|Project 6  Attendance Tracking System using Computer Vision|Project 7  Recommender System for Fashion Apparel|Project 8  Nearest Document Search|Share this:|Related Articles|Winning Solutions and Approaches from the Machine Learning Hikeathon: Feature Engineering Special!|Learn how to Build and Deploy a Chatbot in Minutes using Rasa (IPL Case Study!)|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"It is not the strongest or the most intelligent who will survive but those who can best manage change.Evolution is the only way anything can survive in this universe. And when it comes to industry relevant education in a fast evolving domain like Machine Learning and Artificial Intelligence  it is necessary to evolve or you will simply perish (over time).I have personally experienced this first hand while building Analytics Vidhya. It still amazes me to see where we started and where we are today. During this period, there have been several ups and downs, several product launches, product re-launches and what not! But one thing has been a constant in our story  constant evolution!So, when I got an invite to be a judge on the panel judging Capstone projects done by students of PGP in Data Science with ML & AI program at Praxis Business School, the same school where I had reviewed the program almost 4 years back  I was curious. I was curious to see and learn how their evolution had panned out.My interaction with the students four years ago was quite different from my experience sitting in a panel of judges for Capstone projects. You get to see the final outcome coming from a rigorous program as opposed to just having a classroom interaction. This is like the proof of the pudding!I was hoping to find out answers to 2 broad questions in the process:With those questions in mind  I boarded an early morning flight to Bengaluru and was in the Praxis campus by 9:00 a.m. Since the evaluations were supposed to start at 10:30 a.m., I had some time on my hand.I used this time to catch up with the course faculty Gourab Nath, and other judges of our esteemed panel  Suresh Bommu (Advanced Analytics Practice Head at Wipro Limited) and Rudrani Ghosh(Director at American Express Merchant Recommender and Signal Processing team).I also grabbed some authentic South Indian breakfast in the process. For people who are not aware  Praxis Business School offers a year-long program  PGP in Data Science with ML & AI at both its campuses  Kolkata and Bengaluru. The program is structured in a manner where the first 9 months are spent in the classroom with in-house and industry faculty and the last 3 months are spent as an intern with an industry partner.The Capstone project happens before the internship actually starts. So, students spent a total of 9 months in the classroom and had been doing these projects for the last 3 months (month 6  month 9 in the curriculum).The last time I had visited Praxis was in 2015 and I was dead sure that the program would have evolved. The question was how much? In which direction? What are the key takeaways for the students and how are the students from Praxis doing in the real world?So, let me share my findings based on the interaction with Gourab and the rest of the panel.The first noticeable change was the name of the program itself. Back in 2015, the Program was called PGP in Business Analytics as most of the material in the course was related to Business Analytics and Statistical Modelling.Over time, the program has evolved a lot  I was surprised to see the number of topics that are covered in the program. Here is a screenshot of topics covered in the curriculum, picked directly from their site:The program has clearly evolved a lot. It not only includes Machine Learning and Deep Learning, but also Big Data Tools and Business-Focused topics. As far as I can see  the program has evolved a lot and has become a comprehensive course for data scientists.I think the best way to judge this is to look at the projects. So  I held this off and the projects were sufficient proof by themselves.Needless to say, I was pretty excited by these discussions and with the context of this evolution  I was ready for what the rest of the day was supposed to be.Here are the views of Gourab Nath, part of the judging panel and Assistant Professor of Praxis Data Science Program:Collection of images is a challenging task for projects that involves topics like face recognition. Previously we were using an approach which was a little time-consuming.So, this time we decided to take a more systematic approach to collect the images that can massively same time of our participants. The teams working on such projects designed and developed an easy-to-handle application for facial image collection.A participant was requested to sit in front of the computer where we had the software running and all he/she needed to do was to enter his/her names and press a capture button to start the image collection process.The students at Praxis Business School are highly encouraged not to be hugely dependent on the tools and the packages and focus more on writing algorithms. This approach helps them to code better no matter what programming languages they use.A glance at the list of projects confirmed my views until now. I could see projects on Machine Learning, Natural Language Processing (NLP) and Computer Vision (CV).More importantly  it looked like these projects were not based on some open datasets. The problems mentioned were unique and I was not aware of many open datasets addressing these problems. Now, I was curious and excited to see what students have and how they have done.Heres the list of Capstone Projects done by students at Praxis Business School:Just to put things in perspective  most of the students presenting to us did not have any knowledge of predictive modeling and machine learning till July 2018  when they started with the program.Lets look at each capstone project in a bit more detail to understand what it was about plus the tools and techniques used in each project.Customer reviews have a huge influence on potential buyers of any product. A number of false reviews may drive the influence either in a positive direction or a negative direction. Any of these cases may make the customers take wrong decisions and the trustworthiness of the online opinions could be an issue.In this project, we investigate opinion spam in reviews.Note that this problem is different from email spam classification. Email spam usually refers to unsolicited commercial advertisements to attract people towards some products or services and hence they usually contain some prominent features.Our specific problem is more challenging because untruthful opinion spam is much harder to deal with. These kinds of spamming material can be carefully crafted and made indistinguishable.Tools: Python [Packages: NLTK, sklearn]
Techniques: Shingle Method, n-grams, Feature ExtractionYou open amazon.com and find that lots of customers have given great reviews about a well-branded mobile phone you are interested in. You wonder  are these good reviews due to the camera of the phone? Or, how good is the battery of the phone? And what about the display?While the number of reviews is really large and its almost impractical for the readers to go through all of them for evaluating the product, answers to these kinds of questions can be really helpful in making useful decisions.In this project, our focus is to identify various features of a mobile phone that the customers are talking about in their reviews and mine the customers opinion on these features.Further, we focus on identifying the polarity of these opinions and summarize the reviews. Finally, we develop a user-interface that summarizes the opinions about the features of the phone and rank the customer reviews based on its utility. We also propose an architecture that can perform the same on the reviews of any mobile phones.Tools: Python [Packages: NLTK, SpaCy, sklearn], Wix.com (for the website creation)Techniques: Fuzzy Matching, POS tagging, Association Rules Mining, Compactness Pruning, Redundancy Pruning, identifying sentiments based on the word list and weights in AFINN and WordNetCheck out a demonstration of this project below:How many times has this happened to you  you started a movie on your computer at night and fell asleep in the middle of it? And when you woke up the next day, you simply have no clue about how far you watched it? Happens to the best of us.In this project, we focus on developing an application that will be able to detect if you are asleep and automatically pause the video for you. The system waits to see if you wake up in the next 30 minutes. In case you dont, it will save a snapshot of the screen, close all the windows and shut down your computer automatically.Tool: Python, Open CV, Tensorflow, KerasTechniques: Viola-Jones algorithm on Rapid Object Detection using a Boosted Cascade of Simple Features, Inception V3, LSTMPicture this  you are watching a video on your computer but are feeling way too lazy to use the mouse or the keyboard to control the video player. Sounds familiar?We have a solution for you!In this project, we focus on making the computer recognize some special gestures which will enable one to control a video player by just using those gestures.For example, showing your palm in front of the system will enable the pause and the un-pause function. You will also be able to control the volume, fast forward a video or rewind it. You will also be able to do a wide range of other things like changing the slides of your PPT, changing pages, scrolling, etc. without grabbing your mouse or keyboard.Tool: Python [Packages: Open CV, PyPI (Keyboard and mouse package), Tensorflow, Keras]
Techniques: Green Screen (for background subtraction), Single-Shot Multi-box Detector (SSD)Students are asked to create teams for their projects or their assignments, which is of course a very common thing in every school and college. The class representative (CR) creates a Google spreadsheet and shares it with everyone.Students, after deciding who they want to team up with, populate the spreadsheet with the names of their team members. But the CR must remember the rules given by their Professor  the team size should be three and every team must have one female member at least.So, the CR checks the restrictions and if everything is fine, he/she shares it with the Professor. This is one way to do it.Or, you can do it the smart way.You stand with your teams in front of the computer, the computer checks the restrictions, recognizes you, and fills in the database with your names and photos.But remember, the computer wont allow you to register if the constraints are not satisfied or when at least one of the members in your team is already registered as members of any other team. So, you cannot fool it!Tool: Python [Packages: Open CV, Tensorflow backend, Keras, Imutils, face_recognition, pickle, dlib, cmake, tkinter (for GUI development)]
Techniques: VGG-NET 19, HOG DetectorIn this project, we developed a system to record class attendance using computer vision.After a faculty enters the system using a password and sets the period, the camera opens up to capture the picture of the class. The number of snapshots of the class is first passed through a face detector followed by a face recognizer.After the system recognizes the students, it updates the attendance spreadsheet and saves the captured image in its respective image directory  labeling it by the date and time of the day. The unidentified students are marked as absent.Tools: Python [Packages: dlib, OpenCV, Tensorflow, Keras, sklearn, tkinter (for GUI development)]
Techniques: Haar Cascade Classifier, HOG, Siamese Model (One Shot Learning), kNNThe use of a recommender system in e-commerce companies is a highly targeted approach that can generate a high conversion rate. These systems help customers discover the products which they might be interested in and will likely purchase.In this project, we have created a recommender system for a small fashion apparel industry that:Tools: PythonTechniques: kNN, Collaborative Filtering, Content-Based Filtering, AutoencodersHeres a demo video of this project:In this project, we have created a nearest document search engine for News reading. The application will not just recommend you related news but also give you the sentiment and highlight important words associated with the news. If the news is big and you do not want to read the full news, fair enough, this app will have a summarized version ready for you.Tools: Python [Packages: NLTK, sklearn, sumy, vadderSentiment, tkinter (for GUI development)]
Techniques: kNN, KDTree, Word Cloud, Lex Rank SummarizerOne of the most critical questions I had was  are these projects industry relevant? Bridging the gap between academia and industry has been a significant challenge in data science. It turns out the answer is quite comprehensive.In the last 4 years, the number of companies hiring has increased 4 times (from 15 in 2015 to 60 in 2018-19) and the average salary has doubled (5LPA in 2015 to 9LPA in 2018-19).So, here are the thoughts of my fellow panelists on this topic:I am very impressed on the scope, objectives, and contents of the capstone projects executed by Praxis students. The majority of the projects are around the application of deep learning concepts which they have learned as a part of the course work.The entire project execution and development activities were well planned and organized. Starting from defining the problem statement, challenges, real-time application and finally presenting the results.  Suresh Bommu,Advanced Analytics Practice Head at Wipro LimitedWhat really stood out for me was the effort put in by students in attempting to create an end-to-end product with a UI as well as the variety of projects and its extended application.  Rudrani Ghosh,Director at American Express Merchant Recommender and Signal Processing teamI loved the day and would live it again without second thoughts. But there were a few things which stood out for me:It was great to see the high level of projects presented by these students. As I mentioned, I was glad to see the students picking up challenging problems on not openly available datasets.At the end of the day, I had to rush back to the airport. Day trips to Bengaluru are bad! And the fact that I had to rush through projects for a few students only made it worse. I would have loved to spend more than a day  the Energy of the class, the faculty and the judges was infectious  Looking at these projects  I can confidently say that Praxis Business School continues to offer one of the best full time program in Machine Learning and Deep Learning in India.",https://www.analyticsvidhya.com/blog/2019/04/8-awesome-data-science-capstone-projects-from-praxis-business-school/
Learn how to Build and Deploy a Chatbot in Minutes using Rasa (IPL Case Study!),Learn everything about Analytics|Introduction|Table of Contents|Why should you use the Rasa Stack for Building Chatbots|Anatomy of our IPL Chatbot|Extracting User Intent from a Message|Making Interactive Conversations|Talking to your IPL chatbot|Getting IPL Data using CricAPI|Bringing the Chatbot to Life (Integrating Rasa and Slack)||Where should you go from here?|End Notes,"Overview of the Rasa Chatbot|Setting up the IPL Chatbot|Installing Rasa and its Dependencies|Training the NLU classifier|Predicting the Intent|Designing the conversational flow|Defining the Domain|Setting Policies|Training the Conversation Model|Creating a Slack Application|Installing Ngrok|Pushing the Chatbot to Slack|Share this:|Related Articles|8 Awesome Data Science Capstone Projects from Praxis Business School|How I Built Animated Plots in R to Analyze my Fitness Data (and you can too!)|
Mohd Sanad Zaki Rizvi
|46 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you ever been stuck at work while a pulsating cricket match was going on? You need to meet a deadline but you just cant concentrate because your favorite team is locked in a fierce battle for a playoff spot. Sounds familiar?Ive been in this situation a lot in my professional career and checking my phone every 5 minutes was not really an option! Being a data scientist, I looked at this challenge from the lens of an NLP enthusiast. Building a chatbot that could fetch me the scores from the ongoing IPL (Indian Premier League) tournament would be a lifesaver.So I did just that! Using the awesome Rasa stack for NLP, I built a chatbot that I could use on my computer anytime. No more looking down at the phone and getting distracted.And the cherry on top? I deployed the chatbot to Slack, the popular platform for de facto team communications. Thats right  I could check the score anytime without having to visit any external site. Sounds too good an opportunity to pass up, right?In this article, I will guide you on how to build your own Rasa chatbot in minutes and deploy it in Slack. With the ICC Cricket World Cup around the corner, this is a great time to get your chatbot game on and feed your passion for cricket without risking your job.The Rasa Stack is a set of open-source NLP tools focused primarily on chatbots. In fact, its one of the most effective and time efficient tools to build complex chatbots in minutes. Below are three reasons why I love using the Rasa Stack:These features differentiate Rasa from other chatbot building platforms, such as Googles DialogFlow. Heres a sneak peek into the chatbot well soon be building:Lets understand how our Rasa powered IPL chatbot will work before we get into the coding part. Understanding the architecture of the chatbot will go a long way in helping us tweak the final model.There are various approaches we can take to build this chatbot. How about simply using the quickest and most efficient method? Check out a high-level overview of our IPL chatbot below:Lets break down this architecture (keep referring to the image to understand this):I have created two versions of the project on GitHub:So, go ahead and clone the Practice Version project from GitHub:And cd into the practice_version:A quick note on a couple of things you should be aware of before proceeding further:You can use the code below to install all the dependencies of the Rasa Stack:This step might take a few minutes because there are quite a few files to install. You will also need to install a spaCy English language model:Lets move on!The first thing we want to do is figure out the intent of the user. What does he or she want to accomplish? Lets utilize Rasa and build an NLU model to identify user intent and its related entities.Look into the practice_version folder you downloaded earlier:The two files we will be using are highlighted above.As you can see, the format of training data for intent is quite simple in Rasa. You just have to:Lets write some intent examples in Python for the scenario when the user wants to get IPL updates:You can include as many examples as you want for each intent. In fact,make sure to include slangs and short forms that you use while texting. The idea is to make the chatbot understand the way we type text. Feel free to refer to the complete version where I have given plenty of examples for each intent type.Lets choose the former as it suits our example:If you have made it this far, you have already done most of the work for the intent extraction model. Lets train it and see it in action!You can train the classifier by simply following the command below:Using Windows? You can run the following python code:Lets test how good our model is performing by giving it a sample text that it hasnt been trained on for extracting intent. You can open an iPython/Python shell and follow the following steps:Here is what the output looks like:Not only does our NLU model perform well on intent extraction, but it also ranks the other intents based on their confidence scores. This is a nifty little feature that can be really useful when the classifier is confused between multiple intents.One of the most important aspects of a chatbot application is its ability to be interactive. Think back to a chatbot youve used before. Our interest naturally piques if the chatbot is able to hold a conversation, right?The chatbot is expected to extract all the necessary information needed to perform a particular task using the back and forth conversation it has with the end user.Take a moment to think of the simplest conversation our chatbot can have with a user. What would be the flow of such a conversation? Lets write it in the form of a story!Lets see how we can teach a simple conversation like that to Rasa:The general format is:This is called a user story path. I have provided a few stories in thedata/stories.md file for your reference. This is the training data for Rasa Core.The way it works is:Check out the data/stories.md file in the complete_version of the project for more such examples. Meanwhile, here is a nice visualization of the basic story paths generated by Rasa for our IPL chatbot:The above illustration might look complicated, but its simply listing out various possible user stories that I have taught Rasa. Here are a few things to note from the above graph:Write the following in your stories.md file:Now, generate a similar graph for your stories using the following command:This is very helpful when debugging the conversational flow of the chatbot.Now, open up thedomain.yml file. You will be familiar with most of the features mentioned here:The domain is the world of your chatbot. It contains everything the chatbot should know, including:Rasa Core generates the training data for the conversational part using the stories we provide. It also lets you define a set of policies to use when deciding the next action of the chatbot. These policies are defined in the policies.yml file.So, open thatfile and copy the following code:Here are a few things to note about the above policies (taken from Rasa Cores policies here):You can train the model using the following command:Or if you are on Windows, you can use the full Python command:This will train the Rasa Core model and we can start chatting with the bot right away!Before we proceed further, lets try talking to our chatbot and see how it performs. Open a new terminal and type the following command:Once it loads up, try having a conversation with your chatbot. You can start by saying Hi. The following video shows my interaction with the chatbot:I got an error message when trying to get IPL updates:The chatbot understood my intent to get news about the IPL. So what went wrong? Its simple  we still havent written the backend code for that! So, lets build up the backend next.We will use theCricAPI for fetching IPL related news. It is free for 100 requests per day, which (I hope) is more than enough to satiate that cricket crazy passion you have.You need to first signup on the website to get access to their API:You should be able to see your API Key once you are logged in:Save this key as it will be really important for our chatbot. Next, open your actions.py file and update it with the following code:Fill in the API_KEY with the one you got from CricAPI and you should be good to go. Now, you can again try talking to your chatbot. This time, be prepared to be amazed.Open a new terminal and start your action server:This will activate the server that is running on the actions.py file and will be working in the background for us. Now, restart the chatbot in the command line:And this time, it should give you some IPL news when asked. Isnt that awesome? We have already built a complete chatbot without doing any complex steps!So we have the chatbot ready. Its time to deploy it and integrate it into Slack as I promised at the start of this article. Fortunately for us, Rasa handles 90% of the deployment part on its own.Note: You need to have a workspace in Slack before proceeding further. If you do not have one, then you can refer to this.Now that we have a workspace to experiment with, we need an application to attach your bot. Create the app on the below link:1. Click on Create App, give a name to the app, and select your workspace:This will redirect you to your app dashboard. From there, you can select theBots option:2. Click Add a Bot User > Give a name to your bot. In my case, I have named it iplbot. Now, we need to add it to our workspace so we can chat with it! Go back to the above app dashboard and scroll down to find the Install App to Workspace option:Once you do that, Slack will ask you to authorize the application. Go ahead and accept the authorization.3. Before we are able to connect any external program to our Slack bot, we need to have an auth token that we need to provide when trying to connect with it. Go back to the app dashboard and select the OAuth & Permissions option:4. This will open the permissions settings of the app. Select the Bot User OAuth Access Token and save it (I have hidden them for security reasons). This token is instrumental in connecting to our chatbot.Our work isnt over yet. We need another useful tool to deploy our chatbot to Slack. Thats ngrokand you can use the following link to download it:We are now one step away from deploying our own chatbot! Exciting times await us in the next section.We need onlyfive commands to get this done as Rasa takes care of everything else behind the scenes.This will give an output like the below image:The highlighted link is the link on the internet that is connected to your computers port 5055. This is what ngrok does  it lets your computers local programs be exposed on the internet. In a way, this is a shortcut for using a cloud service to deploy your app.You will get a message like this:Notice that the Rasa Core server is running at port 5002.In the above URL, replace the ngrok part with your ngrok URL:Once youve added the events, click the Save Changes button at the bottom of the screen.Now you can just refresh your Slack page and start chatting right away with your bot! Heres a conversation with my chatbot:Youll find the below links useful if you are looking for similar challenges. I have built a Zomato-like chatbot for Restaurant Search problem using both Rasa Core and Rasa NLU models. I will be teaching this in much more detail in our course on Natural Language Processing.The links to the course are below for your reference:I would love to see different approaches and techniques from our community. Try to use different pipelines in Rasa Core, explore more Policies, fine-tune those models, check out what other features CricAPI provides, etc. There are so many things you can try! Dont stop yourself here  go on and experiment.Feel free to discuss and provide your feedback in the comments section below. The full code for my project is available here.You should also check out these two articles on building chatbots:",https://www.analyticsvidhya.com/blog/2019/04/learn-build-chatbot-rasa-nlp-ipl/
How I Built Animated Plots in R to Analyze my Fitness Data (and you can too!),Learn everything about Analytics|Introduction|Table of Contents|Extracting the Health Data from your Fitness App|Importing the XML Data File into R|Pre-processing our Health Data|Drawing up our Hypothesis|Exploring the Health Data|Preparing our Personalized Fitness Tracker Dashboard||End Notes,"Heart Rate|Step Count|Active energy burned|Stairs Climbed|Distance Covered|Share this:|Related Articles|Learn how to Build and Deploy a Chatbot in Minutes using Rasa (IPL Case Study!)|DataHack Radio #22: Exploring Computer Vision and Data Engineering with Dat Tran|
Analytics Vidhya Content Team
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We are currently in the midst of a global fitness revolution. Most of the people I know are geeking out over the latest gadget in the market that will help them achieve their fitness goals. My focus, however, is slightly different. I cant help it  Im a data scientist after all!All these fitness trackers, bands, even our smartphones  they all store our health data via certain applications, like Healthkit on iOS, Google Fit on Android, etc. We are literally a few touches away from accessing our health data  distance covered, steps-taken, calories burned, heart rate, etc.Now, I wanted to analyze certain trends in my fitness level. The applications I have dont quite offer that depth or level of analysis. So I turned to the one thing I love  data visualization in R. I could easily extract this data from the apps and perform all sorts of analysis in R, including building animated plots.Thats right  I used my health data and analyzed all sorts of metrics using really cool animated plots in R! And in this article, I will show how you can easily make these plots using just a few lines of R code.I have written this article in a step-by-step manner. My recommendation would be to follow the sequence mentioned below:Im an iOS aficionado so I used the HealthKit app on my device to store my health data. You can follow the below steps to export your data:Voila! You will receive a zip file containing XML objects. Download and read it into your R console (discussed in the next section).Android users can also extract their health data but the steps will be a little different. Follow the steps mentioned in thislinkexport your health data from the Google Fit app.Once you have downloaded your health data, you need to import it using an R-friendly format. Use the below code block to read the XML file:Note: Install the XML package first from the CRAN repository if you havent done so already.Well have to transform a few variables and add new features before we can prepare visualizations and build our dashboard. This will help us in easily subsetting and preparing intuitive plots. Well be using the date column to add new columns like years, month, week, hour, etc.For this, we require the lubridate package in R. I personally love this package  it is very useful when were dealing with date and time data.Now, lets look at the structure of the cleaned and sorted data:Output:Quite a lot of observations for us to begin our analysis. But whats the first thing a data scientist should do before anything else? Yes, it is necessary to set your hypothesis first.Our aim here is to use the readily available health-app data to study and answer the below pointers:Any other you can think of? Let me know and we can add that into the final analysis!Lets look at what kind of observations were stored by the Apple Watch, the fitness band, and the health app:Output:We have sufficient observations under each type. We will narrow down our focus to a few important variables in the next section.But first, begin by installing and importing a few important libraries that will help us in subsetting the data and generating plots:The wait is over! Lets begin preparing our dashboard to create intuitive and advanced visualizations. We are primarily interested in the following metrics:Lets take them up one by one.Easily the most critical metric in our dataset.Lets look at the heart rate since I started using these fitness tracking devices. We will group the mean values of the heart rate by date, month and year. You can do this using the following code block:Output:Having too many dates on the X-axis will look haphazard. So, scale the X-axis by just representing the month and year:Output:Nice! This is a good start. Lets take things up a notch, though. Yes, Im talking about animated plots in R!You must have come across animated plots on social media. I certainly cant scroll by without seeing one or two of those. How about we create an animated plot ourselves?Well be needing the gganimate package for this:Output:Looks cool, right? We got a really good looking plotusing just one line of code.Now, we know that the normal resting heart-rate should be somewhere between 60-100 bpm and the normal active heart-rate should be between 100-120 bpm.There have been times when the heart rate has crossed these boundaries. It could potentially be that there was some extra activity done on those days. Could be data points be outliers? Lets look at the median heart-rate values to figure this out:Output:Some of the spikes still remain prominent. This could be due to an increased number of steps or stairs climbed.That naturally leads us into the next metric of our health data.Many recent studies reveal that you need to take a certain amount of steps per day to stay healthy. For the purposes of our project here, well assume the below categories apply:Please note that this categorization is entirely for the sake of our study and not recommended by any medical professional.So, lets create a plot that shows us the total number of steps taken every day per year:Output:Thats a bit of an eye-opener for me. This plot shows I was somewhat active till May 2018 and then started taking more steps daily.An increasing trend is clearly visible.But there is a decrease post-February 2019. I wanted to dig a little deeper to understand this at a more granular level.So, I created a plot that summarises the weekly step count. Since we see an abnormal hike in 2019, lets look at the weekly median step count:Output:We can see that the median number of steps for all days is pretty much the same. So we can safely say there are some outliers present in the step count data. Instead of plotting the median, you can plot mean counts. This will show you that one of the Thursdays has an extremely high observation.Time to animate our plot:Output:Lovely! Lets move on to the next health metric.I feel this is an underrated metric in our fitness trackers. We tend to focus on the number of steps we walked to see if we covered enough ground. What about the calories we burned? Thats a pretty interesting metric if you ask me.We will again create a time series plot that will show us the total calories burned daily:Output:A good amount of active calories have been burnt on most days. The range falls between 400-600 kcal daily. But there are a number of observations where the calories burnt are between 0-200 kcal.So, in order to transition to a healthier lifestyle, I should burn about 500 calories every day in order to lose 1 pound in a week.There is a sharp decline towards the end of our plot due to an inadequate number of observations for April 2019.What else can we do with this data? To understand which days require more physical activity to burn the required amount of calories, we can draw up a heatmap. Lets do this in a way such that every day of every month is taken into account.Output:The above plot can easily be interpreted in the following way:On Sunday, in the first week of the 5th month (in 2018), the calories burned are close to 200. Thats quite low compared to our given aim. The activity looks quite good overall,except on Sundays. Thats pretty relatable, right?We can animate such heatmaps or color density plots using the transition_states() function in the gganimate package:Look at this cool visualization:Output:Looks perfect! No surprise to see that there is significantly less energy being burned on weekends as compared to weekdays.This is a unique metric. A few people I know run up and down stairs to build up their fitness. I certainly dont do that but lets see what we can squeeze out of this data.For each year, we will aggregate the total number of stairs climbed in a month. Then well plot a bar chart which compares the flights climbed from the previous year:Output:Anything that jumps out at you? The maximum number of stairs climbed is in September and November 2018. Theres a logical explanation to that  I was part of the organizing team at a conference. Hence, the spike in the data.Now here, we can correlate our findings with the burned energy plot we saw earlier. We saw a decreasing trend in the energy burned in 2019, right? Notice how the number of stairs climbed is decreasing in 2019. Thats partly down to inadequate data again in April 2019.Animating this plot is as easy as before, come on lets give it a try!Output:Using the above method, we can again aggregate the distance traveled (in kilometers) for different months corresponding to their respective years:Output:As expected, the distance traveled in November 2018 really stands out. Theres a lot of insight to be gained from each plot weve generated in this article. This has been quite a fun journey for me!Lastly, another animated plot for the distance covered:Output:Data Visualization is one of the crucial steps in the robust analysis of any data. Not just fancy datasets, you can check your environment for any data sources and use them for your own personalized projects. Isnt that exciting?We showed you plenty of plots to get you started with your own fitness dashboard. Transfer the data and begin practicing! Do not forget to update the community if you prepare any better or improved plots.",https://www.analyticsvidhya.com/blog/2019/04/how-built-personalized-interactive-fitness-tracker-dashboard-r/
DataHack Radio #22: Exploring Computer Vision and Data Engineering with Dat Tran,Learn everything about Analytics|Introduction|Dat Trans Background and Journey into Data Science|Data Science at idealo  Focusing on Computer Vision|Data Engineering Experience|Challenges Faced in Implementing Data Science and Data Engineering|Keeping yourself Updated on the Latest Data Science Techniques|Advice to Aspiring Data Science Professionals|Dats Data Science Hiring Process|Future Trends in Machine Learning|End Notes,"Share this:|Related Articles|How I Built Animated Plots in R to Analyze my Fitness Data (and you can too!)|Predicting Movie Genres using NLP  An Awesome Introduction to Multi-Label Classification|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How do computer vision techniques work in an industry setting? How does an organization use data engineering to scale up its operations?These are questions every aspiring data scientist must be aware of. Dat Tran, Head of Data Science at idealo internet GmbH, is the perfect person to shed light on these questions.Dat has worked on a variety of data engineering projects before he came to idealo, and now leads a team of data scientists who work on really cool computer vision problems. This is one of my favorite episodes since we launched DataHack Radio  the depth and breadth of topics covered, plus Dats incredible knowledge, make this a must-listen.In this episode of the DataHack Radio podcast, Kunal and Dat cover multiple topics, including:I have penned down a few highlights from the podcast below. But I strongly recommend listening to the entire conversation! The energy Dat brings to this episode is incredible.You can subscribe to the DataHack Radio podcast on any of the below platforms:Dats journey into data science isnt your run-of-the-mill story. He hadnt even heard of machine learning during his undergrad days, where his focus was on investment banking. But Dat quickly realized it wasnt the field for him. So what next?Back to the drawing board  a Masters degree! During this time, a couple of his friends were starting out in machine learning and it wasnt long before Dat was drawn into this wonderfully complex field.He landed a job in the advanced analytics department at Accenture. This was back when Big Data was starting to become the ultimate buzzword in the industry  a great time to enter this field. Dat moved to Pivotal Inc. a year later (joining as a data scientist), recognizing that this was a brilliant opportunity to get more hands-on experience in machine learning.At Pivotal, Dat worked on a variety of projects spanning different industries, including automotive and airlines. He worked there for over two years and credits a lot of his current knowledge and experience to his time at Pivotal. He gave talks at multiple PyData conferences as well during this time  a truly impressive achievement.Dat is now working as the Head of Data Science at idealo internet GmbH, a successful Berlin-based startup and one of the largest portals in the German e-commerce market.idealo is a price comparison site (for products as well as hotels) so you can imagine the numerous data science functions the team performs  price prediction, indexing, developing and using a recommendation engine, among other things. Dats team, however, focuses on applying computer vision.A fair question to ask  what role does computer vision have in a price comparison site? Well, idealo has a ton of images of products and hotels:Dat explained this section using a really intuitive example. idealo has approximately 2 million accommodations listed with 130 images per accommodation (on average). Now, there are all kinds of hotels  small-sized, medium ones, and the big players (the luxurious 5-star ones).The pictures of these hotel rooms vary depending on who took them. The non-luxury hotels typically have images taken by owners themselves while the 5-star hotels send images taken by professionals. There is quite a big gap in the image quality between these two categories.Dat and his team use an array of computer vision concepts to analyze and make use of these images:Really interesting stuff! Its a pleasure to see computer vision making inroads in the industry, isnt it?I came across Dats talk at PyData on YouTube  it doesnt take long to realize he is a data engineering expert. His talk is on How you really get your data science models into production the cool way! and you can check it out below:At idealo, there are a variety of tools being used for data engineering, such as AWS for training and Kubernetes for putting models into production.I personally feel data engineering is a very overlooked aspect (by aspiring data scientists) of the overall data science project lifecycle. You will most certainly face questions on model deployment and other aspects of software engineering in your data scientist interview. This section of the podcast will provide you with a birds eye view of an industry-ready process.Data science and data engineering are inextricably linked  you cannot separate them for all intents and purposes. Dat explained this using the example of a neural network (a convolutional neural network (CNN), to be precise). There are quite a few CNN frameworks to choose from, like RESNET, MobileNET, VGG, etc.The challenge with these CNN models is they have tons of hyperparameters, hence making them quite large. This brings up the age-old debate of balancing accuracy and speed. You can get away with it in research but when youre working with production environments? That is a significant obstacle.Dat mentioned quite a few common challenges from the data engineering specific side as well, including:How can we use a Keras trained model on a TensorFlow backend?Do we need to transform our images into certain formats?How do we benchmark our model results?You best learn about these things when you do them yourself.As we alluded to earlier, Dat has done most of his data science learning on the job. There is nothing like practical hands-on experience to indelibly ingrain concepts.Outside of that, there are so many options to learn from these days (everything is a quick Google search away!):A major challenge with these platforms is that we dont get a structured path or answer to a specific problem. That, again, is why experience is king in data science.Software engineering is a key facet of data science most aspiring professionals are unaware of. And you simply cant get away from it in an industry role. So heres Dats advice for you:You need kind of an engineering background. Learn the basics  how to write clean code, version control, testing, and move on to data science then.\And this really, REALLY important point:The Machine Learning aspect is a small part of a big software project!Knowing mathematics, statistics, machine learning algorithms and even tools like R and Python is good, but these dont differentiate you from the competition. Everyone else is learning the same thing. So what else is there? It comes down to that one thing again  software engineering.Dat uses a straightforward set of pointers and rounds to judge a candidates ability:Which machine learning functions will see a major improvement and focus in the coming years?One of my favorite DataHack Radio episodes so far! Dat brings a ton of enthusiasm and knowledge to the podcast that really shines through in the way he explains his role, the challenges his team faces from both a data science as well as a data engineering perspective, his advice to aspiring data scientists, among other things.A pleasure listening to him elaborate on relevant industry problems and how to overcome them. What was your favorite part of the episode? Let us know in the comments section below.",https://www.analyticsvidhya.com/blog/2019/04/datahack-radio-exploring-computer-vision-data-engineering-podcast-dat-tran/
Predicting Movie Genres using NLP  An Awesome Introduction to Multi-Label Classification,Learn everything about Analytics|Introduction|Table of Contents|Brief Introduction to Multi-Label Classification|Setting up our Multi-Label Classification Problem Statement|About the Dataset|Our Strategy to Build a Movie Genre Prediction Model|Implementation: Using Multi-Label Classification to Build a Movie Genre Prediction Model (in Python)|Where to go from here?|End Notes,"Import the required libraries|Load Data|Data Exploration and Pre-processing|Converting Text to Features|Build Your Movie Genre Prediction Model|Create Inference Function|Share this:|Related Articles|DataHack Radio #22: Exploring Computer Vision and Data Engineering with Dat Tran|A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python|
Prateek Joshi
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I was intrigued going through this amazing article on building a multi-label image classification model last week. The data scientist in me started exploring possibilities of transforming this idea into a Natural Language Processing (NLP) problem.That article showcases computer vision techniques to predict a movies genre. So I had to find a way to convert that problem statement into text-based data. Now, most NLP tutorials look at solving single-label classification challenges (when theres only one label per observation).But movies are not one-dimensional. One movie can span several genres. Now THAT is a challenge I love to embrace as a data scientist. I extracted a bunch of movie plot summaries and got down to work using this concept of multi-label classification. And the results, even using a simple model, are truly impressive.In this article, we will take a very hands-on approach to understanding multi-label classification in NLP. I had a lot fun building the movie genre prediction model using NLP and Im sure you will as well. Lets dig in!Im as excited as you are to jump into the code and start building our genre classification model. Before we do that, however, let me introduce you to the concept of multi-label classification in NLP. Its important to first understand the technique before diving into the implementation.The underlying concept is apparent in the name  multi-label classification. Here, an instance/record can have multiple labels and the number of labels per instance is not fixed.Let me explain this using a simple example. Take a look at the below tables, where X represents the input variables and y represents the target variables (which we are predicting):We cannot apply traditional classification algorithms directly on this kind of dataset. Why? Because these algorithms expect a single label for every input, when instead we have multiple labels. Its an intriguing challenge and one that we will solve in this article.You can get a more in-depth understanding of multi-label classification problems in the below article:There are several ways of building a recommendation engine. When it comes to movie genres, you can slice and dice the data based on multiple variables. But heres a simple approach  build a model that can automatically predict genre tags! I can already imagine the possibilities of adding such an option to a recommender. A win-win for everyone.Our task is to build a model that can predict the genre of a movie using just the plot details (available in text form).Take a look at the below snapshot from IMDb and pick out the different things on display:Theres a LOT of information in such a tiny space:Genres tell us what to expect from the movie. And since these genres are clickable (at least on IMDb), they allow us to discover other similar movies of the same ilk. What seemed like a simple product feature suddenly has so many promising options. We will use the CMU Movie Summary Corpus open dataset for our project. You can download the dataset directly from this link.This dataset contains multiple files, but well focus on only two of them for now:We know that we cant use supervised classification algorithms directly on a multi-label dataset. Therefore, well first have to transform our target variable. Lets see how to do this using a dummy dataset:Here, X and y are the features and labels, respectively  it is a multi-label dataset. Now, we will use the Binary Relevance approach to transform our target variable, y. We will first take out the unique labels in our dataset:Unique labels = [ t1, t2, t3, t4, t5 ]
There are 5 unique tags in the data. Next, we need to replace the current target variable with multiple target variables, each belonging to the unique labels of the dataset. Since there are 5 unique labels, there will be 5 new target variables with values 0 and 1 as shown below:We have now covered the necessary ground to finally start solving this problem. In the next section, we will finally make an Automatic Movie Genre Prediction System using Python!We have understood the problem statement and built a logical strategy to design our model. Lets bring it all together and start coding!We will start by importing the libraries necessary to our project:Lets load the movie metadata file first. Use \t as the separator as it is a tab separated file (.tsv):Oh wait  there are no headers in this dataset. The first column is the unique movie id, the third column is the name of the movie, and the last column contains the movie genre(s). We will not use the rest of the columns in this analysis.Lets add column names to the aforementioned three variables:Now, we will load the movie plot dataset into memory. This data comes in a text file with each row consisting of a movie id and a plot of the movie. We will read it line-by-line:Next, split the movie ids and the plots into two separate lists. We will use these lists to form a dataframe:Lets see what we have in the movies dataframe:Perfect! We have both the movie id and the corresponding movie plot.Lets add the movie names and their genres from the movie metadata file by merging the latter into the former based on themovie_idcolumn:Great! We have added both movie names and genres. However, the genres are in a dictionary notation. It will be easier to work with them if we can convert them into a Python list. Well do this using the first row:Output:We cant access the genres in this row by using just.values( ). Can you guess why? This is because this text is a string, not a dictionary. We will have to convert this string into a dictionary. We will take the help of the json library here:Output:We can now easily access this rows genres:Output:This code helps us to extract all the genres from the movies data. Once done, add the extracted genres as lists back to the movies dataframe:Some of the samples might not contain any genre tags. We should remove those samples as they wont play a part in our model building process:Output:Only 411 samples had no genre tags. Lets take a look at the dataframe once again:Notice that the genres are now in a list format. Are you curious to find how many movie genres have been covered in this dataset? The below code answers this question:Output:There are over 363 unique genre tags in our dataset. That is quite a big number. I can hardy recall 5-6 genres! Lets find out what are these tags. We will use FreqDist( ) from the nltk library to create a dictionary of genres and their occurrence count across the dataset:I personally feel visualizing the data is a much better method than simply putting out numbers. So, lets plot the distribution of the movie genres:Next, we will clean our data a bit. I will use some very basic text cleaning steps (as that is not the focus area of this article):Lets apply the function on the movie plots by using the apply-lambda duo:Feel free to check the new versus old movie plots. I have provided a few random samples below:In the clean_plot column, all the text is in lowercase and there are also no punctuation marks. Our text cleaning has worked like a charm.The function below will visualize the words and their frequency in a set of documents. Lets use it to find out the most frequent words in the movie plots column:Most of the terms in the above plot are stopwords. These stopwords carry far less meaning than other keywords in the text (they just add noise to the data). Im going to go ahead and remove them from the plots text. You can download the list of stopwords from the nltk library:Lets remove the stopwords:Check the most frequent terms sans the stopwords:Looks much better, doesnt it? Far more interesting and meaningful words have now emerged, such as police, family, money, city, etc.I mentioned earlier that we will treat this multi-label classification problem as a Binary Relevance problem. Hence, we will now one hot encode the target variable, i.e.,genre_new by using sklearns MultiLabelBinarizer( ). Since there are 363 unique genre tags, there are going to be 363 new target variables.Now, its time to turn our focus to extracting features from the cleaned version of the movie plots data. For this article, I will be using TF-IDF features. Feel free to use any other feature extraction method you are comfortable with, such as Bag-of-Words, word2vec, GloVe, or ELMo.I recommend checking out the below articles to learn more about the different ways of creating features from text:I have used the 10,000 most frequent words in the data as my features. You can try any other number as well for themax_features parameter.Now, before creating TF-IDF features, we will split our data into train and validation sets for training and evaluating our models performance. Im going with a 80-20 split  80% of the data samples in the train set and the rest in the validation set:Now we can create features for the train and the validation set:We are all set for the model building part! This is what weve been waiting for.Remember, we will have to build a model for every one-hot encoded target variable. Since we have 363 target variables, we will have to fit 363 different models with the same set of predictors (TF-IDF features).As you can imagine, training 363 models can take a considerable amount of time on a modest system. Hence, I will build a Logistic Regression model as it is quick to train on limited computational power:We will use sk-learns OneVsRestClassifier class to solve this problem as a Binary Relevance or one-vs-all problem:Finally, fit the model on the train set:Predict movie genres on the validation set:Lets check out a sample from these predictions:It is a binary one-dimensional array of length 363. Basically, it is the one-hot encoded form of the unique genre tags. We will have to find a way to convert it into movie genre tags.Luckily, sk-learn comes to our rescue once again. We will use theinverse_transform( ) function along with the MultiLabelBinarizer( ) object to convert the predicted arrays into movie genre tags:Output:Wow! That was smooth.However, to evaluate our models overall performance, we need to take into consideration all the predictions and the entire target variable of the validation set:Output:We get a decent F1 score of 0.315. These predictions were made based on a threshold value of 0.5, which means that the probabilities greater than or equal to 0.5 were converted to 1s and the rest to 0s.Lets try to change this threshold value and see if that improves our models score:Now set a threshold value:I have tried 0.3 as the threshold value. You should try other values as well. Lets check the F1 score again on these new predictions.Output:That is quite a big boost in our models performance. A better approach to find the right threshold value would be to use a k-fold cross validation setup and try different values.Wait  we are not done with the problem yet. We also have to take care of the new data or new movie plots that will come in the future, right? Our movie genre prediction system should be able to take a movie plot in raw form as input and generate its genre tag(s).To achieve this, lets build an inference function. It will take a movie plot text and follow the below steps:Lets test this inference function on a few samples from our validation set:Yay! Weve built a very serviceable model. The model is not yet able to predict rare genre tags but thats a challenge for another time (or you could take it up and let us know the approach you followed).If you are looking for similar challenges, youll find the below links useful. I have solved a Stackoverflow Questions Tag Prediction problem using both machine learning and deep learning models in our course on Natural Language Processing.The links to the course are below for your reference:I would love to see different approaches and techniques from our community to achieve better results. Try to use different feature extraction methods, build different models, fine-tune those models, etc. There are so many things that you can try. Dont stop yourself here  go on and experiment!Feel free to discuss and comment in the comment section below. The full code is available here.",https://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/
A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python,Learn everything about Analytics|Introduction|Table of Contents|The Road to Q-Learning|Why Deep Q-Learning?|Deep Q-Networks|Challenges in Deep RL as Compared to Deep Learning|Putting it all Together|Implementing Deep Q-Learning in Python using Keras & OpenAI Gym|End Notes,"RL Agent-Environment|Markov Decision Process (MDP)|Q Learning|1. Target Network|2. Experience Replay|Share this:|Related Articles|Predicting Movie Genres using NLP  An Awesome Introduction to Multi-Label Classification|8 Useful R Packages for Data Science You Arent Using (But Should!)|
Ankit Choudhary
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 1: Install keras-rl library|Step 2: Install dependencies for the CartPole environment|Step 3: Lets get started!,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have always been fascinated with games. The seemingly infinite options available to perform an action under a tight timeline  its a thrilling experience. Theres nothing quite like it.So when I read about the incredible algorithms DeepMind was coming up with (like AlphaGo and AlphaStar), I was hooked. I wanted to learn how to make these systems on my own machine. And that led me into the world of deep reinforcement learning (Deep RL).Deep RL is relevant even if youre not into gaming. Just check out the sheer variety of functions currently using Deep RL for research:What about industry-ready applications? Well, here are two of the most commonly cited Deep RL use cases:The scope of Deep RL is IMMENSE. This is a great time to enter into this field and make a career out of it.In this article, I aim to help you take your first steps into the world of deep reinforcement learning. Well use one of the most popular algorithms in RL, deep Q-learning, to understand how deep RL works. And the icing on the cake? We will implement all our learning in an awesome case study using Python.There are certain concepts you should be aware of before wading into the depths of deep reinforcement learning. Dont worry, Ive got you covered.I have previously written various articles on the nuts and bolts of reinforcement learning to introduce concepts like multi-armed bandit, dynamic programming, Monte Carlolearning and temporal differencing. I recommend going through these guides in the below sequence:These articles are good enough for getting a detailed overview of basic RL from the beginning.However, note that the articles linked above are in no way prerequisites for the reader to understand Deep Q-Learning. We will do a quick recap of the basic RL concepts before exploring what is deep Q-Learning and its implementation details.A reinforcement learning task is about training an agent which interacts with its environment. The agent arrives at different scenarios known as states by performing actions. Actions lead to rewards which could be positive and negative.The agent has only one purpose here  to maximize its total reward across an episode.This episode is anything and everything that happens between the first state and the last or terminal state within the environment. We reinforce the agent to learn to perform the best actions by experience. This is the strategy or policy.Lets take an example of the ultra-popular PubG game:Now, in order to kill that enemy or get a positive reward, there is a sequence of actions required. This is where the concept of delayed or postponed reward comes into play. The crux of RL is learning to perform these sequences and maximizing the reward.An important point to note  each state within an environment is a consequence of its previous state which in turn is a result of its previous state. However, storing all this information, even for environments with short episodes, will become readily infeasible.To resolve this, we assume that each state follows a Markov property, i.e., each state depends solely on the previous state and the transition from that state to the current state. Check out the below maze to better understand the intuition behind how this works:Now, there are 2 scenarios with 2 different starting points and the agent traverses different paths to reach the same penultimate state. Now it doesnt matter what path the agent takes to reach the red state. The next step to exit the maze and reach the last state is by going right. Clearly, we only needed the information on the red/penultimate state to find out the next best action which is exactly what the Markov property implies.Lets say we know the expected reward of each action at every step. This would essentially be like a cheat sheet for the agent! Our agent will know exactly which action to perform.It will perform the sequence of actions that will eventually generate the maximum total reward. This total reward is also called the Q-value and we will formalise our strategy as:The above equation states that the Q-value yielded from being at state s and performing action a is the immediate reward r(s,a) plus the highest Q-value possible from the next state s. Gamma here is the discount factor which controls the contribution of rewards further in the future.Q(s,a) again depends on Q(s,a) which will then have a coefficient of gamma squared. So, the Q-value depends on Q-values of future states as shown here:Adjusting the value of gamma will diminish or increase the contribution of future rewards.Since this is a recursive equation, we can start with making arbitrary assumptions for all q-values. With experience, it will converge to the optimal policy. In practical situations, this is implemented as an update:where alpha is the learning rate or step size. This simply determines to what extent newly acquired information overrides old information.Q-learning is a simple yet quite powerful algorithm to create a cheat sheet for our agent. This helps the agent figure out exactly which action to perform.But what if this cheatsheet is too long? Imagine an environment with 10,000 states and 1,000 actions per state. This would create a table of 10 million cells. Things will quickly get out of control!It is pretty clear that we cant infer the Q-value of new states from already explored states. This presents two problems:Heres a thought  what if we approximate these Q-values with machine learning models such as a neural network? Well, this was the idea behind DeepMinds algorithm that led to its acquisition by Google for 500 million dollars!In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output. The comparison between Q-learning & deep Q-learning is wonderfully illustrated below:So, what are the steps involved in reinforcement learning using deep Q-learning networks (DQNs)?The section in green represents the target. We can argue that it is predicting its own value, but since R is the unbiased true reward, the network is going to update its gradient using backpropagation to finally converge.So far, this all looks great. We understood how neural networks can help the agent learn the best actions. However, there is a challenge when we compare deep RL to deep learning (DL):As you can see in the above code, the target is continuously changing with each iteration. In deep learning, the target variable does not change and hence the training is stable, which is just not true for RL.To summarise, we often depend on the policy or value functions in reinforcement learning to sample actions. However, this is frequently changing as we continuouslylearn what to explore. As we play out the game, we get to know more about the ground truth values of states and actions and hence, the output is also changing.So, we try to learn to mapfor a constantly changing input and output. But then what is the solution?Since the same network is calculating the predicted value and the target value, there could be a lot of divergence between these two. So, instead of using 1one neural network for learning, we can use two.We could use a separate network to estimate the target. This target network has the same architecture as the function approximator but with frozen parameters. For every C iterations (a hyperparameter), the parameters from the prediction network are copied to the target network. This leads to more stable training because it keeps the target function fixed (for a while):To perform experience replay, we store the agents experiences =(,,,+1)What does the above statement mean? Instead of running Q-learning on state/action pairs as they occur during simulation or the actual experience, the system stores the data discovered for [state, action, reward, next_state]  in a large table.Lets understand this using an example.Suppose we are trying to build a video game bot where each frame of the game represents a different state. During training, we could sample a random batch of 64 frames from the last 100,000 frames to train our network. This would get us a subset within which the correlation amongst the samples is low and will also provide better sampling efficiency.The concepts we have learned so far? They all combine to make the deep Q-learning algorithm that was used to achive human-level level performance in Atari games (using just the video frames of the game).I have listed the steps involved in a deep Q-network (DQN) below:Alright, so we have a solid grasp on the theoretical aspects of deep Q-learning. How about seeing it in action now? Thats right  lets fire up our Python notebooks!We will make an agent that can play a game called CartPole.We can also use an Atari game but training an agent to play that takes a while (from a few hours to a day). The idea behind our approach will remain the same so you can try this on an Atari game on your machine.CartPole is one of the simplest environments in the OpenAI gym (a game simulator). As you can see in the above animation, the goal of CartPole is to balance a pole thats connected with one joint on top of a moving cart.Instead of pixel information, there are four kinds of information given by the state (such as the angle of the pole and position of the cart). An agent can move the cart by performing a series of actions of 0 or 1, pushing the cart left or right.We will use thekeras-rl library here which lets us implement deep Q-learning out of the box.From the terminal, run the following code block:Assuming you have pip installed, you need to install the following libraries:First, we have to import the necessary modules:Then, set the relevant variables:Next, we will build a very simple single hidden layer neural network model:Now, configure and compile our agent. We will set our policy as Epsilon Greedy and our memory as Sequential Memory because we want to store the result of actions we performed and the rewards we get for each action.Test our reinforcement learning model:This will be the output of our model:Not bad! Congratulations on building your very first deep Q-learning model. OpenAI gym provides several environments fusing DQN on Atari games. Those who have worked with computer vision problems might intuitively understand this since the input for these are direct frames of the game at each time step, the model comprises of convolutional neural network based architecture.There are some more advanced Deep RL techniques, such as Double DQN Networks, Dueling DQN and Prioritized Experience replay which can further improve the learning process. These techniques give us better scores using an even lesser number of episodes. I will be covering these concepts in future articles.I encourage you to try theDQN algorithm on at least 1 environment other than CartPole to practice and understand how you can tune the model to get the best results.",https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/
8 Useful R Packages for Data Science You Arent Using (But Should!),Learn everything about Analytics|Introduction|The R Packages Well Cover in this Article|Data Visualization|DataExplorer|esquisse|Machine Learning|MLR  Machine Learning in R|parsnip|Ranger|purrr|Utilities: Other Awesome R Packages|rtweet|Reticulate|BONUS|Installr|GitHubInstall  An Easy Way to Install R Packages from GitHub|End Notes,"Share this:|Related Articles|A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python|Build your First Multi-Label Image Classification Model in Python|
Analytics Vidhya Content Team
|40 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Im a big fan of R  its no secret. I have relied on it since my days of learning statistics back in university. In fact, R is still my go-to language for machine learning projects.Three things primarily attracted me to R:R offers a plethora of packages for performing machine learning tasks, including dplyr for data manipulation, ggplot2 for data visualization, caret for building ML models, etc.There are even R packages for specific functions, including credit risk scoring, scraping data from websites, econometrics, etc. Theres a reason why R is beloved among statisticians worldwide  the sheer amount of R packages available makes life so much easier.In this article, I will showcase eight R packages that have gone under the radar among data scientists but are incredibly useful for performing specific machine learning tasks. To get you started, I have included an example along with the code for each package.Trust me, your love for R is about to undergo another revolution!I have broadly divided these R packages into three categories:R is an amazing tool for visualizing data.The ease with which we can generate all kinds of plots with just one or two lines of code? Truly a time saver.R provides seemingly countless ways to visualize your data. Even when Im using Python for a certain task, I come back to R for exploring and visualizing my data. Im sure most R users feel the same way!Lets look at a few awesome but lesser-known R packages for performing exploratory data analysis.This is my go-to package for performing exploratory data analysis. From plotting the structure of the data to Q-Q plots and even creating reports for your dataset, this package does it all.Lets see what DataExplorer can do using an example. Consider that we have stored our data in the data variable. Now, we want to figure out the percentage of missing values in every feature present. This is extremely useful when were working with massive datasets and computing the sum of missing values might be time-consuming.You can install DataExplorer using the below code:Now lets see what DataExplorer can do for us:We get a really intuitive plot for missing values:One of my favorite aspects of DataExplorer is the comprehensive report we can generate using just one line of code:Below are the different kinds of factors we get in this report:You can access the full report throughthis link. A VERY useful package.How about a drag-and-drop add-in for generating plots in R? Thats right  esquisse is a package that lets you get on with creating plots without having to code them.Esquisse is built on top of the ggplot2 package. That means you can interactively explore your data in the esquisse environment by generating ggplot2 graphs.Use the below code to install and load up esquisse on your machine:You can also launch the esquisse add-in via the RStudio menu. The user interface of esquisse looks like this:Pretty cool, right? Go ahead and play around with different types of plots  its an eye-opening experience.Ah, building machine learning models in R. The holy grail we data scientists strive for when we take up new machine learning projects. You might have used the caret package for building models before.Now, let me introduce you to a few under-the-radar R packages that might change the way you approach the model building process.One of the biggest reasons Python surged ahead of R was thanks to its machine learning focused libraries (like scikit-learn). For a long time, R lacked this ability. Sure you could use different packages for performing different ML tasks but there was no one package that could do it all. We had to call three different libraries for building three different models.Not ideal.And then the MLR package came along. It is an incredible package which allows us to perform all sorts of machine learning tasks. MLR includes all the popular machine learning algorithms we use in our projects.I strongly recommend going through the below article to deep dive into MLR:Lets see how to install MLR and build a random forest model on the iris dataset:Output:A common issue with different functions available in R (that do the same thing) is that they can have different interfaces and arguments. Take the random forest algorithm for example. The code you would use in the randomforest package and the caret package are different, right?Like MLR,parsnipremoves the problem of referring to multiple packages for a certain machine learning algorithm. It successfully imitates Pythons scikit-learn package in R.Lets look at the below simple example to give you an insight into how parsnip works for a linear regression problem:Output:Ranger is one of my favorite R packages. I regularly use random forests to build baseline models  especially when Im participating in data science hackathons.Heres a question  how many times have you encountered slow random forest computation for huge datasets in R? It happens way too often on my old machine.Packages likecaret, random forests and rf take a lot of time to compute the results. The Ranger package accelerates our model building process for the random forest algorithm. It helps you swiftly create a large number of trees in less amount of time.Lets code a random forest model using Ranger:Output:Quite an impressive performance. You should try out Ranger on more complex datasets and see how much faster your computations become.Exhausted while running your linear regression model on different parts of data and computing the evaluation metrics for each model? The purrr package comes to your rescue.You can also build generalized linear models (glm) for different data pieces and compute p-values for every feature in the form of a list. The advantages of purrr are endless!Lets see an example to understand its functionality. We will build a linear regression model here and subset the R-squared values:OutputSo did you observe? Thisexample uses purrr to solve a fairly realistic problem:Saves us a lot of time, right? Instead of running three different models and three commands to subset the R-squared value, we just use one line of code.Lets look at some other packages that dont necessarily fall under the machine learning umbrella. I have found these useful in terms of working with R in general.Sentiment analysis is one of the most popular applications of machine learning. Its an inescapable reality in todays digital world. And Twitter is a prime target for extracting Tweets and building models to understand and predict sentiment.Now, there are a few R packages for extracting/scraping Tweets and performing sentiment analysis. The rtweet package does the same. So how is it different from the other packages out there?rtweet also helps you check for tweet trends from R itself. Awesome!All users must be authorized to interact with Twitters API.To become authorized, follow the instructions below:1. Make a Twitter app2. Create and save your access tokenFor a detailed step by step procedure to get authentication from Twitter please follow this link here.You can search for tweets with certain hashtags simply by the line of code mentioned below. Lets try and search for all the tweets with the hashtag #avengers since Infinity War is all set for release.You can even access the user IDs of people following a certain page. Lets see an example:You can do a whole lot more with this package. Try it out and do not forget to update the community if you find something exciting.Love coding in R and Python both but want to stick to RStudio? Reticulate is the answer! The package solves this prominent problem by providing a Python interface in R. You can easily use major python libraries like numpy, pandas and matplotlib inside R itself!You can also transfer your progress with data easily from Python to R and R to Python with just one line of code. Isnt that amazing? Check out the below code block to see how easy it is to run python in R.Before you move on to directly installing reticulate in R, you will have to install TensorFlow and Keras first.And you are good to go! Run the commands I have provided above in the screenshot and try out your data science projects in a similar manner.Here are two other utility R packages for all your programming nerds!Do you update your R packages individually? It can be a tedious task, especially when there are multiple packages at play.The InstallR package allows you to update R and all its packages using just one command!Instead of checking the latest version of every package, we can use InstallR to update all the packages in one go.Which package do you use for installing libraries from GitHub? Most of us relied on the devtools package for a long time. It seemed to be the only way. But there was a caveat  we needed to remember the developers name to install a package:With the githubinstall package, the developer name is no longer required.This is by no means an exhaustive list. There are plenty of other R packages which serve useful functions but have been overlooked by the majority.Do you know of any packages that I have missed in this article? Or have you used any of the above-mentioned ones for your project? I would love to hear from you! Connect with me in the comments section below and lets talk R!",https://www.analyticsvidhya.com/blog/2019/04/8-useful-r-packages-data-science/
Build your First Multi-Label Image Classification Model in Python,Learn everything about Analytics|Introduction|Table of Contents|What is Multi-Label Image Classification?|How is Multi-Label Image Classification different from Multi-Class Image Classification?|Steps to Build your Multi-Label Image Classification Model|Understanding the Multi-Label Image Classification Model Architecture|Case Study: Solving a Multi-Label Image Classification Problem|Next Steps and Experimenting on your own|End Notes,"Load and pre-process the data|Define the models architecture|Train the model|Make predictions|Share this:|Related Articles|8 Useful R Packages for Data Science You Arent Using (But Should!)|DataHack Radio #21: Detecting Fake News using Machine Learning with Mike Tamir, Ph.D.|
Pulkit Sharma
|44 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you working with image data? There are so many things we can do using computer vision algorithms:This got me thinking  what can we do if there are multiple object categories in an image? Making an image classification model was a good start, but I wanted to expand my horizons to take on a more challenging task  building a multi-label image classification model!I didnt want to use toy datasets to build my model  that is too generic. And then it struck me  movie/TV series posters contain a variety of people. Could I build my own multi-label image classification model to predict the different genres just by looking at the poster?The short answer  yes! And in this article, I have explained the idea behind multi-label image classification. We will then build our very own model using movie posters. You will be amazed by the impressive results our model generates. And if youre an Avengers or Game of Thrones fan, theres an awesome (spoiler-free) surprise for you in the implementation section.Excited? Good, lets dive in!Lets understand the concept of multi-label image classification with an intuitive example. Check out the below image:The object in image 1 is a car.Thatwas a no-brainer. Whereas, there is no car in image 2  only a group of buildings. Can you see where we are going with this? We have classified the images into two classes, i.e., car or non-car.When we have only two classes in which the images can be classified, this is known as a binary image classification problem.Lets look at one more image:How many objects did you identify? There are way too many  a house, a pond with a fountain, trees, rocks, etc. So,When we can classify an image into more than one class (as inthe image above), it is known as a multi-label image classification problem.Now, heres a catch  most of us get confused between multi-label and multi-class image classification. Even I was bamboozled the first time I came across these terms. Now that I have a better understanding of the two topics, let me clear up the difference for you.Supposewe are givenimages of animalsto be classifiedinto their corresponding categories. For ease of understanding, lets assume there are a total of 4 categories (cat, dog, rabbit and parrot) in which a given image can be classified. Now, there can be two scenarios:Lets understand each scenario through examples, starting with the first one:Here, we have images which contain only a single object. The keen-eyed among you will have noticed there are4 different types of objects (animals)in this collection.Eachimage herecan only be classified either as a cat, dog, parrot or rabbit. There are no instances where a single image will belong to more than one category.1. When there are more than two categories in which the images can be classified, and2. An image does not belong to more than one categoryIf both of the above conditions are satisfied, it is referred to as a multi-class image classification problem.Now, lets consider the second scenario  check out the below images:These are all labels of the givenimages. Each image here belongs to more than one class and hence it is a multi-label image classification problem.These two scenarios should help you understandthe difference between multi-class and multi-label image classification. Connect with me in the comments section below this article if you need any further clarification.Before we jump into the next section, I recommend going through this article Build your First Image Classification Model in just 10 Minutes!. It will help you understand how to solve a multi-class image classification problem.Now that we have an intuition about multi-label image classification, lets dive into the steps you should follow to solve such a problem.The first stepis to get our data in a structured format. This applied to be both binary as well as multi-class image classification.You should have a folder containing all the images on which you want to train your model. Now, for trainingthis model, we also require the true labels of images. So, you should also have a .csv file which contains the names of all the training images and their corresponding true labels.We will learn how to create this .csv file later in this article. For now, just keep in mind that the data should be in a particular format. Once the data is ready, we can divide the further steps as follows:First, load all the images and then pre-process them as per your projects requirement. To check how our model will perform on unseen data (test data), we create a validation set. We train our model on the training set and validate it using the validation set (standard machine learning practice).The next step is to define the architecture of the model. This includes deciding the number of hidden layers, number of neurons in each layer, activation function, and so on.Time to train our model on thetraining set! We pass the training images and their corresponding true labels to train the model. We also pass the validation images here which help us validate how well the model will perform on unseen data.Finally, we use the trained model to get predictions on new images.Now, the pre-processing steps for a multi-label image classification taskwill be similar to that of a multi-class problem. The key difference is in the step where we define the model architecture.We use a softmax activation function in the output layer for a multi-class image classification model. For each image, we want to maximize the probability for a single class. As the probability of one class increases, the probability of the other class decreases. So, we can say that the probability of each class is dependent on the other classes.But in case of multi-label image classification, we can have more than one label for a single image. We want the probabilities to be independent of each other. Using the softmax activation function will not be appropriate. Instead, we can use the sigmoid activation function. This will predict the probability for each class independently. It will internally create n models (n here is the total number of classes), one for each class and predict the probability for each class.Using sigmoid activation function will turn the multi-label problem to n  binary classification problems. So for each image, we will get probabilities defining whether the image belongs to class 1 or not, and so on. Since we have converted it into a n  binary classification problem, we will use the binary_crossentropy loss. Our aim is to minimize this loss in order to improve the performance of the model.This is the major change we have to make while defining the model architecture for solving a multi-label image classification problem. The training part will be similar to that of a multi-class problem. We will pass the training images and their corresponding true labels and also the validation set to validate our models performance.Finally, we will take a new image and use the trained model to predict the labels for this image. With me so far?Congratulations on making it this far! Your reward  solving an awesome multi-label image classification problem in Python. Thats right  time to power up your favorite Python IDE!Lets set up the problem statement.Our aim is to predict the genre of a movie using just its poster image.Can you guess why it is a multi-label image classification problem? Think about it for a moment before you look below.Amovie can belong to more than one genre, right? It doesnt just have to belong to one category, like action or comedy. The movie can be a combination of two or more genres. Hence, multi-label image classification.The dataset well be using contains the poster images of several multi-genre movies. I have made some changes in the dataset and converted it into a structured format, i.e. a folder containing the images and a .csv file for true labels. You can download the structured dataset from here. Below are a few posters from our dataset:You can download the original dataset along with the ground truth values here if you wish.Lets get coding!First, import all the required Pythonlibraries:Now, read the .csv file and look at thefirst five rows:There are 27 columns in this file. Lets print the names of these columns:The genre column contains the list for each image which specifies the genre of that movie. So, from the head of the .csv file, the genre of the first image is Comedy and Drama.Theremaining 25 columnsare the one-hot encoded columns. So, if a movie belongs to the Action genre, its value will be 1, otherwise 0.The image can belong to25 different genres.We will build a model that will return the genre of a given movie poster.But before that, do you remember the first step for building any image classification model?Thats right  loading and preprocessing the data. So, lets read inall the training images:A quick lookat the shape of the array:There are 7254 posterimages and all the images have been converted to a shape of (400, 300, 3). Lets plot and visualize one of the images:This is the poster for the movie Trading Places. Lets also print the genre of this movie:This movie has a single genre  Comedy. The next thing our model would require is the true label(s) for all these images. Can you guess what would be the shape of the true labels for 7254 images?Lets see.We know there are atotal of 25 possible genres. For each image, we will have 25 targets, i.e., whether the movie belongs to that genre or not. So, all these 25 targets will have a value of either 0 or 1.We will remove the Id and genre columns from the train file and convert the remaining columns to an array which will be the target for our images:The shape of the output array is (7254, 25) as we expected. Now, lets create a validation set which will help us check the performance of our model on unseen data. We will randomly separate 10% of the images as ourvalidation set:The next step is to define the architecture of our model. The output layer will have 25 neurons (equal to the number of genres)and well use sigmoid as the activation function.I will be using a certain architecture (given below) to solve this problem. You can modify this architecture as well by changing the number of hidden layers, activation functions and other hyperparameters.Lets print our model summary:Quite a lot of parameters to learn! Now,compile the model.Ill use binary_crossentropy as the loss functionandADAM as the optimizer(again, you can use other optimizers as well):Finally, we are at the most interesting part training the model. We will train the model for 10 epochs and alsopass the validation data which we created earlier in order to validate the models performance:We can see that the training loss has been reduced to 0.24 and the validation loss is also in sync. Whats next? Its time to make predictions!All you Game of Thrones (GoT)and Avengers fans  this ones for you. Lets take the posters for GoT and Avengers and feed them to our model.Download the poster for GOT and Avengers before proceeding.Before making predictions, we need to preprocess these images using the same steps we saw earlier.Now, we will predict the genre for these posters using our trained model. The model will tell us the probability for each genre and we will take the top 3 predictions from that.Impressive! Our model suggests Drama, Thriller and Action genres for Game of Thrones. That classifies GoT pretty well in my opinion. Lets try our model on the Avengers poster. Preprocess the image:And then make the predictions:The genresour model comes up with areDrama, Action and Thriller. Again, these are pretty accurate results. Can the model perform equally well for Bollywood movies ? Lets find out. We will use this Golmal 3 poster.You know what to do at this stage load and preprocess the image:And then predict the genre for this poster:Golmaal 3 was a comedyand our model has predicted it as the topmost genre. The other predicted genres are Drama and Romance a relatively accurate assessment.We can see that the model is able to predict the genres just by seeing their poster.This is how we can solve a multi-label image classification problem. Our model performed really well even though we only had around 7000 images for training it.You can try and collect more posters for training. My suggestion would be to make the dataset in such a way that all the genre categories will have comparatively equal distribution. Why?Well, if a certain genre is repeating in most of the training images, our model might overfit on that genre. And for every new image, the model might predict the same genre. To overcome this problem, you should try to have an equal distribution of genre categories.These are some of the key points which you can try to improve the performance of your model. Any other you can think of? Let me know!There are multiple applications of multi-label image classificationapart from genre prediction. You can use this technique to automatically tagimages, for example. Suppose you want to predict the type and colorof a clothing itemin an image. You can build a multi-label image classification model which will help you to predict both!I hope this article helped you understand the concept of multi-label image classification. If you have any feedback or suggestions, feel free to share them in the comments section below. Happy experimenting!",https://www.analyticsvidhya.com/blog/2019/04/build-first-multi-label-image-classification-model-python/
"DataHack Radio #21: Detecting Fake News using Machine Learning with Mike Tamir, Ph.D.",Learn everything about Analytics|Introduction|The Idea Behind FakerFact|Collecting Data for Training the FakerFact Algorithms and Combating Bias|Updating the Datasets to Keep Up with the Growing Number of Articles|Dealing with Unknown Biases in the Data|Mike Tamirs Industry Experience|The Near Future of Natural Language Processing (NLP)|End Notes,"Share this:|Related Articles|Build your First Multi-Label Image Classification Model in Python|Top 5 Interesting Applications of GANs for Every Machine Learning Enthusiast!|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Fake news is one of the biggest scourges in our digitally connected world. That is no exaggeration. It is no longer limited to little squabbles  fake news spreads like wildfire and is impacting millions of people every day.How do you deal with such a sensitive issue? Millions of articles are being churned out every day on the internet  how do you tell real from fake? Its not as easy as turning to a simple fact checker. They are typically built on a story-by-story basis. Can we turn to machine learning?Its a prevalent and pressing issue  and hence we invited Mike Tamir, Ph.D., as our guest on DataHack Radio. Mike has been working on a project called FakerFact that aims to identify and separate truth from fiction. His teams approach is based on using machine learning algorithms of the Natural Language Processing (NLP) variety.In this episode, Kunal and Mike discuss several aspects of the FakerFact algorithms, including:And much, much more. I would recommend this podcast to EVERY data scientist  it touches on a critical issue plaguing our society.All our DataHack Radio podcast episodes are available on the below platforms  subscribe today!I have summarized the episode discussion in this article. Happy listening!The challenge of misinformationhas been prevalent for years now and we still havent got our arms around it as a society.You might know how difficult it is to detect intent in text if youve worked on NLP projects. The sheer amount of layers in the human language feels overwhelming! To make a machine understand it  thats a lot of effort.Things have been improving however in the last few years. Theres been a huge leap in NLP frameworks. We have demonstrated the ground-breaking developments here. In short, NLP techniques can now parse through the given text and perform all sorts of human-level tasks.FakerFact is Mike Tamirs project which he started with a few fellow researchers acouple of years back. Most fact checkers available online tend to be black and white  they attempt to tell you if a piece of given information is real or fake. FakerFact takes adifferent angle to fact-checking:Can we teach machine learning algorithms to tell the difference between bits of text that are just about education, reporting, etc.versus bits of text that are presenting opinions, using satire, are filled with hate speech, have a hidden agenda, etc.?You can read more here about how FakerFact works and how you can use it in your browser.Thats one of the hardest challenges in data science.Mike and his team start with top-level domains. They use different algorithms for doing a reversebootstrapping process. This helps the team carry down from the domain level to the individual article level for training.One of the most important things they have to pay attention to is stratification. This is pretty understandable  you dont want the model to be biased based on the samples, right? Mike illustrated this point using a brilliant example of right-wing v left-wing articles.As a data scientist, you are going to love this section of the podcast. Its really important for us to understand and mitigate bias right at the start of our data collection process. You can imagine how critical that is for a fact-basedapplication like FakerFact.Most of the fake news datasets we see online are based on certain events, like the 2016 US elections. Thats a very specific sample and can lead to serious bias in the model if used exclusively. Its important to diversify using different domains and time periods.Now, separating the truth from fiction is what FakerFact aims to do. That means it relies on the audience to tell the algorithms whether a particulararticle is credible or not. But can you rely entirely on your audience to generate that insight? No! The Fakerfact team has several strategies in place to mitigate any bias that might come from user feedback.Collecting the data once isnt going to cut it given how quickly information spreads in todays connected world and the number of articles being churned out. Mike and his team constantly update their datasets. Theyre on their fifth iteration right now.We are constantly scraping data. We have millions and millions of articles that are fed into our dataset.Of course, this means that with each update, the team needs to run and check their baseline results all over again. Are they performing at the same level? Do they need to change the architecture? Questions like these are essential to keep FakerFact at the top of the game.Anyone whos worked on even a slightly complicated NLP project knows theres no smooth sailing to building a model. There will be obstacles along the way. You might miss out on a certain point, or an unknown bias might creep in which no one would have thought of in a million years.Mike picked up two examples his team encountered when building the FakerFact model. The first was about authors promoting themselves on Twitter.But its the second example that really stood out for me. On a certain website (name mentioned in the podcast), the FakerFact algorithm consistently pulled up the articles. The team couldnt figure out why  the articles looked like usual journalistic pieces. Can you guess what the issue was?The algorithms were parsing through the comments section of each article. So the results became invariably biased (thats the state in most political or journalistic websites). A classic example of unknown bias.Changing gears, Kunal asked Mike to touch on his rich industry experience, especially his previous role at Uber as the Head of Data Science. I have summarized this part of the podcast below:And finally  where does Mike Tamir see NLP heading in the next few years?Its safe to well continue to see dramatic improvements in how we are able to work on text.2018 was a breakthrough year for NLP. We saw frameworks and libraries like BERT, ULMFiT, Transformer-XL, among others. But the base for that was built in 2017. Going forward, perhaps in the next 2-3 years, Mike said he could see these techniques merging.Its already happening in 2019 and should continue to pick up the pace going forward. Really interesting times lie ahead!Fake news is no laughing matter anymore. It has transformed quickly from being a mere nuisance to costing lives around the world. Any step towards dealing with it in the right way is a welcome sight. I personally quite like FakerFacts approach to this.I loved Mikes ability to explain complicated concepts and tie them together into an understandable format. It certainly helps to know how FakerFact functions under the hood and the different ways the team uses to mitigate bias. Its a goldmine of information for those of us working in NLP.",https://www.analyticsvidhya.com/blog/2019/04/datahack-radio-machine-learning-identify-fake-news-mike-tamir/
Top 5 Interesting Applications of GANs for Every Machine Learning Enthusiast!,Learn everything about Analytics|Introduction|Table of Contents|Introductory Explanation of GANs|Applications of GANs|End Notes,"GANs for Image Editing|Using GANs for Security|Generating Data with GANs|GANs for Attention Prediction|GANs for 3D Object Generation|Share this:|Related Articles|DataHack Radio #21: Detecting Fake News using Machine Learning with Mike Tamir, Ph.D.|Top 5 Machine Learning GitHub Repositories and Reddit Discussions from March 2019|
Faizan Shaikh
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Can you guess whats common among all the faces in this image?None of these people are real! These faces were generated by a computer vision technique called GANs, or Generative Adversarial Networks. Full marks to you if you guessed it correctly!The term GAN was introduced by the Ian Goodfellow in 2014 but the concept has been around since as far back as 1990 (pioneered by Jrgen Schmidhuber). But it was only after Goodfellows paper on the subject that they gained popularity in the community. And since then, theres been no looking back for GANs!In fact, GANs are now ubiquitous. Data scientists and deep learning researchers use this technique to generate photorealistic images, change facial expressions, create computer game scenes, visualize designs, and more recently, even generate awe-inspiring artwork! Thats right  the recent news about AI generated art? It was GANs at work:In this article, we will look at five intriguing applications of GANs that are prevalent in the industry. You might even have come across a few of them without realizing how they worked. I have also provided links to research papers for each GAN application which I encourage you to check out.Strap in  this is going to be fun!Right, we have a sense of what GANs can do. But how do they work? What goes on underneath all the wonderful applications this powerful algorithm produces? Lets understand this using a popular example.Theres a forger (who creates fake artistry) and an investigator tasked with detecting these fake artworks.The task of this forger is to create fraudulent imitations of original paintings by famous artists (like Leonardo Da Vinci). If he/she can pass off this work as the original art piece, the forger can potentially net a lot of money.On the other side of this situation, the art investigators task is to catch these forgers. How does he/she do it? The investigator knows what are the properties which set the original artist apart and what kind of painting he/she would have created. The investigator leverages this knowledge against the piece at hand to check if it is real or not.This contest of forger vs investigator goes on, which ultimately makes world-class investigators (and unfortunately world-class forgers); a battle between good and evil.Now, consider both forger and investigator as robots. When you train the forger to be a painter and the investigator to tell a fake painting from the real one  you now have an algorithmic painter at hand! Thats essentially how GANs work on the inside. Awesome, arent they?I havent got into the intricate details of GANs here. This is just the tip of the iceberg. If you are interested in learning more about GANs, you should go through this article:Now that we have an intuition of how GANs work, lets put on our exploration hats! Its time to dive into the interesting applications of GANs that are commonly used in the industry right now.Most image editing software these days dont give us much flexibility to make creative changes in pictures. For example, lets say you want to change the appearance of a 90-year-old person by changing his/her hairstyle. This cant be done by the current image editing tools out there. But guess what? Using GANs, we can reconstruct images and attempt to change the appearance drastically.This amazing paper demonstrates this very cutting edge application.Another similar application is image de-raining (or literally removing rainy texture from images). Want an example? Check out the below image taken from this paper:The rise of artificial intelligence has been wonderful for most industries. But theres a real concern that has shadowed the entire AI revolution  cyber threats. Even deep neural networks are susceptible to being hacked.A constant concern of industrial applications is that they should be robust to cyber attacks. Theres a lot of confidential information on the line! GANs are proving to be of immense help here, directly addressing the concern of adversarial attacks.These adversarial attacks use a variety of techniques to fool deep learning architectures.GANs are used to make existing deep learning models more robust to these techniques. How? By creating more such fake examples and training the model to identify them. Pretty clever stuff.A technique called SSGAN is used to do steganalysis of images and detect harmful encodings which shouldnt have been there.Who among us wouldnt love to collect more data for building our deep learning model?The availability of data in certain domains is a necessity, especially in domains where training data is needed to model supervideepeeop learning algorithms. The healthcare industry comes to mind here.GANs shine again as they can be used to generate synthetic data for supervision. Thats right! You know where to go next time you need more data. For instance, this paper explores the creation of synthetic data with the help of GANs for training deep learning algorithms by creating realistic eye images.When we see an image, we tend to focus on a particular part (rather than the entire image as a whole). This is called attention and is an important human trait. Knowing where a person would look beforehand would certainly be a useful feature for businesses, as they can optimize and position their products better.For example, game designers can focus on a particular portion of the game to enhance the features and make it more engrossing.This enthralling idea is explored in this paper,where the authors try to identify the most appealing parts of given images using GANs.It wont surprise you to know GANs are quite popular in the gaming industry.Game designers work countless hours recreating 3D avatars and backgrounds to give them a realistic feel. And let me assure you, it certainly takes a lot of effort to create 3D models by imagination.Does this seem unrealistic? Then I suggest you watch this video. You might believe the incredible power of GANs, wherein they can be used to automate the entire process!Here is an open source implementation of the same. Go ahead and try it out if you find the idea interesting!There are a plethora of applications of GANs regularly published in research. Im sure many more will follow soon. I hope the above applications encouraged you to think of more such ideas  perhaps you could come up with your own GAN! If you have any ideas or suggestions, do comment below.Finally, let me share with you a few additional resources in case you want to try things on your own, or just understand how GANs work in general. You can start out with our beginners guide to graspthe basics or refer to the below resources:",https://www.analyticsvidhya.com/blog/2019/04/top-5-interesting-applications-gans-deep-learning/
Top 5 Machine Learning GitHub Repositories and Reddit Discussions from March 2019,Learn everything about Analytics|Introduction|GitHub Repositories|PyTorch Implementation of DeepMinds BigGAN|NVIDIAs SPADE|SiamMask  Fast Online Object Tracking and Segmentation|3-D Human Pose Detection|DeepCamera  Worlds First AutoML Deep Learning Edge AI Platform|Reddit Discussions|Want to Write a Machine Learning Research Paper? Here are a Few Tips and Best Practices|Ever Wondered how to use your Trained sklearn/xgboost/lightgbm Models in Production?||Automated Machine Learning will Radically Change Data Science Roles|Data Scientist Job Hunting Tips|Improving your Business Acumen as a Data Scientist|End Notes,"Share this:|Related Articles|Top 5 Interesting Applications of GANs for Every Machine Learning Enthusiast!|Computer Vision Tutorial: A Step-by-Step Introduction to Image Segmentation Techniques (Part 1)|
Analytics Vidhya Content Team
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"GitHub repositories and Reddit discussions  both platforms have played a key role in my machine learning journey. They have helped me develop my knowledge and understanding of machine learning techniques and business acumen.Both GitHub and Reddit also keep me abreast of the latest developments in machine learning  a MUST for anyone working in this field!And if youre a programmer, well  GitHub is like a temple for you. You can easily download the code and replicate it on your machine. This makes learning new ideas and building a diverse skillset even easier.I am delighted to pick out the top GitHub repositories and Reddit discussions for this month. The Reddit threads I have featured deal with both the technical aspect of machine learning as well as the career-related one. This ability to combine the two is what separates machine learning experts from the amateurs.Below are the monthly articles we have covered so far in this series:So, lets get the ball rolling for March!If I had to pick one reason for my fascination with computer vision, it would be GANs (Generative Adversarial Networks). They were invented by Ian Goodfellow only a few years back and have blossomed into a whole body of research. The recent AI art youve been seeing on the news? Its all powered by GANs.DeepMind came up with the concept of BigGAN last year but we have waited a while for a PyTorch implementation. This repository includes pretrained models (128128, 256256, and 512512) as well. You can install this in just one line of code:And if youre interested in reading the full BigGAN research paper, head over here.The ability to work with image data is becoming a defining trait for anyone interested in deep learning. The advent and rapid bloom of computer vision algorithms has played a significant part in this transformation. It wont surprise you to know that NVIDIA is one of the prime leaders in this domain.Just check out their developments from 2018:And now, NVIDIA folks have come up with another stunning release  the ability to synthesize photorealistic images given an input semantic layout. How good is it? The below comparison provides a nice illustration:SPADE has outperformed existing methods on the popular COCO dataset. The repository we have linked above will host the PyTorch implementation and pretrained models for this technique (be sure to bookmark/star it).This video shows how beautifully SPADE works on 40,000 images scraped from Flickr:This repository is based on the Fast Online Object Tracking and Segmentation: A Unifying Approach paper. Heres a sample result using this technique:Awesome! The technique, called SiamMask, is fairly straightforward, versatile and extremely fast. Oh, did I mention the object tracking is done in real-time? That certainly got my attention. This repository contains pretrained models as well to get you started.The paper will be presented at the prestigious CVPR 2019 (Computer Vision and Pattern Recognition) conference in June. The authors have demonstrated their approach in the below video:Have you ever worked on a pose detection project? I have and let me tell you  it is superb. Its a testament to the progress we have made as a community in deep learning. Who would have thought 10 years ago we would be able to predict a persons next body movement?This GitHub repository is a PyTorchimplementation of the Self-Supervised Learning of 3D Human Pose using Multi-view Geometry paper. The authors have pioneered a new technique called EpipolarPose,a self-supervised learning method for estimating a humans pose in 3D.The EpipolarPose technique estimates 2D poses from multi-view images during the training phase. It then uses epipolar geometry to generate a 3D pose. This, in turn, is used to train the 3D pose estimator. This process is illustrated in the above image.This paper has also been accepted to the CVPR 2019 conference. Its shaping up to be an excellent line-up!This is a unique repository in many ways. Its a deep learning model open sourced to protect your privacy. The entire DeepCamera concept is based on automated machine learning (AutoML). So you dont even need any programming experience to train a new model.DeepCamera works on Android devices. You can integrate the code with surveillance cameras as well. Theres a LOT you can do with DeepCameras code, including:And a whole host of other things. Building your own AI-powered model has never been this easy!I have divided this months Reddit discussions into two categories:Lets start with the technical aspect.Data scientists are fascinated by research papers. We want to read them, code them and perhaps even write one from scratch. How cool would it be to present your own research paper at a top-tier ML conference?I certainly fall in the want to write a research paper category. This discussion, started by a research veteran, delves into the best practices we should follow when writing a research paper. Theres a lot of insight and experience here  a must-read for all of us!Heres the GitHub repository with all the best tips, tricks and ideas in one place. Treat these pointers as a set of guidelines, and not rules written in stone.How do you put your trained machine learning models into production? How do you deploy them? These are VERY common questions you will face in your data science interview (and the job, of course). If you are not sure what this is, I strongly suggest reading about it NOW.This discussion thread is about an open source library that converts your machine learning models into native code (C, Python, Java) with zero dependencies. You should scroll through the thread as there are a few common questions the author has addressed in detail.You can find the full code in this GitHub repository. Below is the list of models this library currently supports:Lets switch focus now and go through some machine learning career discussions. These are applicable to ALL machine learning professionals, aspiring as well as established.Will the emergence of automated machine learning be a disadvantage to the field itself? Thats a question most of us have been wondering about. Most articles I come across predict all doom and gloom. Some even claim that data scientists wont be required in 5 years!Source: ThemocracyThe author of this thread presents a wonderful argument against the general consensus. It is highly unlikely that data science will die out due to automation.The discussion rightly argues that data science is not just about data modeling. That is only 10% of the whole process. An important part of the data science lifecycle is the human intuition behind the models. Data cleaning, data visualization and a hint of logic are what drives this entire process.Heres a gem, and a solid argument, that got my attention:We developed all sorts of statistics software in the last century and yet, it hasnt replaced statisticians.Looking to land your first data science role? Finding it a daunting process? Ive been there. Its one of the biggest obstacles to overcome in our respective data science journeys.Thats why I wanted to highlight this particular thread. Its a really insightful discussion, where data science professionals and beginners discuss how to break into this field. The author of the post offers some in-depth thoughts on the data science job hunt process along with tips to clear each interview round.One sentence that really stood out from this discussion:Remember, the increase in interview requests and increased knowledge is not just a correlation, its a causation. As youre applying, learn something new everyday.We at Analytics Vidhya aim to help you land your first data science role. Check out the below awesome resources that will help you get started:Domain knowledge  that key ingredient in the overall data scientist recipe. Its often overlooked or misunderstood by aspiring data scientists. And that often translates to rejections in interviews. So how can you build up your business acumen to complement your existing technical data science skills?This Reddit discussion offers quite a few useful ideas. The ability to translate your ideas and your results into business terms is VITAL. Most stakeholders youll face in your career will not understand technical jargon.Heres my favorite pick from the discussion:You need to get to know your business partners better. Find out what they do day to day, what their processes are, how they generate the data youre going to use. If you understand how they see X and Y, youll be better able to help them when they come to you with problems.We at Analytics Vidhya strongly believe in building a structured thinking mindset. We have put together our experience and knowledge on this topic in the below comprehensive course:This course contains various case studies which will also help you get an intuition of how businesses work and think.I especially enjoyed the Reddit discussions from last month. I urge you to learn more about how the production environment works in a machine learning project. Its considered almost mandatory now for a data scientist so you cant get away from it.You should also take part in these Reddit discussions. Passive scrolling is good for gaining knowledge but adding your own perspective will help fellow aspirants too! This is an intangible feeling, but one you will cherish and appreciate the more experience you gain.Which discussion did you find the most insightful? And which GitHub repository stood out for you? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
Computer Vision Tutorial: A Step-by-Step Introduction to Image Segmentation Techniques (Part 1),Learn everything about Analytics|Introduction|Table of Contents|What is Image Segmentation?|Why do we need Image Segmentation?|The Different Types of Image Segmentation|Region-based Segmentation|Edge Detection Segmentation|Image Segmentation based on Clustering|Mask R-CNN|Summary of Image Segmentation Techniques|End Notes,"So how does image segmentation work?|Share this:|Related Articles|Top 5 Machine Learning GitHub Repositories and Reddit Discussions from March 2019|16 OpenCV Functions to Start your Computer Vision journey (with Python code)|
Pulkit Sharma
|35 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Whats the first thing you do when youre attempting to cross the road? We typically look left and right, take stock of the vehicles on the road, and make our decision. Our brain is able to analyze, in a matter of milliseconds, what kind of vehicle (car, bus, truck, auto, etc.) is coming towards us. Can machines do that?The answer was an emphatic no till a few years back. But the rise and advancements in computer vision have changed the game. We are able to build computer vision models that can detect objects, determine their shape, predict the direction the objects will go in, and many other things. You might have guessed it  thats the powerful technology behind self-driving cars!Now, there are multiple ways of dealing with computer vision challenges. The most popular approach I have come across is based on identifying the objects present in an image, aka, object detection. But what if we want to dive deeper? What if just detecting objects isnt enough  we want to analyze our image at a much more granular level?As data scientists, we are always curious to dig deeper into the data. Asking questions like these is why I love working in this field!In this article, I will introduce you to the concept of image segmentation. It is a powerful computer vision algorithm that builds upon the idea of object detection and takes us to a whole new level of working with image data. This technique opens up so many possibilities  it has blown my mind.The Part 2 of this series is also live now: Computer Vision Tutorial: Implementing Mask R-CNN for Image Segmentation (with Python Code)If youre new to deep learning and computer vision, I recommend the below resources to get an understanding of the key concepts:Lets understand image segmentation using a simple example. Consider the below image:Theres only one object here  a dog. We can build a straightforward cat-dog classifier model and predict that theres a dog in the given image. But what if we have both a cat and a dog in a single image?We can train a multi-label classifier, in that instance. Now, theres another caveat  we wont know the location of either animal/object in the image.Thats where image localization comes into the picture (no pun intended!). It helps us to identify the location of a single object in the given image. In case we have multiple objects present, we then rely on the concept of object detection (OD). We can predict the location along with the class for each object using OD.Before detecting the objects and even before classifying the image, we need to understand what the image consists of. Enter  Image Segmentation.We can divide or partition the image into various parts called segments. Its not a great idea to process the entire image at the same time as there will be regions in the image which do not contain any information. By dividing the image into segments, we can make use of the important segments for processing the image. That, in a nutshell, is how image segmentation works.An image is a collection or set of different pixels. We group together the pixels that have similar attributes using image segmentation. Take a moment to go through the below visual (itll give you a practical idea of image segmentation): Source : cs231n.stanford.eduObject detection builds a bounding box corresponding to each class in the image. But it tells us nothing about the shape of the object. We only get the set of bounding box coordinates. We want to get more information  this is too vague for our purposes.Image segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image.Why do we need to go this deep? Cant all image processing tasks be solved using simple bounding box coordinates? Lets take a real-world example to answer this pertinent question.Cancer has long been a deadly illness. Even in todays age of technological advancements, cancer can be fatal if we dont identify it at an early stage. Detecting cancerous cell(s) as quickly as possible can potentially save millions of lives.The shape of the cancerous cells plays a vital role in determining the severity of the cancer. You might have put the pieces together  object detection will not be very useful here. We will only generate bounding boxes which will not help us in identifying the shape of the cells.Image Segmentation techniques make a MASSIVE impact here. They help us approach this problem in a more granular manner and get more meaningful results. A win-win for everyone in the healthcare industry. Source: WikipediaHere, we can clearly see the shapes of all the cancerous cells. There are many other applications where Image segmentation is transforming industries:There are even more applications where Image Segmentation is very useful. Feel free to share them with me in the comments section below this article  lets see if we can build something together. We can broadly divide image segmentation techniques into two types. Consider the below images:Can you identify the difference between these two? Both the images are using image segmentation to identify and locate the people present.Let me quickly summarize what weve learned. If there are 5 people in an image, semantic segmentation will focus on classifying all the people as a single instance. Instance segmentation, on the other hand. will identify each of these people individually.So far, we have delved into the theoretical concepts of image processing and segmentation. Lets mix things up a bit  well combine learning concepts with implementing them in Python. I strongly believe thats the best way to learn and remember any topic.One simple way to segment different objects could be to use their pixel values. An important point to note  the pixel values will be different for the objects and the images background if theres a sharp contrast between them.In this case, we can set a threshold value. The pixel values falling below or above that threshold can be classified accordingly (as an object or the background). This technique is known as Threshold Segmentation.If we want to divide the image into two regions (object and background), we define a single threshold value. This is known as the global threshold.If we have multiple objects along with the background, we must define multiple thresholds. These thresholds are collectively known as the local threshold.Lets implement what weve learned in this section. Download this image and run the below code. It will give you a better understanding of how thresholding works (you can use any image of your choice if you feel like experimenting!).First, well import the required libraries.Lets read the downloaded image and plot it:It is a three-channel image (RGB). We need to convert it into grayscale so that we only have a single channel. Doing this will also help us get a better understanding of how the algorithm works.Now, we want to apply a certain threshold to this image. This threshold should separate the image into two parts  the foreground and the background. Before we do that, lets quickly check the shape of this image:(192, 263)The height and width of the image is 192 and 263 respectively. We will take the mean of the pixel values and use that as a threshold. If the pixel value is more than our threshold, we can say that it belongs to an object. If the pixel value is less than the threshold, it will be treated as the background. Lets code this:Nice! The darker region (black) represents the background and the brighter (white) region is the foreground. We can define multiple thresholds as well to detect multiple objects:There are four different segments in the above image. You can set different threshold values and check how the segments are made. Some of the advantages of this method are:But there are some limitations to this approach. When we dont have significant grayscale difference, or there is an overlap of the grayscale pixel values, it becomes very difficult to get accurate segments.What divides two objects in an image? There is always an edge between two adjacent regions with different grayscale values (pixel values). The edges can be considered as the discontinuous local features of an image.We can make use of this discontinuity to detect edges and hence define a boundary of the object. This helps us in detecting the shapes of multiple objects present in a given image. Now the question is how can we detect these edges? This is where we can make use of filters and convolutions. Refer tothis articleif you need to learn about these concepts.The below visual will help you understand how a filter colvolves over an image :Heres the step-by-step process of how this works:The values of the weight matrix define the output of the convolution. My advice  it helps to extract features from the input. Researchers have found that choosing some specific values for these weight matrices helps us to detect horizontal or vertical edges (or even the combination of horizontal and vertical edges).One such weight matrix is the sobel operator. It is typically used to detect edges. The sobel operator has two weight matrices  one for detecting horizontal edges and the other for detecting vertical edges. Let me show how these operators look and we will then implement them in Python.Sobel filter (horizontal) =Sobel filter (vertical) =Edge detection works by convolving these filters over the given image. Lets visualize them on this article.It should be fairly simple for us to understand how the edges are detected in this image. Lets convert it into grayscale and define the sobel filter (both horizontal and vertical) that will be convolved over this image:Now, convolve this filter over the image using the convolve function of the ndimage package from scipy.Lets plot these results:Here, we are able to identify the horizontal as well as the vertical edges. There is one more type of filter that can detect both horizontal and vertical edges at the same time. This is called the laplace operator:Lets define this filter in Python and convolve it on the same image:Next, convolve the filter and print the output:Here, we can see that our method has detected both horizontal as well as vertical edges. I encourage you to try it on different images and share your results with me. Remember, the best way to learn is by practicing!This idea might have come to you while reading about image segmentation. Cant we use clustering techniques to divide images into segments? We certainly can!In this section, well get an an intuition of what clustering is (its always good to revise certain concepts!) and how we can use of it to segment images.Clustering is the task of dividing the population (data points) into a number of groups, such that data points in the same groups are more similar to other data points in that same group than those in other groups. These groups are known as clusters.One of the most commonly used clustering algorithms isk-means. Here, the k represents the number of clusters (not to be confused with k-nearest neighbor). Lets understand how k-means works:The key advantage of using k-means algorithm is that it is simple and easy to understand. We are assigning the points to the clusters which are closest to them.Lets put our learning to the test and check how well k-means segments the objects in an image. We will be usingthis image,so download it, read it and and check its dimensions:Its a 3-dimensional image of shape (192, 263, 3). For clustering the image using k-means, we first need to convert it into a 2-dimensional array whose shape will be (length*width, channels). In our example, this will be (192*263, 3).(50496, 3)We can see that the image has been converted to a 2-dimensional array. Next, fit the k-means algorithm on this reshaped array and obtain the clusters. The cluster_centers_ function of k-means will return the cluster centers and labels_ function will give us the label for each pixel (it will tell us which pixel of the image belongs to which cluster).I have chosen 5 clusters for this article but you can play around with this number and check the results. Now, lets bring back the clusters to their original shape, i.e. 3-dimensional image, and plot the results.Amazing, isnt it? We are able to segment the image pretty well using just 5 clusters. Im sure youll be able to improve the segmentation by increasing the number of clusters.k-means works really well when we have a small dataset. It can segment the objects in the image and give impressive results. But the algorithm hits a roadblock when applied on a large dataset (more number of images).It looks at all the samples at every iteration, so the time taken is too high. Hence, its also too expensive to implement. And since k-means is a distance-based algorithm, it is only applicable to convex datasets and is not suitable for clustering non-convex clusters.Finally, lets look at a simple, flexible and general approach for image segmentation.Data scientists and researchers at Facebook AI Research (FAIR) pioneered a deep learning architecture, called Mask R-CNN, that can create a pixel-wise mask for each object in an image. This is a really cool concept so follow along closely!Mask R-CNN is an extension of the popular Faster R-CNNobject detection architecture. Mask R-CNN adds a branch to the already existing Faster R-CNN outputs.The Faster R-CNN method generates two things for each object in the image:Mask R-CNN adds a third branch to this which outputs the object mask as well. Take alook at the below image to get an intuition of how Mask R-CNN works on the inside:Source: arxiv.orgMask R-CNN is the current state-of-the-art for image segmentation and runs at 5 fps.I have summarized the different image segmentation algorithms in the below table.. I suggest keeping this handy next time youre working on an image segmentation challenge or problem!b. Fast operation speedc. When the object and background have high contrast, this method performs really wellb. k-means is a distance-based algorithm. It is not suitable for clustering non-convex clusters.b. It is also the current state-of-the-art for image segmentationThis article is just the beginning of our journey to learn all about image segmentation. In the next article of this series, we will deep dive into the implementation of Mask R-CNN. So stay tuned!I have found image segmentation quite a useful function in my deep learning career. The level of granularity I get from these techniques is astounding. It always amazes me how much detail we are able to extract with a few lines of code. Ive mentioned a couple of useful resources below to help you out in your computer vision journey:I always appreciate any feedback or suggestions on my articles, so please feel free to connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
16 OpenCV Functions to Start your Computer Vision journey (with Python code),"Learn everything about Analytics|Introduction|Table of Contents|What is Computer Vision?|Why use OpenCV for Computer Vision Tasks?|Reading, Writing and Displaying Images|Changing Color Spaces||Resizing Images|Image Rotation||Image Translation||Simple Image Thresholding||Adaptive Thresholding||Image Segmentation (Watershed Algorithm)|Bitwise Operations||Edge Detection||Image Filtering||Image Contours||Scale Invariant Feature Transform (SIFT)||Speeded-Up Robust Features (SURF)||Feature Matching|Face Detection||End Notes","Share this:|Related Articles|Computer Vision Tutorial: A Step-by-Step Introduction to Image Segmentation Techniques (Part 1)|Infographic: 11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals)|
saurabh pal
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Computer vision is among the hottest fields in any industry right now. It is thriving thanks to the rapid advances in technology and research. But it can be a daunting space for newcomers. There are some common challenges data scientists face when transitioning into computer vision, including:I certainly faced most of these challenges and Im sure most of you must have as well.These are the right questions to ask as a beginner in computer vision  so good news! You are in the right place.In this article, we will answer most of these questions through the awesome OpenCV library. It stands out like a beacon for computer vision tasks and is easily the most popular CV library around.But OpenCV comes with a caveat  it can be a little tough to navigate for newcomers. There are a plethora of functions available inside OpenCV, but it can become daunting to:I personally believe learning how to navigate OpenCV is a must for any computer vision enthusiast. Hence, I decided to write this article detailing the different (common) functions inside OpenCV, their applications, and how you can get started with each one. There is Python code in this article so be ready with your Notebooks!Note: This article assumes you are familiar with computer vision terminology. If youre new to the topic, check out the below resources:Let me quickly explain what computer vision is before we dive into OpenCV. Its good to have an intuitive understanding of what well be talking about through the rest of the article.The ability to see and perceive the world comes naturally to us humans. Its second nature for us to gather information from our surroundings through the gift of vision and perception.Take a quick look at the above image. It takes us less than a second to figure out theres a cat, a dog and a pair of human legs. When it comes to machines, this learning process becomes complicated. The process of parsing through an image and detecting objects involves multiple and complex steps, including feature extraction (edges detection, shapes, etc), feature classification, etc.Computer Vision is a field of deep learning that enables machines to see, identify and process images like humans.Computer vision is one of the hottest fields in the industry right now. You can expect plenty of job openings to come up in the next 2-4 years. The question then is  are you ready to take advantage of these opportunities?Take a moment to ponder this  which applications or products come to your mind when you think of computer vision? The list is HUGE. We use some of them everyday! Features like unlocking our phones using face recognition, our smartphone cameras, self-driving cars  computer vision is everywhere.OpenCV, orOpen Source Computer Vision library, started out as a research project at Intel. Its currently the largest computer vision library in terms of the sheer number of functions it holds.OpenCV contains implementations of more than 2500 algorithms! It is freely available for commercial as well as academic purposes. And the joy doesnt end there! The library has interfaces for multiple languages, including Python, Java,and C++.The first OpenCV version, 1.0, was released in 2006 and the OpenCV community has grown leaps and bounds since then.Now, lets turn our attention to the idea behind this article  the plethora of functions OpenCV offers! We will be looking at OpenCV from the perspective of a data scientist and learning about some functions that make the task of developing and understanding computer vision models easier.Machines see and process everything using numbers, including images and text. How do you convert images to numbers  I can hear you wondering. Two words  pixel values:Every number represents the pixel intensity at that particular location. In the above image, I have shown the pixel values for a grayscale image where every pixel contains only one value i.e. the intensity of the black color at that location.Note that color images will have multiple values for a single pixel. These values represent the intensity of respective channels  Red, Green and Blue channels for RGB images, for instance.Reading and writing images is essential to any computer vision project. And the OpenCV library makes this function a whole lot easier.Now, lets see how to import an image into our machine using OpenCV. Download the image from here.By default, the imread function reads images in the BGR (Blue-Green-Red) format. We can read images in different formats using extra flags in the imread function:A color space is a protocol for representing colors in a way that makes them easily reproducible. We know that grayscale images have single pixel values and color images contain 3 values for each pixel  the intensities of the Red, Green and Blue channels.Most computer vision use cases process images in RGB format. However, applications like video compression and device independent storage  these are heavily dependent on other color spaces, like the Hue-Saturation-Value or HSV color space.As you understand a RGB image consists of the color intensity of different color channels, i.e. the intensity and color information are mixed in RGB color space but in HSV color space the color and intensity information are separated from each other. This makes HSV color space more robust to lighting changes.OpenCV reads a given image in the BGR format by default. So, youll need to change the color space of your image from BGR to RGB when reading images using OpenCV. Lets see how to do that:Machine learning models work with a fixed sized input. The same idea applies to computer vision models as well. The images we use for training our model must be of the same size.Now this might become problematic if we are creating our own dataset by scraping images from various sources. Thats where the function of resizing images comes to the fore.Images can be easily scaled up and down using OpenCV. This operation is useful for training deep learning models when we need to convert images to the models input shape. Different interpolation and downsampling methods are supported by OpenCV, which can be used by the following parameters:OpenCVs resize function uses bilinear interpolation by default.You need a large amount of data to train a deep learning model. Im sure you must have comes across this line of thought in form or another. Its partially true  most deep learning algorithms are heavily dependent on the quality and quantity of the data.But what if you do not have a large enough dataset? Not all of us can afford to manually collect and label images.Suppose we are building an image classification model for identifying the animal present in an image. So, both the images shown below should be classified as dog:But the model might find it difficult to classify the second image as a Dog if it was not trained on such images. So what should we do?Let me introduce you to the technique of data augmentation. This method allows us to generate more samples for training our deep learning model. Data augmentation uses the available data samples to produce the new ones, by applying image operations like rotation, scaling, translation, etc. This makes our model robust to changes in input and leads to better generalization.Rotation is one of the most used and easy to implement data augmentation techniques. As the name suggests, it involves rotating the image at an arbitrary angle and providing it the same label as the original image. Think of the times you have rotated images in your phone to achieve certain angles  thats basically what this function does.Image translation is a geometric transformation that maps the position of every object in the image to a new location in the final output image. After the translation operation, an object present at location (x,y) in the input image is shifted to a new position (X,Y):X = x + dxY = y + dyHere, dx and dy are the respective translations along different dimensions.Image translation can be used to add shift invariance to the model, as by tranlation we can change the position of the object in the image give more variety to the model that leads to better generalizability which works in difficult conditions i.e. when the object is not perfectly aligned to the center of the image.This augmentation technique can also help the model correctly classify images with partially visible objects. Take the below image for example. Even when the complete shoe is not present in the image, the model should be able to classify it as a Shoe.This translation function is typically used in the image pre-processing stage. Check out the below code to see how it works in a practical scenario:Thresholding is an image segmentation method. It compares pixel values with a threshold value and updates it accordingly. OpenCV supports multiple variations of thresholding. A simple thresholding function can be defined like this:if Image(x,y) > threshold , Image(x,y) = 1otherswise, Image(x,y) = 0Thresholding can only be applied to grayscale images.A simple application of image thresholding could be dividing the image into its foreground and background.In case of adaptive thresholding, different threshold values are used for different parts of the image. This function gives better results for images with varying lighting conditions  hence the term adaptive.Otsus binarization method finds an optimal threshold value for the whole image. It works well for bimodal images (images with 2 peaks in their histogram).Image segmentation is the task of classifying every pixel in the image to some class. For example, classifying every pixel as foreground or background. Image segmentation is important for extracting the relevant parts from an image.The watershed algorithm is a classic image segmentation algorithm. It considers the pixel values in an image as topography. For finding the object boundaries, it takes initial markers as input. The algorithm then starts flooding the basin from the markers till the markers meet at the object boundaries.Image Source :- MathworksLets say we have a topography with multiple basins. Now, if we fill different basins with water of different color, then the intersection of different colors will give us the object boundaries. This is the intuition behind the watershed algorithm.Bitwise operations include AND, OR, NOT and XOR. You might remember them from your programming class! In computer vision, these operations are very useful when we have a mask image and want to apply that mask over another image to extract the region of interest.In the above figure, we can see an input image and its segmentation mask calculated using the Watershed algorithm. Further, we have applied the bitwise AND operation to remove the background from the image and extract relevant portions from the image. Pretty awesome stuff!Edges are the points in an image where the image brightness changes sharply or has discontinuities. Such discontinuities generally correspond to:Edges are very useful features of an image that can be used for different applications like classification of objects in the image and localization. Even deep learning models calculate edge features to extract information about the objects present in image.Edges are different from contours as they are not related to objects rather they signify the changes in pixel values of an image. Edge detection can be used for image segmentation and even for image sharpening.In image filtering, a pixel value is updated using its neighbouring values. But how are these values updated in the first place?Well, there are multiple ways of updating pixel values, such as selecting the maximum value from neighbours, using the average of neighbours, etc. Each method has its own uses. For example, averaging the pixel values in a neighbourhood is used for image blurring.Image Source:- GoogleGaussian filtering is also used for image blurring that gives different weights to the neighbouring pixels based on their distance from the pixel under consideration.For image filtering, we use kernels. Kernels are matrices of numbers of different shapes like 3 x 3, 5 x 5, etc. A kernel is used to calculate the dot product with a part of the image. When calculating the new value of a pixel, the kernel center is overlapped with the pixel. The neighbouring pixel values are multiplied with the corresponding values in the kernel. The calculated value is assigned to the pixel coinciding with the center of the kernel.In the above output, the image on the right shows the result of applying Gaussian kernels on an input image. We can see that the edges of the original image are suppressed. The Gaussian kernel with different values of sigma is used extensively to calculate the Difference of Gaussian for our image. This is an important step in the feature extraction process because it reduces the noise present in the image.A contour is a closed curve of points or line segments that represents the boundaries of an object in the image. Contours are essentially the shapes of objects in an image.Unlike edges, contours are not part of an image. Instead, they are an abstract collection of points and line segments corresponding to the shapes of the object(s) in the image.We can use contours to count the number of objects in an image, categorize objects on the basis of their shapes, or select objects of particular shapes from the image.Keypoints is a concept you should be aware of when working with images. These are basically the points of interest in an image. Keypoints are analogous to the features of a given image.They are locations that define what is interesting in the image. Keypoints are important, because no matter how the image is modified (rotation, shrinking, expanding, distortion), we will always find the same keypoints for the image.Scale Invariant Feature Transform (SIFT) is a very popular keypoint detection algorithm. Itconsists of the following steps:Features extracted from SIFT can be used for applications like image stitching, object detection, etc. The below code and output show the keypoints and their orientation calculated using SIFT.Speeded-Up Robust Features (SURF) is an enhanced version of SIFT. It works much faster and is more robust to image transformations. In SIFT, the scale space is approximated using Laplacian of Gaussian. Wait  that sounds too complex. What is Laplacian of Gaussian?Laplacian is a kernel used for calculating the edges in an image. The Laplacian kernel works by approximating a second derivative of the image. Hence, it is very sensitive to noise. We generally apply the Gaussian kernel to the image before Laplacian kernel thus giving it the name Laplacian of Gaussian.In SURF, the Laplacian of Gaussian is calculated using a box filter (kernel). The convolution with box filter can be done in parallel for different scales which is the underlying reason for the enhanced speed of SURF (compared to SIFT). There are other neat improvements like this in SURF  I suggest going through the research paperto understand this in-depth.The features extracted from different images using SIFT or SURF can be matched to find similar objects/patterns present in different images. The OpenCV library supports multiple feature-matching algorithms, like brute force matching, knn feature matching, among others.In the above image, we can see that the keypoints extracted from the original image (on the left) are matched to keypoints of its rotated version. This is because the features were extracted using SIFT, which is invariant to such transformations.OpenCV supports haar cascade based object detection. Haar cascades are machine learning based classifiers that calculate different features like edges, lines, etc in the image. Then, these classifiers train using multiple positive and negative samples.Trained classifiers for different objects like faces,eyes etc are available in the OpenCV Github repo , you can also train your own haar cascade for any object.Make sure you go through the below excellent article that teaches you how to build a face detection model from video using OpenCV:Building a Face Detection Model from Video using Deep Learning (OpenCV Implementation)And if youre looking to learn the face detection concept from scratch, then this article should be of interest.OpenCV is truly an all emcompassing library for computer vision tasks. I hope you tried out all the above codes on your machine  the best way to learn computer vision is by applying it on your own. I encourage you to build your own applications and experiment with OpenCV as much as you can.OpenCV is continually adding new modules for latest algorithms from Machine learning, do check out their Github repository and get familiar with implementation. You can even contribute to the library which is a great way to learn and interact with the community.Are you a computer vision newcomer? Start your journey here:",https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
Infographic: 11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals),Learn everything about Analytics|Introduction,"Share this:|Related Articles|16 OpenCV Functions to Start your Computer Vision journey (with Python code)|DataHack Radio #20: Building Interpretable Machine Learning Models with Christoph Molnar|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Do you often work with reports in Excel? Or regularly build dashboards and visualizations in Tableau or Power BI? If you answered yes to either question  and you want to transition into data science  youve come to the right place!Business Intelligence (BI) professionals hold a sizeable advantage over most other folks transitioning into data science. Think about it:We face this question from a lot of BI/MIS/reporting professionals. That prompted us to list down our thoughts in this 11 step learning path.In addition, Sunil Ray has explored each step in a lot more detail in this excellent article. He has expertly provided real-world examples you can relate to and also highlighted how you can overcome challenges at each step during your transition into data science. The article got a very positive feedback from our community so make sure you check it out.The infographic detailing each of these steps is provided below. Download it, print it out and keep it handy during your data science transition journey! And if you have any feedback or want to share your experience with us, go ahead and let us know in the comments section below this article.Note:The terms Business Intelligence, MIS, reporting, dashboarding have been used interchangeably in the infographic. There is very little difference and a lot of overlap in these roles and designations.",https://www.analyticsvidhya.com/blog/2019/03/infographic-11-steps-transition-data-science-reporting-mis-bi/
DataHack Radio #20: Building Interpretable Machine Learning Models with Christoph Molnar,Learn everything about Analytics|Introduction|Christoph Molnars Background|Interest in Interpretable Machine Learning|Machine Learning Interpretability Research|End Notes,"Share this:|Related Articles|Infographic: 11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals)|8 Excellent Pretrained Models to get you Started with Natural Language Processing (NLP)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"How do we build interpretable machine learning models? Or, in other words, how do we build trust in the models we design? This is such a critical question in every machine learning project. But we tend to overlook this in our haste to build more accurate models.Take a moment to think about this  how many times have you turned to complex techniques like ensemble learning and and neural networks to improve your models accuracy while sacrificing interpretability? In a real-world industry/business setting, this is unacceptable.We need to find a way to use these powerful ML algorithms and still make them work in business setting. So in this episode #20 of our DataHack Radio podcast, we welcome Christoph Molar, author of the popular book  Interpretable Machine Learning. Who better to talk about this fundamental and critical topic?This DataHack Radio episode is full of essential machine learning aspects every data scientist, manager, team lead and senior executive must be aware of. Kunal Jain and Christoph had a multi-layered conversation on several topics, including:All our DataHack Radio podcast episodes are available on the below platforms. Subscribe today to stay updated on all the latest machine learning developments!Statistics is at the core of data science. You cannot simply waltz into the machine learning world without building a solid base in statistics first.Christophs background, especially his university education, embodies that thought. He has a rich background in statistics. Both his Bachelors and Masters degrees were in the area of statistics from theLudwig-Maximilians Universitt Mnchen in Germany.During this stretch, Christoph came across machine learning and was instantly fascinated with it. He started taking part in online ML competitions and hackathons. It didnt take him long to figure out that linear regression, useful as it was in terms of learning, wasnt going to cut in these hackathons.So, he delved deeper into the domain. Decision trees, random forest, ensemble learning  Christoph didnt leave any stone unturned in his quest to learn and master these algorithms. Once he finished his Masters, he worked as a Statistical Consultant for a couple of years in the medical domain before stints at a few other organizations.During this period, Christoph was also doing his own research on the side. Any thoughts on what his area of interest was? You guessed it  interpretable machine learning.Source: xkcdInterpretable machine learning is not a topic we come across often when were learning this domain (or even working on it). Everyone knows about it yet only a few truly discuss it. So what triggered Christophs interest in this area of research?For Christoph, it all hearkens back to his university days. This was taught as part of his statistics education. So when he was learning about a certain topic, like linear or logistic regression, he learned it from the ground up. That involved not just learning how to built a model, but also how to interpret the inner workings that generated the final output.A big reason for delving into interpretable ML was Christophs experience with non-machine learning folks (and Im sure everyone would have experienced this at some point):I used to ask people Why dont you use machine learning for the problem youre working on? The answer was always We cant explain how it works. Management will not accept a black box model.If that sounds familiar, youre not alone! This inability to understand how models work is quite prevalent in the industry. No wonder a lot of machine learning projects fail before theyve had a chance of picking up steam.All of this learning naturally translated into Christophs machine learning forays. He started exploring methods to make machine learning models interpretable, including looking at projects, reading research papers, etc. One of the methods he came across is called LIME, or Locally Interpretable Model-Agnostic Explanations. We have an excellent article around this you should check out:According to Christoph, there wasnt one specific blog or tutorial which emphasized interpretable machine learning across techniques. And that was how the idea of writing a book on the topic was born.Christophs research into machine learning interpretability is focused on model-agnostic methods (as opposed to model-specific methods). The former approach is more generalizable in nature while the latter deep dives into the model at hand.For the model-agnostic methods, they work by changing the features of the input data and observing how the predictions change. For example, how much does the performance of a model drop if we remove a feature? This helps understand feature importance as well. Youll have come across this concept while learning the random forest technique.You might be inclined to think  wouldnt model-specific methods be better? Its a fair question. The advantage of model-agnostic methods is that they adapt to the evolving spectre of machine learning models. This is applicable to complex techniques like neural networks, and even those that havent become mainstream yet.If youre a R user, make sure you check out Christophs interpretable machine learning package called iml. You can find it here.Here, Christoph mentioned a very valid point about the definition of interpretability. Everyone seems to have a different understanding of the concept. A business user might be happy with an overview of how the model works, another user might want to fully grasp each step the model took to produce the final result. That fuzziness is a challenge for any researcher.I strongly believe this topic should be covered in every machine learning course or training. We simply cannot walk into an industry setting and start building a complex web of models without being able to explain how they work.Can you imagine a self-driving car malfunctioning and the developers struggling to understand where their code went wrong? Or a model detecting an illness where none exists?I hope to see more traction on this in the coming days and weeks. Until then, make sure you listen to this episode and share it with your network. I look forward to hearing your thoughts and feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2019/03/datahack-radio-interpretable-machine-learning-christoph-molnar/
8 Excellent Pretrained Models to get you Started with Natural Language Processing (NLP),Learn everything about Analytics|Introduction|Pretrained NLP Models Covered in this Article|Multi-Purpose NLP Models|ULMFiT|Transformer|Googles BERT|Googles Transformer-XL|OpenAIs GPT-2|Word Embeddings|ELMo|Flair|Other Pretrained Models|StanfordNLP|End Notes,"Why use pretrained models?|Resources to learn and read more about ULMFiT:|Resources to learn and read more about Transformer:|Resources to learn and read more about BERT:|Resources to learn and read more about Transformer-XL:|Resources to learn and read more about GPT-2:|Resources to learn and read more about ELMo:|Resources to learn and read more about Flair:|Resources to learn and read more about StanfordNLP:|Share this:|Related Articles|DataHack Radio #20: Building Interpretable Machine Learning Models with Christoph Molnar|5 Amazing Deep Learning Frameworks Every Data Scientist Must Know! (with Illustrated Infographic)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Natural Language Processing (NLP) applications have become ubiquitous these days. I seem to stumble across websites and applications regularly that are leveraging NLP in one form or another. In short, this is a wonderful time to be involved in the NLP domain.This rapid increase in NLP adoption has happened largely thanks to the concept of transfer learningenabled through pretrained models. Transfer learning, in the context of NLP, is essentially the ability to train a model on one dataset and then adapt that model to perform different NLP functions on a different dataset.This breakthrough has made things incredibly easy and simple for everyone, especially folks who dont have the time or resources to build NLP models from scratch. Its perfect for beginners as well who want to learn or transition into NLP.In this article, I have showcased the top pretrained models you can use to start your NLP journey and replicate the state-of-the-art research in this field. You can check out my article on the top pretrained models in Computer Vision here.If you are a beginner in NLP, I recommend taking our popular course  NLP using Python.I have classified the pretrained models into three different categories based on their application:Multi-purpose models are the talk of the NLP world. These models power the NLP applications we are excited about  machine translation, question answering systems, chatbots, sentiment analysis, etc. A core component of these multi-purpose NLP models is the concept of language modelling.In simple terms, the aim of a language model is to predict the next word or character in a sequence. Well understand this as we look at each model here.If youre a NLP enthusiast, youre going to love this section. Now, lets dive into 5 state-of-the-art multi-purpose NLP model frameworks. I have provided links to the research paper and pretrained models for each model. Go ahead and explore them!ULMFiT was proposed and designed byfast.ais Jeremy Howard and DeepMinds Sebastian Ruder. You could say that ULMFiT was the release that got the transfer learning party started last year.As we have covered in this article, ULMFiT achieves state-of-the-art results using novel NLP techniques. This method involves fine-tuning a pretrained language model, trained on theWikitext 103 dataset, to a new dataset in such a manner that it does not forget what it previously learned.ULMFiT outperforms numerous state-of-the-art on text classification tasks. What I liked about ULMFiT is that it needs very few examples to produce these impressive results. Makes it easier for folks like you and me to understand and implement it on our machines!In case you were wondering, ULMFiT stands forUniversal Language Model Fine-Tuning. The word Universal is quite apt here  the framework can be applied to almost any NLP task.The Transformer architecture is at the core of almost all the recent major developments in NLP. It was introduced in 2017 by Google. Back then, recurrent neural networks (RNN) were being used for language tasks, like machine translation and question answering systems.This Transformer architecture outperformed both RNNs and CNNs (convolutional neural networks). The computational resources required to train models were reduced as well. A win-win for everyone in NLP. Check out the below comparison:As per Google, Transformer applies a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their respective position. It does so using a fixed-sized context (aka the previous words). Too complex to get? Lets take an example to simplify this.She found the shells on the bank of the river. The model needs to understand that bank here refers to the shore and not a financial institution. Transformer understands this in a single step. I encourage you to read the full paper I have linked below to gain an understanding of how this works. It will blow your mind.The below animation wonderfully illustrates how Transformer works on a machine translation task:Google released an improved version of Transformer last year called Universal Transformer. Theres an even newer and more intuitive version, called Transformer-XL, which we will cover below.The BERT framework has been making waves ever since Google published their results, and then open sourced the code behind it. We can debate whether this marks a new era in NLP, but theres not a shred of doubt that BERT is a very useful framework that generalizes well to a variety of NLP tasks.BERT, short forBidirectionalEncoderRepresentations, considers the context from both sides (left and right) of a word. All previous efforts considered one side of a word at a time  either the left or the right. This bidirectionality helps the model gain a much better understanding of the context in which the word(s) was used. Additionally, BERT is designed to do multi-task learning, that is, it can perform different NLP tasks simultaneously.BERT is the firstunsupervised,deeply bidirectionalsystem for pretraining NLP models. It was trained using only a plain text corpus.At the time of its release, BERT was producing state-of-the-art results on 11 Natural Language Processing (NLP) tasks. Quite a monumental feat! You can train your own NLP model (such as a question-answering system) using BERT in just a few hours (on a single GPU).This release by Google could potentially be a very important one in the long-term for NLP. This concept could become a bit tricky if youre a beginner so I encourage you to read it a few times to grasp it. I have also provided multiple resources below this section to help you get started with Transformer-XL.Picture this  youre halfway through a book and suddenly a word or sentence comes up that was referred to at the start of the book. Now, you or I can recall what it was. But a machine, understandably, struggles to model long-term dependency.One way to do this, as we saw above, is by using Transformers. But they are implemented with a fixed-length context. In other words, theres not much flexibility to go around if you use this approach.Transformer-XL bridges that gap really well. Developed by the Google AI team, it is a novel NLP architecture that helps machines understand context beyond that fixed-length limitation.Transformer-XL is up to 1800 times faster than a typical Transformer.Youll understand this difference through the below 2 GIFs released by Google:Vanilla TransformerTransformer-XLTransformer-XL, as you might have predicted by now, achieves new state-of-the-art results on various language modeling benchmarks/datasets. Heres a small table taken from their page illustrating this:The Transformer-XL GitHub repository, linked above and mentioned below, contains the code in both PyTorch and TensorFlow.Now, this is a pretty controversial entry. A few people might argue that the release of GPT-2 was a marketing stunt by OpenAI. I certainly understand where theyre coming from. However, I believe its important to still at least try out the code OpenAI has released.First, some context for those who are not aware what Im talking about. OpenAI penned a blog post (link below) in February where they claimed to have designed a NLP model, called GPT-2, that was so good that they couldnt afford to release the full version for fear of malicious use. That certainly got the communitys attention.GPT-2 was trained to predict the next occurring word in 40GB of internet text data. This framework is also a transformer-based model trained on a dataset of 8 million web pages. The results they have published on their site are nothing short of astounding. The model is able to weave an entirely legible story based on a few sentences we input. Check out this example:Incredible, right?The developers have released a much smaller version of GPT-2 for researchers and engineers to test. The original model has 1.5 billion parameters  the open source sample model has 117 million.Most of the machine learning and deep learning algorithms we use are incapable of working directly with strings and plain text. These techniques require us to convert text data into numbers before they can perform any task (such as regression or classification).So in simple terms, word embeddings are the text blocks that are converted into numbers for performing NLP tasks.A word bmbedding format generally tries to map a word using a dictionary to a vector.You can get a much more in-depth explanation of word embeddings, its different types, and how to use them on a dataset in the below article. If you are not familiar with the concept, I consider this guide a must-read:In this section, well look at two state-of-the-art word embeddings for NLP. I have also provided tutorial links so you can get a practical understanding of each topic.No, this ELMo isnt the (admittedly awesome) character from Sesame Street. But this ELMo, short for Embeddings from Language Models, is pretty useful in the context of building NLP models.ELMo is a novel way of representing words in vectors and embeddings. These ELMo word embeddings help us achieve state-of-the-art results on multiple NLP tasks, as shown below:Lets take a moment to understand how ELMo works. Recall what we discussed about bidirectional language models earlier. Taking a cue from this article, ELMo word vectors are computed on top of a two-layer bidirectional language model (biLM). This biLM model has two layers stacked together. Each layer has 2 passes  forward pass and backward pass:ELMo word representations consider the full input sentence for calculating the word embeddings. So, the term read would have different ELMo vectors under different context. A far cry from the older word embeddings when the same vector would be assigned to the word read regardless of the context in which it was used.Flair is not exactly a word embedding, but a combination of word embeddings. We can call Flair more of a NLP library that combines embeddings such as GloVe, BERT, ELMo, etc. The good folks at Zalando Research developed and open-sourced Flair.The team has released several pretrained models for the below NLP tasks:Not convinced yet? Well, this comparison table will get you there:Flair Embedding is the signature embedding that comes packaged within the Flair library. It is powered by contextual string embeddings.You should go through this article to understand the core components that power Flair.What I especially like about Flair is that it supports multiple languages. So many NLP releases are stuck doing English tasks. We need to expand beyond this if NLP is to gain traction globally!Speaking of expanding NLP beyond the English language, heres a library that is already setting benchmarks. The authors claim that StanfordNLP supports over 53 languages  that certainly got our attention!Our team was among the first to work with the library and publish the results on a real-world dataset. We played around with it and found that StanfordNLP truly does open up a lot of possibilities of applying NLP techniques on non-English languages. like Hindi, Chinese and Japanese.StanfordNLP is a collection of pretrained state-of-the-art NLP models.These models arent just lab tested  they were used by the authors in the CoNLL 2017 and 2018 competitions. All the pretrained NLP models packaged in StanfordNLP are built on PyTorch and can be trained and evaluated on your own annotated data.The two key reasons we feel you should consider StanfordNLP are:This is by no means an exhaustive list of pretrained NLP models. There are a lot more available and you can check out a few of them on this site.Here are a couple of useful resources for learning NLP:I would love to hear your thoughts on this list. Have you used any of these pretrained models before? Or you have perhaps explored other options? Let me know in the comments section below  I will be happy to check them out and add them to this list.",https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/
5 Amazing Deep Learning Frameworks Every Data Scientist Must Know! (with Illustrated Infographic),Learn everything about Analytics|Introduction|Table of Contents|What is a Deep Learning Framework?|TensorFlow|Keras|PyTorch|Caffe|Deeplearning4j|Comparing these 5 Deep Learning Frameworks|End Notes & Illustrated Infographic,"TensorFlow|Keras|PyTorch|Caffe|Deeplearning4j|Share this:|Related Articles|8 Excellent Pretrained Models to get you Started with Natural Language Processing (NLP)|A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text|
Pulkit Sharma
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I have been a programmer since before I can remember. I enjoy writing codes from scratch  this helps me understand that topic (or technique) clearly. This approach is especially helpful when were learning data science initially.Try to implement a neural network from scratch and youll understand a lot of interest things. But do you think this is a good idea when building deep learning models on a real-world dataset? Its definitely possible if you have days or weeks to spare waiting for the model to build.For those of us who dont have access to infinite computational resources, youve come to the right place.Heres the good news  we now have easy-to-use, open source deep learning frameworks that aim to simplify the implementation of complex and large-scale deep learning models. Using these amazing frameworks, we can implement complex models like convolutional neural networks in no time.In this article, we will look at 5 super useful deep learning frameworks, their advantages, and their applications. Well compare each framework to understand when and where we can use each one.We have created a really cool infographic as well which espouses the value of each deep learning framework. Its available at the end of this article and is a must-have for every data scientist.Lets understand this concept using an example. Consider the below collection of images:There are various categories in this image  Cat, Camel, Deer, Elephant, etc. Our task is to classify these images into their corresponding classes (or categories). A quick Google search tells us that Convolutional Neural Networks (CNNs) are very effective for such image classification tasks.So all we need to do is implement the model, right? Well, if you start to code a Convolutional Neural Network from scratch, it will take days (or even weeks) until you get a working model. We cant afford to wait that long!This is where deep learning frameworks have truly changed the landscape.A deep learning framework is an interface, library or a tool which allows us to build deep learning models more easily and quickly, without getting into the details of underlying algorithms. They provide a clear and concise way for defining models using a collection of pre-built and optimized components.Instead of writing hundreds of lines of code, we can use a suitable framework to help us to build such a model quickly. Below are some of the key features of a good deep learning framework:These is the criteria I used to pick out my top 5 deep learning frameworks. Lets dive into each of them in detail.
TensorFlow was developed by researchers and engineers from the Google Brain team. It is far and away the most commonly used software library in the field of deep learning (though others are catching up quickly).The two things I really like about TensorFlow  its completely open source and has excellent community support. TensorFlow has pre-written codes for most of the complex deep learning models youll come across, such as Recurrent Neural Networks and Convolutional Neural Networks.One of the biggest reasons TensorFlow is so popular is its support for multiple languages to create deep learning models, such as Python, C++ and R. It has proper documentations and walkthroughs for guidance (did you really expect shabby work from Google?).There are numerous components that go into making TensorFlow. The two standout ones are:The flexible architecture of TensorFlow enables us to deploy our deep learning models on one or more CPUs (as well as GPUs). Below are a few popular use cases of TensorFlow:There are many more use cases. If you have used TensorFlow outside the applications Ive mentioned above, I would love to hear from you! Let me know in the comments section below this article and well discuss.Installing TensorFlow is also a pretty straightforward task.For CPU-only:For CUDA-enabled GPU cards:Learn how to build a neural network model using TensorFlow from the below comprehensive tutorials:
Are you comfortable using Python? If yes, then youll instantly connect with Keras. It is the perfect framework for you to start your deep learning journey.Keras is written in Python and can run on top of TensorFlow (as well as CNTK and Theano). The TensorFlow interface can be a bit challenging as it is a low-level library and new users might find it difficult to understand certain implementations.Keras, on the other hand, is a high-level API, developed with a focus to enable fast experimentation. So if want quick results, Keras will automatically take care of the core tasks and generate the output. Both Convolutional Neural Networks and Recurrent Neural Networks are supported by Keras. It runs seamlessly on CPUs as well as GPUs.A common complaint from deep learning beginners is that they are unable to properly understand complex models. If youre one such user, Keras is for you! It is designed to minimize user actions and makes it really easy to understand models.We can broadly classify models in Keras into two categories:Keras has multiple architectures, mentioned below, for solving a wide variety of problems. This includes one of my all-time favorites  image classification!You can refer to the official Keras documentationto get a detailed understanding of how the framework works.You can install Keras with just one line of code:Intrigued by Keras? Continue your learning with the below tutorial, where youll understand how to implement a neural network using Keras:Remember when we said TensorFlow is the most commonly used deep learning framework right now? It might not hold that mantle for too long given the rapid pace with which data scientists and developers are embracing Facebooks PyTorch.I am a huge PyTorch advocate. Among all the frameworks I have worked on, PyTorch is the most flexible.PyTorch is a port to the Torch deep learning framework which can be used for building deep neural networks and executing tensor computations. Torch is a Lua-based framework whereas PyTorch runs on Python.PyTorch is a Python package which provides Tensor computations. Tensors are multidimensional arrays just like numpys ndarrays which can run on GPU as well. PyTorch uses dynamic computation graphs. Autograd package of PyTorch builds computation graphs from tensors and automatically computes gradients.Instead of predefined graphs with specific functionalities, PyTorch provides a framework for us to build computational graphs as we go, and even change them during runtime. This is valuable for situations where we dont know how much memory is going to be required for creating a neural network.You can work on all sorts of deep learning challenges using PyTorch, including:If youre wondering how to install PyTorch on your machine, hold on for a moment. The installation steps vary depending on your operating system, the package you want to use to install PyTorch, the tool/language youre working with, CUDA and a few other dependencies.Check the installation steps of PyTorch for your machine here. Once youre ready with the framework, check out the below two resources to build your first neural network using PyTorch:Caffe is another popular deep learning framework geared towards the image processing field. It was developed byYangqing Jia during his Ph.D at the University of Claifornia, Berkeley. And yes, its open source as well!First, a caveat  Caffes support for recurrent networks and language modeling is not as great as the above three frameworks. But where Caffe stands out is its speed of processing and learning from images. That is easily its primary USP.Caffe can process over sixty million images on a daily basis with a single NVIDIA K40 GPU. Thats 1 ms/image for inference and 4 ms/image for learning.It provides solid support for interfaces like C, C++, Python, MATLAB as well as the traditional command line.The Caffe Model Zoo framework allows us to access pretrained networks, models and weights that can be applied to solve deep learning problems. These models work on the below tasks:You can check the installation of caffe and the documentation for more details.
Any Java programmers in our community? Heres your ideal deep learning framework! Deeplearning4j is implemented in Java and is hence more efficient as compared to Python. It uses the tensor library called ND4J which provides an ability to work with n-dimensional arrays (also called tensors). This framework also supports both CPUs and GPUs.Deeplearning4j treats the task of loading data and training algorithms as separate processes. This separation of functions provides a whole lot of flexibility. And who wouldnt like that, especially in deep learning?!Deeplearning4j works with different data types as well:The kind of deep learning models you can build using Deeplearning4j are:Go through the installation steps and documentation of Deeplearning4j to get started with this framework.We have covered five of the most popular deep learning frameworks out there. Each has its own unique set of features which is why data scientists go for one over the other.Have you decided which one you want to use? Or perhaps you are planning to switch to a completely new framework? Whatever the case, its important to understand the advantages as well as limitations of each framework. You dont want to end up surprised if you face an error at some point!Some frameworks work extremely well with image data but fail to parse through text data. Other frameworks perform well with both image and text data but their inner working can be difficult to understand.In this section, we will compare our five deep learning frameworks using the below criteria:The table below compares these frameworks:Its a pretty handy table for the next time youre working with these frameworks!All of these frameworks are open source, support CUDA and have pretrained models to help you get started. But, what should be the right starting point and which framework should you choose to build your (initial) deep learning models? Lets discuss!Well start with TensorFlow. TensorFlow works well on images as well as sequence-based data. If you are a beginner in deep learning, or dont have a solid understanding of mathematical concepts like linear algebra and calculus, then the steep learning curve of TensorFlow might be daunting.I totally understand that this aspect can be complex for folks who are just starting out. My suggestion would be to keep practicing, keep exploring the community, and keep reading articles to get the hang of TensorFlow.Once you have a good understanding of the framework, implementing deep learning models will be very easy for you.Keras is a pretty solid framework to start your deep learning journey. If you are familiar with Python and are not doing some high-level research or developing some special kind of neural network, Keras is for you.The focus is more on achieving results rather than getting bogged down by the model intricacies. So if you are given a project related to, say image classification or sequence models, start with Keras. You will be able to get a working model very quickly.Keras is also integrated in TensorFlow and hence you can also build your model using tf.keras.As compared to TensorFlow, PyTorch is more intuitive. One quick project with both these frameworks will make that abundantly clear.Even if you dont have a solid mathematics or a pure machine learning background, you will be able to understand PyTorch models. You can define or manipulate the graph as the model proceeds which makes PyTorch more intuitive.PyTorch does not have any visualization tool like TensorBoard but you can always use a library like matplotlib. I wouldnt say PyTorch is better than TensorFlow, but both these deep learning frameworks are incredibly useful.Caffe works very well when were building deep learning models on image data. But when it comes to recurrent neural networks and language models, Caffe lags behind the other frameworks we have discussed. The key advantage of Caffe is that even if you do not have strong machine learning or calculus knowledge, you can build deep learning models.Caffe is primarly used for building and deploying deep learning models for mobile phones and other computationally constrained platforms.Like I mentioned before, Deeplearning4j is a paradise for Java programmers. It offers massive support for different neural networks like CNNs, RNNs and LSTMs. It can process a huge amount of data without sacrificing speed. Sounds like too good an opportunity to pass up!Are there any other deep learning frameworks youve worked on? I would love to hear your thoughts and feedback on that plus the ones we covered in this article. Connect with me in the comments section below.And remember, these frameworks are essentially just tools that help us get to the end goal. Choosing them wisely can reduce a lot of effort and time.As promised, here is the infographic with detailed about each deep learning framework we have covered. Download it, print it, and use it next time youre building a deep learning model!",https://www.analyticsvidhya.com/blog/2019/03/deep-learning-frameworks-comparison/
A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text,"Learn everything about Analytics|Introduction|Table of Contents|What is ELMo?|Understanding how ELMo works|How is ELMo different from other word embeddings?|Implementation: ELMo for Text Classification in Python|What else we can do with ELMo?|End Notes
","Embeddings from Language Models (ELMo)|1. Understanding the Problem Statement|2. About the Dataset
|3. Import Libraries
|4. Read and Inspect the Data
|5. Text Cleaning and Preprocessing|6. Brief Intro to TensorFlow Hub||7. PreparingELMo Vectors|8. Model Building and Evaluation|Share this:|Related Articles|5 Amazing Deep Learning Frameworks Every Data Scientist Must Know! (with Illustrated Infographic)|29 Inspiring Women Blazing a Trail in the Data Science World|
Prateek Joshi
|26 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I work on differentNatural Language Processing (NLP) problems (the perks of being a data scientist!). Each NLP problem is a unique challenge in its own way. Thats just a reflection of how complex, beautiful and wonderful the human language is.But one thing has always been a thorn in an NLP practitioners mind is the inability (of machines) to understand the true meaning of a sentence. Yes, Im talking about context. Traditional NLP techniques and frameworks were great when asked to perform basic tasks. Things quickly went south when we tried to add context to the situation.The NLP landscape has significantly changed in the last 18 months or so. NLP frameworks like Googles BERT and Zalandos Flair are able to parse through sentences and grasp the context in which they were written.One of the biggest breakthroughs in this regard came thanks to ELMo, a state-of-the-art NLP framework developed by AllenNLP. By the time you finish this article, you too will have become a big ELMo fan  just as I did.In this article, we will explore ELMo (Embeddings from Language Models) and use it to build a mind-blowing NLP model using Python on a real-world dataset.Note: This article assumes you are familiar with the different types of word embeddings and LSTM architecture. You can refer to the below articles to learn more about the topics:No, the ELMo we are referring to isnt the character from Sesame Street! A classic example of the importance of context.ELMo is a novel way to represent words in vectors or embeddings. These word embeddings are helpful in achieving state-of-the-art (SOTA) results in several NLP tasks:NLP scientists globally have started using ELMo for various NLP tasks, both in research as well as the industry. You must check out the original ELMo research paper here https://arxiv.org/pdf/1802.05365.pdf. I dont usually ask people to read research papers because they can often come across as heavy and complex but Im making an exception for ELMo. This one is a really cool explanation of how ELMo was designed.Lets get an intuition of how ELMo works underneath before we implement it in Python. Why is this important?Well, picture this. Youve successfully copied the ELMo code from GitHub into Python and managed to build a model on your custom text data. You get average results so you need to improve the model. How will you do that if you dont understand the architecture of ELMo? What parameters will you tweak if you havent studied about it?This line of thought applies to all machine learning algorithms. You need not get into their derivations but you should always know enough to play around with them and improve your model.Now, lets come back to how ELMo works.As I mentioned earlier, ELMo word vectors are computed on top of a two-layer bidirectional language model (biLM). This biLM model has two layers stacked together. Each layer has 2 passes  forward pass and backward pass:As the input to the biLM is computed from characters rather than words, it captures the inner structure of the word. For example, the biLM will be able to figure out that terms like beauty and beautiful are related at some level without even looking at the context they often appear in. Sounds incredible!Unlike traditional word embeddings such as word2vec and GLoVe, the ELMo vector assigned to a token or word is actually a function of the entire sentence containing that word. Therefore, the same word can have different word vectors under different contexts.I can imagine you asking  how does knowing that help me deal with NLP problems? Let me explain this using an example.Suppose we have a couple of sentences:Take a moment to ponder the difference between these two. The verb read in the first sentence is in the past tense. And the same verb transforms into present tense in the second sentence. This is a case of Polysemy wherein a word could have multiple meanings or senses.Language is such a wonderfully complex thing.Traditional word embeddings come up with the same vector for the word read in both the sentences. Hence, the system would fail to distinguish between the polysemous words. These word embeddings just cannot grasp the context in which the word was used.ELMo word vectors successfully address this issue. ELMo word representations take the entire input sentence into equation for calculating the word embeddings. Hence, the term read would have different ELMo vectors under different context.And now the moment you have been waiting for  implementing ELMo in Python! Lets take this step-by-step.The first step towards dealing with any data science challenge is defining the problem statement. It forms the base for our future actions.For this article, we already have the problem statement in hand:Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing (NLP). This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc., the task is to identify if the tweets have a negative sentiment towards such companies or products.It is clearly a binary text classification task wherein we have to predict the sentiments from the extracted tweets.Heres a breakdown of the dataset we have:You can download the dataset from this page.Note that you will have to register or sign-in to do so.Caution: Most profane and vulgar terms in the tweets have been replaced with $&@*#. However, please note that the dataset might still contain text that could be considered profane, vulgar, or offensive.Alright, lets fire up our favorite Python IDE and get coding!Import the libraries well be using throughout our notebook:Output: ((7920, 3), (1953, 2))The train set has 7,920 tweets while the test set has only 1,953. Now lets check the class distribution in the train set:Output:0 0.744192
1 0.255808
Name: label, dtype: float64Here, 1 represents a negative tweet while 0 represents a non-negative tweet.Lets take a quick look at the first 5 rows in our train set:We have three columns to work with. The column tweet is the independent variable while the column label is the target variable.We would have a clean and structured dataset to work with in an ideal world. But things are not that simple in NLP (yet).We need to spend a significant amount of time cleaning the data to make it ready for the model building stage. Feature extraction from the text becomes easy and even the features contain more information. Youll see a meaningful improvement in your models performance the better your data quality becomes.So lets clean the text weve been given and explore it.There seem to be quite a few URL links in the tweets. They are not telling us much (if anything) about the sentiment of the tweet so lets remove them.We have used Regular Expressions (or RegEx) to remove the URLs.Note: You can learn more about Regex in this article.Well go ahead and do some routine text cleaning now.Id also like to normalize the text, aka, perform text normalization. This helps in reducing a word to its base form. For example, the base form of the words produces, production, and producing is product. It happens quite often that multiple forms of the same word are not really that important and we only need to know the base form of that word.We will lemmatize (normalize) the text by leveraging the popular spaCy library.Lemmatize tweets in both the train and test sets:Lets have a quick look at the original tweets vs our cleaned ones:Check out the above columns closely. The tweets in the clean_tweet column appear to be much more legible than the original tweets.However, I feel there is still plenty of scope for cleaning the text. I encourage you to explore the data as much as you can and find more insights or irregularities in the text.Wait, what does TensorFlow have to do with our tutorial?TensorFlow Hub is a library that enables transfer learning by allowing the use of many machine learning models for different tasks. ELMo is one such example. Thats why we will access ELMo via TensorFlow Hub in our implementation.Before we do anything else though, we need to install TensorFlow Hub. You must install or upgrade your TensorFlow package to at least 1.7 to use TensorFlow Hub:We will now import the pretrained ELMo model. A note of caution  the model is over 350 mb in size so it might take you a while to download this.I will first show you how we can get ELMo vectors for a sentence. All you have to do is pass a list of string(s) in the object elmo.Output: TensorShape([Dimension(1), Dimension(8), Dimension(1024)])The output is a 3 dimensional tensor of shape (1, 8, 1024):Hence, every word in the input sentence has an ELMo vector of size 1024.Lets go ahead and extract ELMo vectors for the cleaned tweets in the train and test datasets. However, to arrive at the vector representation of an entire tweet, we will take the mean of the ELMo vectors of constituent terms or tokens of the tweet.Lets define a function for doing this:You might run out of computational resources (memory) if you use the above function to extract embeddings for the tweets in one go. As a workaround, split both train and test set into batches of 100 samples each. Then, pass these batches sequentially to the function elmo_vectors( ).I will keep these batches in a list:Now, we will iterate through these batches and extract the ELMo vectors. Let me warn you, this will take a long time.Once we have all the vectors, we can concatenate them back to a single array:I would advice you to save these arrays as it took us a long time to get the ELMo vectors for them. We will save them as pickle files:Use the following code to load them back:Lets build our NLP model with ELMo!We will use the ELMo vectors of the train dataset to build a classification model. Then, we will use the model to make predictions on the test set. But before all of that, split elmo_train_new into training and validation set to evaluate our model prior to the testing phase.Since our objective is to set a baseline score, we will build a simple logistic regression model using ELMo vectors as features:Prediction time! First, on the validation set:We will evaluate our model by the F1 score metric since this is the official evaluation metric of the contest.Output: 0.789976The F1 score on the validation set is pretty impressive. Now lets proceed and make predictions on the test set:Prepare the submission file which we will upload on the contest page:These predictions give us a score of 0.875672 on the public leaderboard. That is frankly pretty impressive given that we only did fairly basic text preprocessing and used a very simple model. Imagine what the score could be with more advanced techniques. Try them out on your end and let me know the results!We just saw first hand how effective ELMo can be for text classification. If coupled with a more sophisticated model, it would surely give an even better performance. The application of ELMo is not limited just to the task of text classification. You can use it whenever you have to vectorize text data.Below are a few more NLP tasks where we can utilize ELMo:ELMo is undoubtedly a significant progress in NLP and is here to stay. Given the sheer pace at which research in NLP is progressing, other new state-of-the-art word embeddings have also emerged in the last few months, like Google BERT and Falandos Flair. Exciting times ahead for NLP practitioners!I strongly encourage you to use ELMo on other datasets and experience the performance boost yourself. If you have any questions or want to share your experience with me and the community, please do so in the comments section below. You should also check out the below NLP related resources if youre starting out in this field:",https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/
29 Inspiring Women Blazing a Trail in the Data Science World,"Learn everything about Analytics|Introduction|Women in the AV Community|Learn, compete, hack and get hired.","Emily Glassberg Sands|Carla Gentry|Monica Rogati||Yael Garten||Sarah Nooravi|Cassie Kozyrkov|KateStrachnyi|Dr. Jeannette Wing||Kristen Kehrer|Vivian Zhang|Rachel Thomas|Dr. Elena Grewal|Jana Eggers|Caitlin Smallwood||Daphne Koller||Professor Melanie Mitchell|Fei-Fei Le|Anima Anandkumar|Pavleen Kaur|Shilpi Bhabhra|Parul Pandey|Divya Choudhary|Srishti Gupta|Mathangi Sri|Prathrna Bhat||Anchal Gupta||Preeti Agarwal|Tanvi Purohit|Aishwarya Singh|Share this:|Related Articles|A Step-by-Step NLP Guide to Learn ELMo for Extracting Features from Text|DataHack Radio #19: The Path to Artificial General Intelligence with Professor Melanie Mitchell|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"This article has been updated on Womens Day, 2019.This womens day, we at Analytics Vidhya are celebrating the power of women in data science.Women around the world are blazing a trail in the data science world and what better day to honor and appreciate them than today? There are a lot of women data scientists (but still not enough in our opinion) and leaders out there who are inspiring both women AND men.Women excel at communication, nurturing a positive atmosphere in the team, problem solving, asking the right questions (among a whole host of other things!). These are all primary leadership qualities and we hope to see a lot more women in data science leadership roles in 2019.Lets take a look at the women who are doing it all and inspiring us to be a better version of ourselves every single day. I encourage you to follow them on social media and read more about their accomplishments.And to all the women, Happy Womens Day!In her role as the Head of Data Science at Coursera, her mission is to build a better learning platform through data-drive decisions and products. She is a Ph.D in Economics from Harvard University. She has an impressive list of awards and honors she won while at Harvard. You can hear her on our DataHack Radio podcast here.Carla is currently working as the Digital Marketing Manager at Samtec Inc. She has over 20 years worth of experience working for companies like Johnson & Johnson, Hershey, Kraft, among others. She is a very popular and active social media user and her posts are always worth reading for their invaluable knowledge. Carla was one of the earliest guests on our DataHack Radio podcast.Monica is an independent data science and AI advisor. She helps companies make the most of their data  both technically and strategically. She has previously worked as the VP of Data at Jawbone, a Senior Data Scientist at LinkedIn and is also currently a guest lecturer at Stanford University.Yael is the Director of Siri Analytics, Evaluation and Data Engineering at Apple where her mission is to improve Siri by using Data as the voice of their customers. Previously, she has worked as a Director of Data Science at LinkedIn. She also works as an advisor at various startups and is a leading voice at various conferences.Sarah is currently working as the Senior Data Scientist at Operam. She is one of the most popular and helpful personalities on LinkedIn. Sarah regularly posts content aimed at helping aspiring data scientists break into the field and is always available for helpful and insightful advice.Cassie is the Chief Decision Scientist at Google.She is a well-known speaker in the data science sphere, and often pens down her thoughts in articulate fashion in this field. In her DataHack Radio podcast episode, she takes us on a journey into her life at Google and how she went from being a Statistician at Google to her current role.Kate is a data visualization master and a leading data science voice on LinkedIn. She has inspired countless aspiring data science professionals to take up storytelling with data. Im a huge fan of her Tableau skills and her willingness to give back to the community through her knowledge. Make sure you check out her course  Tableau Visual Best Practices: Go for Good to GREAT!Dr. Jeannette Wing is theAvanessians Director of the Data Science Institute and Professor of Computer Science at Columbia University. She has over 4 decades of experience in academia and the industry, and there is no one better to give a perspective on how computer science has evolved, and how it meshes with the data science world. And thats we talk to her about in this DataHack Radio episode!Kristen is a data scientist with 9 years experience who delivers innovative and actionable machine learning solutions in business. Like Kate, she is a leading data science voice on LinkedIn and is always sharing her rich knowledge with the community. She has interviewed numerous data scientists over the years and has drawn on her experience to create the Up-Level your Data Science Resume course.Vivian is the CTO and Chief Data Scientist at the popular NYC Data Science Academy. She has almost a decades worth of experience in the field of research and data science. She was featured in the list of 9 Women Leading the Pack in Data Analytics by Forbes in August 2016.The co-founder of fast.ai, Rachel was selected as one of the 20 Incredible Women in AI by Forbes. Fast.ai has created courses taken by over 100,000 students from around the world. Shes also a very popular writer and keynote speaker and her articles have been translated into various other languages.Dr. Elena is the head of Data Science at Airbnb. She is leading a team of 100+ data scientists. She did her graduation in economics and political science from Yale and spent a year in Indiaworkingon a project to bring clean water to children who had acute diarrhoea.Jana is the CEO of Nara Logics, an AI company focused on turning big data into smart actions. She has over 25 years experience in the business industry and is an active speaker, and contributor at various AI startups.Caitlin is the Vice President of Science and Analytics at Netflix. Her team develops new models and algorithms that directly improve the Netflix service. At Netflix, she was previously leading a team of mathematicians, statisticians and data scientists who worked on algorithms research, development, predictive modeling to bring to you a seamless experience while using Netflix. She is one of the leading figures in the data science community.Daphne is co-founder of the ultra popular online learning platform Coursera. She has also part of the Stanford University faculty for almost 19 years. Her research areas include AI and its applications in biomedical sciences.She has beenfelicitated withvarious awards & accolades which includeACM Infosys Awards, MacArthur Foundation Fellowship and many more.Melanie Mitchell is Professor of Computer Science at Portland State University and the author of multiple books on artificial intelligence. She has over three decades of experience in academia, computer science and artificial intelligence. She spoke to us about artificial general intelligence (AGI) and the challenges we face to get there.Fei-Fei Li is a Professor of Computer Science at Stanford University. She is truly one of the most influential people in technology and data science. She has previously worked as the Chief Scientist of Artificial Intelligence and Machine Learning at Google Cloud. Check out her brilliant TED Talk on Computer Vision.Anima Anandkumar is the Director of ML Research at NVIDIA and a Bren Professor at Caltech. She has spearheaded research in tensor-algebraic methods, non-convex optimization, probabilistic models and deep learning.In this section, we celebrate some of the women who are regular contributors to our AV community.Pavleen is a data visualization wizard. Her Tableau series, from beginners to advanced, has been incredibly well received in the community. She is a final year undergrad and a major data science enthusiast. The future of data science is very bright with her!Shilpi leads Retail and Marketplace analytics practice at Flipkart. Her expertise is in driving data-based decisions for product and business. During her 14 years of work experience, she has led several data sciences projects for implementation in the field of Retail and Marketing.Parul is one of the best writers youll come across in the machine learning and artificial intelligence space. She writes comprehensive articles and breaks complex concepts into easy-to-digest information. One of the most approachable and knowledgeable personalities to follow on LinkedIn for data science.Divya Choudhary is a data scientist, currently pursuing her MS in Data Science from the University of Souther California. With 4 years of work experience before starting her Masters, Divya is a computer science engineer who has traversed her professional career from being an analyst to a decision scientist to a data scientist.Srishti works on training deep learning models and building end to end ML pipelines to deploy them at Hike for a variety of data science problems such as recommendations, image stylisation, growth and more. Srishti has a Masters in Machine Learning from Georgia Tech and used to work at Apple before joining Hike in her current role.Mathangi is the Head of Data Science at PhonePe.She has 15+ years of proven track record in building world-class data science solutions and products. She has extensively worked on building chatbots and productizing text mining insights. She has 5 Patent grants and 20+ patents pending in the area of intuitive customer service, indoor positioning and user profiles.Prarthana Bhat is a Data Scientist at Flutura Desicion Science and Analytics. Prarthana is skilled in SQL, R and Business Intelligence. She is an active contributor on Analytics Vidhya andwas the first female data scientist to secure a rank in top 3on Analytics Vidhya. She has over 5 years of experience in the data industry.Anchal is a Data Scientist at Paisabazaar.com. She is a regular participant in the Analytics Vidhya hackathons and can often be found contributing to AVs Slack channel.Preeti is the Assistant Vice President at JP Morgan Chase & Co.. She is also a Analytics Vidhya Data Science Volunteer in Mumbai. She has 11+ years of experience in Telecom & Banking domain. She loves data science and wants to spread her knowledge to the world. Preeti is a graduate from Nagpur University.Tanvi Purohit is working as a Consultant with Deloitte. She is skilled in RDBMS and passionate about the field of data science. She has a working experience of over 5 years, out of which 4 have been spent working with data. She has worked in data warehousing and data analytics, and is a trained strategist. Tanvi is also a AV Volunteer in Mumbai.Aishwarya joined Analytics Vidhya as an intern and has bloomed into her data science role. She loves reading and learning, and that dovetails nicely into her daily role as a data science professional. Aishwarya is also an excellent writer and has penned down plenty of articles on various data science techniques, primarily focusing on time series analysis.And finally, please note that this is by no means an exhaustive list. There are so many wonderful women leading the way in data science and if you know of them, please post their names and links in the comments below.",https://www.analyticsvidhya.com/blog/2019/03/inspiring-women-blazing-a-trail-in-the-data-science-world/
DataHack Radio #19: The Path to Artificial General Intelligence with Professor Melanie Mitchell,Learn everything about Analytics|Introduction|Professor Melanie Mitchells Background|Ph.D in Computer Science and the Development of Copycat|Thoughts on the rise of Deep Learning|Future Trends in Artificial Intelligence and Deep Learning|Machine Learning Algorithms are still far away from Artificial General Intelligence (AGI)|Upcoming Book on Artificial Intelligence: A Guide for Thinking Humans|End Notes,"Share this:|Related Articles|29 Inspiring Women Blazing a Trail in the Data Science World|Hands-On Introduction to creditR: An Amazing R Package to Enhance Credit Risk Scoring and Validation|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"People underestimate how complex intelligence is.How close are we to Artificial General Intelligence (AGI)? It seems we take a step closer to that reality with every breakthrough. And yet, it feels like a million miles away in the future.Why are we still so distant from AGI despite the unabated rise in computational hardware? Whats holding us back from programming machines that generalize to multiple domains?We invited Professor Melanie Mitchell to answer these pertinent and pressing questions in episode #19 of the DataHack Radio podcast. She is Professor of Computer Science at Portland State University and the author of multiple books on artificial intelligence.Professor Melanie brings over three decades of teaching and academia experience to this DataHack Radio episode. It was delightful listening to her thoughts on topics like:I have summarized the key points discussed in this episode here. Make sure you tune in and listen to the full podcast!Listen and subscribe to this, and all previous DataHack Radio podcast episodes, on any of the below platforms:Where did it all start for Professor Melanie Mitchell? How did she become enthralled by the field of computer science?As she tells us in this episode, her interest was kindled during her undergraduate days via a book by Douglas Hofstadter titled Gdel, Escher, Bach: an Eternal Golden Braid. Its essentially a book about artificial intelligence that inspired Professor Melanie to pursue research in this field.She contacted Mr. Douglas (the author) to pick his brains about certain topics in AI. These conversations carried over into Professor Melanies Ph.D in Computer Science with Douglas Hofstadter as her thesis advisor. Its a great example of how persistence and belief in your passion can fuel you to achieve your dreams.AI was a fairly well-known field of research back in the mid-1980s and early 1990s. Neural Networks were just starting to become popular.Most of us think of them as a dense network of layers and neurons now but it took a good while for them to acquire the deep moniker. Back then, these neural networks were fairly shallow. There simply wasnt enough computational power to generate any sort of deep neural network!Professor Melanie holds a Ph.D in Computer Science from the University of Michigan. Her dissertation was around the development of a program that can make analogies, called Copycat. Its considered one of the earliest approaches to analogy-making.You can read more about Copycat and its functionalities here. Her research was about attempting to get machines to generalize to new domains. Yes, that means artificial general intelligence  an area of research we are still trying to make headway with in 2019.Most of the breakthroughs weve seen in artificial intelligence have been thanks to improvements in computing and the availability of large datasets, rather than any mind-blowing insights. For example, Deep convolutional neural networks (CNNs), a raging trend these days, were invented back in the 1980s!These deep CNNs were used on problems like handwritten digit recognition but to a very limited degree. The computational resources just werent there. Now? Most of us with a half-decent machine can build an accurate digit recognition model!There is a lot of data in the world. Its just not labelled, though.The biggest takeaway from the last year or two? The unabated rise of unsupervised learning. Machines are able to learn what are the important features in a model from looking at the data. Unlike supervised learning, theres no need to label the training data. This means a significant reduction in the model training cost.You can see why its an area most researchers would pursue! This line of thought could potentially be our way to making AGI a reality in the coming decades.People underestimate how complex intelligence is.Professor Melanie recently penned a really thought-provoking article in the New York Times titled Artificial Intelligence Hits the Barrier of Meaning. It talks about how machine learning algorithms dont (yet) understand things the way humans do. These algorithms still cant understand context  a vital aspect of our own thinking and behavior.Take the example of autonomous cars. They have been on the verge of becoming mainstream for a number of years  but were still not sure when theyll truly be ready.If theres a paper bag on the road, we dont have to worry about driving over it. Autonomous cars, on the other hand, have a lot of trouble figuring out which obstacles they should avoid and ones they dont need to.Professor Melanie has written multiple books over the years and has one coming up later this year, titled Artificial Intelligence: A Guide for Thinking Humans. The book is targeted to people from all backgrounds. But it will go a little deeper than most general AI books out there.The theme of the book is how much does AI actually need to understand the data that its dealing with in order to be reliable.Topics covered in the book include how artificial intelligence algorithms work, what are their applications, their limitations, etc.We read and come across industry perspectives on artificial intelligence all the time. Its refreshing to hear from people working in academia about their thoughts on artificial intelligence and where its headed in the near future. Thats especially enriching when it comes from someone as experienced and well-versed as Professor Melanie Mitchell.AGI is a much debated topic and theres no real consensus on when it might come about. It feels like we are getting closer with each breakthrough in the field and yet are a million miles away from the end goal. What are your thoughts on AGI? Do you feel we are any closer to it now than, perhaps, 10 years ago?",https://www.analyticsvidhya.com/blog/2019/03/datahack-radio-19-the-path-to-artificial-general-intelligence-with-professor-melanie-mitchell/
Hands-On Introduction to creditR: An Amazing R Package to Enhance Credit Risk Scoring and Validation,Learn everything about Analytics|Introduction|Table of Contents|Why should you use creditR?|Getting Started with creditR|A List of Functions inside creditR|An Application of the creditR Package|End Notes|Bug Fixes and About the Author,"Share this:|Related Articles|DataHack Radio #19: The Path to Artificial General Intelligence with Professor Melanie Mitchell|11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals)|
Guest Blog
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Machine learning is disrupting multiple and diverse industries right now. One of the biggest industries to be impacted  finance.Functions like fraud detection, customer segmentation, employee or client retention are primary machine learning targets. The one we are going to focus on in this article is called credit risk scoring.Credit scoring is a statistical analysis performed by lenders and financial institutions to access a personscreditworthiness. Lenders use credit scoring, among other things, to decide on whether to extend or deny credit.  InvestopediaMachine learning algorithms are often developed as challenger models because this is a field where regulatory requirements need to be met. This got me thinking  how can I make things easier for professionals working in the field?Out of that came the creditR package! It allows you to easily create your base models for credit risk scoring before machine learning applications. Additionally, the package also contains some functions that can be used to validate these processes.The package aims to facilitate the applications of the methods of variable analysis, variable selection, model development, model calibration, rating scale development and model validation. Through the functions defined, these methodologies can be applied quickly on all modeling data or a specific variable.In this article, we will first understand the nuts and bolts of the creditR package. Well then get our hands dirty in R by deep diving into a comprehensive example using creditR.The package was issued for the use of credit risk professionals. Basic level knowledge about credit risk scoring methodologies is required for use of the package.Perceptions of credit risk modeling are rapidly transforming as the demand for machine learning models in the field increases. However, many regulators are still very cautious about transitioning into machine learning techniques. Therefore, a possible speculation might be that during this transformation phase, machine learning algorithms will proceed along with the traditional methods.Trust may be achieved on the part of the regulators once it is established that machine learning algorithms, while challenging the conventions of the field, are also producing more robust results than the traditional methods. Moreover, the new methods of interpreting machine learning algorithms may help to create a more transparent process.The creditR package offers both possibilities for automating the use of traditional methods and also for the validation of traditional and machine learning models.In order to install the creditR package, you should have the devtools package installed. The devtools package can be installed by running the following code:The creditR package can be installed using the install_github function found in the devtools package:The functions available under the package are listed below.Output:Weve aprsed through the theory aspect. Now lets get our hands dirty in R!An example application of creditR is shared below in a study of how some common steps in credit risk scoring are carried out using the functions provided in the package.Real-world practices were taken into consideration in the preparation of this example.The general application is structured under two main headings as modeling and model validation, and the details as to what the corresponding code does can be seen in the comment lines.Only important outputs have been shared in this article.This R script is designed to make the creditR package easier to understand. Obtaining a high accuracy model is not within the scope of this study.Output:WOE transformation is a method which transforms the variable into a categorical variable through its relationship with the target variable. The following woerules object contains the WOE rules.With the help of the woe.binning.deploy function, the rules were run on the data set. The variables we needed are assigned to the train_woe object with the help of the woe.get.clear.data function.Information value and univariate gini can be used as variable selection methods. Generally, a threshold value of 0.30 is used for IV and 0.10 is used for univariate Gini.Output:Output:Output:There are too many variables in real life to manage with correlation matrices. Hence, clustering is performed to determine variables with similar characteristics. This particular example of clustering does not make sense because of the small number of variables, but the method in general can be very useful in data sets with a large amounts of variables.Output:In some cases, average correlations of clusters are important because the number of clusters may not be set correctly. Therefore, if the cluster has a high average correlation, it should be examined in detail. The correlation value, which is only one variable in cluster 1, is NaN.Output:A model was formed with the variables included in the data set. When the variables are examined by the model summary, it seems that the variables are meaningful. Then, with the help of the woe.glm.feature.importance function, the weights of the variables are calculated. In fact, weights are calculated on the basis of the effect of a single unit change on the probability.Output:Output:In real life, institutions use rating scales instead of continuous PD values. Due to some regulatory issues or to adapt to changing market/portfolio conditions, the models are calibrated to different central tendencies.Regression and Bayesian calibration methods are included in the package. The numerical function that can perform calibration by embedding in the enterprise system can be obtained as output with the help of the calibration object$calibration_formula code.Output:The Bayesian calibration method is applied over the rating scale. We can easily create the rating scale with the help of the master.scale function. However, in real life, rating scales can only be created after many trials.The summary is added to the output. Details can be seen by running the R script. In addition, the example is only aimed to introduce the function within the scope of this study, hence, the PD values do not increase monotonically.Output:In order to apply Bayesian calibration, the score variable is created in the data set. Then the rating scale is calibrated to 5% central tendency.Output:In real life applications, it is difficult to understand the concept of probability for employees who are not familiar with risk management. Therefore, there is a need to create a scaled score. This can be done simply by using the scaled.score function.After the modeling phase, the model validation is performed to validate different expectations such as the accuracy and stability of the model. In real life, a qualitative validation process is also applied.Note: Model calibration is performed for illustration only. Model validation tests proceed through the original master scale as follows.In the models created by logistic regression, the problem of multicollinearity should be taken into consideration. Although different threshold values are used, vif values greater than 5 indicate this problem.Output:Generally, acceptable lower limit is 0.40 for Gini coefficient. However, this may vary according to model types.Output:0.3577422Output:The scorecards generally are revised in a long-term basis because the process creates an important operational cost. Therefore, the stability of the models reduces the need to revise. In addition, institutions want models that are stable since these models are used as input of many calculations like impairment, capital, risk weighted asset etc.System Stability Index is a test used to measure the model and variable stability. SSI values above 0.25 indicate that variable stability is impaired.Output:The HHI test measures the concentration of the master scale since the main purpose of the master scale is to differantiate the risk. Over 0.30 HHI values indicate high concentration. This may be due to modeling phase or the incorrect creation of the master scale.Output:0.1463665With the help of the anchor.point function, it is tested whether the default rate is compatible with average PD at the expected levels.Output:Chi square test can also be used as a calibration test. The chisquare.test function can be used to perform the test in the specified confidence level.Output:Binomial test can also be applied as a calibration test. The one-tail binomial test is usually used for IRB models, while the two-tail test is used for IFRS 9 models. But the two-tail test will be more convenient for general use, except for IRB.Output:Modeling and model validation need to be managed to ensure continuity. When the R environment is managed correctly, this manageable modeling and validation environment can be provided easily by institutions.Institutions are designing much more efficient business processes using open source environments such as R or Python with big data technologies. From this perspective, creditR offers organizational convenience to the application of modeling and validation methods.The creditR package provides users with a number of methods to perform traditional credit risk scoring, as well as some of those for testing model validity, which can also be applied to ML algorithms. Moreover, as the package provides automation in the application of the traditional methods, the operational costs for these processes can be reduced.Furthermore, these models can be compared with the machine learning models in order to demonstrate that the ML models also meet the regulatory requirements, the meeting of which is the precondition for the application of ML models.Please inform the author about the errors you have encountered while using the package via the e-mail address that is shared below.Ayhan Dis  Senior Risk ConsultantAyhan Dis is a Senior Risk Consultant. He works on consulting projects like IFRS 9/IRB model development and validation, as well as advanced analytics solutions including ML/DL in areas such as fraud analytics, customer analytics and risk analytics, using Python, R, Base SAS and SQL fluently.Over the course of his work experience, he has worked with various types of data such as twitter, weather, credit risk, electric hourly price, stock price and customer data to offer solutions to his clients from sectors such as banking, energy, insurance, financing and pharmaceutical industry.As a data science enthusiast, he thinks that the real thrill of data science is not found in establishing ones technical abilities; instead, it is found in blending data science together with big data to reveal insights which can be integrated with the bussiness processes, through artificial intelligence.",https://www.analyticsvidhya.com/blog/2019/03/introduction-creditr-r-package-enhance-credit-risk-scoring-validation-r-codes/
11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals),"Learn everything about Analytics|Introduction|1) Start performing detective analytics and generate insights from reports|2. Learn statistics to support your insights about reports|3. Presentyour findings to the right group||4. Explore an open source tool to generate reports OR to perform detective analysis||5. Understand the model building / predictive modeling steps||6. Methods to evaluate your models performance|7. Introduction to predictive modeling with Linear and Logistic regression||8. Identify the business problem (related to your role), convert it to a data problem and make predictions||9. Share your models results with the business owners and earn their trust|10. Keep learning new algorithms, engage in the data science community and focus on profile building|11. Focus on transitioning to a data science role within your organization|End Notes","Why transition into data science in easier for a Business Intelligence (BI) professional:|Example 1:|Example 2:|Example 3:|Which of the above examples is more meaningful for business users?|Challenges and Solutions:|Challenges and Solutions:|Challenges and Solutions:||Challenges and Solutions:|Challenges and Solutions:|Challenges and Solutions:|Challenges and Solutions:||Challenges and Solutions:|Challenges and Solutions:|Challenges and Solutions:|Challenges and Solutions:|Share this:|Related Articles|Hands-On Introduction to creditR: An Amazing R Package to Enhance Credit Risk Scoring and Validation|Top 5 Data Science GitHub Repositories and Reddit Discussions (February 2019)|
Sunil Ray
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The rapid rise of data science as a professional field has lured in people from all backgrounds. Engineers, computer scientists, marketing and finance graduates, analysts, human resource personnel  everyone wants a piece of the data science pie.Analytics Vidhya has already published acomprehensive learning pathfor beginners to get into data science. So why am I focusing on professionals working in business intelligence / MIS / reporting specifically? Let me explain.I regularly encounter talented business intelligence (BI) professionals looking to land their first data science role. They are often frustrated by the perceived lack of opportunities for them. A lot of them feel that their role is repetitive, or they just need to perform whatever has been asked from them.They actually miss the fact that they are closer to data science opportunities than any other professional out there.Business Intelligence (BI) professionals hold a massive advantage over almost anyone trying to transition into data science because of following reasons:In other words, these folks work in the first half of a data science project. Thats already more industry experience than most aspiring data scientists!If you are one such transitioner looking to jump from a BI / MIS / reporting role to data science, this article is for you. You can consider these 11 steps as a learning path you can follow. In fact, I would strongly encourage you to implement these steps in your current BI role. Start where you are and practice till you break into data science!P.S. For the rest of the article  Business Intelligence, MIS, reporting, dashboarding have been used interchangeably. There is very little difference and a lot of overlap in these roles and designations.So are you ready to take this journey with me? Lets take this ride step-by-step.Lets start off by looking at three examples of reporting thata BI (MIS / Reporting / Business Intelligence) professional does on a day-to-day basis.This BI professional has generated a report containing details about business sourcing at the City and Region levels along with the quality of the business.Here, the BI professional has generated the same report with an addition of RAG (Red-Amber-Green) analysis for the Rejection Score column. A lower rejection score means a higher quality of business.In this example, the BI professional has taken things to another level by adding insights about the report. You can see that (s)he has written the top 2 findings taken from the report. Ive taken a simple example here to add interpretability to your report. You can add more visuals/charts depending on the type of information you are sharing.Looking at the above three examples, I would side towards Example 3 adding more value to the business because:These actually help business users a lot. When you work with senior level executives, youll find that most of them need actionable items to work on. They would not want to spend time focusing on interpreting the report and doing in-depth analysis.To generate a similar report, a BI professional should havecuriosity, attention to detail, command over any onetool (Excel / SQL / QlikView / Tableau), along with knowledge about the business.This skillset is not only limited to folks working in BI! Its also very critical to becoming a good data science professional. In most cases, 60-70% of a data scientists job is about business understanding, data exploration and generating insights about the problem at hand.A BI professional has a hugeadvantagehere compared to other professionals who aretransitioninginto data science. You can start practicing TODAY and this skill set will help you do well in your current role as well. Its a win-win!Its now time to support your insights with some statistical metrics. Dont just limit yourself to generating insights based on visual interpretations. Take a look at the image below  whats your first reaction?I can say that the average business sourced, post the contest, is higher as compared to before. Now, the question is whether contest is the factor behind the boost in average business sourced or is it just a random increase?.Here, we need to rely on certain statistics concepts to support our insights, like doing a z-test/t-test or other statistical tests. Having a good knowledge of statistics will help you in these situations.You should have a solid understanding of the below statistics topics if you want to land a data science role:And here is a list of useful resources to help you get started with these topics:Performing detective and statistical analysis will not help you land a data science role if you dont share your findings with the right group.Presenting stories is one of the key skills a data science professional must possess.Here, I strongly recommend practicing this storytelling skill in your current role as well. You can startwith the following:Heres an essential recommendation that has personally helped me in my career  add visualization(s) to your slide(s). The words you write in the presentation (or speak during a meeting) should add context to your visualizations. Confused? Let me explain using an example.Look at the visualization below. It showcases details about Sachin Tendulkars test match career. You can talk about various metrics here using the graphs and numbers. This also shows why business understanding is so important  you cant talk about metrics you have no experience with!You should check out this excellent article  The Art of Story Telling in Data Science and how to create data stories.Your concernsare understandable but you have to start somewhere to gather experience! My suggestion would be to start by sharing insights with your manager, experienced teammates or your customers (if thats possible). This will give your confidence a much-needed boost so start practicing!So far, Ive not talked about any tool for generating reports and insights. I have deliberately avoided going into questions like  which tool should you pick? Or which one is right or better? That is because my objective was to get youcomfortable with detective analysis, statistical concepts and honing your communication skills so you can present your findings using your current working tool.Now, its time to learn a tool which has:You can pick up any tool among SAS / R / Python as all these tools have the above-listed capabilities. Here, your initial task is very specific while learning a new tool  get yourself comfortable with performing data exploration, visualization, detective analysis and statistical tests. You dont need to have complete expertise over any of these tools (not initially, anyway).If youre not sure which tool to pick, I would suggest going through thisawesome article written by Kunal Jain that compares the various pros and cons of these three tools.You can look at the below tutorials for learning data exploration using SAS/R or Python:Comprehensive Guide to Data Visualization in RIts finally time to move to the most attractive part of data science  model building! Before you dive into specific models, I recommend first understanding the type of problems that exist. Here is an article explaining the basics of predictive modeling/machine learning  Machine Learning basics for a newbie.Broadly, we can divide the model building process into 5 steps:Im parking the first two steps (Problem Definition and Hypothesis Generation) to cover later in the article. Well talk about data exploration in this section.The data exploration step is similar to detective analysis where our primary objective is to understand the behavior of variable(s) individually and with each other as well. Here, good knowledge of statistics will help you a lot. This step focuses on both insight generation as well as data cleaning. You could be required to impute missing values, detect and deal with outliers, and perform multiple types of transformation.Ive written a comprehensive guide on the steps involved in data exploration.You can practice all these methods on a dataset from your industry or using any open dataset.During our model building process, we train our model on a dataset where the target is known beforehand and then apply it on the test dataset to predict the target variable. We obviously want to be accurate while estimating the target variable.How can we check whether we are accurate or not? We need a metric which will help us to evaluate our model result against the actual observations. Lets understand this using an example.We have a customer base  C1, C2 and C2. Weve estimated that only C3 will buy product A from this customer base. As it turns out, both C2 and C3 bought the product. This meanswere 66.6% accurate (2 out of 3 predictions are correct). This accuracy is known as our Evaluation Metric.The evaluation metric will change depending on the type of problem you are solving. Here is a list of common evaluation metricsyou should be know.Youve decided on the evaluation metric but do you have the actual results to evaluate your model? You cant jump into the future to prepare a test dataset! In this scenario, we reserve a particular sample of the dataset on which we do not train the model. Later, we evaluate the model on this sample before finalizing it. This method is known as model validation. You can refer to this article on various validation techniques which includes practical examples in R and Python.You have understood the dataset and looked at the metrics to evaluate your models performance. Whats next?Applying modeling techniques! Do not start learning multiple techniques simultaneously. Focus on only two for now  Linear and Logistic Regression. These two techniques will help you predict continuous and categorical variables.For example:Below are two good articles to learn linear and logistic regression and practice using a tool of your choice:So, where can you find a dataset for your domain? Finding a business problem can be difficult.You should talk to the leadership or team managers and take one of their business challenges as your project. Here, the first step is to convert the business problem to a data problem. Then, start moving down the steps we had discussed in point #5 earlier  hypothesis generation, data collection, data exploration, data cleaning and finally model building and validation.One of the major advantages you have as a BI professional is that you are already familiar with the variables in the dataset. Your detective analytics skills will help you understand the variable(s) relationships as well. You can jump to tasks like data cleaning, transformation, identifying the right evaluation metric, setting validation set and finally model building.You should take some time and watch the below webinar by Tavish Srivastava to understand the importance of defining the problem statement and hypothesis generation:I also recommend going through the below articles on building models easily and effectively in R and Python:After building your model, you should share the results with your supervisor or the people who make decisions (like the team or project manager). As a data science professional, it is very critical to share your findings (like which feature(s) is making animpact on the target variable). You should also communicate regular updates around the comparison between your model result and the actual numbers.This process will also help you to tune and improve your model. If the model is performing well, then there is a high chance you will get another assignment or get involved with the core data science team. Thats what we are aiming for, right?Learning never stops in data science. It is an ever-evolving field and we need to keep evolving with it. Youve learned linear and logistic regression so far  extend your knowledge beyond that now. Learn algorithms like decision trees, random forest, and even neural networks.And like I mentioned before, you should learn by applying. Theoretical knowledge is good to have but its useless if you dont put it into practice. Pick up the datasets we spoke about earlier and apply these newly learned algorithms. You are likely to see a significant improvement in your model!Now, lets take a step outside the tools and techniques. I want to emphasize on the importance of building your network and profile in the data science community.Start attending data science focused events like meetups and conferences. You will meet like-minded people as well as experienced professionals who can guide you. I have seen plenty of aspiring data science professionals acquiring job offers through these events so I an vouch for their usefulness!You should also focus on the digital aspect of your profile. You have clearly been working with data science projects so showcase your work to the community! Upload your code to GitHub and start publishing blogs/articles on your findings. This helps prospective employers see that you possess good knowledge about the subject.While there are no easy ways to transition into data science, there are certain well-trodden paths. One of them is switching to the data science team in your current organization. Let me explain why you should focus on this rather than other paths (at least for starters).I could go on, but you get the idea. Always make it a first preference to look for opportunities in your current work place. Talk to folks in a senior role or from the data science team. Build up your network and trust me, it does pay off eventually.That was quite an exhilarating journey! I have made this transition myself quite a number of years ago. I have seen this field evolve over time and my aim in this article was to help you make that switch. You already have a number of steps covered that most aspiring data science professionals dont, so make it count!If you have any questions on this learning path, or any feedback on this article, let me know in the comments section below. Meanwhile, here are a few additional resources to learn data science and give yourself the best chance of breaking into this field:",https://www.analyticsvidhya.com/blog/2019/03/11-steps-data-science-role-reporting-mis-bi-professionals/
Top 5 Data Science GitHub Repositories and Reddit Discussions (February 2019),Learn everything about Analytics|Introduction|Top Data Science GitHub Repositories (February 2019)|StyleGAN  Generating Life-Like Human Faces|OpenAIs Ground-Breaking Language Model  GPT-2|SC-FEGAN : Face Editing Generative Adversarial Network with Users Sketch and Color|LazyNLP for Creating Massive Text Datasets|Subsync  Automating Subtitles Synchronization with the Video|BONUS:Flickr-Faces-HQ Dataset (FFHQ)|Reddit Discussions|Are you Expected to Solve Hard Coding Challenges to work in the Machine Learning Industry?|Key Points Every Student (Graduate or Post-Graduate) Should Keep in Mind While Pursuing Machine Learning|The Significance of p=0.05|A List of Very Useful but Lesser-Known Python Libraries|A list of Lesser-Known R Packages|End Notes,"Share this:|Related Articles|11 Steps to Transition into Data Science (for Reporting / MIS / BI Professionals)|Tips and Tricks to Ace Data Science Interviews  Brand New Podcast Series by Analytics Vidhya!|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I love GitHub. I have been a regular daily user of the various features the platform offers. That wasnt always the case, however.I had vaguely heard about GitHub during my early data science learning days. The people I spoke to, even some of the influencers, espoused the value of GitHub as a code hosting / sharing / showcase platform. And since I was only just starting to learning R, I couldnt really map the need of such a platform.How wrong I was! GitHub is a goldmine for data science professionals, regardless of whether youre established or just starting out. GitHub will be of tremendous help irrespective of whether you are learning / following NLP, Computer Vision, GANs or any other data science development.I was truly won over once I realized all the big data science focused companies (Google, Facebook, Amazon, Uber, etc.) regularly open sourced their code on the platform.It is the best way to keep up with the breakneck developments happening in our field. You even get to download the code and replicate it on your own machine! What more could a data scientist ask for?In this article, we continue our monthly series of showcasing the best GitHub repositories and Reddit discussions from the month just gone by. February was a HUGE month in terms of open source data science libraries.Lets get cracking!You should also check out our top GitHub and Reddit picks for January here:The above image seems like a typical collage  nothing to see here. What if I told you none of the people in this collection are real? Thats right  these folks do not exist.All these faces were produced by an algorithm called StyleGAN. While GANs have been getting steadily better since their invention a few years back, StyleGAN has taken the game up by several notches. The developers have proposed two new, automated methods to quantify the quality of these images and also open sourced a massive high-quality dataset of faces.This repository contains the official TensorFlow implementation of the algorithm. Below are a few key resources to learn more about StyleGAN:GPT-2 won the unofficial most talked about Natural Language Processing (NLP) library award in February. The way they went about launching GPT-2 raised quite a few eyebrows. The team claims that the model works so well they cannot fully open source it for fear of malicious use.You can imagine why that attracted headlines and questions. They have, however, released a smaller version of the model which is available on this GitHub repository weve linked above.GPT-2 is a largelanguage model with 1.5 billion parameters. The model has been trained on a datasetof 8 million web pages. The aim behind the model is to predict the next word, given all the previous words within some text. Is it state-of-the-art? Well have to take OpenAIs word for it (for now).Here are a couple of additional resources to learn more about GPT-2:Another GAN library?! Thats right  GANs are taking the data science world by storm. SC-FEGAN is as cool in terms of style as the StyleGAN algorithm we covered above.The above image perfectly illustrates what SC-FEGAN does. You can edit all sorts of facial images using the deep neural network the developers have trained. We can all become artists just sitting in front of our computers!The repository helpfully includes steps to help you build the SC-FEGAN model on your own machine. Give it a try! And if computational power is a challenge, hop over to Google Colaboratory and utilize their free GPU offering.The premise behind LazyNLP is simple  it enables you to crawl, clean up and deduplicate websites to create massive monolingual datasets.What do I mean by massive? According to the developer, LazyNLP will allow you to create datasets larger than the one used by OpenAI for training the GPT-2 model. The full scale one. That certainly had my full attention.This GitHub repository lists down the 5 steps youll need to follow to create your own custom NLP dataset. If youre in any way interested in NLP, you should definitely check out this release.How often have you found yourself frustrated at subtitles being out of sync with the video? This repository happens to be your savior in such situations.Subsync is about language-agnostic automatic synchronization of subtitles to video, so that subtitles are aligned to the correct starting point within the video. The algorithm was built using the Fast Fourier Transform technique in Python.Subsync works inside the VLC Media Player as well! The model takes about 20-30 seconds to train (depending on the video length).I wanted to include this in the article for anyone searching for high-quality images. The dataset consists of 70,000 super high-quality images (1024 x 1024). Theres a lot of variety in the faces, such as age, ethnicity, image background, etc.Its ideal for learning and experimenting with GANs. Let me know in the comments section below if you use it!I like this question because of how relevant it is in todays world. The thread has close to 200 comments from experienced data scientists and machine learning researchers debating whether these coding challenges are a good or bad thing in an interview round.Theres a lot of experience here so this is a discussion you really should pay close attention to. The essential question it comes down to is  should data science/machine learning professionals be judged extremely tightly on their coding skills or should algorithms/concepts take preference?We also aim to help you crack these data science interviews in our course offering. Make sure you check it out!If youre a full-time student trying to pursue machine learning on the side  this thread is for you. The author of the post has very lucidly written down the pain points he/she is facing in this regard. Im sure a lot of you will relate to these challenges.There is a lot of solid advice in this thread. I personally liked this bit:It sounds like you may be casting too wide a net initially. If you dont have a comfortable mental framework in place yet for how to organize information about different sub-parts of ML, then each sub-part of ML is going to be its own independent thing for you to learn about. Research papers, etc arent a good place to start.Have you ever wondered why the cut-off for p-value is 0.05? This Reddit discussion initially started as a humorous one but turned into a full-blown statistics discussion pretty quickly.This is a good place to understand the significance not just of p-value, but of statistics in general. I really liked this comment from one of the users:P-values are one of a number of useful indicators but thats about it. People look to stats expecting it to have hard and fast rules, but thats not how it works. Stats isnt about giving you the right answer, its about giving you a less-wrong answer.We are all familiar with Pandas, NumPy and Scikit-Learn. We see these in articles all the time and rely heavily on them during our own projects. We can call them essential to our day-to-day work.But there are plenty of lesser known Python libraries that can be potentially useful. These libraries go under the radar due to various reasons. This thread is a collection of tons of such libraries with their potential applications.I particularly found plotnine helpful for my work with visualizations. Which one(s) did you like?I get a lot of messages from R users asking when well show the tool some love. Here you go! Much like the above Python thread, this one focuses on some of the more useful R packages.My favourite? Catterplots! The package allows you to create scatter plots with cat shaped points. Its majestic. If you work with MS Office tools a lot, you might find the officer package helpful.I have been curating this monthly list for over a year now and each month I learn something that blows my mind. I strongly encourage you to regularly check the Trending GitHub section to see whats doing well.Which GitHub repository and Reddit discussion did you like? Let me know in the comment section below!Also  if your discovery sensors want more  check out thetop GitHub repositories from 2018.",https://www.analyticsvidhya.com/blog/2019/03/top-5-data-science-github-repositories-reddit-discussions-february-2019/
Tips and Tricks to Ace Data Science Interviews  Brand New Podcast Series by Analytics Vidhya!,Learn everything about Analytics|Introduction|The Idea Behind Ace Data Science Interviews Podcast|Episode #1:Top 3 Mistakes Data Scientists Make in Interviews|Episode #2:3 Game-Changing Tips to Ace Data Science Interviews|Episode #3:Top 4 Ways to Showcase Real-World Data Science Projects|End Notes,"1. Not Thinking in a Structured Manner|2. Not Communicating your Thought Process with the Interviewer|3. Expecting only Technical Interviews as part of the Interview Process|1. Share Updates on LinkedIn Regarding your Recent Data Science Projects|2. Ask High-Quality Meaningful Questions to the Interviewer|3. Dont Try to Fumble Out Answers for a Topic you havent Studied Before|1. Get an Internship in a Data Science Organization|2. Participate in Data Science Hackathons and Competitions|3. Attend Meetups and Network|4. Identify Problems you Face and Try to Solve them using Data Science|Related Articles|Top 5 Data Science GitHub Repositories and Reddit Discussions (February 2019)|[Announcement] Launching Analytics Vidhya Certified Programs and new Courses portal|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you finding it tough to land a data science role? Do you feel fully prepared before you enter the interview room but you just cant figure out why you keep getting rejected?Ive been there. Thousands of fellow aspiring data science professionals have been there. Its a tough pill to swallow.Before I landed a data science role, I worked in the learning and development field for over 5 years. I held little to no technical experience, forget data science! So when I started studying up on the various tools and techniques, I did so without any proper plan or structure.And I kept botching my data science interviews. No matter how much I studied or did certifications, I couldnt break into this field.So what was I doing wrong? Did this mean I couldnt ever land a data science role?Certainly not! But most data science aspirants give up early or dont have anyone to guide them. Thats a major reason Kunal Jain, Analytics Vidhyas CEO and Founder, decided to launch the Ace Data Science Interviews venture, including a brand new podcast seriesand acomprehensive end-to-end course!Kunal has worked with several companies over the last 10 years to set up their data science teams. He has also helped millions of people learn about data science careers through Analytics Vidhya. Who better than Kunal to guide you through your own journey?In this weekly podcast, Kunal will share his experience about data science interviews. Each episode will cover topics like what works (and doesnt) in data science interviews, ways to perform better in these interviews, among other things.Each episode will cover a certain topic and take a question from you, the audience. Reach out to us at [emailprotected]!The podcast is available on the below platforms:The first 3 episodes are already out! This article is my understanding and summary of each episode. Make sure you subscribe and listen every week. We look forward to hearing your thoughts and feedback!Learning a topic means accepting youll make mistakes. The key is learning from them and taking appropriate action. The same applies to data science. Its a vast and complex field so its inevitable youll make some mistakes in your journey.However, we have noticed most people making the same common mistakes. I covered them in details in my article here, and Kunal picked his top 3 based on his own experience conducting these interviews. Lets go through them individually:When people come across case studies, guesstimates and puzzles during their data science interview, the first instinct is to jump the answer. Theres not much thought behind how to structure your thoughts  a big no-go for an interviewer.Lets understand this with an example. Suppose you have recently joined a transport company as the CEO. The company has been posting heavy losses recently. How would you go about turning around the situation?In Kunals experience, people start listing off ideas, like analyze the pricing, look at the overall costs, look at the route planning, etc. That is the absolute wrong way to go about things! Consider this a rejection for sure.Lay down a framework using pen and paper (or a whiteboard). Kunal has explained how to do this for our case study in this episode. Putting a structure to your thoughts showcases your thinking ability  a must-have skill in data science.Focusing too much on the answer and not on the process is a sure shot way to failing your data science interview.Think about it for a moment  the aim behind an interview is to help the interviewer understand the thought and reasoning behind your problem solving skills. Right? The interviewer does not care much about if your answer is precise to the decimal point.There might not even be a right or wrong answer in the first place! So make sure you communicate your thought process to the interviewer, including the assumptions you are making. Its a win for both sides.There are several layers to a data science interview. The process isnt one-dimensional! Just preparing for questions around tools and techniques will not land you the role. Of course these technical skills matter, but there are other equally important topics you will be judged on.There will be an interaction with the project team, a case study related to the domain, role plays, and much more. You should prepare for all these formats.We have put together a 7-step framework for understanding and acing each of these different interview rounds in the Ace Data Science Interviews course. Head over there and check it out!In this episode, Kunal shares his thoughts on 3 uncommon yet immensely useful steps you can take to ace your next data science interview. Most people wont mention these tips, but they can be game-changing in your journey!One of the best ways to showcase your knowledge is through projects. This is especially true when youre a fresher or coming from a non-data science field. We encourage you to share updated on LinkedIn about your recent side projects, or blogs, or GitHub repositories.Take feedback from data science influencers or subject matter experts. Data science is still a small community and these folks are typically very happy to provide their thoughts if you ask appropriately.How will doing all this help land a job? Well, in our experience and discussions with data science recruiters, they admit they go through a candidates LinkedIn profile (including recent activity) before the interview. Sounds like a great opportunity to show-off, right? An interview is meant to be a two-way discussion. We are no longer in the Q and A era where you sit and rattle off answers to cliched questions. At least not in data science. These interviews give you a chance to assess the company and the interviewer.Candidates still dont use this opportunity as much as they should! They either dont ask any questions, or keep the questions limited to topics like work timings or the team size. This shows an inability to think outside the box and worse, a lack of curiosity. A big red flag in data science.Do your research about the company and the project beforehand. Prepare questions and try to pry as much information out of the interviewer as possible, such as the companys past performance, future plans, your role, etc.This is a very common mistake people have. There will be certain questions you wont know the answer to. Thats ok  its human to not know everything. But candidates still try to answer these questions by making up answers on the spot. This isnt a great look.Let the interviewer know that you arent an expert on the topic. Highlight a way to solve the problem which you already know. For example, if you dont know how a boosting algorithm works, you could solve the same problem using a technique you do know. And later, learn boosting and get back to the interviewer.This shows two important things to the interviewer:A common question we come across in our community  how do I showcase my data science skills if I dont have any previous work experience? Every interviewer demands it, and no fresher can seemingly grasp it. A classic catch-22 situation!Here are 4 ways to showcase your experience on real-world projects, even if you dont have previous industry experience:Internships are a very effective way to break into data science  even for experienced people. We have personally seen so many successful transitions enabled by internships. The bar for an intern is slightly lower than that for a full-time employee.You should absolutely check out this comprehensive guide to cracking your first data science internship. It is full of tips, tricks and resources to help you prepare for the interview process and the actual internship.If you are looking for a guided journey with mentorship  check out ourCertified Program on Data Science for Beginners (with Interviews). This program will complement your foray into data science and give you a huge advantage in your internship search.Hackathons and competitions usually provide industry related problems and challenges. So you not only get to work with these datasets, you can also gauge your standing among the top data scientists globally. Its a tremendous learning.You wont get a high score everytime. In fact, the first few times might be brutal in terms of where you finish. But dont give up! The key is in learning from each competition and improving the next time you participate. Eventually, your hard work will pay off.We regularly host hackathons on the DataHack platform. So go ahead and enroll there. There are plenty of FREE practice problems and datasets there as well. Practice, practice, practice!Meetups are a great way of meeting new people from various domains and expanding your network. Attend as many meet-ups as you can in your city. We have seen plenty of people form teams at these meetups and solve problems using data science.Quite often, people struggle to stick to a domain. There are so many datasets available online that it becomes tempting to pick one at random and try to solve it. While I like the enthusiasm, its not a great way to build up your own learning.The problem becomes useful when you add your own context to it. That will enable you to truly understand whats at stake. You would be aware of the depth of the problem and can decide which technique to use when. Its a powerful way to learn data science.So pick a problem that you have personally faced, or perhaps a recurring problem in your domain. As an interviewer, I would definitely be impressed with the proactive action.Kunal has illustrated this point using a few awesome examples. Make sure you listen to this episode!We are absolutely thrilled to be going on this new venture with you. Successfully clearing data science interviews is a challenge many have faced, and yet only a few have cleared. We hope this podcast, our course, and our platform will help you land your dream data science role.Below are the resources we mentioned in the article. Make sure you subscribe today and well see you around:",https://www.analyticsvidhya.com/blog/2019/02/tips-tricks-ace-data-science-interviews-podcast-analytics-vidhya/
[Announcement] Launching Analytics Vidhya Certified Programs and new Courses portal,Learn everything about Analytics|Analytics Vidhyas Journey in Trainings and Courses|Vision behind Analytics Vidhya Courses|1. Launch of Analytics Vidhyas Courses platform|2. Launch of Analytics Vidhya Certified Programs|End Notes,"So, what is new in Analytics Vidhyas Courses platform?|As usual  our loyal community members get an exciting launch offer. For the first 100 buyers  the programs are available at a 10% discount. You can use the code LAUNCH10 to avail the discount.|Related Articles|Tips and Tricks to Ace Data Science Interviews  Brand New Podcast Series by Analytics Vidhya!|Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixels Camera!|
Kunal Jain
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"You Dream Big,You Think Big,You Grow BigI have to confess  I have been living a dream for the last 5 years.When I started Analytics Vidhya  it was a blog, run with a passion to share knowledge with people interested in data science (or Analytics, as it was called then!). Little did I know we would get this big.The lines in the quote above resonate with me because that is how we have grown. We had a dream of what the world would look like in a few years, we thought about what we should do in this fast growing world of machine learning and data science, and that is how we grew!Our journey in trainings couldnt have been more interesting. We wanted to democratise data science and machine learning, but we were not sure if launching courses would be the right way to do it.Our community loved our blogs and the content we were creating, they trusted us with their career decisions and discussions. And we wanted to be doubly sure that they would love our courses. When we talked to our community members  we quickly realised how wrong we were!Almost all the community members unanimously said they would love to have courses and trainings from Analytics Vidhya. They already loved our content and the way we could simplify data science for them.We launched our trainings portal in May 2018 and we received phenomenal response from the community. As I write this article, more than 60,000 people have benefited from our courses.Our love for simplified content resonated with the community once more.During this period  we were closely watching our users and consistently taking feedback from them, which has led to 2 exciting and important offerings from Analytics Vidhya. But before I announce them, I wanted to take a couple of minutes and explain our vision behind Analytics Vidhyas Courses.Our courses aim to provide high quality, applied data science courses with industry ready projects at a compelling price.While there is no dearth of courses and trainings today  they are either too theoretical or they are very expensive. We believe that data science courses should be applied courses with a ton of hands-on projects and exercises. They need to make people ready for the next generation of jobs. And we believe we can provide these courses to our community at a very compelling price.Based on the feedback from our users and putting our brains and designers on the first platform  we are now excited to announce the launch of Analytics Vidhyas Courses platform. The DNA of all our courses remains the same:Example Certificate on our Courses PortalThe philosophy behind Analytics Vidhya Certified Programs is actually very simple. These programs have been designed to provide a comprehensive solution to our users and help them in their journey. Each program comes with a 12 month access and is built around a philosophy of providing end to end learning to beginners.Here are a few benefits about Analytics Vidhya Certified Programs:There are 3 programs up for grabs:When I look back at our journey  we have definitely come a long way. What makes me more proud is the impact we have created and the value we have added in peoples life. What keeps me excited is the 10x impact (of what we have already done) we will be creating in the next 3 years!So while we are living our dreams  it is time you started living yours. And if it is Data Science and Machine Learning  our programs are there for you.",https://www.analyticsvidhya.com/blog/2019/02/launching-analytics-vidhya-certified-programs-new-courses-portal/
Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixels Camera!,Learn everything about Analytics|Introduction|Table of Contents|Introduction to Image Segmentation|Algorithms for Image Segmentation|Getting Started with Googles DeepLab|Introduction to Atrous Convolutions|Depthwise Separable Convolutions|Depthwise Convolution|Pointwise Convolution|Understanding the DeepLab Model Architecture|Training DeepLabV3+ on a Custom Dataset|End Notes,"1. Semantic Segmentation|2. Instance Segmentation|Understanding the Techniques used in the Encoding and Decoding Phases|DeepLab V3+ Decoder|Training our Image Segmentation Model|Evaluting our Image Segmentation Model|Related Articles|[Announcement] Launching Analytics Vidhya Certified Programs and new Courses portal|7 Steps to crack your first Data Science Internship (Tips, Tricks and Resources!)|
saurabh pal
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Us humans are supremely adept at glancing at any image and understanding whats within it. In fact, its an almost imperceptible reaction from us. It takes us a fraction of a second to analyze.Its a completely different ball game for machines. There have been numerous attempts over the last couple of decades to make machines smarter at this task  and we might finally have cracked it, thanks to deep learning (and computer vision) techniques!These deep learning algorithms are especially prevalent in our smartphone cameras. They analyze every pixel in a given image to detect objects, blur the background, and a whole host of tricks.Most of these smartphones use multiple cameras to create that atmosphere. Google is in a league of its own, though. And I am delighted to be sharing an approach using their DeepLab V3+ model, which is present in Google Pixel phones, in this article!Lets build your first image segmentation model together!This article requires a good understanding of Convolutional Neural Networks (CNNs). Head over to the below article to learn about CNNs (or get a quick refresher):Image segmentation is the task of partitioning an image into multiple segments. This makes it a whole lot easier to analyze the given image. And essentially, isnt that what we are always striving for in computer vision? The below image perfectly illustrates the results of image segmentation:Source:gfycatThis is quite similar to grouping pixels together on the basis of specific characteristic(s). Now these characteristics can often lead to different types of image segmentation, which we can divide into the following:Lets take a moment to understand these concepts.Check out the below image:This is a classic example of semantic segmentation at work. Every pixel in the image belongs to one a particular class  car, building, window, etc. And all pixels belonging to a particular class have been assigned a single color. Awesome, right?To formally put a definition to this concept,Semantic segmentation is the task of assigning a class to every pixel in a given image.Note here that this is significantly different from classification. Classification assigns a single class to the whole image whereas semantic segmentation classifies every pixel of the image to one of the classes.Two popular applications of semantic segmentation include:I love the above image! It neatly showcases how instance segmentation differs from semantic segmentation. Take a second to analyze it before reading further.Different instances of the same class are segmented individually in instance segmentation. In other words, the segments are instance-aware. We can see in the above image that different instances of the same class (person) have been given different labels.Image segmentation is a long standing computer Vision problem. Quite a few algorithms have been designed to solve this task, such as the Watershed algorithm, Image thresholding , K-means clustering, Graph partitioning methods, etc.Many deep learning architectures (like fully connected networks for image segmentation) have also been proposed, but Googles DeepLab model has given the best results till date. Thats why well focus on using DeepLab in this article.DeepLab is a state-of-the-art semantic segmentation model designed and open-sourced by Google back in 2016. Multiple improvements have been made to the model since then, including DeepLab V2 , DeepLab V3 and the latest DeepLab V3+.We will understand the architecture behind DeepLab V3+ in this section and learn how to use it on our custom dataset.The DeepLab model is broadly composed of two steps:What kind of techniques are used in both these phases? Lets find out!The DeepLab architecture is based on combining two popular neural network architectures:We need to make sure our model is robust to changes in the size of objects when working with CNNs. This is because if our model was trained using only images of small objects, then it might not perform well with scaled versions of the input images.This problem can be resolved by using spatial pyramid pooling networks. These use multiple scaled versions of the input for training and hence capture multi-scale information.Spatial pyramid pooling networks are able to encode multi-scale contextual information. This is done by probing the incoming features or pooling operations at multiple rates and with an effective field of view.Spatial pyramid pooling networks generally use parallel versions of the same underlying network to train on inputs at different scales and combine the features at a later step.Not everything present in the input will be useful for our model. We would want to extract only the crucial features that can be used to represent most of the information. Thats just a good rule of thumb to follow in general.This is where the Encoder-Decoder networks perform well. They learn to transform the input into a dense form that can be used to represent all the input information (even reconstruct the input).Dilated ConvolutionNormal ConvolutionSpatial pyramid pooling uses multiple instances of the same architecture. This leads to an increase in the computational complexity and the memory requirements of training. Not all of us have GPUs running freely so how do we go about mitigating this?As usual, Google has the answer.DeepLab has introduced the concept of atrous convolutions, a generalized form of the convolution operation. Atrous convolutions require a parameter called rate which is used to explicitly control the effective field of view of the convolution. The generalized form of atrous convolutions is given as:The normal convolution is a special case of atrous convolutions with r = 1.Hence, atrous convolutions can capture information from a larger effective field of view while using the same number of parameters and computational complexity.DeepLab uses atrous convolution with rates 6, 12 and 18.The name Atrous Spatial Pyramid Pooling (ASPP)was born thanks toDeepLab using Spatial Pyramid Pooling with atrous convolutions.Here, ASPP uses 4 parallel operations, i.e. 1 x 1 convolution and 3 x 3 atrous convolution with rates [6, 12, 18]. It also adds image level features with Global Average Pooling. Bilinear upsampling is used to scale the features to the correct dimensions.Depthwise convolutions is a technique for performing convolutions with less number of computations than a standard convolution operation. This involves breaking down the convolution operation into two steps:Lets understand this using an example.Suppose we have an image of size 12 x 12 composed of 3 channels. So, the shape of the input will be 12 x 12 x 3. We want to apply a convolution of 5 x 5 on this input.Since we have 3 kernels of 5 x 5 for each input channel, applying convolution with these kernels gives an output shape of 8 x 8 x 1. We need to use more kernels and stack the outputs togetherin order to increase the number of output channels.Ill illustrate these two concepts using diagrams to give you an intuitive understanding of what were talking about.In this first step, we apply a convolution with a single kernel of shape 5 x 5 x 1, giving us an output of size 8 x 8 x 3:Now, we want to increase the number of channels. Well use 1 x 1 kernels with a depth matching the depth of the input image (3 in our case). This 1 x 1 x 3 convolution gives an output of shape 8 x 8 x 1. We can use as many 1 x 1 x 3 convolutions as required to increase the number of channels:Lets say we want to increase the number of channels to 256. What should we do? I want you to think about this before you see the solution.We can use 256 1 x 1 x 3 over the input of 8 x 8 x 3 and get an output shape of 8 x 8 x 256.DeepLab V3 uses ImageNets pretrained Resnet-101 with atrous convolutions as its main feature extractor. In the modified ResNet model, the last ResNet block uses atrous convolutions with different dilation rates. It uses Atrous Spatial Pyramid Pooling and bilinear upsampling for the decoder module on top of the modified ResNet block.DeepLab V3+ uses Aligned Xception as its main feature extractor, with the following modifications:The encoder is based on anoutput stride(ratio of the original image size to the size of the final encoded features) of 16. Instead of using bilinear upsampling with a factor of 16, the encoded features are first upsampled with a factor of 4 and concatenated with corresponding low level features from the encoder module having the same spatial dimensions.Before concatenating, 1 x 1 convolutions are applied on the low level features to reduce the number of channels. After concatenation, a few 3 x 3 convolutions are applied and the features are upsampled by a factor of 4. This gives the output of the same size as that of the input image.Lets get our hands dirty with coding! First, clone Google researchs Github repo to download all the code to your local machine.Preparing the dataset: For training the DeepLab model on our custom dataset, we need to convert the data to the TFRecord format. Move your dataset to model/research/deeplab/datasets. Our dataset directory should have the following structure:TFRecord is TensorFlows custom binary data storage format. It makes it easier to work with huge datasets because binary data occupies much less space and can be read very efficiently.The TFRecords format comes in very handy when working with datasets that are too large to be stored in the memory. Now only the data thats required at the time is read from the disk. Sounds like a win-win!Now, run the build_voc2012_data.py with the values of flags changed according to our directory structure. This converts your data to TFRecord format and saves it to the location pointed by   output_dir.Open segmentation_dataset.py and add a DatasetDescriptor corresponding to your custom dataset. For example, we used the Pascal dataset with 1464 images for training and 1449 images for validation.And now its time train our own image segmentation model!We need to run the train.py file present in the models/research/deeplab/ folder. Change the Flags according to your requirements.This will train the model on your dataset and save the checkpoint files to train_logdir.Now that we have the checkpoint files for our trained model, we can use them to evaluate its performance. Run the eval.py script with the changed FLAGs. This will evaluate the model on the images mentioned in the val.txt file.We ran the training phase for 1000 steps and got meanIntersectionOverUnion of 0.834894478.Remember that the the model_variant for both training and evaluation must be same.Similarly, run vis.py with respective FLAGs for visualizing our results:Lets see some results from our trained model.   Looking good! Congratulations on training and running your first image segmentation model.That was quite a lot of learning to digest! Once youve familiarized yourself with these concepts, try using it for your custom dataset (Kitti is a good choice because of its small size) and find out more cool practical use cases.I strongly encourage you to check out the DeepLab paper and the Google AI blog post regarding this release:I look forward to sharing your feedback, suggestions, and experience using DeepLab. You can connect with me in the comments section below.",https://www.analyticsvidhya.com/blog/2019/02/tutorial-semantic-segmentation-google-deeplab/
"7 Steps to crack your first Data Science Internship (Tips, Tricks and Resources!)",Learn everything about Analytics|Introduction|Table of Contents|1. Getting Familiar with the Basic Data Science Terminology|2. Start your Data Science Journey|3. Build your Digital Portfolio (Online Profile)|4. Dos and Donts for Crafting your Data Science Resume|5. Prepare for your Data Science Internship Interview|6. Boost your Chances of Selection|7. What will you Learn during the Internship?|End Notes,"1.1 What is Data Science?|1.2 Data Scientist vs Statistician|1.3 Common Terminologies in Data Science|2.1 Understanding Statistics and Probability|2.2 Good Coding Skills (Pick a Programming Language)|2.3 Basic Machine Learning Algorithms|3.1 Work on Projects|3.2 Create a GitHub Profile|3.3 Write Blogs|3.4 Create and Optimize your LinkedIn Profile|5.1 Structured Thinking|5.2 Knowledge about the Company youre Applying to|6.1 Advanced Machine Learning|6.2 Participate in Data Science Competitions|7.1 How to solve real-world projects|7.2 Ways to tell Data Stories (Exploratory Data Analysis)|7.3 Team Work|7.4 Gain practical experience in the field|Share this:|Related Articles|Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixels Camera!|DataHack Radio #18: Andriy Burkovs Journey to Writing the Ultimate 100-Page Machine Learning Book|
Aishwarya Singh
|9 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"I came across all kinds of advice when I was looking for a data science internship. Theres no dearth of people espousing the value of internships in data science. But surprisingly, not many people talk about how to land that internship.My learning journey during my internship with Analytics Vidhya was equal parts challenging and fulfilling. I realized how vast and complex data science is and how unprepared I was for a full-time role. My path to become a data scientist would have been far more arduous and difficult one if I hadnt first interned.Even for experience people  internships are a very effective way to break into data science. We have now seen so many successful transitions enabled by internships.If you are looking for tips to prepare yourself for a data science internship, then youve come to the right place!In this article, Ive drawn on my experience on the key aspects you need to know to land your first internship in data science. Each section is filled with plenty of tips, tricks, and resources. It wont be easy  but you would know what needs to be done.If you are looking for a guided journey with mentorship  check out our Certified Program: Data Science for Beginners (with Interviews). This program will complement your foray into data science and give you a huge advantage in your internship search.Note: The focus of this article will be only on the technical skills that one needs for a data science internship. If you have any suggestions that can help the community, do share your thoughts in the comment section below.Whats the first step, the absolute ground zero, before you start applying to internships? Understanding what data science is!Lets take a moment to answer this question, look at the different roles in data science, and familiarize ourselves with common terminologies in this field.Its important to know what youll be working on in the first place. Answer this question before anything else  why do you want to work in data science? Is it because you love programming, math, statistics and the opportunities they offer? Or are you going with the flow since Data Science and Machine Learning are currently trending?The amount of data being generated every day is increasing exponentially! The sources of data and the ability to collect and store it has come a long way in the last decade. Companies are using a variety of tools and techniques to mine patterns in the data and gather useful insights. That, in a nutshell, is what data science is all about.Data really powers everything that we do.  Jeff Weiner, LinkedIn CEOSimply put, data science involves the use of various techniques to understand data and build predictive models to make business decisions. A few popular applications of data science include fraud detection, sports analytics, airline route planning, etc. Here is an article that lists down 13 mind-blowing applications of data science:So if data science is all about deriving insights and finding patterns from the data, then what is the difference between a data scientist and a statistician? Excellent question! Lets find out.Both data scientists and statisticians work with the data to derive useful insights from it. A statistician is focused on identifying the relationship in the data while a data scientist works towards using the relationships and building models to predict future outcomes. The aim of a data scientist is to build a generalized model with high accuracy.Statisticians often use tools like R, Excel, or MATLAB, since these have a number of libraries for data analysis. Data scientists, on the other hand, mostly work with Python, Apache Spark, etc. for exploring the data and building models. Below is a cool infographic that summarizes the differences between these two roles:Heres another well illustrated article showcasing the variety of roles available in data science. Please understand that data scientist isnt the only job in this field!Data science is a complex and vast field. Lets understand its different components so you can narrow down your area of focus in the long term.Machine Learning: Machine learning is the use of algorithms (such as linear regression, logistic regression, decision trees, etc.) to learn from the data and make informed decisions. For example, using past data of people who have taken loans and trying to predict if theyll come back for another loan.Deep Learning: Deep Learning is a subset of machine learning, designed to mimic the decision making capabilities of humans. For instance, identifying the objects in a given image, or classifying images as a cat or dog. If you are still skeptical, about the difference between machine learning and deep learning, you can check our this link: Deep Learning vs. Machine Learning  the essential differences you need to know!Natural Language Processing (NLP): NLP is a branch of data science that deals with analyzing, understanding, and deriving information from text data. All those reviews you see on Amazon? Or all the Tweets you browse through daily? NLP techniques are used to parse through them and understand the sentiment of users. NLP is one of the hottest fields in data science right now.Computer Vision: As the name suggests, computer vision gives machines the ability to see and understand their surroundings. Ever notice how Facebook automatically suggests tags in a picture? Or how self-driving cars detect objects on the road? These are prime examples of computer vision. This is another field thatll see a lot of jobs come up in the next few years.Recommendation Engines:Anyone who has ever used Flipkart or Amazon has been part of a recommendation engine. This consists of analyzing the past user behavior to offer relevant recommendations or suggestions. Customers who bought this also bought or Recommended for you based on your past purchase are examples of recommendation engines at work.So youve decided to take the plunge. You want to become a data scientist and nothing can stop you. First, congratulations on picking the hottest field in the industry!If youre a fresher with no industry experience, internships are the best way to land a role in data science. They offer you a chance to get industry experience while working with experienced veterans. There is so much to learn in those few months that will shape your professional career.In the next few sections, we shall look at the essential skills required to land your first data science internship.Note: As mentioned previously, we will focus on the technical aspect of your portfolio rather than the soft skills (such as good attitude, confidence, etc.) required to clear a typical data science internship interview.Statistics and probability are the fundamental core skills required for data science. Without a solid understanding of these two, you wont make much headway in this field (or the interview process!). From analyzing the data and making valuable inferences to understanding how the model works, the basic concepts of stats and probability are integrated in the data science ecosystem.There are a number of statistical techniques and probability distributions we can leverage to understand the structure of the given data. Here are a few important topics that you will be using while working on a data science problem:You can expect a bunch of questions in your interview from these two fields (statistics and probability). Below is a list of useful resources to help you get started (or revise certain concepts):Yes, you need to know programming to become a data scientist. Theres no getting away from it. AutoML (automated machine learning) is gradually being accepted in the industry but right now, theres no alternative to cold hard coding skills.The two most popular programming tools these days for data science are Python and R. You must be familiar with at least one of the two. These are both open source programming languages, with a massive active community thats growing by the day.R is mainly used for exploratory work and is preferred for statistical analysis tasks. It has a comparatively bigger library base for statistical packages. On the other hand, Python is preferred for machine learning and deep learning tasks. It has numerous machine learning and deep learning libraries and packages.Here are a few articles to help you get started with Python and R:Python is definitely more in-demand in the industry these days. Its an easy choice if youre inclined towards learning advanced machine learning topics and of course, deep learning. The flexibility Python provides is unparalleled in these tasks. R is a wonderfully adept tool for doing exploratory analysis, including producing some really insightful and aesthetically pleasing plots.If you have covered the basics of statistics and probability, and have worked on your coding skills, the next step would be to learn about the basics of machine learning. Make yourself familiar with common machine learning algorithms, like linear regression, logistic regression, decision tree, random forest, naive bayes, k-nearest neighbour, and support vector machines.Try to focus on one algorithm at a time and understand the intuition behind each technique. Having a theoretical knowledge of the algorithms and how they work is as important as being able to implement the algorithm. If you know how the algorithm works, it will be easier for you to understand the various parameters of the algorithm, tuning those parameters and also for deciding which algorithm to use with which type of data.You can refer to this article to learn the above mentioned algorithms in detail:Youve worked so hard to learn all these new concepts. You should complement all that effort with learning how to showcase your skills.Statistics, programming and machine learning alone will likely not land you that internship. You need to build your digital presence. Showcase your immense potential and demonstrate the skills you have acquired during your data science journey. Let the world know!In this section, well look at the different ways you can leverage to build your profile.I believe the best way to learn anything is by putting your knowledge into practice. Nothing says I know this technique like showcasing it in a project.Building an end-to-end project gives you an idea about the different possibilities and challenges a data scientist potentially faces in a day-to-day role.You can look for open source projects relevant to your field of interest. Trust me, theres no dearth of data on the internet. Im a huge fan of fiction so I love using NLP to analyze the work of my favorite authors. This shows your passion for data science and gives you an edge in the eyes of your future employer.Here are a few practice problems to get some valuable hands-on experience:Remember these are just to get you started. You can browse through our entire project list here Practice Problems on the DataHack Platform. If you need ideas for projects related to machine learning, have a look at this Discussion thread. Get your hands dirty!
You should also start building your GitHub profile at this stage. This is essentially your data science resume which anyone in the world can access.Most data science recruiters and interviewers look at the candidates GitHub profile to evaluate his/her potential. While you work on your projects, you can simultaneously list down the problem statement and code on your GitHub. Ive put together a small checklist you can follow next time youre adding your code to GitHub:Ill tell you a big secret that propelled my data science career  writing articles. I have made it a habit to take notes ehenever Im learning a new concept. Its easy converting that into an article later. This helps me understand the technique in a much clearer and lucid manner.You should also do the same! Our community is happy to share their thoughts and feedback with you. When you put your articles out in public, people often share their views  such as adding a visualization of actual vs predicted could be helpful, which can help you improve.Quora can be considered as an alternate option to writing blogs (which is where I first started writing). Breaking down a complex topic into easy-to-understand words helped you grasp the topic and fine tunes your structured thinking skills.To start with, you can write about some basic topics, like Data Exploration using the matplotlib library, your approach and solution for a practice problem, a summary or notes of a MOOC you completed, etc.LinkedIn is the worlds biggest professional network. You should be on it even if youre a fresher or still finishing up graduate school.Recruiters often use LinkedIn to either verify your profile or reach out to you in case of an opportunity. You can consider it as your second resume, or the digital version of your paper resume. If you apply for an internship and your profile isnt updated (or doesnt exist), you might miss out.Optimize your LinkedIn profile according to the internship youre applying to. Update your past experience (if any), education level, projects and interest. If you havent already created a profile, do it now.You should also start building your network by connecting with people in data science.There are PLENTY of them including tons of influencers who regularly post useful developments. You should consider this step utterly mandatory.Your resume is essentially your professional careers highlight reel. Its the first thing the recruiter/hiring manager looks at so crafting the perfect resume is absolutely critical in your quest to get an internship.Even if you possess every skill listed in an internships requirements section, theres a good chance you might not get the interview call if your resume is not up to the mark.You must, absolutely must, spend a good amount of time on creating and perfecting your resume.So what are some key things to keep in mind while doing this?Make sure your resume is up to date and does not have any spelling mistakes. Check it twice, perhaps even thrice. Make your colleague or friend review it from a recruiters perspective.Source: Ryan Eccleston (Behance)Always keep this in mind when youre creating or updating your resume:Write what you know, and know what you write.Remember that project you did in first year of college, some 2-3 years ago, the details of which you cant recall? Either study about it, or dont add it on your resume. Having 10 projects that you cannot talk about is a red flag for the recruiter! The same goes for all the technical skills you pen down.Here is an excellent article on Tips to Prepare an Outstanding CV for Data Science Roles.Make sure you take out some time and watch the below video by Kristen Kehrer. She has a wealth of experience in this field and has gone through hundreds of resumes in her career. In this video, she talks about the important points one needs to remember while building a data science resume and provides tips and tricks for cracking the interview process.You can also check out her course  Up Level your Data Science Resume, to get a deeper insight into how data science resumes should be designed.The biggest challenge in getting a data science internship is undoubtedly the interview process. Given that you dont have previous work experience in this field, what aspects of your resume will the recruiter look at? What skills should you demonstrate in your resume and in the actual interview?Big questions! Knowing how to navigate these tricky waters could make or break your chances of getting the internship.You will, of course, mention the projects you have worked on (or are currently in progress). But apart from that, there are certain topics which the interviewer will be keen to test you on, irrespective of the background you come from. This section looks at the key things you need to focus on and prepare for the interview.The ability to structure your thoughts is an invaluable skill in the complex world of data science. The interviewer will judge you on your ability to break down a problem statement into smaller steps. How you do that is where the goldmine lies.For any given problem statement, it is necessary to identify what the end goal is. The next step is to understand the data youre given and pen down a process required to get to that end goal. And all of this happens in a finite time frame (the interviewer does not have all day!). Are you seeing why its so important to have a structured thinking mindset?To check your structured thinking skills, you would be given a question like  How many mails are being sent at the moment? Thats what I was asked during my interview. Or how many red colored cars are on the road in Bangalore? How many cigarettes are sold in a day in India?For example, if I want to understand why charge offs have increased suddenly in credit card portfolio over the last month, I would lay it down in a structure similar to this:These are questions with no precise solution. So how do you go about solving them? The first thing to understand is that the interviewer does not expect an exact numerical answer. Instead, they are trying to understand how you look at the problem and your approach to getting the final answer. Its a good idea to ask for a pen and paper (or a whiteboard) so you can demonstrate your thinking step-by-step.I highly recommend going through this article  The Art of Structured Thinking and Analyzing. The piece explores everything you need to know about structured thinking and showcases a few examples to help you enhance your structured thinking skills.You might feel that this point isnt relevant to the discussion. This isnt something that needs to be mentioned since everyone goes through the job description before applying. Its a fair point.But just browsing through the JD isnt good enough.We regularly hear from recruiters how prospective candidates walk in without having read about the role they are interviewing for. I have personally seen people take up the internship and leave within a couple of weeks because they did not like their role.You must know what the company does and the organizations vision before you decide to apply for the job. There are no two ways about it.Id suggest researching about the company to understand what they work on. How do you see yourself fitting in? Can you directly see an impact you could make with your skillset? You must also go through the job description thoroughly and ask questions during the interview to understand your fit with the company. This will save your time and the companys as well.I encourage you to go through this guide that lists down the important topics to cover while preparing for a data science interview:The pointers weve seen so far can safely be shelved under the MUST-HAVE category. You simply cant make do without ensuring you check each one of them. But you can further enhance your existing skillsetto stand out from the competition. And who doesnt want to do that?!In this section, I have drawn on my own internship experience to give you a few additional tips and tricks to boost your chances of getting selected.Nothing will impress the interviewer more than watching you confidently answer advanced machine learning questions. Most folks theyll interview will be able to solve basic questions. Holding advanced ML knowledge will most definitely give you the edge.Make sure you have covered the basic machine learning topics first that we discussed earlier (stats, probability, regression, tree algorithms, etc.). You can then safely jump into the advanced ML algorithms, recommendation systems, time series forecasting algorithms, etc.At this stage in your career, is not necessary to know ALL the algorithms in detail. Im sure youll 3-4 techniques that you find really helpful so learn them well  and rattle them off in your interview. You should have a fair understanding of the algorithm(s) and the math behind it. You can choose a particular field based on your interest and explore the various techniques in that domain.To give an example, if you are interested in time series, you start exploring the different forecasting techniques, the concept of stationarity or even pick a project on time series and work on it. Or if NLP is the field that interests you, you can work on understanding how features are extracted from text based data, what algorithms can be used on textual data and so on.Here are some important links to get you started:This adds a MASSIVE boost to your resume and increases your chances of getting the internship. Having worked on, or completed a project, is proof that your knowledge is not restricted to just books. You have clearly made an attempt to translate your theoretical learning to a real-world dataset  a sure-shot sign that your curiosity, passion and will to learn is quite high.To start with, I encourage you to participate in data science competitions. Start with the hackathons listed on AVs DataHack platform or on Kaggle. These platforms provide problem statements which mimic real-world scenarios, thus giving you an invaluable exposure to what life in the industry will feel like.You also get to compete with (and learn from) top data scientists from around the world. This acts as a good barometer for your own progress. Keep practicing and youll be surprised how quickly you rise in the leaderboard rankings. Practice is KING in data science.You can check out the upcoming hackathons and webinars here: DataHack Contests. But for anyone starting out for the first time, I would suggest going through our practice problems first.What can an internship give you that textbooks, MOOCs and videos cant?Practical experience.The single most valuable commodity the hiring manager will value when pouring over your profile. I realized how useful this is during my own internship with Analytics Vidhya.There is so much you can learn from your internship if you go in with an open mind and a willingness to learn every single day. Thats exactly how you succeed in data science!In this section, I have penned down the major takeaways I experienced during my data science internship.You would be working on a real-life project during your internship. This is invaluable experience. Once youre on board, you might well find yourself entrenched in the end-to-end data science lifecycle, including defining the problem statement and building models.If you previously participated in data science competitions, you will have an idea about the different challenges data scientists come across. But heres the caveat.The problem statements and the datasets provided in these competitions are very different from real-world scenarios. The datasets are messy and unstructured in the industry. Theres a ton of data cleaning work required before any model can be built.In fact, dont be surprised if 70-80% of your tasks involve data cleaning.You will learn how to structure a problem statement, understand the domain and the data required to solve the problem, and then figure out sources to extract that data. The next step is to get knee deep into research. Find out the approaches other data scientists have taken to solve similar problems.This will give you a fair idea about what should work well and what is not worth investing time on. While experiments are encouraged in data science, theres a limit to how much creative freedom youll get from your manager. Filter out the aspects you know wont work beforehand.People often spend more time working on building a model than understanding the data. I myself used to do that for a long time. It was during my internship, while I was working with on a project when I realized how wrong my approach was. NoI cannot stress enough how important it is to really understand the data you have. There are so many levels and hidden aspects in a dataset which we often overlook in our haste to build models. This is something you will learn during your internship (but should be prepared for beforehand).Spend as much time exploring the data as you can! Plot graphs, find patterns, and just dive into it like its the best work in the world (because it is!). Try to understand the distribution, look for factors that affect your target variable and make inferences. Build a hypothesis, visualize data, find insights, and most importantly, discuss your findings with your teammates.One of the perks of a data science internship is working with incredibly smart and supportive people. Data science projects require collaboration and coordination among colleagues as you work towards the end goal. I consider myself lucky to be a part of such a great team.The best part about working in a team is that you always have someone to discuss your thoughts (and clarify your doubts). For instance, during my internship at Analytics Vidhya, we took part as a team in a huge hackathon. The dataset had multiple files, so we divided the task and each of us worked on understanding a particular file and shareed our knowledge with the rest of the team.It was an amazing experience.I learned different approaches to tackle a problem and techniques to improve/optimize my code during those discussions. Working as a team would not only help you build your soft skills, but also hone your technical skills. A win-win combination!When you start your data science job search, you will most likely find that most companies ask for some experience in the domain. You should find out the kind of problems your company is working on, and think of ways in which you can contribute. Discuss your ideas with the people who are working on the project(s).You should also try to understand the roles of other people in the company. You can talk and discuss with people in different teams. For instance, talk to the marketing team and understand if you can perhaps think of a data drive solution to their problems. Make the most of the opportunity you have got. Be curious, ask relevant questions and learn from your team.I had a ball writing this article. There was so much I learned during my internship and this made me relive those moments. When I look back at that time, there was so much I didnt know. I hope this article will help you overcome the obstacles I initially faced.If you have any questions or want to share your experience with me and the community, please do so in the comments section below. You can also check out the below resources to accelerate your chances of getting a data science internship:",https://www.analyticsvidhya.com/blog/2019/02/ultimate-guide-first-data-science-internship-tips-tricks-resources/
DataHack Radio #18: Andriy Burkovs Journey to Writing the Ultimate 100-Page Machine Learning Book,Learn everything about Analytics|Introduction|Andriy Burkovs Background & Foray into Artificial Intelligence|Gaining Valuable Industry Experience Post-Ph.D|Andriys Interest and Work in NLP|Idea behind the Hundred-Page Machine Learning Book|Challenges and Trade-offs while Writing the Book|Advice to People Entering Data Science (Aspiring Data Science Professionals)|End Notes,"Share this:|Related Articles|7 Steps to crack your first Data Science Internship (Tips, Tricks and Resources!)|Dont Miss this Comprehensive 7 Step Process to Ace Data Science Interviews!|
Pranav Dar
|One Comment|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Have you seen most of the recommended books on Machine Learning only to feel overwhelmed by their thickness and the amount of effort it will take to read those books?If you feel that way  dont worry! You are not alone. A lot of people face this situation but do very little about it. Not Andriy Burkov! Andriy saw this and thought that the ideal Machine Learning book for beginners should be written within 100 pages.More importantly  he wrote the book and published it. His recently launched Hundred-Page Machine Learning Book has quickly ascended the bestseller list and is perched at #1 on Amazon (under the machine learning category). The book has even been endorsed by the great Peter Norvig!It was our pleasure hosting him on episode #18 of DataHack Radio. Kunal and Andriy had a rich discussion on several topics, including:All DataHack Radio episodes are available on the below podcast platforms. Subscribe today!Andriys professional career began in Ukraine at the turn of the millennium. He created his own online startup while doing his graduation in Computer Engineering and Networking. But after working on this for 3 years, the dotcom bubble burst and his investor decided to withdraw.Andriy wasnt about to give up on his dreams. The fire within him to build another online startup continued to burn bright. This wasnt possible in Ukraine back then though, given the economic situation. Attracting another investor was proving to be impossible.The first thought Andriy had was to move to Europe with his family. They considered France but eventually settled on Quebec, Canada. The primary reason was immigration purposes and Quebec seemed a good fit as it has a French dwelling community.Once in Canada, Andriy spent some time looking for jobs before finally settling on doing his Masters in computer science and artificial intelligence. He converted this into a Ph.D, choosing multi-agent systems as his primary topic. His thesis Leveraging Repeated Games for Solving Complex Multi-agent Decision Problems.Want to see how much the field of AI has changed in the last decade? Heres an eye-opening anecdote Andriy told us:One of my ex-colleagues, when he finished his Ph.D in Quebec City (several years before me), couldnt find a job in artificial intelligence. It was such an exotic field.Its mind-blowing how quickly technology changes our lives.Two paths opened up once Andriy finished his Ph.D  research or teaching. The former appealed to him far more than becoming a full-time professor. So he decided to dip his toes into an industry role with Fujitsu, a Japanese multinational company. He worked with Fujistsu for 2 years before moving on.From there, Andriy shifted to WANTED Technologies, a job announcement portal for companies. His role was primarily about turning terabytes of job announcements from online job boards into structured knowledge. An excellent experience that has played a big part in his professional career.That was followed by a move to a company that was acquired twice, the second time by Gartner. In Andriys own words:I survived two acquisitions!There was an understandable seed of doubt  will working for such a huge organization stifle creativity? But Gartners work culture soon put any doubts to rest.Andriy leads a team working on the research and development side of a product called Talent Neuron. It is aportal that combines big data and statistical insights to provide global talent, location and competitive intelligence for any industry or function.A quick glance at Andriys LinkedIn profile tells us he is a Natural Language Processing (NLP) expert. Thats a topic we at Analytics Vidhya are very passionate about. So picking Andriys brains on NLP felt like a natural fit!His teams role at Gartner is geared more towards applied text analytics, rather than core computational linguistics. I have mentioned a few intriguing tasks the team works on (or has worked on):This is one of the most absorbing sections in this episode, especially for NLP enthusiasts. Andriy spoke about the complexities and nuances of language modeling with several examples. A must-listen!How did the idea of writing this book come about? It definitely wasnt the usual route! The thought hadnt even crossed Andriys mind initially.Theres a huge following on LinkedIn Andriy has built up over the years. He posts some superb stuff related to machine learning everyday (you should definitely give him a follow).I have a ton of machine learning and artificial intelligence books at home. But I never end up finishing any of them. I asked myself why?A huge reason for that is weve become increasingly busy. We barely have time to finish a book, especially one that runs over 500 pages. He put his thoughts in a LinkedIn post, saying that if he ever wrote a book on machine learning, it wouldnt be more than 100 pages long.He had no intention of actually writing it though! The post went viral  hundreds of likes, tons of comments. The majority of comments could be divided into two categories:I took a week to think about it. I told myself  I will try to write several chapters. If it goes well, well see. If it doesnt go well, Ill stop.The first three chapters were written over a weekend and the response? Overwhelmingly positive. And while the number of pages ran a shade above hundred, Andriy still managed to pack in all the fundamentals and essential knowledge a data scientist should have. A tremendous achievement!One of the most prominent things about the book is that its available to read online for anyone. If you like it or found it useful, you should buy the paperback or hardcover edition. I really appreciate Andriys thought process behind doing this.Given the crisp nature of the book, leaving certain concepts out was inevitable. But which ones? And to what degree? Those were critical questions Andriy addressed in this section.The idea, as we discussed earlier, was to include all the fundamentals, such as the mathematics behind core machine learning algorithms. So topics like reinforcement learning and back propagation were either not included or just brushed upon. A fair trade-off, in my opinion.Heres a brief summary of the key points Andriy spoke about:Just hearing Andriy talk about his book made me appreciate how hard the journey must have been. I have personally read the book and couldnt recommend it enough. It is a gem and will help thousands of aspiring data scientists make the leap into this field.Have you read the book yet? Let me know in the comments section below!",https://www.analyticsvidhya.com/blog/2019/02/datahack-radio-hundred-page-machine-learning-book-andriy-burkov/
Dont Miss this Comprehensive 7 Step Process to Ace Data Science Interviews!,"Learn everything about Analytics|Introduction|Heres the 7 Step Data Science Interview Process|1. Understand the Different Roles, Skills and Interviews in Data Science|2. Getting Ready for Interviews  Build your Digital Presence|3. Prepare your Resume and Start Applying|Common Mistake People Make at this Stage|4. Telephonic Screening|5. Getting through the Assignments|6. In-Person Interaction(s)|7. Post-Interview Steps|End Notes and Infographic","Common Mistake People Make at this Stage|Common Mistake People Make at this Stage|Common Mistake People Make at this Stage|Common Mistake People Make at this Stage|Common Mistake People Make at this Stage|Common Mistake People Make at this Stage|Share this:|Related Articles|DataHack Radio #18: Andriy Burkovs Journey to Writing the Ultimate 100-Page Machine Learning Book|Its a Record-Breaking Crowd! A Must-Read Tutorial to Build your First Crowd Counting Model using Deep Learning|
Pranav Dar
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Are you looking to get a break in data science but struggling to clear interviews? Are you scared of getting into data science interviews? Or you just arent sure what to expect in these interviews?You might even know the data science tools and techniques relevant to the job. And yet the employers keep rejecting you. It certainly doesnt help when job descriptions require multiple years of experience for a seemingly entry-level role!Ive been there. I have a rich background in learning and development, a non-technical and non-data science field. It took me months of hard work and disciplined effort, and numerous botched interviews, before finally landing a data science role here at Analytics Vidhya.My aim in this article is to list down a framework we have come up with to help you ace your data science interviews. We have drawn on our own experience plus reached out to multiple data science experts to craft this comprehensive 7 step framework. It has already helped multiple people, including me, streamline their data science interview preparation.This framework is part of the Ace Data Science Interviews course.This course has been created based on hundreds of interviews we have taken and worked with companies to help them find data science talent.The 7 step process starts right from the stage you start researching the different roles that interest you. And it goes all the way up to the completion of in-person interviews.Keep in mind that this is a comprehensive framework. You might not have to go through each and every step in your interview journey.In this article, we will analyze all these steps and look at the mistakes people make in each step. Additionally, all these 7 steps are packaged together neatly in a wonderfully illustrated infographic at the end of this article. Keep it handy throughout your interview preparation!Have you picked the role you want to pursue in data science? The most common answer I get from most folks is I want to be a data scientist. What else is there in data science?The first thing you need to understand is that there are a variety of roles in the data science ecosystem. A typical data science project has a lifecycle thats made up of several functions. A data scientist is just one component in a successful data science project. Heres a quick run through of the different job roles that currently exist:I could go on but you get the idea. I recommend spending time to research these different roles and pinpointing one that you see yourself in. We have crafted an intuitive question sheet as part of the Ace Data Science Interviews course that helps you make this decision.The next step is to understand the skills required in these roles.For example, you need to have strong Python and Software Engineering background for a data engineer role  but communication skills are not that critical. On the other hand, if you want to get into a Business Analyst role  you need to have good communication and problem solving skills. You may not need to know Python.Now that you know which role you want to pursue and what skills are required to get there, its essential to map the kind of interviews you might face. Again, these interviews will differ from role to roleA data scientist will be grilled on his/her machine learning knowledge, grasp on the tool(s), domain expertise, communication skills, among other things. A data engineer will primarily be tested on his/her software engineering and programming skills. Understand the different nuances expected in your role and prepare accordingly.Ill circle back to my original question  which data science role do you see yourself in? Most people Ive seen make a huge mistake at this stage. They dont put in the time to understand the nuances of these different roles.Hence, their interview preparation remains the same regardless of the role. Dont make this mistake!Even if the interview format is the same, the expectation of the interviewer would change depending on the role you choose.My suggestion would be to talk to people in your network who are in this field. Pick their brain on what their understanding is of each role. You can also leave your questions for me below this article  I will be happy to answer any queries.Done your due diligence so far? Good, because its time to move on to step #2  building your digital presence.More than 80% recruiters we spoke to admitted they check a candidates LinkedIn profile before calling them for an interview. Thats right  we are living in the middle of a digital revolution. Simply relying on a 1 or 2 page resume is not enough. The recruiting firm wants evidence to back up the claims in your resume.The good news  theres no dearth of options to do this:The options are endless!You need to pick the medium(s) depending on the roles you want to apply for and your own strengths (for instance, a Business Analyst might not need a GitHub account).Dont leave this step till the end! Ive seen people creating a GitHub profile a few hours before applying to a job. By the time you are doing or applying for interviews  you wont have the desired digital clout.This last minute dash simply doesnt leave enough time to optimize your details according to the role requirements.The same story applies to LinkedIn. It takes patience and commitment to build your network. It took me over a year to connect with the right people in data science and build up my profile there.Our advice  you need to do this at least 6 months before you apply for the jobs.This will not only ensure that you get enoughtime to build your presence, but also take away the pressurefrom the overall process.If I had to pick the toughest step in the data science process, this would be it. Yes, I am putting this above the in-person technical interviews. You can study and brush up your data science knowledge. But crafting the perfect resume? Thats a different ball game altogether.Im sure a lot of you would agree. Every recruiter and hiring manager has their own criteria for judging candidates. So designing a crisp and concise resume is the first thing you should consider. A few tips to do this:Once your resume is ready, expand your job search. Remember that job portals are not the only way to apply for data science roles!In fact, they are the least effective manner of searching for jobs. There are more than 9 different ways to apply for jobs and job portals happen to be just one of them.We discuss each of these 9 ways in the Ace Data Science Interviews course. This includes tips and tricks to optimize your applications in order to land that dream interview!This one is easy  themost common mistake I see people making at this stage is they go about doing passive job searching. They do all the hard work, spend days and weeks learning the right things, and then flush it away by being unaware of how to apply.Most of us typically come back from office, and then log into a few popular job portals and apply there. Its pretty easy, after all. Your resume is already uploaded  all you do is find a relevant job and click a button. This strategy no longer works. The recruiter will have hundreds of applications on the portal, all looking almost the same.Impossible to distinguish between that. It becomes a game of chance. Even if this strategy works,it will be months before you get any thing meaningful from thischannel.We are in the digital era, folks! Get creative! You spent all that time building your network in step #2, put that to good use. Most data science interviews you land will be through non-job portal channels.If you have reached this stage  congratulate yourself! It is time for your first real interaction with the recruiter.Depending on the company, there could be a call only with a recruiter, or only with the hiring manager (or both). But the fundamentals remain the same. If the hiring manager is taking the call  you should expect a few technical questions as part of the process.Since this is a telephonic or a virtual call setup  you can keep your answers to most of the common questions ready. I wouldnt recommend reading them straight from the screen, though. You should be prepared with the broad pointers and take it from there.Take this round as seriously as all the other steps. A casual vibe is enough to throw the recruiter off. Additionally, always minimize distractions in the room where youll take the call. You can also take notes throughout the call for reference later. I have givena comprehensive run down of the dos and donts to ace the phone interviews in the course.Too many people fail to ask questions at this stage. This is a great opportunity to understand more about the role, the company, and your fit within that ecosystem. Dont just stick with the old what are the work timings questions! Showcase your curiosity and passion for the role.You should be prepared to ask questions which you can not find in open public information. These are the small things that add up big time in the overall scheme of things. The hiring manager will appreciate your interest. Thats how rapport building is done.If the telephonic round went well, theres a good chance you might be asked to do an assignment. Now, not every company has this round. It varies from role to role and project to project. But its best to be prepared, right?You can expect to face one of the below types of assignments:Typically, these assignments act as filters and would usually be basic in nature to ensure you have the skills you claim on your CV. The submission could be in the form of a presentation, Jupyter notebook or a submission on a self-evaluation platform.Aspiring data science professionals tend to only meet the bare expectations in the assignment. Asked to build a model? Sure, heres a report about the results. The thought of going beyond that, perhaps exploring the data and discerning patterns, doesnt occur to most candidates.Whatever the assignment  you should go the extra mile to solve the problem. Dont stop by just doing what is expected. This is your chance to impress the hiring manager even before the face-to-face interview starts.These assignments will map closely with the role youve applied for. A wonderful opportunity to explore what youll be potentially getting into. And if your telephonic round didnt go very well, this offers you a chance to get back on track.Youve researched the role, crafted a crisp and exquisite resume, successfully applied and cleared the telephonic and assignment round. You know whats coming next  its finally time for the face-to-face interviews!You can safely expect to face multiple rounds and formats of interactions. Youll meet with plenty of people throughout these rounds. In-person interviews can take anywhere between half a day and an entire day to finish. The different people you might interact with:A hands-on hiring manager could also ask you to sit with the team through the day and brainstorm on a problem. The assignment round we saw earlier could also factor into this stage.Youll be judged on your structured thinking, analytical and logical reasoning, puzzle solving skills, programming knowledge, machine learning techniques, among other things.The use of whiteboards has become quite common now in data science interviews. Writing down SQL queries, or explaining your thought process  be ready for all these aspects.Check out this comprehensive interview guide to prepare for variety of interview rounds and questionsOne pet peeve of mine is when the candidate doesnt ask meaningful questions. I know we covered this in the telephonic interview but its even more essential in a face-to-face setting. Asking questions is a sign of curiosity and interest in the role. If you have already asked the questions about the role in a previous round  you can always ask the interviewer about their journey in the organisation.The in-person rounds can be quite grueling. There are just so many of them! Candidates at times dont expect this and start losing their focus halfway through the day. Mental preparation is as important as the rest. Patience is a virtue, and a data science professional is expected to posses bucket loads of it.Finally, once you have gone undergone the in-person interviews, you should follow-up with a thank you note. Also, make sure youre honoring any commitment you might have made, such as sharing a past presentation or a piece of code). Youve done the hard work, its now time to wrap up things and bring that dream role home!Regardless of how you felt the interview went, you should not be unprofessional at this stage. Yes, even if you dont hear back from the interviewer after you sent a thank you note.This can come across as desperate. The data science community is still a small world and your reputation will precede you. Dont do anything to harm your chances of getting another interview opportunity in the future.And that wraps up the 7 step framework for data science interviews! Here are a few resources you should go through to boost your chances of acing the next data science interview youll face:And as promised, here is the infographic we have created on this 7-step framework. Download it, save it, and use it as a checklist for your next data science interview preparation!",https://www.analyticsvidhya.com/blog/2019/02/7-step-process-ace-data-science-interviews/
Its a Record-Breaking Crowd! A Must-Read Tutorial to Build your First Crowd Counting Model using Deep Learning,Learn everything about Analytics|Introduction|Table of Contents|What is Crowd Counting?|Why is Crowd Counting useful?|Understanding the Different Computer Vision Techniques for Crowd Counting|Understanding the Architecture and Training Method of CSRNet|Building your own Crowd Counting model|End Notes,"Act like a Crowd Counting Scientist|1. Detection-based methods|2. Regression-based methods|3. Density estimation-based methods|4. CNN-based methods|Underlying Mathematics (Recommended, but optional)|Share this:|Related Articles|Dont Miss this Comprehensive 7 Step Process to Ace Data Science Interviews!|An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library|
Pulkit Sharma
|75 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Artificial Intelligence and Machine Learning is going to be our biggest helper in coming decade!Today morning, I was reading an article which reported that an AI system won against 20 lawyers and the lawyers were actually happy that AI can take care of repetitive part of their roles and help them work on complex topics. These lawyers were happy that AI will enable them to have more fulfilling roles.Today, I will be sharing a similar example  How to count number of people in crowd using Deep Learning and Computer Vision? But, before we do that  let us develop a sense of how easy the life is for a Crowd Counting Scientist.Lets start!Can you help me count / estimate number of people in this picture attending this event?Ok  how about this one?Source: ShanghaiTech DatasetYou get the hang of it. By end of this tutorial, we will create an algorithm for Crowd Counting with an amazing accuracy (compared to humans like you and me). Will you use such an assistant?P.S. This article assumes that you have a basic knowledge of how convolutional neural networks (CNNs) work. You can refer to the below post to learn about this topic before you proceed further:This article is highly inspired by the paper  CSRNet : Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes.Crowd Counting is a technique to count or estimate the number of people in an image. Take a moment to analyze the below image:Source: ShanghaiTech DatasetCan you give me an approximate number of how many people are in the frame? Yes, including the ones present way in the background.The most direct method is to manually count each person but does that make practical sense?Its nearly impossible when the crowd is this big!Crowd scientists (yes, thats a real job title!) count the number of people in certain parts of an image and then extrapolate to come up with an estimate. More commonly, we have had to rely on crude metrics to estimate this number for decades.Surely there must be a better, more exact approach?Yes, there is!While we dont yet have algorithms that can give us the EXACT number, most computer vision techniques can produce impressively precise estimates. Lets first understand why crowd counting is important before diving into the algorithm behind it.Lets understand the usefulness of crowd counting using an example. Picture this  your company just finished hosting a huge data science conference. Plenty of different sessions took place during the event.You are asked to analyze and estimate the number of people who attended each session. This will help your team understand what kind of sessions attracted the biggest crowds (and which ones failed in that regard). This will shape next years conference, so its an important task!There were hundreds of people at the event  counting them manually will take days! Thats where your data scientist skills kick in. You managed to get photos of the crowd from each session and build a computer vision model to do the rest!There are plenty of other scenarios where crowd counting algorithms are changing the way industries work:Can you come up with some other use cases? Let me know in the comments section below! We can connect and try to figure out how we can use crowd counting techniques in your scenario.Broadly speaking, there are currently four methods we can use for counting the number of people in a crowd:Here, we use a moving window-like detector to identify people in an image and count how many there are. The methods used for detection require well trained classifiers that can extract low-level features. Although these methods work well for detecting faces, they do not perform well on crowded images as most of the target objects are not clearly visible.We were unable to extract low level features using the above approach. Regression-based methods come up trumps here. We first crop patches from the image and then, for each patch, extract the low level features.We first create a density map for the objects. Then, the algorithm learn a linear mapping between the extracted features and their object density maps. We can also use random forest regression to learn non-linear mapping.Ah, good old reliable convolutional neural networks (CNNs). Instead of looking at the patches of an image, we build an end-to-end regression method using CNNs. This takes the entire image as input and directly generates the crowd count. CNNs work really well with regression or classification tasks, and they have also proved their worth in generating density maps.CSRNet, a technique we will implement in this article, deploys a deeper CNN for capturing high-level features and generating high-quality density maps without expanding the network complexity. Lets understand what CSRNet is before jumping to the coding section.CSRNet uses VGG-16 as the front end because of its strong transfer learning ability. The output size from VGG is th of the original input size. CSRNet also uses dilated convolutional layers in the back end.But what in the world are dilated convolutions? Its a fair question to ask. Consider the below image:The basic concept of using dilated convolutions is to enlarge the kernel without increasing the parameters. So, if the dilation rate is 1, we take the kernel and convolve it on the entire image. Whereas, if we increase the dilation rate to 2, the kernel extends as shown in the above image (follow the labels below each image). It can be an alternative to pooling layers.Im going to take a moment to explain how the mathematics work. Note that this isnt mandatory to implement the algorithm in Python, but I highly recommend learning the underlying idea. This will come in handy when you need to tweak or modify your model.Suppose we have an input x(m,n), a filter w(i,j), and the dilation rate r. The output y(m,n) will be:We can generalize this equation using a (k*k) kernel with a dilation rate r. The kernel enlarges to:([k + (k-1)*(r-1)] * [k + (k-1)*(r-1)])So the ground truth has been generated for each image. Each persons head in a given image is blurred using a Gaussian kernel. All the images are cropped into 9 patches, and the size of each patch is th of the original size of the image. With me so far?The first 4 patches are divided into 4 quarters and the other 5 patches are randomly cropped. Finally, the mirror of each patch is taken to double the training set.That, in a nutshell, are the architecture details behind CSRNet. Next, well look at its training details, including the evaluation metric used.Stochastic Gradient Descent is used to train the CSRNet as an end-to-end structure.During training, the fixed learning rate is set to 1e-6. The loss function is taken to be the Euclidean distance in order to measure the difference between the ground truth and estimated density map. This is represented as:where N is the size of the training batch. The evaluation metric used in CSRNet is MAE and MSE, i.e., Mean Absolute Error and Mean Square Error. These are given by:Here, Ci is the estimated count:L and W are the width of the predicted density map.Our model will first predict the density map for a given image. The pixel value will be 0 if no person is present. A certain pre-defined value will be assigned if that pixel corresponds to a person. So, calculating the total pixel values corresponding to a person will give us the count of people in that image. Awesome, right?And now, ladies and gentlemen, its time to finally build our own crowd counting model!Ready with your notebook powered up?We will implement CSRNet on the ShanghaiTech dataset. This contains 1198 annotated images of a combined total of 330,165 people. You can download the dataset fromhere.Use the below code block to clone the CSRNet-pytorch repository. This holds the entire code for creating the dataset, training the model and validating the results:Please install CUDA and PyTorch before you proceed further. These are the backbone behind the code well be using below.Now, move the dataset into the repository you cloned above and unzip it. Well then need to create the ground truth values. Themake_dataset.ipynb file is our savior. We just need to make minor changes in that notebook:Now, lets generate the ground truth values for images in part_A and part_B:Generating the density map for each image is a time taking step. So go brew a cup of coffee while the code runs.So far, we have generated the ground truth values for images in part_A. We will do the same for the part_B images. But before that, lets see a sample image and plot its ground truth heatmap:Things are getting interesting!Lets count how many people are present in this image:270.32568Similarly, we will generate values for part_B:Now, we have the images as well as their corresponding ground truth values. Time to train our model!We will use the .json files available in the cloned directory. We just have to change the location of the images in the json files. To do this, open the .json file and replace the current location with the location where your images are located.Note that all this code is written in Python 2. Make the following changes if youre using any other Python version:Made the changes? Now, open a new terminal window and type the following commands:Again, sit back because this will take some time. You can reduce the number of epochs in the train.py file to accelerate the process. A cool alternate option is to download the pre-trained weightsfrom here if you dont feel like waiting.Finally, lets check our models performance on unseen data. We will use the val.ipynb file to validate the results. Remember to change the path to the pretrained weights and images.Check the MAE (Mean Absolute Error) on test images to evaluate our model:We got an MAE value of 75.69 which is pretty good. Now lets check the predictions on a single image:Wow, the original count was 382 and our model estimated there were 384 people in the image. That is a very impressive performance!Congratulations on building your own crowd counting model!I encourage you to try out this approach on different images and share your results in the comments section below. Crowd counting has so many diverse applications and is already seeing adoption by organizations and government bodies.It is a useful skill to add to your portfolio. Quite a number of industries will be looking for data scientists who can work with crowd counting algorithms. Learn it, experiment with it, and give yourself the gift of deep learning!Did you find this article useful?Feel free to leave your suggestions and feedback for me below, and Ill be happy to connect with you.You should also check out the below resources to learn and explore the wonderful world of computer vision:",https://www.analyticsvidhya.com/blog/2019/02/building-crowd-counting-model-python/
An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library,Learn everything about Analytics|Introduction|Table of Contents|What is an Outlier?|Why do we need to Detect Outliers?|Why should we use PyOD for Outlier Detection?|Features of PyOD|Installing PyOD in Python|Outlier Detection Algorithms used in PyOD|Extra Utilities provided by PyOD|Implementation of PyOD|End Notes,"Angle-Based Outlier Detection (ABOD)|k-Nearest Neighbors Detector|Isolation Forest|Histogram-based Outlier Detection |Local Correlation Integral (LOCI)|Feature Bagging|Clustering Based Local Outlier Factor|PyOD on a Simulated Dataset|PyOD on the Big Mart Sales Problem|Share this:|Like this:|Related Articles|Its a Record-Breaking Crowd! A Must-Read Tutorial to Build your First Crowd Counting Model using Deep Learning|Introduction to Flair for NLP: A Simple yet Powerful State-of-the-Art NLP Library|
LAKSHAY ARORA
|11 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"My latest data science project involved predicting the sales of each product in a particular store. There were several ways I could approach the problem. But no matter which model I used, my accuracy score would not improve.I figured out the problem after spending some time inspecting the data  outliers!This is a commonly overlooked mistake we tend to make. The temptation is to start building models on the data youve been given. But thats essentially setting yourself up for failure.There are no shortcuts to data exploration. Building models will only get you so far if youve skipped this stage of your data science project. After a point of time, youll hit the accuracy ceiling  the models performance just wont budge.Data exploration consists of many things, such as variable identification, treating missing values, feature engineering, etc. Detecting and treating outliers is also a major cog in the data exploration stage. The quality of your inputs decide the quality of your output!PyOD is one such library to detect outliers in your data. It provides access to more than 20 different algorithms to detect outliers and is compatible with both Python 2 and 3. An absolute gem!In this article, I will take you on a journey to understand outliers and how you can detect them using PyOD in Python.This article assumes you have a basic knowledge of machine learning algorithms and the Python language. You can refer to this article -Essentials of Machine Learning, to understand or refresh these concepts.An outlier is any data point which differs greatly from the rest of the observations in a dataset. Lets see some real life examples to understand outlier detection:There are a plethora of reasons why outliers exist. Perhaps an analyst made an error in the data entry, or the machine threw up an error in measurement, or the outlier could even be intentional! Some people do not want to disclose their information and hence input false information in forms.Outliers are of two types: Univariate and Multivariate. A univariate outlier is a data point that consists of extreme values in one variable only, whereas a multivariate outlier is a combined unusual score on at least two variables. Suppose you have three different variables  X, Y, Z. If you plot a graph of these in a 3-D space, they should form a sort of cloud. All the data points that lie outside this cloud will be the multivariate outliers.I would highly recommend you to read this amazing guide on data exploration which covers outliers in detail.Outliers can impact the results of our analysis and statistical modeling in a drastic way. Check out the below image to visualize what happens to a model when outliers are present versus when they have been dealt with:But heres the caveat  outliers arent always a bad thing. Its very important to understand this. Simply removing outliers from your data without considering how theyll impact the results is a recipe for disaster.Our tendency is to use straightforward methods like boxplots, histograms and scatter-plots to detect outliers. But dedicated outlier detection algorithms are extremely valuable in fields which process large amounts of data and require a means to perform pattern recognition in larger datasets.Applications like fraud detection in finance and intrusion detection in network security require intensive and accurate techniques to detect outliers. Can you imagine how embarrassing it would be if you detected an outlier and it turned out to be genuine?The PyOD library can step in to bridge this gap. Lets see what its all about.Numerous outlier detection packages exist in various programming languages.I particularly found these languages helpful in R. But when I switched to Python, there was a glaring lack of an outlier detection library. How was this even possible?!Existing implementations like PyNomalyare not specifically designed for outlier detection (though its still worth checking out!). To fill this gap,Yue Zhao, Zain Nasrullah, and Zheng Lidesigned and implemented the PyOD library.PyOD is a scalable Python toolkit for detecting outliers in multivariate data. It provides access to around 20 outlier detection algorithms under a single well-documented API.PyOD has several advantages and comes with quite a few useful features. Heres my pick of the bunch:Time to power up our Python notebooks! Lets first install PyOD on our machines:As simple as that!Note that PyOD also contains some neural network based models which are implemented in Keras. PyOD willNOT install Keras or TensorFlow automatically. You will need to install Keras and other libraries manually if you want to use neural net based models.Lets see the outlier detection algorithms that power PyOD. Its well and good implementing PyOD but I feel its equally important to understand how it works underneath. This will give you more flexibility when youre using it on a dataset.Note: We will be using a term Outlying scorein this section. It means every model, in some way, scores a data point than uses threshold value to determine whether the point is an outlier or not.Enough talk  lets see some action. In this section, well implement the PyOD library in Python. Im going to use two different approaches to demonstrate PyOD:First, lets import the required libraries:Now, well import the models we want to use to detect the outliers in our dataset. We will be using ABOD (Angle Based Outlier Detector) and KNN (K Nearest Neighbors):Now, we will create a random dataset with outliers and plot it. Create a dictionary and add all the models that you want to use to detect the outliers:Fit the data to each model we have added in the dictionary, Then, see how each model is detecting outliers:Looking good!Now, lets see how PyOD does on the  famous Big Mart Sales Problem.Go ahead and download the dataset from the above link.Lets start with importing the required libraries and loading the data:Lets plot Item MRP vs Item Outlet Sales to understand the data:The range of Item Outlet Sales is from 0 to 12000 and Item MRP is from 0 to 250. We will scale down both these features to a range between 0 and 1. This is required to create a explainable visualization (it will become way too stretched otherwise). As for this data, using the same approach will take much more time to create the visualization.Note: If you dont want the visualization, you can use the same scale to predict whether a point is an outlier or not.Store these values in the NumPyarray for using in our models later:Again, we will create a dictionary. But this time, we will add some more models to it and see how each model predicts outliers.You can set the value of the outlier fraction according to your problem and your understanding of the data. In our example, I want to detect 5% observations that are not similar to the rest of the data. So, Im going to set the value of outlier fraction as 0.05.Now, we will fit the data to each model one by one and see how differently each model predicts the outliers.OUTPUTIn the above plots, the white points are inliers surrounded by red lines, and the black points are outliers in the blue zone.That was an incredible learning experience for me as well. I spent a lot of time researching PyOD and implementing it in Python. I would encourage you to do the same. Practice using it on different datasets  its such a useful library!PyOD already supports around 20 classical outlier detection algorithms which can be used in both academic and commercial projects. Its contributors are planning to enhance the toolbox by implementing models that will work well with time series and geospatial data.If you have any suggestions/feedback related to the article, please post them in the comments section below. I look forward to hearing your experience using PyOD as well. Happy learning.Check out the below awesome courses to learn data science and its various aspects:",https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/
Introduction to Flair for NLP: A Simple yet Powerful State-of-the-Art NLP Library,Learn everything about Analytics|Introduction|Table of contents|What is Flair Library?|What Gives Flair the Edge?|Introduction to Contextual String Embeddings for Sequence Labeling|Performing NLP Tasks in Python using Flair|End Notes,"Setting up the Environment|About the Dataset|1. Text Classification Using Flair Embeddings|2. Part of Speech (POS) Tagging with Flair|Share this:|Like this:|Related Articles|An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library|DataHack Radio #17: Reinforcement Learning with Professor Balaraman Ravindran|
Sharoon Saxena
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",There are two primary factors powering contextual string embeddings:|Why use Colab?|  Step 1: Import the data into the local Environment of Colab:|Step 2: Installing Flair|Step 3: Preparing text to work with Flair|Step 4: Word Embeddings with Flair|Step 5: Vectorizing the text||Step 6: Partitioning the data for Train and Test Sets|Step 6: Building the Model and Defining Custom Evaluator (for F1 Score)|Step 7: Time for predictions!|Step 1: Importing the dataset|Step 2 : Extracting Sentences and PoS Tags from the dataset|Step 3: Tagging the text using NLTK and Flair|Step 4: Evaluating the PoS tags from NLTK and Flair against the tagged dataset,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Last couple of years have been incredible for Natural Language Processing (NLP)as a domain! We have seen multiple breakthroughs  ULMFiT, ELMo, Facebooks PyText, Googles BERT, among many others. These have rapidly accelerated the state-of-the-art research in NLP (and language modeling, in particular).We can now predict the next sentence, given a sequence of preceding words.Whats even more important is that machines are now beginning to understand the key element that had eluded them for long.Context!Understanding context has broken down barriers that had prevented NLP techniques making headway before. And today, we are going to talk about one such library  Flair.Until now, the words were either represented as a sparse matrix or as word embeddings such as GLoVe, Bert and ELMo, and the results have been pretty impressive. But, theres alwaysroom for improvement and Flair is willing to stand up to it.In this article, we will first understand what Flair is and the concept behind it. Then well dive into implementing NLP tasks using Flair. Get ready to be impressed by its accuracy!Please note that this article assumes familiarity with NLP concepts. You can go through the below articles if you need a quick refresher:Flair is a simple natural language processing (NLP) library developed and open-sourced by Zalando Research. Flairs framework builds directly on PyTorch, one of the best deep learning frameworks out there. The Zalando Research team has also released several pre-trained models for the following NLP tasks:All of this looks promising. But what truly caught my attention was when I saw Flair outperforming several state-of-the-art results in NLP. Check out this table:Note: F1 score is an evaluation metric primarily used for classification tasks. Its often used in machine learning projects over the accuracy metric when evaluating models. The F1 score takes into consideration the distribution of the classes present.There are plenty of awesome features packaged into the Flair library. Heres my pick of the most prominent ones:Context is so vital when working on NLP tasks. Learning to predict the next character based on previous characters forms the basis of sequence modeling.Contextual String Embeddings leverage the internal states of a trained character language model to produce a novel type of word embedding.In simple terms, it uses certain internal principles of a trained character model, such that words can have different meaning in different sentences.Note:A language and character model is a probability distribution of Words / Characters such that every new word or character depends on the words or characters that came before it. Have a lookhereto know more about it.Lets look at an example to understand this:Explanation:Language is such a wonderful yet complex thing. You can read more about Contextual String Embeddings in thisResearch Paper.Its time to put Flair to the test! Weve seen what this awesome library is all about. Now lets see firsthand how it works on our machines.Well use Flair to perform all the below NLP tasks in Python:We will be using Google Colaboratory for running our code. One of the best things about Colab is that it provides GPU support for free! It is pretty handy for training deep learning models.All you need is a stable internet connection.Well be working on the Twitter Sentiment Analysis practice problem. Go ahead and download the dataset from there (youll need to register/log in first).The problem statement posed by this challenge is:The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.Overview of steps:Step 1: Import the data into the local Environment of Colab:Step 2: Installing FlairStep 3: Preparing text to work with FlairStep 4: Word Embeddings with FlairStep 5: Vectorizing the textStep 6: Partitioning the data for Train and Test SetsStep 7: Time for predictions!You can find the file ID in the shareable link of the dataset file in the drive.Importing the dataset into the Colab notebook:All the emoticons and symbols have been removed from the data and the characters have been converted to lowercase. Additionally, our dataset has already been divided into train and test sets.You can download this clean dataset fromhere.A Brief look at Flair Data TypesThere are two types of objects central to this library  Sentence and Token objects. A Sentence holds a textual sentence and is essentially a list of Tokens:Feel free to first go through this article if youre new to word embeddings: An Intuitive Understanding of Word Embeddings.You would have noticed we just used some of the most popular word embeddings above. Awesome! You can remove the comments # to use all the embeddings.Now you might be asking  What in the world are Stacked Embeddings? Here, we can combine multiple embeddings to build a powerful word representation model without much complexity. Quite like ensembling, isnt it?We are using the stacked embedding of Flair only for reducing the computational time in this article.Feel free to play around with this and other embeddings by using any combination you like.Testing the stacked embeddings:Well be showcasing this using two approaches.Mean of Word Embeddings within a TweetWe will be calculating the following in this approach:For each sentence:Document Embedding: Vectorizing the entire TweetYou can choose either approach for your model.Now that our text is vectorised, we can feed it to our machine learning model!Defining custom F1 evaluator for XGBoostBuilding the XGBoost modelOur model has been trained and is ready for evaluation!Note: The parameters were taken from this Notebook.I uploaded the predictions to the practice problem page with 0.2 as probability threshold:Note: According to Flairs official documentation, stacking of the flair embedding with other embeddings often yields even better results, But, there is a catch..It might take a VERY LONG time to compute on a CPU. I highly recommend leveraging a GPU for faster results. You can use the free one within Colab!We will be using a subset of the Conll-2003 dataset, is a pre-tagged dataset in English. Download the dataset fromhere.Overview of steps:Step 1: Importing the datasetStep 2 : Extracting Sentences and PoS Tags from the datasetStep 3: Tagging the text using NLTK and FlairStep 4: Evaluating the PoS tags from NLTK and Flair against the tagged datasetThe data file contains one word per line, with empty lines representing sentence boundaries. We have extracted the essentials aspects we require from the dataset. Lets move on to step 3.First, import the required libraries:This will download all the necessary files to tag the text using NLTK.The PoS tags are in this format:[(token_1, tag_1), .. , (token_n, tag_n)]Lets extract PoS from this:The NLTK tags are ready for business.Importing the libraries first:Tagging using FlairThe result is in the below format:token_1 <tag_1> token_2 <tag_2> .. token_n <tag_n>Note: We can use different taggers available within the Flair library. Feel free to tinker around and experiment. You can find the list here. Extract the sentence-wise tags as we did in NLTKAha! We have finally tagged the corpus and extracted them sentence-wise. We are free to remove all the punctuation and special symbols.We have tagged the corpus using NLTK and Flair, extracted and removed all the unnecessary elements.Lets see it for ourselves:OUTPUT:corpus SOCCER JAPAN GET LUCKY WIN CHINA IN SURPRISE DEFEAT 
actual NN NNP VB NNP NNP NNP IN DT NN 
nltk NNP NNP NNP NNP NNP NNP NNP NNP NNP 
flair NNP NNP VBP JJ NN NNP IN NNP NNP 

corpus Nadim Ladki
actual NNP NNP
nltk NNP NNP
flair NNP NNP

corpus AL AIN United Arab Emirates 
actual NNP NNP NNP NNPS CD
nltk NNP NNP NNP VBZ JJ
flair NNP NNP NNP NNP CDThat looks convincing!Here, we are doing word-wise evaluation of the tags with the help of a custom-made evaluator.corpus Japan coach Shu Kamo said The Syrian own goal proved lucky for us 
actual  NNP NN NNP NNP VBD POS DT JJ JJ NN VBD JJ IN PRP 
nltk    NNP VBP NNP NNP VBD DT JJ JJ NN VBD JJ IN PRP 
flair    NNP NN NNP NNP VBD DT JJ JJ NN VBD JJ IN PRP Note that in the example above, the actual POS tags contain redundancy compared to NLTK and flair tags as shown(in bold). Therefore we will not be considering the POS tagged sentences where the sentences are of unequal length.Finally we evaluate the POS tags of NLTK and Flair against the POS tags provided by the dataset.Our Result:NLTK Score:85.38654023442645Flair Score:90.96172124773179Well, well, well. I can see why Flair has been getting so much attention in the NLP community.Flair clearly provides an edge in word embeddings and stacked word embeddings. These can be implemented without much hassle due to its high level API. The Flair embedding is something to keep an eye on in the near future.I love that the Flair library supports multiple languages. The developers are additionally currently working on Frame Detection using flair. The future looks really bright for this library.I personally enjoyed working and learning the ins and outs of this library. I hope you found the tutorial useful and will be using Flair to your advantage next time you take up an NLP challenge.",https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/
DataHack Radio #17: Reinforcement Learning with Professor Balaraman Ravindran,Learn everything about Analytics|Introduction|Professor Balaraman Ravindrans Background|Ph.D in Reinforcement Learning and Working with Andrew Barto|Research at IIT-Madras|Professor Ravis Work in the Industry|Robert Bosch Centre for Data Science and Artificial Intelligence|End Notes,"Professor Ravi was kind enough to share his Doctoral dissertation presentation on this topic which you can download here. It is a MUST-READ for anyone pursuing this field.|Share this:|Related Articles|Introduction to Flair for NLP: A Simple yet Powerful State-of-the-Art NLP Library|Top 5 Data Science GitHub Repositories and Reddit Discussions (January 2019)|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Reinforcement learning is an intriguing but complex topic to get your head around. Its also one of the most promising skills a data scientist can add to their portfolio. Reinforcement Learning has sprung up some of the biggest ground-breaking developments in the last few years, including powering Google DeepMinds popular AlphaGo program.Who better to demystify the aura around this vast field than Indias foremost researcher on Reinforcement Learning? Yes, were talking about none other than the eminent Professor Balaraman Ravindran!Professor B. Ravindran has an incredibly rich background in academic research, headlined by his work in reinforcement learning. His Google Scholar profile shows his research papers have been cited over 2,200+ times!He has a penchant of explaining the most complex topics in words even beginners are able to grasp. Its just one of the many reasons his talk at DataHack Summit 2018 was a super hit among our community.In this podcast, Kunal speaks to Professor Ravi about his background, his interest and research in reinforcement learning, and the intricacies and nuances of this field.All DataHack Radio episodes are available on the below podcast platforms. Subscribe today!Neural networks were all the rage back in the 1990s. Given Professor Ravis passion for computational models, it was inevitable he would start delving into this subject.As he explored state-of-the-art neural networks during his Masters (from IISc), he realized that the approaches were moving away from explaining how humans learn. That led to his foray into the world of reinforcement learning (and we are all really grateful for that!).He started reading research papers on neuroscience as his fascination with reinforcement learning continued to grow. So what was the state of RL back then? This quote from Professor Ravi sums it up perfectly:I was the only person in my Masters class who was working on Reinforcement Learning.One of the biggest drawbacks in India was that there wasnt a single resource available to learn reinforcement learning. So Professor Ravi along with other researchers wrote a survey paper and emailed it out. This survey picked up pace in the community and even caught the eye of the Oxford press. They asked Professor Ravi and team to write a chapter on RL for their handbook on neural computation (published in 1996).This survey served as Professor Ravis entree into his Ph.D, which he successfully completed from the University of Massachusetts. His Ph.D advisor? None other than the great Andrew Barto!One of the questions Professor Ravi and Andrew Barto wrestled with concerned the human psyche. How are humans so good at learning one problem and transferring it to another task so quickly? The duo attempted to solve this question for tasks that were similar in nature.If youre wondering what similar here means, you arent the only one! Professor Ravis research came to the point where he needed to formally define this word in the context of his research. Thats how they came up with the concept of abstract algebra and the mathematics ofhomomorphisms. A lot of transfer learning frameworks have actually been built onhomomorphisms.Here is his formal definition of similarity:Two things are similar if, for everything I can do for situation A, there is a decision in situation B that has similar effects.Later, while working with one his students, Professor Ravi discovered that this notion of similarity in RL was exactly what computer scientists call graphs. A fascinating insight into how the approach works!The mathematical notions of similarity that we define are all on an abstract model of the world.Professor Ravi joined IIT-Madras faculty following his Ph.D. There he explored multiple areas of research since reinforcement learning had still not picked up steam. He explored domains like Natural Language Understanding and learning on graphs.Circling back to RL, Professor Ravi has continued to work on homomorphisms at IIT-Madras. Another stream he has been working on is learning complex policies. The question he has been exploring deals with how to design agents that can mimic human thinking (the same question he was pursuing with Andrew Barto).Hierarchical reinforcement learning frameworks have been another area of interest and research (including a ton of work on attention modeling). Complementing that is deep reinforcement learning (on ATARI games) which really took off in 2014. This gave Professor Ravi more complex domains to work on. He has explained the Deep RL concept using a superb analogy. This section is a MUST-LISTEN for everyone in data science. You will be able to understand the explanation even if youre a relative beginner in this field.This section is a nice microcosm of the magic behind Professor B. Ravindrans teaching methods.Another direction of research Professor Ravi has been looking at is going beyond rewards. We usually work with rewards when experimenting with a RL approach, right? But there are plenty of other signals apart from this in real-world scenarios. Its critical to understand this if we are to integrate reinforcement learning in a human-centric world.What makes Professor B. Ravindrans experience unique is that his work isnt limited to academia and research. He has worked on multiple industry projects, including a few on:What about RL specific projects? Those do come along, but not as often as one might think. He has worked on RL projects where the optimization component is rolled into the problem (where theres no pre-defined model).Professor Ravi is also the head of theRobert Bosch Centre for Data Science and Artificial Intelligence at IIT-Madras. This was founded in 2017with a vision to become an internationally renowned centre for data science research, where long-standing fundamental research problems, cutting across disciplines, are targeted and solved.I highly recommend following their GitHub page to check out their latest work.I had the pleasure of meeting Professor Ravi at DataHack Summit 2018. He is a very down-to-earth person with an incredible enthusiasm for this field. He has an infectious joy about him and hearing him talk about RL feels like a dream come true.All these qualities come out in this episode as well. What an awesome episode with one of the top people in our community. Happy listening and do share your feedback with us below.",https://www.analyticsvidhya.com/blog/2019/02/datahack-radio-reinforcement-learning-professor-balaraman-ravindran/
Top 5 Data Science GitHub Repositories and Reddit Discussions (January 2019),Learn everything about Analytics|Introduction|GitHub Repositories|Flair (State-of-the-Art NLP Library)|face.evoLVe  High Performance Face Recognition Library|YOLOv3|FaceBoxes: A CPU Real-Time Face Detector with High Accuracy|Transformer-XL from Google AI|Reddit Discussions|Data Scientist is the new Business Analyst|What is Something in Data Science that Blew your Mind?|The Things Top Data Scientists Struggled with Early in their Career|Why do Deep Neural Networks Generalize Well?|AMA with DeepMinds AlphaStar Team!|End Notes,"There were a few other awesome data science repositories created in January. Make sure you check them out:|Share this:|Like this:|Related Articles|DataHack Radio #17: Reinforcement Learning with Professor Balaraman Ravindran|MyStory: How I Successfully Switched from Application Development to Data Science|
Sharoon Saxena
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Theres nothing quite like GitHub and Reddit for data science. Both platforms have been of immense help to me in my data science journey.GitHub is the ultimate one-stop platform for hosting your code. It excels at easing the collaboration process between team members.Most leading data scientists and organizations use GitHub to open-source their libraries and frameworks. So not only do we stay up-to-date with the latest developments in our field, we get to replicate their models on our own machines!Reddit discussions are on the same end of that spectrum. Leading researchers and brilliant minds come together to discuss and extrapolate the latest topics and breakthroughs in machine learning and data science. There is A LOT to learn from these two platforms.I have made it a habit to check both these platforms at least twice a week. Its changed the way I learn data science. I encourage everyone reading this to do the same!In this article, well focus on the latest open-source GitHub libraries and Reddit discussions from January 2019. Happy learning!You can also browse through the 25 best GitHub repositories from 2018. The list contains libraries covering multiple and diverse domains, including NLP, Computer Vision, GANs, AutoML, among others.2018 was a watershed year for Natural Language Processing (NLP). Libraries like ELMo and Googles BERT were ground-breaking releases. As Sebastian Ruder said, NLPs ImageNet moment has arrived!Lets keep that trend going into the new year! Flair is another superb NLP library thats easy to understand and implement. And the best part? Its very much state-of-the-art!Flair was developed and open-sourced by Zalando Research and is based on PyTorch.The library has outperformed previous approaches on a wide range of NLP tasks:Here, F1 is the accuracy evaluation metric. I am currently exploring this library and plan to pen down my thoughts in an article soon. Keep watching this space!Face recognition algorithms for computer vision are ubiquitous in data science now. We covered a few libraries in last years GitHub series as well. Add this one to the growing list of face recognition libraries you must try out.face.evoLVe is a High Performance Face Recognition Library based on PyTorch. Itprovides comprehensive functions for face related analytics and applications, including:This library is a must-have for the practical use and deployment of high performance deep face recognition, especially for researchers and engineers.YOLO is a supremely fast and accurate framework for performing object detection tasks. It was launched three years back and has seen a few iterations since, each better than the last.This repository is a complete pipeline of YOLOv3 implemented in TensorFlow. Thiscan be used on a dataset to train and evaluate your own object detection model. Below are the key highlights of this repository:If youre new to YOLO and are looking to understand how it works, I highly recommend checking out this essential tutorial.One of the biggest challenges in computer vision is managing computational resources. Not everyone has multiple GPUs lying around. Its been quite a hurdle to overcome.Step up FaceBoxes. Its a novel face detecting approach thats shown impressive performance on both speed and accuracy using CPUs.This repository in a PyTorch implementation of FaceBoxes. It contains the code to install, train and evaluate a face detection model. No more complaining about a lack of computation power  give FaceBoxes a try today!Heres another game-changing NLP framework. Its no surprise to see the Google AI team behind it (theyre the ones who came up with BERT as well).Long range dependencies have been a thorn in the side of NLP. Even with the significant progress made last year, this concept wasnt quite dealt with. RNN and Vanilla transformers were used but they were not quite good enough. THat gap has now been filled by Google AIs Transformer-XL. A few key points to note about this library:This repository contains the code for Transformer-XL in both TensorFlow and PyTorch. See if you can match (or even beat) the state-of-the-art results in NLP!Dont be fooled by the hot-take in the headline. This is a serious discussion about the current state of data science and how its taught around the world.Its always been difficult to pin down specific labels on different data science roles. The functions and tasks vary  so who should learn exactly what? This thread looks at how educational institutes are only covering the basic concepts and claiming to teach data science.For all of you who are in the early stage of learning  make sure you browse through this discussion. Youll learn a lot about how recruiters perceive potential candidates holding a certification or degree from an institute claiming they are data scientists.Youll of course learn a bit about what a business analyst does as well, and how thats different to the data scientist role.What is that one thing about data science that made you go WOW. For me, it was when I realized how I could use data science as a game-changer in the sports industry.There are a lot of uncanny theories and facts in this discussion thread that will keep you engaged. Here are a couple of cool answers taken from the thread:How much of the world can be modeled with well known distributions. The fact that so many things are normally distributed makes me think we are in a simulation.The first thing that ever blew my mind and wanted me to pursue a career in data science was United Airlines saving 170,000 of fuel each year by changing the type of paper used to make their in flight magazine.Most data scientists will vouch that they had a difficult time understanding certain concepts during their initial days. Even something as straightforward as imputing missing values can become an arduous exercise in frustration.This thread is a goldmine for all you data science enthusiasts. It comprises of experienced data scientists sharing their experience on how they managed to learn or get past concepts they initially found hard to grasp. Some of these might even seem familiar to you:Neural networks have long had a black box reputation (its not really true anymore). Things get even more muddy when the concept expands to deep neural networks (DNNs). These DNNs are at the heart of plenty of recent state-of-the-art results so its essential to understand how they work.A key question discussed in this thread is on how deep neural networks generalize so well. If you were of the same thought that theres no answer to that  prepare to have your mind blown!This thread comprises of views and perspective of put forth by deep learning experts. Theres a lot of links and resources included to dive deeper into the topic as well. But do note that a basic understanding of neural networks will help you get more involved in the discussion.You can learn more about Neural Networks here.Googles DeepMind stunned the world when their AlphaGo creation beat Go champion Lee Sedol. Theyve gone and done it again!Their latest algorithm, AlphaStar, was trained on the popular StarCraft 2 game. AlphaStar emphatically swatted aside the top two StarCraft players, winning by an impressive 10-1 margin.This Reddit discussion thread was a AMA (Ask Me Anything) hosted by two DeepMind AlphaStars creators. They discussed a wide-variety of topics with the Reddit community, explaining how the algorithm works, how much training data was used, what the hardware setup was like, etc.A couple of interesting questions covered in the discussion:How many games needed to be played out in order to get to the current level? Or in other words: how many games is 200 years of learning in your case?What other approaches were tried? I know people were quite curious about whether any tree searches, deep environment models, or hierarchical RL techniques would be involved, and it appears none of them were; did any of them make respectable progress if tried?What a way to start 2019! Progress in NLP is happening at a breakneck pace. Do watch out for my article on Flair soon. Of course, DeepMinds AlphaStar has also been a huge breakthrough in reinforcement learning. Lets hope this can be modelled n a real-world scenario soon.What are your thoughts on this? Which library did you find the most useful? Let me know your feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2019/02/top-5-data-science-github-reddit-january-2019/
MyStory: How I Successfully Switched from Application Development to Data Science,Learn everything about Analytics|Introduction|Table of Contents|Background: Change was Necessary|Career Move: Work on the Technology you Love|What I learned During my Transition into Data Science?|The Different Activities and Resources that Helped my Transition|Challenges Faced During this Transition|PRAXIS: The beginning of new challenges and opportunities|Campus Life: A Bag Full of Memories|The First Analytics Job Offer: Outcome of the Intense Routine|The next BIG move: Learn and grow|End Notes,"About the Author|Share this:|Like this:|Related Articles|Top 5 Data Science GitHub Repositories and Reddit Discussions (January 2019)|Introduction to StanfordNLP: An Incredible State-of-the-Art NLP Library for 53 Languages (with Python code)|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Making a transition into data science is a journey paved with obstacles and learning. There is so much to learn and implement! This can get especially challenging if youre coming from a non-technical background.But isnt that the great thing about learning? We get to experiment with concepts, apply them in a safe academic environment, and add to our knowledge through practical applications. The experience becomes even richer when youve worked in the corporate field for a number of years  everything seems novel and new again!I recently made a career transition into data science. This article is my experience with this switch, my learning, and my advice to all aspiring future data science professionals looking for their first big break.Like the majority of students graduating with a B.Tech degree in Information Technology, I was placed in one of the top Indian multinational consulting companies in 2012. The future held so much promise for me!I worked as an Application Developer in Java for a BFSI client for 4 years. However, my learning graph slowed down over this period. I was restless even though I was in a comfort zone. The role was great but I was looking for professional growth.So I decided it was finally time for a change.I began preparing for GMAT to pursue a Masters degree abroad. I cleared TOEFL and GMAT but unfortunately had to drop my plan due to some financial obligations. So, what next?The best way of penetrating into a new field is first understanding the current technologies.The buzzwords back in 2016 were, as you might have guessed, Data Science and Machine Learning.I had vaguely heard about these terms through online articles. I started exploring career options in this field and found that Statistics was the base of Data Science. This meshed perfectly with my interests  Statistics has always fascinated me. There is nothing better than working in a field that you love!A quick Google search on Analytics Machine Learning Tutorials led me to Indias largest data science community, Analytics Vidhya. I went through their articles on educational institutions providing courses for careers in Data Science. A deep dive into this domain made me believe that Praxis Business School was the correct option for me.This was an institution providing a full time analytics course with an industry-ready course structure, a set of renowned professors, comparatively low and manageable tuition fee, and of course, the successful placement records that Praxis had shown since its inception.The last move was to have a quick conversation with Praxis alumni to understand out their experience. This convinced me to apply to the Praxis Business School Post Graduate Program in Business Analytics and fortunately I got selected!I have spent most of my professional career in programming before switching to Data Science. I studied statistics as a kid but those concepts were long forgotten. Making this transition was indeed tough, but not impossible.The key is to never stop learning. During this switch, I realized that you dont need to unlearn your existing skills to pick up a new one. I used my programming skills as a bridge between IT and Data Science to structure my machine learning code more logically.This transition also helped me understand that the presentation of project results varies significantly from industry to industry. For example, in the IT industry, the output of a web development project is a web page that is completely understandable by the stakeholders. In the world of data science, the output is (usually) numbers. It is the role of a data science professional to reveal these numbers to the customers/stakeholders using an indicative story.For those who are ready to start your transition into Data Science, I recommend reading the below suggestions carefully:Despite the industry focused and well-structured curriculum that Praxis provides in its full-time program, its very essential to know where you stand in the world of Data Science. To get into the ecosystem, I participated in as many hackathons as possible, especially those organized by Analytics Vidhya and Kaggle.These hackathons give a sense of real-life data science problems and also provide a leaderboard to compare yourself against the top data scientists. Even now, when I am into this field professionally, I try my best to attend the great meetups organized by Analytics Vidhya, Pycon India, etc. to name a few. These meetups are the best source of knowing whats happening in Data Science around the world and meeting some great minds.Also, I always read the blog section of AV  it comprises of the latest developments in Data Science, and also explains ML algorithms in simple and easy to understand language. Moreover, I have some great LinkedIn connections who share wonderful articles related to AI and ML regularly. I also used to refer to the UCI Machine Learning Repository which is another great source for datasets.The resources are unlimited but you have to know how to search and find them as self-learning is the best thing that one can gift oneself.The biggest challenge is to stitch not one but many skills together to be an effective data scientist  skills like statistics, machine learning, databases, visualization techniques, programming and of course the art of storytelling. Also, there is always a new tool or package or an entire algorithm coming up every now and then.Its a tough job to cope with the speed with which this field is progressing. Therefore, in addition to the classroom studies, I used to devote an extra hour daily to read articles and blogs published in AV, Quora and LinkedIn so that I stay updated with the latest technologies. Also, I have reached out to the faculty team at Praxis in case of doubts with any concerned subjects and every time they proved to be very helpful.Coming back to student life after spending 4 years in the corporate sector was quite a challenge. From the first day at Praxis, it was a known fact that the next one year was not going to be easy. A variety of nearly 25 different subjects distributed over 3 trimesters were covered during a span of nine months.Most subjects were totally new to me. But I was hooked due to my previous programming experience and love for statistics. There were subjects in different domains, like:We were exposed to different tools and technologies like R, Python, SAS, Tableau, Hadoop, Spark, etc. The curriculum was very well designed with concepts and real-life application-based case studies going hand-in-hand.My days at Praxis were full of assignments  tons of quizzes, semester exams, team presentations and, of course, the main group capstone project. The environment at Praxis and the quality and attitude of the faculty was superb. This new world of new subjects and exams turned out to be very interesting and the challenges worth taking.Additionally, the class was a mix of highly talented professionals from diverse industries (there were a few freshers as well). This encouraged healthy discussions, brainstorming, and learning from each other. In addition to academics, there were various cultural and sports events that lent a good balance between studying and campus life.The placement season in Praxis starts as early as November every year. But the preparation for it starts from Day 1 of joining Praxis. I was fortunate enough to get a job offer from the National Payments Corporation Of India (NCPI) in the role of a Data Scientist. This happened through the campus placement program. Three other colleagues were placed in NCPI, while others were placed in organizations across different verticals and function areas. In NPCI, my team was building a Fraud Risk Management Model to predict and prevent different kinds of fraud transactions  ATM, UPI, POS and E-commerce. We worked on different technologies, such as Python, R, PySpark, Julia, Tableau and Hive. It was my first experience working with petabytes of data. The NPCI journey, though short, was interesting, challenging and a great learning experience of nearly one and a half years. The classroom training received at Praxis and the rigor that we were made to go through in that one year turned out to be very helpful in not just getting the job but in performing with distinction.It is exciting setting targets and moving forward to achieve them. After a successful professional career in NPCI, I got the opportunity to move to one of the BIG 5: AMAZON.I joined Amazon as a Data Subject Matter Expert (Data SME) in the Alexa Data Services branch of Amazon Analytics. This was indeed a dream come true and I personally believe that my academic and industry background, my decision to leave my job and enroll into a full-time program in analytics at Praxis, and my stint with NCPI were all contributors to this achievement.My current role involves working with Alexa Machine Learning Scientists to enhance the Alexa experience. I am quite new to this venture and am committed to making this a successful one.It is my utmost pleasure to share my story on the portal which has an equal contribution in building my Data Science career as far as self-study through blogs and competitive hackathons are concerned.As artificial intelligence and machine learning are grabbing the world with their presence, we also need to learn new things, take risks, work a little harder and start looking for opportunities.If you are serious about a career in Data Science, take time off and register for a full-time program,  and use that time to dive deep into the domain. It is quite complex and will take a lot of time and work  but the results are worth the effort.Data Science, AI and ML  these are great opportunities  if you like technology and numbers, go for it  and as they say at Praxis  CELEBRATE YOUR WORTH.Ankita Ghoshal  Data SME at Amazon Alexa Data ServicesI have more than 6 years of work experience in the area of Web Application Development and Machine Learning, including Fraud Risk Management, Anti-Money Laundering, E-commerce and other domains.I graduated from ITER, Bhubaneswar in IT and later pursued PGP in Data Science from Praxis Business School, Kolkata. I started my professional career with TCS and switched to NPCI through Praxis Campus placement. After working with NPCI for 1.5 years, I transitioned to Amazon India where I am currently working. I enjoy going through AV articles/blogs on new technologies and ML algorithms and like spending my leisure time painting and cooking.This is a sponsored post and the opinions expressed in this article are exclusively of the author. A few minor edits have been made by Analytics Vidhya.",https://www.analyticsvidhya.com/blog/2019/02/mystory-how-learned-data-science-successfully-switched-careers/
Introduction to StanfordNLP: An Incredible State-of-the-Art NLP Library for 53 Languages (with Python code),Learn everything about Analytics|Introduction|Table of Contents|What is StanfordNLP and Why Should You Use it?|Setting up StanfordNLP in Python|Using StanfordNLP to Perform Basic NLP Tasks|Implementing StanfordNLP on the Hindi Language|Using CoreNLPs API for Text Analytics|My Thoughts on using StanfordNLP  Pros and Cons|End Notes,"A couple of important notes|Tokenization|Lemmatization|Parts of Speech (PoS) Tagging|Dependency Extraction|Processing text in Hindi (Devanagari Script)|a. Setting up the CoreNLPClient|b. Dependency Parsing and POS|c. Named Entity Recognition and Co-Reference Chains|Share this:|Like this:|Related Articles|MyStory: How I Successfully Switched from Application Development to Data Science|A Must-Read NLP Tutorial on Neural Machine Translation  The Technique Powering Google Translate|
Mohd Sanad Zaki Rizvi
|2 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A common challenge I came across while learning Natural Language Processing (NLP)  can we build models for non-English languages? The answer has been no for quite a long time.Each language has its own grammatical patterns and linguistic nuances. And there just arent many datasets available in other languages.Thats where Stanfords latest NLP library steps in  StanfordNLP.I could barely contain my excitement when I read the news last week. The authors claimed StanfordNLP could support more than 53 human languages! Yes, I had to double-check that number.I decided to check it out myself. Theres no official tutorial for the library yet so I got the chance to experiment and play around with it. And I found that it opens up a world of endless possibilities. StanfordNLP contains pre-trained models for rare Asian languages like Hindi, Chinese and Japanese in their original scripts.The ability to work with multiple languages is a wonder all NLP enthusiasts crave for. In this article, we will walk through what StanfordNLP is, why its so important, and then fire up Python to see it live in action. Well also take up a case study in Hindi to showcase how StanfordNLP works  you dont want to miss that!Here is StanfordNLPs description by the authors themselves:StanfordNLP is the combination of the software package used by the Stanford team in the CoNLL 2018 Shared Task on Universal Dependency Parsing, and the groups official Python interface to the Stanford CoreNLP software.Thats too much information in one go! Lets break it down:StanfordNLP is a collection of pre-trained state-of-the-art models. These models were used by the researchers in the CoNLL 2017 and 2018 competitions. All the models are built on PyTorch and can be trained and evaluated on your own annotated data. Awesome!Additionally, StanfordNLP also contains an official wrapper to the popular behemoth NLP library  CoreNLP. This had been somewhat limited to the Java ecosystem until now. You should check out this tutorial to learn more about CoreNLP and how it works in Python.Below are a few more reasons why you should check out this library:What more could an NLP enthusiast ask for? Now that we have a handle on what this library does, lets take it for a spin in Python!There are some peculiar things about the library that had me puzzled initially. For instance, you need Python 3.6.8/3.7.2 or later to use StanfordNLP. To be safe, I set up a separate environment in Anaconda for Python 3.7.1. Heres how you can do it:1. Open conda prompt and type this:2. Now activate the environment:3. Install the StanfordNLP library:4. We need to download a languages specific model to work with it. Launch a python shell and import StanfordNLP:then download the language model for English (en):This can take a while depending on your internet connection. These language models are pretty huge (the English one is 1.96GB).which should give an output like torch==1.0.0Thats all! Lets dive into some basic NLP processing right away.StanfordNLP comes with built-in processors to perform five basic NLP tasks:Lets start by creating a text pipeline:The processors =  argument is used to specify the task. All five processors are taken by default if no argument is passed. Here is a quick overview of the processors and what they can do:Lets see each of them in action.This process happens implicitly once the Token processor is run. It is actually pretty quick. You can have a look at tokens by using print_tokens():The token object contains the index of the token in the sentence and a list of word objects (in case of a multi-word token). Each word object contains useful information, like the index of the word, the lemma of the text, the pos (parts of speech) tag and the feat (morphological features) tag.This involves using the lemma property of the words generated by the lemma processor. Heres the code to get the lemma of all the words:This returns a pandas data frame for each word and its respective lemma:The PoS tagger is quite fast and works really well across languages. Just like lemmas, PoS tags are also easy to extract:Notice the big dictionary in the above code? It is just a mapping between PoS tags and their meaning. This helps in getting a better understanding of our documents syntactic structure.The output would be a data frame with three columns  word, pos and exp (explanation). The explanation column gives us the most information about the text (and is hence quite useful).Adding the explanation column makes it much easier to evaluate how accurate our processor is. I like the fact that the tagger is on point for the majority of the words. It even picks up the tense of a word and whether it is in base or plural form.Dependency extraction is another out-of-the-box feature of StanfordNLP. You can simply call print_dependencies() on a sentence to get the dependency relations for all of its words:The library computes all of the above during a single run of the pipeline. This will hardly take you a few minutes on a GPU enabled machine.We have now figured out a way to perform basic text processing with StanfordNLP. Its time to take advantage of the fact that we can do the same for 51 other languages!StanfordNLP really stands out in its performance and multilingual text parsing support. Lets dive deeper into the latter aspect.First, we have to download the Hindi language model (comparatively smaller!):Now, take a piece of text in Hindi as our text document:This should be enough to generate all the tags. Lets check the tags for Hindi:The PoS tagger works surprisingly well on the Hindi text as well. Look at  for example. The PoS tagger tags it as a pronoun  I, he, she  which is accurate.CoreNLP is a time tested, industry grade NLP tool-kit that is known for its performance and accuracy. StanfordNLP has been declared as an official python interface to CoreNLP. That is a HUGE win for this library.There have been efforts before to create Python wrapper packages for CoreNLP but nothing beats an official implementation from the authors themselves. This means that the library will see regular updates and improvements.StanfordNLP takes three lines of code to start utilizing CoreNLPs sophisticated API. Literally, just three lines of code to set it up!1. Download the CoreNLP package. Open your Linux terminal and type the following command:2. Unzip the downloaded package:3. Start the CoreNLP server:Note: CoreNLP requires Java8 to run. Please make sure you have JDK and JRE 1.8.x installed.pNow, make sure that StanfordNLP knows where CoreNLP is present. For that, you have to export $CORENLP_HOME as the location of your folder. In my case, this folder was in the home itself so my path would be likeAfter the above steps have been taken, you can start up the server and make requests in Python code. Below is a comprehensive example of starting a server, making requests, and accessing data from the returned object.The above examples barely scratch the surface of what CoreNLP can do and yet it is very interesting, we were able to accomplish from basic NLP tasks like Parts of Speech tagging to things like Named Entity Recognition, Co-Reference Chain extraction and finding who wrote what in a sentence in just few lines of Python code.What I like the most here is the ease of use and increased accessibility this brings when it comes to using CoreNLP in python.Exploring a newly launched library was certainly a challenge. Theres barely any documentation on StanfordNLP! Yet, it was quite an enjoyable learning experience.A few things that excite me regarding the future of StanfordNLP:There are, however, a few chinks to iron out. Below are my thoughts on where StanfordNLP could improve:Make sure you check out StanfordNLPs official documentation.There is still a feature I havent tried out yet. StanfordNLP allows you to train models on your own annotated data using embeddings from Word2Vec/FastText. Id like to explore it in the future and see how effective that functionality is. I will update the article whenever the library matures a bit.Clearly, StanfordNLP is very much in the beta stage. It will only get better from here so this is a really good time to start using it  get a head start over everyone else.For now, the fact that such amazing toolkits (CoreNLP) are coming to the Python ecosystem and research giants like Stanford are making an effort to open source their software, I am optimistic about the future.",https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/
A Must-Read NLP Tutorial on Neural Machine Translation  The Technique Powering Google Translate,Learn everything about Analytics|Introduction|Table of Contents|Machine Translation  A Brief History|Understanding the Problem Statement|Introduction to Sequence-to-Sequence (Seq2Seq) Modeling|Implementation in Python using Keras|End Notes,"Import the Required Libraries|Read the Data into our IDE|Text Pre-Processing|Model Building|Share this:|Like this:|Related Articles|Introduction to StanfordNLP: An Incredible State-of-the-Art NLP Library for 53 Languages (with Python code)|Busted! 11 Myths Data Science Transitioners Need to Avoid|
Prateek Joshi
|15 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If you talk to a man in a language he understands, that goes to his head. If you talk to him in his own language, that goes to his heart.  Nelson MandelaThe beauty of language transcends boundaries and cultures. Learning a language other than our mother tongue is a huge advantage. But the path to bilingualism, or multilingualism, can often be a long, never-ending one.There are so many little nuances that we get lost in the sea of words. Things have, however, become so much easier with online translation services (Im looking at you Google Translate!).I have always wanted to learn a language other than English. I tried my hand at learning German (or Deutsch), back in 2014. It was both fun and challenging. I had to eventually quit but I harboured a desire to start again.Fast-forward to 2019, I am fortunate to be able to build a language translator for any possible pair of languages. What a boon Natural Language Processing has been!In this article, we will walk through the steps of building a German-to-English language translation model using Keras. Well also take a quick look at the history of machine translation systems with the benefit of hindsight.This article assumes familiarity with RNN, LSTM, and Keras. Below are a couple of articles to read more about them:Most of us were introduced to machine translation when Google came up with the service. But the concept has been around since the middle of last century.Research work in Machine Translation (MT) started as early as 1950s, primarily in the United States. These early systems relied on huge bilingual dictionaries, hand-coded rules, and universal principles underlying natural language.In 1954, IBM held a first ever public demonstration of a machine translation. The system had a pretty small vocabulary of only 250 words and it could translate only 49 hand-picked Russian sentences to English. The number seems minuscule now but the system is widely regarded as an important milestone in the progress of machine translation.This image has been taken from the research paper describing IBMs systemSoon, two schools of thought emerged:In 1964, the Automatic Language Processing Advisory Committee (ALPAC) was established by the United States government to evaluate the progress in Machine Translation. ALPAC did a little prodding around and published a report in November 1966 on the state of MT. Below are the key highlights from that report:Not exactly a glowing recommendation!A long dry period followed this miserable report. Finally, in 1981, a new system called the METEO System was deployed in Canada for translation of weather forecasts issued in French into English. It was quite a successful project which stayed in operation until 2001.The worlds first web translation tool, Babel Fish, was launched by the AltaVista search engine in 1997.And then came the breakthrough we are all familiar with now  Google Translate. It has since changed the way we work (and even learn) with different languages.Source: translate.google.comLets circle back to where we left off in the introduction section, i.e., learning German. However, this time around I am going to make my machine do this task. The objective is to convert a German sentence to its English counterpart using a Neural Machine Translation (NMT) system.We will use German-English sentence pairs data from http://www.manythings.org/anki/. You can download it fromhere.Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, DNA sequence modeling, among others. Our aim is to translate given sentences from one language to another.Here, both the input and output are sentences. In other words, these sentences are a sequence of words going in and out of a model. This is the basic idea of Sequence-to-Sequence modeling. The figure below tries to explain this method.A typical seq2seq model has 2 major components  a) an encoder
b) a decoderBoth these parts are essentially two different recurrent neural network (RNN) models combined into one giant network:Ive listed a few significant use cases of Sequence-to-Sequence modeling below (apart from Machine Translation, of course):Its time to get our hands dirty! There is no better feeling than learning a topic by seeing the results first-hand. Well fire up our favorite Python environment (Jupyter Notebook for me) and get straight down to business.Our data is a text file (.txt) of English-German sentence pairs. First, we will read the file using the function defined below.Lets define another function to split the text into English-German pairs separated by \n. Well then split these pairs into English sentences and German sentences respectively.We can now use these functions to read the text into an array in our desired format.The actual data contains over 150,000 sentence-pairs. However, we will use only the first 50,000 sentence pairs to reduce the training time of the model. You can change this number as per your systems computation power (or if youre feeling lucky!).Quite an important step in any project, especially so in NLP. The data we work with is more often than not unstructured so there are certain things we need to take care of before jumping to the model building part.(a) Text CleaningLets first take a look at our data. This will help us decide which pre-processing steps to adopt.We will get rid of the punctuation marks and then convert all the text to lower case.(b) Text to Sequence ConversionA Seq2Seq model requires that we convert both the input and the output sentences into integer sequences of fixed length.But before we do that, lets visualise the length of the sentences. We will capture the lengths of all the sentences in two separate lists for English and German, respectively.Quite intuitive  the maximum length of the German sentences is 11 and that of the English phrases is 8.Next, vectorize our text data by using Kerass Tokenizer() class. It will turn our sentences into sequences of integers. We can then pad those sequences with zeros to make all the sequences of the same length.Note that we will prepare tokenizers for both the German and English sentences:The below code block contains a function to prepare the sequences. It will also perform sequence padding to a maximum sentence length as mentioned above.We will now split the data into train and test set for model training and evaluation, respectively.Its time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. This has to be done for both the train and test datasets.Now comes the exciting part!Well start off by defining our Seq2Seq model architecture:Model ArchitectureWe are using the RMSprop optimizer in this model as its usually a good choice when working with recurrent neural networks.Please note that we have used sparse_categorical_crossentropy as the loss function. This is because the function allows us to use the target sequence as is, instead of the one-hot encoded format. One-hot encoding the target sequences using such a huge vocabulary might consume our systems entire memory.We are all set to start training our model!We will train it for 30 epochs and with a batch size of 512 with a validation split of 20%. 80% of the data will be used for training the model and the rest for evaluating it. You may change and play around with these hyperparameters.We will also use theModelCheckpoint()function to save the model with the lowest validation loss. I personally prefer this method over early stopping.Lets compare the training loss and the validation loss.As you can see in the above plot, the validation loss stopped decreasing after 20 epochs.Finally, we can load the saved model and make predictions on the unseen data  testX.These predictions are sequences of integers. We need to convert these integers to their corresponding words. Lets define a function to do this:Convert predictions into text (English):Lets put the original English sentences in the test dataset and the predicted sentences in a dataframe:We can randomly print some actual vs predicted instances to see how our model performs:Our Seq2Seq model does a decent job. But there are several instances where it misses out on understanding the key words. For example, it translates im tired of boston to im am boston.These are the challenges you will face on a regular basis in NLP. But these arent immovable obstacles. We can mitigate such challenges by using more training data and building a better (or more complex) model.You can access the full code from this Github repo.Even with a very simple Seq2Seq model, the results are pretty encouraging. We can improve on this performance easily by using a more sophisticated encoder-decoder model on a larger dataset.Another experiment I can think of is trying out the seq2seq approach on a dataset containing longer sentences. The more you experiment, the more youll learn about this vast and complex space.If you have any feedback on this article or have any doubts/questions, kindly share them in the comments section below.",https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/
Busted! 11 Myths Data Science Transitioners Need to Avoid,"Learn everything about Analytics|Introduction|Career-Related Myths|1. Ph.D is Mandatory to Become a Data Scientist|2. A Full-Time Data Science Degree is a Must for Making the Transition|3. All your previous Work Experience will Translate to the Data Science Domain|4. Necessary to have a Computer Science/Mathematics/Statistics/Programming Background|Tools and Frameworks-Related Myths|5. Learning a Tool is Enough to Become a Data Scientist|6. Deep Learning Requires Computational Power that Only Top Companies Have|7. Once Built, AI Systems will Continue to Evolve and Generalize by Themselves|Data Science Role-Related Myths|8. Universe of AI Jobs|9. Data Science is Only About Building Predictive Models|10. Participating in Data Science Competitions Translates to Real-Life Projects|11. Data Collection is a Breeze, the Focus should be on Building Models|End Notes","Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Breaking down the myth|Share this:|Like this:|Related Articles|A Must-Read NLP Tutorial on Neural Machine Translation  The Technique Powering Google Translate|Build a Recurrent Neural Network from Scratch in Python  An Essential Read for Data Scientists|
Pranav Dar
|12 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Changing your Domain Entirely|Staying in the same Domain|Technical qualities|Soft Skills,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Transitions into data science are tough, even scary! And it is not because you need to learn maths, stats and coding! You need to do that, but you also need to battle out the myths you hear from people around you and find your own path through them!I had my own share of these popular myths and they made my life difficult. Here are the few I had heard:You need a Ph.D to have a chance of becoming a data scientist. Two is even better!Participate in data science competitions, that will tell you how the industry works.You need tons of computational resources to build deep learning models. You can only get that at the top tech firms.I had promised to myself that once I have seen them through  Ill help others by debunking these myths.There are multiple myths floating around that add a false aura around data science roles.Dont fall for them!These myths often make you feel like only geniuses can work in data science. This is just not true. Whether youre a recent graduate, an experienced professional, or a leader, its important to understand how data science works and you will find your place in the industry.This article is my revenge on those myths!I have also done some clustering  and presented the myths in three types  Career related myths, Tools and Framework related myths and Data Science role related myths. Read about them and make sure you dont fall for them!You can also check out my article on the 13 common mistakes amateur data scientists make. Its full of resources so you dont want to miss that!Holding a Ph.D. degree is an amazing achievement. It requires years of hard work and dedication. I have utmost respect for people who are willing to put in that amount of effort.But is it compulsory to do a Ph.D in order to become a data scientist? This is a heavily role dependent question. There are several layers to peel off here so lets get down to it.To understand this, lets broadly divide the role of a data scientist into two categories:Its important to understand the distinction between these two roles. The Applied Data Science is primarily about working with existing algorithms and understanding how they work. In other words, its all about applying these techniques in your project. You DO NOT need a Ph.D for this role.Most folks fit into the above category. Most of the openings and job descriptions you see or hear about are for these roles.But what if you are more interested in a research role? Then yes, you might need a Ph.D. Creating new algorithms from scratch, researching them, writing scientific papers, etc.  these fit a Ph.D candidates mindset. It also helps if the Ph.D adds to the domain you want to work in. For example, a Ph.D in linguistics will be immensely helpful for a career in NLP.Another misunderstood aspect of a Ph.D is the opportunity cost. Its a massive commitment from your side  both mentally and financially. Rachel Thomas wrote about this question here and I suggest taking a look. Its a well-balanced point of view from a leading researcher in this field.As Rachel mentions in her post, there are tons of data science pioneers who dont hold a Ph.D:So which role do you see yourself in? Thats a critical question to answer before you jump into data science.Much like the Ph.D dilemma, this is another myth Ive seen aspiring data science professionals fall for. With the amount of interest data science has generated in the last few years, how would you differentiate yourself from the competition? Splashing out money to get a degree seems like a good starting point. Its an understandable reaction.Heres the good news  this is a myth perpetuated predominantly by institutes.In a vast and complex field like data science, practical experience is king. There are numerous projects you can pick up and work on right now. Or find a problem you are passionate about solving and see if data science techniques can be applied there.There are plenty of resources available online for learning. Books, MOOCs, blogs, videos, and so on. Start from Analytics Vidhyas learning path. Its completely free, comprehensive in nature (contains all the aforementioned resources), and provides a structure around your learning  an invaluable feature.Because of the lack of formal education in this field, transitioning into data science boils down to sheer hard work, discipline and practical experience. Those are the differentiating factors a recruiting manager will look at.Greg Brockman, the co-founder and CTO of OpenAI, doesnt even hold a college degree!You have a solid 5-10 years of experience in *some* industry. You are a well-respected professional whos calling the cards. But youve recently become enamoured with data science and all it can do for your business and career. You cant wait to bring all that experience to your new field.Sounds familiar? Good. But if you are of the thought that your entire experience will translate to your new role, I suggest you re-think.There are two sides to this story:Lets understand the implications of each of these points.If you are changing your domain entirely (for example, coming into data science after years of software testing), your work experience will most likely count for nothing. Not only are you switching to an entirely new line of work, you are also looking for a new role. When the recruiter looks at your resume, the first thought is  what value will he/she add to the organization/project?. Unfortunately, the answer in this case is usually close to zero.Why? Because as a newcomer, you dont have any experience of how that domain works. When youre given real-world data, how will you work with it if you dont understand how those features impact the final decision?This is a reality most people skim over or prefer not to face. Its an entirely wrong way to go about your career switch and will only end up harming your prospects. Understand the situation, talk to people who have made this switch, and align your expectations accordingly. Going in blind into such a big decision is a one-way ticket to failure.Coming to scenario #2  what should you expect if youre staying in the same domain but switching to data science? Then your prospects look a lot sunnier. You have the added advantage of knowing the industry. You should already be aware of the nuances present in the domain so you will understand the data youre working with. That is a MASSIVE benefit. The recruiting manager will factor that into the final decision.I highly recommend the second scenario if at all possible. Stay in the same field you have always worked with and understand how you can apply data science there.This article by Kunal Jain contains plenty of practical tips and tricks to help you overcome a lack of experience in this field.This is essentially a continuation of the first two myths we covered. Most of the folks youll come across in data science will have an engineering/computer science background. Theyll have experience in at least have one, if not more, of the below fields:Does this mean someone coming from a completely different background cant get into data science?Absolutely not! Trust me, I speak from experience. I spent 5 years working in a learning and development role before transitioning into data science. It can be done.However, there are certain things youll need to consider that people coming from this background already have. Data science is a nuanced field comprising of several aspects. As a beginner, you will be required to learn concepts from scratch. It will often be a frustrating experience. Your technical colleagues might know more. They might be ahead of you initially at every turn.And thats where the dedication and discipline characteristics we spoke about earlier come into play.For example, folks with a computer science background will already have a handle on how programming works. They can switch between languages relatively easily. I, on the other hand, struggled initially with learning R. I didnt know left from right when it came to coding. But I persisted and eventually broke through.There is nothing to suggest you cant do that too. Once you have decided this is the field for you, you should pull out all the stops. Still sceptical? Then check out these inspiring stories from folks with a non-technical background who successfully made the transition:Python or R  which tool should you learn? If I got a penny each time I came across this question..There is a widely held belief that mastering data science is about learning how to apply techniques in Python or R. Or any other tool. That tool has become the central point around which all other data science functions revolve.The assumption (or myth) is that being able to write code using existing libraries (numpy, scikit-learn, caret, etc.) should be enough to label yourself an expert. Why arent the job offers rolling in yet?That one really drives recruiting managers right up the wall.Data science requires a combination of multiple skills. Programming is not at the centre of the data science spectrum  it is just one part of a whole. Lets divide the spectrum of skills into two parts:Understanding how a certain technique works will help you become a better data scientist. This is why we encourage everyone to learn algorithms from scratch. Learn how changing a certain parameter will impact the final model. This will eventually pay off when youre working on a large-scale project in the industry.The margin for error and experimentation is slim where stakeholders come into the picture. We have plenty of articles on our blog explaining machine learning and deep learning techniques from the ground up. Go through them and try to understand and replicate the code yourself.It will be an invaluable addition to your skillset.Soft skills often get overlooked by aspiring data scientists. They certainly arent taught in any online courses or offline classrooms. And yet these are qualities interviewers look for.How can you pick up these skills?! By following a disciplined approach and practising every chance you get. Here are a few resources for your perusal:Deep learning seems to propagate more myths than any other branch of data science I have come across, including:But the most common myth Ive heard  you need a considerable amount of hardware to perform deep learning tasks. When I first heard about deep learning, I pictured a room full of IBM supercomputers being operated by dozens of data scientists.Dont get me wrong, a deep learning model will always perform more efficiently when it has a powerful hardware setup to run on. But you dont need a supercomputer to work with deep learning. It just might take longer than expected to train the model on your machine.What if the data youre working with is huge? Running it locally might not work. Google, as always, has the answer to that. Google Colab is a free cloud service that doubles as a coding notebook. But heres the best part  Colab comes with FREE GPU support!Thats right, you can leverage the power of a GPU for free and run your deep learning model there. Colab runs through your web browser, so theres no computational cost to your machine. What else could a deep learning enthusiast ask for?Here are a couple of resources to acquaint you with aspects of deep learning most people wont teach:Hollywood has done its best to showcase AI systems in the form of robots who possess human-level intelligence. Movies like Blade Runner and Terminator became cult classics for this very reason. The consensus they portray is that once an AI system has been built, it will continue to work and evolve by itself.So once you build a model for say, fraud detection, the model will adapt to any changes thrown at it. If the entire financial landscape changed, or new features were added to the data, its expected that the system will continue to function equally well.This state, that AI systems evolve by themselves, is called Artificial General Intelligence (AGI). Unfortunately, we are not at that state yet. Not even close.We are very much in the narrow stage right now. The models we build cannot generalize to other tasks, or even to big changes in the data. Lets take the example of last years GDPR policy. Would an existing system adapt to the changes without manual human intervention? Are the systems we build smart enough to incorporate the ethics aspect? Can an AI system, built to recommend products to the customer, integrate a new product without any prior information about it?Thats why labelling images in an object detection problem is such a crucial task. That level of intelligence isnt available to machines yet.Yes, DeepMind and other similar high-level research organizations have definitely made progress in this regard. You might have come across the news about neural networks creating their own neural networks. But these developments are way too few and far between. And we havent figured out how to get them into commercial applications.I strongly recommend watching the below video by Michael B. Jordan regarding the state of AI currently and the challenges we face. Its a great reference point if you can bring it up in an interview situation. What comes to your mind when you think about the perfect artificial intelligence team? The most common answer I have heard is to get as many top data scientists as possible. Who wouldnt like a bunch of top talent working on the same project?But that just isnt a practical solution for most of us. One unicorn data scientist is difficult to source, let alone tons of them. And will your team of data scientists be aware of the non-AI parts of a project? Do they understand how hardware works?Simply put  an artificial intelligence project has a universe of jobs attached to it. It isnt only limited to the role of a data scientist.Applied artificial intelligence is a complex field. It requires working with different disciplines across the length and breadth of the project. A plethora of interdisciplinary roles exist:Note that the roles and number of folks staffed will vary depending on the project. The idea Im trying to get across is that AI isnt a cut and dry field. Its not a straightforward path. If someone tries to sell you on a project that is staffed with just data scientists, it might be time to sound the alarm bells.This is especially relevant for people operating in a senior role (team leaders, managers, CxOs, etc.). Its VERY important to understand each role in order to create a successful project.I recommend taking the below course to fully grasp how an AI project works. This includes how to hire the perfect AI team, and other intricate details every AI leader (or even enthusiast) should know:You can also read about the jobs that might be impacted as AI continues to grow.Being able to predict an event is a powerful thing. And thats what stands out to newcomers in data science. Building models that can predict what a customer will buy next sounds like a must-have skill, right?In fact, when I describe data science or machine learning to a non-technical person, their first reaction is quite similar. The hype around this field is unprecedented. Apparently, a data scientist is only building predictive models all day at work.Is that not what DJ Patil meant when he described the role of a data scientist as the sexiest job of the 21st century? Well, not quite.There are multiple layers in a data science project. The model building part is just a speck in the overall data science lifecycle (Ill cover the different roles in data science in the next section). To give you a general idea, the steps involved in a typical data science lifecycle are:Nothing is as straightforward as they teach you in a classroom or a course. Experience is the best way to learn how a project works. Try talking to someone who has seen the end-to-end process. Even better, get an internship and get a first-hand account of what makes a data science project tick.Additionally, data science isnt limited to simply making predictions. Im sure youve come across the market-basket analysis concept. Its a combination of clustering techniques and association rules. Or how about anomaly detection? The ability to figure out outliers in the data. There is so much to learn!Data science competitions are an excellent stepping stone in your data science journey. You get to practice your skills on a dataset, showcase it to the world, and even stand a chance to win prizes.These hackathons and competitions have increased multi-fold in the last 4-5 years as more and more people want a piece of the data science cake. Most aspiring data science professionals include these competitions on their resume.The problem from an interview standpoint? Recruiters have started paying less and less attention to this aspect of your portfolio.There are plenty of reasons why recruiters dont consider your competition experience. Ill laser that down to one:Real-world projects are an entirely different beast compared to what you see in competitions.Data science competitions have clean and almost spotless datasets. If there are missing values, you can impute them using a plethora of techniques. What matters is the accuracy of your model, not the way you got there.Real-world projects have end-to-end pipelines which involve working with a bunch of people. Most of us will always have to work with messy and untidy data.The old saying about spending 70-80% of your time just collecting and cleaning data is true. Tasks like data cleaning and feature engineering will take up the majority of your time.This LinkedIn post is an excellent read on the standard methodology one can use for analytical models. You can also refer to the section above where we spoke about the different stages involved in a typical data science project.                                                                     Source: Revolution AnalyticsAdditionally, we cant just build a stacked complex ensemble model. Clients demand transparency so the simpler model usually wins out. Interpretability is key in a corporate setting. The project is accountable for the model behaving poorly.As I mentioned in this article, getting a good score on a competition leaderboard is excellent for measuring your learning progress, but interviewers will want to know how you can optimize your algorithm for impact, not for the sake of increasing accuracy. Talk to data science experts, try to understand how these projects work, build your network in the domain of your choice, and try to structure your thoughts to align accordingly.Well wrap up this article with another myth around building models. This is a conversation I had with a fresher data scientist recently:Pranav Dar: Whats your favorite part of a data science role apart from designing models?Fresher DS: I like the feature engineering part.PD: Sounds fair. How do you usually collect data for your projects?Fresher DS: Um, I usually just download it from one of the open-source platforms.PD: OK..but what if the data is skewed or biased? How do you verify the identity of the data? And what will you do when youre asked to collect data from multiple sources that require database skills?Fresher DS: I hadnt thought about that..That, unfortunately, is a conversation I have on a far too regular basis. Most experienced data science professionals are well aware of this situation as well. Expect to be tested on this subject thoroughly in an interview.Data is being generated at an unprecedented pace but collecting and cleaning it isnt getting any easier. Without building a pipeline to collect the data, your data science project is going nowhere. Typically, this is the role of a data engineer (but data scientists are expected to know this function as well).I cannot overstate the importance of the data collection step. Collecting honest and accurate data is imperative to your final model working well. As Wikipedia puts it, The goal for all data collection is to capture quality evidence that allows analysis to lead to the formulation of convincing and credible answers to the questions that have been posed.There are too many sources of data available. How do you connect to each? What data format do you receive from each? Whats the cost of data collection from each of these sources? This is a microcosm of the kind of questions youll need to ask in a real-world setting.Roles like database manager, database architect and data engineer have taken on a new level of importance. Maintaining the integrity of the data and the aforementioned pipeline is as important as any other task that succeeds it.I had initially planned to make this a short piece. But the importance of busting these misconceptions took precedence over everything else. There are a lot more myths following data science like a shadow.I would love to hear your thoughts on this article and if there are other myths that you have come across (or believed yourself). Lets empower the new wave of data science professionals with resources to overcome the mistakes we have made!",https://www.analyticsvidhya.com/blog/2019/01/myths-data-science-transition/
Build a Recurrent Neural Network from Scratch in Python  An Essential Read for Data Scientists,Learn everything about Analytics|Introduction|Table of Contents|Flashback: A Recap of Recurrent Neural Network Concepts|Sequence Prediction using RNN|Coding RNN using Python|End Notes,"Step 0: Data Preparation|Step 1: Create the Architecture for our RNN model|Step 2: Train the Model|Step 3: Get predictions|Share this:|Like this:|Related Articles|Busted! 11 Myths Data Science Transitioners Need to Avoid|[Big Announcement] Your favourite hackathons now open for practice on DataHack!|
Faizan Shaikh
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",Step 2.1: Check the loss on training data|Step 2.2: Check the loss on validation data|Step 2.3: Start actual training|Step 2.3.1: Forward Pass||Step 2.3.2 : Backpropagate Error|Step 2.3.3 : Update weights,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Humans do not reboot their understanding of language each time we hear a sentence. Given an article, we grasp the context based on our previous understanding of those words. One of the defining characteristics we possess is our memory (or retention power).Can an algorithm replicate this? The first technique that comes to mind is a neural network (NN). But the traditional NNs unfortunately cannot do this. Take an example of wanting to predict what comes next in a video. A traditional neural network will struggle to generate accurate results.Thats where the concept of recurrent neural networks (RNNs) comes into play. RNNs have become extremely popular in the deep learning space which makes learning them even more imperative. A few real-world applications of RNN include:In this article, well first quickly go through the core components of a typical RNN model. Then well set up the problem statement which we will finally solve by implementing an RNN model from scratch in Python.We can always leverage high-level Python libraries to code a RNN. So why code it from scratch? I firmly believe the best way to learn and truly ingrain a concept is to learn it from the ground up. And thats what Ill showcase in this tutorial.This article assumes a basic understanding of recurrent neural networks. In case you need a quick refresher or are looking to learn the basics of RNN, I recommend going through the below articles first:Lets quickly recap the core concepts behind recurrent neural networks.Well do this using an example of sequence data, say the stocks of a particular firm. A simple machine learning model, or an Artificial Neural Network, may learn to predict the stock price based on a number of features, such as the volume of the stock, the opening value, etc. Apart from these, the price also depends on how the stock fared in the previous fays and weeks. For a trader, this historical data is actually a major deciding factor for making predictions.In conventional feed-forward neural networks, all test cases are considered to be independent. Can you see how thats a bad fit when predicting stock prices? The NN model would not consider the previous stock price values  not a great idea!There is another concept we can lean on when faced with time sensitive data  Recurrent Neural Networks (RNN)!A typical RNN looks like this:This may seem intimidating at first. But once we unfold it, things start looking a lot simpler:It is now easier for us to visualize how these networks are considering the trend of stock prices. This helps us in predicting the prices for the day. Here, every prediction at time t (h_t) is dependent on all previous predictions and the information learned from them. Fairly straightforward, right?RNNs can solve our purpose of sequence handling to a great extent but not entirely.Text is another good example of sequence data. Being able to predict what word or phrase comes after a given text could be a very useful asset. We want our models towrite Shakespearean sonnets!Now, RNNs are greatwhen it comes to context that is short or small in nature. But in order to be able to build a story and remember it, our models should be able to understand the context behind the sequences, just like a human brain.In this article, we will work on a sequence prediction problem using RNN. One of the simplest tasks for this is sine wave prediction. The sequence contains a visible trend and is easy to solve using heuristics. This is what a sine wave looks like:We will first devise a recurrent neural network from scratch to solve this problem. Our RNN model should also be able to generalize well so we can apply it on other sequence problems.We will formulate our problem like this  given a sequence of 50 numbers belonging to a sine wave, predict the 51st number in the series. Time to fire up your Jupyter notebook (or your IDE of choice)!Ah, the inevitable first step in any data science project  preparing the data before we do anything else.What does our network model expect the data to be like? It would accept a single sequence of length 50 as input. So the shape of the input data will be:Here, types_of_sequences is 1, because we have only one type of sequence  the sine wave.On the other hand, the output would have only one value for each record. This will of course be the 51st value in the input sequence. So its shape would be:Lets dive into the code. First, import the necessary libraries:To create a sine wave like data, we will use the sine function from Pythons math library:Visualizing the sine wave weve just generated:Print the shape of the data:Our next task is defining all the necessary variables and functions well use in the RNN model. Our model will take in the input sequence, process it through a hidden layer of 100 units, and produce a single valued output:We will then define the weights of the network:Here,Finally, we will define the activation function, sigmoid, to be used in the hidden layer:Now that we have defined our model, we can finally move on with training it on our sequence data. We can subdivide the training process into smaller steps, namely:Step 2.1 : Check the loss on training data
Step 2.1.1 : Forward Pass
Step 2.1.2 : Calculate Error
Step 2.2 : Check the loss on validation data
Step 2.2.1 : Forward Pass
Step 2.2.2 : Calculate Error
Step 2.3 : Start actual training
Step 2.3.1 : Forward Pass
Step 2.3.2 : Backpropagate Error
Step 2.3.3 : Update weightsWe need to repeat these steps until convergence. If the model starts to overfit, stop! Or simply pre-define the number of epochs.We will do a forward pass through our RNN model and calculate the squared error for the predictions for all records in order to get the loss value.We will do the same thing for calculating the loss on validation data (in the same loop):You should get the below output:We will now start with the actual training of the network. In this, we will first do a forward pass to calculate the errors and a backward pass tocalculate the gradients and update them. Let me show you these step-by-step so you can visualize how it works in your mind.In the forward pass:Here is the code for doing a forward pass (note that it is in continuation of the above loop):After the forward propagation step, we calculate the gradients at each layer, and backpropagate the errors. We will use truncated back propagation through time (TBPTT), instead of vanilla backprop. It may sound complex but its actually pretty straight forward.The core difference in BPTT versus backprop is that the backpropagation step is done for all the time steps in the RNN layer. So if our sequence length is 50, we will backpropagate for all the timesteps previous to the current timestep.If you have guessed correctly, BPTT seems very computationally expensive. So instead of backpropagating through all previous timestep , we backpropagate till x timesteps to save computational power. Consider this ideologically similar to stochastic gradient descent, where we include a batch of data points instead of all the data points.Here is the code for backpropagating the errors:Lastly, we update the weights with the gradients of weights calculated. One thing we have to keep in mind that the gradients tend to explode if you dont keep them in check.This is a fundamental issue in training neural networks, called the exploding gradient problem. So we have to clamp them in a range so that they dont explode. We can do it like thisOn training the above model, we get this output:Looking good! Time to get the predictions and plot them to get a visual sense of what weve designed.We will do a forward pass through the trained weights to get our predictions:Plotting these predictions alongside the actual values:I cannot stress enough how useful RNNs are when working with sequence data. I implore you all to take this learning and apply it on a dataset. Take a NLP problem and see if you can find a solution for it. You can always reach out to me in the comments section below if you have any questions.In this article, we learned how to create a recurrent neural network model from scratch by using just the numpy library. You can of course use a high-level library like Keras or Caffe but it is essential to know the concept youre implementing.Do share your thoughts, questions and feedback regarding this article below. Happy learning!",https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/
[Big Announcement] Your favourite hackathons now open for practice on DataHack!,Learn everything about Analytics|Hackathons are the best way of learning|Announcement|What does it mean for you?|New Practice Problems,"Practice Problem: Food Demand Forecasting Challenge||Practice Problem: HR Analytics Challenge||Practice Problem: Face Counting Challenge|Practice Problem: Identify the Sentiments|Practice Problem: Predict Number of Upvotes||End Notes|Share this:|Like this:|Related Articles|Build a Recurrent Neural Network from Scratch in Python  An Essential Read for Data Scientists|How Coursera uses Data Visualization and Clustering to Categorize Content|
Ankit Choudhary
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python  
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"We frequently get requests from our participants and users for access to datasets of closed hackathons. It gives me immense pleasure to announce that we will be opening most of the past hackathons for community practice.To start with, we are opening 5 hackathons as practice problems for all our users. We will open up more in coming days.The new hackathons will essentially be clones of the original hackathons with the replica of the following databases maintained:Leaderboard: This is a new development as for these contests, the leaderboard would not be blank on launch but would be populated by the leaderboard from the original contest. Participants would be able to try training that model which they did not get the time to do during the contest and check whether it actually beats No 1 Submissions:Anyone can freely participate in these contests. However, those who participated in the original contest would also be able to access the old submissions that they had made during the original contest.More Discussions! More Collaboration! More learning!:Original contests might have had constraints w.r.t. sharing and discussing approach or solutions. However, now all the solutions and approaches would be discussed openly by all participants. Users can use the facility of sharing approach, code on leaderboard and also use the discussion forum to collaborate on using various techniques to learn and compete.Team Participation:Participants can form a team of 2 to attempt the problems in tandem and learn from each others mistakes or just build a wonderful ensemble model to beat that top scoreIt means DataHack is going to be lot more fun, lot more engagement and you dont need to wait for the weekends to keep your learning going. You learn5 exciting challenges awaiting you at the DataHack! Read on.I am personally very excited about this announcement and I am sure our community will love this.More datasets, more learning, more solutions from experts => a tone of learning and fun! Do let us know how you feel about this and which problems would you attack first!",https://www.analyticsvidhya.com/blog/2019/01/big-announcement-your-favourite-hackathons-now-open-for-practice-on-datahack/
How Coursera uses Data Visualization and Clustering to Categorize Content,Learn everything about Analytics|Introduction|Table of Contents|The Previous Course Categories|t-SNE to the Rescue|The General Structure of Courseras Content,"About the Authors|Share this:|Like this:|Related Articles|[Big Announcement] Your favourite hackathons now open for practice on DataHack!|DataHack Radio #16: Kaggle Grandmaster SRKs Journey and Advice for Data Science Competitions|
Guest Blog
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Coursera is one of the biggest educational platforms in the world. Launched in 2012 by Andrew Ng, the content on Coursera has increased multi-fold since. In fact, Andrew Ngs Introduction to Machine Learning course has spawned thousands of careers in data science.But with increased content comes the challenge of defining which category each course belongs to. This is such a critical aspect of a platform that serves up pinpoint recommendations to its users.Courses on Coursera cover topics ranging fromphotographytoprobabilistic graphical modelstoconstitutional struggles in the Muslim world. This diversity makes them hard to categorize. A couple of years ago, we overhauled our course categories and implemented a new categorization system we call domains and subdomains. This article covers how we defined and implemented that new system.Courseras original categorization scheme dated back to our founding in 2012, and was heavily influenced by the content available at the time. For example, we had five categories of computer science subfields, but only one category for all of the humanities. The categories were also manually and arbitrarily defined, resulting in redundancies (e.g., Food and Nutrition being nearly a subset of Health and Society) and vagueness (e.g., Information, Tech & Design).Critically, the original categorization scheme was not meeting our need of effectively matching learner to content. For example, the Medicine category attracted two distinct groups of learnersbecause it contained two distinct groups of courses. The first were courses that appealed to healthcare practitioners (e.g., onclinical kidney transplantationorbiocontainment for infectious diseases). The second were courses on public health issues that appealed to non-practitioners.As our catalog expands to thousands of courses, we need a principled organization technique. We want categories that help the learner find the best content for them. This translates to the following criteria:Rather than re-coding by hand, or replicating traditional university departments, we took a data-driven approach.We sought to group our courses so that someone interested in one course in that group, say,playing the guitar, might also be interested in other courses in that group, say,songwritingorjazz improvisation. The algorithm known as t-distributed stochastic neighbor embedding (t-SNE) satisfies this requirement.t-SNE identifies an arrangement of courses such that courses sharing common learners are close and courses that do not share common learners are far apart. For example,Complex AnalysisandGalois Theoryare close together since many learners take both, whileTaking Care of HorsesandGeneral Relativityare farther apart as the two courses do not share many learners.We utilized the t-SNE algorithm on courses in 2015 to produce the scatter plot output shown below. Each dot represents a single course. We then grouped these courses into categories by clustering (represented by the coloring).Figure 1: t-SNE visualization of courses colored by cluster, circa 2015.Figure 2. General subject area of courses.Looking at Figures 1 and 2, the first thing we see is that courses are organized in a globally consistent way: humanities, social sciences, and business courses are in the top right half of the plot, while natural sciences, engineering, and computational sciences courses fall in the bottom left half.Digging in to a more granular level reveals additional nuance:Figure 3. General division of science and humanities courses.Figure 4. Substructure of courses within each half of the plot.Dissecting each of these large regions further, we see that even within each grouping, courses are arranged logically. For example, courses in natural history span a continuum roughly from the biological sciences to the physical sciences. Similarly, courses in humanities and social science range roughly from music to the visual arts, humanities, and social sciences, and then practical business.Even at the level of individual courses, t-SNE captures the interdisciplinary nature of courses. Courses in business law, for example, fall on the boundary between business and law, and courses on quantitative methods for the social sciences fall between math and the social sciences.Figure 5. Interdisciplinary courses sit roughly between the right clusters.Returning to the set of courses previously categorized as medicine, we now have three sub-categories. First is a relatively disjointed cluster of courses targeted to the medical professional (e.g., Ebola: Essential Knowledge for the Healthcare Professional). Second is a cluster of courses on healthcare policy (e.g., Systems Thinking in Public Health), and lastly, we have a cluster on basic biology (e.g., Introduction to Genetics and Evolution).The result is noteworthy because there is no strong reason why t-SNE should arrange our courses by subject matter. We werent feeding in course descriptions or transcripts, just the enrollment behavior of learners. We attribute the clusterability of courses to the fact that learners are much more likely to be interested in multiple courses in a particular subject area rather than to be influenced in their course decision by non-subject-matter factors such as the style of instruction or the institution offering the course.That said, this assumption does not hold across the full catalog. For example, among non-English content, the language of instruction is more a driver of enrollment than the subject area. Correspondingly, a French or Russian course is more likely to be grouped with other courses taught in the respective language than it is to be grouped with other courses on the same subject.After some clean up, we landed at 36 course clusters, which rolled up into nine larger clusters. On the Coursera platform today, we term the original clusters subdomains and the larger clusters domains. We launched the new system of domains and subdomains in summer 2015. In the past three years, it has become an integral part of Courseras content discovery experience, allowing us to scale content categorization and drive personalization for each and every learner on our platform.This post originally appeared on Coursera Engineerings Medium page.This post was a collaboration by the data science team at Coursera, the worlds leading platform for higher education. The team builds the statistical models and machine learning algorithms that power content discovery and help scale an engaging and personalized learning experience; leads the quantitative measurement, experimentation, and inference that informs the companys product, business, and content strategy; and develops the analytics and direct data access for the platforms university partners and enterprise customers. We believe the next generation of teaching and learning is personalized, accessible, and efficient  reaching a world of learners who need it  and are united in building that vision through data-informed decisions and data-powered products.",https://www.analyticsvidhya.com/blog/2019/01/coursera-data-driven-content-categorization-algorithm/
DataHack Radio #16: Kaggle Grandmaster SRKs Journey and Advice for Data Science Competitions,Learn everything about Analytics|Introduction|SRKs Background|Transition from Software Engineering to Data Science|Participating in Data Science Competitions|Does Participating in Data Science Competitions Help in an Industry Role?|The Secret Behind Making the First Submission within Hours of a Competition Launch|Advice to Aspiring Data Scientists|End Notes,"Share this:|Related Articles|How Coursera uses Data Visualization and Clustering to Categorize Content|Introduction to Monte Carlo Tree Search: The Game-Changing Algorithm behind DeepMinds AlphaGo|
Pranav Dar
|3 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Ladies and gentlemen, presenting Analytics Vidhyas top community member and Kaggle Grandmaster  Sudalai Rajkumar, aka SRK!He is an inspiration and enigma in the data science community. I had the pleasure of meeting him at DataHack Summit 2018 and was blown away by his humility and intelligence. I jumped at the first chance of getting him on our DataHack podcast.Whether its his analytical acumen, wealth of experience, or down-to-earth demeanour, we could all learn something from SRK. In this episode, we cover:And much, much more!I have picked out the highlights of SRKs discussion with Kunal in this article. Happy listening!DataHack Radio is now on all the popular podcast platforms. Subscribe today and listen to this and all previous episodes below:Finding patterns in data is something I love to do.SRKs background and journey into data science wasnt a straightforward one. He completed his B.E. in Mechanical Engineering from PSG College of Engineering in 2010. During his final year there, he received two offers  one from the mechnical field and one from an analytics company.Remember, this was back when the term data science hadnt been coined yet. It was plain old analytics. After weighing up both offers, SRK went with the analytics option. Working with data and finding patterns piqued his curiosity and that led to his first foray into this field.But he didnt start working on an analytics project straight off the bat. His was more of a software engineering role (quite a lot of our community will relate to that!). SRKs responsibilities included writing codes to put models built by analysts into production. He worked with Python at that time, which, as he admits, was a lucky break in hindsight.We might be familiar with pandas and numpy nowadays, but back then SRK worked exclusively with native Python functions (lists, tuples, dictionaries, etc.).A year in the job is enough to ask the question  where do I go from here? Most of us have been there and SRK faced up to it in 2011. He got a switch within the same organization and moved into an analytics role.Which tools were considered state-of-the-art at that point? The experienced readers among you will recall that Python wasnt close to what it is now. Quite a lot of the packages we use these days did not exist eight years ago. Instead, R was primarily used to perform the exploratory analysis and build models. Of course, SAS also had a big market share. Deployment and production was done in Python. A neat segregation of tasks, right?During his role transition, SRK worked with R for the first 1 year or so. He then switched to Python for the modeling part of his job as well.His work was majorly in the financial domain at that point. He worked with different kinds of credit risk models, weight risk models, marketing models, etc. Initially his team worked with linear and logistic regression models. But the focus slowly shifted to more advanced machine learning techniques, like random forest and GBMs.This was the period when deep learning had started to penetrate into mainstream applications. Given his penchant for patterns and trying new things, SRK started experimenting with a few deep learning models using libraries like Theano and PyBrain.During his first year in the analytics project, SRK started reading and learning about analytics outside his working environment. Hes candid enough to admit that at one point, he thought analytics was only about linear and logistic regression!  Goes to show how big a role self-learning plays in our life.He was exposed to the world of machine learning algorithms during his learning phase. But his day-to-day role didnt involve anything outside building linear models. Thats when SRK discovered the usefulness of data science competitions.Participating in these competitions was far more difficult then as compared to now. There were no blogs or MOOCs teaching one how to crack competitions. SRK relied on a librarys official documentation page and his own analytical acumen to figure out the way forward. It did take a lot of time to experiment and learn but it helped him immensely in his learning.But heres where the best are separated from the rest  SRK spent, on average, 3-5 hours every day after work to learn new concepts or brush up old ones. That is the level of hard work we need to put in to succeed in this field.Weve heard this question come up plenty of times. Who better to answer it than a Kaggle Grandmaster?SRKs first role in analytics was in the R&D side of things. So he had leeway to experiment with new algorithms. The knowledge and experience he gained through data science competitions came in handy for him here. Instead of learning on the job, it became more about experimenting on the job. Something we all can strive for more!Admittedly, not all of us will get the chance to play around with new algorithms in our day-to-day roles. But the learning we gain through these hackathons is invaluable. It isnt limited to just applying new techniques. Critical thinking, brainstorming, working with numbers (data intuition), structured thinking, ability to experiment  these are characteristics that will help you in making business decisions. You might not see it happening straightaway but patience is key to success, especially in data science.This has always intrigued me. Whenever a new competition page goes up on Analytics Vidhya or Kaggle, SRK usually has his first submission ready within a few hours. How is that even possible?As it turns out, most folks who regularly participate in these competitions have a generic code base ready before the dataset is released. They just modify the code based on the problem statement. The hyperparameter tuning and all other experiments are done post that.Acing data science competitions is a tough proposition. I have struggled to crack the top 10 in any popular competition Ive taken part in. SRK gave us a lot of practical advice in this episode which should help you streamline your learning process.What was your favorite part about this episode? And if youve ever had the fortune of meeting SRK, I would love to know your experience.",https://www.analyticsvidhya.com/blog/2019/01/datahack-radio-tips-crack-data-science-competitions-kaggle-grandmaster/
Introduction to Monte Carlo Tree Search: The Game-Changing Algorithm behind DeepMinds AlphaGo,Learn everything about Analytics|Introduction|Table of Contents|Game AI  A Summary|Components of the AlphaGo|The Game Tree Concept|Tree Search Algorithms|Monte Carlo Tree Search|End Notes,"Uninformed Search|Best First Search|Minimax|Tree Traversal & Node Expansion|Rollout|Complete Walkthrough with an example|Share this:|Like this:|Related Articles|DataHack Radio #16: Kaggle Grandmaster SRKs Journey and Advice for Data Science Competitions|Top 10 Presentations from rstudio::conf 2019  The Best R Conference of the Year!|
Ankit Choudhary
|4 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"A best of five game series, $1 million dollars in prize money  A high stakes shootout. Between 9 and 15 March, 2016, the second-highest ranked Go player, Lee Sidol, took on a computer program named AlphaGo.AlphaGo emphatically outplayed and outclassed Mr. Sidol and won the series 4-1. Designed by Googles DeepMind, the program has spawned many other developments in AI, including AlphaGo Zero. These breakthroughs are widely considered as stepping stones towards Artificial General Intelligence (AGI).In this article, I will introduce you to the algorithm at the heart of AlphaGo  Monte Carlo Tree Search (MCTS). This algorithm has one main purpose  given the state of a game, choose the most promising move.To give you some context behind AlphaGo, well first briefly look at the history of game playing AI programs. Then, well see the components of AlphaGo, the Game Tree Concept, a few tree search algorithm, and finally dive into how the MCTS algorithm works.AI is a vast and complex field. But before AI officially became a recognized body of work, early pioneers in computer science wrote game-playing programs to test whether computers could solve human-intelligence level tasks.To give you a sense of where Game Playing AI started from and its journey till date, I have put together the below key historical developments:And this is just skimming the surface! There are plenty of other examples where AI programs exceeded expectations. But this should give you a fair idea of where we stand today.The core parts of the Alpha Go comprise of:In this blog, we willfocus on the working of Monte Carlo Tree Searchonly. This helps AlphaGo and AlphaGo Zero smartly explore and reach interesting/good states in a finite time period which in turn helps the AI reach human level performance.Its application extends beyond games. MCTS can theoretically be applied to any domain that can be described in terms of {state,action} pairs and simulation used to forecast outcomes. Dont worry if this sounds too complex right now, well break down all these concepts in this article.Game Trees are the most well known data structures that can represent a game. This concept is actually pretty straightforward.Each node of a game tree represents a particular state in a game. On performing a move, one makes a transition from a node to its children. The nomenclature is very similar to decision trees wherein the terminal nodes are called leaf nodes.For example, in the above tree, each move is equivalent to putting a cross at different positions. This branches into various other states where a zero is put at each position to generate new states. This process goes on until the leaf node is reached where the win-loss result becomes clear.Our primary objective behind designing these algorithms is to find best the path to follow in order to win the game. In other words, look/search for a way of traversing the tree that finds the best nodes to achieve victory.The majority of AI problems can be cast as search problems, which can be solved by finding the best plan, path, model or function.Tree search algorithms can be seen as building a search tree:The tree branches out because there are typically several different actions that can be taken in a given state. Tree search algorithms differ depending on which branches are explored and in what order.Lets discuss a few tree search algorithms.Uninformed Search algorithms, as the name suggests, search a state space without any further information about the goal. These are considered basic computer science algorithms rather than as a part of AI. Two basic algorithms that fall under this type of search are depth first search (DFS) and breadth first search (BFS). You can read more about them in this blog post.The Best First Search (BFS) method explores a graph by expanding the most promising node chosen according to a specific rule.The defining characteristic of this search is that, unlikeDFSorBFS(which blindly examine/expand a cell without knowing anything about it), BFS uses an evaluation function (sometimes called a heuristic) to determine which node is the most promising, and then examines this node.For example, A* algorithm keeps a list of open nodes which are next to an explored node. Note that these open nodes have not been explored. For each open node, an estimate of its distance from the goal is made. New nodes are chosen to explore based on the lowest cost basis, where the cost is the distance from the origin node plus the estimate of the distance to the goal.For single-player games, simple uninformed or informed search algorithms can be used to find a path to the optimal game state. What should we do for two-player adversarialgames where there is another player to account for? The actions of both players depend on each other.For these games, we rely on adversarial search. This includes the actions of two (or more) adversarial players. The basic adversarial search algorithm is called Minimax.This algorithm has been used very successfully for playing classic perfect-information two-player board games such as Checkers and Chess. In fact, it was (re)invented specifically for the purpose of building a chess-playing program.The core loop of the Minimax algorithm alternates between player 1 and player 2, quite like the white and black players in chess. These are called the min player and the max player. All possible moves are explored for each player.For each resulting state, all possible moves by the other player are also explored. This goes on until all possible move combinations have been tried out to the point where the game ends (with a win, loss or draw). The entire game tree is generated through this process, from the root node down to the leaves:Each node is explored to find the moves that give us the maximum value or score.Games like tic-tac-toe, checkers and chess can arguably be solved using the minimax algorithm. However, things can get a little tricky when there are a large number of potential actions to be taken at each state. This is because minimax explores all the nodes available. It can become frighteningly difficult to solve a complex game like Go in a finite amount of time.Go has a branching factor of approximately 300 i.e. from each state there are around 300 actions possible, whereas chess typically has around 30 actions to choose from. Further, the positional nature of Go, which is all about surrounding the adversary, makes it very hard to correctly estimate the value of a given board state. For more information on rules for Go, please refer this link.There are several other games with complex rules that minimax is ill-equipped to solve. These include Battleship Poker with imperfect information and non-deterministic games such as Backgammon and Monopoly. Monte Carlo Tree Search, invented in 2007, provides a possible solution.The basic MCTS algorithm is simple: a search tree is built, node-by-node, according to the outcomes of simulated playouts. The process can be broken down into the following steps:Before we delve deeper and understand tree traversal and node expansion, lets get familiar with a few terms.UCB ValueUCB1, or upper confidence bound for a node, is given by the following formula:where,What do we mean by a rollout? Until we reach the leaf node, we randomly choose an action at each step and simulate this action to receive an average reward when the game is over.Flowchart for Monte Carlo Tree SearchTree Traversal & Node ExpansionYou start with S0, which is the initial state. If the current node is not a leaf node, we calculate the values for UCB1 and choose the node that maximises the UCB value. We keep doing this until we reach the leaf node.Next, we ask how many times this leaf node was sampled. If its never been sampled before, we simply do a rollout (instead of expanding). However, if it has been sampled before, then we add a new node (state) to the tree for each available action (which we are calling expansion here).Your current node is now this newly created node. We then do a rollout from this step.Lets do a complete walkthrough of the algorithm to truly ingrain this concept and understand it in a lucid manner.Iteration 1:Initial StateRollout from S1Post BackpropogationThe way MCTS works is that we run it for a defined number of iterations or until we are out of time. This will tell us what is the best action at each step that one should take to get the maximum return.Iteration 2:Backpropogation from S2Iteration 3:Iteration 4:That is the gist of this algorithm. We can perform more iterations as long as required (or is computationally possible). The underlying idea is thatthe estimate of values at each node becomes more accurate as the number of iterations keep increasing.Deepminds AlphaGo and AlphaGo Zero programs are far more complex with various other facets that are outside the scope of this article. However, the Monte Carlo Tree Search algorithm remains at the heart of it. MCTS plays the primary role in making complex games like Go easier to crack in a finite amount of time. Some open source implementations of MCTS are linked below:Implementation in PythonImplementation in C++I expect reinforcement learning to make a lot of headway in 2019. It wont be surprising to see a lot more complex games being cracked by machines soon. This is a great time to learn reinforcement learning!I would love to hear your thoughts and suggestions regarding this article and this algorithm in the comments section below. Have you used this algorithm before? If not, which game would you want to try it out on?",https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago/
Top 10 Presentations from rstudio::conf 2019  The Best R Conference of the Year!,Learn everything about Analytics|Introduction|Talks Covered in this Article|Tools and Frameworks|Deploying a Machine Learning Model via the Plumber Package  James Blair|pagedown: Create Beautiful PDFs with R Markdown and CSS  Yihui Xie & Romain Lesur|Mastering Techniques in R|Working with Categorical Data in R without Losing your Mind  Amelia McNamara|Melt the Clock: Tidy Time Series Analysis  Earo Wang|Visualizing Uncertainity with Hypothetical Outcome Plots (HOPs)  Claus O. Wilke|Learning R|Teaching Data Science with Puzzles  Irene Stevens|R for Organizational Thinking|Empowering a Data Team with RStudio Addins  Hao Zhu|Using Data Effectively: Beyond Art and Science  Hilary Parker|Data Science as a Team Sport  Angela Bassa|Deep Learning|Why TensorFlow Eager Execution Matters  Sigrid Keydana|Bonus: The Next Million R Users  Carl Howe|End Notes,"Share this:|Like this:|Related Articles|Introduction to Monte Carlo Tree Search: The Game-Changing Algorithm behind DeepMinds AlphaGo|Must-Read Tutorial to Learn Sequence Modeling (deeplearning.ai Course #5)|
Pranav Dar
|5 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"First, a confession  Im a big fan of R! It has been a bit under the radar in recent years with the rise of Python, but it remains my go-to tool of choice for doing data science tasks.Let me list down the key reasons why Ive always appreciated R:I feel like a kid on Christmas eve whenever someone pulls out R code on the stage!So when I came to know about the rstudio::conf last year (which I covered here), I felt giddy with joy. An entire conference on R!This years conference, rstudio::conf 2019, promised an ever bigger and better show than 2018. And in this article, I have penned down my thoughts on my favorite topics presented at rstudio::conf 2019.I have provided as many resources as I could find regarding each presentation.These resources should help you get started with the topic or package on your own.Note: This years conference was held in Austin, Texas, from 15th to 18th January, 2019. The first two days, 15th and 16th, were exclusively for workshops. This article focuses on the main conference talks which were held on 17th and 18th January.I have categorized the presentations according to the content.Click on each presentation title to access the slide deck for that talk.We dont talk enough about model deployment in machine learning. It gets lost in the hackathons and competitions we participate in when were learning our way into this line of work. But without learning how deployment works, our model isnt going to see the light of day.So it was a welcome sight to see a practical presentation on this topic. James Blair introduced us to the plumber package, which converts our existing R code to a web API that other services can connect to. You should check out the packages official documentation to scour through some examples and understand how to use it in your project.You can install the plumber package from CRAN via the below code:R Markdown is a thing of beauty. But I have personally struggled to generate user-friendly HTML or PDF formats from it (perhaps I might need to work harder there). The pagedown package certainly looks like its going to be my new best friend.As Yihui and Romain showcased in their talk, you can use pagedown to generate beautiful PDF documents, including business cards, resumes, posters, letters, e-books, research papers, and more. You can check out these examples in their presentation linked above. It looks really neat!You can install the package from GitHub using the below command (its available via CRAN too):The concept of factors bamboozled me when I started out with R. I hadnt heard about this anywhere else. But as I worked with categorical data, I started to see its value to the kind of analysis I used to do.I do accept that it can still be a frustrating feature to grasp. Most of the categorical data questions I found on Stackoverflow and other sites pertained to the use of factors. There are workarounds, of course, but you cant really get away for long without having to reply on factors.This presentation by Amelia McNamara talked about the forcats package, a suite of tools that solves common categorical data problems with factors. The package is part of tidyverse so if you have that installed, just load it up:Time series data is messy. Anyone who has even touched the topic has been through that experience. I have tried some shoddy tricks in the past to make my time series analysis presentable but it takes way too much time.So when I saw Earo Wangs presentation title, I was intrigued. Earo spoke about unifying the time series workflow using two packages  tsibble and fable. This presentation basically talks about these two packages using examples. Check out the two slides below which show the value of a tidy time series pipeline:Yes, theres a lot of code included in the presentation itself so you can replicate it on your own machine and watch the magic unfold.Visualizing uncertainty? It can be intimidating for most data science professionals. And communicating that uncertain level to non-technical people? Thats just a whole new layer of difficulty added on top!Hypothetical Outcome Plots are a veyr useful way of making non-experts understand uncertainty in your data. You should read UW Interactive Data labs paper on this concept to understand how HOPs work. According to their research, HOPs enables viewers to infer properties of the distribution using mental processes like counting and integration.This presentation presents several ideas on how to generate these HOPs in R. Its worth taking a few minutes to browse through their thinking and code. You can also install the ungeviz package which is a set of tools to visualize uncertainty.One of my favorite presentations from the conference. Puzzles play a key role in gauging how a person thinks, and whether he/she has the potential to thrive in data science. I cant recommend them enough to all aspiring data scientists  practice, practice, practice.This presentation, and the idea behind it, focus on the programming skills required for dealing with untidy data. Yep, I was intrigued too. Ms. Irene Stevens presented a series of puzzles she coined Tidies of March. The name comes from the popular tidyverse package since these puzzles test your data wrangling skills in R.There is a really cool sandwiches examples included in the above linked GitHub repository that you should check out. It was a real hit during Irenes presentation.Have you worked with addins inside RStudio before? If not, you will find this presentation a good place to start. The content is comprehensive enough and covers the following topics:If youve ever created an R package before, youll find working with addins a breeze.Im a huge fan of how StitchFix has built their entire business strategy around data science. Check out their algorithms specific web page  its a walk down paradise lane for any data science professional.Hilary Parker, a Data Scientist at StitchFix, walked us through the basic data science process in their organization. He meat of the talk comes at slide #25, where she introduces the magick package. The aim of the package is to modernize and improve high-quality image processing in R.There are a few design recommendations, tips and tricks in the presentation as well which you might find helpful.Data Science project managers and CxOs  this one is right up your alley. What should you do when your small data science team starts expanding? That line of thought is inevitable once the team starts producing successful results. Should you integrate the team into a certain discipline?This excellent presentation by Angela Bassa also covers questions like should you hire specialists for different roles or hold out for unicorn data scientists? Whats the trade-off of hiring junior talent as opposed to a few senior ones? What should be a data science teams composition?These arent just theoretical questions. As we enter a golden age in terms of data collection, leadership needs to understand how a data science team functions in the overall scheme of things. Failure to answer these questions effectively could potentially have long term implications for the organization.The R universe has been notoriously slow in adopting deep learning frameworks. Its one of the reasons Python has raced ahead in the industry. But most DL libraries have made their way into R in recent times.In this presentation, Sigrid Keydana gave us an overview of the Keras framework. The talk then shifted to the main topic  TensorFlow Eager Execution in R. This was demonstrated on two key deep learning concepts:Theres a ton of code available in the presentation. You should go through this post to understand how TensorFlow works in case you havent used it before.Note: To view the full presentation, youll have to download/clone the GitHub repository I have linked above. Then unzip the folder and click the .html file.While this doesnt fit any particular category, Ive included this presentation to showcase how effective plots can be. This talk was all about RStudios survey around the globe to understand their user base and predict their next million users.The four key questions they asked (and answered through plots) were:I love the R community. There is a wonderful feeling of inclusivity among R users that I havent found anywhere else. Do you use (and love) R regularly? Hit me up in the comments section below this article and lets discuss a few ideas!These were my top picks among all the presentations at rstudio::conf 2019. Karl Broman has taken the time to put together resources from all the talks in one single place. Talks were divided into three parallel tracks so you can pick and choose depending on your interests.",https://www.analyticsvidhya.com/blog/2019/01/top-highlights-rstudioconf-2019-best-r-conference/
Must-Read Tutorial to Learn Sequence Modeling (deeplearning.ai Course #5),Learn everything about Analytics|Introduction|Table of Contents|Course Structure|Course 5: Sequence Models|Recurrent Neural Network (RNN) Model|Module 2: Natural Language Processing & Word Embeddings|Part 1  Introduction to Word Embeddings|Part 2  Learning Word Embeddings: Word2Vec & GloVe|Part 3  Applications using Word Embeddings|Module 3: Sequence Models & Attention Mechanism|End Notes,"Module 1: Recurrent Neural Networks|But first, why sequence models?|Notations well use in this article|Backpropagation through time|Different types of RNNs|Language model and sequence generation|Vanishing gradients with RNNs|Gated Recurrent Units (GRUs)|Long Short Term Memory (LSTM)|Bidirectional RNN|Deep RNNs|Word Representation|Using word embeddings|Properties of word embeddings|Embedding matrix|Learning Word Embeddings|Word2Vec|Negative Sampling|GloVe word vectors|Sentiment Classification|Basic Models|Picking the most likely sentence|Beam Search|Refinements to Beam Search|Error analysis in beam search|Attention Model|Share this:|Like this:|Related Articles|Top 10 Presentations from rstudio::conf 2019  The Best R Conference of the Year!|27 Amazing Data Science Books Every Data Scientist Should Read|
Pulkit Sharma
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"The ability to predict what comes next in a sequence is fascinating. Its one of the reasons I became interested in data science! Interestingly  human mind is really good at it, but that is not the case with machines. Given a mysterious plot in a book, the human brain will start creating outcomes. But, how to teach machines to do something similar?Thanks to Deep Learning  we can do lot more today than what was possible a few years back. The ability to work with sequence data, like music lyrics, sentence translation, understanding reviews or building chatbots  all this is now possible thanks to sequence modeling.And thats what we will learn in this article. Since this is part of our deeplearning.ai specialization series, I expect that the reader will be aware of certain concepts. In case you havent yet gone through the previous articles or just need a quick refresher, here are the links:In this final part, we will see how sequence models can be applied in different real-world applications like sentiment classification, image captioning, and many other scenarios.We have covered quite a lot in this series so far. Below is a quick recap of the concepts we have learned:Its time to turn our focus to sequence modeling. This course (officially labelled course #5 of the deep learning specialization taught by Andrew Ng) is divided into three modules:Ready? Lets jump into module 1!The objectives behind the first module of course 5 are:Dont worry if these abbreviations sound daunting  well clear them up in no time.To answer this question, Ill show you a few examples where sequence models are used in real-world scenarios.Speech recognition:Quite a common application these days (everyone with a smartphone will know about this). Here, the input is an audio clip and the model has to produce the text transcript. The audio is considered a sequence as it plays over time. Also, the transcript is a sequence of words.Sentiment Classification:Another popular application of sequence models. We pass a text sentence as input and the model has to predict the sentiment of the sentence (positive, negative, angry, elated, etc.). The output can also be in the form of ratings or stars.
DNA sequence analysis:Given a DNA sequence as input, we want our model to predict which part of the DNA belongs to which protein.
Machine Translation:We input a sentence in one language, say French, and we want our model to convert it into another language, say English. Here, both the input and the output are sequences:
Video activity recognition:This is actually a very upcoming (and current trending) use of sequence models. The model predicts what activity is going on in a given video. Here, the input is a sequence of frames.
Name entity recognition:Definitely my favorite sequence model use case. As shown below, we pass a sentence as input and want our model to identify the people in that sentence:
Now before we go further, we need todiscuss a few important notations that you will see throughout the article.We represent a sentence with X. To understand further notifications, lets take a sample sentence:X : Harry and Hermione invented a new spell.Now, to represent each word of the sentence, we use x<t>:For the above sentence, the output will be:y = 1 0 1 0 0 0 0Here, 1 represents that the word represents a persons name (and 0 means its anything but). Below are a few common notations we generally use:At this point its fair to wonder  how do we represent n individual word in a sequence? Well, this is where we lean on a vocabulary, or a dictionary. This is a list of words that we use in our representations. A vocabulary might look like this:The size of the vocabulary might vary depending on the application. One potential way of making a vocabulary is by picking up the most frequently occurring words from the training set.Now, suppose we want to represent the word harry which is in the 4075th position in our vocabulary. We one-hot encode this vocabulary to represent harry:To generalize, x<t> is an one-hot encoded vector. We will put 1 in the 4075th position and all the remaining words will be represented as 0.If the word is not in our vocabulary, we create an unknown <UNK> tag and add it in the vocabulary. As simple as that!We use Recurrent Neural Networks to learn mapping from X to Y, when either X or Y, or both X and Y, are some sequences. But why cant we just use a standard neural network for these sequence problems?Im glad you asked! Let me explain using an example. Suppose we build the below neural network:There are primarily two problems with this:We need a representation that will help us to parse through different sentence lengths as well as reduce the number of parameters in the model. This is where we use a recurrent neural network. This is how a typical RNN looks like:A RNN takes the first word (x<1>) and feeds it into a neural network layer which predicts an output (y<1>). This process is repeated until the last time step x<Tx>which generates the last output y<Ty>. This is the network where the number of words in input as well as the output are same.The RNN scans through the data in a left to right sequence. Note that the parameters that the RNN uses for each time step are shared. We will have parameters shared between each input and hidden layer (Wax), every timestep (Waa) and between the hidden layer and the output (Wya).So if we are making predictions for x<3>, we will also have information about x<1> and x<2>. A potential weakness of RNN is that it only takes information from the previous timesteps and not from the ones that come later. This problem can be solved using bi-directional RNNs which we will discuss later. For now, lets look at forward propagation steps in a RNN model:a<0> is a vector of all zeros and we calculate the further activations similar to that of a standard neural network:Similarly, we can calculate the output at each time step. The generalized form of these formulae can be written as:We can write these equations in an even more simpler way:We horizontally stack Waa and Wya to get Wa. a<t-1> and x<t> are stacked vertically. Rather than carrying around 2 parameter matrices, we now have just 1 matrix. And that, in a nutshell, is how forward propagation works for recurrent neural networks.You might see this coming  the backpropagation steps work in the opposite direction to forward propagation. We have a loss function which we need to minimize in order to generate accurate predictions. The loss function is given by:We calculate the loss at every timestep and finally sum all these losses to calculate the final loss for a sequence:In forward propagation, we move from left to right, i.e., increasing the indices of time t. In backpropagation, we are going from right to left, i.e., going backward in time (hence the name backpropagation through time).So far, we have seen scenarios where the length of input and output sequences was equal. But what if the length differs? Lets see these different scenarios in the next section.We can have different types of RNNs to deal with use cases where the sequence length differs. These problems can be classified into the following categories:Many-to-manyThe name entity recognition examples we saw earlier fall under this category. We have a sequence of words, and for each word, we have to predict whether it is a name or not. The RNN architecture for such a problem looks like this:For every input word, we predict a corresponding output word.Many-to-oneConsider the sentiment classification problem. We pass a sentence to the model and it returns the sentiment or rating corresponding to that sentence. This is a many-to-one problem where the input sequence can have varied length, whereas there will only be a single output. The RNN architecture for such problems will look something like this:Here, we get a single output at the end of the sentence.One-to-manyConsider the example of music generation where we want to predict the lyrics using the music as input. In such scenarios, the input is just a single word (or a single integer), and the output can be of varied length. The RNN architecture for this type of problems looks like the below:There is one more type of RNN which is popularly used in the industry. Consider the machine translation application where we take an input sentence in one language and translate it into another language. It is a many-to-many problem but the length of the input sequence might or might not be equal to the length of output sequence.In such cases, we have an encoder part and a decoder part. The encoder part reads the input sentence and the decoder translates it to the output sentence:Suppose we are building a speech recognition system and we hear the sentence the apple and pear salad was delicious. What will the model predict  the apple and pair salad was delicious or the apple and pear salad was delicious?I would hope the second sentence! The speech recognition system picks this sentence by using a language model which predicts the probability of each sentence.But how do we build a language model?Suppose we have an input sentence:Cats average 15 hours of sleep a day.The steps to build a language model will be:We take the first input word and make a prediction for that. The output here tells us what is the probability of any word in the dictionary. The second output tells us the probability of the predicted word given the first input word:Each step in our RNN model looks at some set of preceding words to predict the next word. There are various challenges associated with training an RNN model and we will discuss them in the next section.One of the biggest problems with a recurrent neural network is that it runs into vanishing gradients. How? Consider the two sentences:The cat, which already ate a bunch of food, was full.The cat, which already ate a bunch of food, were full.Which of the above two sentences is grammatically correct? Its the first one (read it again in case you missed it!).Basic RNNs are not good at capturing long term dependencies. This is because during backpropagation, gradients from an output y would have a hard time propagating back to affect the weights of earlier layers. So, in basic RNNs, the output is highly affected by inputs closer to that word.If the gradients are exploding, we can clip them by setting a pre-defined threshold.GRUs are a modified form of RNNs. They are highly effective at capturing much longer range dependencies and also help with the vanishing gradient problem. The formula to calculate activation at timestep t is:A hidden unit of RNN looks like the below image:The inputs for a unit are the activations from the previous unit and the input word of that timestep. We calculate the activations and the output at that step. We add a memory cell to this RNN in order to remember the words present far away from the current word. Lets look at the equations for GRU:c<t> = a<t>where c is a memory cell.At every timestep, we overwrite the c<t> value as:This acts as a candidate for updating the c<t> value. We also define an update gate which decides whether or not we update the memory cell. The equation for update gate is:Notice that we are using sigmoid to calculate the update value. Hence, the output of update gate will always be between 0 and 1. We use the previous memory cell value and the update gate output to update the memory cell. The update equation for c<t> is given by:When the gate value is 0, c<t> = c<t-1>, i.e., we do not update c<t>. When the gate value is 1, c<t> = c<t> and the value is updated. Lets understand this mind-bending concept using an example:We have the gate value as 1 when we are at the word cat. For all other words in the sequence, the gate value is 0 and hence the information of the cat will be carried till the word was. We expect the model to predict was in place of were.This is how GRUs helps to memorize long term dependencies. Here is a visualization to help you understand the working of a GRU:For every unit, we have three inputs : a<t-1>, c<t-1> and x<t> and three outputs : a<t>, c<t> and y(hat)<t>.LSTMs are all the rage in deep learning these days. They might not have a lot of industry applications right now because of their complexity but trust me, they will very, very soon. Its worth taking out time to learn this concept  it will come in handy in the future.Now to understand LSTMs, lets recall all the equations we saw for GRU:We have just added one more gate while calculating the relevance for c<t> and this gate tells us how relevant c<t-1> is for updating c<t>. For GRUs, a<t> = c<t>.LSTM is a more generalized and powerful version of GRU. The equation for LSTM is:This is similar to that of GRU, right? We are just using a<t-1> instead of c<t-1>. We also have an update gate:We also have a forget gate and an output gate in LSTM. The equations for these gates are similar to that of the update gate:Finally, we update the c<t> value as:And the activations for the next layer will be:So which algorithm should you use  GRU or LSTM?Each algorithm has its advantages. Youll find that their accuracy varies depending on the kind of problem youre trying to solve. The advantage with GRU is that it has a simpler architecture and hence we can build bigger models, but LSTM is more powerful and effective as it has 3 gates instead of 2.The RNN architectures we have seen so far focus only on the previous information in a sequence. How awesome would it be if our model can take into account both the previousas well as the later information of the sequence while making predictions at a particular timestep?Yes, thats possible! Welcome to the world of bidirectional RNNs. But before I introduce you to what Bidirectional RNNs are and how they work, lets first look at why we need them.Consider the named entity recognition problem where we want to know whether a word in a sequence represents a person. We have the following example:He said, Teddy bears are on sale!If we feed this sentence to a simple RNN, the model will predict Teddy to be the name of a person. It just doesnt take into account what comes after that word. We can fix this issue with the help of bidirectional RNNs.Now suppose we have an input sequence of 4 words. A bidirectional RNN will look like:To calculate the output from a RNN unit, we use the following formula:Similarly, we can have bidirectional GRUs and bidirectional LSTMs. One disadvantage of using bidirectional RNNs is that we have to look at the entire sequence of data before making any prediction. But the standard B-RNN algorithm is actually very effective for building and designing most NLP applications.Remember what a deep neural network looks like?We have an input layer, some hidden layers and finally an output layer. A deep RNN also looks like this. We take a similar network and unroll that in time:Here, the generalized notation for activations is given as:a[l]<t> = activation of lth later at time tSuppose we want to calculate a[2]<3> :Thats it for deep RNNs. Take a deep breath, that was quite a handful to digest in one go. And now, time to move to module 2!The objectives for studying the second module are:Up to this point, we have used a vocabulary to represent words:To represent a single word, we created a one-hot vector:Now, suppose we want our model to generalize between different words. We train the model on the following sentence:I want a glass of Apple juice.We have given I want a glass of Apple as the training sequence and juice as the target. We want our model to generalize the solution of, say:I want a glass of Orange ____ .Why wont our previous vocabulary approach work? Because it lacks the flexibility to generalize. We will try to calculate the similarity between the vector representing the words Apple and Orange. Well inevitably get zero as output as the product of any two one-hot vectors is always zero.Instead of having a one-hot vector for representations, what if we represent each word with a set of features? Check out this table:We can find similar words using this approach. Quite useful, isnt it?Say, we have 300 features for each word. So, for example, the word Man will be represented bya 300 dimensional vector named e5391.We can also use these representations for visualization purposes. Convert the 300 dimensional vector into a 2-d vector and then plot it. Quite a few algorithms exist for doing this but my favorite one is easily t-SNE.Word embeddings really help us to generalize well when working with word representations.Suppose you are performing a named entity recognition task and only have a few records in the training set. In such a case, you can either take pretrained word embeddings available online or create your own embeddings. These embeddings will have features for all the words in that vocabulary.Here are the two primary steps for replacing one-hot encoded representations with word embeddings:Next, we will look at the properties of word embeddings.Suppose you get a question  If Man is to Woman, then King is to?. Most keen puzzle solvers will have seen these kind of questions before!The likely answer to this question is Queen. But how will the model decide that? This is actually one of the most widely used examples to understand word embeddings. We have embeddings for Man, Woman, King and Queen. The embedding vector of Man will be similar to that of Woman and the embedding vector of King will be similar to that of Queen.We can use the following equation:eman  ewoman = eking  ?Solving this gives us a 300 dimensional vector with a value equal to the embeddings of queen. We can use a similarity function to determine the similarity between two word embeddings as well. The similarity function is given by:This is a cosine similarity. We can also use the Euclidean distance formula:There are a few other different types of similarity measures which youll find in core recommendation systems.We actually end up learning an embedding matrix when we implement a word embeddings algorithm. If were given a vocabulary of 10,000 words and each word has 300 features, the embedding matrix, represented as E, will look like this:To find the embeddings of the word orange which is at the 6257th position, we multiply the above embedding matrix with the one-hot vector of orange:E . O6257 = e6257The shape of E is (300, 10k), and of O is (10k, 1). Hence, the embedding vector e will be of the shape (300, 1).Consider we are building a language model using a neural network. The input to the model is I want a glass of orange and we want the model to predict the next word.We will first learn the embeddings of each of the words in the sequence using a pre trained word embedding matrix and then pass those embeddings to a neural network which will have a softmax classifier at the end to predict the next word.This is how the architecture will look like. In this example we have 6 input words, each word is represented by a 300 dimensional vector and hence the input of the sequence will be 6*300 = 1800 dimensional. The parameters for this model are:We can reduce the number of input words, to decrease the input dimensions. We can say that we want our model to use previous 4 words only to make prediction. In this case the input will be 1200 dimensional. The input can also be referred as context and there can be various ways to select the context. Few possible ways are:This is how we can solve language modeling problem where we input the context and predict some target words. In the next section, we will look at how Word2Vec can be applied for learning word embeddings.It is a simple and more efficient way to learn word embeddings. Consider we have a sentence in our training set:I want a glass of orange juice to go along with my cereal.We use a skip gram model to pick a few context and target words. In this way we create a supervised learning problem where we have an input and its corresponding output. For context, instead of having only last 4 words or last 1 word, we randomly pick a word to be the context word and then randomly pick another word within some window (say 5 to the left and right) and set that as the target word. Some of the possible context  target pairs could be:These are only few pairs, we can have many more pairs as well. Below are the details of the model:Vocab size = 10,000kNow, we want to learn a mapping from some context (c) to some target (t). This is how we do the mapping:Oc -> E -> ec -> softmax -> y(hat)Here, ec = E.OcHere softmax is calculating the probability of getting the target word (t) as output given the context word (c).Finally, we calculate the loss as:Using a softmax function creates a couple of problems to the algorithm, one of them is computational cost. Everytime we calculate the probability:We have to carry out the sum over all 10,000 words in the vocabulary. If we use a larger vocabulary of say 100,000 words or even more the computation gets really slow. Few solutions to this problem are:Using a hierarchical softmax classifier. So, instead of classifying some word into 10,000 categories in one go, we first classify it into either first 5000 categories or later 5000 categories, and so on. In this way we do not have to compute the sum over all 10,000 words every time. The flow of hierarchical softmax classifier looks like:One question that might arise in your mind is how to choose the context c? One way could be to sample the context word at random. The problem with random sampling is that the common words like is, the will appear more frequently whereas the unique words like orange, apple might not even appear once. So, we try to choose a method which gives more weightage to less frequent words and less weightage to more frequent words.In the next section we will see a technique that helps us to reduce the computation cost and learn much better word embeddings.In the skip gram models, as we have seen earlier, we map context words to target words which allows us to learn word embeddings. One downside of that model was high computational cost due to softmax. Consider the same example that we took earlier:I want a glass of orange juice to go along with my cereal.What negative sampling will do is, it creates a new supervised learning problem, where given a pair of words say orange and juice, we will predict whether it is a context-target pair? For the above example, the new supervised learning problem will look like:Since orange-juice is a context-target pair, we set the Target value as 1, whereas, orange-king is not a pair for above example, and hence Target is 0. These 0 values represent that it is a negative sample. We now apply a logistic regression to calculate the probability of whether the pair is a context-target pair or not. The probability is given by:We can have k pair of words for training the model. k can range between 5-20 for smaller dataset while for larger dataset, we choose smaller k (2-5). So, if we build a neural network and the input is orange (one hot vector of orange):We will have 10,000 possible classification problems each corresponding to different words from the vocabulary. So, this network will tell all the possible target words corresponding to the context word orange. Here, instead of having one giant 10,000 way softmax, which is computationally very slow, we have 10,000 binary classification problems which is comparatively faster as compared to the softmax.Context word is chosen from the sequence and once it is chosen, we randomly pick another word from the sequence to be a positive sample and then pick few of the other random words from the vocabulary as negative samples. In this way, we can learn word embeddings using simple binary classification problems. Next we will see even simpler algorithm for learning word embeddings.We will work on the same example:I want a glass of orange juice to go along with my cereal.Previously, we were sampling pairs of words (context and target) by picking two words that appears in close proximity to each other from our text corpus. GloVe or Global Vectors for word representation makes it more explicit. Lets say:Xij = number of times i appears in context of jHere, i is similar to the target (t) and j is similar to the context (c). GloVe minimizes the following:Here, f(Xij) is the weighing term. It gives less weightage to more frequent words (such as stop words like this, is, of, a, ..) and more weightage to less frequent words. Also, f(Xij) = 0 when (Xij) = 0. It has been found that minimizing the above equation finally leads to a good word embeddings. Now, we have seen many algorithms for learning word embeddings. Next we will see the application using word embeddings.You must already be well aware of what sentiment classification is so Ill make this quick. Check out the below table which contains some text and its corresponding sentiment:The applications of sentiment classification are varied, diverse and HUGE. But In most cases youll encounter, the training doesnt come labelled. This is where word embeddings come to the rescue. Lets see how we can use word embeddings to build a sentiment classification model.We have the input as: The dessert is excellent.Here, E is the pretrained embedding matrix of, say, 100 billion words. We multiple the one-hot encoded vectors of each word with the embedding matrix to get the word representations. Next, we sum up all these embeddings and apply a softmax classifier to decide what should be the rating of that review.It only takes the mean of all the words, so if the review is negative but it has more positive words, then the model might give it a higher rating. Not a great idea. So instead of just summing up the embedding to get the output, we can use an RNN for sentiment classification.This is a many-to-one problem where we have a sequence of inputs and a single output. You are now well equipped to solve this problem. Welcome to the final module of the series! Below are the two objectives we will primarily achieve in this module:Im going to keep this section industry relevant, so well cover models which are useful for applications like machine translation, speech recognition, etc. Consider this example  we are tasked with building a sequence-to-sequence model where we want to input a French sentence and translate it into English. The problem will look like:Here x<1>, x<2> are the inputs and y<1>, Y<2> are outputs. To build a model for this, we have an encoder part which takes an input sequence. The encoder is built as an RNN, or LSTM, or GRU. After the encoder part, we build a decoder network which takes the encoding output as input and is trained to generate the translation of the sentence.This network is popularly used for Image Captioning as well. As input, we have the images features (generated using a convolutional neural network).The decoder model of a machine translation system is quite similar to that of a language model. But there is one key difference between the two. In a language model, we start with a vector of all zeros, whereas in machine translation, we have an encoder network:The encoder part of the machine translation model is a conditional language model where we are calculating the probability of outputs given an input:Now, for the input sentence:We can have multiple translations like:We want the best translation out of all the above sentences.The good news? There is an algorithm that helps us choose the most likely translation.This is one of the most commonly used algorithms for generating the most likely translations. The algorithm can be understood using the below 3 steps:Step 1:It picks the first translated word and calculates its probability:Instead of just picking one word, we can set a bean width (B) to say B=3. It will pick the top 3 words that can possibly be the first translated word. These three words are then stored in the computers memory.Step 2:Now, for each selected word in step 1, this algorithm calculates the probability of what the second word could be:If the beam width is 3 and there are 10,000 words in the vocabulary, the total number of possible combinations will be 3 * 10,000 = 30,000. We evaluate all these 30,000 combinations and pick the top 3 combinations.Step 3:We repeat this process until we get to the end of the sentence.By adding one word at a time, beam search decides the most likely translation for any given sentence. Lets look at some of the refinements we can do to beam search in order to make it more effective.Beam search maximizes this probability:This probability is calculated by multiplying probabilities of different words. Since particular probabilities are very tiny numbers (between 0 and 1), if we multiply such small numbers multiple time, final output is very small which creates problem in computations. So, instead we can use the following formula to calculate the probabilities:So, instead of maximizing the products, we maximize the log of a product. Even using this objective function, if the translated sentence has more words, their product will go down to more negative values, and hence we can normalize the function as:So, for all the sentences selected using beam search, we calculate this normalized log likelihood and then pick the sentence which gives the highest value. There is one more detail that I would like to share and it is how to decide the beam width B?If the beam width is more, we will have better results but the algorithm will become slow. On the other hand, choosing smaller B will make the algorithm run faster but the results will not be accurate. There is no hard rule to choose beam width and it can vary according to the applications. We can try different values and then choose the one that gives the best results.Beam search is an approximation algorithm which outputs the most likely translations based on the beam width. But it is not always necessary that it will generate the correct translation everytime. If we are not getting the correct translations, we have to analyse whether it is due to the beam search or our RNN model is causing problems. If we find that the beam search is causing the problem, we can increase the beam width and hopefully we will get better results. How to decide whether we should focus on improving the beam search or the model?Suppose the actual translation is:Jane visits Africa in September (y*)And the translation that we got from the algorithm is:Jane visited Africa last September (y(hat))RNN will compute P(y* | x) and P(y(hat) | x)Case 1:P(y* | x) > P(y(hat) | x)This means beam search chose y(hat) but y* attains higher probability. So, beam search is at fault and we might consider increasing the beam width.Case 2:P(y* | x) <= P(y(hat) | x)This means that y* is better translation than y(hat) but RNN predicted the opposite. Here, RNN is at fault and we have to improve the model.So, for each translation, we decide whether RNN is at fault or the beam search. Finally we figure out what fraction of errors are caused due to beam search vs RNN model and update either beam search or RNN model based on which one is more at fault. In this way we can improve the translations.Up to this point, we have seen the encoder-decoder architecture for machine translation where one RNN reads the input and the other one outputs a sentence. But when we get very long sentences as input, it becomes very hard for the model to memorize the entire sentence.What attention models do is they take small samples from the long sentence and translate them, then take another sample and translate them, and so on.We use an alpha parameter to decide how much attention should be given to a particular input word while we generate the output.<1,2> = For generating the first word, how much attention should be given to the second input wordLets understand this with an example:So, for generating the first output y<1>, we take attention weights for each word. This is how we compute the attention:If we have Tx input words and Ty output words, then the total attention parameters will be Tx * Ty.You might already have gathered this  Attention models are one of the most powerful ideas in deep learning.Sequence models are awesome, arent they? They have a ton of practical applications  we just need to know the right technique to use in specific situations. And my hope is that you will have learned those techniques in this guide.Word embeddings are a great way to represent words and we saw how these word embeddings can be built and use. We have gone through different applications of word embeddings and finally we covered attention models as well which are one of the most powerful ideas for building sequence models.If you have any query or feedback related to the article, feel free to share them in the comments section below. Looking forward to your responses!",https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/
27 Amazing Data Science Books Every Data Scientist Should Read,Learn everything about Analytics|Introduction|Books on Statistics|Books on Probability|Books on Machine Learning|Books on Deep Learning|Books on Natural Language Processing (NLP)|Books on Computer Vision|Books on Artificial Intelligence|Books on Python|Books on R,"Bonus:|Statistics in Plain English|Think Stats: Probability and Statistics for Programmers|Introduction to Statistical Learning|Probability: For the Enthusiastic Beginner|Introduction to Probability|An Introduction to Probability Theory and its Applications|The Hundred-Page Machine Learning Book|Machine Learning|Elements of Statistical Learning|Deep Learning|Deep Learning with Python|Neural Networks and Deep Learning|Natural Language Processing with Python|Foundations of Statistical Natural Language Processing|Speech and Language Processing|Computer Vision: Algorithms and Applications|Programming Computer Vision with Python|Computer Vision: Models, Learning, and Inference|Artificial Intelligence: A Modern Approach|Artificial Intelligence for Humans|The Master Algorithm|Fluent Python: Clear, Concise, and Effective Programming|Programming Python: Powerful Object-Oriented Programming|Mastering Python for Data Science|R for Data Science|R for Everyone|R Cookbook|Share this:|Like this:|Related Articles|Must-Read Tutorial to Learn Sequence Modeling (deeplearning.ai Course #5)|Get Started with PyTorch  Learn How to Build Quick & Accurate Neural Networks (with 4 Case Studies!)|
Pranav Dar
|8 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Every person has their own way of learning. What helped me break into data science was books. There is nothing like opening your mind to a world of knowledge condensed into a few hundred pages. There is a magic and allure to books that I have never found in any other medium of learning.If you only read the books that everyone else is reading, you can only think what everyone else is thinking.  Haruki MurakamiLearning Data Science on your own can be a very daunting task! There are numerous ways to learn today  MOOCs, workshops, degrees, diplomas, articles, and so on. But putting them in a structure and focusing on a structured path to become a data scientist is of paramount importance.But there are hundreds of books out there about data science. How do you choose where to start? Which books are ideal for learning a certain technique or domain? While theres no one-shoe-fits-all answer to this, I have done my best to cut down the list to these 27 books well see shortly.I have divided the books into different domains to make things easier for you:At the bottom of the article, you will find a superbly illustrated infographic mentioning each book. You can use that as a to-read shelf and strike them off as you go down the list! You can also download a High Resolution copy of this infographic. Its perfect for printing as its in a PDF format.
Without any further ado, lets dive right in.Author: Timothy C. UrdanI started my journey into the world of statistics with this beauty of a book. Its written for absolute beginners and in a way that makes you come back for more. The writing style and explanations provided do justice to the title  Statistics in Plain English. You could recommend it to any non-technical person and they would get the hang of these topics, its that good!Author: Allen B. DowneyYoull find this book at the top of most data science book lists. The book comes with plenty of resources. Use the above link to go to the book home page and youll see resources like data files, codes, solutions, etc. It will be especially useful for folks who know the basics of Python. The language is used to demonstrate real world examples.Authors: Gareth James, Daniela Witten, Trevor Hastie and Robert TibshiraniAn all-time classic. This book is recommended or referenced in most machine learning courses Ive come across, its just that well written. It covers basic statistics as well as machine learning techniques. The awesome thing about this book is that each concept is explained with case studies in R. So once you have a handle on programming, you can always come back and try out each concept again. What better way to ingrain a concept than by practicing it multiple times?Author: David MorinIdeal book for beginners. It is written for college students so all of you looking to learn probability from scratch will appreciate the way this is written. All the basics are covered  combinatorics, the rules of probability, Bayes theorem, expectation value, variance, probability density, common distributions, the law of large numbers, the central limit theorem, correlation, and regression.Authors: J. Laurie Snell and Charles Miller GrinsteadAnother introductory book covering basic probability concepts. Like the book above, this one is a comprehensive text written with college graduate students in mind. Why do I keep repeating that, you might be wondering. Its because I want to emphasize that if theres a place to start learning from scratch, its a book thats written for students who havent ever ventured into this field before.Author: William FellerAs the books description states, its a complete guide to the theory and practical applications of probability theory. I recommend reading this if you really want to deep dive into the world of probability. Its a VERY comprehensive text and might not be to a beginners taste. If youre learning probability just to get into data science, you can get away with reading either of the two probability books mentioned above.Author: Andriy BurkovI love this book. Having read a ton of books trying to teach machine learning from various angles and perspectives, I struggled to find one that could succinctly summarize difficult topics and equations. Until Andriy Burkov managed to do it in some 100-odd pages. It is beautifully written, is easy to understand and has been endorsed by thought leaders like Peter Norvig. Need I say more? Beginner or established, every data scientist should get their hands on this book.Author: Tom MitchellBefore all the hype came about, Tom Mitchells book on machine learning was the go-to text to understand the math behind various techniques and algorithms. I would suggest brushing up on your math before taking this up. But you dont need any background in AI or statistics to understand these concepts. It was the first-ever book I read on ML! Its modestly priced so its definitely worth adding to your collection.Authors: Trevor Hastie, Robert Tibshirani and Jerome FriedmanAnd were back with another classic by Hastie and Tibsharani! Its the natural successor to the Introduction to Statistical Learning book we covered earlier. While there are a few overlaps with that book, this one takes a more advanced look at what we call machine learning algorithms. Topics like neural networks, matrix factorization, spectral clustering are covered apart from the common ML techniques.Authors: Ian Goodfellow, Yoshua Bengio and Aaron CourvilleWhat a list of rockstar authors! The Deep Learning book is widely regarded as the best resource for beginners. Its divided into three sections: Applied Math and Machine Learning Basics, Modern Practical Deep Learning Frameworks, and Deep Learning Research. It is to-date the most cited book in the deep learning community. Keep it by your bedside, worship it and reference it often  this will be your companion whenever you start your deep learning journey.Author: Francois CholletA really cool way of learning deep learning (or machine learning for that matter) is by programming side-by-side with the theory. And thats the approach Francois Chollet follows in the Deep Learning with Python book. Concepts are taught using the popular Keras library. Francois is the creator of Keras so who better to teach you this topic? I also recommend following Francois on Twitter  there is a lot we can learn from him.Author: Michael NielsenThis is a free online book to learn about the core component that powers deep learning  neural networks. I quite like the way this book has been written. It takes a practical approach to teaching and looks at deep learning topics from the lens of a beginner. You will not learn any programming language in this book  its a good old fashioned text book on the underlying insights behind neural networks.Authors: Steven Bird, Ewan Klein and Edward LoperAnother book in this collection which sticks to the learn by doing policy. Youll pick up Python concepts you otherwise wouldnt have and will navigate the world of NLP using the NLTK library (Natural Language Toolkit). While this shouldnt be the only resource you refer to for learning NLP (its far too complex a field for that), it offers a pretty decent introduction to the topic.Authors: Christopher Manning and Hinrich SchutzePublished almost two decades ago, this text still serves as an excellent introduction to natural languages processing. Its a very comprehensive guide to the broader sub-topics in NLP, like Text Categorization, Parts-of-Speech Tagging, Probabilistic Parsing, among various other things. The authors have provided a rigorous coverage of mathematical and linguistic foundations. Again, the book is quite detailed so keep that in mind.Authors: Daniel Jurafsky and James H. MartinThe emphasis of this book is on practical applications and scientific evaluation in the scope of natural language and speech. I included this book to expand our horizons beyond text  to look at speech recognition as well. And why not? Its an area of research that is thriving nowadays with a plethora of applications coming out everyday. Jurafsky and Martin have written an in-depth book on NLP and computational linguistics. This one is from the masters themselves.Author: Richard SzeliskiExplore a variety of common computer vision techniques in this book, especially ones used for analyzing and interpreting images. While this was published almost 9 years ago, the examples and methodology illustrated by Richard Szeliski are applicable today as well. Its a comprehensive text that takes a scientific approach to solving basic vision challenges. The website I have linked to above contains a free PDF copy of the bookAuthor: Jan Erik SolemBefore you dive into this awesome book, go to the website Ive linked above and download the datasets, the code notebooks and clone the GitHub repository mentioned there. They are excellent companions in this REALLY hands-on introduction to the world of computer vision. As the author states, Youll learn techniques for object recognition, 3D reconstruction, stereo imaging, augmented reality, and other computer vision applications as you follow clear examples written in Python.Author: Dr. Simon J.D. PrinceThe book starts off from scratch by introducing us to the concepts of probability and quickly picks up pace from there. While some of the frameworks introduced here have seen more advanced versions come out, this book is nonetheless relevant in the current context. More than 70 algorithms have been introduced and the text is beautifully complemented by over 350 illustrations. The website also contains PowerPoint slides, if thats the kind of learning you prefer.Authors: Stuart Russell and Peter NorvigA book written by Stuart Russell and Peter Norvig? I am sold. It is the leading book in Artificial Intelligence. More than 1300 universities in over 100 countries reference/cite this book in their curriculum. Given who the authors are, it isnt surprising to see the book length  1100 pages. Covering the length and breadth of AI components  speech recognition, autonomous vehicles, machine translation, and computer vision among other things, this can be considered the Bible of AI.Author: Jeff HeatonWhat are the foundational algorithms underneath artificial intelligence? This book packs a lot of technical know-how into just 222 pages. This is volume 1 of a series of books on the techniques behind AI (dimensionality, distance metrics, clustering, error calculation, hill climbing, Nelder Mead, and linear regression). There is an accompanying site as well which contains examples cited in the book + a GitHub repository containing the code.Author: Pedro DomingosIf youre looking for a technical book on AI, this isnt it. What it is, however, is a masterful text on how machine learning is remaking business, politics, science and war. It is a thoughtful and thought-provoking book on where AI is right now, and where it might end up taking the human race. Will we ever find a single algorithm (or The Master Algorithm) that is capable of driving all knowledge from data? Join Pedro Domingos in his quest to find out.Author: Luciano RamalhoThere are way too many resources out there to learn Python but nothing teaches you programming like a good old-fashioned book. As you might expect from a coding book, its a hands-on guide to help you understand how Python works and how to write awesome and effective Python code. Luciano Ramalho also covers a few popular libraries youll find yourself regularly using in data science projects. With a length of 794 pages, this book is worth the spend.Author: Mark LutzWait, another Python book?! If you thought the above book taught you everything you need to know about Python, think again. This is a vast programming language with a lot more left to cover. Once youve mastered the fundamentals from the above book by Luciano Ramalho, take a gander on this one by Mark Lutz. There are in-depth tutorials on a wide variety of topics: databases, networking, text processing, GUIs, etc. Tons and tons of examples are included. A must-read for programming geeks.Author: Samir MadhavanThe two books we have covered so far for learning Python looked at the language from a programming perspective. Now its time to learn it from the data science angle. Which data science libraries are commonly used and how? How can you create data visualizations and mine for patterns in Python? And how can you code advanced data science/machine learning techniques to build models? These questions and more are answered by Samir Madhavan in this excellent write-up.Authors: Garrett Grolemund and Hadley WickhamAnyone who has remotely heard of R programming will have brushed across Hadley Wickhams work. His work in this language is unparalleled  I could go on and on about him. I couldnt recommend this book highly enough. Youll learn how to import different kinds of data into R, the different data structures, and how to transform, visualize and model your data. The perfect book to learn data science through coding in R.Author: Jared P. LanderI learned R way before I even heard about Python. I have a special place for it in my heart and Jared Landers R for Everyone played a big part in that. I got this book through one of my acquaintances and was immediately taken by how well it was written. It claims to be for everyone and lives up to its name. This is a great book if youre from a non-technical and non-statistical background.Author: Paul TeetorThe R Cookbook is an excellent addition to your budding data science reading list. It contains more than 200 practical recipes to help you get started with analyzing and manipulating data in R. Each recipe looks at a different problem. Its meant for beginners, intermediate users and advanced practitioners alike. Whether its learning new programming skills or brushing up your concepts, this cookbook is for everyone.And as promised, here is the full infographic covering all the books we saw in this article:",https://www.analyticsvidhya.com/blog/2019/01/27-amazing-data-science-books-every-data-scientist-should-read/
Get Started with PyTorch  Learn How to Build Quick & Accurate Neural Networks (with 4 Case Studies!),Learn everything about Analytics|Introduction|What isPyTorch?|Building Neural Nets usingPyTorch|Use Case 1: Handwritten Digital Classification|Use Case 2: Object Image Classification|Use Case 3: Sentiment Text Classification|Use Case #4: Image Style Transfer|End Notes|References,"Contents|Share this:|Like this:|Related Articles|27 Amazing Data Science Books Every Data Scientist Should Read|Build your First Image Classification Model in just 10 Minutes!|
Shivam Bansal
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"PyTorch v TensorFlow  how many times have you seen this polarizing question pop up on social media? The rise of deep learning in recent times has been fuelled by the popularity of these frameworks. There are staunch supporters of both, but a clear winner has started to emerge in the last year.PyTorch was one of the most popular frameworks in 2018. It quickly became the preferred go-to deep learning framework among researchers in both academia and the industry. After using PyTorch for the last few weeks, I can confirm that it is highly flexible and an easy-to-use deep learning library.In this article, we will explore what PyTorch is all about. But our learning wont stop with the theory  we will code through 4 different use cases and see how well PyTorch performs. Building deep learning models has never been this fun!Note: This article assumes that you have a basic understanding of deep learning concepts. If not, I recommend going through this article.Lets understand what PyTorch is and why it has become so popular lately, before diving into its implementation.PyTorch is a Python based scientific computing package that is similar to NumPy, but with the added power of GPUs. It is also a deep learning framework that provides maximum flexibility and speed during implementing and building deep neural network architectures.Recently, PyTorch 1.0 was released and it was aimed to assist researchers by addressing four major challenges:Intrinsically, there are two main characteristics of PyTorch that distinguish it from other deep learning frameworks:Imperative Programming: PyTorch performs computations as it goes through each line of the written code. This is quite similar to how a Python program is executed. This concept is called imperative programming. The biggest advantage of this feature is that your code and programming logic can be debugged on the fly.Dynamic Computation Graphing: PyTorch is referred to as a defined by run framework, which means that the computational graph structure (of a neural network architecture) is generated during run time. The main advantage of this property is that it provides a flexible and programmatic runtime interface that facilitates the construction and modification of systems by connecting operations. In PyTorch, a new computational graph is defined at each forward pass. This is in stark contrast to TensorFlow which uses a static graph representation.PyTorch 1.0 comes with an important feature called torch.jit,a high-level compiler that allows the user to separate the models and code. It also supports efficient model optimization on custom hardware, such as GPUs or TPUs.Lets understand PyTorch through a more practical lens. Learning theory is good, but it isnt much use if you dont put it into practice!A PyTorch implementation of a neural network looks exactly like a NumPy implementation. The goal of this section is to showcase the equivalent nature of PyTorch and NumPy. For this purpose, lets create a simple three-layered network having 5 nodes in the input layer, 3 in the hidden layer, and 1 in the output layer. We will use only one training example with one row which has five features and one target.The first step is to doparameter initialization. Here,the weights and bias parameters for each layer are initialized as the tensor variables. Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices.After the parameter initialization step, a neural network can be defined and trained in four key steps:Lets see each of these steps in a bit more detail.Forward Propagation: In this step, activations are calculated at every layer using the two steps shown below. These activations flow in the forward direction from the input layer to the output layer in order to generate the final output.The following code blocks show how we can write these steps in PyTorch. Notice that most of the functions, such as exponential and matrix multiplication, are similar to the ones in NumPy.Loss Computation: In this step, the error (also called loss) is calculated in the output layer. A simple loss function can tell the difference between the actual value and the predicted value. Later, we will look at different loss functions available in PyTorch.Backpropagation: The aim of this step is to minimize the error in the output layer by making marginal changes in the bias and the weights. These marginal changes are computed using the derivatives of the error term.Based on the Calculus principle of the Chain rule, the delta changes are back passed to hidden layers where corresponding changes in their weights and bias are made. This leads to an adjustment in the weights and bias until the error is minimized.Updating the Parameters: Finally, the weights and bias are updated using the delta changes received from the above backpropagation step.Finally, when these steps are executed for a number of epochs with a large number of training examples, the loss is reduced to a minimum value. The final weight and bias values are obtained which can then be used to make predictions on the unseen data.In the previous section, we saw a simple use case of PyTorch for writing a neural network from scratch. In this section, we will use different utility packages provided within PyTorch (nn, autograd, optim, torchvision, torchtext, etc.) to build and train neural networks.Neural networks can be defined and managed easily using these packages. In our use case, we will create a Multi-Layered Perceptron (MLP) network for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.The first step, as with any project youll work on, isdata preprocessing. We need totransform the raw dataset into tensors and normalize them in a fixed range. The torchvisionpackage provides a utility called transforms which can be used to combine different transformations together.The first transformation converts the raw data into tensor variables and the second transformation performs normalization using the below operation:x_normalized = x-mean / stdThe values 0.5 and 0.5 represent the mean and standard deviation for 3 channels: red, green, and blue.Another excellent utility of PyTorch is DataLoader iterators which provide the ability to batch, shuffle and load the data in parallel using multiprocessing workers. For the purpose of evaluating our model, we will partition our data into training and validation sets.The neural network architectures in PyTorch can be defined in a class which inherits the properties from the base class from nn package called Module. This inheritance from thenn.Module class allows us to implement, access, and call a number of methods easily. We can define all the layers inside the constructor of the class, and the forward propagation steps inside the forward function.We will define a network with the following layer configurations: [784, 128,10]. This configuration represents the 784 nodes (28*28 pixels) in the input layer, 128 in the hidden layer, and 10 in the output layer. Inside the forward function, we will use the sigmoid activation function in the hidden layer (which can be accessed from the nn module).Define the loss function and the optimizer using the nn and optim package:We are now ready to train the model. The core steps will remain the same as we saw earlier: Forward Propagation, Loss Computation, Backpropagation, and updating the parameters.Once the model is trained, make the predictions on the validation data.Lets take things up a notch.In this use case, we will create convolutional neural network (CNN) architectures in PyTorch. We will perform object image classification using the popular CIFAR-10 dataset. This dataset is also included in the torchvision package. The entire procedure to define and train the model will remain the same as the previous use case, except the introduction of additional layers in the network.Lets load and transform the dataset:We will create the architecture with three convolutional layers for low-level feature extraction, three pooling layers for maximum information extraction, and two linear layers for linear classification.Define the loss function and the optimizer:Once the model is trained, we can generate predictions on the validation set.Well pivot from computer vision use cases to natural language processing. The idea is to showcase the utility of PyTorch in a variety of domains.In this section, well leverage PyTorch for text classification tasks using RNN (Recurrent Neural Networks) and LSTM (Long Short Term Memory) layers. First, we will load a dataset containing two fieldstext and target. The target contains two classes, class1 and class2, and our task is to classify each text into one of these classes.You can download the datasethere.I highly recommend setting seeds before getting into the heavy coding. This ensures that the results you will see are the same as mine  a very useful (and helpful) feature when learning new concepts.In the preprocessing step, convert the text data into a padded sequence of tokens so that it can be passed into embedding layers. I will use the utilities provided in the Keras package, but the same can be done using the torchtext package as well.Next, we need to convert the tokens into vectors. I will use pretrained GloVe word embeddings for this purpose. We will load these word embeddings and create an embedding matrix containing the word vector for every word in the vocabulary.Define the model architecture with embedding layers and LSTM layers:Create training and validation sets:Define loss and optimizers:Training the model:Finally, we can obtain the predictions:Lets look at one final use case where we will perform artistic style transfer. This is one of the most creative projects I have worked on and hopefully youll have fun with this as well. The basic idea behind the style transfer concept is:This concept was introduced in the paper: Image Style Transfer using Convolutional Networks. An example of style transfer is shown below:Awesome, right? Lets look at its implementation in PyTorch. The process involves six steps:Object Related Features: In the original paper, the authors have suggested that more valuable information about objects and context can be extracted from the initial layers of the network. This is because in the higher layers, the information space becomes more complex and detailed pixel information is lost.Style Related Features: In order to obtain the texture information from the second image, the authors used correlations between different features in different layers. This is explained in detail in point 4 below.But before get there, lets look at the structure of a typical VGG19 model:For object information extraction, Conv42 was the layer of interest. Its present in the 4th convolutional block with a depth of 512. For style representation, the layers of interest were the first convolutional layer of every convolutional block in the network, i.e., conv11, conv21, conv31, conv41, and conv51. These layers were selected purely based on the authors experiments and I am only replicating their results in this article.In the end, we can view the predicted results. I ran it for only a small number of iterations, but one can run up to 3000 iterations (if computation resources are no bar!).There are plenty of other use cases where PyTorch can, and has been used. It has quickly become the darling of researchers around the globe. The majority of the open source libraries and developments youll see happening nowadays have a PyTorch implementation available on GitHub.In this article, I have illustrated what PyTorch is and how you can get started with implementing it in different use cases. One should treat this guide as the starting point. The performance in every use case can be improved with more data, more fine-tuning of network parameters, and most importantly, applying creative skills while building network architectures. Thanks for reading and do leave your feedback in the comments section below.",https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/
Build your First Image Classification Model in just 10 Minutes!,Learn everything about Analytics|Introduction|Project to apply Image Classification|Problem Statement|Table of Contents|What is Image Classification?||Setting up the Structure of our Image Data|Breaking Down the Process of Model Building|Setting up the Problem Statement and Understanding the Data|Steps to Build our Model|Picking up a Different Challenge|End Notes,"Stage 1: Loading and pre-processing the data|Stage 2: Defining the models architecture|Stage 3: Training the model|Stage 4: Estimating the models performance|Share this:|Like this:|Related Articles|Get Started with PyTorch  Learn How to Build Quick & Accurate Neural Networks (with 4 Case Studies!)|A Hands-On Introduction to Time Series Classification (with Python Code)|
Pulkit Sharma
|60 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Build a deep learning model in a few minutes? Itll take hours to train! I dont even have a good enough machine. Ive heard this countless times from aspiring data scientists who shy away from building deep learning models on their own machines.You dont need to be working for Google or other big tech firms to work on deep learning datasets! It is entirely possible to build your own neural network from the ground up in a matter of minutes without needing to lease out Googles servers. Fast.ais studentsdesigned a model on the Imagenet dataset in 18 minutes  and I will showcase something similar in this article.Deep learning is a vast field so well narrow our focus a bit and take up the challenge of solving an Image Classification project. Additionally, well be using a very simple deep learning architecture to achieve a pretty impressive accuracy score.You can consider the Python code well see in this article as a benchmark for building Image Classification models. Once you get a good grasp on the concept, go ahead and play around with the code, participate in competitions and climb up the leaderboard!If youre new to deep learning and are fascinated by the field of computer vision (who isnt?!), do check out the Computer Vision using Deep Learning course. Its a comprehensive introduction to this wonderful field and will set you up for what is inevitably going to a huge job market in the near future.More than 25% of the entire revenue in E-Commerce is attributed to apparel & accessories. A major problem they face is categorizing these apparels from just the images especially when the categories provided by the brands are inconsistent. This poses an interesting computer vision problem that has caught the eyes of several deep learning researchers.Fashion MNIST is a drop-in replacement for the very well known, machine learning hello world  MNIST dataset which can be checked out atIdentify the digitspractice problem. Instead of digits, the images show a type of apparel e.g. T-shirt, trousers, bag, etc. The dataset used in this problem was created by Zalando Research.Practice NowConsider the below image:You will have instantly recognized it  its a (swanky) car. Take a step back and analyze how you came to this conclusion  you were shown an image and you classified the class it belonged to (a car, in this instance). And that, in a nutshell, is what image classification is all about.There are potentiallyn number of categories in which a given image can be classified. Manually checking and classifying images is a very tedious process. The task becomes near impossible when were faced with a massive number of images, say 10,000 or even 100,000. How useful would it be if we could automate this entire process and quickly label images per their corresponding class?Self-driving cars are a great example to understand where image classification is used in the real-world. To enable autonomous driving, we can build an image classification model that recognizes various objects, such as vehicles, people, moving objects, etc. on the road. Well see a couple more use cases later in this article but there are plenty more applications around us. Use the comments section below the article to let me know what potential use cases you can come with up!Now that we have a handle on our subject matter, lets dive into how an image classification model is built, what are the prerequisites for it, and how it can be implemented in Python.Our data needs to be in a particular format in order to solve an image classification problem. We will see this in action in a couple of sections but just keep these pointers in mind till we get there.You should have 2 folders, one for the train set and the other for the test set. In the training set, you will have a .csv file and an image folder:The .csv file in our test set is different from the one present in the training set. This test set .csv file contains the names of all the test images, but they do not have any corresponding labels. Can you guess why? Our model will be trained on the images present in the training set and the label predictions will happen on the testing set imagesIf your data is not in the format described above, you will need to convert it accordingly (otherwise the predictions will be awry and fairly useless).Before we deep dive into the Python code, lets take a moment to understand how an image classification model is typically designed. We can divide this process broadly into 4 stages. Each stage requires a certain amount of time to execute:Let me explain each of the above steps in a bit more detail. This section is crucial because not every model is built in the first go. You will need to go back after each iteration, fine-tune your steps, and run it again. Having a solid understanding of the underlying concepts will go a long way in accelerating the entire process.Data is gold as far as deep learning models are concerned. Your image classification model has a far better chance of performing well if you have a good amount of images in the training set. Also, the shape of the data varies according to the architecture/framework that we use.Hence, the critical data pre-processing step (the eternally important step in any project). I highly recommend going through the Basics of Image Processing in Python to understand more about how pre-processing works with image data.But we are not quite there yet. In order to see how our model performs on unseen data (and before exposing it to the test set), we need to create a validation set. This is done by partitioning the training set data.In short, we train the model on the training data and validate it on the validation data. Once we are satisfied with the models performance on the validation set, we can use it for making predictions on the test data.Time required for this step: We require around 2-3 minutesfor this task.This is another crucial step in our deep learning model building process. We have to define how our model will look and that requires answering questions like:And many more. These are essentially the hyperparameters of the model which play a MASSIVE part in deciding how good the predictions will be.How do we decide these values? Excellent question! A good idea is to pick these values based on existing research/studies. Another idea is to keep experimenting with the values until you find the best match but this can be quite a time consuming process.Time required for this step: It should take around 1 minute to define the architecture of the model.For training the model, we require:We also define the number of epochs in this step. For starters, we will run the model for 10 epochs (you can change the number of epochs later).Time required for this step: Since training requires the model to learn structures, we need around 5 minutes to go through this step.And now time to make predictions!Finally, we load the test data (images) and go through the pre-processing step here as well. We then predict the classes for these images using the trained model.Time required for this step: ~ 1 minute.We will be picking up a really cool challenge to understand image classification. We have to build a model that can classify a given set of images according to the apparel (shirt, trousers, shoes, socks, etc.). Its actually a problem faced by many e-commerce retailers which makes it an even more interesting computer vision problem.This challenge is called Identify the Apparels and is one of the practice problems we have on our DataHack platform. You will have to register and download the dataset from the above link.We have a total of 70,000 images (28 x 28 dimension), out of which 60,000 are from the training set and 10,000 from the test one. The training images are pre-labelled according to the apparel type with 10 total classes. The test images are, of course, not labelled. The challenge is to identify the type of apparel present in all the test images.We will build our model on Google Colab since it provides a free GPU to train our models.Time to fire up your Python skills and get your hands dirty. We are finally at the implementation part of our learning!Lets look at each step in detail.Step 1: Setting up Google ColabSince were importing our data from a Google Drive link, well need to add a few lines of code in our Google Colab notebook. Create a new Python 3 notebook and write the following code blocks:This will install PyDrive. Now we will import a few required libraries:Next, we will create a drive variable to access Google Drive:To download the dataset, we will use the ID of the file uploaded on Google Drive:Replace the id in the above code with the ID of your file. Now we will download this file and unzip it:You have to run these code blocks every time you start your notebook.Step 2 : Import the libraries well need during our model building phase.Step 3: Recall the pre-processing steps we discussed earlier. Well be using them here after loading the data.Next, we will read all the training images, store them in a list, and finally convert that list into a numpy array.As it is a multi-class classification problem (10 classes), we will one-hot encode the target variable.Step 4: Creating a validation set from the training data.Step 5: Define the model structure.We will create a simple architecture with 2 convolutional layers, one dense hidden layer and an output layer.Next, we will compile the model weve created.Step 6: Training the model.In this step, we will train the model on the training set images and validate it using, you guessed it, the validation set.Step 7: Making predictions!Well initially follow the steps we performed when dealing with the training data. Load the test images and predict their classes using themodel.predict_classes() function.Lets import the test file:Now, we will read and store all the test images:We will also create a submission file to upload on the DataHack platform page (to see how our results fare on the leaderboard).Download this sample_cnn.csv file and upload it on the contest page to generate your results and check your ranking on the leaderboard. This will give you a benchmark solution to get you started with any Image Classification problem!You can try hyperparameter tuning and regularization techniques to improve your models performance further. I ecnourage you to check out this article to understand this fine-tuning step in much more detail  A Comprehensive Tutorial to learn Convolutional Neural Networks from Scratch.Lets test our learning on a different dataset. Well be cracking the Identify the Digits practice problem in this section. Go ahead and download the dataset. Before you proceed further, try to solve this on your own. You already have the tools to solve it  you just need to apply them! Come back here to check your results or if you get stuck at some point.In this challenge, we need to identify the digit in a given image. We have a total of 70,000 images  49,000 labelled ones in the training set and the remaining 21,000 in the test set (the test images are unlabelled). We need to identify/predict the class of these unlabelled images.Ready to begin? Awesome! Create a new Python 3 notebook and run the following code:Submit this file on the practice problem page to get a pretty decent accuracy number. Its a good start but theres always scope for improvement. Keep playing around with the hyperparameter values and see if you can improve on our basic model.Who said deep learning models required hours or days to train. My aim here was to showcase that you can come up with a pretty decent deep learning model in double-quick time. You should pick up similar challenges and try to code them from your end as well. Theres nothing like learning by doing!The top data scientists and analysts have these codes ready before aHackathoneven begins. They use these codes to make early submissions before diving into a detailed analysis. Once they have a benchmark solution, they start improving their model using different techniques.Did you find this article helpful? Do share your valuable feedback in the comments section below. Feel free to share your complete code notebooks as well which will be helpful to our community members.",https://www.analyticsvidhya.com/blog/2019/01/build-image-classification-model-10-minutes/
A Hands-On Introduction to Time Series Classification (with Python Code),Learn everything about Analytics|Introduction|Table of contents|Introduction to Time Series Classification|Setting up the Problem Statement|Reading and Understanding the Data|Preprocessing Steps|Building a Time Series Classification model|End Notes,"1) Classifying ECG/EEG signals|2) Image Classification|3) Classifying Motion Sensor Data|Share this:|Like this:|Related Articles|Build your First Image Classification Model in just 10 Minutes!|DataHack Radio #15: Exploring the Applications & Potential of Reinforcement Learning with Xander Steenbrugge|
Aishwarya Singh
|13 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Classifying time series data? Is that really possible? What could potentially be the use of doing that? These are just some of the questions you must have had when you read the title of this article. And its only fair  I had the exact same thoughts when I first came across this concept!The time series data most of us are exposed to deals primarily with generating forecasts. Whether thats predicting the demand or sales of a product, the count of passengers in an airline or the closing price of a particular stock, we are used to leveraging tried and tested time series techniques for forecasting requirements.But as the amount of data being generated increases exponentially, so does the opportunity to experiment with new ideas and algorithms. Working with complex time series datasets is still a niche field, and its always helpful to expand your repertoire to include new ideas.And that is what I aim to do in the article by introducing you to the novel concept of time series classification. We will first understand what this topic means and its applications in the industry. But we wont stop at the theory part  well get our hands dirty by working on a time series dataset and performing binary time series classification. Learning by doing  this will help you understand the concept in a practical manner as well.If you have not worked on a time series problem before, I highly recommend first starting with some basic forecasting. You can go through the below article for starters:Time series classification has actually been around for a while. But it has so far mostly been limited to research labs, rather than industry applications. But there is a lot of research going on, new datasets being created and a number of new algorithms being proposed. When I first came across this time series classification concept, my initial thought was  how can we classify a time series and what does a time series classification data look like? Im sure you must be wondering the same thing.As you can imagine, time series classification data differs from a regular classification problem since the attributes have an ordered sequence. Lets have a look at some time series classification use cases to understand this difference.ECG, or electrocardiogram, records the electrical activity of the heart and is widely be used to diagnose various heart problems. These ECG signals are captured using external electrodes.For example, consider the following signal sample which represents the electrical activity for one heartbeat. The image on the left represents a normal heartbeat while the one adjacent to it represents a Myocardial Infarction.The data captured from the electrodes will be in time series form, and the signals can be classified into different classes. We can also classify EEG signals which record the electrical activity of the brain.Images can also be in a sequential time-dependent format. Consider the following scenario:Crops are grown in a particular field depending upon the weather conditions, soil fertility, availability of water and other external factors. A picture of this field is taken daily for 5 years and labeled with the name of the crop planted on the field. Do you see where Im going with this? The images in the dataset are taken after a fixed time interval and have a defined sequence, which can be an important factor in classifying the images.Sensors generate high-frequency data that can identify the movement of objects in their range. By setting up multiple wireless sensors and observing the change in signal strength in the sensors, we can identify the objects direction of movement.What other applications can you think of where we can apply time series classification? Let me know in the comments section below the article.We will be working on the Indoor User Movement Prediction problem. In this challenge, multiple motion sensors are placed in different rooms and the goal is to identify whether an individual has moved across rooms, based on the frequency data captured from these motion sensors.There are four motion sensors (A1, A2, A3, A4) placed across two rooms. Have a look at the below image which illustrates where the sensors are positioned in each room. The setup in these two rooms was created in 3 different pairs of rooms (group1, group2, group3).A person can move along any of the six pre-defined paths shown in the above image. If a person walks on path 2, 3, 4 or 6, he moves within the room. On the other hand, if a person follows path 1 or path 5, we can say that the person has moved between the rooms.The sensor reading can be used to identify the position of a person at a given point in time. As the person moves in the room or across rooms, the reading in the sensor changes. This change can be used to identify the path of the person.Now that the problem statement is clear, its time to get down to coding! In the next section, we will look at the dataset for the problem which should help clear up any lingering questions you might have on this statement. You can download the dataset from this link: Indoor User Movement Prediction.Our dataset comprises of 316 files:Lets have a look at the datasets. Well start by importing the necessary libraries.Before loading all the files, lets take a quick sneak peek into the data we are going to deal with. Reading the first two files from the movement data:The files contain normalized data from the four sensors  A1, A2, A3, A4. The length of the csv files (number of rows) vary, since the data corresponding to each csv is for a different duration. To simplify things, let us suppose the sensor data is collected every second. The first reading was for a duration of 27 seconds (so 27 rows), while another reading was for 26 seconds (so 26 rows).We will have to deal with this varying length before we build our model. For now, we will read and store the values from the sensors in a list using the following code block:We now have a list sequences that contains the data from the motion sensors and targets which holds the labels for the csv files. When we print sequences[0], we get the values of sensors from the first csv file:As mentioned previously, the dataset was collected in three different pairs of rooms  hence three groups. This information can be used to divide the dataset into train, test and validation sets. We will load the DatasetGroup csv file now:We will take the data from the first two sets for training purposes and the third group for testing.Since the time series data is of varying length, we cannot directly build a model on this dataset. So how can we decide the ideal length of a series? There are multiple ways in which we can deal with it and here are a few ideas (I would love to hear your suggestions in the comment section):Lets find out the minimum, maximum and mean length:Most of the files have lengths between 40 to 60. Just 3 files are coming up with a length more than 100. Thus, taking the minimum or maximum length does not make much sense. The 90th quartile comes out to be 60, which is taken as the length of sequence for the data. Lets code it out:Now that the dataset is prepared, we will separate it based on the groups. Preparing the train, validation and test sets:We have prepared the data to be used for an LSTM (Long Short Term Memory) model. We dealt with the variable length sequence and created the train, validation and test sets. Lets build a single layer LSTM network.Note: You can get acquainted with LSTMs in this wonderfully explained tutorial. I would advice you to go through that first as itll help you understand how the below code works.We will now train the model and monitor the validation accuracy:I got an accuracy score of 0.78846153846153844. Its quite a promising start but we can definitely improve the performance of the LSTM modelby playing around with the hyperparameters, changing the learning rate, and/or the number of epochs as well.That brings us to the end of this tutorial. The idea behind penning this down was to introduce you to a whole new world in the time series spectrum in a practical manner.Personally, I found the preprocessing step the most complex section of all the ones we covered. Yet, it is the most essential one as well (otherwise the whole point of time series data will fail!). Feeding the right data to the model is equally important when working on this kind of challenge.Here is a really cool time series classification resource which I referred to and found the most helpful:I would love to hear your thoughts and suggestions in the comment section below.",https://www.analyticsvidhya.com/blog/2019/01/introduction-time-series-classification/
DataHack Radio #15: Exploring the Applications & Potential of Reinforcement Learning with Xander Steenbrugge,Learn everything about Analytics|Introduction|Xander Steenbrugges Background|Foray into the World of Reinforcement Learning|Current State of Reinforcement Learning in the Industry|Challenges in Reinforcement Learning|Resources to Get Started with Reinforcement Learning|End Notes,"Share this:|Like this:|Related Articles|A Hands-On Introduction to Time Series Classification (with Python Code)|A Comprehensive Learning Path for Deep Learning in 2019|
Pranav Dar
|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy  
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If intelligence was a cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.  Yann LeCun,Founding Father of Convolutional NetsReinforcement learning algorithms have been knocking on the door of industrial applications in recent years. Will they finally blow the door wide open in 2019? What are some of the biggest obstacles holding back reinforcement learning? And is there a ceiling we can put on where RL will take us in the future?We welcome 2019 on DataHack Radio with a stellar episode #15 featuring Xander Steenbrugge, as he navigates us through the wide-ranging and intricate world of reinforcement learning. And yes, those above questions have been very expertly handled in this episode.Xander has a knack of taking the most complex topics and breaking them down into easy-to-understand concepts, a truly invaluable asset. I came across Xander thanks to his popular YouTube channel Arxiv Insights and truly appreciated his presentation style when I saw him live at DataHack Summit 2018. His ability to explain challenging subjects is on full display in this episode as well.This article aims to highlight the key aspects discussed in this episode, including Xanders thoughts about reinforcement learning and related topics. I encourage you to listen to the full episode where Xander elaborates on his RL theories and ideas in much more detail. Happy listening!You can subscribe to DataHack Radio on any of the below platforms to receive notifications every time a new episode is published or to trawl through our archives:Xander pursued civil engineering from the University of Ghent, Belgium during his graduation. His entire education background was focused on electronics, such as making transistors, microcircuits, etc. He picked up coding with the aim of making the idea to execution phase much faster than it was when working purely with electronics.Not surprisingly, Xanders final thesis for his Masters degree (finished in 2015) was onbrain-computer interfaces that could perform brainwave (EEG) classification. You might have seen an application of this system on YouTube  when a patient puts on a headset wired with this mechanism, he/she can move the cursor on a connected computer screen with his/her thoughts.There was a ton of pre-processing work involved since the EEG signal data had a ton of noise. Once the data was cleaned, Xander performed manual feature extraction before feeding the data to a machine learning classifier. Neural networks were still in their relative infancy back when Xander was working on his project. Given the same data now, he would love to straightaway apply CNNs (convolutional neural networks) to the EEG signals. Fascinating stuff!Xander, working as a machine learning consultant, came across the 2015 paper by DeepMind where they introduced the DQN algorithm. The fact that you could play any kind of game with the same algorithm? That breakthrough really intrigued Xander and led him to explore this wonderfully complex field of reinforcement learning. Heres his take on this line of work at a very simplistic level:Its not as difficult as it seems. Its supervised learning, but with a few tweaks.  XanderIs it really that simple, you ask? Heres a summary of Xander explaining his thought process by putting these two types of learning in contrast:The difference between supervised learning and reinforcement learning is that for RL, we have an agent that is moving around in an environment with the ability to take actions (like moving in a specific direction). This agent could be an algorithm, or a person, or an object. The action it takes affect the input that comes from the environment. Only once the agent is put through a few iterations can we tell how far away it is from achieving the end goal. When it comes to supervised learning, the input and output are already very well defined from the start.A reinforcement learning system can learn to do something that we as humans dont know how to do.  XanderIts no secret that progress in reinforcement learning has been slower than other domains. The idea to execution phase Xander referred to earlier takes a lot of time in RL. In academia, these agents are trained in simulations (like the ATARI game environment) because these algorithms are very sample inefficient. In other words, we need to show these agent a whole host of examples before they learn something substantial.When we get to a real-world setting, that amount of data is more often than not sparse (as a lot of data scientists will relate to!). Additionally, we would need the algorithm to generalize to a different setting depending on what the requirement is. These are two major challenges that have held up reinforcement learnings penetration into commercial products and services.Having said that, Xander mentioned a really cool use case where reinforcement learning has successfully been applied  robotics farming. Listen to the podcast to understand the granular aspects of how this technology works.We are at the start of a very big revolution, where we could go from hard-coded robots to smart learning robots.  XanderAnother interesting nugget from the podcast  most of the research is still focused on single-agent reinforcement learning (in comparison to multi-agent RL) as there are still a plethora of problems left to solve there.Here are two major obstacles we face with the current state of reinforcement learning:Reinforcement learning is a huge field encompassing multiple topics and subjects. Right now, theres no one singular platform that offers you a straight path into this space. According to Xander, first understanding supervised learning from scratch is a good idea since reinforcement learning builds upon that foundation. So familiarize yourself with how an image classifier works before jumping into RL concepts.Xanders learning journey started from this blog post by Andrej Karpathy, called Pong from Pixels. Its a slightly lengthy read, but clearly illustrates how one can go from supervised learning to reinforcement learning. If youre looking for a more visually appealing guide, check out Xanders Introduction to Reinforcement Learning video:Heres another excellent introduction to RL for beginnersby Faizan Shaikh. You should also check out OpenAIs educational resource on RL titled Spinning Up. Its a comprehensive list of resources and topics  it has personally been super helpful for me.A very pleasant and crisp introduction to reinforcement learning in under 50 minutes. I had very little idea about multi-agent reinforcement learning before this podcast so that was a really cool section for me. Xanders list of resources that I shared above is good enough to get your hands dirty so I hope to see a lot more of you from our community taking up RL in the near future.An exquisite podcast to kick off 2019. Theres a lot more coming on DataHack Radio this year so keep your learning hats on and till then, happy listening!",https://www.analyticsvidhya.com/blog/2019/01/datahack-radio-reinforcement-learning-xander/
A Comprehensive Learning Path for Deep Learning in 2019,Learn everything about Analytics|Introduction|Who is this Learning Path for?|Summary  Learning Path for Deep Learning in 2019,"Go ahead and access the most comprehensive learning path for deep learning to follow in 2019! You will need to register on the training portal which will also enable you to track your progress in each section.|Share this:|Like this:|Related Articles|DataHack Radio #15: Exploring the Applications & Potential of Reinforcement Learning with Xander Steenbrugge|The Ultimate Learning Path to Become a Data Scientist and Master Machine Learning in 2019|
Pranav Dar
|7 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"If there is one area in data science which has led to the growth of Machine Learning and Artificial Intelligence in the last few years, it is Deep Learning. From research labs in universities with low success in industry to powering every smart device on the planet  Deep Learning and Neural Networks have started a revolution.Deep learning is ubiquitous  whether its Computer Vision applications or breakthroughs in the field of Natural Language Processing, we are living in a deep learning-fueled world.Thanks to the rapid advances in technology, more and more people are able to leverage the power of deep learning. At the same time, it is a complex field and can appear daunting for newcomers.With these things in mind, we are thrilled to launch a comprehensive learning path for deep learning in 2019! This learning path is filled with resources like books, courses and articles, and also has tests/quizzes to apply your freshly acquired knowledge.But what truly differentiates our learning path from anything else out there is the structure we have put around this path.This essentially takes the confusion out of the entire learning process and lets your focus on what matters  DEEP LEARNING!This learning path has been designed for anyonewho wants to learn Deep Learning, regardless of your level.This structured path will be especially useful for folks looking to get straight to the action by learning through doing. Whether you are a complete fresher, or transitioning from a different field, or are looking to upskill yourself, this plan should give you the necessary direction.If you plan to improve your existing deep learning skills or complement them with advanced concepts  this plan will guide you through that journey.Here is a high-level overview of the core concepts you should know (and master) in the deep learning sphere:You can view and enroll in the deep learning path here.Additionally, we have created a cool infographic to give you an idea of what to expect from the learning path, and to act as a checklist for your journey. Its a month-by-month break down of what you should learn, and what skills/knowledge you should complement that monthly effort with (as shown in the white boxes below).",https://www.analyticsvidhya.com/blog/2019/01/comprehensive-learning-path-deep-learning-2019/
The Ultimate Learning Path to Become a Data Scientist and Master Machine Learning in 2019,Learn everything about Analytics|Introduction|Summary  Learning Path to become a Data Scientist in 2019,"Access the complete learning path to become a data scientist in 2019 here. You will need to register yourself on our training portal  this will enable you to keep track of what you have covered.|Share this:|Like this:|Related Articles|A Comprehensive Learning Path for Deep Learning in 2019|The 15 Most Popular Data Science and Machine Learning Articles on Analytics Vidhya in 2018|
Pranav Dar
|19 Comments|Top Analytics Vidhya Users|Join the nextgen data science ecosystem|Popular posts|Recent Posts|

How Search Engines like Google Retrieve Results: Introduction to Information Extraction using Python and spaCy 
|

4 Unique Methods to Optimize your Python Code for Data Science 
|

A Beginner-Friendly Guide to PyTorch and How it Works from Scratch 
|

9 Powerful Tips and Tricks for Working with Image Data using skimage in Python 
",No header found,ANALYTICS VIDHYA|DATA SCIENTISTS|COMPANIES| JOIN OUR COMMUNITY :|,No header found,"Learning paths are immensely popular among our readers and with good reason! Learning paths take away the pain and confusion from the learning process. For those who dont know what a learning path is  we take the pain of going through all the resources available on data science, machine learning and Artificial Intelligence, select the best ones, and arrange them in a logical sequence for you to follow.If this sounds like a lot of work  it is. And it is done with the purpose of eliminating that huge amount of work you would otherwise have to do. We understand that when you are starting out in data science, the sheer amount of resources can be overwhelming. And hence we created learning paths. And our learning paths were an instant success with our community.If learning paths were useful in 2018, this year they would be the most helpful tool for a person trying to become a data scientist. Why? Because the amount of content and information out there has increased multi-folds. And so has the confusion and the amount of knowledge expected out of a data scientist.Broadly, the learning path to become a data scientist can be divided into the following steps:We have broken down all these steps month on month  so if you start following the learning path, you know exactly what you need to follow and what you need to cover every month starting today.You can access the full learning path here and register yourself to start your journey today. Our training portal enables you to track your progress after each section thus helping you to stay on track throughout the year.Here is an image laying out what should you do month on month to become a data scientist by the end of 2019. If you put in all the efforts as mentioned in the learning path  you will be well placed to get into a data scientist role before the end of the year.We have one more gift to make your new year truly special. Join Analytics Vidhyas CEO and Founder Kunal Jain on January 10th for an exclusive webinar where he will elaborate on how to get the most out of this learning path. He will discuss the roadmap to become a data scientist in 2019.Get your questions answered and your doubts clarified by one of the eminent personalities in this field!We have done all the hard work to make sure you have all you need on 1st January to become a data scientist this year. Now, it is your turn. As always, if you have any questions  do let us know. See you more often on Analytics Vidhya in 2019!",https://www.analyticsvidhya.com/blog/2019/01/learning-path-data-scientist-machine-learning-2019/
